2020.cl-2.8,P12-2034,0,0.0855961,"Missing"
2020.cl-2.8,P19-3019,0,0.0644081,"Missing"
2020.cl-2.8,C18-1158,0,0.0224283,"“neural fake news detector,” a linear classifier on top of the hidden state of the last token of the examined article, fine-tuned to classify whether the news text was machine-generated or not. The experiments in this article are based on the Grover-Mega classifier, fine-tuned for the target task (see Section 3). Fake News Detection Approaches Beyond Stylometry. The other most extensively studied NLP-based approach for fake news detection is based on fact-checking. This approach has recently gained increasing attention thanks to several synthetic (Thorne et al. 2018) and real-world data sets (Hanselowski et al. 2018; Wang 2017; Popat et al. 2018; Augenstein et al. 2019). The performance of current models is still far from that of humans (Thorne et al. 2019; Schuster et al. 2019), but with their advancements they can still play a positive role in detection. Another line of work for fake news detection utilizes non-textual information such as how content is propagated, by which users, its originating URL, and other metadata (Castillo, Mendoza, and Poblete 2011; Gupta et al. 2014; Zhao, Resnick, and Mei 2015; Kochkina, Liakata, and Zubiaga 2018; Liu and Wu 2018), as well as incorporation of users’ explicit"
2020.cl-2.8,C18-1288,0,0.076973,"Missing"
2020.cl-2.8,P09-2078,0,0.26331,"Missing"
2020.cl-2.8,P14-1095,0,0.0327736,"common application of stylometry is detecting human-written misinformation. Mihalcea and Strapparava (2009) found specific words that are highly correlated with true and false statements. Ott et al. (2011) and Feng, Banerjee, and Choi (2012) used a richer set of features such as POS tag frequencies and constituency structure to identify deceptive writing. Following these observations and the increasing interest in fake news, recent studies applied stylometry on entire news articles (Pisarevskaya 2017), short news reports (P´erez-Rosas et al. 2018), fact and political statements (Nakashole and Mitchell 2014; Rashkin et al. 2017), and posts in social media (Volkova et al. 2017). The success of these studies is mostly attributed to stylistic changes in human language when lying or deceiving (Bond and Lee 2005; Frank, Menasco, and O’Sullivan 2008). In this work, we evaluate the viability of this approach on machine-generated text, where stylistic differences between truth and lie might be more subtle. Machine-Generated Text Detection. Detecting text’s provenance is similar to authorship attribution and, therefore, stylometry can be effective. Indeed, Gehrmann, Strobelt, and Rush (2019) show the exi"
2020.cl-2.8,P11-1032,0,0.576439,"me 46, Number 2 1. Introduction Many previous studies on stylometry—the extraction of stylistic features from written text—showed promising results on text classification. Two of stylometry’s common applications are: (1) Detecting the provenance of text (i.e., identifying the author) in order to prevent impersonations (Tweedie, Singh, and Holmes 1996; Brennan, Afroz, and Greenstadt 2012; Afroz et al. 2014; Caliskan-Islam et al. 2015; Neal et al. 2017; Sari, Vlachos, and Stevenson 2017); and (2) Detecting misinformation in text due to deception (Enos et al. 2007; Mihalcea and Strapparava 2009; Ott et al. 2011; Feng, Banerjee, and Choi 2012; Afroz, Brennan, and Greenstadt 2012), fake news (Rashkin et al. 2017; P´erez-Rosas et al. 2018), or other false or illegal content (Choshen et al. 2019). In the former, the classifier identifies language features that correlate with a specific person or group. The latter, misinformation detection, relies on idiosyncrasies of lies, namely, style and language characteristics that are unique to text that is false or misleading. Stylometry has recently gained attention as a potential answer to concerns that language models (LMs) could be used to mass-produce malici"
2020.cl-2.8,C18-1287,0,0.0828122,"Missing"
2020.cl-2.8,W17-4213,0,0.210752,"tly, as detailed later in this article, stylometry was used to distinguish machine- from human-writers. Another common application of stylometry is detecting human-written misinformation. Mihalcea and Strapparava (2009) found specific words that are highly correlated with true and false statements. Ott et al. (2011) and Feng, Banerjee, and Choi (2012) used a richer set of features such as POS tag frequencies and constituency structure to identify deceptive writing. Following these observations and the increasing interest in fake news, recent studies applied stylometry on entire news articles (Pisarevskaya 2017), short news reports (P´erez-Rosas et al. 2018), fact and political statements (Nakashole and Mitchell 2014; Rashkin et al. 2017), and posts in social media (Volkova et al. 2017). The success of these studies is mostly attributed to stylistic changes in human language when lying or deceiving (Bond and Lee 2005; Frank, Menasco, and O’Sullivan 2008). In this work, we evaluate the viability of this approach on machine-generated text, where stylistic differences between truth and lie might be more subtle. Machine-Generated Text Detection. Detecting text’s provenance is similar to authorship attrib"
2020.cl-2.8,D18-1003,0,0.0492307,"r classifier on top of the hidden state of the last token of the examined article, fine-tuned to classify whether the news text was machine-generated or not. The experiments in this article are based on the Grover-Mega classifier, fine-tuned for the target task (see Section 3). Fake News Detection Approaches Beyond Stylometry. The other most extensively studied NLP-based approach for fake news detection is based on fact-checking. This approach has recently gained increasing attention thanks to several synthetic (Thorne et al. 2018) and real-world data sets (Hanselowski et al. 2018; Wang 2017; Popat et al. 2018; Augenstein et al. 2019). The performance of current models is still far from that of humans (Thorne et al. 2019; Schuster et al. 2019), but with their advancements they can still play a positive role in detection. Another line of work for fake news detection utilizes non-textual information such as how content is propagated, by which users, its originating URL, and other metadata (Castillo, Mendoza, and Poblete 2011; Gupta et al. 2014; Zhao, Resnick, and Mei 2015; Kochkina, Liakata, and Zubiaga 2018; Liu and Wu 2018), as well as incorporation of users’ explicit feedback, such as abuse report"
2020.cl-2.8,P18-1022,0,0.0497899,"tic features and applied a neural network to the same task. Although these classifiers could be fooled by an aware writer that intentionally imitates other’s style (Brennan, Afroz, and Greenstadt 2012), this approach was found useful for de-anonymizing cybercriminals in forums (Afroz et al. 2014), identifying programmers (Caliskan-Islam et al. 2015), and more (Neal et al. 2017). In a related line of study, stylometry was applied to, rather than detecting the specific person, identifying characteristics of the author, such as age and gender (Goswami, Sarkar, and Rustagi 2009), political views (Potthast et al. 2018), or IQ (Abramov and Yampolskiy 2019). Recently, as detailed later in this article, stylometry was used to distinguish machine- from human-writers. Another common application of stylometry is detecting human-written misinformation. Mihalcea and Strapparava (2009) found specific words that are highly correlated with true and false statements. Ott et al. (2011) and Feng, Banerjee, and Choi (2012) used a richer set of features such as POS tag frequencies and constituency structure to identify deceptive writing. Following these observations and the increasing interest in fake news, recent studies"
2020.cl-2.8,D17-1317,0,0.145693,"Missing"
2020.cl-2.8,E17-2043,0,0.0315245,"Missing"
2020.cl-2.8,D19-1341,1,0.8918,"Missing"
2020.cl-2.8,D19-1292,0,0.0225455,"r the news text was machine-generated or not. The experiments in this article are based on the Grover-Mega classifier, fine-tuned for the target task (see Section 3). Fake News Detection Approaches Beyond Stylometry. The other most extensively studied NLP-based approach for fake news detection is based on fact-checking. This approach has recently gained increasing attention thanks to several synthetic (Thorne et al. 2018) and real-world data sets (Hanselowski et al. 2018; Wang 2017; Popat et al. 2018; Augenstein et al. 2019). The performance of current models is still far from that of humans (Thorne et al. 2019; Schuster et al. 2019), but with their advancements they can still play a positive role in detection. Another line of work for fake news detection utilizes non-textual information such as how content is propagated, by which users, its originating URL, and other metadata (Castillo, Mendoza, and Poblete 2011; Gupta et al. 2014; Zhao, Resnick, and Mei 2015; Kochkina, Liakata, and Zubiaga 2018; Liu and Wu 2018), as well as incorporation of users’ explicit feedback, such as abuse reporting (Tschiatschek et al. 2018). Social network platforms, ISPs, and even individual users can use such methods to"
2020.cl-2.8,W18-5501,0,0.0296083,"n a large news corpus. Grover also includes a “neural fake news detector,” a linear classifier on top of the hidden state of the last token of the examined article, fine-tuned to classify whether the news text was machine-generated or not. The experiments in this article are based on the Grover-Mega classifier, fine-tuned for the target task (see Section 3). Fake News Detection Approaches Beyond Stylometry. The other most extensively studied NLP-based approach for fake news detection is based on fact-checking. This approach has recently gained increasing attention thanks to several synthetic (Thorne et al. 2018) and real-world data sets (Hanselowski et al. 2018; Wang 2017; Popat et al. 2018; Augenstein et al. 2019). The performance of current models is still far from that of humans (Thorne et al. 2019; Schuster et al. 2019), but with their advancements they can still play a positive role in detection. Another line of work for fake news detection utilizes non-textual information such as how content is propagated, by which users, its originating URL, and other metadata (Castillo, Mendoza, and Poblete 2011; Gupta et al. 2014; Zhao, Resnick, and Mei 2015; Kochkina, Liakata, and Zubiaga 2018; Liu and Wu 2"
2020.cl-2.8,W17-2623,0,0.161831,"AD, the U.S. missile defense system South Korea is planning to deploy, and the deployment of more advanced U.S. military equipment as part of the North&apos;s armada’ move out of its east coast. Mr. Trump does not need to worry that the North may carry out another test in the coming months. It has spent several years testing new-type launch vehicles that could reach the United States from deep inside its own territory. Figure 1 Examples of the fake class in our experiments. (a) In the news question answering (Section 4), a CNN article is presented with two examples of questions (bold) from newsQA (Trischler et al. 2017) and Grover’s generated answer (red). The first answer is verified by a human annotator to be false and the second as true. (b) In article modification (m = 6) (Section 4), the negations are marked with a cross-line for deletions and underline for addition. (c) In the article extension case (Section 5), the bold red text is the generation of GPT-2 Medium to extend the prefix. Attack and Defense Capabilities. We adopt an adversarial setting similar to that of Zellers et al. (2019). Our attacker wishes to generate fake text, that contains unverified or false claims, en masse, using a language mo"
2020.cl-2.8,P17-2102,0,0.132648,"nformation. Mihalcea and Strapparava (2009) found specific words that are highly correlated with true and false statements. Ott et al. (2011) and Feng, Banerjee, and Choi (2012) used a richer set of features such as POS tag frequencies and constituency structure to identify deceptive writing. Following these observations and the increasing interest in fake news, recent studies applied stylometry on entire news articles (Pisarevskaya 2017), short news reports (P´erez-Rosas et al. 2018), fact and political statements (Nakashole and Mitchell 2014; Rashkin et al. 2017), and posts in social media (Volkova et al. 2017). The success of these studies is mostly attributed to stylistic changes in human language when lying or deceiving (Bond and Lee 2005; Frank, Menasco, and O’Sullivan 2008). In this work, we evaluate the viability of this approach on machine-generated text, where stylistic differences between truth and lie might be more subtle. Machine-Generated Text Detection. Detecting text’s provenance is similar to authorship attribution and, therefore, stylometry can be effective. Indeed, Gehrmann, Strobelt, and Rush (2019) show the existence of distributional differences between human-written texts and ma"
2020.cl-2.8,P17-2067,0,0.654797,"additions to otherwise truthful news stories. Our data sets contain articles produced by both malicious and responsible uses of language models, and the detector’s task is to identify the malicious ones. In one data set, we produce text by prompting an LM to extend news articles with relevant claims. We simulate malicious user, who only accepts the LM’s suggestion if the claim is factually false, and a responsible user, who only accepts correct claims. The produced sentences are short and concise statements, similarly to fake news and false claims as represented in human-generated data sets (Wang 2017; Augenstein et al. 2019). In another data set, we modify existing news articles to include false information by inverting article statements. In this case, the LM is used to automatically identify the most plausible edit locations. This is similar to (mis-)using an autocorrect tool that suggests local modifications. We find that with the state-of-the-art stylometry-based classifier, even a single autogenerated sentence within a wall of human-written text is detectable with high accuracy, yet the truthfulness of a single sentence remains largely undecidable. Moreover, even a relatively weak LM"
2020.emnlp-main.420,D19-1668,0,0.155867,"ently, we note that the canvas cx,σ depends only on the first t elements t of σ. Hence we can combine into one pass the loss calculations of trajectories that are the same in the first t steps but different at the t + 1 step. Switching 2 We implement a batch version of the algorithm. n X x,σ log p(ax,σ t |ct ; θ) n−tσ (8) t+1 The whole process is illustrated in Algorithm 1. In this way, we can compute in expectation n/2 action losses per pass. 4 Experiments We test BLM’s capacity to rewrite specified portions of text on three tasks: text infilling (Zhu et al., 2019), ancient text restoration (Assael et al., 2019) and style transfer (Shen et al., 2017). Fig. 4 displays example inputs and outputs for these tasks. We also measure the perplexity of BLM on language modeling benchmarks and compare with traditional left-to-right language models. Experimental Details In all experiments, the sequence representations in BLM are obtained using the encoder module of transformer base (Vaswani et al., 2017) (6 layers, 8 heads, dmodel = 512, df f = 2048, dk = dv = 64). The MLP used for blank prediction has one hidden layer of size 1024. Weight decay, learning rate, and dropout are tuned based on the loss on the vali"
2020.emnlp-main.420,P19-1285,0,0.162908,"5 Table 5: The estimated perplexity of BLM with the number of MC samples m on WikiText-103. Table 4: Accuracy and BLEU scores for style transfer. everyone that i spoke with was very helpful and kind . everyone that i spoke with was rude and unprofessional . everyone that i spoke with wasn’t helpful or kind. the beans were in the burro in the rice was nowhere to be found . the beans were in the burro in the rice was the best i found . the beans were in the burro and the rice was plentiful PTB WT2 WT103 LSTM (Grave et al., 2016) AWD-LSTM (Merity et al., 2017) TCN (Bai et al., 2018) Transformer (Dai et al., 2019) Adaptive (Baevski and Auli, 2018) Transformer-XL (Dai et al., 2019) 82.3 57.3 88.7 54.5 99.3 65.8 - 48.7 45.2 30.1 18.7 18.3 InsT (our implementation) BLM 77.3 69.2 91.4 81.2 39.4 42.5 Table 6: Perplexity on the PTB and WikiText datasets. there is definitely not enough room in that part of the venue . there is always enough parking in that part of the venue . there is so much room in that part of the venue it is n’t terrible , but it is n’t very good either . it is n’t fancy , but it is still very good either . it is n’t perfect , but it is very good . Figure 6: Example generations by BLM for"
2020.emnlp-main.420,2020.acl-main.225,0,0.0926608,"ing and following context. ∗ Equal contribution Our code is available at https://github.com/ Varal7/blank_language_model 1 Existing approaches focus on adapting left-toright language models for text infilling. Intricate inference algorithms leveraging dynamic programming or gradient search are proposed to find the filling content that has a high likelihood within the surrounding context (Sun et al., 2017; Liu et al., 2019a; Zaidi et al., 2020). These methods make simplified Markov assumptions, require high decoding time complexity, and cannot adapt to variable infilling length. Alternatively, Donahue et al. (2020) predict the concatenation of the infilling content, but do not guarantee that the output will match the number of missing spans in the input. In this work, we introduce the Blank Language Model (BLM), which uses a special “ ” symbol to control where tokens can be placed. The generation of BLM follows the grammar of replacing a blank with a word and possibly adjoining blanks. By jointly modeling context and missing content, BLM supports the control of generation location and produces consistent infilling of variable length. Our model can start from a single blank or partial text with blanks in"
2020.emnlp-main.420,N16-1024,0,0.0290158,"e them in rewriting tasks, one needs to specify the insertion length in advance and heuristically determine the generation order among the masks (Fedus et al., 2018; Wang and Cho, 2019; Ghazvininejad et al., 2019). Similarly, XL-Net requires absolute positional embedding and thus does not support unknown-length text infilling (Yang et al., 2019; Shih et al., 2019). BLM provides a natural formulation for generative modeling that can dynamically accommodate insertions of various length. Another line of work focuses on finding an optimal language generation order, such as syntaxbased generation (Dyer et al., 2016) and learning adaptive generation order (Gu et al., 2019a). These approaches are tailored to generation from scratch in a specific order. Our model instead is attuned for text rewriting, where the missing parts can be located anywhere in the input text, and the algorithm must flexibly complete them. 3 Blank Language Models A blank language model (BLM) generates sequences by creating and filling in blanks. Generation starts with a single blank and ends when there is no blank. In each step, the model selects a blank “ ”, predicts a word w, and fills the blank with “w”, “ w”, “w ”, or “ w ”. This"
2020.emnlp-main.420,P16-5005,0,0.0308265,"Missing"
2020.emnlp-main.420,Q19-1042,0,0.0182906,"rious sequence models for non-autoregressive machine translation (Gu et al., 2017). The Insertion Transformer supports dynamic canvas with word insertion (Stern et al., 2019), but does not allow users to specify where to insert. The model is unaware of which parts of the canvas are contiguous text spans that should remain intact, and which (potentially scattered) parts need to be filled in. Directly forcing the Insertion Transformer to perform text infilling can therefore lead to suboptimal solutions. The Levenshtein Transformer combines insertion and deletion through complex policy learning (Gu et al., 2019b). Its insertion mechanism is a two-stage process in which placeholders are first predicted and then filled-in in a masked language model (MLM) manner. In text infilling where the blanks/placeholders are given, it reduces to an MLM. MLMs are commonly used in representation learning (Devlin et al., 2018; Joshi et al., 2020). To use them in rewriting tasks, one needs to specify the insertion length in advance and heuristically determine the generation order among the masks (Fedus et al., 2018; Wang and Cho, 2019; Ghazvininejad et al., 2019). Similarly, XL-Net requires absolute positional embedd"
2020.emnlp-main.420,2020.tacl-1.5,0,0.0277075,"in intact, and which (potentially scattered) parts need to be filled in. Directly forcing the Insertion Transformer to perform text infilling can therefore lead to suboptimal solutions. The Levenshtein Transformer combines insertion and deletion through complex policy learning (Gu et al., 2019b). Its insertion mechanism is a two-stage process in which placeholders are first predicted and then filled-in in a masked language model (MLM) manner. In text infilling where the blanks/placeholders are given, it reduces to an MLM. MLMs are commonly used in representation learning (Devlin et al., 2018; Joshi et al., 2020). To use them in rewriting tasks, one needs to specify the insertion length in advance and heuristically determine the generation order among the masks (Fedus et al., 2018; Wang and Cho, 2019; Ghazvininejad et al., 2019). Similarly, XL-Net requires absolute positional embedding and thus does not support unknown-length text infilling (Yang et al., 2019; Shih et al., 2019). BLM provides a natural formulation for generative modeling that can dynamically accommodate insertions of various length. Another line of work focuses on finding an optimal language generation order, such as syntaxbased gener"
2020.emnlp-main.420,N18-1169,0,0.0950063,"by 4.4 and 3.3 points, respectively. We posit that L-BLM’s advantage lies in its ability to maximize the joint likelihood of the completions over all slots. In contrast, PythiaWord’s is only aware of one slot at a time, and beam search is performed locally within each slot. 4.3 Sentiment Transfer The goal of sentiment transfer is to modify the sentiment of a sentence while maintaining its topic (Shen et al., 2017). An example is described on the third row of Fig. 4. Inspired by the way humans perform rewriting, we follow a recent line of work in style transfer that adopts a two-step approach (Li et al., 2018; Xu et al., 2018; Wu et al., 2019b): 1. Remove words and expressions of high polarity from the source sentence; 2. Complete the partial sentence with words and expressions of the target sentiment. Specifically, we adapt the Mask-And-Infill (M&I) framework of Wu et al. (2019b). We perform Step 1 by training a Bi-LSTM sentiment classifier and masking words whose attention weight is above average. We evaluate the contribution of our model as an infilling module in Step 2 in place of their fine-tuned BERT model. To this end, we train two instances of BLM on the dataset, one for each sentiment. At"
2020.emnlp-main.420,P19-1406,0,0.0361182,"ng task (Zhu et al., 2019), a model should: (1) provide fine-grained control over the generation location, (2) accommodate a variable number of missing tokens, and (3) respect both the preceding and following context. ∗ Equal contribution Our code is available at https://github.com/ Varal7/blank_language_model 1 Existing approaches focus on adapting left-toright language models for text infilling. Intricate inference algorithms leveraging dynamic programming or gradient search are proposed to find the filling content that has a high likelihood within the surrounding context (Sun et al., 2017; Liu et al., 2019a; Zaidi et al., 2020). These methods make simplified Markov assumptions, require high decoding time complexity, and cannot adapt to variable infilling length. Alternatively, Donahue et al. (2020) predict the concatenation of the infilling content, but do not guarantee that the output will match the number of missing spans in the input. In this work, we introduce the Blank Language Model (BLM), which uses a special “ ” symbol to control where tokens can be placed. The generation of BLM follows the grammar of replacing a blank with a word and possibly adjoining blanks. By jointly modeling conte"
2020.emnlp-main.420,2021.ccl-1.108,0,0.120015,"Missing"
2020.emnlp-main.420,N19-4009,0,0.0688093,"Missing"
2020.emnlp-main.420,D15-1044,0,0.15327,"Missing"
2020.emnlp-main.420,W19-2304,0,0.0734233,"venshtein Transformer combines insertion and deletion through complex policy learning (Gu et al., 2019b). Its insertion mechanism is a two-stage process in which placeholders are first predicted and then filled-in in a masked language model (MLM) manner. In text infilling where the blanks/placeholders are given, it reduces to an MLM. MLMs are commonly used in representation learning (Devlin et al., 2018; Joshi et al., 2020). To use them in rewriting tasks, one needs to specify the insertion length in advance and heuristically determine the generation order among the masks (Fedus et al., 2018; Wang and Cho, 2019; Ghazvininejad et al., 2019). Similarly, XL-Net requires absolute positional embedding and thus does not support unknown-length text infilling (Yang et al., 2019; Shih et al., 2019). BLM provides a natural formulation for generative modeling that can dynamically accommodate insertions of various length. Another line of work focuses on finding an optimal language generation order, such as syntaxbased generation (Dyer et al., 2016) and learning adaptive generation order (Gu et al., 2019a). These approaches are tailored to generation from scratch in a specific order. Our model instead is attuned"
2020.emnlp-main.420,P19-1482,0,0.0943326,"y. We posit that L-BLM’s advantage lies in its ability to maximize the joint likelihood of the completions over all slots. In contrast, PythiaWord’s is only aware of one slot at a time, and beam search is performed locally within each slot. 4.3 Sentiment Transfer The goal of sentiment transfer is to modify the sentiment of a sentence while maintaining its topic (Shen et al., 2017). An example is described on the third row of Fig. 4. Inspired by the way humans perform rewriting, we follow a recent line of work in style transfer that adopts a two-step approach (Li et al., 2018; Xu et al., 2018; Wu et al., 2019b): 1. Remove words and expressions of high polarity from the source sentence; 2. Complete the partial sentence with words and expressions of the target sentiment. Specifically, we adapt the Mask-And-Infill (M&I) framework of Wu et al. (2019b). We perform Step 1 by training a Bi-LSTM sentiment classifier and masking words whose attention weight is above average. We evaluate the contribution of our model as an infilling module in Step 2 in place of their fine-tuned BERT model. To this end, we train two instances of BLM on the dataset, one for each sentiment. At test time, the corresponding BLM"
2020.emnlp-main.420,P18-1090,0,0.0262485,"ints, respectively. We posit that L-BLM’s advantage lies in its ability to maximize the joint likelihood of the completions over all slots. In contrast, PythiaWord’s is only aware of one slot at a time, and beam search is performed locally within each slot. 4.3 Sentiment Transfer The goal of sentiment transfer is to modify the sentiment of a sentence while maintaining its topic (Shen et al., 2017). An example is described on the third row of Fig. 4. Inspired by the way humans perform rewriting, we follow a recent line of work in style transfer that adopts a two-step approach (Li et al., 2018; Xu et al., 2018; Wu et al., 2019b): 1. Remove words and expressions of high polarity from the source sentence; 2. Complete the partial sentence with words and expressions of the target sentiment. Specifically, we adapt the Mask-And-Infill (M&I) framework of Wu et al. (2019b). We perform Step 1 by training a Bi-LSTM sentiment classifier and masking words whose attention weight is above average. We evaluate the contribution of our model as an infilling module in Step 2 in place of their fine-tuned BERT model. To this end, we train two instances of BLM on the dataset, one for each sentiment. At test time, the c"
2020.emnlp-main.420,D18-1324,0,\N,Missing
2020.emnlp-main.420,Q19-1006,0,\N,Missing
2020.emnlp-main.420,D19-1633,0,\N,Missing
2020.emnlp-main.420,W19-3620,0,\N,Missing
2020.emnlp-main.420,N16-1035,0,\N,Missing
2020.emnlp-main.705,P19-1620,0,0.0436022,"Missing"
2020.emnlp-main.705,N03-1020,0,0.0602103,"user populations. 2. We demonstrate that our information-needdriven model can generate much higher quality captions on this task than those of state-of-theart traditional generic captioning systems. 3. We propose a novel synthetic pre-training routine that greatly improves the performance of reinforcement learning under this new paradigm. 8756 2 Related Work Since the early days of the field, human-written references have been used for the supervised training and evaluation of text generation systems, including image captioning, summarization, and other related applications (Edmundson, 1969; Lin and Hovy, 2003; Ordonez et al., 2011; Vinyals et al., 2015). Recently, researchers have begun to consider a multitude of different objectives for reference comparison (B¨ohm et al., 2019; Gao et al., 2019), or even parametric regressions trained on human judgements (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018). Though diverse in approach, each ultimately relies on designing a robust general-purpose metric. In practice, engineering such a metric is challenging—if at all possible (Sp¨arck Jones, 1994, 1999). Here we take a more empirical approach by relying on the information need expressed by users’"
2020.emnlp-main.705,W02-0109,0,0.332097,"Missing"
2020.emnlp-main.705,J13-2002,0,0.0133966,"ves the performance of reinforcement learning under this new paradigm. 8756 2 Related Work Since the early days of the field, human-written references have been used for the supervised training and evaluation of text generation systems, including image captioning, summarization, and other related applications (Edmundson, 1969; Lin and Hovy, 2003; Ordonez et al., 2011; Vinyals et al., 2015). Recently, researchers have begun to consider a multitude of different objectives for reference comparison (B¨ohm et al., 2019; Gao et al., 2019), or even parametric regressions trained on human judgements (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018). Though diverse in approach, each ultimately relies on designing a robust general-purpose metric. In practice, engineering such a metric is challenging—if at all possible (Sp¨arck Jones, 1994, 1999). Here we take a more empirical approach by relying on the information need expressed by users’ questions. Many studies have observed that reference-trained captioning models suffer from systematic usability issues—including being rigid, neglecting relevant image aspects, and regurgitating frequent phrases (Wang et al., 2017; Dai et al., 2017). As a result, much effort"
2020.emnlp-main.705,N18-1158,0,0.0540072,"Missing"
2020.emnlp-main.705,P19-1101,0,0.016233,"e generation model at test time. Information Need: We assume that the QA data from D is derived by the following process: 1. an image x is drawn from distribution p(x); 2. a question-answer pair (q, a) targeting an informative detail of x perceived as important to a user in D is drawn from distribution p(q, a|x). The operating assumption is that the marginal distribution over (q, a) pairs represents the visual interests of the typical user. In other words, answers to common questions represent the type of information that is often considered important. This is comparable to content selection (Peyrard, 2019). Question Anticipation: We do not assume the existence of a “gold” caption. Rather, the caption y is assumed to be a latent variable, and Gθ (y|x) is a 8757 Caption: A bus is boarding passengers at a stop. Captioning Model REINFORCE Question: Why is the bus stopped? Question Answering Model Answer: boarding passengers Figure 2: Overview of our proposed approach to the C AP WAP task. The captioning model Gθ (y|x) is learned using supervision from question-answer-image triples. Generated text that can be used to answer the question correctly, according to an extractive question answering model,"
2020.emnlp-main.705,N18-2103,0,0.0157446,"inforcement learning under this new paradigm. 8756 2 Related Work Since the early days of the field, human-written references have been used for the supervised training and evaluation of text generation systems, including image captioning, summarization, and other related applications (Edmundson, 1969; Lin and Hovy, 2003; Ordonez et al., 2011; Vinyals et al., 2015). Recently, researchers have begun to consider a multitude of different objectives for reference comparison (B¨ohm et al., 2019; Gao et al., 2019), or even parametric regressions trained on human judgements (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018). Though diverse in approach, each ultimately relies on designing a robust general-purpose metric. In practice, engineering such a metric is challenging—if at all possible (Sp¨arck Jones, 1994, 1999). Here we take a more empirical approach by relying on the information need expressed by users’ questions. Many studies have observed that reference-trained captioning models suffer from systematic usability issues—including being rigid, neglecting relevant image aspects, and regurgitating frequent phrases (Wang et al., 2017; Dai et al., 2017). As a result, much effort has been focused on developin"
2020.emnlp-main.705,P18-2124,0,0.0295367,"eter settings and optimization choices. 8758 Initialization: We initialize Gθ (y|x) using maximum likelihood estimation (MLE) on a corpus of ˜ ), as common out-of-domain generic captions (˜ x, y practice (Ranzato et al., 2016). This warm-starts our policy with an initial set of grounded image concepts, albeit not necessarily the ones we ultimately care about. Given the generic reference ˜ = (˜ y y1 , . . . , y˜n ), we minimize the cross-entropy: LXE (θ) = − n X ˜ , y˜j<i ) log Gθ (˜ yi |x (2) i=1 QA Model: We implement the QA model M using a BERTLARGE extractive model fine-tuned on SQuAD 2.0 (Rajpurkar et al., 2018)—which contains unanswerable questions. As an extractive model, M predicts a span yi...j . Important for our use-case, M is both able to be accurate when predicting the answer a when a is present in y, and also able to abstain from answering when a is not logically entailed (i.e., predict “no answer”). QA Reward: We take R(y, q, a) from Eq. 1 as the F1 score of the predicted answer with the gold answer. We control for reward noise with a confidence threshold for predicting “no answer.” Policy Gradient: We use REINFORCE with a baseline (Williams, 1992) to compute the policy gradient ∇θ LQA (θ)"
2020.emnlp-main.705,D16-1264,0,0.126522,"Missing"
2020.emnlp-main.705,D19-1320,0,0.0177595,"g relevant image aspects, and regurgitating frequent phrases (Wang et al., 2017; Dai et al., 2017). As a result, much effort has been focused on developing secondary, corrective objectives—for instance, “discriminability” losses encouraging captions to be unique (Dai and Lin, 2017; Liu et al., 2018; Luo et al., 2018). While these measures provide some fixes, they do not necessarily reflect user information needs—a central concept in C AP WAP. The idea of using QA for assessing information quality has been proposed in recent work for text summarization (Arumae and Liu, 2019; Eyal et al., 2019; Scialom et al., 2019). The primary distinctions with our work are both the domain (images) and how questions are obtained—both of which impact the task objective and learning procedure. In this prior work, questions are generated programmatically (e.g., following Hermann et al., 2015). Such “questions” may not necessarily reflect real user preferences. Our work focuses on QA not as just another method to improve standard referencebased metrics, but as a key, flexible way of formulating user information need—and as such we focus on challenging, real QA datasets. Furthermore, we train on this signal, rather than rel"
2020.emnlp-main.705,H94-1018,0,0.131943,"ms, including image captioning, summarization, and other related applications (Edmundson, 1969; Lin and Hovy, 2003; Ordonez et al., 2011; Vinyals et al., 2015). Recently, researchers have begun to consider a multitude of different objectives for reference comparison (B¨ohm et al., 2019; Gao et al., 2019), or even parametric regressions trained on human judgements (Louis and Nenkova, 2013; Peyrard and Gurevych, 2018). Though diverse in approach, each ultimately relies on designing a robust general-purpose metric. In practice, engineering such a metric is challenging—if at all possible (Sp¨arck Jones, 1994, 1999). Here we take a more empirical approach by relying on the information need expressed by users’ questions. Many studies have observed that reference-trained captioning models suffer from systematic usability issues—including being rigid, neglecting relevant image aspects, and regurgitating frequent phrases (Wang et al., 2017; Dai et al., 2017). As a result, much effort has been focused on developing secondary, corrective objectives—for instance, “discriminability” losses encouraging captions to be unique (Dai and Lin, 2017; Liu et al., 2018; Luo et al., 2018). While these measures provi"
2020.emnlp-main.705,D19-1514,0,0.0210884,"earning information need while maintaining fluency. Table 7 shows how synthetic pre-training regularizes the model to stay closer to human-level production patterns. Similarly, Table 8 shows how using the QA model to provide rewards (as opposed to a simple keyword search) helps the model avoid spurious rewards. Future Work: The C AP WAP paradigm introduces new challenges for learning effective systems, some of which our approach solves, and others which it still leaves open (e.g., maintaining fluency and fidelty). While some may be addressed by large-scale multi-modal models (Li et al., 2019; Tan and Bansal, 2019), it is still unclear whether they would fully cover the diversity of information that real users are interested in (e.g., OCR). 7 Conclusion We defined and studied the C AP WAP task, where question-answer pairs provided by users are used as a source of supervision for learning their visual information needs. Our results indicate that measuring caption content by its ability to logically support the answers to typical QA pairs from a target audience is (1) not only feasible, but also (2) a good proxy for uncovering information need. We hope this work will motivate the image captioning field to"
2020.emnlp-main.705,2020.acl-main.450,0,0.020024,"our work are both the domain (images) and how questions are obtained—both of which impact the task objective and learning procedure. In this prior work, questions are generated programmatically (e.g., following Hermann et al., 2015). Such “questions” may not necessarily reflect real user preferences. Our work focuses on QA not as just another method to improve standard referencebased metrics, but as a key, flexible way of formulating user information need—and as such we focus on challenging, real QA datasets. Furthermore, we train on this signal, rather than rely on it solely for evaluation (Wang et al., 2020). Efforts to leverage VQA resources to drive image captioning, and vice-versa, via variations of transfer learning, have also received extensive interest in recent years (Li et al., 2018; Wu et al., 2019; Yang and Xu, 2019). As opposed to optimizing metrics for specific VQA or supervised captioning benchmarks, the primary focus in C AP WAP is on modeling the target user population in order to anticipate the correct information-need. In a similar vein, VQA and textual QA resources have also been leveraged for active learning (Shen et al., 2019; Li et al., 2017), where the model learns to query"
2020.emnlp-main.705,P19-1348,0,0.0210925,"owing Hermann et al., 2015). Such “questions” may not necessarily reflect real user preferences. Our work focuses on QA not as just another method to improve standard referencebased metrics, but as a key, flexible way of formulating user information need—and as such we focus on challenging, real QA datasets. Furthermore, we train on this signal, rather than rely on it solely for evaluation (Wang et al., 2020). Efforts to leverage VQA resources to drive image captioning, and vice-versa, via variations of transfer learning, have also received extensive interest in recent years (Li et al., 2018; Wu et al., 2019; Yang and Xu, 2019). As opposed to optimizing metrics for specific VQA or supervised captioning benchmarks, the primary focus in C AP WAP is on modeling the target user population in order to anticipate the correct information-need. In a similar vein, VQA and textual QA resources have also been leveraged for active learning (Shen et al., 2019; Li et al., 2017), where the model learns to query its environment for information it is uncertain about to help improve its performance on the given task. The key distinction with our work is the directionality of the questions. In C AP WAP, the model u"
2021.emnlp-main.406,N19-1423,0,0.0142367,"des when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks.1 1 Figure 1: Our CAT model G can save computational resources by exiting early on certain inputs—while guaranteeing predictive consistency with the full model F. Introduction Large pre-trained language models have become the de facto standard approach for solving natural language processing tasks (Devlin et al., 2019; Liu et al., 2019). Despite their impressive performance, however, their often massive computational burden makes them costly to run (Schwartz et al., 2019; Sharir et al., 2020). Concerns about their efficiency have kindled a large body of research in the field (Sanh et al., 2020; Schwartz et al., 2020; Fan et al., 2020). For multilayered architectures such as the Transformer, a popular approach is adaptive early exiting (Schwartz et al., 2020; Xin et al., 2020a, inter alia). Early exiting takes advantage of the observation that task instances vary in complexity. In this setting, “early” clas"
2021.emnlp-main.406,P17-2059,0,0.0187133,"e test set performance of our original F model (Albert-xlarge), are contained in Table 2. 5.2 Baselines In addition to our main methods discussed in §4.2, we compare to several non-CP baselines. Note that the following methods are not guaranteed to give well-calibrated performance (as our CP ones are). Static. We use the same number of layers for all inputs. We choose the exit layer as the first one that obtains the desired consistency on average on Dcal . 1 − , where pmax denotes the maximum softmax k response of our early classifier. Softmax values are calibrated using temperature scaling (Guo, 2017) on another held-out (labeled) data split, Dscale . Meta threshold. Even if perfectly calibrated, pmax from softmax thresholding is not measuring k consistency likelihood P(G(X) = F(X) |X = x), but rather P(G(X) = Y |X = x). This is equivalent if F is an oracle, but breaks down when F is not. We also experiment with thresholding the confidence value of our meta classifier (§3.2) in a similar way (i.e., exiting when it exceeds 1 − ). 5.3 Evaluation For each task, we use a proper training, validation, and test set. We use the training set to learn F and G. We perform model selection on the vali"
2021.emnlp-main.406,2020.acl-main.593,0,0.0500948,"Missing"
2021.emnlp-main.406,2020.emnlp-demos.6,0,0.0999367,"Missing"
2021.emnlp-main.406,2020.sustainlp-1.11,0,0.050474,"Missing"
2021.emnlp-main.406,2021.naacl-main.52,1,0.854941,"G, we need a reliable signal for inferring whether or not the current prediction is likely to be stable. Past work (e.g., Schwartz et al., 2020) rely on potentially poorly correlated metrics such as the early classifier’s softmax response. We address this challenge by instead directly learning meta “consistency predictors” for each of the l − 1 early classifiers of our l layer model, by leveraging patterns in past predictions.4 Figure 2 demonstrates the progression of meta confidence scores across layers when applied to “easy” versus “hard” instances from the VitaminC fact verification task (Schuster et al., 2021). We pair the scores of our meta classifier for each layer with a stopping rule that is calibrated using a unique twist on standard conformal prediction. Traditionally, CP is used to construct prediction sets that cover the desired target (e.g., Yn+1 ) with high probability. We invert the CP problem to first infer the multi-label set of inconsistent layers, and then exit at the first layer that falls in its complement. We then demonstrate that this can be reduced to setting a simple (but well-calibrated) exit threshold for the meta classifier scores. Our resulting algorithm is (1) fast to comp"
2021.emnlp-main.406,2020.acl-main.204,0,0.0358074,"Missing"
2021.naacl-main.411,W98-1409,1,0.319268,"Mueller et al., 2017; Fan et al., 2017; Guu et al., 2018) on text generation also control aspects of the produced text, such as style and length. While these typically utilize tokens to control the modification, using prototypes to generate text is also very common (Guu et al., 2017; Li, 2018; Shah et al., 2019). In this work, we utilize aggregation specific prototypes to guide aggregation cognizant surface realization. Data-to-text Summrization Traditional approaches for data-to-text generation have operated on symbolic data from databases. McKeown and Radev (1995); Radev and McKeown (1998); Barzilay et al. (1998) introduce two components of content selection and surface realization. Content selection identifies and aggregates key symbolic data from the database which can then be realized into text using templates. Unlike modern data-totext systems (Wiseman et al., 2018; Puduppully et al., 2019; Sharma et al., 2019; Wenbo et al., 2019) these approaches capture document consensus and aggregation cognisance. While the neural 3 approaches alleviate the need for human intervenWe compare the selected content with other entries in the database, identifying two contradictions. tion, they do need an abundance"
2021.naacl-main.411,P16-1046,0,0.0847181,"Missing"
2021.naacl-main.411,2020.acl-main.225,0,0.0208383,"marization (Zhang et al., 2018; Lebanoff et al., 2018; Baumel et al., 2018; Amplayo and Lapata, 2019; Fabbri et al., 2019). Despite producing fluent text, these techniques may generate false information which is not faithful to the original inputs (Puduppully et al., 2019; Kry´sci´nski et al., 2019), especially in low resource scenarios. In this work, we are interested in producing faithful and fluent text cognizant of aggregation amongst input documents, where few parallel examples are available. Recent language modeling approaches (Devlin et al., 2018; Stern et al., 2019; Shen et al., 2020; Donahue et al., 2020) can also be extended for text completion. Our work is a text-infilling language model where we generate words in place of relation specific blanks to produce a faithful summary. Prior work (Mueller et al., 2017; Fan et al., 2017; Guu et al., 2018) on text generation also control aspects of the produced text, such as style and length. While these typically utilize tokens to control the modification, using prototypes to generate text is also very common (Guu et al., 2017; Li, 2018; Shah et al., 2019). In this work, we utilize aggregation specific prototypes to guide aggregation cognizant surfac"
2021.naacl-main.411,P19-1102,0,0.0226521,"Missing"
2021.naacl-main.411,Q18-1031,0,0.0303101,"uppully et al., 2019; Kry´sci´nski et al., 2019), especially in low resource scenarios. In this work, we are interested in producing faithful and fluent text cognizant of aggregation amongst input documents, where few parallel examples are available. Recent language modeling approaches (Devlin et al., 2018; Stern et al., 2019; Shen et al., 2020; Donahue et al., 2020) can also be extended for text completion. Our work is a text-infilling language model where we generate words in place of relation specific blanks to produce a faithful summary. Prior work (Mueller et al., 2017; Fan et al., 2017; Guu et al., 2018) on text generation also control aspects of the produced text, such as style and length. While these typically utilize tokens to control the modification, using prototypes to generate text is also very common (Guu et al., 2017; Li, 2018; Shah et al., 2019). In this work, we utilize aggregation specific prototypes to guide aggregation cognizant surface realization. Data-to-text Summrization Traditional approaches for data-to-text generation have operated on symbolic data from databases. McKeown and Radev (1995); Radev and McKeown (1998); Barzilay et al. (1998) introduce two components of conten"
2021.naacl-main.411,P17-1097,0,0.0181003,"examples are available. Recent language modeling approaches (Devlin et al., 2018; Stern et al., 2019; Shen et al., 2020; Donahue et al., 2020) can also be extended for text completion. Our work is a text-infilling language model where we generate words in place of relation specific blanks to produce a faithful summary. Prior work (Mueller et al., 2017; Fan et al., 2017; Guu et al., 2018) on text generation also control aspects of the produced text, such as style and length. While these typically utilize tokens to control the modification, using prototypes to generate text is also very common (Guu et al., 2017; Li, 2018; Shah et al., 2019). In this work, we utilize aggregation specific prototypes to guide aggregation cognizant surface realization. Data-to-text Summrization Traditional approaches for data-to-text generation have operated on symbolic data from databases. McKeown and Radev (1995); Radev and McKeown (1998); Barzilay et al. (1998) introduce two components of content selection and surface realization. Content selection identifies and aggregates key symbolic data from the database which can then be realized into text using templates. Unlike modern data-totext systems (Wiseman et al., 2018"
2021.naacl-main.411,N19-1238,0,0.0575838,"Missing"
2021.naacl-main.411,D18-1446,0,0.0513535,"Missing"
2021.naacl-main.411,2020.acl-main.703,0,0.0578985,"Missing"
2021.naacl-main.411,C18-1241,0,0.0476647,"Missing"
2021.naacl-main.411,P19-1195,0,0.0623783,"Missing"
2021.naacl-main.411,J98-3005,0,0.836002,"ask since the amount of comparative summaries is not sufficient 1 Introduction for their training. In this paper, we propose a novel approach to Articles written about the same topic rarely exhibit multi-document summarization based on a neural full agreement. To present an unbiased overview interpretation of traditional concept-to-text generof such material, a summary has to identify points of consensus and highlight contradictions. For in- ation systems. Specifically, our work is inspired by the symbolic multi-document summarization stance, in the healthcare domain, where studies system of (Radev and McKeown, 1998) which prooften exhibit wide divergence of findings, such duces summaries that explicitly highlight agreecomparative summaries are generated by human 2 experts for the benefit of the general public. Ide- ments, contradictions and other relations across input documents. While their system was based ally, this capacity will be automated given a large on human-crafted templates and thus limited to a number of relevant articles and continuous influx narrow domain, our approach learns different comof new ones that require a summary update to keep ponents of the generation pipeline from data. 1 Our"
2021.naacl-main.411,D15-1044,0,0.139058,"Missing"
2021.naacl-main.411,P17-1099,0,0.0472955,"n, we have human annotators score our models on relevance and fluency. Given a reference summary, relevance indicates if the generated text shares similar information. Fluency represents if the generated text is grammatically correct and written in well-formed English. Annotators rate relevance and fluency on a 1-4 likert scale (Albaum, 1997). We have 3 annotators score every data point and report the average across the scores. Baselines In order to demonstrate the effectiveness of our method, we compare it against text2text and data2text state-of-the-art (sota) methods. Copy-gen (Text2text): See et al. (2017) is a sota technique for summarization, which can copy from the input or generate words. Transformer (Text2text): Hoang et al. (2019) is a summarization system using a pretrained Transformer. GraphWriter (Data2text): Koncel-Kedziorski et al. (2019) is a graph transformer based model, which generates text using a seed title and a knowledge graph. Takes the database XG as input. Entity (Data2text): Puduppully et al. (2019) is an entity based data2text model, takes XG as input. Implementation Details Our policy network is a three layer feedforward neural network. We use a Transformer (Vaswani et"
2021.naacl-main.411,W18-6545,0,0.0477936,"Missing"
2021.naacl-main.411,D19-1323,0,0.018996,"19). In this work, we utilize aggregation specific prototypes to guide aggregation cognizant surface realization. Data-to-text Summrization Traditional approaches for data-to-text generation have operated on symbolic data from databases. McKeown and Radev (1995); Radev and McKeown (1998); Barzilay et al. (1998) introduce two components of content selection and surface realization. Content selection identifies and aggregates key symbolic data from the database which can then be realized into text using templates. Unlike modern data-totext systems (Wiseman et al., 2018; Puduppully et al., 2019; Sharma et al., 2019; Wenbo et al., 2019) these approaches capture document consensus and aggregation cognisance. While the neural 3 approaches alleviate the need for human intervenWe compare the selected content with other entries in the database, identifying two contradictions. tion, they do need an abundance of parallel data, 5214 Figure 2: Illustrating the flow of our Nutribullets Hybrid system. In this example, our model takes in four Pubmed studies to produce a database (a). The Content Selection model selects two tuples (bold) and identifies the aggregation operator as Contradiction (b). Finally, the Surfa"
2021.naacl-main.411,2020.emnlp-main.420,1,0.840147,"multi-document summarization (Zhang et al., 2018; Lebanoff et al., 2018; Baumel et al., 2018; Amplayo and Lapata, 2019; Fabbri et al., 2019). Despite producing fluent text, these techniques may generate false information which is not faithful to the original inputs (Puduppully et al., 2019; Kry´sci´nski et al., 2019), especially in low resource scenarios. In this work, we are interested in producing faithful and fluent text cognizant of aggregation amongst input documents, where few parallel examples are available. Recent language modeling approaches (Devlin et al., 2018; Stern et al., 2019; Shen et al., 2020; Donahue et al., 2020) can also be extended for text completion. Our work is a text-infilling language model where we generate words in place of relation specific blanks to produce a faithful summary. Prior work (Mueller et al., 2017; Fan et al., 2017; Guu et al., 2018) on text generation also control aspects of the produced text, such as style and length. While these typically utilize tokens to control the modification, using prototypes to generate text is also very common (Guu et al., 2017; Li, 2018; Shah et al., 2019). In this work, we utilize aggregation specific prototypes to guide aggre"
2021.naacl-main.411,D19-1304,0,0.020125,"utilize aggregation specific prototypes to guide aggregation cognizant surface realization. Data-to-text Summrization Traditional approaches for data-to-text generation have operated on symbolic data from databases. McKeown and Radev (1995); Radev and McKeown (1998); Barzilay et al. (1998) introduce two components of content selection and surface realization. Content selection identifies and aggregates key symbolic data from the database which can then be realized into text using templates. Unlike modern data-totext systems (Wiseman et al., 2018; Puduppully et al., 2019; Sharma et al., 2019; Wenbo et al., 2019) these approaches capture document consensus and aggregation cognisance. While the neural 3 approaches alleviate the need for human intervenWe compare the selected content with other entries in the database, identifying two contradictions. tion, they do need an abundance of parallel data, 5214 Figure 2: Illustrating the flow of our Nutribullets Hybrid system. In this example, our model takes in four Pubmed studies to produce a database (a). The Content Selection model selects two tuples (bold) and identifies the aggregation operator as Contradiction (b). Finally, the Surface Realization model"
2021.naacl-main.411,D18-1356,0,0.0290729,"on (Guu et al., 2017; Li, 2018; Shah et al., 2019). In this work, we utilize aggregation specific prototypes to guide aggregation cognizant surface realization. Data-to-text Summrization Traditional approaches for data-to-text generation have operated on symbolic data from databases. McKeown and Radev (1995); Radev and McKeown (1998); Barzilay et al. (1998) introduce two components of content selection and surface realization. Content selection identifies and aggregates key symbolic data from the database which can then be realized into text using templates. Unlike modern data-totext systems (Wiseman et al., 2018; Puduppully et al., 2019; Sharma et al., 2019; Wenbo et al., 2019) these approaches capture document consensus and aggregation cognisance. While the neural 3 approaches alleviate the need for human intervenWe compare the selected content with other entries in the database, identifying two contradictions. tion, they do need an abundance of parallel data, 5214 Figure 2: Illustrating the flow of our Nutribullets Hybrid system. In this example, our model takes in four Pubmed studies to produce a database (a). The Content Selection model selects two tuples (bold) and identifies the aggregation ope"
2021.naacl-main.411,P18-4013,0,0.0216017,"rmer (Text2text): Hoang et al. (2019) is a summarization system using a pretrained Transformer. GraphWriter (Data2text): Koncel-Kedziorski et al. (2019) is a graph transformer based model, which generates text using a seed title and a knowledge graph. Takes the database XG as input. Entity (Data2text): Puduppully et al. (2019) is an entity based data2text model, takes XG as input. Implementation Details Our policy network is a three layer feedforward neural network. We use a Transformer (Vaswani et al., 2017) implementation for Surface Realization. We train an off-the-shelf Neural CRF tagger (Yang and Zhang, 2018) for entity extraction. We use BERT (Devlin et al., 2018) based classifiers to predict the relation between two entities in a text trained using crowdsourced annotations from (Shah et al., 2021). Futher implementation details can be found in A. 6 Results In this section, we describe the performance of our Nutribullet Hybrid system and baselines on summarization and summary updates. We report empirical results , human evaluation and present sample outputs, highlighting the benefits of our method. Single and Multi-issues Summarization: We describe the results on the task of generating summaries."
2021.naacl-main.52,2020.acl-main.656,0,0.038544,"EVER 2.0 text st−1 and an updated claim c, we learn shared task. Teams were asked to create claims that 629 Model Train data AUC Prec. Rec. F1 Edit dist. ALBERT PAWS-full 71.34 72.20 64.90 65.27 63.18 60.61 63.56 60.48 BOW ALBERT ALBERT VitC-diff VitC-diff VitC-full 79.87 89.87 91.97 70.85 80.69 82.63 67.84 82.06 84.49 68.55 81.18 83.18 Table 3: Factual revision flagging scores for models aware of the full sentence-pair (full) and aware only of the modified words (diff). We use ALBERT-base. break FEVER-trained models. We take all SUP and REF claims and their gold evidence sentences. Triggers (Atanasova et al., 2020): A set of 186 FEVER claims paraphrased adversarially to contain universal adversarial triggers (Wallace et al., 2019). Its small size leads to high variance results. ANLI (Nie et al., 2020): An adversarial dataset for MNLI- and FEVER-based models. The creation was performed in three iterative rounds in which a model was trained, and then crowdworkers devised adversarial inputs, and the process repeated. PAWS (Zhang et al., 2019): A dataset of altered Wikipedia sentences using word swapping and back-translation. Human annotators labeled whether the modified sentence is a paraphrase or not. We"
2021.naacl-main.52,D15-1075,0,0.119854,"Missing"
2021.naacl-main.52,N19-1423,0,0.0619188,"etrieved up to 500 of its most recent revisions. In May 2020, we added all COVID19 related articles5 and all of their 41K revisions at the time. Combined together, this resulted in a total of ∼200 million revisions. For each revision, we identified all of the modified sentences and stored two versions: (1) before, and (2) after the edit. In our task, we are only interested in edits made with an intent to introduce a factual modification— i.e., a change for which one can make a claim that is supported by one sentence, but not by the other.6 To expedite annotation, we trained a BERT classifier (Devlin et al., 2019) on a small labeled set of revised sentences determined to be factual (Yang et al., 2017), and used this model to select the top 305K edited sentences from the corpus for manual annotation. Trained human annotators were then presented with the sentence pairs, and were asked to mark the ones that indeed represented a factual change. Sentences lacking self-contained context were filtered (e.g., short expressions from tables or bulleted lists). Example annotations are presented in Table 1. Note that these annotations can also be 3 https://bit.ly/Wiki_Neutral_POV https://bit.ly/Wiki_popular_pages"
2021.naacl-main.52,2020.emnlp-main.580,0,0.0928844,"s provide sentence-level rationales (DeYoung et al., 2020; Petroni et al., 2020) but do not enforce the model’s verdict to rely on them—leading to a potential discrepancy. V ITAMIN C ensures the verdict is conditioned on the retrieved evidence. Moreover, we use the revision history as distant supervision for word-level rationales, allowing for finer-grained explanations (Camburu et al., 2018; Lei et al., 2016; Portelli et al., 2020; Thorne et al., 2019b). Factually Consistent Generation. Generating texts that match given facts is a known chalFact Verification. The FEVER dataset (Thorne lenge (Fan et al., 2020; Kryscinski et al., 2020; et al., 2018) fueled the development of many fact- Lewis et al., 2020b; Parikh et al., 2020; Shah et al., checking models (e.g., see Hanselowski et al., 2020; Tian et al., 2020) as language models tend to 2018; Nie et al., 2019a,b; Yoneda et al., 2018, in- degenerate and hallucinate (Holtzman et al., 2020; ter alia). The claim creation process, however, Schuster et al., 2020; Zhou et al., 2020). Morerequired crowd-workers to write claims related over, evaluation is non-trivial, and usually manual. to Wikipedia articles, and was found to engender V ITAMIN C includes s"
2021.naacl-main.52,D18-1028,0,0.0515449,"efuting pairs. For example, rather than stating, “there are x confirmed cases of coronavirus in the US”, one can write “there are more than z confirmed cases of coronavirus in the US”, which is supported if x > z and refuted otherwise. For revisions that only add new information or that remove outdated facts without replacing them, annotators wrote a single claim. 3.3 Adding Synthetic Revisions Naturally, the real Wikipedia revisions we collect mostly describe facts that frequently change over time, or that are prone to mistakes and corrections (such as quantitative values, see Appendix A.1) (Faruqui et al., 2018; Yang et al., 2017). Sensitivity to contrastive contexts, however, is desirable behavior for any claim. This can both ensure consistency with external sources of truth, and improve the model’s faithfulness via connecting the verdict with a specific evidence (Jacovi and Goldberg, 2020; Ross et al., 2020). For example, we require the model to not only classify the claim “Tom Hanks was honored by a president” as true, but to also change its verdict to false if paired with a (fictional) contrasting evidence. As a result, we can verify that the model prioritizes sentence-pair inference over 7 626"
2021.naacl-main.52,N18-2017,0,0.0542551,"Missing"
2021.naacl-main.52,2020.acl-main.386,0,0.0432129,"or that remove outdated facts without replacing them, annotators wrote a single claim. 3.3 Adding Synthetic Revisions Naturally, the real Wikipedia revisions we collect mostly describe facts that frequently change over time, or that are prone to mistakes and corrections (such as quantitative values, see Appendix A.1) (Faruqui et al., 2018; Yang et al., 2017). Sensitivity to contrastive contexts, however, is desirable behavior for any claim. This can both ensure consistency with external sources of truth, and improve the model’s faithfulness via connecting the verdict with a specific evidence (Jacovi and Goldberg, 2020; Ross et al., 2020). For example, we require the model to not only classify the claim “Tom Hanks was honored by a president” as true, but to also change its verdict to false if paired with a (fictional) contrasting evidence. As a result, we can verify that the model prioritizes sentence-pair inference over 7 626 We sourced our annotators through TransPerfect. Factual 7 3 7 3 Wikipedia sentences before and after a revision, presented with V ITAMIN C claims if the revision is factual. Before More stringent actions were taken in China once the seriousness of the outbreak became apparent, such as"
2021.naacl-main.52,D16-1011,1,0.868011,"Missing"
2021.naacl-main.52,2020.acl-main.703,0,0.049402,"force the model’s verdict to rely on them—leading to a potential discrepancy. V ITAMIN C ensures the verdict is conditioned on the retrieved evidence. Moreover, we use the revision history as distant supervision for word-level rationales, allowing for finer-grained explanations (Camburu et al., 2018; Lei et al., 2016; Portelli et al., 2020; Thorne et al., 2019b). Factually Consistent Generation. Generating texts that match given facts is a known chalFact Verification. The FEVER dataset (Thorne lenge (Fan et al., 2020; Kryscinski et al., 2020; et al., 2018) fueled the development of many fact- Lewis et al., 2020b; Parikh et al., 2020; Shah et al., checking models (e.g., see Hanselowski et al., 2020; Tian et al., 2020) as language models tend to 2018; Nie et al., 2019a,b; Yoneda et al., 2018, in- degenerate and hallucinate (Holtzman et al., 2020; ter alia). The claim creation process, however, Schuster et al., 2020; Zhou et al., 2020). Morerequired crowd-workers to write claims related over, evaluation is non-trivial, and usually manual. to Wikipedia articles, and was found to engender V ITAMIN C includes supervised data for training biases that allow an evidence-agnostic model to sequence-to-sequence"
2021.naacl-main.52,P19-1334,0,0.0447748,"s (Jiang et al., 2020), and search snippets (Augenstein et al., 2019). These resources all assume static ground truths. In contrast, V ITAMIN C compares objective claims to a dynamic source of truth, and requires models to change their verdicts accordingly. Annotation Bias. Annotation artifacts are common in many NLP datasets, and affect performance on adversarial and contrastive examples (Gardner et al., 2020; Ribeiro et al., 2020; Ross et al., 2020). Sentence-pair inference tasks such as fact verification (Paul Panenghat et al., 2020; Schuster et al., 2019) and NLI (Gururangan et al., 2018; McCoy et al., 2019; Poliak et al., 2018; Tsuchiya, 2018) are no exception. Alleviating this bias requires either modeling solutions (Karimi Mahabadi et al., 2020; Pratapa et al., 2020; Shah et al., 2020; Thorne and Vlachos, 2020; Utama et al., 2020b), which have limited effectiveness (Utama et al., 2020a), or adversarially removing troublesome training examples (Bras et al., 2020) or manually collecting new ones (Nie et al., 2020; Thorne et al., 2019a), which is model specific. Instead, our dataset design avoids single-sentence artifacts and provides model-agnostic challenging examples that increase the robustn"
2021.naacl-main.52,P19-1256,0,0.0395043,"we use the revision history as distant supervision for word-level rationales, allowing for finer-grained explanations (Camburu et al., 2018; Lei et al., 2016; Portelli et al., 2020; Thorne et al., 2019b). Factually Consistent Generation. Generating texts that match given facts is a known chalFact Verification. The FEVER dataset (Thorne lenge (Fan et al., 2020; Kryscinski et al., 2020; et al., 2018) fueled the development of many fact- Lewis et al., 2020b; Parikh et al., 2020; Shah et al., checking models (e.g., see Hanselowski et al., 2020; Tian et al., 2020) as language models tend to 2018; Nie et al., 2019a,b; Yoneda et al., 2018, in- degenerate and hallucinate (Holtzman et al., 2020; ter alia). The claim creation process, however, Schuster et al., 2020; Zhou et al., 2020). Morerequired crowd-workers to write claims related over, evaluation is non-trivial, and usually manual. to Wikipedia articles, and was found to engender V ITAMIN C includes supervised data for training biases that allow an evidence-agnostic model to sequence-to-sequence models, and provides auto625 2 Related Work matic evaluation via the fact verification classifier. 3 recursively recycled for re-training the automated BERT"
2021.naacl-main.52,2020.acl-main.441,0,0.0639633,"beiro et al., 2020; Ross et al., 2020). Sentence-pair inference tasks such as fact verification (Paul Panenghat et al., 2020; Schuster et al., 2019) and NLI (Gururangan et al., 2018; McCoy et al., 2019; Poliak et al., 2018; Tsuchiya, 2018) are no exception. Alleviating this bias requires either modeling solutions (Karimi Mahabadi et al., 2020; Pratapa et al., 2020; Shah et al., 2020; Thorne and Vlachos, 2020; Utama et al., 2020b), which have limited effectiveness (Utama et al., 2020a), or adversarially removing troublesome training examples (Bras et al., 2020) or manually collecting new ones (Nie et al., 2020; Thorne et al., 2019a), which is model specific. Instead, our dataset design avoids single-sentence artifacts and provides model-agnostic challenging examples that increase the robustness of trained models. Explainability. Current fact verification datasets provide sentence-level rationales (DeYoung et al., 2020; Petroni et al., 2020) but do not enforce the model’s verdict to rely on them—leading to a potential discrepancy. V ITAMIN C ensures the verdict is conditioned on the retrieved evidence. Moreover, we use the revision history as distant supervision for word-level rationales, allowing f"
2021.naacl-main.52,P02-1040,0,0.112906,"Missing"
2021.naacl-main.52,2020.emnlp-main.89,0,0.0381649,"dict to rely on them—leading to a potential discrepancy. V ITAMIN C ensures the verdict is conditioned on the retrieved evidence. Moreover, we use the revision history as distant supervision for word-level rationales, allowing for finer-grained explanations (Camburu et al., 2018; Lei et al., 2016; Portelli et al., 2020; Thorne et al., 2019b). Factually Consistent Generation. Generating texts that match given facts is a known chalFact Verification. The FEVER dataset (Thorne lenge (Fan et al., 2020; Kryscinski et al., 2020; et al., 2018) fueled the development of many fact- Lewis et al., 2020b; Parikh et al., 2020; Shah et al., checking models (e.g., see Hanselowski et al., 2020; Tian et al., 2020) as language models tend to 2018; Nie et al., 2019a,b; Yoneda et al., 2018, in- degenerate and hallucinate (Holtzman et al., 2020; ter alia). The claim creation process, however, Schuster et al., 2020; Zhou et al., 2020). Morerequired crowd-workers to write claims related over, evaluation is non-trivial, and usually manual. to Wikipedia articles, and was found to engender V ITAMIN C includes supervised data for training biases that allow an evidence-agnostic model to sequence-to-sequence models, and provides"
2021.naacl-main.52,2020.lrec-1.850,0,0.0287366,"es (Jo et al., 2019), Wikipedia references (Sathe et al., 2020), multiple articles (Jiang et al., 2020), and search snippets (Augenstein et al., 2019). These resources all assume static ground truths. In contrast, V ITAMIN C compares objective claims to a dynamic source of truth, and requires models to change their verdicts accordingly. Annotation Bias. Annotation artifacts are common in many NLP datasets, and affect performance on adversarial and contrastive examples (Gardner et al., 2020; Ribeiro et al., 2020; Ross et al., 2020). Sentence-pair inference tasks such as fact verification (Paul Panenghat et al., 2020; Schuster et al., 2019) and NLI (Gururangan et al., 2018; McCoy et al., 2019; Poliak et al., 2018; Tsuchiya, 2018) are no exception. Alleviating this bias requires either modeling solutions (Karimi Mahabadi et al., 2020; Pratapa et al., 2020; Shah et al., 2020; Thorne and Vlachos, 2020; Utama et al., 2020b), which have limited effectiveness (Utama et al., 2020a), or adversarially removing troublesome training examples (Bras et al., 2020) or manually collecting new ones (Nie et al., 2020; Thorne et al., 2019a), which is model specific. Instead, our dataset design avoids single-sentence artifac"
2021.naacl-main.52,D19-1250,0,0.0775299,"odel consid1 2 1 Introduction The V ITAMIN C dataset and our models are available at: https://github.com/TalSchuster/VitaminC Etymology of V ITAMIN C: Contrastive evidence keeps fact verification models robust and healthy, hence “Vitamin C.” 624 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 624–643 June 6–11, 2021. ©2021 Association for Computational Linguistics ers the provided evidence rather than relying on built-in static knowledge, such as that obtained via language model pre-training (Petroni et al., 2019; Roberts et al., 2020). This is particularly important for scenarios in which the source of truth is mutable (e.g., the current US president, or new declarations as in Figure 1). Second, this setting discourages certain biases and idiosyncrasies—such as exploiting differences in how true vs. false claims are posed—that are common in similar crowd-sourced datasets (Poliak et al., 2018; Schuster et al., 2019). Indeed, we show that augmenting both fact verification models and NLI models with V ITAMIN C data improves their robustness to adversarial inputs. Furthermore, our emphasis on contrastive"
2021.naacl-main.52,S18-2023,0,0.051879,"Missing"
2021.naacl-main.52,D18-1003,0,0.0605265,"as in Figure 1). Second, this setting discourages certain biases and idiosyncrasies—such as exploiting differences in how true vs. false claims are posed—that are common in similar crowd-sourced datasets (Poliak et al., 2018; Schuster et al., 2019). Indeed, we show that augmenting both fact verification models and NLI models with V ITAMIN C data improves their robustness to adversarial inputs. Furthermore, our emphasis on contrastive contexts allows us to expand on the scope of commonly considered tasks. Most of the fact verification literature focuses on resolving claims to be true or false (Popat et al., 2018; Thorne and Vlachos, 2018; Wang, 2017). The surrounding ecosystem, however, includes additional challenges, some of which we explore here: Documents such as Wikipedia articles are updated frequently; which edits represent factual changes? For a given claim and (refuting or supporting) evidence pair, which words or phrases in the evidence are most relevant? If we know that a certain claim is true, can we modify an out-dated document to be consistent with it? We show that the unique structure of our V ITAMIN C dataset can be leveraged to provide both supervised and distantly supervised data for"
2021.naacl-main.52,2020.fever-1.7,1,0.856243,"design avoids single-sentence artifacts and provides model-agnostic challenging examples that increase the robustness of trained models. Explainability. Current fact verification datasets provide sentence-level rationales (DeYoung et al., 2020; Petroni et al., 2020) but do not enforce the model’s verdict to rely on them—leading to a potential discrepancy. V ITAMIN C ensures the verdict is conditioned on the retrieved evidence. Moreover, we use the revision history as distant supervision for word-level rationales, allowing for finer-grained explanations (Camburu et al., 2018; Lei et al., 2016; Portelli et al., 2020; Thorne et al., 2019b). Factually Consistent Generation. Generating texts that match given facts is a known chalFact Verification. The FEVER dataset (Thorne lenge (Fan et al., 2020; Kryscinski et al., 2020; et al., 2018) fueled the development of many fact- Lewis et al., 2020b; Parikh et al., 2020; Shah et al., checking models (e.g., see Hanselowski et al., 2020; Tian et al., 2020) as language models tend to 2018; Nie et al., 2019a,b; Yoneda et al., 2018, in- degenerate and hallucinate (Holtzman et al., 2020; ter alia). The claim creation process, however, Schuster et al., 2020; Zhou et al.,"
2021.naacl-main.52,2020.emnlp-main.629,0,0.0563968,"claims to a dynamic source of truth, and requires models to change their verdicts accordingly. Annotation Bias. Annotation artifacts are common in many NLP datasets, and affect performance on adversarial and contrastive examples (Gardner et al., 2020; Ribeiro et al., 2020; Ross et al., 2020). Sentence-pair inference tasks such as fact verification (Paul Panenghat et al., 2020; Schuster et al., 2019) and NLI (Gururangan et al., 2018; McCoy et al., 2019; Poliak et al., 2018; Tsuchiya, 2018) are no exception. Alleviating this bias requires either modeling solutions (Karimi Mahabadi et al., 2020; Pratapa et al., 2020; Shah et al., 2020; Thorne and Vlachos, 2020; Utama et al., 2020b), which have limited effectiveness (Utama et al., 2020a), or adversarially removing troublesome training examples (Bras et al., 2020) or manually collecting new ones (Nie et al., 2020; Thorne et al., 2019a), which is model specific. Instead, our dataset design avoids single-sentence artifacts and provides model-agnostic challenging examples that increase the robustness of trained models. Explainability. Current fact verification datasets provide sentence-level rationales (DeYoung et al., 2020; Petroni et al., 2020) but do not e"
2021.naacl-main.52,2020.acl-main.442,0,0.0646897,", 2019). Other recent datasets cover verification against tables (Chen et al., 2020), relational databases (Jo et al., 2019), Wikipedia references (Sathe et al., 2020), multiple articles (Jiang et al., 2020), and search snippets (Augenstein et al., 2019). These resources all assume static ground truths. In contrast, V ITAMIN C compares objective claims to a dynamic source of truth, and requires models to change their verdicts accordingly. Annotation Bias. Annotation artifacts are common in many NLP datasets, and affect performance on adversarial and contrastive examples (Gardner et al., 2020; Ribeiro et al., 2020; Ross et al., 2020). Sentence-pair inference tasks such as fact verification (Paul Panenghat et al., 2020; Schuster et al., 2019) and NLI (Gururangan et al., 2018; McCoy et al., 2019; Poliak et al., 2018; Tsuchiya, 2018) are no exception. Alleviating this bias requires either modeling solutions (Karimi Mahabadi et al., 2020; Pratapa et al., 2020; Shah et al., 2020; Thorne and Vlachos, 2020; Utama et al., 2020b), which have limited effectiveness (Utama et al., 2020a), or adversarially removing troublesome training examples (Bras et al., 2020) or manually collecting new ones (Nie et al., 2020;"
2021.naacl-main.52,2020.emnlp-main.437,0,0.136459,"Missing"
2021.naacl-main.52,2020.lrec-1.849,0,0.0749692,"tributions are as follows: 1. We pose a contrastive fact verification paradigm that requires sensitivity to changes in data; 2. We introduce V ITAMIN C, a new large-scale dataset that supports this paradigm; 3. We demonstrate that training on V ITAMIN C leads to better performance on standard tasks; 4. We show how V ITAMIN C opens the door to additional research directions in fact verification. achieve unexpectedly high performance (Schuster et al., 2019). Other recent datasets cover verification against tables (Chen et al., 2020), relational databases (Jo et al., 2019), Wikipedia references (Sathe et al., 2020), multiple articles (Jiang et al., 2020), and search snippets (Augenstein et al., 2019). These resources all assume static ground truths. In contrast, V ITAMIN C compares objective claims to a dynamic source of truth, and requires models to change their verdicts accordingly. Annotation Bias. Annotation artifacts are common in many NLP datasets, and affect performance on adversarial and contrastive examples (Gardner et al., 2020; Ribeiro et al., 2020; Ross et al., 2020). Sentence-pair inference tasks such as fact verification (Paul Panenghat et al., 2020; Schuster et al., 2019) and NLI (Gururan"
2021.naacl-main.52,2020.cl-2.8,1,0.884722,"et al., 2016; Portelli et al., 2020; Thorne et al., 2019b). Factually Consistent Generation. Generating texts that match given facts is a known chalFact Verification. The FEVER dataset (Thorne lenge (Fan et al., 2020; Kryscinski et al., 2020; et al., 2018) fueled the development of many fact- Lewis et al., 2020b; Parikh et al., 2020; Shah et al., checking models (e.g., see Hanselowski et al., 2020; Tian et al., 2020) as language models tend to 2018; Nie et al., 2019a,b; Yoneda et al., 2018, in- degenerate and hallucinate (Holtzman et al., 2020; ter alia). The claim creation process, however, Schuster et al., 2020; Zhou et al., 2020). Morerequired crowd-workers to write claims related over, evaluation is non-trivial, and usually manual. to Wikipedia articles, and was found to engender V ITAMIN C includes supervised data for training biases that allow an evidence-agnostic model to sequence-to-sequence models, and provides auto625 2 Related Work matic evaluation via the fact verification classifier. 3 recursively recycled for re-training the automated BERT classifier in the future to expand the corpus further (we also introduce this as a task, see §4.1). The V ITAMIN C Dataset V ITAMIN C (abbreviated Vit"
2021.naacl-main.52,D19-1341,1,0.919825,"6–11, 2021. ©2021 Association for Computational Linguistics ers the provided evidence rather than relying on built-in static knowledge, such as that obtained via language model pre-training (Petroni et al., 2019; Roberts et al., 2020). This is particularly important for scenarios in which the source of truth is mutable (e.g., the current US president, or new declarations as in Figure 1). Second, this setting discourages certain biases and idiosyncrasies—such as exploiting differences in how true vs. false claims are posed—that are common in similar crowd-sourced datasets (Poliak et al., 2018; Schuster et al., 2019). Indeed, we show that augmenting both fact verification models and NLI models with V ITAMIN C data improves their robustness to adversarial inputs. Furthermore, our emphasis on contrastive contexts allows us to expand on the scope of commonly considered tasks. Most of the fact verification literature focuses on resolving claims to be true or false (Popat et al., 2018; Thorne and Vlachos, 2018; Wang, 2017). The surrounding ecosystem, however, includes additional challenges, some of which we explore here: Documents such as Wikipedia articles are updated frequently; which edits represent factual"
2021.naacl-main.52,2020.acl-main.704,0,0.0415208,"Missing"
2021.naacl-main.52,2020.acl-main.770,0,0.0345115,"e useful explanations for predictions of neural models (Lei et al., 2016). Such explanations can be particularly useful for semi-automated fact verification, since they allow users to quickly interpret and trust the model’s verdict.9 In Figure 2, for example, the date of the first identified case can explain the verdict for the claim. As first proposed by Lei et al. (2016), the standard definition of extractive rationales asks for selecting the minimal set of input tokens that is sufficient for preserving the model’s prediction. Here we use a slightly modified definition following Shah et al. (2020), where we identify the minimal set of evidence tokens where removing them BOW. We use an MLP on top of a bag-of-words representation. Each sentence is encoded as e∗ , the average fastText (Bojanowski et al., 2017) word 8 To focus on the inference task, as opposed to a full end-to-end embedding of its edited words (i.e., that were resystem, we assume that we have access to an oracle retriever. 9 moved or modified in the revision). The MLP input Roitero et al. (2020) showed that explanations can increase is then taken as [et−1 ; et ; |et − et−1 |; et · et−1 ]. the agreement between users and ex"
2021.naacl-main.52,2020.emnlp-main.613,0,0.436579,"their verdicts accordingly. Annotation Bias. Annotation artifacts are common in many NLP datasets, and affect performance on adversarial and contrastive examples (Gardner et al., 2020; Ribeiro et al., 2020; Ross et al., 2020). Sentence-pair inference tasks such as fact verification (Paul Panenghat et al., 2020; Schuster et al., 2019) and NLI (Gururangan et al., 2018; McCoy et al., 2019; Poliak et al., 2018; Tsuchiya, 2018) are no exception. Alleviating this bias requires either modeling solutions (Karimi Mahabadi et al., 2020; Pratapa et al., 2020; Shah et al., 2020; Thorne and Vlachos, 2020; Utama et al., 2020b), which have limited effectiveness (Utama et al., 2020a), or adversarially removing troublesome training examples (Bras et al., 2020) or manually collecting new ones (Nie et al., 2020; Thorne et al., 2019a), which is model specific. Instead, our dataset design avoids single-sentence artifacts and provides model-agnostic challenging examples that increase the robustness of trained models. Explainability. Current fact verification datasets provide sentence-level rationales (DeYoung et al., 2020; Petroni et al., 2020) but do not enforce the model’s verdict to rely on them—leading to a potential"
2021.naacl-main.52,D19-1221,0,0.0294729,"data AUC Prec. Rec. F1 Edit dist. ALBERT PAWS-full 71.34 72.20 64.90 65.27 63.18 60.61 63.56 60.48 BOW ALBERT ALBERT VitC-diff VitC-diff VitC-full 79.87 89.87 91.97 70.85 80.69 82.63 67.84 82.06 84.49 68.55 81.18 83.18 Table 3: Factual revision flagging scores for models aware of the full sentence-pair (full) and aware only of the modified words (diff). We use ALBERT-base. break FEVER-trained models. We take all SUP and REF claims and their gold evidence sentences. Triggers (Atanasova et al., 2020): A set of 186 FEVER claims paraphrased adversarially to contain universal adversarial triggers (Wallace et al., 2019). Its small size leads to high variance results. ANLI (Nie et al., 2020): An adversarial dataset for MNLI- and FEVER-based models. The creation was performed in three iterative rounds in which a model was trained, and then crowdworkers devised adversarial inputs, and the process repeated. PAWS (Zhang et al., 2019): A dataset of altered Wikipedia sentences using word swapping and back-translation. Human annotators labeled whether the modified sentence is a paraphrase or not. We evaluate whether a PAWS-trained classifier can be used for our factual revision flagging task. 5.2 Factual Revision Fl"
2021.naacl-main.52,P17-2067,0,0.0603283,"ges certain biases and idiosyncrasies—such as exploiting differences in how true vs. false claims are posed—that are common in similar crowd-sourced datasets (Poliak et al., 2018; Schuster et al., 2019). Indeed, we show that augmenting both fact verification models and NLI models with V ITAMIN C data improves their robustness to adversarial inputs. Furthermore, our emphasis on contrastive contexts allows us to expand on the scope of commonly considered tasks. Most of the fact verification literature focuses on resolving claims to be true or false (Popat et al., 2018; Thorne and Vlachos, 2018; Wang, 2017). The surrounding ecosystem, however, includes additional challenges, some of which we explore here: Documents such as Wikipedia articles are updated frequently; which edits represent factual changes? For a given claim and (refuting or supporting) evidence pair, which words or phrases in the evidence are most relevant? If we know that a certain claim is true, can we modify an out-dated document to be consistent with it? We show that the unique structure of our V ITAMIN C dataset can be leveraged to provide both supervised and distantly supervised data for these new questions. Our key contribut"
2021.naacl-main.52,N18-1101,0,0.116028,"s perform on the V ITAMIN C tasks? (2) Does V ITAMIN C increases the robustness of models against adversarial examples? (3) Can V ITAMIN C improve interpretability by providing supervision for anchoring words? 5.1 Related Datasets In addition to V ITAMIN C, we train and evaluate on several related datasets, which we briefly describe: FEVER (Thorne et al., 2018): A popular fact verification dataset based on Wikipedia. We use the provided SUP and REF claim-evidence pairs. For NEI claims, we randomly sample neutral evidence from the article with the highest BM25 score (Fisch et al., 2021). MNLI (Williams et al., 2018): A large and diverse dataset for natural language inference. The threeway sentence-pair entailment prediction is similar to fact verification. We use the hypothesis as the claim and the premise as the evidence and evaluate on the “mismatched” evaluation set. Symmetric (Schuster et al., 2019): A set of challenging symmetric, synthetic extensions to FEVER’s evaluation set that avoid claim-only bias. Adversarial (Thorne et al., 2019c): Adversarial Automatic Revisions. Given an outdated con- examples created by participants of the FEVER 2.0 text st−1 and an updated claim c, we learn shared task."
2021.naacl-main.52,2020.emnlp-demos.6,0,0.0965709,"Missing"
2021.naacl-main.52,Q16-1029,0,0.0462022,"Missing"
2021.naacl-main.52,D17-1213,0,0.0265516,"articles5 and all of their 41K revisions at the time. Combined together, this resulted in a total of ∼200 million revisions. For each revision, we identified all of the modified sentences and stored two versions: (1) before, and (2) after the edit. In our task, we are only interested in edits made with an intent to introduce a factual modification— i.e., a change for which one can make a claim that is supported by one sentence, but not by the other.6 To expedite annotation, we trained a BERT classifier (Devlin et al., 2019) on a small labeled set of revised sentences determined to be factual (Yang et al., 2017), and used this model to select the top 305K edited sentences from the corpus for manual annotation. Trained human annotators were then presented with the sentence pairs, and were asked to mark the ones that indeed represented a factual change. Sentences lacking self-contained context were filtered (e.g., short expressions from tables or bulleted lists). Example annotations are presented in Table 1. Note that these annotations can also be 3 https://bit.ly/Wiki_Neutral_POV https://bit.ly/Wiki_popular_pages 5 https://wikimediafoundation.org/covid19 6 Many edits only reflect grammatical correctio"
2021.naacl-main.52,W18-5515,0,0.0386159,"history as distant supervision for word-level rationales, allowing for finer-grained explanations (Camburu et al., 2018; Lei et al., 2016; Portelli et al., 2020; Thorne et al., 2019b). Factually Consistent Generation. Generating texts that match given facts is a known chalFact Verification. The FEVER dataset (Thorne lenge (Fan et al., 2020; Kryscinski et al., 2020; et al., 2018) fueled the development of many fact- Lewis et al., 2020b; Parikh et al., 2020; Shah et al., checking models (e.g., see Hanselowski et al., 2020; Tian et al., 2020) as language models tend to 2018; Nie et al., 2019a,b; Yoneda et al., 2018, in- degenerate and hallucinate (Holtzman et al., 2020; ter alia). The claim creation process, however, Schuster et al., 2020; Zhou et al., 2020). Morerequired crowd-workers to write claims related over, evaluation is non-trivial, and usually manual. to Wikipedia articles, and was found to engender V ITAMIN C includes supervised data for training biases that allow an evidence-agnostic model to sequence-to-sequence models, and provides auto625 2 Related Work matic evaluation via the fact verification classifier. 3 recursively recycled for re-training the automated BERT classifier in the future"
2021.tacl-1.5,C14-1218,0,0.0202342,"arge amounts of monolingual data. The size of data for both languages is key: High-quality monolingual embeddings are required for successful matching. This assumption, however, does not hold for ancient languages, where we can typically access a few thousands of words at most. Decoding Cipher Texts Man-made ciphers have been the focal point for most of the early work on decipherment. They usually use EM algorithms, which are tailored towards these specific types of ciphers, most prominently substitution ciphers (Knight and Yamada, 1999; Knight et al., 2006). Later work by Nuhn et al. (2013), Hauer et al. (2014), and Kambhatla et al. (2018) addresses the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipherment previously done"
2021.tacl-1.5,D18-1102,0,0.0130824,"al data. The size of data for both languages is key: High-quality monolingual embeddings are required for successful matching. This assumption, however, does not hold for ancient languages, where we can typically access a few thousands of words at most. Decoding Cipher Texts Man-made ciphers have been the focal point for most of the early work on decipherment. They usually use EM algorithms, which are tailored towards these specific types of ciphers, most prominently substitution ciphers (Knight and Yamada, 1999; Knight et al., 2006). Later work by Nuhn et al. (2013), Hauer et al. (2014), and Kambhatla et al. (2018) addresses the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipherment previously done by human experts (Snyder et a"
2021.tacl-1.5,D13-1087,0,0.0197793,"the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipherment previously done by human experts (Snyder et al., 2010; Berg-Kirkpatrick and Klein, 2013; Luo et al., 2019). Our work incorporates many linguistic insights about the structure of valid alignments introduced in prior work, such as monotonicity. We further expand the linguistic foundation by incorporating phonetic regularities that have been beneficial in early, pre-neural decipherment work (Knight et al., 70 2006). However, our model is designed to handle challenging cases not addressed by prior work, where segmentation of the ancient scripts is unknown. Moreover, we are interested in dead languages without a known relative and introduce an unsupervised measure of language closene"
2021.tacl-1.5,P10-1107,1,0.554942,"al. (2018) addresses the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipherment previously done by human experts (Snyder et al., 2010; Berg-Kirkpatrick and Klein, 2013; Luo et al., 2019). Our work incorporates many linguistic insights about the structure of valid alignments introduced in prior work, such as monotonicity. We further expand the linguistic foundation by incorporating phonetic regularities that have been beneficial in early, pre-neural decipherment work (Knight et al., 70 2006). However, our model is designed to handle challenging cases not addressed by prior work, where segmentation of the ancient scripts is unknown. Moreover, we are interested in dead languages without a known relative and introduce an unsupe"
2021.tacl-1.5,L18-1429,0,0.0580564,"Missing"
2021.tacl-1.5,P13-1154,0,0.0315018,"s constructed from large amounts of monolingual data. The size of data for both languages is key: High-quality monolingual embeddings are required for successful matching. This assumption, however, does not hold for ancient languages, where we can typically access a few thousands of words at most. Decoding Cipher Texts Man-made ciphers have been the focal point for most of the early work on decipherment. They usually use EM algorithms, which are tailored towards these specific types of ciphers, most prominently substitution ciphers (Knight and Yamada, 1999; Knight et al., 2006). Later work by Nuhn et al. (2013), Hauer et al. (2014), and Kambhatla et al. (2018) addresses the problem using a heuristic search procedure, guided by a pretrained language model. To the best of our knowledge, these methods developed for tackling man-made ciphers have so far not been successfully applied to archaeological data. One contributing factor could be the inherent complexity in the evolution of natural languages. Deciphering Ancient Scripts Our research is most closely aligned with computational decipherment of ancient scripts. Prior work has already featured several successful instances of ancient language decipher"
D07-1009,P05-1018,1,0.878172,"f existing work on text structuring and hierarchical learning. Then, we define the insertion task and introduce our hierarchical ranking approach to sentence insertion. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work Text Structuring The insertion task is closely related to the extensively studied problem of sentence ordering.3 Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (Lapata, 2003; Karamanis et al., 2004; Okazaki et al., 2004; Barzilay and Lapata, 2005; Bollegala et al., 2006; Elsner and Charniak, 2007). These methods induce a total ordering based on pairwise relations between sentences. Researchers have shown that identifying precedence relations does not require deep semantic interpretation of input sentences: shallow distributional features are sufficient for accurate prediction. Our approach employs similar features to represent nodes at the lowest level of the hierarchy. The key departure of our work from previous research is the incorporation of hierarchical structure into a corpus-based approach to ordering. While in symbolic generat"
D07-1009,N04-1015,1,0.892734,"on of every inserted sentence. In cases where multiple insertions occur over time to the same article, they are treated independently of each other. To eliminate spam, we place constraints on inserted sentences: (1) a sentence has at least 8 tokens and at most 120 tokens; (2) the MINIPAR parser (Lin, 1998) can identify a subject or an object in a sentence. This process yields 4051 insertion/article pairs, from which 3240 pairs are used for training and 811 pairs for testing. These insertions are derived from 1503 Wikipedia articles. Relative to other corpora used in text structuring research (Barzilay and Lee, 2004; Lapata, 2003; Karamanis et al., 2004), texts in 6 Insertion is only one type of recorded update, others include deletions and sentence rewriting. our collection are long: an average article has 32.9 sentences, organized in 3.61 sections and 10.9 paragraphs. Our corpus only includes articles that have more than one section. When sentences are inserted between paragraphs, by convention we treat them as part of the previous paragraph. Evaluation Measures We evaluate our model using insertion accuracy at the section and paragraph level. This measure computes the percentage of matches between the"
D07-1009,P06-1049,0,0.0125209,"ucturing and hierarchical learning. Then, we define the insertion task and introduce our hierarchical ranking approach to sentence insertion. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work Text Structuring The insertion task is closely related to the extensively studied problem of sentence ordering.3 Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (Lapata, 2003; Karamanis et al., 2004; Okazaki et al., 2004; Barzilay and Lapata, 2005; Bollegala et al., 2006; Elsner and Charniak, 2007). These methods induce a total ordering based on pairwise relations between sentences. Researchers have shown that identifying precedence relations does not require deep semantic interpretation of input sentences: shallow distributional features are sufficient for accurate prediction. Our approach employs similar features to represent nodes at the lowest level of the hierarchy. The key departure of our work from previous research is the incorporation of hierarchical structure into a corpus-based approach to ordering. While in symbolic generation and discourse analys"
D07-1009,P03-1069,0,0.430694,"antially. In the following section, we provide an overview of existing work on text structuring and hierarchical learning. Then, we define the insertion task and introduce our hierarchical ranking approach to sentence insertion. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work Text Structuring The insertion task is closely related to the extensively studied problem of sentence ordering.3 Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (Lapata, 2003; Karamanis et al., 2004; Okazaki et al., 2004; Barzilay and Lapata, 2005; Bollegala et al., 2006; Elsner and Charniak, 2007). These methods induce a total ordering based on pairwise relations between sentences. Researchers have shown that identifying precedence relations does not require deep semantic interpretation of input sentences: shallow distributional features are sufficient for accurate prediction. Our approach employs similar features to represent nodes at the lowest level of the hierarchy. The key departure of our work from previous research is the incorporation of hierarchical stru"
D07-1009,P00-1010,0,0.0260116,"fledged temporal interpretation. Instead, we extract these features based on two categories of temporal cues: verb tense and date information. The verb tense feature captures whether a paragraph contains at least one sentence using the same tense as the inserted sentence. For instance, this feature would occur for the inserted sentence in Figure 1 since both the sentence and chosen paragraph employ the past tense. Another set of features takes into account the relation between the dates in a paragraph and those in an inserted sentence. We extract temporal expressions using the TIMEX2 tagger (Mani and Wilson, 2000), and compute the time interval for a paragraph bounded by its earliest and latest dates. We record the degree of overlap between the paragraph time in88 T1 T2 J1 J2 J3 J4 Section 0.575 0.7 0.675 0.725 Paragraph 0.5 0.525 0.55 0.55 Tree Dist 1.85 1.55 1.55 1.45 Table 1: Accuracy of human insertions compared against gold standard from Wikipedia’s update log. T1 is a subset of the data annotated by judges J1 and J2, while T2 is annotated by J3 and J4. terval and insertion sentence time interval. 5 Experimental Set-Up Corpus Our corpus consists of Wikipedia articles that belong to the category “L"
D07-1009,P02-1047,0,0.0365797,"are incorrectly predicted. In response to this mistake, the features associated with the correct section, n2 , are added to the weights, and the features of the incorrectly predicted section, n3 , are subtracted from the weights. An alternative update strategy would be to continue to update the feature weights of the leaf nodes, `1 and `3 . However, by identifying the exact source of path divergence we preserve the previously learned balance between leaf node features. 4 Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis (Marcu and Echihabi, 2002; Lapata, 2003; Elsner et al., 2007). We consider three types of features: lexical, positional, and temporal. This section gives a general overview of these features (see code for further details.) Lexical Features Lexical features have been shown to provide strong cues for sentence positioning. To preserve text cohesion, an inserted sentence has to be topically close to its surrounding sentences. At the paragraph level, we measure topical overlap using the TF*IDF weighted cosine similarity between an inserted sentence and a paragraph. We also use a more linguistically refined similarity measu"
D07-1009,C04-1108,0,0.0165276,"provide an overview of existing work on text structuring and hierarchical learning. Then, we define the insertion task and introduce our hierarchical ranking approach to sentence insertion. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work Text Structuring The insertion task is closely related to the extensively studied problem of sentence ordering.3 Most of the existing algorithms represent text structure as a linear sequence and are driven by local coherence constraints (Lapata, 2003; Karamanis et al., 2004; Okazaki et al., 2004; Barzilay and Lapata, 2005; Bollegala et al., 2006; Elsner and Charniak, 2007). These methods induce a total ordering based on pairwise relations between sentences. Researchers have shown that identifying precedence relations does not require deep semantic interpretation of input sentences: shallow distributional features are sufficient for accurate prediction. Our approach employs similar features to represent nodes at the lowest level of the hierarchy. The key departure of our work from previous research is the incorporation of hierarchical structure into a corpus-based approach to ordering"
D07-1009,P04-1015,0,0.0509631,"all paragraphs within a single section. In fact, when the perceptron update rule of (Dekel et al., 2004) – which modifies the weights of every divergent node along the predicted and true paths – is used in the ranking framework, it becomes virtually identical with the standard, flat, ranking perceptron of Collins (2002).5 In contrast, our approach shares the idea of (Cesa-Bianchi et al., 2006a) that “if a parent class has been predicted wrongly, then errors in the children should not be taken into account.” We also view this as one of the key ideas of the incremental perceptron algorithm of (Collins and Roark, 2004), which searches through a complex decision space step-by-step and is immediately updated at the first wrong move. Our work fuses this idea of selective hierarchical updates with the simplicity of the perceptron algorithm and the flexibility of arbitrary feature sharing inherent in the ranking framework. 3 The Algorithm In this section, we present our sentence insertion model and a method for parameter estimation. Given a hierarchically structured text composed of sections and paragraphs, the sentence insertion model determines the best paragraph within 5 The main remaining difference is that"
D07-1009,W02-1001,0,0.0265712,"re continuously updated by multiple authors. Logs of these updates are publicly available, and are used for training and testing of our algorithm. Figure 1 shows an example of a Wikipedia insertion. We believe this data will more closely mirror potential applications than synthetic collections used in previous work on text structuring. Our hierarchical training method yields significant improvement when compared to a similar nonhierarchical model which instead uses the standard 2 Data and code used in this paper are available at http://people.csail.mit.edu/edc/emnlp07/ 84 perceptron update of Collins (2002). We also report human performance on the insertion task in order to provide a reasonable upper-bound on machine performance. An analysis of these results shows that our method closes the gap between machine and human performance substantially. In the following section, we provide an overview of existing work on text structuring and hierarchical learning. Then, we define the insertion task and introduce our hierarchical ranking approach to sentence insertion. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work Tex"
D07-1009,N03-1030,0,0.0183956,"ally analyzed as a tree-like structure (Reiter and Dale, 1990), a linear view is prevalent in data-driven methods to text structuring.4 Moving beyond a linear representation enables us to handle longer texts where a local view of coherence does not suffice. At the same time, our approach does not require any manual rules for handling tree insertions, in contrast to symbolic text planners. 3 Independently and simultaneously with our work, Elsner and Charniak (2007) have studied the sentence insertion task in a different setting. 4 Though statistical methods have been used to induce such trees (Soricut and Marcu, 2003), they are not used for ordering and other text-structuring tasks. In 2001, Mr Aziz was declared ’Finance Minister of the Year’ by Euromoney and Banker’s Magazine. Shaukat Aziz (born March 6, 1949, Karachi, Pakistan) has been the Finance Minister of Pakistan since November 1999. He was nominated for the position of Prime Minister after the resignation of Zafarullah Khan Jamali on June 6, 2004. Education Aziz attended Saint Patrick’s school, Karachi and Abbottabad Public School. He graduated with a Bachelor of Science degree from Gordon College, Rawalpindi, in 1967. He obtained an MBA Degree in"
D07-1009,W03-1016,0,0.0226254,"sts of Wikipedia articles that belong to the category “Living People.” We focus on this category because these articles are commonly updated: when new facts about a person are featured in the media, a corresponding entry in Wikipedia is likely to be modified. Unlike entries in a professionally edited encyclopedia, these articles are collaboratively written by multiple users, resulting in significant stylistic and content variations across texts in our corpus. This property distinguishes our corpus from more stylistically homogeneous collections of biographies used in text generation research (Duboue and McKeown, 2003). We obtain data on insertions6 from the update log that accompanies every Wikipedia entry. For each change in the article’s history, the log records an article before and after the change. From this information, we can identify the location of every inserted sentence. In cases where multiple insertions occur over time to the same article, they are treated independently of each other. To eliminate spam, we place constraints on inserted sentences: (1) a sentence has at least 8 tokens and at most 120 tokens; (2) the MINIPAR parser (Lin, 1998) can identify a subject or an object in a sentence. Th"
D07-1009,N07-1055,0,0.0219006,"to this mistake, the features associated with the correct section, n2 , are added to the weights, and the features of the incorrectly predicted section, n3 , are subtracted from the weights. An alternative update strategy would be to continue to update the feature weights of the leaf nodes, `1 and `3 . However, by identifying the exact source of path divergence we preserve the previously learned balance between leaf node features. 4 Features Features used in our experiments are inspired by previous work on corpus-based approaches for discourse analysis (Marcu and Echihabi, 2002; Lapata, 2003; Elsner et al., 2007). We consider three types of features: lexical, positional, and temporal. This section gives a general overview of these features (see code for further details.) Lexical Features Lexical features have been shown to provide strong cues for sentence positioning. To preserve text cohesion, an inserted sentence has to be topically close to its surrounding sentences. At the paragraph level, we measure topical overlap using the TF*IDF weighted cosine similarity between an inserted sentence and a paragraph. We also use a more linguistically refined similarity measure that computes overlap considering"
D07-1009,P94-1002,0,0.0542211,"chnical reports, and web pages. These documents 1 http://stats.wikimedia.org/EN/ TablesWikipediaEN.htm 83 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 83–91, Prague, June 2007. 2007 Association for Computational Linguistics are commonly organized hierarchically into sections and paragraphs to aid reader comprehension. For documents where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). Rather than ignoring the inherent hierarchical structure of these texts, we desire to directly model such hierarchies and use them to our advantage – both representationally in our features and algorithmically in our learning procedure. To achieve this goal, we introduce a novel method for sentence insertion that operates over a hierarchical structure. Our document representation includes features for each layer of the hierarchy. For example, the word overlap between the inserted sentence and a section header would be included as an upper-level section feature, whereas a comparison of the se"
D07-1009,P04-1050,0,\N,Missing
D08-1035,A00-2004,0,0.557241,"e transcript corpus described by Malioutov and Barzilay (2006). Our system does not require parameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3. U&I requires no parameter tuning, and is used “out of the box.” In all experiments, we assume that the number of desired segments is provided. Preprocessing Standard preprocessing techniques are applied to the text for all comparisons. The Porter (1980) stemming algorithm is applied to group equivalent lexical items. A set of stop-words is also removed, using the same list originally employed by several competitive systems (Choi, 2000; 341 Textbook U&I M CS LCS EG BAYES S EG BAYES S EG - CUE BAYES S EG - CUE - PROP Meetings U&I M CS LCS EG BAYES S EG BAYES S EG - CUE BAYES S EG - CUE - PROP Pk .370 .368 .370 .339 .339 .343 Pk .297 .370 .309 .264 .261 .258 WD .376 .382 .385 .353 .353 .355 WD .347 .411 .322 .319 .316 .312 Table 1: Comparison of segmentation algorithms. Both metrics are penalties, so lower scores indicate better performance. BAYES S EG is the cohesion-only Bayesian system with marginalized language models. BAYES S EG - CUE is the Bayesian system with cue phrases. BAYES S EG - CUE - PROP adds the linguisticall"
D08-1035,P08-1095,0,0.0141009,"is topic, see (Grosz, 1977). Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation. More recently, cue phrases have been applied to topic segmentation in the supervised setting. In a supervised system that is distinct from the unsupervised model described above, Galley et al. (2003) automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation. Elsner and Charniak (2008) specify a list of cue phrases by hand; the cue phrases are used as a feature in a maximum-entropy classifier for conversation disentanglement. Unlike these approaches, we identify candidate cue phrases automatically from unlabeled data and incorporate them in the topic segmentation task without supervision. the set of all K language models.2 A linear segmentation is ensured by the additional constraint that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1 + 1 (the next segment). To obtain a high likelihood, the language models associated with each segment should conce"
D08-1035,P05-1045,0,0.00974588,"Missing"
D08-1035,P02-1026,0,0.401694,"uding the benchmark ICSI meeting dataset (Janin et al., 2003) and a new textual corpus constructed from the contents of a medical textbook. In both cases our model achieves performance surpassing multiple state-of-the-art baselines. Moreover, we demonstrate that the addition of cue phrases can further improve segmentation performance over cohesion-based methods. In addition to the practical advantages demonstrated by these experimental results, our model reveals interesting theoretical properties. Other re335 searchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002). We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment. This finding demonstrates that a relationship between discourse segmentation and entropy is a natural consequence of modeling topic structure in a generative Bayesian framework. In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. 2 Related Work Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quant"
D08-1035,P07-1094,0,0.0467381,"Missing"
D08-1035,J86-3001,0,0.744062,"neous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not ge"
D08-1035,P94-1002,0,0.0597817,"tors that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 2003). In this paper, we situate lexical cohesion in a Bayesian framework, allowing other sources of information to be incorporated without the need for labeled data. We formalize lexical cohesion in a generative model in which the text for each seg334 Proceedin"
D08-1035,J93-3003,0,0.110594,"ons within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple"
D08-1035,P95-1015,0,0.117445,"boundary from a special language model φ, which is shared across all topics and all documents in the dataset. For sentences that are not at segment boundaries, the likelihood is as before: p(xt |z, Θ, φ) = Q i∈xt θzt ,i . For sentences that immediately follow segment boundaries, we draw the first ` words from (`) φ instead. Writing xt for the ` cue words in xt , ˜ t for the remaining words, the likelihood for a and x segment-initial sentence is, Y Y p(xt |zt 6= zt−1 , Θ, φ) = φi θzt ,i . (`) i∈xt i∈˜ xt We draw φ from a symmetric Dirichlet prior φ0 . Following prior work (Galley et al., 2003; Litman and Passonneau, 1995), we consider only the first word of each sentence as a potential cue phrase; thus, we set ` = 1 in all experiments. a stationary Markov chain by repeatedly sampling among the hidden variables in the model. The most commonly-used sampling-based technique is Gibbs sampling, which iteratively samples from the conditional distribution of each hidden variable (Bishop, 2006). However, Gibbs sampling is slow to converge to a stationary distribution when the hidden variables are tightly coupled. This is the case in linear topic segmentation, due to the constraint that zt ∈ {zt−1 , zt−1 + 1} (see Sect"
D08-1035,P06-1004,1,0.893175,"tation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments. Hearst’s T EXT T ILING (1994) introduced the idea that unsupervised segmentation 1 Code and materials for this work are available at http://groups.csail.mit.edu/rbg/code/ bayesseg/. can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot"
D08-1035,P93-1020,0,0.0510594,"ahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 200"
D08-1035,J02-1002,0,0.522371,"s transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries. For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author. This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter). Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. Pk and WindowDiff are penalties, so lower values indicate better segmentations. We use the evaluation source code provided by Malioutov and Barzilay (2006). System configuration W"
D08-1035,P06-1003,0,0.395561,"Missing"
D08-1035,P01-1064,0,0.306539,"yesian framework.1 1 Introduction Topic segmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments. Hearst’s T EXT T ILING (1994) introduced the idea that unsupervised segmentation 1 Code and materials for this work are available at http://groups.csail.mit.edu/rbg/code/ bayesseg/. can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau"
D08-1035,P03-1071,0,\N,Missing
D08-1109,C04-1080,0,0.0289439,"rain highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incor"
D08-1109,P05-1033,0,0.0470847,"Missing"
D08-1109,P91-1017,0,0.422414,"tropy measure corresponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multi"
D08-1109,P02-1033,0,0.0358833,"ch efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since"
D08-1109,feldman-etal-2006-cross,0,0.071316,"sing cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Baye"
D08-1109,H05-1110,0,0.0700005,"Missing"
D08-1109,P07-1094,0,0.677275,"rk assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particula"
D08-1109,N06-1041,0,0.137328,"994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particular language. 3 Model We propose a bilingual model for unsupervised partof-speech tagging that jointly tags parallel streams of text in two languages. Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set. Our key hypothesis is that the patterns of ambiguity found in each language at the part-of-speech level will differ in syst"
D08-1109,D07-1031,0,0.0365362,"ast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual"
D08-1109,J94-2001,0,0.271465,"Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Hagh"
D08-1109,P03-1058,0,0.0062772,"ferentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentatio"
D08-1109,P06-1146,0,0.0361479,"Missing"
D08-1109,W97-0213,0,0.0603417,"ponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphol"
D08-1109,P05-1044,0,0.30992,"the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particular language. 3 Model We propose a bilingual model for unsupervised partof-speech tagging that jointly tags parallel streams of text in two languages. Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set. Our key hypothesis is that the patterns of ambiguity found in each language at the part-of-speec"
D08-1109,P08-1084,1,0.538531,"hen annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of a"
D08-1109,P98-2230,0,0.200773,"Missing"
D08-1109,H05-1107,0,0.0265007,"chronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has be"
D08-1109,N01-1026,0,0.131354,"Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include"
D08-1109,H01-1035,0,\N,Missing
D08-1109,P91-1034,0,\N,Missing
D08-1109,C98-2225,0,\N,Missing
D10-1037,N04-1015,1,0.488704,"the aspect-specific sentiment ratings (Snyder and Barzilay, 2007). Second, we consider a multi-aspect extractive summarization task in which a system extracts key properties for a pre-specified set of aspects. On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts. Moreover, jointly learning content and task parameters yields additional gains over independently learned models. 2 Related Work Prior research has demonstrated the usefulness of content models for discourse-level tasks. Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang"
D10-1037,D08-1083,0,0.0233702,"nalysis task. All assume that relevant sentences have been annotated. For instance, 378 Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, they limit it to eventrelevant sentences. Since these sentences are more likely to contain information of interest, the extraction performance increases. Another approach, taken by Choi and Cardie (2008) and Somasundaran et al. (2009) uses linguistic resources to create a latent model in a taskspecific fashion to improve performance, rather than assuming sentence-level task relevancy. Choi and Cardie (2008) address a sentiment analysis task by using a heuristic decision process based on wordlevel intermediate variables to represent polarity. Somasundaran et al. (2009) similarly uses a bootstrapped local polarity classifier to identify sentence polarity. McDonald et al. (2007) propose a model which jointly identifies global polarity as well as paragraph- and sentence-level polarity, all of whi"
D10-1037,P09-1064,0,0.0147947,"still optimize it using any gradient-based optimization solver; in our experiments, we used the LBFGS algorithm (Liu et al., 1989). 3.4 Inference We must predict a label sequence y for each sentence s of the document. We assume a loss function over a sequence labeling y and a proposed labeling yˆ, which decomposes as: X L(y, yˆ) = L(y j , yˆj ) j=1 In our case, we must marginalize out the sentence topic T : X P (y j |s) = P (y j , T |s) T = X Pθ (T |s)Pφ (y j |s, T ) T This minimum risk criterion has been widely used in NLP applications such as parsing (Goodman, 1999) and machine translation (DeNero et al., 2009). Note that the above formulation differs from the standard CRF due to the latent topic variables. Otherwise the inference task could be accomplished by directly obtaining posteriors over each y j state using the Forward-Backwards algorithm on the sentence CRF. Finding yˆ can be done efficiently. First, we obtain marginal token posteriors as above. Then, the expected loss of a token prediction is computed as follows: X P (y j |s)L(y j , yˆj ) yˆj j where each position loss is sensitive to the kind of error which is made. Failing to extract a token is penalized to a greater extent than extracti"
D10-1037,N07-1055,0,0.0106073,"iment ratings (Snyder and Barzilay, 2007). Second, we consider a multi-aspect extractive summarization task in which a system extracts key properties for a pre-specified set of aspects. On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts. Moreover, jointly learning content and task parameters yields additional gains over independently learned models. 2 Related Work Prior research has demonstrated the usefulness of content models for discourse-level tasks. Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patward"
D10-1037,N09-1037,0,0.0244119,"Missing"
D10-1037,W04-3234,0,0.0348991,"Missing"
D10-1037,W06-3808,0,0.0296827,"h indicates whether a given word was most likely emitted from the underlying topic or from a background distribution. Multi-Aspect Sentiment Ranking The goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects the user satisfaction for each aspect (Snyder and Barzilay, 2007). One of the challenges in this task is to attribute sentimentbearing words to the aspects they describe. Information about document structure has the potential to greatly reduce this ambiguity. Following standard sentiment ranking approaches (Wilson et al., 2004; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007), we employ ordinary linear regression to independently map bag-of-words representations into predicted aspect ranks. In addition to commonly used lexical features, this set is augmented Task Multi-aspect sentiment Multi-aspect summarization Amazon Yelp Labeled Train Test 600 65 35 48 24 48 Unlabeled — 12,684 33,015 Avg. Size Words Sents 1,027 20.5 214 178 11.7 11.2 Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and unlabeled documents are used for training the content model, while only the labeled training"
D10-1037,J99-4004,0,0.0283871,"ex due to the summation over T , we can still optimize it using any gradient-based optimization solver; in our experiments, we used the LBFGS algorithm (Liu et al., 1989). 3.4 Inference We must predict a label sequence y for each sentence s of the document. We assume a loss function over a sequence labeling y and a proposed labeling yˆ, which decomposes as: X L(y, yˆ) = L(y j , yˆj ) j=1 In our case, we must marginalize out the sentence topic T : X P (y j |s) = P (y j , T |s) T = X Pθ (T |s)Pφ (y j |s, T ) T This minimum risk criterion has been widely used in NLP applications such as parsing (Goodman, 1999) and machine translation (DeNero et al., 2009). Note that the above formulation differs from the standard CRF due to the latent topic variables. Otherwise the inference task could be accomplished by directly obtaining posteriors over each y j state using the Forward-Backwards algorithm on the sentence CRF. Finding yˆ can be done efficiently. First, we obtain marginal token posteriors as above. Then, the expected loss of a token prediction is computed as follows: X P (y j |s)L(y j , yˆj ) yˆj j where each position loss is sensitive to the kind of error which is made. Failing to extract a token"
D10-1037,N09-1041,1,0.699094,"consider a multi-aspect extractive summarization task in which a system extracts key properties for a pre-specified set of aspects. On both tasks, our method for incorporating content structure consistently outperforms structureagnostic counterparts. Moreover, jointly learning content and task parameters yields additional gains over independently learned models. 2 Related Work Prior research has demonstrated the usefulness of content models for discourse-level tasks. Examples of such tasks include sentence ordering (Barzilay and Lee, 2004; Elsner et al., 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007). These approaches co"
D10-1037,P06-2063,0,0.0230655,"of the algorithm using automatically induced content models against the gold standard structural information. Multi-Aspect Review Summarization The goal of this task is to extract informative phrases that identify information relevant to several predefined aspects of interest. In other words, we would like our system to both extract important phrases (e.g., cheap food) and label it with one of the given aspects (e.g., value). For concrete examples and lists of aspects for each data set, see Figures 3b and 3c. Variants of this task have been considered in review summarization in previous work (Kim and Hovy, 2006; Branavan et al., 2009). This task has elements of both information extraction and phrase-based summarization — the phrases we wish to extract are broader in scope than in standard template-driven IE, but at the same time, the type of selected information is restricted to the defined aspects, similar to query-based summarization. The difficulty here is that phrase selection is highly context-dependent. For instance, in TV reviews such as in Figure 3b, the highlighted phrase “easy to read” might refer to either the menu or the remote; broader 5 http://dvd.ign.com/index/reviews.html 383 context"
D10-1037,W04-1013,0,0.0624557,"Missing"
D10-1037,P07-1055,0,0.158236,"marization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007). These approaches consider global contextual information when determining whether a given sentence is relevant to the underlying analysis task. All assume that relevant sentences have been annotated. For instance, 378 Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, they limit it to eventrelevant sentences. Since th"
D10-1037,P04-1035,0,0.184496,"iment, the sentence in which it appears does not specify the aspect to which it relates. Resolving this ambiguity requires information about global document structure. A central challenge in utilizing such information lies in finding a relevant representation of content structure for a specific text analysis task. For 1 Code and processed data presented here are available at http://groups.csail.mit.edu/rbg/code/content structure.html instance, when performing single-aspect sentiment analysis, the most relevant aspect of content structure is whether a given sentence is objective or subjective (Pang and Lee, 2004). In a multi-aspect setting, however, information about the sentence topic is required to determine the aspect to which a sentiment-bearing word relates (Snyder and Barzilay, 2007). As we can see from even these closely related applications, the content structure representation should be intimately tied to a specific text analysis task. In this work, we present an approach in which a content model is learned jointly with a text analysis task. We assume complete annotations for the task itself, but we learn the content model from raw, unannotated text. Our approach is implemented in a discrimin"
D10-1037,P05-1015,0,0.034547,"clude a feature which indicates whether a given word was most likely emitted from the underlying topic or from a background distribution. Multi-Aspect Sentiment Ranking The goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects the user satisfaction for each aspect (Snyder and Barzilay, 2007). One of the challenges in this task is to attribute sentimentbearing words to the aspects they describe. Information about document structure has the potential to greatly reduce this ambiguity. Following standard sentiment ranking approaches (Wilson et al., 2004; Pang and Lee, 2005; Goldberg and Zhu, 2006; Snyder and Barzilay, 2007), we employ ordinary linear regression to independently map bag-of-words representations into predicted aspect ranks. In addition to commonly used lexical features, this set is augmented Task Multi-aspect sentiment Multi-aspect summarization Amazon Yelp Labeled Train Test 600 65 35 48 24 48 Unlabeled — 12,684 33,015 Avg. Size Words Sents 1,027 20.5 214 178 11.7 11.2 Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and unlabeled documents are used for training the content model, while on"
D10-1037,D07-1075,0,0.0136421,", 2007), extraction-based summarization (Haghighi and Vanderwende, 2009) and text segmentation (Chen et al., 2009). Since these tasks are inherently tied to document structure, a content model is essential to performing them successfully. In contrast, the applications considered in this paper are typically developed without any discourse information, focusing on capturing sentencelevel relations. Our goal is to augment these models with document-level content information. Several applications in information extraction and sentiment analysis are close in spirit to our work (Pang and Lee, 2004; Patwardhan and Riloff, 2007; McDonald et al., 2007). These approaches consider global contextual information when determining whether a given sentence is relevant to the underlying analysis task. All assume that relevant sentences have been annotated. For instance, 378 Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, they limit it to eventrele"
D10-1037,D08-1027,0,0.0283757,"Missing"
D10-1037,N07-1038,1,0.950898,"tral challenge in utilizing such information lies in finding a relevant representation of content structure for a specific text analysis task. For 1 Code and processed data presented here are available at http://groups.csail.mit.edu/rbg/code/content structure.html instance, when performing single-aspect sentiment analysis, the most relevant aspect of content structure is whether a given sentence is objective or subjective (Pang and Lee, 2004). In a multi-aspect setting, however, information about the sentence topic is required to determine the aspect to which a sentiment-bearing word relates (Snyder and Barzilay, 2007). As we can see from even these closely related applications, the content structure representation should be intimately tied to a specific text analysis task. In this work, we present an approach in which a content model is learned jointly with a text analysis task. We assume complete annotations for the task itself, but we learn the content model from raw, unannotated text. Our approach is implemented in a discriminative framework using latent variables to represent facets of content structure. In this framework, the original task features (e.g., lexical ones) are conjoined with latent variab"
D10-1037,D09-1018,0,0.014958,"at relevant sentences have been annotated. For instance, 378 Pang and Lee (2004) refine the accuracy of sentiment analysis by considering only the subjective sentences of a review as determined by an independent classifier. Patwardhan and Riloff (2007) take a similar approach in the context of information extraction. Rather than applying their extractor to all the sentences in a document, they limit it to eventrelevant sentences. Since these sentences are more likely to contain information of interest, the extraction performance increases. Another approach, taken by Choi and Cardie (2008) and Somasundaran et al. (2009) uses linguistic resources to create a latent model in a taskspecific fashion to improve performance, rather than assuming sentence-level task relevancy. Choi and Cardie (2008) address a sentiment analysis task by using a heuristic decision process based on wordlevel intermediate variables to represent polarity. Somasundaran et al. (2009) similarly uses a bootstrapped local polarity classifier to identify sentence polarity. McDonald et al. (2007) propose a model which jointly identifies global polarity as well as paragraph- and sentence-level polarity, all of which are observed in training dat"
D10-1037,C04-1146,0,0.0222285,"Missing"
D10-1083,N10-1083,0,0.211554,"Missing"
D10-1083,W06-2920,0,0.0481362,"203568 699605 206678 89334 191467 # Word Types 49206 18356 28393 72325 28931 16458 20057 # Tags 45 25 12 54 22 47 41 Table 2: Statistics for various corpora utilized in experiments. See Section 5. The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task. 5.1 Data Sets Following the set-up of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English. For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora (Buchholz and Marsi, 2006) which include gold POS tags (used for evaluation). We train and test on the CoNLL-X training set. Statistics for all data sets are shown in Table 2. 5.2 Setup Models To assess the marginal utility of each component of the model (see Section 3), we incrementally increase its sophistication. Specifically, we evaluate three variants: The first model (1TW) only encodes the one tag per word constraint and is uniform over type-level tag assignments. The second model (+PRIOR) utilizes the independent prior over type-level tag assignments P (T |ψ). The final model 858 (+FEATS) utilizes the tag prior"
D10-1083,D08-1036,0,0.668081,"(Haghighi and Klein, 2006) and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags. We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, 4 Typically, the performance stabilizes after only 10 iterations. 5 We choose these two metrics over the Variation Information measure due to the deficiencies discussed in Gao and Johnson (2008). we perform five runs with different random initialization of sampling state. Hyperparameter settings are sorted according to the median one-to-one metric over runs. We report results for the best and median hyperparameter settings obtained in this way. Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) result"
D10-1083,P07-1094,0,0.73509,"of-the art systems. Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward na¨ıve-Bayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. 2 Related Work Recent work has made significant progress on unsupervised POS tagging (M´erialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and John854 son, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant"
D10-1083,N06-1041,1,0.961221,"port performance exceeding that of state-of-the art systems. Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward na¨ıve-Bayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. 2 Related Work Recent work has made significant progress on unsupervised POS tagging (M´erialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and John854 son, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). Th"
D10-1083,E09-1042,0,0.05008,"and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method. In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple na¨ıveBayes approach also yields substantial performance gains, without the associated training complexity. 3 P (φ, θ|T , α, β) = (P (φt |α)P (θt |T , α)) t=1 Generative Story We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As i"
D10-1083,D07-1031,0,0.738911,"0. 2010 Association for Computational Linguistics Language English Danish Dutch German Spanish Swedish Portuguese Original case 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. token-level HMM to reflect lexicon sparsity. This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007). There are two key benefits of this model architecture. First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a typelevel tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns). Second, the reduced number of hidden variables and parameters dramatically speeds up learning and inference. We evaluate our model on seven languages exhibiting substantial syntactic variation. On several languages, we report performance exceeding that of state-of-the art systems."
D10-1083,P10-2040,0,0.44291,"aghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and John854 son, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context. Other approaches encode sparsity as a soft constraint. For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags. This design does not guarantee “structural zeros,” but biases towards sparsity. A more forceful approach for encoding"
D10-1083,J94-2001,0,0.761988,"Missing"
D10-1083,P09-1057,0,0.652416,"2007; Gao and Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010). These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable. By design, they readily capture regularities at the token-level. However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity. Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (Grac¸a et al., 2009; Ravi and Knight, 2009). In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, tokenlevel HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a 853"
D10-1083,E95-1020,0,0.645596,"Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and John854 son, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context. Other approaches encode sparsity as a soft constraint. For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags. This design does not guarantee “structural zeros,” but biases towards sparsity. A more forceful a"
D10-1083,P05-1044,0,0.54027,"several languages, we report performance exceeding that of state-of-the art systems. Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward na¨ıve-Bayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts. 2 Related Work Recent work has made significant progress on unsupervised POS tagging (M´erialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson, 2007; Goldwater and Griffiths, 2007; Gao and John854 son, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 199"
D10-1120,W09-0106,0,0.0418509,"7) demonstrated empirical gains on a synthetic corpus, our experiments focus on unsupervised category refinement on real language data. Universal Rules in NLP Despite the recent surge of interest in multilingual learning (Kuhn, 2004; Cohen and Smith, 2009a; Snyder et al., 2009; BergKirkpatrick and Klein, 2010), there is surprisingly little computational work on linguistic universals. On the acquisition side, Daum´e III and Campbell (2007) proposed a computational technique for discovering universal implications in typological features. More closely related to our work is the position paper by Bender (2009), which advocates the use of manually-encoded cross-lingual generalizations for the development of NLP systems. She argues that a system employing such knowledge could be easily adapted to a particular language by specializing this high level knowledge based on the typological features of the language. We also argue that cross-language universals are beneficial for automatic language processing; however, our focus is on learning language-specific adaptations of these rules from data. 3 For each observed coarse symbol s: 1. Draw top-level infinite multinomial over subsymbols βs ∼ GEM(γ). 2. For"
D10-1120,P10-1131,0,0.754224,"spite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These rules incorporate the definitional properties of syntactic categories in terms"
D10-1120,W06-2920,0,0.0604008,"of hand-crafted dependency rules designed by Michael Collins3 for deterministic parsing, shown in Table 3. Unlike the universals from Table 1, these rules alone are enough to construct a full dependency tree. Thus they allow us to judge whether the model is able to improve upon a human-engineered deterministic parser. Moreover, with this dataset we can assess the additional benefit of using rules tailored to an individual language as opposed to universal rules. 6 dencies with the Collins head finding rules (Collins, 1999); for the other languages we use data from the 2006 CoNLL-X Shared Task (Buchholz and Marsi, 2006). Each dataset provides manually annotated part-of-speech tags that are used for both training and testing. For comparison purposes with previous work, we limit the cross-lingual experiments to sentences of length 10 or less (not counting punctuation). For English, we also explore sentences of length up to 20. The final output metric is directed dependency accuracy. This is computed based on the Viterbi parses produced using the final unnormalized variational distribution q(z) over dependency structures. Hyperparameters and Training Regimes Unless otherwise stated, in experiments with rule-bas"
D10-1120,D08-1092,0,0.0186701,"n explanation of the ruleset is provided in Section 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These"
D10-1120,P07-1036,0,0.045162,"languages share a high-level Indo-European ancestry, they cover a diverse range of syntactic phenomenon. Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010). 2 Related Work Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a). The way we apply constraints is clos1235 est to the latter two approaches of posterior regularization and generalized expectation criteria. In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010). This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its parameters. In their approach, parameters are"
D10-1120,N09-1009,0,0.505595,"set is provided in Section 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These rules incorporate the"
D10-1120,P09-2001,0,0.331782,"set is provided in Section 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These rules incorporate the"
D10-1120,P07-1009,0,0.0163131,"Missing"
D10-1120,P09-1041,0,0.178641,"a diverse range of syntactic phenomenon. Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010). 2 Related Work Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a). The way we apply constraints is clos1235 est to the latter two approaches of posterior regularization and generalized expectation criteria. In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010). This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its parameters. In their approach, parameters are estimated using a modified EM algorithm, where the E-step minimiz"
D10-1120,P07-1035,0,0.0148978,"Missing"
D10-1120,P09-1042,0,0.101254,"d Work Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a). The way we apply constraints is clos1235 est to the latter two approaches of posterior regularization and generalized expectation criteria. In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010). This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its parameters. In their approach, parameters are estimated using a modified EM algorithm, where the E-step minimizes the KL-divergence between the model posterior and the set of distributions that satisfies the constraints. Our approach also expresses constraints as expectations on the posterior; we utilize the machinery of their framework within a variational inference algorithm with a mean field approximation. Generalized exp"
D10-1120,P06-1111,0,0.0961114,"and Swedish. Though these languages share a high-level Indo-European ancestry, they cover a diverse range of syntactic phenomenon. Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010). 2 Related Work Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a). The way we apply constraints is clos1235 est to the latter two approaches of posterior regularization and generalized expectation criteria. In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010). This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its parameters. In their appro"
D10-1120,N09-1012,1,0.722165,"Missing"
D10-1120,P04-1061,0,0.74553,"e is formed. Each node of the dependency tree is comprised of three random variables: an observed coarse symbol s, a hidden refined subsymbol z, and an observed word x. In the following let the parent of the current node have symbol s0 and subsymbol z 0 ; the root node is generated from separate root-specific distributions. Subsymbol refinement is an optional component of the full model and can be omitted by deterministically equating s and z. As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004). First we draw symbol s from a finite multinomial s z x θszc - βs πss0 z0 c - φsz - coarse symbol (observed) refined subsymbol word (observed) distr over child coarse symbols for each parent s and z and context c top-level distr over subsymbols for s distr over subsymbols for each s, parent s0 and z 0 , and context c distr over words for s and z Figure 1: Graphical representation of the model and a summary of the notation. There is a copy of the outer plate for each distinct symbol in the observed coarse tags. Here, node 3 is shown to be the parent of nodes 1 and 2. Shaded variables are obser"
D10-1120,P04-1060,0,0.247121,"ategories. An explanation of the ruleset is provided in Section 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such a"
D10-1120,D07-1072,0,0.0328162,"vised counterpart. The constraints they studied are corpus- and languagespecific. Our work demonstrates that a small set of language-independent universals can also serve as effective constraints. Furthermore, we find that our method outperforms the generalized expectation approach using corpus-specific constraints. Learning to Refine Syntactic Categories Recent research has demonstrated the usefulness of automatically refining the granularity of syntactic categories. While most of the existing approaches are implemented in the supervised setting (Finkel et al., 2007; Petrov and Klein, 2007), Liang et al. (2007) propose a non-parametric Bayesian model that learns the granularity of PCFG categories in an unsupervised fashion. For each non-terminal grammar symbol, the model posits a Hierarchical Dirichlet Process over its refinements (subsymbols) to automatically learn the granularity of syntactic categories. As with their work, we also use nonparametric priors for category refinement and employ variational methods for inference. However, our goal is to apply category refinement to dependency parsing, rather than to PCFGs, requiring a substantially different model formulation. While Liang et al. (2007)"
D10-1120,J93-2004,0,0.0410905,"experiments each coarse symbol corresponds to only one refined symbol. This is easily effected during inference by setting the HDP variational approximation truncation level to one. For each experiment we run 50 iterations of variational updates; for each iteration we perform five steps of gradient search to compute the update for the variational distribution q(z) over dependency structures. Experimental Setup Datasets and Evaluation We test the effectiveness of our grammar induction approach on English, Danish, Portuguese, Slovene, Spanish, and Swedish. For English we use the Penn Treebank (Marcus et al., 1993), transformed from CFG parses into depen3 Personal communication. 1240 7 Results In the following section we present our primary cross-lingual results using universal rules (Section 7.1) before performing a more in-depth analysis of model properties such as sensitivity to ruleset selection and inference stability (Section 7.2). English Danish Portuguese Slovene Spanish Swedish DMV 47.1 33.5 38.5 38.5 28.0 45.3 PGI 62.3 41.6 63.0 48.4 58.4 58.3 No-Split 71.5 48.8 54.0 50.6 64.8 63.3 HDP-DEP 71.9 (0.3) 51.9 (1.6) 71.5 (0.5) 50.9 (5.5) 67.2 (0.4) 62.1 (0.5) Table 4: Directed dependency accuracy u"
D10-1120,P09-1009,1,0.91835,"on 5. Introduction Despite surface differences, human languages exhibit striking similarities in many fundamental aspects of syntactic structure. These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing. In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/dependency/ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules. As input to the model, we assume a corpus annotated with coarse syntactic categories (i.e., high-level part-ofspeech tags) and a set of universal rules defined over these categories, such as those in Table 1. These rules incorporate the definitional propertie"
D10-1120,E06-1027,0,\N,Missing
D10-1120,banea-etal-2008-bootstrapping,0,\N,Missing
D10-1120,kamps-etal-2004-using,0,\N,Missing
D10-1120,C00-1044,0,\N,Missing
D10-1120,H05-1044,0,\N,Missing
D10-1120,ruppenhofer-etal-2008-finding,0,\N,Missing
D10-1120,W03-1017,0,\N,Missing
D10-1120,H05-2017,0,\N,Missing
D10-1120,H05-1043,0,\N,Missing
D10-1120,W06-1642,0,\N,Missing
D10-1120,W03-1014,0,\N,Missing
D10-1120,J03-4003,0,\N,Missing
D10-1120,P03-1054,0,\N,Missing
D10-1120,C04-1200,0,\N,Missing
D10-1120,P08-1081,0,\N,Missing
D10-1120,2007.sigdial-1.5,0,\N,Missing
D10-1120,H05-1091,0,\N,Missing
D10-1120,P05-1017,0,\N,Missing
D12-1125,W06-2920,0,0.414456,"al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007), they are not identical and yield different parsing performance (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). The goal of our work is to automate this process and construct mappings that are optimized for performance on downstream tasks (here we focus on parsing). As our results show, we achieve this goal 0.35 Unigram Frequency -Investors [are appealing] to the Securities and Exchange Commission not to [limit] their access to information [about stock purchases] and sales [by corporate insiders] English German 0.3 0.25 0.2 0.15 -Einer der sich [für de"
D12-1125,D08-1092,0,0.028264,"comes from distributional features that capture global statistics. Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In p"
D12-1125,D11-1005,0,0.215234,"further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz and Marsi, 2006; Niv"
D12-1125,P07-1035,0,0.040376,"Missing"
D12-1125,N06-1041,0,0.0691889,"Missing"
D12-1125,P03-1054,0,0.0143301,"on of similarities in POS tag statistics across languages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other l"
D12-1125,P08-1068,0,0.0511973,"Missing"
D12-1125,D10-1083,1,0.849245,"Noun Order of Demonstrative and Noun Values SVO, SOV, VSO, VOS, OVS, OSV Postpositions, Prepositions, Inpositions Genitive-Noun, Noun-Genitive Adjective-Noun, Noun-Adjective Demonstrative-Noun, Noun-Demonstrative, before and after Table 1: The set of typological features that we use for source language selection. The first column gives the ID of the feature as listed in WALS. The second column describes the feature and the last column enumerates the allowable values for each feature; besides these values each feature can also have a value of ‘No dominant order’. vised POS induction algorithm (Lee et al., 2010).11 Our mapping algorithm then learns the connection between these clusters and universal tags. For initialization, we perform multiple random restarts and select the one with the lowest final objective score. 7 Results We first present the results of our model using the gold POS tags for the target language. Table 2 summarizes the performance of our model and the baselines. Comparison against Baselines On average, the mapping produced by our model yields parsers with higher accuracy than all of the baselines. These results are consistent for both parsers (McDonald et al., 2011; Cohen et al.,"
D12-1125,D07-1072,0,0.0165044,"s on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our task consists of a target corpus written in"
D12-1125,P05-1010,0,0.0159703,"tag statistics across languages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formula"
D12-1125,H05-1066,0,0.0438179,"rest (using the method from Section 5.3). For the selected source language, we assume access to the mapping of Petrov et al. (2011). Evaluation Measures We evaluate the quality of the derived mapping in the context of the target language parsing accuracy. In both the training and test data, the language-specific tags are replaced with universal tags: Petrov’s tags for the source languages and learned tags for the target language. We train two non-lexicalized parsers using source annotations and apply them to the target language. The first parser is a non-lexicalized version of the MST parser (McDonald et al., 2005) successfully used in the multilingual context (McDonald et al., 2011). In the second parser, parameters of the target language are estimated as a weighted mixture of parameters learned from supervised source languages (Cohen et al., 2011). For the parser of Cohen et al. (2011), we trained the model on the four languages used in the original paper — English, German, Czech and Italian. When measuring the performance on each of these four languages, we selected another set of four languages with a similar level of diversity.10 Following the standard evaluation practice in parsing, we use directe"
D12-1125,D11-1006,0,0.507153,"ansfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with language-specific tags. In previous work, their mapping to universal tags was performed manually. Yet, even though some of these mappings have been developed for the same CoNLL dataset (Buchholz"
D12-1125,D10-1120,1,0.94089,"ts Institute of Technology The Hebrew University {yuanzh, roiri, regina}@csail.mit.edu gamir@cs.huji.ac.il Abstract While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources. In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset. To reconcile these cross-lingual annotation differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). In all of these cases, the conversion is performed manually and has to be repeated for each language and annotation scheme anew. We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages. We e"
D12-1125,N07-1051,0,0.0150483,"gram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our task consists of a targ"
D12-1125,P06-1055,0,0.0468946,"anguages. (a) The unigram frequency statistics on five tags for two close languages, English and German. (b) Sample sentences in English and German. Verbs are shown in blue, prepositions in red and noun phrases in green. It can be seen that noun phrases follow prepositions. on a broad range of languages and evaluation scenarios. Syntactic Category Refinement Our work also relates to work in syntactic category refinement in which POS categories and parse tree non-terminals are refined in order to improve parsing performance (Finkel et al., 2007; Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007; Liang et al., 2007). Our work differs from these approaches in two ways. First, these methods have been developed in the monolingual setting, while our mapping algorithm is designed for multilingual parsing. Second, these approaches are trained on the syntactic trees of the target language, which enables them to directly link the quality of newly induced categories with the quality of syntactic parsing. In contrast, we are not given trees in the target language. Instead, our model is informed by mappings derived for other languages. 3 Task Formulation The input to our"
D12-1125,H05-1107,0,0.0195795,"our largest gain comes from distributional features that capture global statistics. Finally, we establish that the mapping quality has a significant impact on the accuracy of syntactic transfer, which motivates further study of this topic. 2 Related Work Multilingual Parsing Early approaches for multilingual parsing used parallel data to bridge the gap between languages when modeling syntactic transfer. In this setup, finding the mapping between various POS annotation schemes was not essential; instead, the transfer algorithm could induce it directly from the parallel data (Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008). However, more recent transfer approaches relinquish this data requirement, learning to transfer from non-parallel data (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2010). These approaches assume access to a common input representation in the form of universal tags, which enables the model to connect patterns observed in the source language to their counterparts in the target language. Despite ongoing efforts to standardize POS tags across languages (e.g., EAGLES initiative (Eynde, 2004)), many corpora are still annotated with la"
D12-1125,I08-3008,0,0.24379,"t, Regina Barzilay Amir Globerson Massachusetts Institute of Technology The Hebrew University {yuanzh, roiri, regina}@csail.mit.edu gamir@cs.huji.ac.il Abstract While the notion of a universal POS tagset is widely accepted, in practice it is hardly ever used for annotation of monolingual resources. In fact, available POS annotations are designed to capture language-specific idiosyncrasies and therefore are substantially more detailed than a coarse universal tagset. To reconcile these cross-lingual annotation differences, a number of mapping schemes have been proposed in the parsing community (Zeman and Resnik, 2008; Petrov et al., 2011; Naseem et al., 2010). In all of these cases, the conversion is performed manually and has to be repeated for each language and annotation scheme anew. We present an automatic method for mapping language-specific part-of-speech tags to a set of universal tags. This unified representation plays a crucial role in cross-lingual syntactic transfer of multilingual dependency parsers. Until now, however, such conversion schemes have been created manually. Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consiste"
D12-1125,D07-1096,0,\N,Missing
D14-1095,D13-1174,0,0.0124657,"e 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 880–885, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Feature morpheme unigrams morpheme bigram phonetic seq. unigrams phonetic seq. bigram number of morphemes morpheme lengths phologically rich languages. This results in improved language model perplexity, better word alignments and higher BLEU scores. Recent work has demonstrated that even morphological analyzers that use little or no supervision can help improve performance in language modeling and machine translation (Chahuneau et al., 2013b; Stallard et al., 2012). It has also been shown that segmentation lattices improve the quality of machine translation systems (Dyer, 2009). In this work, we leverage morphological segmentation to reduce OOV rates in KWS. We investigate segmentations produced by a range of models, including acoustic sub-word units. We incorporate these subword units into a lattice framework within the KWS system. We also demonstrate the value of using alternative segmentations instead of or in combination with morphemes. In addition to improving the performance of KWS systems, this finding may also benefit ot"
D14-1095,kruengkrai-etal-2006-conditional,0,0.0269761,"egmentations. Hence, we choose the top two segmentations per word from the output of the ranker to use in our KWS system. In both filters, we use several features like morpheme unigrams, bigrams, lengths, number of morphemes, and phone sequences corresponding to the morphemes. In our supervised systems, we can encode features that go beyond individual boundaries, like the total number of morphemes in the segmentation. This global view distinguishes our classifier/ranker from traditional approaches that model segmentation as a sequence tagging task (Ruokolainen et al., 2013; Kudo et al., 2004; Kruengkrai et al., 2006). Another departure of our approach is the use of phonetic information, in the form of phonetic sequences corresponding to the morpheme unigrams and bigrams. The hypothesis is that syllabic boundaries are correlated with morpheme boundaries to some extent. The phonetic sequences for words are obtained using a publicly available Text-to-Phone (T2P) system (Lenzo, 1998). Unsupervised Morphological Segmentation We employ a widely-used unsupervised system Morfessor (Creutz and Lagus, 2005) which achieves state-of-the-art unsupervised performance in the MorphoChallenge evaluation. Morfessor uses pr"
D14-1095,N13-1140,0,0.0288465,"e 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 880–885, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Feature morpheme unigrams morpheme bigram phonetic seq. unigrams phonetic seq. bigram number of morphemes morpheme lengths phologically rich languages. This results in improved language model perplexity, better word alignments and higher BLEU scores. Recent work has demonstrated that even morphological analyzers that use little or no supervision can help improve performance in language modeling and machine translation (Chahuneau et al., 2013b; Stallard et al., 2012). It has also been shown that segmentation lattices improve the quality of machine translation systems (Dyer, 2009). In this work, we leverage morphological segmentation to reduce OOV rates in KWS. We investigate segmentations produced by a range of models, including acoustic sub-word units. We incorporate these subword units into a lattice framework within the KWS system. We also demonstrate the value of using alternative segmentations instead of or in combination with morphemes. In addition to improving the performance of KWS systems, this finding may also benefit ot"
D14-1095,W04-3230,0,0.063284,"average 2.5 gold segmentations. Hence, we choose the top two segmentations per word from the output of the ranker to use in our KWS system. In both filters, we use several features like morpheme unigrams, bigrams, lengths, number of morphemes, and phone sequences corresponding to the morphemes. In our supervised systems, we can encode features that go beyond individual boundaries, like the total number of morphemes in the segmentation. This global view distinguishes our classifier/ranker from traditional approaches that model segmentation as a sequence tagging task (Ruokolainen et al., 2013; Kudo et al., 2004; Kruengkrai et al., 2006). Another departure of our approach is the use of phonetic information, in the form of phonetic sequences corresponding to the morpheme unigrams and bigrams. The hypothesis is that syllabic boundaries are correlated with morpheme boundaries to some extent. The phonetic sequences for words are obtained using a publicly available Text-to-Phone (T2P) system (Lenzo, 1998). Unsupervised Morphological Segmentation We employ a widely-used unsupervised system Morfessor (Creutz and Lagus, 2005) which achieves state-of-the-art unsupervised performance in the MorphoChallenge eva"
D14-1095,coltekin-2010-freely,0,0.0202908,"Missing"
D14-1095,W13-3504,0,0.199738,"g corpus, each word has on average 2.5 gold segmentations. Hence, we choose the top two segmentations per word from the output of the ranker to use in our KWS system. In both filters, we use several features like morpheme unigrams, bigrams, lengths, number of morphemes, and phone sequences corresponding to the morphemes. In our supervised systems, we can encode features that go beyond individual boundaries, like the total number of morphemes in the segmentation. This global view distinguishes our classifier/ranker from traditional approaches that model segmentation as a sequence tagging task (Ruokolainen et al., 2013; Kudo et al., 2004; Kruengkrai et al., 2006). Another departure of our approach is the use of phonetic information, in the form of phonetic sequences corresponding to the morpheme unigrams and bigrams. The hypothesis is that syllabic boundaries are correlated with morpheme boundaries to some extent. The phonetic sequences for words are obtained using a publicly available Text-to-Phone (T2P) system (Lenzo, 1998). Unsupervised Morphological Segmentation We employ a widely-used unsupervised system Morfessor (Creutz and Lagus, 2005) which achieves state-of-the-art unsupervised performance in the"
D14-1095,N09-1046,0,0.0111373,"Computational Linguistics Feature morpheme unigrams morpheme bigram phonetic seq. unigrams phonetic seq. bigram number of morphemes morpheme lengths phologically rich languages. This results in improved language model perplexity, better word alignments and higher BLEU scores. Recent work has demonstrated that even morphological analyzers that use little or no supervision can help improve performance in language modeling and machine translation (Chahuneau et al., 2013b; Stallard et al., 2012). It has also been shown that segmentation lattices improve the quality of machine translation systems (Dyer, 2009). In this work, we leverage morphological segmentation to reduce OOV rates in KWS. We investigate segmentations produced by a range of models, including acoustic sub-word units. We incorporate these subword units into a lattice framework within the KWS system. We also demonstrate the value of using alternative segmentations instead of or in combination with morphemes. In addition to improving the performance of KWS systems, this finding may also benefit other applications that currently use morphological segmentation for OOV reduction. 3 Table 1: Example of features used in the supervised filt"
D14-1095,P12-2063,1,0.789403,"rical Methods in Natural Language Processing (EMNLP), pages 880–885, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Feature morpheme unigrams morpheme bigram phonetic seq. unigrams phonetic seq. bigram number of morphemes morpheme lengths phologically rich languages. This results in improved language model perplexity, better word alignments and higher BLEU scores. Recent work has demonstrated that even morphological analyzers that use little or no supervision can help improve performance in language modeling and machine translation (Chahuneau et al., 2013b; Stallard et al., 2012). It has also been shown that segmentation lattices improve the quality of machine translation systems (Dyer, 2009). In this work, we leverage morphological segmentation to reduce OOV rates in KWS. We investigate segmentations produced by a range of models, including acoustic sub-word units. We incorporate these subword units into a lattice framework within the KWS system. We also demonstrate the value of using alternative segmentations instead of or in combination with morphemes. In addition to improving the performance of KWS systems, this finding may also benefit other applications that cur"
D14-1095,P08-2015,0,0.0136628,"Missing"
D14-1109,J93-2003,0,0.0411606,"014 of them work remarkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 3.1 Method In an online learning setup, parameters are updated successively after each sentence. Each update still requires us to find the “strongest violation”, i.e., a candidate tree y˜ that scores higher than the gold tree yˆi : y˜ = arg max {S(ˆ xi , y) + ky − yˆi k1 } y∈T (ˆ xi ) The parameters are then revised so as to select against the offending y˜. Instead of a standard parameter update bas"
D14-1109,W06-2920,0,0.118834,"nding “a violation” rather than the “strongest violation”) impacts the learned parser. To this end, Huang et al. (2012) have established that weaker violations do suffice for separable training sets. 5 m h grandparent! consecutive sibling! arc! m h g s m m +1 g tri-siblings! m h s gc m h m s grand-grandparent! gg t outer-sibling-grandchild! h m grand-sibling! head bigram! h! h h g h m inner-sibling-grandchild! h s s m gc Figure 3: First- to third-order features. Experimental Setup 5 Dataset and Evaluation Measures We evaluate our model on CoNLL dependency treebanks for 14 different languages (Buchholz and Marsi, 2006; Surdeanu et al., 2008), using standard training and testing splits. We use part-of-speech tags and the morphological information provided in the corpus. Following standard practice, we use Unlabeled Attachment Score (UAS) excluding punctuation (Koo et al., 2010; Martins et al., 2013) as the evaluation metric in all our experiments. Baselines We compare our model with the TurboParser (Martins et al., 2013) and our earlier sampling-based parser (Zhang et al., 2014). For both parsers, we directly compare with the recent published results on the CoNLL datasets. We also compare our parser against"
D14-1109,P05-1022,0,0.0709867,"the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head of the modifier, i.e. y(m). The dependency trees are scored according to S(x, y) = θ · φ(x, y), where θ is a vector of parameters and φ(x, y) is a sparse feature vector representation of tree y for sentence x. In this work, φ(x, y) will include up to third-order features as well as a range of global features commonly used in re-ranking methods (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). The parameters θ in the scoring function are estimated on the basis of a training set D = {(ˆ xi , yˆi )}N ˆi and the correspondi=1 of sentences x ing gold (target) trees yˆi . We adopt a max-margin framework for this learning problem. Specifically, we aim to find parameter values that score the gold target trees higher than others: ∀i ∈ {1, · · · , N }, y ∈ T (ˆ xi ), S(ˆ xi , yˆi ) ≥ S(ˆ xi , y) + kˆ yi − yk1 − ξi where ξi ≥ 0 is the slack variable (non-zero values are penalized against) and kˆ yi − yk1 is the hamming distance between the gold tree yˆi and a candidate parse y"
D14-1109,W02-1001,0,0.0893945,"t, span length, neighbors, valency and non-projective arcs features. Implementation Details Following standard practices, we train our model using the passiveaggressive online learning algorithm (MIRA) and parameter averaging (Crammer et al., 2006; 4 We refer the readers to Zhang et al. (2014) and Lei et al. (2014) for the detailed definition of each feature template. 4 Len ≤ 15 Len &gt; 15 3 2 1 0 Arabic Slovene English Chinese German −1 −2 Figure 4: Absolute UAS improvement of our full model over the first-order model. Sentences in the test set are divided into 2 groups based on their lengths. Collins, 2002). By default we use an adaptive strategy for running the hill-climbing algorithm – for a given sentence we repeatedly run the algorithm in parallel5 until the best tree does not change for K = 300 consecutive restarts. For each restart, by default we initialize the tree y (0) by sampling from the first-order distribution using the current learned parameter values (and firstorder scores). We train our first-order and thirdorder model for 10 epochs and our full model for 20 epochs for all languages, and report the average performance across three independent runs. 6 Results Comparison with the B"
D14-1109,D11-1103,0,0.017221,"rithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 3.1 Method In an online learning setup, parameters are updated successively after each sentence. Each update still requires us to find the “strongest violation”, i.e., a candidate tree y˜ that scores higher than the gold tree yˆi : y˜ = arg max {S(ˆ xi , y) + ky − yˆi k1 } y∈T (ˆ xi ) The parameters are then revised so as to select against the offending y˜. Instead of a standard parameter update based on y˜ as in perceptron, stochastic gradient descent, or passive-aggressive updates, our im"
D14-1109,N10-1115,0,0.0185806,"ient randomization of the starting point is critical. Only a small number of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate tha"
D14-1109,P10-1110,0,0.0391499,"Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hillclimbing for parameter updates achieves similar or hi"
D14-1109,N12-1015,0,0.0217513,"Missing"
D14-1109,P08-1067,0,0.052023,"ifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head of the modifier, i.e. y(m). The dependency trees are scored according to S(x, y) = θ · φ(x, y), where θ is a vector of parameters and φ(x, y) is a sparse feature vector representation of tree y for sentence x. In this work, φ(x, y) will include up to third-order features as well as a range of global features commonly used in re-ranking methods (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008). The parameters θ in the scoring function are estimated on the basis of a training set D = {(ˆ xi , yˆi )}N ˆi and the correspondi=1 of sentences x ing gold (target) trees yˆi . We adopt a max-margin framework for this learning problem. Specifically, we aim to find parameter values that score the gold target trees higher than others: ∀i ∈ {1, · · · , N }, y ∈ T (ˆ xi ), S(ˆ xi , yˆi ) ≥ S(ˆ xi , y) + kˆ yi − yk1 − ξi where ξi ≥ 0 is the slack variable (non-zero values are penalized against) and kˆ yi − yk1 is the hamming distance between the gold tree yˆi and a candidate parse y. Algorithm Du"
D14-1109,D07-1123,0,0.0249764,"ritical. Only a small number of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that star"
D14-1109,D10-1125,1,0.926832,"roblems ranging from set cover (Hochbaum, 1982) to MAX-SAT (Resende et al., 1997). Their success is predicated on the observation that most realizations of problems are much easier to solve than the worst-cases. A simpler algorithm will therefore suffice in typical cases. Evidence is accumulating that parsing problems may exhibit similar properties. For instance, methods such as dual decomposition offer certificates of optimality when the highest scoring tree is found. Across languages, dual decomposition has shown to lead to a certificate of optimality for the vast majority of the sentences (Koo et al., 2010; Martins et al., 2011). These remarkable results suggest that, as a combinatorial problem, parsing appears simpler than its broader complexity class would suggest. Indeed, we show that a simpler inference algorithm already suffices for superior results. In this paper, we introduce a randomized greedy algorithm that can be easily used with any rich scoring function. Starting with an initial tree drawn uniformly at random, the algorithm makes only local myopic changes to the parse tree in an attempt to climb the objective function. While a single run of the hill-climbing algorithm may indeed ge"
D14-1109,P14-1130,1,0.540625,"tudy the properties of these approximations in the context of dependency parsing. 3 3.1 Method In an online learning setup, parameters are updated successively after each sentence. Each update still requires us to find the “strongest violation”, i.e., a candidate tree y˜ that scores higher than the gold tree yˆi : y˜ = arg max {S(ˆ xi , y) + ky − yˆi k1 } y∈T (ˆ xi ) The parameters are then revised so as to select against the offending y˜. Instead of a standard parameter update based on y˜ as in perceptron, stochastic gradient descent, or passive-aggressive updates, our implementation follows Lei et al. (2014) where the first-order parameters are broken up into a tensor. Each tensor component is updated successively in combination with the parameters corresponding to MST features (McDonald et al., 2005) and higher-order features (when included).2 3.2 Preliminaries Let x be a sentence and T (x) be the set of possible dependency trees over the words in x. We use y ∈ T (x) to denote a dependency tree for x, and y(m) to specify the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of"
D14-1109,D11-1022,0,0.0585537,"Missing"
D14-1109,P13-2109,0,0.108186,"Missing"
D14-1109,E06-1011,0,0.303532,"algorithm, the greedy method surpasses dual decomposition in second-order parsing; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets.1 1 Introduction Dependency parsing is typically guided by parameterized scoring functions that involve rich features exerting refined control over the choice of parse trees. As a consequence, finding the highest scoring parse tree is a provably hard combinatorial inference problem (McDonald and Pereira, 2006). Much of the recent work on parsing has focused on solving these problems using powerful optimization techniques. In this paper, we follow a different strategy, arguing that a much simpler inference strategy suffices. In fact, we demonstrate that a randomized greedy method of inference surpasses the state-of-the-art performance in dependency parsing. ∗ Both authors contributed equally. Our code is available at https://github.com/ taolei87/RBGParser. 1 Our choice of a randomized greedy algorithm for parsing follows from a successful track record of such methods in other hard combinatorial prob"
D14-1109,P05-1012,0,0.22595,"still requires us to find the “strongest violation”, i.e., a candidate tree y˜ that scores higher than the gold tree yˆi : y˜ = arg max {S(ˆ xi , y) + ky − yˆi k1 } y∈T (ˆ xi ) The parameters are then revised so as to select against the offending y˜. Instead of a standard parameter update based on y˜ as in perceptron, stochastic gradient descent, or passive-aggressive updates, our implementation follows Lei et al. (2014) where the first-order parameters are broken up into a tensor. Each tensor component is updated successively in combination with the parameters corresponding to MST features (McDonald et al., 2005) and higher-order features (when included).2 3.2 Preliminaries Let x be a sentence and T (x) be the set of possible dependency trees over the words in x. We use y ∈ T (x) to denote a dependency tree for x, and y(m) to specify the head (parent) of the modifier word indexed by m in tree y. We also use m to denote the indexed word when there is no ambiguity. In addition, we define T (y, m) as the set of “neighboring trees” of y obtained by changing only the head of the modifier, i.e. y(m). The dependency trees are scored according to S(x, y) = θ · φ(x, y), where θ is a vector of parameters and φ("
D14-1109,C08-1074,0,0.0133099,"implicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 3.1 Method In an online learning setup, parameters are updated successively after each sentence. Each update still requires us to find the “strongest violation”, i.e., a candidate tree y˜ that scores higher than the gold tree yˆi : y˜ = arg max {S(ˆ xi , y) + ky − yˆi k1 } y∈T (ˆ xi ) The parameters are then revised so as to select against the offending y˜. Instead of a standard parameter update based on y˜ as in perceptron, stochastic gradient descent, or passive-aggr"
D14-1109,D07-1100,0,0.110111,"parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hillclimbing for parameter updates achieves similar or higher performance. Another related greedy inference method has been used for non-projective dependency parsing (McDonald and Pereira, 2006). This method"
D14-1109,J10-3001,0,0.0113038,"arkably well in practice. Despite the apparent simplicity of these algorithms, understanding their properties is challenging: often their “theoretical analyses are negative and inconclusive” (Amenta and Ziegler, 1999; Spielman and Teng, 2001). Identifying conditions under which approximations are provably optimal is an active area of research in computer science theory (Dumitrescu and T´oth, 2013; Jonsson et al., 2013). In NLP, randomized and greedy approximations have been successfully used across multiple applications, including machine translation and language modeling (Brown et al., 1993; Ravi and Knight, 2010; Daum´e III et al., 2009; Moore and Quirk, 2008; Deoras et al., 2011). In this paper, we study the properties of these approximations in the context of dependency parsing. 3 3.1 Method In an online learning setup, parameters are updated successively after each sentence. Each update still requires us to find the “strongest violation”, i.e., a candidate tree y˜ that scores higher than the gold tree yˆi : y˜ = arg max {S(ˆ xi , y) + ky − yˆi k1 } y∈T (ˆ xi ) The parameters are then revised so as to select against the offending y˜. Instead of a standard parameter update based on y˜ as in perceptr"
D14-1109,W08-2121,0,0.147127,"Missing"
D14-1109,D08-1059,0,0.0186199,"of restarts suffices for finding (near) optimal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn fr"
D14-1109,D13-1093,0,0.292775,"timal parse trees. 2 Related Work Finding Optimal Structure in Parsing The use of rich-scoring functions in dependency parsing inevitably leads to the challenging combinatorial problem of finding the maximizing parse. In fact, McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hill"
D14-1109,P14-1019,1,0.937837,"McDonald and Pereira (2006) demonstrated that the task is provably NP-hard for non-projective second-order parsing. Not surprisingly, approximate inference has been at the center of parsing research. Examples of these approaches include easy-first parsing (Goldberg and Elhadad, 2010), inexact search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2012; Zhang et al., 2013), partial dynamic programming (Huang and Sagae, 2010) and dual decomposition (Koo et al., 2010; Martins et al., 2011). Our work is most closely related to the MCMC sampling-based approaches (Nakagawa, 2007; Zhang et al., 2014). In our earlier work, we developed a method that learns to take guided stochastic steps towards a high-scoring parse (Zhang et al., 2014). In the heart of that technique are sophisticated samplers for traversing the space of trees. In this paper, we demonstrate that a substantially simpler approach that starts from a tree drawn from the uniform distribution and uses hillclimbing for parameter updates achieves similar or higher performance. Another related greedy inference method has been used for non-projective dependency parsing (McDonald and Pereira, 2006). This method relies on hill-climbi"
D15-1001,Q13-1005,0,0.0828776,"Missing"
D15-1001,P10-1129,1,0.892929,"Missing"
D15-1001,P11-1028,1,0.808885,"t to the castle whereas moving south will take it to the standing archway. An alternative approach is to convert text descriptions to pre-specified representations using annotated training data, commonly used in ∗ Both authors contributed equally to this work. Code is available at http://people.csail.mit. edu/karthikn/mud-play. 2 http://mudstats.com/ 1 1 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1–11, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game rules by analyzing a collection of game-related documents and precompiled traces of the game. In contrast to the above work, our model combines text interp"
D15-1001,D14-1162,0,0.109764,"Missing"
D15-1001,D09-1100,0,0.205242,"Missing"
D15-1001,P15-1150,0,0.0174879,"Missing"
D15-1001,P14-1026,1,0.754131,"Missing"
D15-1001,P10-1083,0,0.143863,"Missing"
D15-1001,D15-1138,0,\N,Missing
D15-1180,D14-1082,0,0.0653448,"Missing"
D15-1180,P14-1129,0,0.0389463,"Missing"
D15-1180,D14-1002,0,0.0291679,"obert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (dynamic) k-max pooling operation for modeling sentences. In addition, Kim (2014) combines CNNs of different filter widths and either static or fine-tuned word vectors. In contrast to the traditional CNN models, our method considers non-consecutive ngrams thereby expanding the representation capacity of the model. Moreover, our model captures non-linear interactions within n-gram snippets through the use of tensors, moving beyond direct linear projection operator used in standard CNNs. As our experiments demonstrate these advancements result in impr"
D15-1180,P15-1162,0,0.0945143,"0.1 38.3 45.1 44.5 49.5 50.6 53.4 51.2 Binary Dev Test 82.4 85.4 86.8 88.0 86.9 88.1 85.7 86.2 87.8 86.8 78.6 81.3 80.7 82.0 87.0 87.0 88.9 88.6 Time (in seconds) per epoch per 10k samples 1657 1939 431 504 140 164 2452 156 32 37 73 5 1 1 28 33 445 28 Table 1: Comparison between our model and other baseline methods on Stanford Sentiment Treebank. The top block lists recursive neural network models, the second block are convolutional network models and the third block contains other baseline methods, including the paragraph-vector model (Le and Mikolov, 2014), the deep averaging network model (Iyyer et al., 2015) and our implementation of neural bag-of-words. The training time of baseline methods is taken from (Iyyer et al., 2015) or directly from the authors. For our implementations, timings were performed on a single core of a 2.6GHz Intel i7 processor. network baselines include standard RNN (Socher et al., 2011b), RNTN with a small core tensor in the composition function (Socher et al., 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al., 2015). These recursive models assume the input sentences are re"
D15-1180,D13-1176,0,0.00849827,"eep neural networks have recently brought about significant advancements in various natural language processing tasks, such as language modeling (Bengio et al., 2003; Mikolov et al., 2010), sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K¨uchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has b"
D15-1180,P14-1062,0,0.623277,"013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K¨uchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (dynamic) k-max pooling operation for modeling s"
D15-1180,C12-2054,0,0.0168089,"Missing"
D15-1180,D14-1181,0,0.280914,"). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (dynamic) k-max pooling operation for modeling sentences. In addition, Kim (2014) combines CNNs of different filter widths and either static or fine-tuned word vectors. In contrast to the traditional CNN models, our method considers non-consecutive ngrams thereby expanding the representation capacity of the model. Moreover, our model captures non-linear interactions within n-gram snippets through the use of tensors, moving beyond direct linear projection operator used in standard CNNs. As our experiments demonstrate these advancements result in improved performance. 3 Background Let x ∈ RL×d be the input sequence such as a document or sentence. Here L is the length of the"
D15-1180,S15-1002,0,0.0409149,"Missing"
D15-1180,P14-1130,1,0.699605,"able performance with increasing data and computation, yet permit easy architectural and operational variations so as to fine tune them to specific applications to reach top performance. Indeed, their success is often contingent on specific architectural and operational choices. 1 Our code and data are available at https://github. com/taolei87/text_convnet We propose to use a feature mapping operation based on tensor products instead of linear operations on stacked vectors. This enables us to directly tap into non-linear interactions between adjacent word feature vectors (Socher et al., 2013; Lei et al., 2014). To offset the accompanying parametric explosion we maintain a low-rank representation of the tensor parameters. Moreover, we show that this feature mapping can be applied to all possible non-consecutive n-grams in the sequence with an exponentially decaying weight depending on the length of the span. Owing to the low rank representation of the tensor, this operation can be performed efficiently in linear time with respect to the sequence length via dynamic programming. Similar to traditional convolution operations, our non-linear feature mapping can be applied successively at multiple levels"
D15-1180,P08-1028,0,0.0402335,"Missing"
D15-1180,D14-1162,0,0.0821077,"he phrase-level annotations in the Stanford Sentiment Treebank, all phrases and the corresponding labels are added as separate instances when training the sequence models. We follow this strategy and report results with and without phrase annotations. Word vectors The word vectors are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data (Turian et al., 2010). In particular, for the English sentiment classification task, we use the publicly available 300-dimensional GloVe word vectors trained on the Common Crawl with 840B tokens (Pennington et al., 2014). This choice of word vectors follows most recent work, such as DAN (Iyyer et al., 2015) and RLSTM (Tai et al., 2015). For Chinese news categorization, there is no widely-used publicly available word vectors. Therefore, we run word2vec (Mikolov et al., 2013) to train 200-dimensional word vectors on the 1.6 million Chinese news articles. Both word vectors are normalized to unit norm (i.e. kwk22 = 1) and are fixed in the experiments without fine-tuning. Hyperparameter setting We perform an extensive search on the hyperparameters of our full model, our implementation of the CNN model (with linear"
D15-1180,D11-1014,0,0.0253753,"e methods on Stanford Sentiment Treebank. The top block lists recursive neural network models, the second block are convolutional network models and the third block contains other baseline methods, including the paragraph-vector model (Le and Mikolov, 2014), the deep averaging network model (Iyyer et al., 2015) and our implementation of neural bag-of-words. The training time of baseline methods is taken from (Iyyer et al., 2015) or directly from the authors. For our implementations, timings were performed on a single core of a 2.6GHz Intel i7 processor. network baselines include standard RNN (Socher et al., 2011b), RNTN with a small core tensor in the composition function (Socher et al., 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al., 2015). These recursive models assume the input sentences are represented as parse trees. As a benefit, they can readily utilize annotations at the phrase level. In contrast, convolutional neural networks are trained on sequence-level, taking the original sequence and its label as training input. Such convolutional baselines include the dynamic CNN with k-max pooling D"
D15-1180,D13-1170,0,0.418089,"d with non-linearities and pooling. The convolution operation itself is a linear mapping over “n-gram vectors” obtained by concatenating consecutive word (or character) representations. We argue that this basic building block can be improved in two important respects. First, the power of n-grams derives precisely from multi-way interactions and these are clearly missed (initially) with linear operations on stacked n-gram vectors. Non-linear interactions within a local context have been shown to improve empirical performance in various tasks (Mitchell and Lapata, 2008; Kartsaklis et al., 2012; Socher et al., 2013). Second, many useful patterns are expressed as non-consecutive phrases, such as semantically close multi-word expressions (e.g.,“not that good”, “not nearly as good”). In typical CNNs, such expressions would have to come together and emerge as useful patterns after several layers of processing. The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations, we appeal to tensor algebra and use low-rank n-gram tensors t"
D15-1180,P15-1150,0,0.0778728,"e and Mikolov, 2014), the deep averaging network model (Iyyer et al., 2015) and our implementation of neural bag-of-words. The training time of baseline methods is taken from (Iyyer et al., 2015) or directly from the authors. For our implementations, timings were performed on a single core of a 2.6GHz Intel i7 processor. network baselines include standard RNN (Socher et al., 2011b), RNTN with a small core tensor in the composition function (Socher et al., 2013), the deep recursive model DRNN (Irsoy and Cardie, 2014) and the most recent recursive model using long-short-term-memory units RLSTM (Tai et al., 2015). These recursive models assume the input sentences are represented as parse trees. As a benefit, they can readily utilize annotations at the phrase level. In contrast, convolutional neural networks are trained on sequence-level, taking the original sequence and its label as training input. Such convolutional baselines include the dynamic CNN with k-max pooling DCNN (Kalchbrenner et al., 2014) and the convolutional model with multi-channel CNN-MC by Kim (2014). To leverage the phrase-level annotations in the Stanford Sentiment Treebank, all phrases and the corresponding labels are added as sep"
D15-1180,P10-1040,0,0.0864155,"input. Such convolutional baselines include the dynamic CNN with k-max pooling DCNN (Kalchbrenner et al., 2014) and the convolutional model with multi-channel CNN-MC by Kim (2014). To leverage the phrase-level annotations in the Stanford Sentiment Treebank, all phrases and the corresponding labels are added as separate instances when training the sequence models. We follow this strategy and report results with and without phrase annotations. Word vectors The word vectors are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data (Turian et al., 2010). In particular, for the English sentiment classification task, we use the publicly available 300-dimensional GloVe word vectors trained on the Common Crawl with 840B tokens (Pennington et al., 2014). This choice of word vectors follows most recent work, such as DAN (Iyyer et al., 2015) and RLSTM (Tai et al., 2015). For Chinese news categorization, there is no widely-used publicly available word vectors. Therefore, we run word2vec (Mikolov et al., 2013) to train 200-dimensional word vectors on the 1.6 million Chinese news articles. Both word vectors are normalized to unit norm (i.e. kwk22 = 1)"
D15-1180,P14-2105,0,0.0197006,"sentiment analysis (Socher et al., 2013; Iyyer et al., 2015; Le and Zuidema, 2015), syntactic parsing (Collobert and Weston, 2008; Socher et al., 2011a; Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014; Devlin et al., 2014; Sutskever et al., 2014). Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks (Mikolov et al., 2010; Kalchbrenner and Blunsom, 2013) to recursive models (Pollack, 1990; K¨uchler and Goller, 1996), and including convolutional neural nets (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014; Shen et al., 2014; Kalchbrenner et al., 2014; Zhang and LeCun, 2015). Our model most closely relates to the latter. Since these models have originally been developed for computer vision (LeCun et al., 1998), their application to NLP tasks introduced a number of modifications. For instance, Collobert et al. (2011) use the max-over-time pooling operation to aggregate the features over the input sequence. This variant has been successfully applied to semantic parsing (Yih et al., 2014) and information retrieval (Shen et al., 2014; Gao et al., 2014). Kalchbrenner et al. (2014) instead propose (d"
D15-1213,P10-1131,0,0.0898945,"Missing"
D15-1213,N09-1009,0,0.0424826,"Missing"
D15-1213,D11-1005,0,0.323127,"Missing"
D15-1213,W08-1301,0,0.0302964,"Missing"
D15-1213,K15-1012,0,0.178419,"Missing"
D15-1213,D12-1001,0,0.0821288,"Missing"
D15-1213,C96-1058,0,0.0316861,"er bound, we train the RBGParser (Lei et al., 2014), the state-of-the-art tensor-based parser, on the full target language training set. We train the first-order model7 with default parameter settings, using the current version of the code.8 Evaluation Measures Following standard practices, we report unlabeled attachment score (UAS) and labeled attachment score (LAS), excluding punctuation. For all experiments, we report results on the test set and omit the development results because of space. Experimental Details For all experiments, we use the arc-factored model and use Eisner’s algorithm (Eisner, 1996) to infer the projective Viterbi parse. We train our model and the baselines for 10 epochs. We set a strong regularization C = 0.001 during learning because cross-lingual transfer contains noise and the models can easily overfit. Other hyper-parameters are set as γ = 0.3 and r = 200 (rank of the tensor). For partial lexicalization, we set the embedding dimension to 50. 7 Results Table 5 and 7 summarize the results for the unsupervised and the semi-supervised scenarios. Averaged across languages, our model outperforms all 6 We use this as a re-implementation of T¨ackstr¨om et al. (2013)’s model"
D15-1213,E14-1049,0,0.00805458,"s. (c) φl : dependency labels. (d) φd : dependency length conjoined with direction. (e) φtu , φtl : selectively shared typological features, as described in Table 4. We further conjoin atomic features (b) and (d) with the family and the typological class of the language, because the arc direction and the word order distribution depends on the typological property of languages (T¨ackstr¨om et al., 2013). We also add a bias term into each feature vector. Partial Lexicalization We utilize multilingual word embeddings to incorporate partial lexical information in our model. We use the CCA method (Faruqui and Dyer, 2014) to generate multilingual word embeddings. Specifically, we project word vectors in each non-English language to the English embedding space. To reduce the noise from the automatic projection process, we only incorporate lexical information for the top100 most frequent words in the following closed classes: pronoun, determiner, adposition, conjunction, particle and punctuation mark. Therefore, we call this feature extension partial lexicalization.5 We follow previous work (Lei et al., 2014) for adding embedding features. For the linear scoring model, we simply append the head and the modifier"
D15-1213,P15-2120,0,0.0189029,"eraction between these features, yielding a richer representation for crosslingual transfer. Moreover, our model handles labeled dependency parsing while previous work only focused on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 hree-way tensor H cφ hc φ d , φl Lφl φ m , φ mc hc φhc M cφ mc e4 Dφ d H φh + Tlφtl e2 Lφl M φm = e1 + Tuφtu H φ h M φ m Dφ d Figure 1: Visual representation for traditional mulal., 2013; Yu et al., 2013a). While these methods φh φ m φ dcombine atomic features into can automatically a compact composite representation, they ca"
D15-1213,N15-1121,1,0.834357,"Missing"
D15-1213,P05-1012,0,0.0154162,"the target language and the trigram distribution of the training data after adding this sentence. The training data includes both the target and the source languages. The trigrams are based on universal POS tags. Note that our method does not require any dependency annotations. To incorporate the new supervision, we simply add the new sentences into the original training set, weighing their impact by a factor of 10. Baselines We compare against different variants of our model. • Direct: a direct transfer baseline (McDonald et al., 2011) that uses only delexicalized features in the MSTParser (McDonald et al., 2005). • NT-Select: our model without the tensor component. This baseline corresponds to the prior feature-based transfer method (T¨ackstr¨om et al., 2013) with extensions to labeled parsing, lexicalization and semi-supervised parsing.6 • Multiway: tensor-based model where typological features are added as an additional component and parameters are factorized in the multiway structure similarly as in Figure 1. • Sup50: our model trained only on the 50 sentences in the target language in the semisupervised scenario. In all the experiments we incorporate partial lexicalization for all variants of our"
D15-1213,D11-1006,0,0.570034,"e sentence that minimizes the KL divergence between the trigram distribution of the target language and the trigram distribution of the training data after adding this sentence. The training data includes both the target and the source languages. The trigrams are based on universal POS tags. Note that our method does not require any dependency annotations. To incorporate the new supervision, we simply add the new sentences into the original training set, weighing their impact by a factor of 10. Baselines We compare against different variants of our model. • Direct: a direct transfer baseline (McDonald et al., 2011) that uses only delexicalized features in the MSTParser (McDonald et al., 2005). • NT-Select: our model without the tensor component. This baseline corresponds to the prior feature-based transfer method (T¨ackstr¨om et al., 2013) with extensions to labeled parsing, lexicalization and semi-supervised parsing.6 • Multiway: tensor-based model where typological features are added as an additional component and parameters are factorized in the multiway structure similarly as in Figure 1. • Sup50: our model trained only on the 50 sentences in the target language in the semisupervised scenario. In al"
D15-1213,P13-2017,0,0.0391811,"Missing"
D15-1213,D10-1120,1,0.858152,"Missing"
D15-1213,P12-1066,1,0.784071,"lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multisource parsing transfer (Hwa et al., 2005; Durrett et al., 2012; Zeman and Resnik, 2008; Yu et al., 2013b; Cohen et al., 2011; Rasooli and Collins, 2015). Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic structure (Naseem et al., 2010; T¨ackstr¨om et al., 2013; Berg-Kirkpatrick and Klein, 2010; Cohen and Smith, 2009; Duong et al., 2015). Our work is closely related to the selectivesharing approaches (Naseem et al., 2012; T¨ackstr¨om et al., 2013). The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on typological features. While this selective sharing idea was first realized in the generative model (Naseem et al., 2012), higher performance was achieved in a discriminative arc-factored model (T¨ackstr¨om et al., 2013). These gains were obtained by a careful construction of features templates that combine standard dependency parsing features and typological features. In co"
D15-1213,petrov-etal-2012-universal,0,0.15634,"Missing"
D15-1213,P15-1013,0,0.013472,"on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 hree-way tensor H cφ hc φ d , φl Lφl φ m , φ mc hc φhc M cφ mc e4 Dφ d H φh + Tlφtl e2 Lφl M φm = e1 + Tuφtu H φ h M φ m Dφ d Figure 1: Visual representation for traditional mulal., 2013; Yu et al., 2013a). While these methods φh φ m φ dcombine atomic features into can automatically a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural co"
D15-1213,P14-1130,1,0.837316,"c-factored model (T¨ackstr¨om et al., 2013). These gains were obtained by a careful construction of features templates that combine standard dependency parsing features and typological features. In contrast, we propose an automated, tensor-based approach that can effectively capture the interaction between these features, yielding a richer representation for crosslingual transfer. Moreover, our model handles labeled dependency parsing while previous work only focused on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 hree-way tensor H cφ hc φ d , φl L"
D15-1213,D15-1039,0,0.390265,"Missing"
D15-1213,W15-1519,0,0.0285373,"-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 hree-way tensor H cφ hc φ d , φl Lφl φ m , φ mc hc φhc M cφ mc e4 Dφ d H φh + Tlφtl e2 Lφl M φm = e1 + Tuφtu H φ h M φ m Dφ d Figure 1: Visual representation for traditional mulal., 2013; Yu et al., 2013a). While these methods φh φ m φ dcombine atomic features into can automatically a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural constraints based on prior knowledge. As our e"
D15-1213,N13-1126,0,0.57202,"Missing"
D15-1213,Q15-1017,0,0.0142068,"tively capture the interaction between these features, yielding a richer representation for crosslingual transfer. Moreover, our model handles labeled dependency parsing while previous work only focused on the unlabeled dependency parsing task. Tensor-based Models Our approach also relates to prior work on tensor-based modeling. Lei et al. (2014) employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. Srikumar and Manning (2014) learn a multi-class label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 hree-way tensor H cφ hc φ d , φl Lφl φ m , φ mc hc φhc M cφ mc e4 Dφ d H φh + Tlφtl e2 Lφl M φm = e1 + Tuφtu H φ h M φ m Dφ d Figure 1: Visual representation for traditional mulal., 2013; Yu et al., 2013a). While these methods φh φ m φ dcombine atomic features into can automatically a compact composite re"
D15-1213,P13-2056,0,0.0170313,"ass label embedding tailored for document classification and POS tagging in the tensor framework. Yu and Dredze (2015), Fried et al. (2015) apply low-rank tensor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization (Primadhanty et al., 2015; Quattoni et al., 2014; Singh et al., 2015) and neural tensor networks (Socher et 1858 hree-way tensor H cφ hc φ d , φl Lφl φ m , φ mc hc φhc M cφ mc e4 Dφ d H φh + Tlφtl e2 Lφl M φm = e1 + Tuφtu H φ h M φ m Dφ d Figure 1: Visual representation for traditional mulal., 2013; Yu et al., 2013a). While these methods φh φ m φ dcombine atomic features into can automatically a compact composite representation, they cannot take into account constraints on feature combination. In contrast, our method can capture features at different composition levels, and more generally can incorporate structural constraints based on prior knowledge. As our experiments show, this approach delivers higher transfer accuracy. 3.1 e3 H cφ hc M cφ mc φ mc tiway tensor. + φtl = e3 φl + φtu = e2 3 = Figure 2: Visual representation for hierarchical tensor, represented as a tree structure. The tensor first cap"
D15-1213,I08-3008,0,0.593846,"Missing"
D16-1011,D16-1053,0,0.0130716,"Missing"
D16-1011,P15-2114,0,0.01452,"Missing"
D16-1011,N15-1184,0,0.0177223,"Missing"
D16-1011,P15-1144,0,0.0457018,"Missing"
D16-1011,D15-1003,0,0.00815356,"btained by the bigram SVM and a neural attention baseline. 2 Related Work Developing sparse interpretable models is of considerable interest to the broader research community(Letham et al., 2015; Kim et al., 2015). The need for interpretability is even more pronounced with recent neural models. Efforts in this area include analyzing and visualizing state activation (Hermans 108 and Schrauwen, 2013; Karpathy et al., 2015; Li et al., 2016), learning sparse interpretable word vectors (Faruqui et al., 2015b), and linking word vectors to semantic lexicons or word properties (Faruqui et al., 2015a; Herbelot and Vecchi, 2015). Beyond learning to understand or further constrain the network to be directly interpretable, one can estimate interpretable proxies that approximate the network. Examples include extracting “if-then” rules (Thrun, 1995) and decision trees (Craven and Shavlik, 1996) from trained networks. More recently, Ribeiro et al. (2016) propose a modelagnostic framework where the proxy model is learned only for the target sample (and its neighborhood) thus ensuring locally valid approximations. Our work differs from these both in terms of what is meant by an explanation and how they are derived. In our c"
D16-1011,P15-1162,0,0.0120911,"Missing"
D16-1011,P14-1062,0,0.019801,"Missing"
D16-1011,D14-1181,0,0.00746687,"Missing"
D16-1011,D15-1180,1,0.706773,"Missing"
D16-1011,N16-1153,1,0.0983889,"Missing"
D16-1011,N16-1082,0,0.07137,"Missing"
D16-1011,N16-3020,0,0.709646,"ng and visualizing state activation (Hermans 108 and Schrauwen, 2013; Karpathy et al., 2015; Li et al., 2016), learning sparse interpretable word vectors (Faruqui et al., 2015b), and linking word vectors to semantic lexicons or word properties (Faruqui et al., 2015a; Herbelot and Vecchi, 2015). Beyond learning to understand or further constrain the network to be directly interpretable, one can estimate interpretable proxies that approximate the network. Examples include extracting “if-then” rules (Thrun, 1995) and decision trees (Craven and Shavlik, 1996) from trained networks. More recently, Ribeiro et al. (2016) propose a modelagnostic framework where the proxy model is learned only for the target sample (and its neighborhood) thus ensuring locally valid approximations. Our work differs from these both in terms of what is meant by an explanation and how they are derived. In our case, an explanation consists of a concise yet sufficient portion of the text where the mechanism of selection is learned jointly with the predictor. Attention based models offer another means to explicate the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al.,"
D16-1011,D15-1044,0,0.0442638,"in terms of what is meant by an explanation and how they are derived. In our case, an explanation consists of a concise yet sufficient portion of the text where the mechanism of selection is learned jointly with the predictor. Attention based models offer another means to explicate the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015). Such models have been successfully applied to many NLP problems, improving both prediction accuracy as well as visualization and interpretability (Rush et al., 2015; Rockt¨aschel et al., 2016; Hermann et al., 2015). Xu et al. (2015) introduced a stochastic attention mechanism together with a more standard soft attention on image captioning task. Our rationale extraction can be understood as a type of stochastic attention although architectures and objectives differ. Moreover, we compartmentalize rationale generation from downstream encoding so as to expose knobs to directly control types of rationales that are acceptable, and to facilitate broader modular use in other applications. Finally, we contrast our work with rationale-based classification (Zaidan"
D16-1011,N07-1033,0,0.589427,", 2015; Rockt¨aschel et al., 2016; Hermann et al., 2015). Xu et al. (2015) introduced a stochastic attention mechanism together with a more standard soft attention on image captioning task. Our rationale extraction can be understood as a type of stochastic attention although architectures and objectives differ. Moreover, we compartmentalize rationale generation from downstream encoding so as to expose knobs to directly control types of rationales that are acceptable, and to facilitate broader modular use in other applications. Finally, we contrast our work with rationale-based classification (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016) which seek to improve prediction by relying on richer annotations in the form of human-provided rationales. In our work, rationales are never given during training. The goal is to learn to generate them. 3 Extractive Rationale Generation We formalize here the task of extractive rationale generation and illustrate it in the context of neural models. To this end, consider a typical NLP task where we are provided with a sequence of words as input, namely x = {x1 , · · · , xl }, where each xt ∈ Rd denotes the vector representation of the ith word. The l"
D16-1011,D16-1076,0,0.0337676,"t al., 2015). Xu et al. (2015) introduced a stochastic attention mechanism together with a more standard soft attention on image captioning task. Our rationale extraction can be understood as a type of stochastic attention although architectures and objectives differ. Moreover, we compartmentalize rationale generation from downstream encoding so as to expose knobs to directly control types of rationales that are acceptable, and to facilitate broader modular use in other applications. Finally, we contrast our work with rationale-based classification (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016) which seek to improve prediction by relying on richer annotations in the form of human-provided rationales. In our work, rationales are never given during training. The goal is to learn to generate them. 3 Extractive Rationale Generation We formalize here the task of extractive rationale generation and illustrate it in the context of neural models. To this end, consider a typical NLP task where we are provided with a sequence of words as input, namely x = {x1 , · · · , xl }, where each xt ∈ Rd denotes the vector representation of the ith word. The learning problem is to map the input sequence"
D16-1197,P14-1129,0,0.08233,"Missing"
D16-1197,N13-1103,1,0.571007,"Missing"
D16-1197,D15-1166,0,0.227048,"Missing"
D16-1197,W98-1308,0,0.687851,"Missing"
D16-1197,P15-1129,0,\N,Missing
D16-1227,N16-1181,0,0.0272026,"retrieving similar Stack Exchange AskUbuntu questions. 1 1 Introduction Modern recommender problems involve complex objects, often described in textual form. In order to learn to predict how disparate objects may go together, it is helpful to first map them into a common representation where they are easily compared, regardless of their origin. Neural models are particularly well-suited for this task as continuous vector representations of objects can be tailored in a flexible way to the desired task. While these models have been shown to be effective across NLP tasks (Sutskever et al., 2014; Andreas et al., 2016; Hermann et al., 2015), they take considerable time to learn and are therefore ill-suited to be adjusted rapidly as additional evidence accumulates. 1 The code/data is available at https://github.com/ youyanggu/rcnn. We cast our text-to-text recommendation problem in two phases. In the first phase, flexible neural textto-vector mappings are learned from currently available data. Such mappings are optimized to function well in a collaborative filtering setting. For example, in the context of recommending food product categories for ingredients based on their Wikipedia pages, the continuous vec"
D16-1227,P15-2114,0,0.0292218,"ovided in terms of natural language descriptions via their associated Wikipedia pages. For example, if given “tomato”, we would predict “canned foods” as one likely category for the ingredient. A small number of categories appear as targets for each ingredient. We also consider the task of predicting questions that are similar to the one provided as a query. The purpose is to facilitate effective question answering by retrieving related past questions (and the associated answers that are available). For this we use Stack Exchange’s AskUbuntu question retrieval dataset used in recent work (dos Santos et al., 2015; Lei et al., 2016) 4 Approach We explain our approach in terms of the first task: predicting product categories from ingredients. Collaborative predictions are made by mapping each ingredient into a vector representation and comparing that representation with an analogous one for product categories. We train these vectors in an end-toend manner to function well as part of the collaborative task. The vector representations are based 2104 on Wikipedia pages that are available for most ingredients and categories in our problem. Rather than derive the vector from the entire article (which can be"
D16-1227,D15-1180,1,0.887657,"Missing"
D16-1227,N16-1153,1,0.883613,"n, and therefore also used for predicting associations. In the second phase, we no longer adjust text-tovector mappings but rather parameterize and learn how the vectors are compared. For example, we can optimize the metric separately for each new ingredient based on a few category observations for that ingredient. The goal of this second phase is to specifically boost the accuracy when the neural baseline (unaware of the new evidence) would otherwise not perform well. Our approach builds on the recent work on recurrent convolutional models to obtain text-to-vector mappings (Lei et al., 2015; Lei et al., 2016). This architecture is particularly well suited for noisy Wikipedia pages as it can learn to omit and highlight different parts of the text, as needed. The additional sequential component is a regularized logistic regression model (for ingredient-product prediction) or a ranking model (for question retrieval). We demonstrate the accuracy of the baseline neural recommender and the gains from the second sequential phase in both of these tasks. 2103 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2103–2108, c Austin, Texas, November 1-5, 2016. 2016 As"
D16-1227,D14-1162,0,0.0843045,"Missing"
D16-1227,D13-1170,0,0.00287704,"the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2103–2108, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2 Related Work A great deal of recent effort has gone into developing flexible neural models for text and their use across variety of NLP tasks. This includes building vector representations for sentences and documents (Le and Mikolov, 2014), convolutional neural network models of text (Collobert and Weston, 2008; Zhang and LeCun, 2015), non-consecutive variants of CNNs (Lei et al., 2015), and compositional architectures (Socher et al., 2013), among many others. Our work is most closely related to the use of such models for question retrieval (Lei et al., 2016) but differs, in particular, in terms of our two-phase collaborative filtering formulation and the ingredient mapping task from Wikipedia pages (cf.(Sutskever et al., 2011; Song and Roth, 2015)). 3 Recommender Problems We explore two recommender problems in this work. In the first problem, we are given a food ingredient, and our goal is to predict which product categories it could appear in. Both ingredients and product categories are provided in terms of natural language de"
D16-1227,N15-1138,0,0.0128892,"sks. This includes building vector representations for sentences and documents (Le and Mikolov, 2014), convolutional neural network models of text (Collobert and Weston, 2008; Zhang and LeCun, 2015), non-consecutive variants of CNNs (Lei et al., 2015), and compositional architectures (Socher et al., 2013), among many others. Our work is most closely related to the use of such models for question retrieval (Lei et al., 2016) but differs, in particular, in terms of our two-phase collaborative filtering formulation and the ingredient mapping task from Wikipedia pages (cf.(Sutskever et al., 2011; Song and Roth, 2015)). 3 Recommender Problems We explore two recommender problems in this work. In the first problem, we are given a food ingredient, and our goal is to predict which product categories it could appear in. Both ingredients and product categories are provided in terms of natural language descriptions via their associated Wikipedia pages. For example, if given “tomato”, we would predict “canned foods” as one likely category for the ingredient. A small number of categories appear as targets for each ingredient. We also consider the task of predicting questions that are similar to the one provided as"
D16-1261,J14-2004,0,0.039626,"Open Information Extraction Existing work in open IE has used external sources from the web to improve extraction accuracy and coverage (Agichtein and Gravano, 2000; Etzioni et al., 2011; Fader et al., 2011; Wu and Weld, 2010). Such research has focused on identifying multiple instances of the same relation, independent of the context in which this information appears. In contrast, our goal is to extract information from additional sources about a specific event described in a source article. Therefore, the novel challenge of our task resides in performing event coreference (Lee et al., 2012; Bejan and Harabagiu, 2014) (i.e identifying other sources describing the same event) while simultaneously reconciling extracted information. Moreover, relations typically considered by open IE systems have significantly higher coverage in online documents than a specific incident described in a few news sources. Hence, we require a different mechanism for finding and reconciling online information. Entity linking, multi-document extraction and event coreference Our work also relates to the task of multi-document information extraction, where the goal is to connect different mentions of the same entity across input docu"
D16-1261,N04-4028,0,0.0590687,"Ng, 2002). The classifier is used to tag each word in a document as one of the entity types or not (e.g. {ShooterName, NumKilled, NumWounded, City, Other} in the Shootings domain). Then, for each tag except Other, we choose the mode of the values to obtain the set of entity extractions from the article.12 Features used in the classifier are provided in the Supplementary material. The features and context window c = 4 of neighboring words are tuned to maximize performance on a dev set. We also experimented with a conditional random field (CRF) (with the same features) for the sequence tagging (Culotta and McCallum, 2004) 10 www.foodshield.org/member/login/ www.bing.com/toolbox/bingsearchapi 12 We normalize numerical words (e.g. ""one"" to ""1"") before taking the mode. 11 but obtained worse empirical performance (see Section 6). The parameters of the base extraction model are not changed during training of the RL model. Evaluation We evaluate the extracted entity values against the gold annotations and report the corpus-level average accuracy on each entity type. For entities like ShooterName, the annotations (and the news articles) often contain multiple names (first and last) in various combinations, so we cons"
D16-1261,Q14-1037,0,0.0122983,"e event) while simultaneously reconciling extracted information. Moreover, relations typically considered by open IE systems have significantly higher coverage in online documents than a specific incident described in a few news sources. Hence, we require a different mechanism for finding and reconciling online information. Entity linking, multi-document extraction and event coreference Our work also relates to the task of multi-document information extraction, where the goal is to connect different mentions of the same entity across input documents (Mann and Yarowsky, 2005; Han et al., 2011; Durrett and Klein, 2014). Since this setup already includes multiple input documents, the model is not required to look for additional sources or decide on their relevance. Also, while the set of input documents overlap in terms of entities mentioned, they do not necessarily describe the same event. Given these differences in setup, the challenges and opportunities of the two tasks are distinct. Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardn"
D16-1261,D11-1142,0,0.0208528,"Missing"
D16-1261,D14-1044,0,0.0144189,"2014). Since this setup already includes multiple input documents, the model is not required to look for additional sources or decide on their relevance. Also, while the set of input documents overlap in terms of entities mentioned, they do not necessarily describe the same event. Given these differences in setup, the challenges and opportunities of the two tasks are distinct. Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015). Though our work also aims at increasing extraction recall for a database, traditional KBC approaches do not require searching for additional sources of information. West et al. (2014) explore query reformulation in the context of KBC. Using existing search logs, they learn how to formulate effective queries for different types of database entries. Once query learning is completed, the model employs several selected queries, and then aggregates the results based on retrieval ranking. This approach is complementary to the proposed method, and can be"
D16-1261,D15-1038,0,0.0220538,"Missing"
D16-1261,P11-1115,0,0.0185811,"aseline IE system achieves high performance on the relations of interest. Hence, given different design goals, the two RL formulations are very different. Our approach is also close 2357 in spirit to the AskMSR system (Banko et al., 2002) which aims at using information redundancy on the web to better answer questions. Though our goal is similar, we learn to query and consolidate the different sources of information instead of using predefined rules. Several slot-filling methods have experimented with query formulation over web-based corpora to populate knowledge bases (Surdeanu et al., 2010; Ji and Grishman, 2011). 3 Framework We model the information extraction task as a markov decision process (MDP), where the model learns to utilize external sources to improve upon extractions from a source article (see Figure 3). The MDP framework allows us to dynamically incorporate entity predictions while also providing flexibility to choose the type of articles to extract from. At each step, the system has to reconcile extracted values from a related article (enew ) with the current set of values (ecur ), and decide on the next query for retrieving more articles. We represent the MDP as a tuple hS, A, T, Ri, wh"
D16-1261,D12-1045,0,0.0546697,"Missing"
D16-1261,P05-1060,0,0.102811,"dentifying other sources describing the same event) while simultaneously reconciling extracted information. Moreover, relations typically considered by open IE systems have significantly higher coverage in online documents than a specific incident described in a few news sources. Hence, we require a different mechanism for finding and reconciling online information. Entity linking, multi-document extraction and event coreference Our work also relates to the task of multi-document information extraction, where the goal is to connect different mentions of the same entity across input documents (Mann and Yarowsky, 2005; Han et al., 2011; Durrett and Klein, 2014). Since this setup already includes multiple input documents, the model is not required to look for additional sources or decide on their relevance. Also, while the set of input documents overlap in terms of entities mentioned, they do not necessarily describe the same event. Given these differences in setup, the challenges and opportunities of the two tasks are distinct. Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (S"
D16-1261,D15-1001,1,0.370071,"recursive Bellman equation (Sutton and Barto, 1998) for the optimal Q: Qi+1 (s, a) = E[r + γ max Qi (s0 , a0 ) |s, a] 0 a Here, r = R(s, a) is the reward and γ is a factor discounting the value of future rewards and the expectation is taken over all transitions involving state s and action a. Since our problem involves a continuous state space S, we use a deep Q-network (DQN) (Mnih et al., 2015) as a function approximator Q(s, a) ≈ Q(s, a; θ). The DQN, in which the Q-function is approximated using a deep neural network, has been shown to learn better value functions than linear approximators (Narasimhan et al., 2015; He et al., 2015) and can capture non-linear interactions between the different pieces of information in our state. We use a DQN consisting of two linear layers (20 hidden units each) followed by rectified linear units (ReLU), along with two separate output layers.7 The network takes the continuous state vector s as input and predicts Q(s, d) and Q(s, q) for reconciliation decisions d and query choices q simultaneously using the different output layers (see Supplementary material for the model architecture). Parameter Learning The parameters θ of the DQN are learnt using stochastic gradient d"
D16-1261,P15-1016,0,0.0182381,"p already includes multiple input documents, the model is not required to look for additional sources or decide on their relevance. Also, while the set of input documents overlap in terms of entities mentioned, they do not necessarily describe the same event. Given these differences in setup, the challenges and opportunities of the two tasks are distinct. Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015). Though our work also aims at increasing extraction recall for a database, traditional KBC approaches do not require searching for additional sources of information. West et al. (2014) explore query reformulation in the context of KBC. Using existing search logs, they learn how to formulate effective queries for different types of database entries. Once query learning is completed, the model employs several selected queries, and then aggregates the results based on retrieval ranking. This approach is complementary to the proposed method, and can be combined with our approac"
D16-1261,P10-1013,0,0.020057,"Missing"
D18-1216,Q17-1010,0,0.0348146,"the ability to transfer, as their encoders are both initialized from enc, which has been trained on source data and unlabeled target data. Oracle We also report the performance of an O RACLE which shares the same architecture as ours but is supervised by the oracle attention. The oracle attention is derived from a held-out dataset with large-scale annotations for the target task (see Appendix 3 for details). This helps us analyze the contribution of our R2A approach in isolation of the inherent limitations of the target tasks. 4.3 Implementation details We use pre-trained fastText embeddings (Bojanowski et al., 2017), a 200-dimension bidirectional LSTM (Hochreiter and Schmidhuber, 1997) for the language encoder, and a 50dimension bi-directional LSTM for the R2A encoder. Dropout (Srivastava et al., 2014) is applied with drop rate 0.1 on the word embeddings and the last hidden layers of the classifiers. All 1908 Source Target S VM R A -S VM‡ R A -C NN‡ T RANS† R A -T RANS‡† O URS‡† O RACLE† Beer aroma+palate Beer look 74.41 74.83 74.94 72.75 76.41 79.53 80.29 Beer look+palate Beer aroma 68.57 69.23 67.55 69.92 76.45 77.94 78.11 Beer look+aroma Beer palate 63.88 67.82 65.72 74.66 73.40 75.24 75.50 Table 3: A"
D18-1216,P17-1171,0,0.0298985,"ve analyses confirm that our R2A model is capable of generating high-quality attention for target tasks. 2 In this paper, we consider a more general setting where one domain contains multiple tasks. Also, we assume having one source domain. However, our proposed method is a general framework and can be easily adapted to problems with multiple source domains. 2 Related Work Attention-based models Attention has been shown to be effective when the model is trained on large amounts of training data (Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contra"
D18-1216,D17-1070,0,0.0329439,"guide the sentence-level attention for a CNN-based classifier. To reach good performance, their model still requires a sufficient amount of training data. Our work differs from theirs as we discern the intrinsic difference between human rationales and machine attention. Moreover, we learn a model to map human rationales into high-quality attention so as to provide a richer supervision for low-resource models. Transfer learning When labeled data on the target task is available, existing approaches typically transfer the knowledge by either fine-tuning an encoder trained on the source tasks(s) (Conneau et al., 2017; Peters et al., 2018) or multi-task learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios. Our work is also related to unsupervised domain 1904 Step 1: Training R2A adaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zh"
D18-1216,I17-2002,0,0.0288353,"., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contrast, our approach is based on the connection between rationales and attention, and can be used for multiple applications. Rationale-based models Zaidan et al. (2007) was the first to explore the value of rationales in low-resource scenarios. They hypothesize that the model confidence should decrease when the rationale words are removed from the inputs, and validate this idea for linear models. Recent work (Zhang et al., 2016) explores the potential of integrating rationales with more complex neural classifiers. In th"
D18-1216,D14-1181,0,0.00806323,"Missing"
D18-1216,D16-1011,1,0.909188,"Missing"
D18-1216,C16-1291,0,0.0176167,"to problems with multiple source domains. 2 Related Work Attention-based models Attention has been shown to be effective when the model is trained on large amounts of training data (Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contrast, our approach is based on the connection between rationales and attention, and can be used for multiple applications. Rationale-based models Zaidan et al. (2007) was the first to explore the value of rationales in low-resource scenarios. They hypothesize that the model confidence should decrease when the rationale"
D18-1216,P17-1164,0,0.0877057,"Missing"
D18-1216,D15-1166,0,0.158164,"Missing"
D18-1216,N18-1202,0,0.0217746,"el attention for a CNN-based classifier. To reach good performance, their model still requires a sufficient amount of training data. Our work differs from theirs as we discern the intrinsic difference between human rationales and machine attention. Moreover, we learn a model to map human rationales into high-quality attention so as to provide a richer supervision for low-resource models. Transfer learning When labeled data on the target task is available, existing approaches typically transfer the knowledge by either fine-tuning an encoder trained on the source tasks(s) (Conneau et al., 2017; Peters et al., 2018) or multi-task learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios. Our work is also related to unsupervised domain 1904 Step 1: Training R2A adaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016; Ganin"
D18-1216,D15-1044,0,0.118907,"Missing"
D18-1216,N16-1174,0,0.0741901,"tion, both qualitative and quantitative analyses confirm that our R2A model is capable of generating high-quality attention for target tasks. 2 In this paper, we consider a more general setting where one domain contains multiple tasks. Also, we assume having one source domain. However, our proposed method is a general framework and can be easily adapted to problems with multiple source domains. 2 Related Work Attention-based models Attention has been shown to be effective when the model is trained on large amounts of training data (Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailore"
D18-1216,N07-1033,0,0.810389,"ning attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contrast, our approach is based on the connection between rationales and attention, and can be used for multiple applications. Rationale-based models Zaidan et al. (2007) was the first to explore the value of rationales in low-resource scenarios. They hypothesize that the model confidence should decrease when the rationale words are removed from the inputs, and validate this idea for linear models. Recent work (Zhang et al., 2016) explores the potential of integrating rationales with more complex neural classifiers. In their model, human rationales are directly used to guide the sentence-level attention for a CNN-based classifier. To reach good performance, their model still requires a sufficient amount of training data. Our work differs from theirs as we disc"
D18-1216,D16-1076,0,0.0761291,"ne distance: (2) where λTatt controls the importance of LTatt . For better transfer, we initialize the encoder in the target classifier as enc from the trained R2A model. 4 r2a i) exp(h˜ uti , qatt α ˆ it = P t r2a i) , uj , qatt j exp(h˜ Pipeline Datasets We evaluate our approach on two transfer settings: transfer among aspects within the same domain and transfer among different domains. Aspect transfer We first consider the transfer problem between multiple aspects of one domain: beer review. We use a subset of the BeerAdvocate3 review dataset (McAuley et al., 2012) introduced by Lei et al. (2016). This dataset contains reviews with ratings (in the scale of [0, 1]) from three aspects of the beer: look, aroma and palate. We treat d(a, b) , max(0, 1 − cos(a, b) − 0.1), 3 1907 https://www.beeradvocate.com Beer Aspects Source Train Source Dev Target Train‡ Target Dev Target Test Look Aroma Palate 43,351 39,825 30,041 10,170 8,772 7,152 200 200 200 200 200 200 4,014 4,212 3,804 Basic classifier We train a linear S VM using bag-of-ngrams representation on the labeled target data. We combine uni-gram, bi-grams, and trigrams as features and use tf-idf weighting. Table 1: Statistics of the beer"
D18-1216,Q17-1036,1,0.823822,"learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios. Our work is also related to unsupervised domain 1904 Step 1: Training R2A adaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016; Ganin et al., 2016; Zhang et al., 2017). In contrast, our model adapts the mapping from rationales to attention; thus after training, it can be applied to different target tasks. labeled source data with rationales R2A unlabeled target data Step 2: R2A inference labeled target data with rationales R2A labeled target data with R2A-generated attention Step 3: Training target classifier 3 Method Problem formulation We assume that we have N source tasks {Si }N i=1 , where each of them has sufficient amounts of labeled examples. Using existing methods (Lei et al., 2016), we can generate rationales for each source example automatically ("
D18-1216,P16-1031,0,0.0273339,"17; Peters et al., 2018) or multi-task learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios. Our work is also related to unsupervised domain 1904 Step 1: Training R2A adaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016; Ganin et al., 2016; Zhang et al., 2017). In contrast, our model adapts the mapping from rationales to attention; thus after training, it can be applied to different target tasks. labeled source data with rationales R2A unlabeled target data Step 2: R2A inference labeled target data with rationales R2A labeled target data with R2A-generated attention Step 3: Training target classifier 3 Method Problem formulation We assume that we have N source tasks {Si }N i=1 , where each of them has sufficient amounts of labeled examples. Using existing methods (Lei et al., 2016), we can generate rationale"
D18-1498,P07-1056,0,0.64681,"er than meta-sources:4  H α(x, ·) = − Rh = K X l=1 is not provided. Additionally, it would be straightforward to add an MoE loss for labeled data in the target domain if they are available, thus extending our framework to a setting where we have fewshot target annotations. The training process is shown in Algorithm 1. 4 Experimental Setup 4.1 α(x, Sl ) · log α(x, Sl ) |Si | K X X i=1 j=1  H α(xSj i , ·) (6) Joint learning Our final objective is the weighted combination of each individual component loss: Task and Dataset Sentiment classification We use the multidomain Amazon reviews dataset (Blitzer et al., 2007), one of the standard benchmark datasets for domain adaptation. It contains reviews on four domains: Books (B), DVDs (D), Electronics (E), and Kitchen appliances (K). We follow the specific experiment settings proposed by Chen et al. (2012) (C HEN 12) and Ziser and Reichart (2017) (Z ISER 17). (7) 1. In C HEN 12, each domain has 2,000 labeled examples for training (1,000 positive and 1,000 negative), and the target test set has 3,000 to 6,000 examples.5 where λ controls the balance of the MoE loss and MTL loss. γ is set to 0 in non-adversarial setting when unlabeled data from the target domain"
D18-1498,W06-1615,0,0.324542,"Missing"
D18-1498,N18-1111,0,0.0519631,"portant to the target domain (Li and Zong, 2008; Luo et al., 2008; Crammer et al., 2008). Others learn a global domain similarity metric using labeled data in a supervised fashion (Yang et al., 2007; Duan et al., 2009; Yu et al., 2018). Alternatively, Mansour et al. (2009) and Bhatt et al. (2016) utilize unlabeled data of the target domain to find a distribution weighted combination of the source domains or to construct an auxiliary training set of the source domain instances close to the target domain instances. Recent adversarial methods on multi-source domain adaptation (Zhao et al., 2018; Chen and Cardie, 2018) align source domains to the target domains globally, without accounting for the distinct importance of each source with respect to a specific target example. The work most related to ours is by Kim et al. ↵(x, S1 ) ↵(x, S2 ) ... ↵(x, SK ) ""#$ pmoe (y|x) Figure 1: Architecture of the MoE model. E is the encoder which maps an input x to a hidden representation E(x); F Si is the classifier on the ith source domain; D is the critic that is only used during adversarial training. M is the metric learning component, which takes the encoding of x and source domains (S1:K ) as input and computes α. (2"
D18-1498,D14-1181,0,0.00368997,"omain adaptation on C HEN 12. It generalizes the domain adversarial network to multiple source domain adaptation by selectively backpropagating the domain discrimination loss according to domain classification error. 4.3 Implementation Details For C HEN 12, since the dataset is in TF-IDF format and the word ordering information is not available, we use a multilayer perceptron (MLP) with an input layer of 5,000 dimensions and one hidden layer of 500 dimensions as our encoder. For Z ISER 17, we instead use a convolutional neural network (CNN) encoder with a combination of kernel widths 3 and 5 (Kim, 2014), each with one hidden layer of size 150, which are then concatenated to a 300 dimension representation.6 For the POS tagging encoder, we use a hierarchical bidirectional LSTM (BiLSTM) network, which contains a character-level BiLSTM for generating individual word representations, followed by a word-level BiLSTM that generates contextualized word representations. For MMD, we follow Bousmalis et al. (2016) and use 19 RBF kernels with the standard deviation parameters ranging from 10−6 to 106 .7 All the models were trained using Adam with weight decay. Learning rate is set to 10−4 for C HEN 12 a"
D18-1498,P17-1060,0,0.152519,"Missing"
D18-1498,P08-2065,0,0.0199251,"autoencoders (Chen et al., 2012), and domain adversarial networks (Tzeng et al., 2014; Ganin et al., 2016; Zhang et al., 2017; Shen et al., 2018). In contrast to these previous approaches, however, our approach not only learns a shared representation space that generalizes well to the target domain, but also captures informative characteristics of individual source domains. Multi-Source domain adaptation The main challenge in using multiple sources for domain adaptation is in learning domain relations. Some approaches assume that all source domains are equally important to the target domain (Li and Zong, 2008; Luo et al., 2008; Crammer et al., 2008). Others learn a global domain similarity metric using labeled data in a supervised fashion (Yang et al., 2007; Duan et al., 2009; Yu et al., 2018). Alternatively, Mansour et al. (2009) and Bhatt et al. (2016) utilize unlabeled data of the target domain to find a distribution weighted combination of the source domains or to construct an auxiliary training set of the source domain instances close to the target domain instances. Recent adversarial methods on multi-source domain adaptation (Zhao et al., 2018; Chen and Cardie, 2018) align source domains to"
D18-1498,N18-1088,0,0.0127377,"task, where the metric is computed over the token-level encodings and multiclass predictions are made at the token (word) level. We use the SANCL dataset (Petrov and McDonald, 2012) which contains part-of-speech (POS) tagging annotations in 5 web domains: Emails, Weblogs, Answers, Newsgroups, and Reviews. Among these, Newsgroups, Reviews, and Answers have both a validation and a test set, and are used as target domains. The test set from Weblogs and Emails are used as individual source domains. The tagging is performed using the Universal POS tagset (Petrov et al., 2012). We also use Twitter (Liu et al., 2018) as an additional training source. Since it differs substantially from other sources and the target domain, we can assess our model’s ability to handle negative transfer. We consider 750 sentences from each SANCL source domain for training, and up to 2,250 sentences from the Twitter dataset to magnify the negative transfer. The validation set in the standard split of each target domain is used for hyper-parameters selection and early-stopping in our experiments. 4.2 Baselines We verify the efficacy of our approach (MoE) in non-adversarial and adversarial settings respectively. In both settings"
D18-1498,N18-1109,0,0.101812,"however, our approach not only learns a shared representation space that generalizes well to the target domain, but also captures informative characteristics of individual source domains. Multi-Source domain adaptation The main challenge in using multiple sources for domain adaptation is in learning domain relations. Some approaches assume that all source domains are equally important to the target domain (Li and Zong, 2008; Luo et al., 2008; Crammer et al., 2008). Others learn a global domain similarity metric using labeled data in a supervised fashion (Yang et al., 2007; Duan et al., 2009; Yu et al., 2018). Alternatively, Mansour et al. (2009) and Bhatt et al. (2016) utilize unlabeled data of the target domain to find a distribution weighted combination of the source domains or to construct an auxiliary training set of the source domain instances close to the target domain instances. Recent adversarial methods on multi-source domain adaptation (Zhao et al., 2018; Chen and Cardie, 2018) align source domains to the target domains globally, without accounting for the distinct importance of each source with respect to a specific target example. The work most related to ours is by Kim et al. ↵(x, S1"
D18-1498,Q17-1036,1,0.841096,"improve performance. 2 $ E(x) F S1 "" F S2 pS1 (y|x) pS2 (y|x) … F SK … pSK (y|x) ! Related Work Unsupervised domain adaptation Most existing domain adaptation methods focus on aligning the feature space between source and target domains to reduce the domain shift (Ben-David et al., 2007; Blitzer et al., 2007, 2006; Pan et al., 2010). Our approach is close to the representation learning approaches, such as the denoising autoencoder (Glorot et al., 2011), the marginalized stacked denoising autoencoders (Chen et al., 2012), and domain adversarial networks (Tzeng et al., 2014; Ganin et al., 2016; Zhang et al., 2017; Shen et al., 2018). In contrast to these previous approaches, however, our approach not only learns a shared representation space that generalizes well to the target domain, but also captures informative characteristics of individual source domains. Multi-Source domain adaptation The main challenge in using multiple sources for domain adaptation is in learning domain relations. Some approaches assume that all source domains are equally important to the target domain (Li and Zong, 2008; Luo et al., 2008; Crammer et al., 2008). Others learn a global domain similarity metric using labeled data"
D18-1498,petrov-etal-2012-universal,0,0.14508,"Missing"
D18-1498,K17-1040,0,0.0552614,"Missing"
D19-1341,W18-5513,0,0.0655748,"Missing"
D19-1341,D15-1075,0,0.0382168,"rs. Again, this can be explained by the bias in the training data that translates to the development set, allowing FEVER-trained models to leverage it. Applying the regularization method, using the same training data, helps to train a more robust model that performs better on our test set, where verification in context is a key requirement. 6 Related Work Large scale datasets are fraught with give-away phrases (McCoy et al., 2019; Niven and Kao, 2019). Crowd workers tend to adopt heuristics when creating examples, introducing bias in the dataset. In SNLI (Stanford Natural Language Inference) (Bowman et al., 2015), entailment based solely on the hypothesis forms a very strong baseline (Poliak et al., 2018; Gururangan et al., 2018). Similarly, as shown by Kaushik and Lipton (2018), reading comprehension models that rely only on the question (or only on the passage referred to by the question) perform exceedingly well on several popular datasets (Weston et al., 2016; Onishi et al., 2016; Hill et al., 2016). To address deficiencies in the SQuAD dataset (Jia R.W LMI·10−6 144 30 67 55 31 9 32 8 10 41 R.W p(l|w) 0.35 0.33 0.35 0.35 0.33 0.31 0.33 0.30 0.32 0.35 Table 4: Re-weighted statistics (l = R EFUTES)"
D19-1341,D18-1546,0,0.0208476,"regularization method, using the same training data, helps to train a more robust model that performs better on our test set, where verification in context is a key requirement. 6 Related Work Large scale datasets are fraught with give-away phrases (McCoy et al., 2019; Niven and Kao, 2019). Crowd workers tend to adopt heuristics when creating examples, introducing bias in the dataset. In SNLI (Stanford Natural Language Inference) (Bowman et al., 2015), entailment based solely on the hypothesis forms a very strong baseline (Poliak et al., 2018; Gururangan et al., 2018). Similarly, as shown by Kaushik and Lipton (2018), reading comprehension models that rely only on the question (or only on the passage referred to by the question) perform exceedingly well on several popular datasets (Weston et al., 2016; Onishi et al., 2016; Hill et al., 2016). To address deficiencies in the SQuAD dataset (Jia R.W LMI·10−6 144 30 67 55 31 9 32 8 10 41 R.W p(l|w) 0.35 0.33 0.35 0.35 0.33 0.31 0.33 0.30 0.32 0.35 Table 4: Re-weighted statistics (l = R EFUTES) for the bigrams from Table 1. The weights were obtained following the optimization of Eq. 3 on the training set which contains three labels. and Liang, 2017), researcher"
D19-1341,P17-1152,0,0.0962021,"Missing"
D19-1341,P19-1334,0,0.0703286,"Missing"
D19-1341,N19-1423,0,0.0714446,"Missing"
D19-1341,N18-2017,0,0.125943,"Missing"
D19-1341,D17-1215,0,0.123267,"Missing"
D19-1341,P19-1459,0,0.0679511,"Missing"
D19-1341,D16-1241,0,0.0313133,"are fraught with give-away phrases (McCoy et al., 2019; Niven and Kao, 2019). Crowd workers tend to adopt heuristics when creating examples, introducing bias in the dataset. In SNLI (Stanford Natural Language Inference) (Bowman et al., 2015), entailment based solely on the hypothesis forms a very strong baseline (Poliak et al., 2018; Gururangan et al., 2018). Similarly, as shown by Kaushik and Lipton (2018), reading comprehension models that rely only on the question (or only on the passage referred to by the question) perform exceedingly well on several popular datasets (Weston et al., 2016; Onishi et al., 2016; Hill et al., 2016). To address deficiencies in the SQuAD dataset (Jia R.W LMI·10−6 144 30 67 55 31 9 32 8 10 41 R.W p(l|w) 0.35 0.33 0.35 0.35 0.33 0.31 0.33 0.30 0.32 0.35 Table 4: Re-weighted statistics (l = R EFUTES) for the bigrams from Table 1. The weights were obtained following the optimization of Eq. 3 on the training set which contains three labels. and Liang, 2017), researchers have proposed approaches for augmenting the existing dataset (Rajpurkar et al., 2018). In most cases, these augmentations are done manually, and involve constructing challenging examples for existing systems"
D19-1341,D14-1162,0,0.0806448,"Missing"
D19-1341,N18-1202,0,0.0746687,"Missing"
D19-1341,S18-2023,0,0.071565,"Missing"
D19-1341,P18-2124,0,0.0212556,"r only on the passage referred to by the question) perform exceedingly well on several popular datasets (Weston et al., 2016; Onishi et al., 2016; Hill et al., 2016). To address deficiencies in the SQuAD dataset (Jia R.W LMI·10−6 144 30 67 55 31 9 32 8 10 41 R.W p(l|w) 0.35 0.33 0.35 0.35 0.33 0.31 0.33 0.30 0.32 0.35 Table 4: Re-weighted statistics (l = R EFUTES) for the bigrams from Table 1. The weights were obtained following the optimization of Eq. 3 on the training set which contains three labels. and Liang, 2017), researchers have proposed approaches for augmenting the existing dataset (Rajpurkar et al., 2018). In most cases, these augmentations are done manually, and involve constructing challenging examples for existing systems. 7 Conclusion This paper demonstrates that the FEVER dataset contains idiosyncrasies that can be easily exploited by fact-checking classifiers to obtain high classification accuracies. Evaluating the claimevidence reasoning of these models necessitates unbiased datasets. Therefore, we suggest a way to turn the evaluation FEVER pairs into symmetric combinations for which a decision that is solely based on the claim is equivalent to a random guess. Tested on these pairs, FEV"
D19-1341,N19-1421,0,0.0120812,"nverse relations (see Figure 1). Examples of generated sentences are provided in Table 2. This new test set completely eliminates the ability of models to rely on cues from claims. Considering the two labels of this test set5 , the probability of a label given the existence of any n-gram in the claim or in the evidence is p(l|w) = 0.5, by construction. Also, as the example in Figure 1 demonstrates, in order to perform well on this dataset, a fact verification classifier may still take advantage of world 4 We use InferSent because BERT, being pretrained on Wikipedia, comprises world knowledge (Talmor et al., 2019). 5 N OT E NOUGH I NFO cases are easy to generate so we focus on the two other labels. knowledge (e.g. geographical locations), but reasoning should only be with respect to the context. 4 Towards Unbiased Training Creating a large symmetric dataset for training is outside the scope of this paper as it would be too expensive. Instead, we propose an algorithmic solution to alleviate the bias introduced by ‘give-away’ n-grams present in the claims. We reweight the instances in the dataset to flatten the correlation of claim n-grams with respect to the labels. Specifically, for ‘give-away’ phrases"
D19-1341,N18-1074,0,0.0634953,"Missing"
D19-1574,N19-1253,0,0.0292554,"parsing performance. Overall, this divergence motivates the development of approaches that better match the two distributions. P@1 P@3 P@5 P@10 13 27 13 33 67 27 60 67 27 80 93 73 Table 4: Precision@k for identifying the best parsing transfer language, for the k typological neighbors. 7 Related Work Other recent progress in cross-lingual parsing has focused on lexical alignment (Guo et al., 2015, 2016; Schuster et al., 2019). Data augmentation (Wang and Eisner, 2017) is another promising direction, but at the cost of greater training demands. Both directions do not directly address structure. Ahmad et al. (2019) showed structuralsensitivity is important for modern parsers; insensitive parsers suffer. Data transfer is an alternative solution to alleviate the typological divergences, such as annotation projection (Tiedemann, 2014) and source treebank reordering (Rasooli and Collins, 2019). These approaches are typically limited by parallel data and imperfect alignments. Our work aims to understand cross-lingual parsing in the context of model transfer, with typology serving as language descriptors, with the goal of eventually addressing the issue of structure. 8 Conclusion Realizing the potential for t"
D19-1574,P15-1119,1,0.84782,"nearest neighbors. As shown in Table 4, we observe that while there is some correlation between the two, they are far from perfectly aligned. TD has the best alignment, which is consistent with its corresponding best parsing performance. Overall, this divergence motivates the development of approaches that better match the two distributions. P@1 P@3 P@5 P@10 13 27 13 33 67 27 60 67 27 80 93 73 Table 4: Precision@k for identifying the best parsing transfer language, for the k typological neighbors. 7 Related Work Other recent progress in cross-lingual parsing has focused on lexical alignment (Guo et al., 2015, 2016; Schuster et al., 2019). Data augmentation (Wang and Eisner, 2017) is another promising direction, but at the cost of greater training demands. Both directions do not directly address structure. Ahmad et al. (2019) showed structuralsensitivity is important for modern parsers; insensitive parsers suffer. Data transfer is an alternative solution to alleviate the typological divergences, such as annotation projection (Tiedemann, 2014) and source treebank reordering (Rasooli and Collins, 2019). These approaches are typically limited by parallel data and imperfect alignments. Our work aims t"
D19-1574,P12-1066,1,0.891892,"s challenging for current models. ∗ 1 The first two authors contributed equally. Code: github.com/ajfisch/TypologyParser One promising direction for handling these divergences is linguistic typology. Linguistic typology classifies languages according to their structural and functional features. By explicitly highlighting specific similarities and differences in languages’ syntactic structures, typology holds great potential for facilitating cross-lingual transfer (O’Horan et al., 2016). Indeed, nonneural parsing approaches have already demonstrated empirical benefits of typology-aware models (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) While adding discrete typological attributes is straightforward for traditional feature-based approaches, for modern neural parsers finding an effective implementation choice is more of an open question. Not surprisingly, the reported results have been mixed. For instance, Ammar et al. (2016) found no benefit to using typology for parsing when using a neuralbased model, while Wang and Eisner (2018) and Scholivet et al. (2019) did in several cases. There are many possible hypotheses that can attempt to explain the state-of-the-art. Might neu"
D19-1574,N19-1385,0,0.0164781,"l neighbors. 7 Related Work Other recent progress in cross-lingual parsing has focused on lexical alignment (Guo et al., 2015, 2016; Schuster et al., 2019). Data augmentation (Wang and Eisner, 2017) is another promising direction, but at the cost of greater training demands. Both directions do not directly address structure. Ahmad et al. (2019) showed structuralsensitivity is important for modern parsers; insensitive parsers suffer. Data transfer is an alternative solution to alleviate the typological divergences, such as annotation projection (Tiedemann, 2014) and source treebank reordering (Rasooli and Collins, 2019). These approaches are typically limited by parallel data and imperfect alignments. Our work aims to understand cross-lingual parsing in the context of model transfer, with typology serving as language descriptors, with the goal of eventually addressing the issue of structure. 8 Conclusion Realizing the potential for typology may require rethinking current approaches. We can further drive performance by refining typology-based similarities into a metric more representative of actual transfer quality. Ultimately, we would like to design models that can directly leverage typological compositiona"
D19-1574,N19-1393,0,0.016085,"al., 2016). Indeed, nonneural parsing approaches have already demonstrated empirical benefits of typology-aware models (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) While adding discrete typological attributes is straightforward for traditional feature-based approaches, for modern neural parsers finding an effective implementation choice is more of an open question. Not surprisingly, the reported results have been mixed. For instance, Ammar et al. (2016) found no benefit to using typology for parsing when using a neuralbased model, while Wang and Eisner (2018) and Scholivet et al. (2019) did in several cases. There are many possible hypotheses that can attempt to explain the state-of-the-art. Might neural models already implicitly learn typological information on their own? Is the hand-specified typology information sufficiently accurate — or provided in the right granularity — to always be useful? How do cross-lingual parsers use, or ignore, typology when making predictions? Without understanding answers to these questions, it is difficult to develop a principled way for robustly incorporating linguistic knowledge as an inductive bias for cross-lingual transfer. In this pape"
D19-1574,N19-1162,1,0.803897,"hown in Table 4, we observe that while there is some correlation between the two, they are far from perfectly aligned. TD has the best alignment, which is consistent with its corresponding best parsing performance. Overall, this divergence motivates the development of approaches that better match the two distributions. P@1 P@3 P@5 P@10 13 27 13 33 67 27 60 67 27 80 93 73 Table 4: Precision@k for identifying the best parsing transfer language, for the k typological neighbors. 7 Related Work Other recent progress in cross-lingual parsing has focused on lexical alignment (Guo et al., 2015, 2016; Schuster et al., 2019). Data augmentation (Wang and Eisner, 2017) is another promising direction, but at the cost of greater training demands. Both directions do not directly address structure. Ahmad et al. (2019) showed structuralsensitivity is important for modern parsers; insensitive parsers suffer. Data transfer is an alternative solution to alleviate the typological divergences, such as annotation projection (Tiedemann, 2014) and source treebank reordering (Rasooli and Collins, 2019). These approaches are typically limited by parallel data and imperfect alignments. Our work aims to understand cross-lingual par"
D19-1574,N13-1126,0,0.0365871,"Missing"
D19-1574,C14-1175,0,0.0131246,"t parsing transfer language, for the k typological neighbors. 7 Related Work Other recent progress in cross-lingual parsing has focused on lexical alignment (Guo et al., 2015, 2016; Schuster et al., 2019). Data augmentation (Wang and Eisner, 2017) is another promising direction, but at the cost of greater training demands. Both directions do not directly address structure. Ahmad et al. (2019) showed structuralsensitivity is important for modern parsers; insensitive parsers suffer. Data transfer is an alternative solution to alleviate the typological divergences, such as annotation projection (Tiedemann, 2014) and source treebank reordering (Rasooli and Collins, 2019). These approaches are typically limited by parallel data and imperfect alignments. Our work aims to understand cross-lingual parsing in the context of model transfer, with typology serving as language descriptors, with the goal of eventually addressing the issue of structure. 8 Conclusion Realizing the potential for typology may require rethinking current approaches. We can further drive performance by refining typology-based similarities into a metric more representative of actual transfer quality. Ultimately, we would like to design"
D19-1574,Q16-1035,0,0.0302847,"Missing"
D19-1574,Q17-1011,0,0.205965,"fine-grained alternative to linguistic typology. Compared to WALS, there are rarely missing values, and the degree of dominance of each dependency ordering is directly encoded — potentially allowing for better modeling of local variance within a language. It is important to note, however, that true directionalities require a parsed corpus to be derived; thus, they are not a realistic option for cross-lingual parsing in practice.3 Nevertheless, we include them for completeness. Surface Statistics, TS : It is possible to derive a proxy measure of typology from part-ofspeech tag sequences alone. Wang and Eisner (2017) found surface statistics to be highly predictive of language typology, while Wang and Eisner (2018) replaced typological features entirely with surface statistics in their augmented dependency parser. Surface statistics have the advantage of being readily available and are not restricted to narrow linguistic definitions, but are less informed by the true underlying structure. We compute the set of hand-engineered features used in (Wang and Eisner, 2018), yielding a real-valued vector T ∈ [0, 1]2380 . 3 Parsing Architecture We use the graph-based Deep Biaffine Attention neural parser of (Dozat"
D19-1574,Q18-1046,0,0.18475,"ingual transfer (O’Horan et al., 2016). Indeed, nonneural parsing approaches have already demonstrated empirical benefits of typology-aware models (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) While adding discrete typological attributes is straightforward for traditional feature-based approaches, for modern neural parsers finding an effective implementation choice is more of an open question. Not surprisingly, the reported results have been mixed. For instance, Ammar et al. (2016) found no benefit to using typology for parsing when using a neuralbased model, while Wang and Eisner (2018) and Scholivet et al. (2019) did in several cases. There are many possible hypotheses that can attempt to explain the state-of-the-art. Might neural models already implicitly learn typological information on their own? Is the hand-specified typology information sufficiently accurate — or provided in the right granularity — to always be useful? How do cross-lingual parsers use, or ignore, typology when making predictions? Without understanding answers to these questions, it is difficult to develop a principled way for robustly incorporating linguistic knowledge as an inductive bias for cross-li"
D19-1574,D15-1213,1,0.850666,"two authors contributed equally. Code: github.com/ajfisch/TypologyParser One promising direction for handling these divergences is linguistic typology. Linguistic typology classifies languages according to their structural and functional features. By explicitly highlighting specific similarities and differences in languages’ syntactic structures, typology holds great potential for facilitating cross-lingual transfer (O’Horan et al., 2016). Indeed, nonneural parsing approaches have already demonstrated empirical benefits of typology-aware models (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) While adding discrete typological attributes is straightforward for traditional feature-based approaches, for modern neural parsers finding an effective implementation choice is more of an open question. Not surprisingly, the reported results have been mixed. For instance, Ammar et al. (2016) found no benefit to using typology for parsing when using a neuralbased model, while Wang and Eisner (2018) and Scholivet et al. (2019) did in several cases. There are many possible hypotheses that can attempt to explain the state-of-the-art. Might neural models already implicitly learn typological infor"
H01-1065,P99-1071,1,0.935785,"chronological order of events and cohesion. This strategy was derived from empirical observations based on experiments asking humans to order information. Evaluation of our augmented algorithm shows a signi cant improvement of the ordering over the two naive techniques we used as baseline. 1. INTRODUCTION Multidocument summarization poses a number of new challenges over single document summarization. Researchers have already investigated issues such as identifying repetitions or contradictions across input documents and determining which information is salient enough to include in the summary [1, 3, 6, 11, 15, 19]. One issue that has received little attention is how to organize the selected information so that the output summary is coherent. Once all the relevant pieces of information have been selected across the input documents, the summarizer has to decide in which order to present them so that the whole text makes sense. In single document summarization, one possible ordering of the extracted information is provided by the input document itself. However, [10] observed that, in single document summaries written by professional summarizers, extracted sentences do not retain their precedence orders in"
H01-1065,W00-1426,0,0.0470244,"ure 8: Evaluation of the the Majority Ordering, the Chronological Ordering and the Augmented Ordering. summary sentences are typically arranged in the same order that they were found in the full document (although [10] reports that human summarizers do sometimes change the original order). In multidocument summarization, the summary consists of fragments of text or sentences that were selected from di erent texts. Thus, there is no complete ordering of summary sentences that can be found in the original documents. The ordering task has been extensively investigated in the generation community [14, 17, 9, 2, 16]. One approach is top-down, using schemas [14] or plans [5] to determine the organizational structure of the text. This appproach postulates a rhetorical structure which can be used to select information from an underlying knowledge base. Because the domain is limited, an encoding can be developed of the kinds of propositional content that match rhetorical elements of the schema or plan, thereby allowing content to be selected and ordered. Rhetorical Structure Theory (RST) allows for more exibility in ordering content. The relations occur between pairs of propositions. Constraints based on int"
H01-1065,W99-0625,0,0.0137047,"nformation across documents. In the case of multidocument summarization of articles about the same event, source articles can contain both repetitions and contradictions. Extracting all the similar sentences would produce a verbose and repetitive summary, while extracting only some of the similar sentences would produce a summary biased towards some sources. MultiGen uses a comparison of extracted similar sentences to select the appropriate phrases to include in the summary and reformulates them as a new text. MultiGen consists of an analysis and a generation component. The analysis component [7] identi es units of text which convey similar information across the input documents using statistical techniques and shallow text analysis. Once similar text units are identi ed, we cluster them into themes. Themes are sets of sentences from di erent documents that contain repeated information and do not necessarily contain sentences from all the documents. For each theme, the generation component [1] identi es phrases which are in the intersection of the theme sentences, and selects them as part of the summary. The intersection sentences are then ordered to produce a coherent text. 3. NAIVE"
H01-1065,P94-1002,0,0.0176866,"1 T2 T3 are summarized by the Chronological Ordering (S1 ) or by the Augmented algorithm (S2 ). 5.1 The Algorithm Our goal is to remove dis uencies from the summary by grouping together topically related themes. This can be achieved by integrating cohesion as an additional constraint to the CO algorithm. The main technical diÆculty in incorporating cohesion in our ordering algorithm is to identify and to group topically related themes across multiple documents. In other words, given two themes, we need to determine if they belong to the same cohesion block. For a single document, segmentation [8] could be used to identify blocks, but we cannot use such a technique to identify cohesion between sentences across multiple documents. The main reason is that segmentation algorithms exploit the linear structure of an input text; in our case, we want to group together sentences belonging to di erent texts. Our solution consists of the following steps. In a preprocessing stage, we segment each input text, so that given two sentences within the same text, we can determine if they are topically related. Assume the themes A and B , where A contains sentences (A1 : : : An ), and B contains sentenc"
H01-1065,P00-1010,0,0.011804,"ished on di erent dates, and articles themselves cover events occurring over a wide range in time. Using chronological order in the summary to describe the main events helps the user understand what has happened. It seems like a natural and appropriate strategy. As mentioned earlier, in our framework, we are ordering themes; in this strategy, we therefore need to assign a date to themes. To identify the date an event occured requires a detailed interpretation of temporal references in articles. While there have been recent developments in disambiguating temporal expressions and event ordering [12], correlating events with the date on which they occurred is a hard task. In our case, we approximate the theme time by its rst publication date; that is, the rst time the theme has been reported in our set of input articles. It is an acceptable approximation for news events; the rst publication date of an event usually corresponds to its occurrence in real life. For instance, in a terrorist attack story, the theme conveying the attack itself will have a date previous to the date of the theme describing a trial following the attack. Articles released by news agencies are marked with a publicat"
H01-1065,C90-2048,0,0.210194,"ure 8: Evaluation of the the Majority Ordering, the Chronological Ordering and the Augmented Ordering. summary sentences are typically arranged in the same order that they were found in the full document (although [10] reports that human summarizers do sometimes change the original order). In multidocument summarization, the summary consists of fragments of text or sentences that were selected from di erent texts. Thus, there is no complete ordering of summary sentences that can be found in the original documents. The ordering task has been extensively investigated in the generation community [14, 17, 9, 2, 16]. One approach is top-down, using schemas [14] or plans [5] to determine the organizational structure of the text. This appproach postulates a rhetorical structure which can be used to select information from an underlying knowledge base. Because the domain is limited, an encoding can be developed of the kinds of propositional content that match rhetorical elements of the schema or plan, thereby allowing content to be selected and ordered. Rhetorical Structure Theory (RST) allows for more exibility in ordering content. The relations occur between pairs of propositions. Constraints based on int"
H01-1065,J93-4004,0,0.0492256,"ure 8: Evaluation of the the Majority Ordering, the Chronological Ordering and the Augmented Ordering. summary sentences are typically arranged in the same order that they were found in the full document (although [10] reports that human summarizers do sometimes change the original order). In multidocument summarization, the summary consists of fragments of text or sentences that were selected from di erent texts. Thus, there is no complete ordering of summary sentences that can be found in the original documents. The ordering task has been extensively investigated in the generation community [14, 17, 9, 2, 16]. One approach is top-down, using schemas [14] or plans [5] to determine the organizational structure of the text. This appproach postulates a rhetorical structure which can be used to select information from an underlying knowledge base. Because the domain is limited, an encoding can be developed of the kinds of propositional content that match rhetorical elements of the schema or plan, thereby allowing content to be selected and ordered. Rhetorical Structure Theory (RST) allows for more exibility in ordering content. The relations occur between pairs of propositions. Constraints based on int"
H01-1065,W00-0403,0,0.0399355,"system has to choose in which order to present the output sentences. In this section, we describe two algorithms for ordering sentences suitable for domain independent multidocument summarization. The rst algorithm, Majority Ordering (MO), relies only on the original orders of sentences in the input documents. It is the rst solution one can think of when addressing the ordering problem. The second one, Chronological Ordering (CO) uses time related features to order sentences. We analyze this strategy because it was originally implemented in MultiGen and followed by other summarization systems [18]. In the MultiGen framework, ordering sentences is equivalent to ordering themes and we describe the algorithms in terms of themes, but the concepts can be adapted to other summarization systems such as [3]. Our evaluation shows that these methods alone do not provide an adequate strategy for ordering. 3.1 is NP-complete; this can be shown by reducing the traveling salesman problem to this problem. Despite this fact, we still can apply this ordering, because typically the length of the output summary is limited to a small number of sentences. For longer summaries, the approximation algorithm d"
H01-1065,J98-3005,1,0.821107,"chronological order of events and cohesion. This strategy was derived from empirical observations based on experiments asking humans to order information. Evaluation of our augmented algorithm shows a signi cant improvement of the ordering over the two naive techniques we used as baseline. 1. INTRODUCTION Multidocument summarization poses a number of new challenges over single document summarization. Researchers have already investigated issues such as identifying repetitions or contradictions across input documents and determining which information is salient enough to include in the summary [1, 3, 6, 11, 15, 19]. One issue that has received little attention is how to organize the selected information so that the output summary is coherent. Once all the relevant pieces of information have been selected across the input documents, the summarizer has to decide in which order to present them so that the whole text makes sense. In single document summarization, one possible ordering of the extracted information is provided by the input document itself. However, [10] observed that, in single document summaries written by professional summarizers, extracted sentences do not retain their precedence orders in"
H05-1042,W03-1016,0,0.0543385,"of content selection components developed for various domains (Kukich, 1983; McKeown, 1985; Sripada et al., 2001; Reiter and Dale, 2000). A common theme across different approaches is the emphasis on coherence: related information is selected “to produce a text that hangs together” (McKeown, 1985). Similarly, our method is also guided by coherence constraints. In our case these constraints are derived automatically, while in symbolic generation systems coherence is enforced by analyzing a large number of texts from a domain-relevant corpus and careful hand-crafting of content selection rules. Duboue and McKeown (2003) were the first to propose a method for learning content selection rules automatically, thus going beyond mere corpus analysis. They treat content selection as a classification task. Given a collection of texts associated with a domain-specific database, their model learns whether a database entry should be selected for presentation or not. Their modeling approach uses an expressive feature space while considering database entries in isolation. Similarly to Duboue and McKeown (2003), we view content selection as a classification task and learn selection rules from a database and its correspond"
H05-1042,P83-1022,0,0.443446,"s of collective content selection on this complex domain. Furthermore, our results empirically confirm the contribution of discourse constraints for content selection. In the following section, we provide an overview of existing work on content selection. Then, we define the learning task and introduce our approach for collective content selection. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work The generation literature provides multiple examples of content selection components developed for various domains (Kukich, 1983; McKeown, 1985; Sripada et al., 2001; Reiter and Dale, 2000). A common theme across different approaches is the emphasis on coherence: related information is selected “to produce a text that hangs together” (McKeown, 1985). Similarly, our method is also guided by coherence constraints. In our case these constraints are derived automatically, while in symbolic generation systems coherence is enforced by analyzing a large number of texts from a domain-relevant corpus and careful hand-crafting of content selection rules. Duboue and McKeown (2003) were the first to propose a method for learning c"
H05-1042,P04-1035,0,0.00367601,"this expression capture the penalty for assigning entities to classes against their individual preferences. For instance, the penalty for selecting an entry x ∈ C+ will equal ind− (x), i.e., x’s individual preference of being ommitted. The third term captures a linking penalty for all pairs of entities (xi , x j ) that are connected by a link of type L, and are assigned to different classes. This formulation is similar to the energy minimization framework, which is commonly used in image analysis (Besag, 1986; Boykov et al., 1999) and has been recently applied in natural language processing (Pang and Lee, 2004). The principal advantages of this formulation lie in its computational properties. Despite seeming intractable — the number of possible subsets to consider for selection is exponential in the number of database entities — the inference problem has an exact solution. Provided that the scores ind+ (x), ind− (x), and linkL (x, y) are 334 positive, we can find a globally optimal label assignment in polynomial time by computing a minimal cut partition in an appropriately constructed graph (Greig et al., 1989). In the following we first discuss how individual preference scores are estimated. Next,"
H05-1042,P98-2209,0,0.507487,"Missing"
H05-1042,W01-0802,0,\N,Missing
H05-1042,C98-2204,0,\N,Missing
J05-3002,C02-1134,0,0.00888176,"Missing"
J05-3002,C00-1007,0,0.0163532,"2) originated during linearization of the lattice and were caused either by incompleteness of the linearizer or by suboptimal scoring of the language model. Mistakes of the first type are caused by missing rules for generating auxiliaries given node features. An example of this phenomenon is the sentence The coalition to have play a central role, which verbalizes the verb construction will have to play incorrectly. Our linearizer lacks the completeness of existing application-independent linearizers, such as the unificationbased FUF/SURGE (Elhadad and Robin 1996) and the probabilistic Fergus (Bangalore and Rambow 2000). Unfortunately, we were unable to reuse any of the existing largescale linearizers because of significant structural differences between input expected by these linearizers and the format of a fusion lattice. We are currently working on adapting Fergus for the sentence fusion task. Mistakes related to suboptimal scoring were the most common (33 out of 42); in these cases, a language model selected ill-formed sentences, assigning a worse score to a better sentence. The sentence The diplomats were given to leave the country in 10 days illustrates a suboptimal linearization of the fusion lattice"
J05-3002,N04-4001,0,0.0207226,"ved from input documents. Sentence fusion is a significant first step toward the generation of abstracts, as opposed to extracts (Borko and Bernier 1975), for multidocument summarization. Unlike extraction methods (used by the vast majority of summarization researchers), sentence fusion allows for the true synthesis of information from a set of input documents. It has been shown that combining information from several sources is a natural strategy for multidocument summarization. Analysis of human-written summaries reveals that most sentences combine information drawn from multiple documents (Banko and Vanderwende 2004). Sentence fusion achieves this goal automatically. Our evaluation shows that our approach is promising, with sentence fusion outperforming sentence extraction for the task of content selection. This article focuses on the implementation and evaluation of the sentence fusion method within the multidocument summarization system MultiGen, which daily summarizes multiple news articles on the same event as part1 of Columbia’s news browsing system Newsblaster (http://newsblaster.cs.columbia.edu/). In the next section, we provide an overview of MultiGen, focusing on components that produce input or"
J05-3002,W97-0703,1,0.234019,"ent at an army base in the area. Fusion sentence: Palestinians fired an antitank missile at a bulldozer. 300 Barzilay and McKeown Sentence Fusion for Multidocument News Summarization 2.2 Theme Selection To generate a summary of predetermined length, we induce a ranking on the themes and select the n highest.2 This ranking is based on three features of the theme: size measured as the number of sentences, similarity of sentences in a theme, and salience score. The first two of these scores are produced by Simfinder, and the salience score is computed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) as described below. Combining different rankings further filters common information in terms of salience. Since each of these scores has a different range of values, we perform ranking based on each score separately, then induce total ranking by summing ranks from individual categories: Rank (theme) = Rank (Number of sentences in theme) + Rank (Similarity of sentences in theme) + Rank (Sum of lexical chain scores in theme) Lexical chains—sequences of semantically related words—are tightly connected to the lexical cohesive structure of the text and have been shown to be useful for determining"
J05-3002,W02-1022,1,0.538495,"Missing"
J05-3002,P01-1008,1,0.274258,"indicate the relationships between nodes). An edge is labeled by the syntactic function of the two nodes it connects (e.g., subject– verb). It is unlikely that an edge connecting a subject and verb in one sentence, for example, corresponds to an edge connecting a verb and an adjective in another sentence. The word similarity measures take into account more than word identity: They also identify pairs of paraphrases, using WordNet and a paraphrasing dictionary. We automatically constructed the paraphrasing dictionary from a large comparable news corpus using the co-training method described in Barzilay and McKeown (2001). The dictionary contains pairs of word-level paraphrases as well as phrase-level paraphrases.4 Several examples of automatically extracted paraphrases are given in Table 2. During alignment, each pair of nonidentical words that do not comprise a synset in 4 The comparable corpus and the derived dictionary are available at http://www.cs.cornell.edu/˜regina/thesis-data/comp/input/processed.tbz2 and http://www.cs.cornell.edu/˜regina/thesis-data/comp/output/comp2-ALL.txt. For details on the corpus collection and evaluation of the paraphrase quality, see Barzilay (2003). 304 Barzilay and McKeown S"
J05-3002,P99-1071,1,0.479926,"Missing"
J05-3002,J93-2003,0,0.0120045,"ficantly distorting its meaning. While earlier approaches for text compression were based on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999), more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced (Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003). The summary sentences, which have been manually compressed, are aligned with the original sentences from which they were drawn. Knight and Marcu (2000) treat reduction as a translation process using a noisychannel model (Brown et al. 1993). In this model, a short (compressed) string is treated as a source, and additions to this string are considered to be noise. The probability of a source string s is computed by combining a standard probabilistic context-free grammar score, which is derived from the grammar rules that yielded tree s, and a word-bigram score, computed over the leaves of the tree. The stochastic channel model creates a large tree t from a smaller tree s by choosing an extension template for each node based on the labels of the node and its children. In the decoding stage, the system searches for the short string"
J05-3002,C96-2183,0,0.194743,"Missing"
J05-3002,N03-1004,0,0.0141413,"t collections, such as the Web, creates both problems and opportunities for natural language systems. On the one hand, the presence of numerous sources conveying the same information causes difficulties for end users of search engines and news providers; they must read the same information over and over again. On the other hand, redundancy can be exploited to identify important and accurate information for applications such as summarization and question answering (Mani and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke, Cormack, and Lynam 2001; Dumais et al. 2002; Chu-Carroll et al. 2003). Clearly, it would be highly desirable to have a mechanism that could identify common information among multiple related documents and fuse it into a coherent text. In this article, we present a method for sentence fusion that exploits redundancy to achieve this task in the context of multidocument summarization. A straightforward approach for approximating sentence fusion can be found in the use of sentence extraction for multidocument summarization (Carbonell and Goldstein 1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy 2002). Once a system finds a set of senten"
J05-3002,J03-4003,0,0.032625,"Missing"
J05-3002,W02-2102,0,0.00743534,"s still a gap between the performance of our system and human performance. An important goal for future work on sentence fusion is to increase the flexibility of content selection and realization. We believe that the process of aligning theme sentences can be greatly improved by having the system learn the similarity function, instead of using manually assigned weights. An interesting question is how such a similarity function can be induced in an unsupervised fashion. In addition, we can improve the flexibility of the fusion algorithm by using a more powerful language model. Recent research (Daume et al. 2002) has show that syntax-based language models are more suitable for language generation tasks; the study of such models is a promising direction to explore. An important feature of the sentence fusion algorithm is its ability to generate multiple verbalizations of a given fusion lattice. In our implementation, this property is utilized only to produce grammatical texts in the changed syntactic context, but it can also be used to increase coherence of the text at the discourse level by taking context into account. In our current system, each sentence is generated in isolation, independently from"
J05-3002,H01-1065,1,0.308984,"rasing, then discuss our implementation. For the word-ordering task, we do not have to consider all the possible traversals, since the number of valid traversals is limited by ordering constraints encoded in the fusion lattice. However, the basis lattice does not uniquely determine the ordering: The placement of trees inserted in the basis lattice from other theme sentences is not restricted by the original basis tree. While the ordering of many sentence constituents is determined by their syntactic roles, some constituents, such as time, location and manner circumstantials, are free to move (Elhadad et al. 2001). Therefore, the algorithm still has to select an appropriate order from among different orders of the inserted trees. The process so far produces a sentence that can be quite different from the extracted sentence; although the basis sentences provides guidance for the generation process, constituents may be removed, added in, or reordered. Wording can also be modified during this process. Although the selection of words and phrases which appear in the basis tree is a safe choice, enriching the fusion sentence with alternative verbalizations has several benefits. In applications such as summar"
J05-3002,W96-0501,0,0.0641651,"ulldozer to build a new embankment in the area—is not a well-formed sentence; however, our language model gave it a better score than its well-formed alternatives, the second and the third sentences (see Section 4 for further discussion). Despite these shortcomings, we preferred entropy-based scoring to symbolic linearization. In the next section, we motivate our choice. 3.3.1 Statistical versus Symbolic Linearization. In the previous version of the system (Barzilay, McKeown, and Elhadad 1999), we performed linearization of a fusion dependency structure using the language generator FUF/SURGE (Elhadad and Robin 1996). As a large-scale linearizer used in many traditional semantic-to-text generation systems, FUF/SURGE could be an appealing solution to the task of surface realization. Because the input structure and the requirements on the linearizer are quite different in text-to-text generation, we had to design rules for mapping between dependency structures produced by the fusion component and FUF/SURGE input. For instance, FUF/SURGE requires that the input contain a semantic role for prepositional phrases, such as manner, purpose, or location, which is not present in our dependency representation; thus"
J05-3002,W99-0625,0,0.011143,"Missing"
J05-3002,P93-1023,1,0.139349,"nce and peer sentences were divided into clauses by the authors. The judges assessed overlap on the clause level between reference and peer sentences. The wording of the instructions was inspired by the DUC instructions for clause comparison. For each clause in the reference sentence, the judge decided whether the meaning of a corresponding clause was conveyed in a peer sentence. In addition to 0 score for no overlap and 1 for full overlap, this framework allows for partial overlap with a score of 0.5. From the overlap data, we computed weighted recall and precision based on fractional count (Hatzivassiloglou and McKeown 1993). Recall is a ratio of weighted clause overlap between a peer and a reference sentence, and the number of clauses in a reference sentence. Precision is a ratio of weighted clause overlap between a peer and a reference sentence, and the number of clauses in a peer sentence. 4.1.5 Grammaticality Assessment. Grammaticality was rated in three categories: grammatical (3), partially grammatical (2), and not grammatical (1). The judge was instructed to rate a sentence in the grammatical category if it contained no grammatical mistakes. Partially grammatical included sentences that contained at most o"
J05-3002,A00-2024,1,0.873365,"which are not part of the intersection are pruned off the basis tree. However, removing all such subtrees may result in an ungrammatical or semantically flawed sentence; for example, we might create a sentence without a subject. This overpruning may happen if either the input to the fusion algorithm is noisy or the alignment has failed to recognize similar subtrees. Therefore, we perform a more conservative pruning, deleting only the self-contained components which can be removed without leaving ungrammatical sentences. As previously observed in the literature (Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000), such components include a clause in the clause conjunction, relative clauses, and some elements within a clause (such as adverbs and prepositions). For example, this procedure transforms the lattice in Figure 6 into the pruned basis lattice shown in Figure 7 by deleting the clause the clash erupted and the verb phrase to better protect Israeli forces. These phrases are eliminated because they do not appear in the other sentences of the theme and at the same time their removal does not interfere with the well-formedness of the fusion sentence. Once these subtrees are removed, the fusion latti"
J05-3002,P95-1034,0,0.0287335,"ion algorithm needs to select among available alternatives. 311 Computational Linguistics Volume 31, Number 3 Linearization of the fusion sentence involves the selection of the best phrasing and placement of auxiliaries as well as the determination of optimal ordering. Since we do not have sufficient semantic information to perform such selection, our algorithm is driven by corpus-derived knowledge. We generate all possible sentences10 from the valid traversals of the fusion lattice and score their likelihood according to statistics derived from a corpus. This approach, originally proposed by Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method used in statistical generation. We trained a trigram model with Good–Turing smoothing over 60 megabytes of news articles collected by Newsblaster using the second version CMU–Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997). The sentence with the lowest length-normalized entropy (the best score) is selected as the verbalization of the fusion lattice. Table 4 shows several verbalizations produced by our algorithm from the central tree in Figure 7. Here, we can see that the lowestscoring sentence is both grammati"
J05-3002,lacatusu-etal-2004-multi,0,0.0494796,"Missing"
J05-3002,P98-1116,0,0.00683125,"ilable alternatives. 311 Computational Linguistics Volume 31, Number 3 Linearization of the fusion sentence involves the selection of the best phrasing and placement of auxiliaries as well as the determination of optimal ordering. Since we do not have sufficient semantic information to perform such selection, our algorithm is driven by corpus-derived knowledge. We generate all possible sentences10 from the valid traversals of the fusion lattice and score their likelihood according to statistics derived from a corpus. This approach, originally proposed by Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method used in statistical generation. We trained a trigram model with Good–Turing smoothing over 60 megabytes of news articles collected by Newsblaster using the second version CMU–Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997). The sentence with the lowest length-normalized entropy (the best score) is selected as the verbalization of the fusion lattice. Table 4 shows several verbalizations produced by our algorithm from the central tree in Figure 7. Here, we can see that the lowestscoring sentence is both grammatical and concise. Table 4 also il"
J05-3002,N03-1020,0,0.162156,"this function is presented in the Appendix. In the resulting tree mapping, the pairs of nodes whose NodeSimilarity positively contributed to the alignment are considered parallel. Figure 5 shows two dependency trees and their alignment. As is evident from the Sim definition, we are considering only one-to-one node “matchings”: Every node in one tree is mapped to at most one node in another tree. This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures, such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003), show that these metrics do not correlate with human judgments on the fusion task, when tested against two reference outputs. This is to be expected: As lexical variability across input sentences grows, the number of possible ways to fuse them by machine as well by human also grows. The accuracy of match between the system output and the reference sentences largely depends on the features of the input sentences, rather than on the underlying fusion method. 6 Pairs of phrases that form an entry in the paraphrasing dictionary are compared as pairs of atomic entries. 306 Barzilay and McKeown Sen"
J05-3002,P99-1072,0,0.0488868,"Missing"
J05-3002,C96-1078,0,0.0688245,"Missing"
J05-3002,N03-2024,1,0.59867,"evel by taking context into account. In our current system, each sentence is generated in isolation, independently from what is said before and what will be said after. Clear evidence of the limitation of this approach is found in the selection of referring expressions. For example, all summary sentences may contain the full description of a named entity (e.g., President of Columbia University Lee Bollinger), while the use of shorter descriptions such as Bollinger or anaphoric expressions in some summary sentences would increase the summary’s readability (Schiffman, Nenkova, and McKeown 2002; Nenkova and McKeown 2003). These constraints can be incorporated into the sentence fusion algorithm, since our alignment-based representation of themes often contains several alternative descriptions of the same object. Beyond the problem of referring-expression generation, we found that by selecting appropriate paraphrases of each summary sentence, we can significantly improve the coherence of an output summary. An important research direction for future work is to develop a probabilistic text model that can capture properties of well-formed texts, just as a language model captures properties of sentence grammaticali"
J05-3002,N03-1024,0,0.014046,"Missing"
J05-3002,P02-1040,0,0.0809299,"f input trees. The pseudocode of this function is presented in the Appendix. In the resulting tree mapping, the pairs of nodes whose NodeSimilarity positively contributed to the alignment are considered parallel. Figure 5 shows two dependency trees and their alignment. As is evident from the Sim definition, we are considering only one-to-one node “matchings”: Every node in one tree is mapped to at most one node in another tree. This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures, such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003), show that these metrics do not correlate with human judgments on the fusion task, when tested against two reference outputs. This is to be expected: As lexical variability across input sentences grows, the number of possible ways to fuse them by machine as well by human also grows. The accuracy of match between the system output and the reference sentences largely depends on the features of the input sentences, rather than on the underlying fusion method. 6 Pairs of phrases that form an entry in the paraphrasing dictionary are compared as pairs of atomic entries"
J05-3002,W00-0403,0,0.0109965,"Missing"
J05-3002,J98-3005,1,0.416226,"the input documents and can synthesize information across sources. 1. Introduction Redundancy in large text collections, such as the Web, creates both problems and opportunities for natural language systems. On the one hand, the presence of numerous sources conveying the same information causes difficulties for end users of search engines and news providers; they must read the same information over and over again. On the other hand, redundancy can be exploited to identify important and accurate information for applications such as summarization and question answering (Mani and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke, Cormack, and Lynam 2001; Dumais et al. 2002; Chu-Carroll et al. 2003). Clearly, it would be highly desirable to have a mechanism that could identify common information among multiple related documents and fuse it into a coherent text. In this article, we present a method for sentence fusion that exploits redundancy to achieve this task in the context of multidocument summarization. A straightforward approach for approximating sentence fusion can be found in the use of sentence extraction for multidocument summarization (Carbonell and Goldstein 1998; Rade"
J05-3002,A00-1021,0,0.0132615,"Missing"
J05-3002,N03-1026,0,0.0113693,"the modifier is dropped, we get an anomaly. The second, more unusual problem is in the equation of Clinton/Dole, Dole/Clinton, and Clinton and Dole. 5. Related Work 5.1 Text-to-Text Generation Unlike traditional concept-to-text generation approaches, text-to-text generation methods take text as input and transform it into a new text satisfying some constraints (e.g., length or level of sophistication). In addition to sentence fusion, compression algorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003) and methods for expansion of a multiparallel corpus (Pang, Knight, and Marcu 2003) are other instances of such methods. Compression methods have been developed for single-document summarization, and they aim to reduce a sentence by eliminating constituents which are not crucial for understanding the sentence and not salient enough to include in the summary. These approaches are based on the observation that the “importance” of a sentence constituent can often be determined based on shallow features, such as its syntactic role and the words it contains. For example, in many cases a relative cl"
J05-3002,N03-1029,0,0.0150977,"procedure), the judge was asked to ignore punctuation. 4.2 Data To evaluate our sentence fusion algorithm, we selected 100 themes following the procedure described in the previous section. Each set varied from three to seven sentences, 13 We were unable to develop a set of rules which works in most cases. Punctuation placement is determined by a variety of features; considering all possible interactions of these features is hard. We believe that corpus-based algorithms for automatic restoration of punctuation developed for speech recognition applications (Beeferman, Berger, and Lafferty 1998; Shieber and Tao 2003) could help in our task, and we plan to experiment with them in the future. 315 Computational Linguistics Volume 31, Number 3 with 4.22 sentences on average. The generated fusion sentences consisted of 1.91 clauses on average. None of the sentences in the test set were fully extracted; on average, each sentence fused fragments from 2.14 theme sentences. Out of 100 sentence, 57 sentences produced by the algorithm combined phrases from several sentences, while the rest of the sentences comprised subsequences of one of the theme sentences. (Note that compression is different from sentence extract"
J05-3002,J02-4004,0,0.00615464,"s of salience. Since each of these scores has a different range of values, we perform ranking based on each score separately, then induce total ranking by summing ranks from individual categories: Rank (theme) = Rank (Number of sentences in theme) + Rank (Similarity of sentences in theme) + Rank (Sum of lexical chain scores in theme) Lexical chains—sequences of semantically related words—are tightly connected to the lexical cohesive structure of the text and have been shown to be useful for determining which sentences are important for single-document summarization (Barzilay and Elhadad 1997; Silber and McCoy 2002). In the multidocument scenario, lexical chains can be adapted for theme ranking based on the salience of theme sentences within their original documents. Specifically, a theme that has many sentences ranked high by lexical chains as important for a single-document summary is, in turn, given a higher salience score for the multidocument summary. In our implementation, a salience score for a theme is computed as the sum of lexical chain scores of each sentence in a theme. 2.3 Theme Ordering Once we filter out the themes that have a low rank, the next task is to order the selected themes into co"
J05-3002,J91-1002,0,\N,Missing
J05-3002,P02-1058,0,\N,Missing
J05-3002,C98-1112,0,\N,Missing
J08-1001,W03-1004,1,0.354824,"w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi [(w · xi ) + b] > 0 Finding the optimal hyperplane is an optimization problem which can be solved efﬁciently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classiﬁcation (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simpliﬁed articles from Britannica Elementary (214 articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difﬁcult.11 Examples of these two categories are given in Table 9. 11 The Britannica corpus was also used by Schwarm and Ostendorf (2005); in addition they make use of a corpus c"
J08-1001,P05-1018,1,0.142174,"Parameter Settings. In order to investigate the contribution of linguistic knowledge on model performance we experimented with a variety of grid representations resulting in different parameterizations of the feature space from which our model is learned. We focused on three sources of linguistic knowledge—syntax, coreference resolution, and salience—which play a prominent role in entity-based analyses of dis4 The collections are available from http://people.csail.mit.edu/regina/coherence/. 5 Short texts may have less than 20 permutations. The corpus described in the original ACL publication (Barzilay and Lapata 2005) contained a number of duplicate permutations. These were removed from the current version of the corpus. 12 Barzilay and Lapata Modeling Local Coherence course coherence (see Section 3.3 for details). An additional motivation for our study was to explore the trade-off between robustness and richness of linguistic annotations. NLP tools are typically trained on human-authored texts, and may deteriorate in performance when applied to automatically generated texts with coherence violations. We thus compared a linguistically rich model against models that use more impoverished representations. Mo"
J08-1001,N04-1015,1,0.127659,"nothing prevents the use of our feature vector representation for conventional classiﬁcation tasks. We offer an illustration in Experiment 3, where features extracted from entity grids are used to enhance the performance of a readability assessment system. Here, the learner takes as input a set of documents labeled with discrete classes (e.g., denoting whether a text is difﬁcult or easy to read) and learns to make predictions for unseen instances (see Section 6 for details on the machine learning paradigm we employ). 4. Experiment 1: Sentence Ordering Text structuring algorithms (Lapata 2003; Barzilay and Lee 2004; Karamanis et al. 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information10 Barzilay and Lapata Modeling Local Coherence bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. The information bearing items can be database entries (Karamanis et al. 2004), propositions (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). In sentence ordering, a document is viewed as a bag of sentenc"
J08-1001,P87-1022,0,0.275966,"Missing"
J08-1001,briscoe-carroll-2002-robust,0,0.0750339,"actic role. Such information can be expressed in many ways (e.g., using constituent labels or thematic role information). Because grammatical relations ﬁgure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reﬂecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [ O – – – – X ]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account."
J08-1001,A00-2018,0,0.0142276,"labels or thematic role information). Because grammatical relations ﬁgure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reﬂecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [ O – – – – X ]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account. Even though the same entity appears in different linguistic forms, for e"
J08-1001,P97-1003,0,0.0427463,"g constituent labels or thematic role information). Because grammatical relations ﬁgure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reﬂecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [ O – – – – X ]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account. Even though the same entity appears in different linguis"
J08-1001,J95-2003,0,0.872683,"syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classiﬁcation tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. 1. Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are oft"
J08-1001,P86-1031,0,0.244676,"Missing"
J08-1001,J04-4001,0,0.0506367,"hat our entity-based representation is well-suited for ranking-based generation and text classiﬁcation tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. 1. Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article f"
J08-1001,P95-1034,0,0.0200585,"oth the underlying discourse representation and the inference procedure. Thus, our work is complementary to computational models developed on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in"
J08-1001,N01-1025,0,0.00483851,"ts that crossed each other and ran the entire width and length of the town. Valletta was one of the ﬁrst towns to be laid out in this way. vector w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi [(w · xi ) + b] > 0 Finding the optimal hyperplane is an optimization problem which can be solved efﬁciently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classiﬁcation (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simpliﬁed articles from Britannica Elementary (214 articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difﬁcult.11 Examples of these"
J08-1001,P98-1116,0,0.00903236,"sentation and the inference procedure. Thus, our work is complementary to computational models developed on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in a ranking function. The top"
J08-1001,P03-1069,1,0.865056,"ied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article focuses on local coherence, which captures text relatedness at the level of sentence-to-sentence transitions. Local coherence is undoubtedly necessary for global coherence and has received considerable attention in computational linguistics (Foltz, Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller ∗ Computer Science and Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu. ∗∗ School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 2004; Karamanis et al. 2004). It is also supported by much psycholinguistic"
J08-1001,N03-1020,0,0.0639576,"Missing"
J08-1001,H01-1046,0,0.0122859,"their syntactic role. Such information can be expressed in many ways (e.g., using constituent labels or thematic role information). Because grammatical relations ﬁgure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reﬂecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [ O – – – – X ]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreferen"
J08-1001,W98-1411,0,0.0678039,"Missing"
J08-1001,P00-1052,0,0.251152,"tive speciﬁcations proposed in the literature, and demonstrate that the predictive power of the theory is highly sensitive to its parameter deﬁnitions. A common methodology for translating entity-based theories into computational models is to evaluate alternative speciﬁcations on manually annotated corpora. Some studies aim to ﬁnd an instantiation of parameters that is most consistent with observable data (Strube and Hahn 1999; Karamanis et al. 2004; Poesio et al. 2004). Other studies adopt a speciﬁc instantiation with the goal of improving the performance of a metric on a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with entity transition information, and show that the distribution of transitions correlates with human grades. Analogously, Hasler (2004) investigates whether Centering Theory can be used in evaluating the readability of automatic summaries by annotating human and machine generated extracts with entity transition information. The present work differs from these approaches in goal and methodology. Although our work builds upon existing linguistic theories, we do not aim to directly implement or reﬁne any of them in particular. We provide our model with sour"
J08-1001,J91-1002,0,0.274875,"reader to Barzilay (2003). Salience. Centering and other discourse theories conjecture that the way an entity is introduced and mentioned depends on its global role in a given discourse. We evaluate the impact of salience information by considering two types of models: The ﬁrst model treats all entities uniformly, whereas the second one discriminates between transitions of salient entities and the rest. We identify salient entities based on their frequency,3 following the widely accepted view that frequency of occurrence correlates with discourse prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991). To implement a salience-based model, we modify our feature generation procedure by computing transition probabilities for each salience group separately, and then 2 When evaluating the output of coreference algorithms, performance is typically measured using a model-theoretic scoring scheme proposed in Vilain et al. (1995). The scoring algorithm computes the recall error by taking each equivalence class S in the gold standard and determining the number of coreference links m that would have to be added to the system’s output to place all entities in S into the same equivalence class produced"
J08-1001,P02-1014,0,0.0212418,"nd the inﬂuence of salience. All these knowledge sources ﬁgure prominently in theories of discourse (see Section 2) and are considered important in determining coherence. Our results empirically validate the importance of salience and syntactic information (expressed by S, O, X, and –) for coherence-based models. The combination of both knowledge sources (Syntax+Salience) yields models with consistently good performance for all our tasks. The beneﬁts of full coreference resolution are less uniform. This is partly due to mismatches between training and testing conditions. The system we employ (Ng and Cardie 2002) was trained on human-authored newspaper texts. The corpora we used in our sentence ordering and readability assessment experiments are somewhat similar (i.e., human-authored narratives), whereas our summary coherence rating experiment employed machine generated texts. It is therefore not surprising that coreference resolution delivers performance gains on the ﬁrst two tasks but not on the latter (see Table 5 in Section 4 and Table 10 in Section 6.3). Our results further show that in lieu of an automatic coreference resolution system, entity classes can be approximated simply by string matchin"
J08-1001,P02-1040,0,0.0958007,"Missing"
J08-1001,J04-3003,0,0.0959281,"Missing"
J08-1001,P05-1065,0,0.62284,"automatically generated summaries. In both experiments, our method yields improvements over state-of-the-art models. We also show the beneﬁts of the entitybased representation in a readability assessment task, where the goal is to predict the comprehension difﬁculty of a given text. In contrast to existing systems which focus on intra-sentential features, we explore the contribution of discourse-level features to this task. By incorporating coherence features stemming from the proposed entity-based representation, we improve the performance of a state-of-the-art readability assessment system (Schwarm and Ostendorf 2005). In the following section, we provide an overview of entity-based theories of local coherence and outline previous work on its computational treatment. Then, we introduce our entity-based representation, and deﬁne its linguistic properties. In the subsequent sections, we present our three evaluation tasks, and report the results of our experiments. Discussion of the results concludes the article. 2. Related Work Our approach is inspired by entity-based theories of local coherence, and is well-suited for developing a coherence metric in the context of a ranking-based text generation system. We"
J08-1001,J01-4004,0,0.256041,"Missing"
J08-1001,J99-3001,0,0.0220453,"; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and Hahn 1999; Poesio et al. 2004), salience concerns how entities are realized in an utterance (e.g., whether they are they pronominalized or not). In other theories, salience is deﬁned in terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday and Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993). More reﬁned accounts expand the notion of salience from a binary distinction to a scalar one; examples include Prince’s (1981) familiarity scale, and Givon’s (1987) and Ariel’s (1988) givenness-continuum. The salience status of an entity is often reﬂected"
J08-1001,W04-3222,0,0.0206683,"Missing"
J08-1001,M95-1005,0,0.437278,"Missing"
J08-1001,J94-2003,0,0.0177928,"Missing"
J08-1001,N01-1003,0,0.0778537,"Missing"
J08-1001,P04-1050,0,\N,Missing
J08-1001,C98-1112,0,\N,Missing
J08-1001,P04-1051,0,\N,Missing
J08-1001,C69-7001,0,\N,Missing
J08-1001,C69-6902,0,\N,Missing
N03-1003,W99-0604,0,\N,Missing
N03-1003,C00-2172,0,\N,Missing
N03-1003,C02-1134,0,\N,Missing
N03-1003,C88-2088,0,\N,Missing
N03-1003,A94-1002,0,\N,Missing
N03-1003,P02-1040,0,\N,Missing
N03-1003,P01-1008,1,\N,Missing
N03-1003,N03-1024,0,\N,Missing
N03-1003,W02-1022,1,\N,Missing
N03-1003,P99-1044,0,\N,Missing
N03-1003,P79-1016,0,\N,Missing
N03-1003,P98-2127,0,\N,Missing
N03-1003,C98-2122,0,\N,Missing
N03-1003,P93-1024,1,\N,Missing
N04-1015,J98-3005,0,\N,Missing
N04-1015,W97-0304,0,\N,Missing
N04-1015,H01-1054,0,\N,Missing
N04-1015,A97-1004,0,\N,Missing
N04-1015,W03-1016,0,\N,Missing
N04-1015,W03-0418,0,\N,Missing
N04-1015,P94-1002,0,\N,Missing
N04-1015,N03-1003,1,\N,Missing
N04-1015,N03-1030,0,\N,Missing
N04-1015,P03-1069,0,\N,Missing
N04-1015,W90-0112,0,\N,Missing
N06-1046,W00-1425,0,0.663667,"San Francisco defenders. d. Holocomb threw to Davis for a leaping catch. After two incompletions in the first quarter, Holcomb found Davis among four San Francisco defenders for a leaping catch. Table 1: Aggregation example (in boldface) from a corpus of football summaries 1 Introduction Aggregation is an essential component of many natural language generation systems (Reiter and Dale, 2000). The task captures a mechanism for merging together two or more linguistic structures into a single sentence. Aggregated texts tend to be more concise, coherent, and more readable overall (Dalianis, 1999; Cheng and Mellish, 2000). Compare, for example, sentence (2) in Table 1 and its nonaggregated counterpart in sentences (1a)–(1d). The difference between the fluent aggregated sentence and its abrupt and redundant alternative is striking. The benefits of aggregation go beyond making texts less stilted and repetitive. Researchers in psycholinguistics have shown that by eliminating redundancy, aggregation facilitates text comprehension and recall (see Yeung (1999) and the references therein). Furthermore, Di Eugenio et al. (2005) demonstrate that aggregation can improve learning in the context of an intelligent tutoring"
N06-1046,P05-1007,0,0.0838581,"Missing"
N06-1046,W05-0618,0,0.301069,"del finds a globally optimal assignment that satisfies partitioninglevel constraints. The computational challenge lies in the complexity of such a model: we need to find an optimal partition in an exponentially large search space. Our approach is based on an Integer Linear Programming (ILP) formulation which can be effectively solved using standard optimization tools. ILP models have been successfully applied in several natural language processing tasks, including relation extraction (Roth and Yih, 2004), semantic role labeling (Punyakanok et al., 2004) and the generation of route directions (Marciniak and Strube, 2005). In the following section, we introduce our local pairwise model and afterward we present our global model for partitioning. 4.1 Learning Pairwise Similarity Our goal is to determine whether two database entries should be aggregated given the similarity of their shared attributes. We generate the training data by considering all pairs hei , ej i ∈ E × E, where E is the set of all entries attested in a given document. An entry pair forms a positive instance if its members belong to the same partition in the training data. For example, we will generate 8×7 2 unordered entry pairs for the eight"
N06-1046,C04-1197,0,0.0140205,"nce. Given the pairwise predictions of a local classifier, our model finds a globally optimal assignment that satisfies partitioninglevel constraints. The computational challenge lies in the complexity of such a model: we need to find an optimal partition in an exponentially large search space. Our approach is based on an Integer Linear Programming (ILP) formulation which can be effectively solved using standard optimization tools. ILP models have been successfully applied in several natural language processing tasks, including relation extraction (Roth and Yih, 2004), semantic role labeling (Punyakanok et al., 2004) and the generation of route directions (Marciniak and Strube, 2005). In the following section, we introduce our local pairwise model and afterward we present our global model for partitioning. 4.1 Learning Pairwise Similarity Our goal is to determine whether two database entries should be aggregated given the similarity of their shared attributes. We generate the training data by considering all pairs hei , ej i ∈ E × E, where E is the set of all entries attested in a given document. An entry pair forms a positive instance if its members belong to the same partition in the training data. For"
N06-1046,W04-2401,0,0.124076,"rements, our approach relies on global inference. Given the pairwise predictions of a local classifier, our model finds a globally optimal assignment that satisfies partitioninglevel constraints. The computational challenge lies in the complexity of such a model: we need to find an optimal partition in an exponentially large search space. Our approach is based on an Integer Linear Programming (ILP) formulation which can be effectively solved using standard optimization tools. ILP models have been successfully applied in several natural language processing tasks, including relation extraction (Roth and Yih, 2004), semantic role labeling (Punyakanok et al., 2004) and the generation of route directions (Marciniak and Strube, 2005). In the following section, we introduce our local pairwise model and afterward we present our global model for partitioning. 4.1 Learning Pairwise Similarity Our goal is to determine whether two database entries should be aggregated given the similarity of their shared attributes. We generate the training data by considering all pairs hei , ej i ∈ E × E, where E is the set of all entries attested in a given document. An entry pair forms a positive instance if its members belon"
N06-1046,W98-1415,0,0.694534,"igent tutoring application. In existing generation systems, aggregation typically comprises two processes: semantic grouping and sentence structuring (Wilkinson, 1995). The first process involves partitioning semantic content (usually the output of a content selection component) into disjoint sets, each corresponding to a single sentence. The second process is concerned with syntactic or lexical decisions that affect the realization of an aggregated sentence. To date, this task has involved human analysis of a domain-relevant corpus and manual development of aggregation rules (Dalianis, 1999; Shaw, 1998). The corpus analysis and knowledge engineering work in such an approach is substantial, prohibitively so in 359 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 359–366, c New York, June 2006. 2006 Association for Computational Linguistics large domains. But since corpus data is already used in building aggregation components, an appealing alternative is to try and learn the rules of semantic grouping directly from the data. Clearly, this would greatly reduce the human effort involved and ease porting generation systems to new domains. In"
N06-1046,N01-1003,0,0.0805436,"and sentence structuring are interleaved in one step, thus enabling the aggregation component to operate over a rich feature space. The common assumption is that other parts of the generation system are already in place during aggregation, and thus the aggregation component has access to discourse, syntactic, and lexical constraints. The interplay of different constraints is usually captured by a set of hand-crafted rules that guide the aggregation process (Scott and de Souza, 1990; Hovy, 1990; Dalianis, 1999; Shaw, 1998). Alternatively, these rules can be learned from a corpus. For instance, Walker et al. (2001) propose an overgenerate-and-rank approach to aggregation within the context of a spoken dialog application. Their system relies on a preference function for selecting an appropriate aggregation among multiple alternatives and assumes access to a large feature space expressing syntactic and pragmatic features of the input representations. The preference function is learned from a corpus of candidate aggregations marked with human ratings. Another approach is put forward by Cheng and Mellish (2000) who use a genetic algorithm in combination with a hand-crafted preference function to opportunist"
N06-1046,H05-1042,1,\N,Missing
N06-1058,P04-1079,0,0.00840198,"tly proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002). All these metrics compute n-gram overlap between a reference and a system output, but measure the overlap in different ways. Our method for reference paraphrasing can be combined with any of these metrics. In this paper, we report experiments with BLEU due to its wide use in the machine translation community. Recently, researchers have explored additional knowledge sources that could enhance automatic evaluation. Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005). Our work complements these approaches: we focus on the impact of paraphrases, and study their contribution to the accuracy of automatic evaluation. 3 Methods The input to our method consists of a reference sentence R = r1 . . . rm and a system-generated sentence W = w1 . . . wp whose words form the sets R and W respectively. The output of the model is a synthetic reference sentence SRW that preserves the meaning of R and has maximal word overlap with W . We generate such a sentence by substituting words from R with contextually equivalent words from W . Our algorit"
N06-1058,W05-0909,0,0.0341451,"e translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002). All these metrics compute n-gram overlap between a reference and a system output, but measure the overlap in different ways. Our method for reference paraphrasing can be combined with any of these metrics. In this paper, we report experiments with BLEU due to its wide use in the machine translation community. Recently, researchers have explored additional knowledge sources that could enhance automatic evaluation. Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005). Our work complements these approaches: we focus on the impact of paraphrases, and study their contribution to the accuracy of automatic evaluation. 3 Methods The input to our method consists of a reference sentence R = r1 . . . rm and a system-generated sentence W = w1 . . . wp whose words form the sets R and W respectively. The output of the model is a synthetic reference sentence SRW that preserves the meaning of R and has maximal word overlap with W . We generate such a sentence by substituting words from R with contextually equivalent words from W . Our algorithm first selects pairs of c"
N06-1058,N03-1003,1,0.798869,"ndard measures such as BLEU and ROUGE with paraphrasing information thereby taking more semantic knowledge into account. In the following section, we provide an overview of existing work on automatic paraphrasing. We then describe our paraphrasing algorithm and explain how it can be used in an automatic evaluation setting. Next, we present our experimental framework and data and conclude by presenting and discussing our results. 2 Related Work Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing, in particular, to sentence level paraphrasing (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). Most of these approaches learn paraphrases from a parallel or comparable monolingual corpora. Instances of such corpora include multiple English translations of the same source text written in a foreign language, and different news articles about the same event. For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation. Our approach differs from traditional work on automatic paraphrasing in goal and methodology. Unlike previous approa"
N06-1058,J92-4003,0,0.0205278,"cy. We use only adequacy scores, which measure how well content is preserved in the translation. 4.1.3 http://infomap-nlp.sourceforge.net 459 1 reference 0.9657 0.9674 0.9677 0.9652 0.9662 2 references 0.9743 0.9763 0.9764 0.9736 0.9744 Table 4: Pearson adequacy correlation scores for rewriting using one and two references, averaged over ten runs. Method WordNet ContextWN LSA Brown Alternative Paraphrasing Techniques To investigate the effect of paraphrase quality on automatic evaluation, we consider two alternative paraphrasing resources: Latent Semantic Analysis (LSA), and Brown clustering (Brown et al., 1992). These techniques are widely used in NLP applications, including language modeling, information extraction, and dialogue processing (Haghighi et al., 2005; Serafin and Eugenio, 2004; Miller et al., 2004). Both techniques are based on distributional similarity. The Brown clustering is computed by considering mutual information between adjacent words. LSA is a dimensionality reduction technique that projects a word co-occurrence matrix to lower dimensions. This lower dimensional representation is then used with standard similarity measures to cluster the data. Two words are considered to be a p"
N06-1058,J02-2001,0,0.019361,"s. Thus, among many possible paraphrases of the reference, we are interested only in those that use words appearing in the system output. Our paraphrasing algorithm is based on the substitute in context strategy. First, the algorithm identifies pairs of words from the reference and the system output that could potentially form paraphrases. We select these candidates using existing lexico-semantic resources such as WordNet. Next, the algorithm tests whether the candidate paraphrase is admissible in the context of the reference sentence. Since even synonyms cannot be substituted in any context (Edmonds and Hirst, 2002), this filtering step is necessary. We predict whether a word is appropriate in a new context by analyzing its distributional properties in a large body of text. Finally, paraphrases that pass the filtering stage are used to rewrite the reference sentence. We apply our paraphrasing method in the context of machine translation evaluation. Using this strategy, we generate a new sentence for every pair of human and machine translated sentences. This synthetic reference then replaces the original human reference in automatic evaluation. The key findings of our work are as follows: (1) Automaticall"
N06-1058,H05-1049,0,0.0694637,"writing that makes a reference closer to the system output. Thus, we focus on words that appear in the system output and aim to determine whether they can be used to rewrite a reference sentence. Our work also has interesting connections with research on automatic textual entailment (Dagan et al., 2005), where the goal is to determine whether a given sentence can be inferred from text. While we are not assessing an inference relation between a reference and a system output, the two tasks face similar challenges. Methods for entailment recognition extensively rely on lexico-semantic resources (Haghighi et al., 2005; Harabagiu et al., 2001), and we believe that our method for contextual substitution can be beneficial in that context. Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002). All these metrics compute n-gram overlap between a reference and a system output, but measure the overlap in different ways. Our method for reference paraphrasing can be combined with any of these metrics. In this paper, we report experiments with BLEU due to its wide use in the mac"
N06-1058,P01-1037,0,0.00546906,"ference closer to the system output. Thus, we focus on words that appear in the system output and aim to determine whether they can be used to rewrite a reference sentence. Our work also has interesting connections with research on automatic textual entailment (Dagan et al., 2005), where the goal is to determine whether a given sentence can be inferred from text. While we are not assessing an inference relation between a reference and a system output, the two tasks face similar challenges. Methods for entailment recognition extensively rely on lexico-semantic resources (Haghighi et al., 2005; Harabagiu et al., 2001), and we believe that our method for contextual substitution can be beneficial in that context. Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002). All these metrics compute n-gram overlap between a reference and a system output, but measure the overlap in different ways. Our method for reference paraphrasing can be combined with any of these metrics. In this paper, we report experiments with BLEU due to its wide use in the machine translation communit"
N06-1058,W04-3250,0,0.117067,"Missing"
N06-1058,C04-1072,0,0.0611857,"cores over BLEU. Moreover, in nine out of ten tests ContextWN outperforms the method based on WordNet. The results of statistical significance testing are summarized in Table 5. All the paraphrasing methods except LSA, exhibit higher correlation with human scores than plain BLEU. Our method significantly outperforms BLEU, and all the other paraphrasebased metrics. This consistent improvement confirms the importance of contextual filtering. 5 Depending on the experimental setup, correlation values can vary widely. Our scores fall within the range of previous researchers (Papineni et al., 2002; Lin and Och, 2004). 460 The third column in Table 4 shows that automatic paraphrasing continues to improve correlation scores even when two human references are paraphrased using our method. 4.3 Evaluation of Paraphrase Quality In the last section, we saw significant variations in MT evaluation performance when different paraphrasing methods were used to generate a synthetic reference. In this section, we examine the correlation between the quality of automatically generated paraphrases and their contribution to automatic evaluation. We analyze how the substitution frequency and the accuracy of those substituti"
N06-1058,N03-2021,0,0.0192653,"t al., 2005), where the goal is to determine whether a given sentence can be inferred from text. While we are not assessing an inference relation between a reference and a system output, the two tasks face similar challenges. Methods for entailment recognition extensively rely on lexico-semantic resources (Haghighi et al., 2005; Harabagiu et al., 2001), and we believe that our method for contextual substitution can be beneficial in that context. Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002). All these metrics compute n-gram overlap between a reference and a system output, but measure the overlap in different ways. Our method for reference paraphrasing can be combined with any of these metrics. In this paper, we report experiments with BLEU due to its wide use in the machine translation community. Recently, researchers have explored additional knowledge sources that could enhance automatic evaluation. Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005). Our work complements these app"
N06-1058,N04-1043,0,0.00684235,".9743 0.9763 0.9764 0.9736 0.9744 Table 4: Pearson adequacy correlation scores for rewriting using one and two references, averaged over ten runs. Method WordNet ContextWN LSA Brown Alternative Paraphrasing Techniques To investigate the effect of paraphrase quality on automatic evaluation, we consider two alternative paraphrasing resources: Latent Semantic Analysis (LSA), and Brown clustering (Brown et al., 1992). These techniques are widely used in NLP applications, including language modeling, information extraction, and dialogue processing (Haghighi et al., 2005; Serafin and Eugenio, 2004; Miller et al., 2004). Both techniques are based on distributional similarity. The Brown clustering is computed by considering mutual information between adjacent words. LSA is a dimensionality reduction technique that projects a word co-occurrence matrix to lower dimensions. This lower dimensional representation is then used with standard similarity measures to cluster the data. Two words are considered to be a paraphrase pair if they appear in the same cluster. We construct 1000 clusters employing the Brown method on 112 million words from the North American New York Times corpus. We keep the top 20 most frequen"
N06-1058,N03-1024,0,0.221211,"LEU and ROUGE with paraphrasing information thereby taking more semantic knowledge into account. In the following section, we provide an overview of existing work on automatic paraphrasing. We then describe our paraphrasing algorithm and explain how it can be used in an automatic evaluation setting. Next, we present our experimental framework and data and conclude by presenting and discussing our results. 2 Related Work Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing, in particular, to sentence level paraphrasing (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). Most of these approaches learn paraphrases from a parallel or comparable monolingual corpora. Instances of such corpora include multiple English translations of the same source text written in a foreign language, and different news articles about the same event. For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation. Our approach differs from traditional work on automatic paraphrasing in goal and methodology. Unlike previous approaches, we are not ai"
N06-1058,P02-1040,0,0.111797,"e goal is to determine whether a given sentence can be inferred from text. While we are not assessing an inference relation between a reference and a system output, the two tasks face similar challenges. Methods for entailment recognition extensively rely on lexico-semantic resources (Haghighi et al., 2005; Harabagiu et al., 2001), and we believe that our method for contextual substitution can be beneficial in that context. Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002). All these metrics compute n-gram overlap between a reference and a system output, but measure the overlap in different ways. Our method for reference paraphrasing can be combined with any of these metrics. In this paper, we report experiments with BLEU due to its wide use in the machine translation community. Recently, researchers have explored additional knowledge sources that could enhance automatic evaluation. Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005). Our work complements these approaches: we focus on the"
N06-1058,W04-3219,0,0.800806,"paraphrasing information thereby taking more semantic knowledge into account. In the following section, we provide an overview of existing work on automatic paraphrasing. We then describe our paraphrasing algorithm and explain how it can be used in an automatic evaluation setting. Next, we present our experimental framework and data and conclude by presenting and discussing our results. 2 Related Work Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing, in particular, to sentence level paraphrasing (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). Most of these approaches learn paraphrases from a parallel or comparable monolingual corpora. Instances of such corpora include multiple English translations of the same source text written in a foreign language, and different news articles about the same event. For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation. Our approach differs from traditional work on automatic paraphrasing in goal and methodology. Unlike previous approaches, we are not aiming to produce any p"
N06-1058,P04-1088,0,0.00683467,".9652 0.9662 2 references 0.9743 0.9763 0.9764 0.9736 0.9744 Table 4: Pearson adequacy correlation scores for rewriting using one and two references, averaged over ten runs. Method WordNet ContextWN LSA Brown Alternative Paraphrasing Techniques To investigate the effect of paraphrase quality on automatic evaluation, we consider two alternative paraphrasing resources: Latent Semantic Analysis (LSA), and Brown clustering (Brown et al., 1992). These techniques are widely used in NLP applications, including language modeling, information extraction, and dialogue processing (Haghighi et al., 2005; Serafin and Eugenio, 2004; Miller et al., 2004). Both techniques are based on distributional similarity. The Brown clustering is computed by considering mutual information between adjacent words. LSA is a dimensionality reduction technique that projects a word co-occurrence matrix to lower dimensions. This lower dimensional representation is then used with standard similarity measures to cluster the data. Two words are considered to be a paraphrase pair if they appear in the same cluster. We construct 1000 clusters employing the Brown method on 112 million words from the North American New York Times corpus. We keep t"
N07-1038,W06-3808,0,0.249182,"with our joint model. In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference. Our experimental results further confirm the strength of the Good Grief model. Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model. 2 Related Work Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003). More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale. While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect ranking tasks. This motivates our exploration of a new method for joint multiple aspect ranking. Ranking The ranki"
N07-1038,P06-1034,0,0.027539,"Missing"
N07-1038,P02-1047,0,0.222501,"proach fails to exploit meaningful dependencies between users’ judgments across different aspects. Knowledge of these dependencies can be crucial in predicting accurate ranks, as a user’s opinions on one aspect can influence his or her opinions on others. The algorithm presented in this paper models the dependencies between different labels via the agreement relation. The agreement relation captures whether the user equally likes all aspects of the item or whether he or she expresses different degrees of satisfaction. Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction. The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are 1 In this paper, ranking refers to the task of assigning an integer from 1 to k to each instance. This task is sometimes referred to as “ordinal regression” (Crammer and Singer, 2001) and “rating prediction” (Pang and Lee, 2005). 300 Proceedings of NAACL HLT 2007, pages 300–307, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics equal. The Good Grief decoding algorithm predi"
N07-1038,P05-1015,0,0.838944,"s all aspects of the item or whether he or she expresses different degrees of satisfaction. Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction. The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are 1 In this paper, ranking refers to the task of assigning an integer from 1 to k to each instance. This task is sometimes referred to as “ordinal regression” (Crammer and Singer, 2001) and “rating prediction” (Pang and Lee, 2005). 300 Proceedings of NAACL HLT 2007, pages 300–307, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics equal. The Good Grief decoding algorithm predicts a set of ranks – one for each aspect – which maximally satisfy the preferences of the individual rankers and the agreement model. For example, if the agreement model predicts consensus but the individual rankers select ranks h5, 5, 4i, then the decoder decides whether to trust the the third ranker, or alter its prediction and output h5, 5, 5i to be consistent with the agreement prediction. To obtain a model well-suited"
N07-1038,W02-1011,0,0.0436244,"encies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. 1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, multiple opinions on related matters are often intertwined throughout a text. For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant. Rather than lumping these aspects into a single score, we would like to capture each aspect of the writer’s opinion separately, thereby providing a more fine-grained view of opinions in the review. To this end, we aim to predict a set of numeric ranks that reflects the user’s satisfaction for each aspect. In the example above, we would as"
N07-1038,P02-1053,0,0.0190287,"gned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. 1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, multiple opinions on related matters are often intertwined throughout a text. For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant. Rather than lumping these aspects into a single score, we would like to capture each aspect of the writer’s opinion separately, thereby providing a more fine-grained view of opinions in the review. To this end, we aim to predict a set of numeric ranks that reflects the user’s satisfaction for each aspect. In the example above, we would assign a numeric"
N07-1038,W03-1017,0,0.173365,"is algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. 1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, multiple opinions on related matters are often intertwined throughout a text. For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant. Rather than lumping these aspects into a single score, we would like to capture each aspect of the writer’s opinion separately, thereby providing a more fine-grained view of opinions in the review. To this end, we aim to predict a set of numeric ranks that reflects the user’s satisfaction for each aspect. In the example above, we would assign a numeric rank from 1-5 for each of: food"
N07-1056,P00-1041,0,0.408655,"ility (Downey and Fellows, 1995). A problem is fixed parameter tractable if we can make the exponential dependence on the parameter k independent of the polynomial dependence on the problem size n. This is the case for our problem: as we will describe below, an algorithm of Alon et al. can be used to achieve a running time of roughly O(2k n2 ). In other words, the path length k only exponentiates a small constant, instead of the problem size n, while the dependence on n is in fact quadratic. 3 Related Work Decoding for selection-and-ordering problems is commonly implemented using beam search (Banko et al., 2000; Corston-Oliver et al., 2002; Jin and 446 Hauptmann, 2001). Being heuristic in nature, this algorithm is not guaranteed to find an optimal solution. However, its simplicity and time efficiency make it a decoding algorithm of choice for a wide range of NLP applications. In applications where beam decoding does not yield sufficient accuracy, researchers employ an alternative heuristic search, A* (Jelinek, 1969; Germann et al., 2001). While in some cases A* is quite effective, in other cases its running time and memory requirements may equal that of an exhaustive search. Time- and memorybounded"
N07-1056,W02-2105,0,0.0309467,"llows, 1995). A problem is fixed parameter tractable if we can make the exponential dependence on the parameter k independent of the polynomial dependence on the problem size n. This is the case for our problem: as we will describe below, an algorithm of Alon et al. can be used to achieve a running time of roughly O(2k n2 ). In other words, the path length k only exponentiates a small constant, instead of the problem size n, while the dependence on n is in fact quadratic. 3 Related Work Decoding for selection-and-ordering problems is commonly implemented using beam search (Banko et al., 2000; Corston-Oliver et al., 2002; Jin and 446 Hauptmann, 2001). Being heuristic in nature, this algorithm is not guaranteed to find an optimal solution. However, its simplicity and time efficiency make it a decoding algorithm of choice for a wide range of NLP applications. In applications where beam decoding does not yield sufficient accuracy, researchers employ an alternative heuristic search, A* (Jelinek, 1969; Germann et al., 2001). While in some cases A* is quite effective, in other cases its running time and memory requirements may equal that of an exhaustive search. Time- and memorybounded modifications of A* (i.e., ID"
N07-1056,P01-1030,0,0.11363,"be long makes it more similar to the the Traveling Salesman Problem (TSP). More precisely, our problem is an instance of the prize collecting traveling salesman problem, in which the salesman is required to visit k vertices at best cost (Balas, 1989; Awerbuch et al., 1995). Since our problem is NP-hard, we might be pessimistic about finding an exact solution. But our problem has an important feature: the length k of the path we want to find is small relative to the number of vertices n. This feature distinguishes our task from other decoding problems, such as decoding in machine translation (Germann et al., 2001), that are modeled using a standard TSP formulation. In general, the connection between n and k opens up a new range of solutions. For example, if we wanted to find the best length-2 path, we could simply try all subsets of 2 vertices in the graph, in all 2 possible orders. This is a set of only O(n2 ) possibilities, so we can check all to identify the best in polynomial time. This approach is very limited, however: in general, its runtime of O(nk ) for paths of length k makes it prohibitive for all but the smallest values of k. We cannot really hope to avoid the exponential dependence on k, b"
N07-1056,H01-1011,0,0.0302579,"th must terminate at t, and the constraint that at = 1 forces that termination to happen after exactly k + 1 edges.4 For those familiar with max-flow, our program can be understood as follows. The variables I force a flow, of value 1, from s to t. The variables f represent a flow with supply k + 1 at s and demand dv at v, being forced to obey “capacity constraints” that let the flow travel only along edges with I = 1. 6 Experimental Set-Up Task We applied our decoding algorithm to the task of title generation. This task has been extensively studied over the last six years (Banko et al., 2000; Jin and Hauptmann, 2001). Title generation is a classic selection-and-ordering problem: during title realization, an algorithm has to take into account both the likelihood of words appearing in the title and their ordering preferences. In the previous approaches, beam search has been used for decoding. Therefore, it is natural to explore more sophisticated decoding techniques like the ones described in this paper. Our method for estimation of selection-andordering preferences is based on the technique described in (Banko et al., 2000). We compute the 4 The network flow constraints allow us to remove the previously pl"
N07-1056,W04-2401,0,0.036221,"e search. Time- and memorybounded modifications of A* (i.e., IDA-A*) do not suffer from this limitation, but they are not guaranteed to find the exact solution. Nor do they provide bounds on the likelihood of finding the exact solution. Newly introduced methods based on local search can effectively examine large areas of a search space (Eisner and Tromble, 2006), but they still suffer from the same limitations. As an alternative to heuristic search algorithms, researchers also employ exact methods from combinatorial optimization, in particular integer linear programming (Germann et al., 2001; Roth and Yih, 2004). While existing ILP solvers find the exact solution eventually, the running time may be too slow for practical applications. Our randomized decoder represents an important departure from previous approaches to decoding selection-and-ordering problems. The theoretically established bounds on the performance of this algorithm enable us to explicitly control the tradeoff between the quality and the efficiency of the decoding process. This property of our decoder sets it apart from existing heuristic algorithms that cannot guarantee an arbitrarily high probability of success. 4 Randomized Decodin"
N09-1010,P07-1092,0,0.0144888,"Beyond Bilingual Learning While most work on multilingual learning focuses on bilingual analysis, some models operate on more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. His model first induces bilingual models for each pair of languages and then combines them. Our work takes a different approach by simultaneously learning from all languages, rather than combining bilingual results. A related thread of research is multi-source machine translation (Och and Ney, 2001; Utiyama and Isahara, 2006; Cohn and Lapata, 2007) where the goal is to translate from multiple source languages to a single target language. Rather than jointly training all the languages together, these models train bilingual models separately, and then use their output to select a final translation. The selection criterion can be learned at training time since these models have access to the correct translation. In unsupervised settings, however, we do not have a principled means for selecting among outputs of different bilingual models. By developing a joint multilingual model we can automatically achieve performance that rivals that of t"
N09-1010,erjavec-2004-multext,0,0.0360144,"tion with mean set to the previous value, and variance to one-tenth of the mean. 4 Experimental Setup We test our model in an unsupervised framework where only raw parallel text is available for each of the languages. In addition, we assume that for each language a tag dictionary is available that covers some subset of words in the text. The task is to learn an independent tagger for each language that can annotate non-parallel raw text using the learned parameters. All reported results are on non-parallel monolingual test data. Data For our experiments we use the MultextEast parallel corpus (Erjavec, 2004) which has been used before for multilingual learning (Feldman et al., 2006; Snyder et al., 2008). The tagged portion of the corpus includes a 100,000 word English text, Orwell’s novel “Nineteen Eighty Four”, and its translation into seven languages: Bulgarian, Czech, Estonian, Hungarian, Romanian, Slovene and Serbian. The corpus also includes a tag lexicon for each of these languages. We use the first 3/4 of the text for learning and the last 1/4 as held-out non-parallel test data. The corpus provides sentence level alignments. To obtain word level alignments, we run GIZA ++ (Och and Ney, 200"
N09-1010,feldman-etal-2006-cross,0,0.166213,"monolingual performance is cut by nearly two thirds. We also examined scenarios where the tag lexicon is reduced in size. In all cases, the multilingual model yielded substantial performance gains. Finally, we examined the performance of our model when trained on all possible subsets of the eight languages. We found that performance improves steadily as the number of available languages increases. 2 Related Work Bilingual Part-of-Speech Tagging Early work on multilingual tagging focused on projecting annotations from an annotated source language to a target language (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, we assume no labeled data at all; our unsupervised model instead symmetrically improves performance for all languages by learning cross-lingual patterns in raw parallel data. An additional distinction is that projection-based work utilizes pairs of languages, while our approach allows for continuous improvement as languages are added to the mix. In recent work, Snyder et al. (2008) presented a model for unsupervised part-of-speech tagging trained from a bilingual parallel corpus. This bilingual model and the model presented here share a number of similarities: both are Bayesian"
N09-1010,H05-1110,0,0.0684736,"exponentially in the number of languages. In addition, crossing alignments must be removed so that the resulting graph structure remains acyclic. In contrast, our multilingual model posits latent cross-lingual tags without explicitly joining or directly connecting the part-of-speech tags across languages. Besides permitting crossing alignments, this structure allows the model to scale gracefully with the number of lan84 guages. Beyond Bilingual Learning While most work on multilingual learning focuses on bilingual analysis, some models operate on more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. His model first induces bilingual models for each pair of languages and then combines them. Our work takes a different approach by simultaneously learning from all languages, rather than combining bilingual results. A related thread of research is multi-source machine translation (Och and Ney, 2001; Utiyama and Isahara, 2006; Cohn and Lapata, 2007) where the goal is to translate from multiple source languages to a single target language. Rather than jointly training all the languages together, these mode"
N09-1010,P07-1094,0,0.293151,"current tag given the previous tag and superlingual tags, and (ii) the next tag given the current tag and superlingual tags. These two quantities are similar to Distribution 1, except here we integrate over the transition parameter φyi−1 and the superlingual tag parameters ωzℓ . We end up with a product of integrals. Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and superlingual parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). For example, the closed form for integrating over the parameter of a superlingual tag with value z is given by: Z count(z, yi , ℓ) + ω0 ωzℓ (yi )P (ωzℓ |ω0 )dωzℓ = count(z, ℓ) + T ℓ ω0 where count(z, yi , ℓ) is the number of times that tag yi is observed together with superlingual tag z in language ℓ, count(z, ℓ) is the total number of times that superlingual tag z appears with an edge into language ℓ, and ω0 is a hyperparameter. The third term in the sampling formula is the emission probability of the current word xℓi given the current tag and all other words and sampled tags, as well as a"
N09-1010,2001.mtsummit-papers.46,0,0.0554812,"e gracefully with the number of lan84 guages. Beyond Bilingual Learning While most work on multilingual learning focuses on bilingual analysis, some models operate on more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. His model first induces bilingual models for each pair of languages and then combines them. Our work takes a different approach by simultaneously learning from all languages, rather than combining bilingual results. A related thread of research is multi-source machine translation (Och and Ney, 2001; Utiyama and Isahara, 2006; Cohn and Lapata, 2007) where the goal is to translate from multiple source languages to a single target language. Rather than jointly training all the languages together, these models train bilingual models separately, and then use their output to select a final translation. The selection criterion can be learned at training time since these models have access to the correct translation. In unsupervised settings, however, we do not have a principled means for selecting among outputs of different bilingual models. By developing a joint multilingual model we can auto"
N09-1010,J03-1002,0,0.002086,"(Erjavec, 2004) which has been used before for multilingual learning (Feldman et al., 2006; Snyder et al., 2008). The tagged portion of the corpus includes a 100,000 word English text, Orwell’s novel “Nineteen Eighty Four”, and its translation into seven languages: Bulgarian, Czech, Estonian, Hungarian, Romanian, Slovene and Serbian. The corpus also includes a tag lexicon for each of these languages. We use the first 3/4 of the text for learning and the last 1/4 as held-out non-parallel test data. The corpus provides sentence level alignments. To obtain word level alignments, we run GIZA ++ (Och and Ney, 2003) on all 28 pairings of the 8 languages. Since we want each latent superlingual variable to span as many languages as possible, we aggregate the pairwise lexical alignments into larger sets of aligned words. These sets of aligned words are generated as a preprocessing step. During sampling they remain fixed and are treated as observed data. We use the set of 14 basic part-of-speech tags provided by the corpus. In our first experiment, we assume that a complete tag lexicon is available, so that for each word, its set of possible parts-of-speech is known ahead of time. In this setting, the averag"
N09-1010,D08-1109,1,0.44881,"at scales well and shows improved performance for individual languages as the total number of languages increases. Languages exhibit ambiguity at multiple levels, making unsupervised induction of their underlying structure a difficult task. However, sources of linguistic ambiguity vary across languages. For example, the word fish in English can be used as either a verb or a noun. In French, however, the noun poisson (fish) is entirely distinct from the verbal form pˆecher (to fish). Previous work has leveraged this idea by building models for unsupervised learning from aligned bilingual data (Snyder et al., 2008). However, aligned data is often available for many languages. The benefits of bilingual learning vary markedly depending on which pair of languages is selected, and without labeled data it is unclear how to determine which supplementary language is most helpful. In this paper, we show that it is possible to leverage all aligned languages simultaneously, achieving accuracy that in most cases outperforms even optimally chosen bilingual pairings. Even in expressing the same meaning, languages take different syntactic routes, leading to variation in part-of-speech sequences. Therefore, an effecti"
N09-1010,N01-1026,0,0.131032,"supervised and supervised monolingual performance is cut by nearly two thirds. We also examined scenarios where the tag lexicon is reduced in size. In all cases, the multilingual model yielded substantial performance gains. Finally, we examined the performance of our model when trained on all possible subsets of the eight languages. We found that performance improves steadily as the number of available languages increases. 2 Related Work Bilingual Part-of-Speech Tagging Early work on multilingual tagging focused on projecting annotations from an annotated source language to a target language (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, we assume no labeled data at all; our unsupervised model instead symmetrically improves performance for all languages by learning cross-lingual patterns in raw parallel data. An additional distinction is that projection-based work utilizes pairs of languages, while our approach allows for continuous improvement as languages are added to the mix. In recent work, Snyder et al. (2008) presented a model for unsupervised part-of-speech tagging trained from a bilingual parallel corpus. This bilingual model and the model presented here share a number of similarit"
N09-1010,N07-1061,0,\N,Missing
N09-1042,N04-1015,1,0.413047,"topic assignment; the topic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies between adjacent words or blocks within a document. For instance, content models (Barzilay and Lee, 2004; Elsner et al., 2007) are implemented as HMMs, where the states correspond to topics of domain-specific information, and transitions reflect pairwise ordering preferences. Even approaches that break text into contiguous chunks (Titov and McDonald, 2008) assign topics based on local context. While these locally constrained models can implicitly reflect some discourse-level constraints, they cannot capture long-range dependencies without an explosion of the parameter space. In contrast, our model captures the entire sequence of topics using a compact representation. As a result, we can explicit"
N09-1042,M95-1004,0,0.122271,"rtion of paragraphs from that topic whose section heading is the same as the reference heading for that topic, yielding a precision score. High precision means that paragraphs assigned to a single topic usually correspond to the same section heading. The harmonic mean of recall and precision is the summary F-score. Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinear metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler and Maxwell, 2005). For the second task, we take the boundaries at which topics change within a document to be a segmentation of that document. We evaluate using the standard penalty metrics Pk and WindowDiff (Beeferman et al., 1999; Pevzner and Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the ends of the windows being improperly segmented with respect to each other. WindowDiff requires that the number of segmentation boundaries between the endpoints be correct as well.8 Our model takes a parameter K which controls the upper"
N09-1042,D08-1035,1,0.86325,"first task, we compare against the hidden topic Markov model (HTMM) (Gruber et al., 2007), which represents topic transitions between adjacent paragraphs in a Markovian fashion, similar to the approach taken in content modeling work. Note that HTMM can only capture local constraints, so it would allow topics to recur noncontiguously throughout a document. We also compare against the structure-agnostic approach of clustering the paragraphs using the CLUTO toolkit,9 which uses repeated bisection to maximize a cosine similarity-based objective. For the segmentation task, we compare to BayesSeg (Eisenstein and Barzilay, 2008),10 a Bayesian topic-based segmentation model that outperforms previous segmentation approaches (Utiyama and Isahara, 2001; Galley et al., 2003; Purver et al., 2006; Malioutov and Barzilay, 2006). BayesSeg enforces the topic contiguity constraint that motivated our model. We provide this baseline with the benefit of knowing the correct number of segments for each document, which is not provided to our system. Note that BayesSeg processes each document individually, so it cannot capture structural relatedness across documents. To investigate the importance of our ordering model, we consider two"
N09-1042,N07-1055,0,0.0238552,"pic assignments are auxiliary variables to the main task of language modeling. More recent work has attempted to adapt the concepts of topic modeling to more sophisticated representations than a bag of words; they use these representations to impose stronger constraints on topic assignments (Griffiths et al., 2005; Wallach, 2006; Purver et al., 2006; Gruber et al., 2007). These approaches, however, generally model Markovian topic or state transitions, which only capture local dependencies between adjacent words or blocks within a document. For instance, content models (Barzilay and Lee, 2004; Elsner et al., 2007) are implemented as HMMs, where the states correspond to topics of domain-specific information, and transitions reflect pairwise ordering preferences. Even approaches that break text into contiguous chunks (Titov and McDonald, 2008) assign topics based on local context. While these locally constrained models can implicitly reflect some discourse-level constraints, they cannot capture long-range dependencies without an explosion of the parameter space. In contrast, our model captures the entire sequence of topics using a compact representation. As a result, we can explicitly and tractably model"
N09-1042,P03-1069,0,0.0872758,"nald, 2008) assign topics based on local context. While these locally constrained models can implicitly reflect some discourse-level constraints, they cannot capture long-range dependencies without an explosion of the parameter space. In contrast, our model captures the entire sequence of topics using a compact representation. As a result, we can explicitly and tractably model global discourse-level constraints. 372 Modeling Ordering Constraints Sentence ordering has been extensively studied in the context of probabilistic text modeling for summarization and generation (Barzilay et al., 2002; Lapata, 2003; Karamanis et al., 2004). The emphasis of that body of work is on learning ordering constraints from data, with the goal of reordering new text from the same domain. Our emphasis, however, is on applications where ordering is already observed, and how that ordering can improve text analysis. From the methodological side, that body of prior work is largely driven by local pairwise constraints, while we aim to encode global constraints. 3 Problem Formulation Our document structure learning problem can be formalized as follows. We are given a corpus of D related documents. Each document expresse"
N09-1042,P06-1004,1,0.484411,"proach taken in content modeling work. Note that HTMM can only capture local constraints, so it would allow topics to recur noncontiguously throughout a document. We also compare against the structure-agnostic approach of clustering the paragraphs using the CLUTO toolkit,9 which uses repeated bisection to maximize a cosine similarity-based objective. For the segmentation task, we compare to BayesSeg (Eisenstein and Barzilay, 2008),10 a Bayesian topic-based segmentation model that outperforms previous segmentation approaches (Utiyama and Isahara, 2001; Galley et al., 2003; Purver et al., 2006; Malioutov and Barzilay, 2006). BayesSeg enforces the topic contiguity constraint that motivated our model. We provide this baseline with the benefit of knowing the correct number of segments for each document, which is not provided to our system. Note that BayesSeg processes each document individually, so it cannot capture structural relatedness across documents. To investigate the importance of our ordering model, we consider two variants of our model that alternately relax and tighten ordering constraints. In the constrained model, we require all documents to follow the same canonical ordering of topics. This is equival"
N09-1042,J02-1002,0,0.0324328,"monic mean of recall and precision is the summary F-score. Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinear metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler and Maxwell, 2005). For the second task, we take the boundaries at which topics change within a document to be a segmentation of that document. We evaluate using the standard penalty metrics Pk and WindowDiff (Beeferman et al., 1999; Pevzner and Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the ends of the windows being improperly segmented with respect to each other. WindowDiff requires that the number of segmentation boundaries between the endpoints be correct as well.8 Our model takes a parameter K which controls the upper bound on the number of latent topics. Note that our algorithm can select fewer than K topics for each document, so K does not determine the number 8 Statistical significance testing is not standardized and usually not reported for the segmentation task, so we omit the"
N09-1042,P06-1003,0,0.34116,"Missing"
N09-1042,W05-0908,0,0.0181644,"phs from that topic whose section heading is the same as the reference heading for that topic, yielding a precision score. High precision means that paragraphs assigned to a single topic usually correspond to the same section heading. The harmonic mean of recall and precision is the summary F-score. Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinear metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler and Maxwell, 2005). For the second task, we take the boundaries at which topics change within a document to be a segmentation of that document. We evaluate using the standard penalty metrics Pk and WindowDiff (Beeferman et al., 1999; Pevzner and Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the ends of the windows being improperly segmented with respect to each other. WindowDiff requires that the number of segmentation boundaries between the endpoints be correct as well.8 Our model takes a parameter K which controls the upper bound on the number of late"
N09-1042,P01-1064,0,0.211056,"between adjacent paragraphs in a Markovian fashion, similar to the approach taken in content modeling work. Note that HTMM can only capture local constraints, so it would allow topics to recur noncontiguously throughout a document. We also compare against the structure-agnostic approach of clustering the paragraphs using the CLUTO toolkit,9 which uses repeated bisection to maximize a cosine similarity-based objective. For the segmentation task, we compare to BayesSeg (Eisenstein and Barzilay, 2008),10 a Bayesian topic-based segmentation model that outperforms previous segmentation approaches (Utiyama and Isahara, 2001; Galley et al., 2003; Purver et al., 2006; Malioutov and Barzilay, 2006). BayesSeg enforces the topic contiguity constraint that motivated our model. We provide this baseline with the benefit of knowing the correct number of segments for each document, which is not provided to our system. Note that BayesSeg processes each document individually, so it cannot capture structural relatedness across documents. To investigate the importance of our ordering model, we consider two variants of our model that alternately relax and tighten ordering constraints. In the constrained model, we require all d"
N09-1042,P04-1050,0,\N,Missing
N09-1042,P03-1071,0,\N,Missing
N12-1008,D09-1014,0,0.0243549,"rds in a document with event boundaries based on the local surroundings of a candidate boundary. The resulting maximum aposteriori problem is: 72 θf (rf ) f ∈F where θf are the potential functions and {rf |f ⊆ {1, . . . , n}, f ∈ F } is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these components are given by the likelih"
N12-1008,P11-1098,0,0.0486312,"h decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task. Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck an"
N12-1008,P07-1036,0,0.273391,"Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task. Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck and McCallum, 2010). Constraints have been explored both at sentence and document level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. In our work we focus on document-level constraints. We utilize both discourse and record-coherence constraints to encourage consistency between local sequence models. There has also be"
N12-1008,P05-1022,0,0.0861833,"Missing"
N12-1008,P11-1054,1,0.840695,"cument level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. In our work we focus on document-level constraints. We utilize both discourse and record-coherence constraints to encourage consistency between local sequence models. There has also been unsupervised work that demonstrates the benefit of domain-specific constraints (Chen et al., 2011). In our work we show that domain-specific constraints based on the common structure of newspaper articles are also useful to guide a supervised model. 3 Model M AP (θ) = Problem Formulation Given a document, our goal is to extract field values and aggregate them into event records. The training data consists of event annotations where each word in the document is tagged with a field and with an event id. If a word is not a filler for a field, it is annotated with a default NULL field value. At test time, the number of events is not given and has to be inferred from the data. Model Structure O"
N12-1008,P03-1028,0,0.0218479,"ext of template extraction and motivate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. T"
N12-1008,J93-3001,0,0.184149,"Missing"
N12-1008,M92-1002,0,0.660563,"Missing"
N12-1008,W02-1001,0,0.0174696,"Missing"
N12-1008,N09-1037,0,0.0103682,"ng Potentials associate words in a document with event boundaries based on the local surroundings of a candidate boundary. The resulting maximum aposteriori problem is: 72 θf (rf ) f ∈F where θf are the potential functions and {rf |f ⊆ {1, . . . , n}, f ∈ F } is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these compone"
N12-1008,P05-1045,0,0.0576092,"Missing"
N12-1008,H92-1045,0,0.0173255,"l, thereby enabling multiple variable values for multi-event documents. The second record coherence potential — Record Density Potential — aims to reduce empty fields in the event record. This potential turns on when a local extractor fails to identify a filler for a field when processing a given event segment. If this segment contains words that are labeled as potential fillers in the context of other events in the training data, we prefer assignments that associate them with the field that otherwise would have been empty. This potential is inspired by the one sense per discourse constraint (Gale et al., 1992) that associates all the occurrences of the word in a document with the same semantic meaning. 1 The potential is defined for the following fields: Terrorist Organization, Weapon, City, and Country. 4 Inference Dual Decomposition The global potentials encode important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi , fj ∈ F , had been empty, we could have found the MAP assignment by solvin"
N12-1008,I11-1081,0,0.0422086,"Missing"
N12-1008,D10-1125,0,0.026922,"e important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi , fj ∈ F , had been empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as required by the infe"
N12-1008,P07-1075,0,0.0108687,"their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a line"
N12-1008,D07-1075,0,0.0737565,"Missing"
N12-1008,D09-1016,0,0.134713,"IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task"
N12-1008,M92-1008,0,0.0727864,"Missing"
N12-1008,D11-1001,0,0.0324077,"empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as required by the inference algorithm: M AP (θ) = max y,z X θj (rj ) + j∈J X X X min L(δ), L(δ) = δf j (rj )]+ max[θj (rj ) + r δ j j∈J f :j∈f X X δf j (rj )] max[θf (rf ) − rf rj f :j∈f end ← T RU E for f ∈ F do X δf j (rj )] rpkf = arg max[θf (rf ) − rf j∈f for j ∈ f do if rljk 6= rpkf j then gfkj (rljk ) + = 1 gfk"
N12-1008,W04-2401,0,0.203758,"Missing"
N12-1008,D10-1001,0,0.0460839,"al potentials encode important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi , fj ∈ F , had been empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as r"
N12-1008,C04-1078,0,0.0183377,"action and motivate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for re71 lations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the jo"
N12-1008,D10-1099,0,0.0208222,"boundaries based on the local surroundings of a candidate boundary. The resulting maximum aposteriori problem is: 72 θf (rf ) f ∈F where θf are the potential functions and {rf |f ⊆ {1, . . . , n}, f ∈ F } is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these components are given by the likelihoods of the corresp"
N12-1008,M92-1023,0,\N,Missing
N13-1103,D11-1039,0,0.878505,"A based technique can determine the equivalence of such expressions. It does this by leveraging the equational inference capabilities of the regular expression domain, making it a form of semantic unification. Thus, the contribution of our work is to show that using semantic unification to find a deeper level of equivalence helps to disambiguate language meanings. In many other domains of interest, determining semantic equivalence is important to the learning process. Previous work on such domains has focused on either heuristic or example-driven measures of semantic equivalence. For example, Artzi and Zettlemoyer (2011) estimate semantic equivalence using a heuristic loss function. Other past work has executed the logical form on an example world or in a situated context and then compared the outputs. This provides a very weak form of semantic equivalence valid only in that world/context (Clarke et al., 2010; Liang et al., 2009; Liang et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). In contrast, our work uses an exact, theoretically sound measure of semantic equivalence that determines whether two logical representations are equivalent in any context, i.e. on any input string. 3 3.1 Backgro"
N13-1103,P09-1010,1,0.804833,"ons from natural language using rule based techniques (Ranta, 1998), and also at automatically generating regular expressions from examples (Angluin, 1987). To the best of our knowledge, however, our work is the first to use training data to learn to automatically generate regular expressions from natural language. Language Grounding There is a large body of research mapping natural language to some form of meaning representation (Kate and Mooney, 2006; Kate et al., 2005; Raymond and Mooney, 2006; Thompson and Mooney, 2003; Wong and Mooney, 2006; Wong and Mooney, 2007; Zelle and Mooney, 1996; Branavan et al., 2009; Mihalcea et al., 2006; Poon and Domingos, 2009). In some of the considered domains the issue of semantic equivalence does not arise because of the way the data is generated. The most directly related work in these domains, is that by Kwiatkowski et al. (2010 and 2011) which is an extension of earlier work on CCG-based semantic parsing by Zettlemoyer and Collins (2005). Similar to our work, Kwiatkowski et al. utilize unification to find possible ways to decompose the logical form. However, they perform only syntactic unification. Syntactic unification determines equality using only variable s"
N13-1103,W10-2903,0,0.48134,"r level of equivalence helps to disambiguate language meanings. In many other domains of interest, determining semantic equivalence is important to the learning process. Previous work on such domains has focused on either heuristic or example-driven measures of semantic equivalence. For example, Artzi and Zettlemoyer (2011) estimate semantic equivalence using a heuristic loss function. Other past work has executed the logical form on an example world or in a situated context and then compared the outputs. This provides a very weak form of semantic equivalence valid only in that world/context (Clarke et al., 2010; Liang et al., 2009; Liang et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). In contrast, our work uses an exact, theoretically sound measure of semantic equivalence that determines whether two logical representations are equivalent in any context, i.e. on any input string. 3 3.1 Background Finding Regexp Equivalence Using DFAs Regular expressions can be equivalently represented as minimal DFAs, which are guaranteed to be equal 828 function sig. regexp function signature regexp cons(R,R,...) and(R,R,...) or(R,R,...) not(R) ab [a-b]&[b-c] a|b ˜(a) rep*(R) repminmax(I,I,R) repm"
N13-1103,W05-1506,0,0.0110274,"d feature counts. We define the features in our model over individual parse productions, admitting the use of dynamic programming to efficiently calculate the unconditioned expected counts. However, when we condition on generating the correct regular expression, as in the first term in (2), the calculation no longer factorizes, rendering exact algorithms computationally infeasible. To handle this, we use an approximate gradient calculation based on the n-best parses. Our n-best parser uses an efficient algorithm developed originally by (Jimenez and Marzal, 2000), and subsequently improved by (Huang and Chiang, 2005). This algorithm utilizes the fact that the first best parse, t1 , makes the optimal choice at each decision point, and the 2nd best parse, t2 must make the same optimal choice at every decision point, except for one. To execute on this intuition, the algorithm first calculates t1 by generating an unpruned CKYstyle parse forest which includes a priority queue of possible subparses for each constituent. The set of possible 2nd best parses T are those that choose the 2nd best subparse for exactly one constituent of t1 but are otherwise identical to t1 . The algorithm chooses t2 = arg maxt∈T p(t)"
N13-1103,P06-1115,0,0.375692,"fragments of the regular expression. InThe dataset used in this work is http://groups.csail.mit.edu/rbg/code/regexp/ available Regular Expression three letter word starting with ’X’ X[A-Za-z]{2} Figure 1: An example text description and its associated regular expression.3 Introduction 1 Text Description at ducing such an alignment during learning is particularly challenging because oftentimes even humans are unable to perform a fragment-by-fragment alignment. We can think of this task as an instance of grounded semantic parsing, similar to the work done in the domain of database queries (Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). However, the current success in semantic parsing relies on two important properties of the data. First, while the past work did not assume the alignment was given, they did assume that finding a fine grained fragmentby-fragment alignment was possible. Secondly, the semantic domains considered in the past were strongly typed. This typing provides constraints which significantly reduce the space of possible parses, thereby greatly reducing the ambiguity. However, in many interesting domains these two properties may not hold. In our doma"
N13-1103,D10-1119,0,0.167971,"ed in this work is http://groups.csail.mit.edu/rbg/code/regexp/ available Regular Expression three letter word starting with ’X’ X[A-Za-z]{2} Figure 1: An example text description and its associated regular expression.3 Introduction 1 Text Description at ducing such an alignment during learning is particularly challenging because oftentimes even humans are unable to perform a fragment-by-fragment alignment. We can think of this task as an instance of grounded semantic parsing, similar to the work done in the domain of database queries (Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). However, the current success in semantic parsing relies on two important properties of the data. First, while the past work did not assume the alignment was given, they did assume that finding a fine grained fragmentby-fragment alignment was possible. Secondly, the semantic domains considered in the past were strongly typed. This typing provides constraints which significantly reduce the space of possible parses, thereby greatly reducing the ambiguity. However, in many interesting domains these two properties may not hold. In our domain, the alignment between the natural language and the reg"
N13-1103,D11-1140,0,0.177948,"acilitates a direct mapping to the natural language description, we must find a regexp which does factorize and be able to compute its equivalence to the regexp we see in the training data. We compute this equivalence by converting each regexp to a minimal deterministic finite automaton (DFA) and leveraging the fact that minimal DFAs are guaranteed to be the same for semantically equivalent regexps (Hopcroft et al., 1979). We handle the additional ambiguity stemming from the weak typing in our domain through the use of a more effective parsing algorithm. The state of the art semantic parsers (Kwiatkowski et al., 2011; 827 Liang et al., 2011) utilize a pruned chart parsing algorithm which fails to represent many of the top parses and is prohibitively slow in the face of weak typing. In contrast, we use an n-best parser which always represents the most likely parses, and can be made very efficient through the use of the parsing algorithm from Jimenez and Marzal (2000). Our approach works by inducing a combinatory categorial grammar (CCG) (Steedman, 2001). This grammar consists of a lexicon which pairs words or phrases with regular expression functions. The learning process initializes the lexicon by pairing"
N13-1103,P09-1011,0,0.00781358,"e helps to disambiguate language meanings. In many other domains of interest, determining semantic equivalence is important to the learning process. Previous work on such domains has focused on either heuristic or example-driven measures of semantic equivalence. For example, Artzi and Zettlemoyer (2011) estimate semantic equivalence using a heuristic loss function. Other past work has executed the logical form on an example world or in a situated context and then compared the outputs. This provides a very weak form of semantic equivalence valid only in that world/context (Clarke et al., 2010; Liang et al., 2009; Liang et al., 2011; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). In contrast, our work uses an exact, theoretically sound measure of semantic equivalence that determines whether two logical representations are equivalent in any context, i.e. on any input string. 3 3.1 Background Finding Regexp Equivalence Using DFAs Regular expressions can be equivalently represented as minimal DFAs, which are guaranteed to be equal 828 function sig. regexp function signature regexp cons(R,R,...) and(R,R,...) or(R,R,...) not(R) ab [a-b]&[b-c] a|b ˜(a) rep*(R) repminmax(I,I,R) repmin(I,R) repexact(I,R"
N13-1103,P11-1060,0,0.309911,"the natural language description, we must find a regexp which does factorize and be able to compute its equivalence to the regexp we see in the training data. We compute this equivalence by converting each regexp to a minimal deterministic finite automaton (DFA) and leveraging the fact that minimal DFAs are guaranteed to be the same for semantically equivalent regexps (Hopcroft et al., 1979). We handle the additional ambiguity stemming from the weak typing in our domain through the use of a more effective parsing algorithm. The state of the art semantic parsers (Kwiatkowski et al., 2011; 827 Liang et al., 2011) utilize a pruned chart parsing algorithm which fails to represent many of the top parses and is prohibitively slow in the face of weak typing. In contrast, we use an n-best parser which always represents the most likely parses, and can be made very efficient through the use of the parsing algorithm from Jimenez and Marzal (2000). Our approach works by inducing a combinatory categorial grammar (CCG) (Steedman, 2001). This grammar consists of a lexicon which pairs words or phrases with regular expression functions. The learning process initializes the lexicon by pairing each sentence in the tra"
N13-1103,D09-1001,0,0.0199784,"hniques (Ranta, 1998), and also at automatically generating regular expressions from examples (Angluin, 1987). To the best of our knowledge, however, our work is the first to use training data to learn to automatically generate regular expressions from natural language. Language Grounding There is a large body of research mapping natural language to some form of meaning representation (Kate and Mooney, 2006; Kate et al., 2005; Raymond and Mooney, 2006; Thompson and Mooney, 2003; Wong and Mooney, 2006; Wong and Mooney, 2007; Zelle and Mooney, 1996; Branavan et al., 2009; Mihalcea et al., 2006; Poon and Domingos, 2009). In some of the considered domains the issue of semantic equivalence does not arise because of the way the data is generated. The most directly related work in these domains, is that by Kwiatkowski et al. (2010 and 2011) which is an extension of earlier work on CCG-based semantic parsing by Zettlemoyer and Collins (2005). Similar to our work, Kwiatkowski et al. utilize unification to find possible ways to decompose the logical form. However, they perform only syntactic unification. Syntactic unification determines equality using only variable substitutions and does not take advantage of the i"
N13-1103,W98-1308,0,0.882366,"Missing"
N13-1103,P06-2034,0,0.0127514,"natural language meaning. 2 Related Work Generating Regular Expressions Past work has looked at generating regular expressions from natural language using rule based techniques (Ranta, 1998), and also at automatically generating regular expressions from examples (Angluin, 1987). To the best of our knowledge, however, our work is the first to use training data to learn to automatically generate regular expressions from natural language. Language Grounding There is a large body of research mapping natural language to some form of meaning representation (Kate and Mooney, 2006; Kate et al., 2005; Raymond and Mooney, 2006; Thompson and Mooney, 2003; Wong and Mooney, 2006; Wong and Mooney, 2007; Zelle and Mooney, 1996; Branavan et al., 2009; Mihalcea et al., 2006; Poon and Domingos, 2009). In some of the considered domains the issue of semantic equivalence does not arise because of the way the data is generated. The most directly related work in these domains, is that by Kwiatkowski et al. (2010 and 2011) which is an extension of earlier work on CCG-based semantic parsing by Zettlemoyer and Collins (2005). Similar to our work, Kwiatkowski et al. utilize unification to find possible ways to decompose the logical"
N13-1103,N06-1056,0,0.164267,"egular Expressions Past work has looked at generating regular expressions from natural language using rule based techniques (Ranta, 1998), and also at automatically generating regular expressions from examples (Angluin, 1987). To the best of our knowledge, however, our work is the first to use training data to learn to automatically generate regular expressions from natural language. Language Grounding There is a large body of research mapping natural language to some form of meaning representation (Kate and Mooney, 2006; Kate et al., 2005; Raymond and Mooney, 2006; Thompson and Mooney, 2003; Wong and Mooney, 2006; Wong and Mooney, 2007; Zelle and Mooney, 1996; Branavan et al., 2009; Mihalcea et al., 2006; Poon and Domingos, 2009). In some of the considered domains the issue of semantic equivalence does not arise because of the way the data is generated. The most directly related work in these domains, is that by Kwiatkowski et al. (2010 and 2011) which is an extension of earlier work on CCG-based semantic parsing by Zettlemoyer and Collins (2005). Similar to our work, Kwiatkowski et al. utilize unification to find possible ways to decompose the logical form. However, they perform only syntactic unific"
N13-1103,P07-1121,0,0.115012,"work has looked at generating regular expressions from natural language using rule based techniques (Ranta, 1998), and also at automatically generating regular expressions from examples (Angluin, 1987). To the best of our knowledge, however, our work is the first to use training data to learn to automatically generate regular expressions from natural language. Language Grounding There is a large body of research mapping natural language to some form of meaning representation (Kate and Mooney, 2006; Kate et al., 2005; Raymond and Mooney, 2006; Thompson and Mooney, 2003; Wong and Mooney, 2006; Wong and Mooney, 2007; Zelle and Mooney, 1996; Branavan et al., 2009; Mihalcea et al., 2006; Poon and Domingos, 2009). In some of the considered domains the issue of semantic equivalence does not arise because of the way the data is generated. The most directly related work in these domains, is that by Kwiatkowski et al. (2010 and 2011) which is an extension of earlier work on CCG-based semantic parsing by Zettlemoyer and Collins (2005). Similar to our work, Kwiatkowski et al. utilize unification to find possible ways to decompose the logical form. However, they perform only syntactic unification. Syntactic unific"
N13-1103,D07-1071,0,0.309376,"Missing"
N13-1103,P09-1110,0,0.0853187,"Missing"
N13-1103,Q13-1005,0,\N,Missing
N15-1005,W13-4916,0,0.0474771,"Missing"
N15-1005,D12-1133,0,0.0834933,"Missing"
N15-1005,Q13-1034,0,0.0481971,"Missing"
N15-1005,D07-1022,0,0.09079,"n is an appealing alternative for pipeline architectures (Goldberg and Tsarfaty, 2008; Hatori et al., 2012; Habash and Rambow, 2005; GahbicheBraham et al., 2012; Zhang and Clark, 2008; Bohnet and Nivre, 2012). These approaches have been particularly prominent for languages with difficult preprocessing, such as morphologically rich languages (e.g., Arabic and Hebrew) and languages that require word segmentation (e.g., Chinese). For the former, joint prediction models typically rely on a lattice structure to represent alternative morphological analyses (Goldberg and Tsarfaty, 2008; Tratz, 2013; Cohen and Smith, 2007). For instance, transitionbased models intertwine operations on the lattice with operations on a dependency tree. Other joint architectures are more decoupled: in Goldberg and Tsarfaty (2008), a lattice is used to derive the best morphological analysis for each part-of-speech alternative, which is in turn provided to the parsing algorithm. In both cases, tractable inference is achieved by limiting the representation power of the scoring function. Our model also uses a lattice to encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full pat"
N15-1005,W02-1001,0,0.186961,"words. System Variants We also compare against a pipeline variation of our model. In our pipeline model, we predict segmentations and POS tags by the same system that we use to generate candidates. The subsequent standard parsing model then operates on the predicted segmentations and POS tags. 5.5 Experimental Details Following our earlier work (Zhang et al., 2014b), we train a first-order classifier to prune the dependency tree space.10 Following common practice, we average parameters over all iterations after training with passive-aggressive online learning algorithm (Crammer et al., 2006; Collins, 2002). We use the same adaptive random restart strategy as in our earlier work (Zhang et al., 2014b) and set K = 300. In addition, we also apply an aggressive early-stop strategy during training for efficiency. If we have found a violation against the ground truth during the first 50 iterations, we immediately stop and update the parameters based on the current violation. The reasoning behind this early-stop strategy is that weaker violations for some training sentences are already sufficient for separable training sets (Huang et al., 2012). 6 Results Comparison to State-of-the-art Systems Table 4"
N15-1005,darwish-etal-2014-using,1,0.297104,"entence, MADA provides a list of possible morphological analyses and POS tags, each associated with a score. The score of each segmentation or POS tag equals the highest score of the MADA analysis in which it appears. In addition, we associate each segmentation with MADA analyses on gender, number and person. Figure 5 shows an example of MADA output for the token Emlyp and the corresponding lattice structure. Classical Arabic We construct the lattice for this corpus in a similar fashion to the SPMRL dataset with two main departures. First, we use the Arabic morphological analyzer developed by Darwish et al. (2014) because MADA is primarily trained for MSA and performs poorly on classical Arabic. Second, we implement a CRF-based morpheme-level POS tagger and generate the POS tag candidates for each morpheme based on their marginal probabilities, truncated by a probability threshold. CTB5 We first re-train the Stanford Chinese word segmenter on CTB5 and generate a top-10 list for each sentence.7 We treat the word boundaries shared across all the 10 candidates as the confident ones, 7 We use 10-fold cross validation to avoid overfitting on the training set. 47 and construct the lattice as described in Sec"
N15-1005,gahbiche-braham-etal-2012-joint,0,0.0290913,"Missing"
N15-1005,P11-2124,0,0.110668,"Missing"
N15-1005,P08-1043,0,0.381847,"Missing"
N15-1005,P05-1071,0,0.302265,"Missing"
N15-1005,P09-2056,0,0.0516058,"Missing"
N15-1005,I11-1136,0,0.104871,"marginal probabilities, truncated by a probability threshold. CTB5 We first re-train the Stanford Chinese word segmenter on CTB5 and generate a top-10 list for each sentence.7 We treat the word boundaries shared across all the 10 candidates as the confident ones, 7 We use 10-fold cross validation to avoid overfitting on the training set. 47 and construct the lattice as described in Section 3.4. Our model then focuses on disambiguating the rest of the word boundaries in the candidates. To generate POS candidates, we apply a CRF-based tagger with Chinese-specific features used in previous work (Hatori et al., 2011). 5.3 Evaluation Measures Following standard practice in previous work (Hatori et al., 2012; Zhang et al., 2014a), we use Fscore as the evaluation metric for segmentation, POS tagging and dependency parsing. We report the morpheme-level F-score for Arabic and the wordlevel F-score for Chinese. In addition, we use TedEval (Tsarfaty et al., 2012) to evaluate the joint prediction on the SPMRL dataset, because TedEval score is the only evaluation metric used in the official report. We directly use the evaluation tools provided on the SPMRL official website.8 5.4 Baselines State-of-the-Art Systems"
N15-1005,P12-1110,0,0.461808,"to the dependency parse. The search space for the algorithm is a combination of parse trees and lattices that encode alternative morphological and POS analyses. The inference algorithm greedily searches over this space, iteratively making local modifications to POS tags and dependency trees. To overcome local optima, we employ multiple restarts. This simple, yet powerful approach can be easily applied to a range of joint prediction tasks. In prior work, joint models have been designed for a specific language. For instance, joint models for Chinese are designed with word segmentation in mind (Hatori et al., 2012), while algorithms for processing Semitic languages are tailored for morpho42 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 42–52, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics logical analysis (Tratz, 2013; Goldberg and Elhadad, 2011). In contrast, we show that our algorithm can be effortlessly applied to all these distinct languages. Language-specific characteristics drive the lattice construction and the feature selection, while the learning and inference methods are languageagnostic. We ev"
N15-1005,N12-1015,0,0.0242024,"Missing"
N15-1005,D11-1109,0,0.508816,"Missing"
N15-1005,W13-4910,0,0.123985,"ht−1 , t0 i, ht−2 , t0 i, ht−1 , t0 , w−1 i, ht−1 , t0 , w0 i ht−1 , t0 , t1 i, ht−2 , t0 , t1 , i, ht−1 , t0 , t2 i, ht−2 , t0 , t2 i ht−2 , t−1 , t0 , t+1 i, ht−2 , t−1 , t0 , t2 i, ht−2 , t0 , t1 , t2 i ht−2 , t−1 , t0 , t1 , t2 i ht0 , pre1 (w0 )i, ht0 , pre2 (w0 )i, ht0 , suf1 (w0 )i, ht0 , suf2 (w0 )i, ht0 , cn (w0 )i, ht0 , len(w0 )i Experimental Setup Table 2: Statistics of datasets. Morphologically Rich Languages (SPMRL) Shared Task 2013 (Seddah et al., 2013).5 We follow the official split for training, development and testing set. We use the core set of 12 POS categories provided by Marton et al. (2013). In the second Arabic dataset, the training set is a dependency conversion of the Arabic Treebank, which primarily includes Modern Standard Arabic (MSA) text. However, we test on a new corpus, which consists of classical Arabic text obtained from the Comprehensive Islamic Library (CIS).6 A native Arabic speaker with background in computational linguistics annotated the morphological segmentation and POS tags. This corpus is an excellent testbed for a joint model because classical Arabic may use rather different vocabulary from MSA, while their syntactic grammars are very similar to each other"
N15-1005,D12-1046,0,0.0923521,"h has demonstrated that joint prediction alleviates error propagation inherent in pipeline architectures, where mistakes cascade from one task to the next (Bohnet et 1 The source code is available at https://github. com/yuanzh/SegParser. Kareem Darwish ALT Research Group Qatar Computing Research Institute kdarwish@qf.org.qa al., 2013; Tratz, 2013; Hatori et al., 2012; Zhang et al., 2014a). However, jointly modeling all the processing tasks inevitably increases inference complexity. Prior work addressed this challenge by introducing constraints on scoring functions to keep inference tractable (Qian and Liu, 2012). In this paper, we propose a method for joint prediction that imposes no constraints on the scoring function. The method is able to handle high-order and global features for each individual task (e.g., parsing), as well as features that capture interactions between tasks. The algorithm achieves this flexibility by operating over full assignments that specify segmentation, POS tags and dependency tree, moving from one complete configuration to another. Our approach is based on the randomized greedy algorithm from our earlier dependency parsing system (Zhang et al., 2014b). We extend this algor"
N15-1005,W13-4904,0,0.123442,"int prediction is an appealing alternative for pipeline architectures (Goldberg and Tsarfaty, 2008; Hatori et al., 2012; Habash and Rambow, 2005; GahbicheBraham et al., 2012; Zhang and Clark, 2008; Bohnet and Nivre, 2012). These approaches have been particularly prominent for languages with difficult preprocessing, such as morphologically rich languages (e.g., Arabic and Hebrew) and languages that require word segmentation (e.g., Chinese). For the former, joint prediction models typically rely on a lattice structure to represent alternative morphological analyses (Goldberg and Tsarfaty, 2008; Tratz, 2013; Cohen and Smith, 2007). For instance, transitionbased models intertwine operations on the lattice with operations on a dependency tree. Other joint architectures are more decoupled: in Goldberg and Tsarfaty (2008), a lattice is used to derive the best morphological analysis for each part-of-speech alternative, which is in turn provided to the parsing algorithm. In both cases, tractable inference is achieved by limiting the representation power of the scoring function. Our model also uses a lattice to encode alternative analyses. However, we employ this structure in a different way. The model"
N15-1005,P12-2002,0,0.0480518,"nd construct the lattice as described in Section 3.4. Our model then focuses on disambiguating the rest of the word boundaries in the candidates. To generate POS candidates, we apply a CRF-based tagger with Chinese-specific features used in previous work (Hatori et al., 2011). 5.3 Evaluation Measures Following standard practice in previous work (Hatori et al., 2012; Zhang et al., 2014a), we use Fscore as the evaluation metric for segmentation, POS tagging and dependency parsing. We report the morpheme-level F-score for Arabic and the wordlevel F-score for Chinese. In addition, we use TedEval (Tsarfaty et al., 2012) to evaluate the joint prediction on the SPMRL dataset, because TedEval score is the only evaluation metric used in the official report. We directly use the evaluation tools provided on the SPMRL official website.8 5.4 Baselines State-of-the-Art Systems For the SPMRL dataset, we directly compare with Bj¨orkelund et al. (2013). This system achieves the best TedEval score in the track of dependency parsing with predicted information and we directly republish the official result. We also compute the F-score of this system on each task using our own evaluation script.9 For the CTB5 dataset, we dir"
N15-1005,P14-1069,0,0.0737027,"alyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency parsing (Zhang et al., 2014b). Our analysis"
N15-1005,P08-1101,0,0.0142698,"also uses a lattice to encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency p"
N15-1005,D10-1082,0,0.360181,"boundaries common across all the top-k candidates as true word boundaries. The remaining tokens (i.e., strings between these boundaries) are treated as words to be further segmented and labeled with POS tags. Figure 3 shows an example of the Chinese word lattice structure we construct. Once the lattice is constructed, the joint prediction model is applied as described above. 4 Features Segmentation Features For both Arabic and Chinese, each segmentation is represented by its score from the preprocessing system, and by the corresponding morphemes (or words in Chinese). Following previous work (Zhang and Clark, 2010), we also add character-based features for Chinese word segmentation, including the first and the last characters in the word, and the length of the word. POS Tag Features Table 1 summarizes the POS tag features employed by the model. First, we use the feature templates proposed in our previous work on Arabic joint parsing and POS correction (Zhang et al., 2014c). In addition, we incorporate character-based features specifically designed for Chinese. These features are mainly inspired by previous transition-based models on Chinese joint POS tagging and word segmentation (Zhang and Clark, 2010)"
N15-1005,P14-1125,0,0.348305,"encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency parsing (Zhang et al."
N15-1005,D14-1109,1,0.831276,"encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency parsing (Zhang et al."
N15-1005,P14-1019,1,0.85485,"encode alternative analyses. However, we employ this structure in a different way. The model samples 43 the full path from the lattice, which corresponds to a valid segmentation and POS tagging assignment. Then the model improves the path and the corresponding tree via a hill-climbing strategy. This architecture allows us to incorporate arbitrary features for segmentation, POS tagging and parsing. In joint prediction models for Chinese, lattice structures are not typically used. Commonly these models are formulated in a transition-based framework at the character level (Zhang and Clark, 2008; Zhang et al., 2014a; Wang and Xue, 2014). While this formulation can handle a large space of possible word segmentations, it can only capture features that are instantiated based on the stack and queue status. Our approach offers two advantages over prior work: (1) we can incorporate arbitrary features for word segmentation and parsing; (2) we demonstrate that a lattice-based approach commonly used for other languages can be effectively utilized for Chinese. Randomized Greedy Inference Our prior work has demonstrated that a simple randomized greedy approach delivers near optimal dependency parsing (Zhang et al."
N15-1121,C10-3009,0,0.640623,"Missing"
N15-1121,W09-1207,0,0.306018,"function by combining a traditional feature scoring function with a tensor-based scoring function. 2 https://github.com/taolei87/RBGParser 1152 Predicate word Predicate POS Argument word Argument POS Pred. + arg. words Pred. + arg. POS Voice + pred. word Path Path + arg. POS Path + pred. POS Path + arg. word Path + pred. word Voice + pred. + arg. POS Voice + pred. POS Table 1: Templates for first-order semantic features. These features are also (optionally) combined with role labels. 3.1 Traditional Scoring Using Manually-designed Features In a typical feature-based approach (Johansson, 2009; Che et al., 2009), feature templates give rise to rich feature descriptions of the semantic structure. The score Ssem (x, ysyn , zsem ) is then defined as the inner product between the parameter vector and the feature vector. In the first-order arc-factored case, Ssem (x, ysyn , zsem ) = w · φ(x, ysyn , zsem ) X = w · φ(p, a, r), (p,a,r)∈zsem where w are the model parameters and φ(p, a, r) is the feature vector representing a single semantic arc (p, a, r) (we suppress its dependence on x and ysyn ). We also experiment with second order features, i.e., considering two arguments associated with the same predicat"
N15-1121,D09-1003,0,0.317743,"Missing"
N15-1121,W09-1205,0,0.0722058,"main test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full model utilizes 4-way tensor"
N15-1121,J02-3001,0,0.656237,"Zhao et al., 2009a). On three out of five languages, the tensor-based model outperforms this system. These results are particularly notable because the system of Zhao et al. (2009a) employs a rich set of language-specific features carefully engineered for this task. Finally, we demonstrate that using four-way tensor yields better performance than its three-way counterpart, highlighting the importance of modeling the relation between role labels and properties of the path. 2 Related Work A great deal of SRL research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among othe"
N15-1121,D09-1059,0,0.335342,"uild our scoring function by combining a traditional feature scoring function with a tensor-based scoring function. 2 https://github.com/taolei87/RBGParser 1152 Predicate word Predicate POS Argument word Argument POS Pred. + arg. words Pred. + arg. POS Voice + pred. word Path Path + arg. POS Path + pred. POS Path + arg. word Path + pred. word Voice + pred. + arg. POS Voice + pred. POS Table 1: Templates for first-order semantic features. These features are also (optionally) combined with role labels. 3.1 Traditional Scoring Using Manually-designed Features In a typical feature-based approach (Johansson, 2009; Che et al., 2009), feature templates give rise to rich feature descriptions of the semantic structure. The score Ssem (x, ysyn , zsem ) is then defined as the inner product between the parameter vector and the feature vector. In the first-order arc-factored case, Ssem (x, ysyn , zsem ) = w · φ(x, ysyn , zsem ) X = w · φ(p, a, r), (p,a,r)∈zsem where w are the model parameters and φ(p, a, r) is the feature vector representing a single semantic arc (p, a, r) (we suppress its dependence on x and ysyn ). We also experiment with second order features, i.e., considering two arguments associated wit"
N15-1121,P14-1130,1,0.649857,"re vectors that capture distinct facets of semantic dependence: predicate, argument, syntactic path and role label. By compressing this sparse representation into lower dimensions, we obtain dense representations for words (predicate, argument) and their connecting paths, uncovering meaningful interactions. The associated parameters are maintained as a four-way low-rank tensor, and optimized for SRL performance. Tensor modularity enables us to employ standard online algorithms for training. Our approach to SRL is inspired by recent success of our tensor-based approaches in dependency parsing (Lei et al., 2014). Applying analogous techniques to SRL brings about new challenges, however. The scoring function needs to reflect the highorder interactions between the predicate, argument, 1150 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1150–1160, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics their syntactic path and the corresponding role label. Therefore, we parametrize the scoring function as a four-way tensor. Generalization to high-order tensors also requires new initialization and update procedures"
N15-1121,Q13-1018,1,0.900111,"Missing"
N15-1121,S14-2082,0,0.0478503,"s, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computation"
N15-1121,J08-2003,1,0.868522,"dhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to prior work, our approach effectively learns lowdimensional representation of words and their roles, eliminating the need for heavy manual feature engineering. Finally, system combination approaches such as reranking typically outperform individual systems (Bj¨orkelund et al., 2010). Our method can be easily integrated as a component in one of those systems. In technical terms, our work builds on our recent tensor-based approach for dependency parsin"
N15-1121,D14-1045,0,0.684785,"tic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to prior work, our approach effectively learns lowdimensional representation of words and their roles, eliminating the need for heavy manual feature engineering. Finally, system combination approaches such as reranking typically outperform individual systems (Bj¨orkelund et al., 2010). Our method can be easily integrated as a component in one of those systems. In techn"
N15-1121,P03-1002,0,0.0928285,"Work A great deal of SRL research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature en"
N15-1121,W08-2121,1,0.852627,"Missing"
N15-1121,J08-2002,0,0.148737,"eady identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach c"
N15-1121,W04-3212,0,0.296176,"L research has been dedicated to designing rich, expressive features. The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in"
N15-1121,D14-1041,0,0.0571193,"re widely adopted by the SRL community. These features describe the predicate, the candidate argument, and the syntactic relation between them (path). Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams), 1151 several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005). More recent approaches explored a broader range of features. Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates. Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014). To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning. Although attractive from this perspective, the kernel-based approach comes with a high computational cost. In contrast to p"
N15-1121,C00-2137,0,0.0458716,"Missing"
N15-1121,D14-1109,1,0.838822,"is a very simple iterative algorithm and is used to find the largest eigenvalues and eigenvectors (or singular values and vectors in SVD case) of a matrix. Its generalization directly applies to our high-order tensor case. 5 Implementation Details Decoding Following Llu´ıs et al. (2013), the decoding of SRL is formulated as a bipartite maximum assignment problem, where we assign arguments to semantic roles for each predicate. We use the maximum weighted assignment algorithm (Kuhn, 1955). For syntactic dependency parsing, we employ the randomized hill-climbing algorithm from our previous work (Zhang et al., 2014). 1155 Input: sparse tensor T , rank number i and fixed rank-1 components P (j), Q(j), R(j) and S(j) for j = 1..(i − 1) Output: new component P (i), Q(i), R(i) and S(i). 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: Randomly initialize four unit vectors p, q, r and s P T 0 = T − j P (j) ⊗ Q(j) ⊗ R(j) ⊗ S(j) repeat p = hT 0 , −, q, r, si and normalize it q = hT 0 , p, −, r, si and normalize it r = hT 0 , p, q, −, si and normalize it s = hT 0 , p, q, r, −i norm = ksk22 until norm converges P (i) = p and Q(i) = q R(i) = r and S(i) = s Figure 2: The iterative power method for highorder tensor initialization."
N15-1121,W09-1209,0,0.434698,"secondary out-of-domain test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full mo"
N15-1121,W09-1208,0,0.304639,"secondary out-of-domain test set, in order to evaluate how well the model generalizes to a different domain. Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments. The predicates in each sentence are also given during both training and testing. However, we neither predict nor use the sense for each predicate. Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009). All approaches extensively explored rich features for the SRL task. We also compare with the stateof-the-art parser (Bj¨orkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009. This system combines the pipeline of dependency parser and semantic role labeler with a global reranker. Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014). We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors. Model Variants Our full mo"
N15-1121,J13-3006,1,\N,Missing
N15-1121,W09-1201,1,\N,Missing
N16-1126,D15-1159,0,0.0300414,"Missing"
N16-1126,W09-1210,0,0.0675706,"Missing"
N16-1126,W09-1207,0,0.0821715,"Missing"
N16-1126,D14-1082,0,0.0952923,"wing standard practice, we use unlabeled attachment scores (UAS) and labeled attachment scores (LAS) as evaluation measure3 . In order to compare with previous reported numbers, we exclude punctuations for PTB in the evaluation, and include punctuations for CoNLL-2009 for consistency. We use RBGParser4 , a state-of-the-art graphbased parser for predicting dependency trees, and then apply our labeling model to obtain the dependency label assignments. To demonstrate the effectiveness of our model on other systems, we also apply it on two additional parsers – Stanford Neural Shift-reduce Parser (Chen and Manning, 2014)5 and TurboParser (Martins et al., 2010)6 . In all reported experiments, we use the default suggested settings to run these parsers. The hyper-parameters of our labeling model are set as follows: r1 = 50, r2 = 30, C = 0.01. Labeling Performance To test the performance of our labeling method, we first train our model using the gold unlabeled dependency trees and evaluate the labeling accuray on CoNLL-2009. Table 3 presents the results. For comparison, we implement a combined system which adds a rich set of traditional, sparse features into the scoring function and jointly train the feature weig"
N16-1126,P15-1033,0,0.0205419,"LAS on 5 out of 7 datasets1 . 1 Regina Barzilay2 Introduction Traditionally in dependency parsing, the tasks of finding the tree structure and labeling the dependency arcs are coupled in a joint achitecture. While it has potential to eliminate errors propogated through a separated procedure, joint decoding introduces other sources of issues that can also lead to non-optimal labeling assignments. One of the issues arises from inexact algorithms adopted in order to solve the hard joint search problem. For instance, many parsers (Nivre et al., 2007; Titov and Henderson, 2007; Zhang et al., 2013; Dyer et al., 2015; Weiss et al., 2015) adopt greedy decoding such as beam search, which may prune away the correct labeling hypothesis in an early decoding stage. Another issue is caused by the absence of rich label 1 Our code is available at https://github.com/ shentianxiao/RBGParser/tree/labeling. In this work, we explore an alternative approach where the dependency labeling is applied as a separate procedure, alleviating the issues described above. The potential of this approach has been explored in early work. For instance, McDonald et al. (2006) applied a separate labeling step on top of the first-order M"
N16-1126,W09-1205,0,0.0805651,"Missing"
N16-1126,P14-1130,1,0.941178,"lgorithm. Second, it becomes relatively cheap to add rich label features given a fixed tree, and the exact algorithm still applies when high-order label features are included. However, due to performance issues, such approach has not been adopted by the top performing parsers. In this work, we show that the labeling procedure, when optimized with recent advanced techniques in parsing, can achieve very high speed and accuracy. Specifically, our approach employs the recent distributional representation learning technique for parsing. We apply and extend the low-rank tensor factorization method (Lei et al., 2014) to the second-order case to learn a joint scoring function over grand-head, head, modifier and their labels. Unlike the prior work which additionally requires 1089 Proceedings of NAACL-HLT 2016, pages 1089–1094, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics traditional sparse features to achieve state-of-the-art performance, our extention alone delivers the same level of accuracy, while being substantially faster. As a consequence, the labeling model can be applied either as a refinement (re-labeling) step on top of existing parsers with negligible"
N16-1126,D10-1004,0,0.0428006,"Missing"
N16-1126,H05-1066,0,0.220018,"Missing"
N16-1126,W06-2932,0,0.0492321,"ivre et al., 2007; Titov and Henderson, 2007; Zhang et al., 2013; Dyer et al., 2015; Weiss et al., 2015) adopt greedy decoding such as beam search, which may prune away the correct labeling hypothesis in an early decoding stage. Another issue is caused by the absence of rich label 1 Our code is available at https://github.com/ shentianxiao/RBGParser/tree/labeling. In this work, we explore an alternative approach where the dependency labeling is applied as a separate procedure, alleviating the issues described above. The potential of this approach has been explored in early work. For instance, McDonald et al. (2006) applied a separate labeling step on top of the first-order MSTParser. The benefit of such approach is two-fold. First, finding the optimal labeling assignment (once the tree structure is produced) can be solved via an exact dynamic programming algorithm. Second, it becomes relatively cheap to add rich label features given a fixed tree, and the exact algorithm still applies when high-order label features are included. However, due to performance issues, such approach has not been adopted by the top performing parsers. In this work, we show that the labeling procedure, when optimized with recen"
N16-1126,W09-1215,0,0.0712821,"Missing"
N16-1126,W07-2218,0,0.0124356,"the outputs of top parsers, achieving the best LAS on 5 out of 7 datasets1 . 1 Regina Barzilay2 Introduction Traditionally in dependency parsing, the tasks of finding the tree structure and labeling the dependency arcs are coupled in a joint achitecture. While it has potential to eliminate errors propogated through a separated procedure, joint decoding introduces other sources of issues that can also lead to non-optimal labeling assignments. One of the issues arises from inexact algorithms adopted in order to solve the hard joint search problem. For instance, many parsers (Nivre et al., 2007; Titov and Henderson, 2007; Zhang et al., 2013; Dyer et al., 2015; Weiss et al., 2015) adopt greedy decoding such as beam search, which may prune away the correct labeling hypothesis in an early decoding stage. Another issue is caused by the absence of rich label 1 Our code is available at https://github.com/ shentianxiao/RBGParser/tree/labeling. In this work, we explore an alternative approach where the dependency labeling is applied as a separate procedure, alleviating the issues described above. The potential of this approach has been explored in early work. For instance, McDonald et al. (2006) applied a separate la"
N16-1126,P15-1032,0,0.0133027,"atasets1 . 1 Regina Barzilay2 Introduction Traditionally in dependency parsing, the tasks of finding the tree structure and labeling the dependency arcs are coupled in a joint achitecture. While it has potential to eliminate errors propogated through a separated procedure, joint decoding introduces other sources of issues that can also lead to non-optimal labeling assignments. One of the issues arises from inexact algorithms adopted in order to solve the hard joint search problem. For instance, many parsers (Nivre et al., 2007; Titov and Henderson, 2007; Zhang et al., 2013; Dyer et al., 2015; Weiss et al., 2015) adopt greedy decoding such as beam search, which may prune away the correct labeling hypothesis in an early decoding stage. Another issue is caused by the absence of rich label 1 Our code is available at https://github.com/ shentianxiao/RBGParser/tree/labeling. In this work, we explore an alternative approach where the dependency labeling is applied as a separate procedure, alleviating the issues described above. The potential of this approach has been explored in early work. For instance, McDonald et al. (2006) applied a separate labeling step on top of the first-order MSTParser. The benefit"
N16-1126,P14-2107,0,0.0243534,"ize the parameter values in an online maximum soft-margin framework, minimizing the structural hinge loss: n o loss(Θ) = max S(ˆl) + kli − ˆlk1 − S(li ) ˆl where kli − ˆlk1 is the number of different labels between li and ˆl. We adjust parameters Θ by ∆Θ via passive-aggressive update:   loss(Θ) ∆Θ = max C, · δΘ kδΘk2 where δΘ = dloss(Θ) denotes the derivatives and C dΘ is a regularization hyper-parameter controlling the maximum step size of each update. To counteract over-fitting, we follow the common practice of averaging parameters over all iterations. Model Best Shared Task Bohnet (2010) Zhang and McDonald (2014) Alberti et al. (2015) RBG + our labeling Catalan UAS LAS - 87.86 - 87.45 91.41 87.91 92.31 89.17 87.31 91.37 88.29 Chinese UAS LAS - 79.17 - 76.99 82.87 78.57 83.34 79.50 77.24 82.16 77.12 Czech UAS LAS - 80.38 - 80.96 86.62 80.59 88.35 83.50 81.90 88.88 84.04 English UAS LAS - 89.88 - 90.33 92.69 90.01 92.37 90.21 90.04 92.75 90.38 German UAS LAS - 87.48 - 88.06 89.88 87.38 90.12 87.79 87.91 90.88 88.68 Japanese UAS LAS - 92.57 - 92.47 92.82 91.87 93.99 93.10 93.38 94.18 93.59 Spanish UAS LAS - 87.64 - 88.13 90.82 87.34 91.71 88.68 87.69 91.50 88.71 Table 2: Pipelined Results on CoNLL-2009."
N16-1126,D13-1093,0,0.0241579,"achieving the best LAS on 5 out of 7 datasets1 . 1 Regina Barzilay2 Introduction Traditionally in dependency parsing, the tasks of finding the tree structure and labeling the dependency arcs are coupled in a joint achitecture. While it has potential to eliminate errors propogated through a separated procedure, joint decoding introduces other sources of issues that can also lead to non-optimal labeling assignments. One of the issues arises from inexact algorithms adopted in order to solve the hard joint search problem. For instance, many parsers (Nivre et al., 2007; Titov and Henderson, 2007; Zhang et al., 2013; Dyer et al., 2015; Weiss et al., 2015) adopt greedy decoding such as beam search, which may prune away the correct labeling hypothesis in an early decoding stage. Another issue is caused by the absence of rich label 1 Our code is available at https://github.com/ shentianxiao/RBGParser/tree/labeling. In this work, we explore an alternative approach where the dependency labeling is applied as a separate procedure, alleviating the issues described above. The potential of this approach has been explored in early work. For instance, McDonald et al. (2006) applied a separate labeling step on top o"
N16-1126,D14-1109,1,0.902876,"Missing"
N16-1153,P13-4021,0,0.0197328,"Missing"
N16-1153,D15-1075,0,0.0326755,"summary of the noisy body, and the encoder-decoder model is trained to act as a denoising auto-encoder. Moreover, training a decoder for the title (rather than the body) is also much faster since titles tend to be short (around 10 words). The encoders pre-trained in this manner are subsequently fine-tuned according to the discriminative criterion described already in Section 3. 5 LSTMs LSTM cells (Hochreiter and Schmidhuber, 1997) have been used to capture semantic information across a wide range of applications, including machine translation and entailment recognition (Bahdanau et al., 2015; Bowman et al., 2015; Rockt¨aschel et al., 2016). Their success can be attributed to neural gates that adaptively read or discard information to/from internal memory states. Specifically, a LSTM network successively reads the input token xt , internal state ct−1 , as well as the visible state ht−1 , and generates the new states c t , ht : it = σ(Wi xt + Ui ht−1 + bi ) ft = σ(Wf xt + Uf ht−1 + bf ) ot = σ(Wo xt + Uo ht−1 + bo ) zt = tanh(Wz xt + Uz ht−1 + bz ) ct = it zt + ft ct−1 ht = ot tanh(ct ) where i, f and o are input, forget and output gates, respectively. Given the visible state sequence {hi }li=1 , we ca"
N16-1153,W14-4012,0,0.00502985,"Missing"
N16-1153,D14-1179,0,0.0156321,"Missing"
N16-1153,P15-2114,0,0.281941,"al., 2016). Previous work on question retrieval has modeled this task using machine translation, topic modeling and knowledge graph-based approaches (Jeon et al., 2005; Li and Manandhar, 2011; Duan et al., 2008; Zhou et al., 2013). More recent work relies on representation learning to go beyond word-based methods. For instance, Zhou et al. (2015) learn word embeddings using category-based metadata information for questions. They define each question as a distribution which generates each word (embedding) independently, and subsequently use a Fisher kernel to assess question similarities. Dos Santos et al. (2015) propose an approach which combines a convolutional neural network (CNN) and a bagof-words representation for comparing questions. In contrast to (Zhou et al., 2015), our model treats each question as a word sequence as opposed to a bag of words, and we apply a recurrent convolutional model as opposed to the traditional CNN model used by dos Santos et al. (2015) to map questions into meaning representations. Further, we propose a training paradigm that utilizes the entire corpus of unannotated questions in a semi-supervised manner. Recent work on answer selection on community QA forums, simila"
N16-1153,P08-1019,0,0.0102075,"Missing"
N16-1153,D14-1002,0,0.00488573,"mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exchange AskUbuntu dataset. and associated filters map local chunks (win"
N16-1153,D13-1176,0,0.0134402,"r to focus temporal averaging in these models on key pieces of the questions. Gating plays a similar role in LSTMs (Hochreiter and Schmidhuber, 1997), though LSTMs do not reach the same level of performance in our setting. Moreover, we counter the scattered annotations available from user-driven associations by training the model largely based on the entire unannotated corpus. The encoder is coupled with a decoder and trained to reproduce the title from the noisy question body. The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Rush et al., 2015). The resulting encoder is subsequently fine-tuned discriminatively on the basis of limited annotations yielding an additional performance boost. 3 http://askubuntu.com/ 1280 We evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos Santos et al., 2015). During training, we directly utilize noisy pairs readily available in the forum, but to have a realistic evaluation of the system performance, we manually annotate 8K pairs of questions. This clean data is used in two splits, one for development and"
N16-1153,P14-1062,0,0.00406558,"e also train three alternative benchmark encoders (LSTMs, GRUs and CNNs) for mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exch"
N16-1153,D14-1181,0,0.0115394,"ive benchmark encoders (LSTMs, GRUs and CNNs) for mapping questions to vector representations. LSTM and GRU-based encoders can be pre-trained analogously to RCNNs, and fine-tuned discriminatively. CNN encoders, on the other hand, are only trained discriminatively. While plausible, neither alternative reaches quite the same level of performance as our 1283 where i and r are input and reset gate respectively. Again, the GRUs can be trained in the same way. CNNs Convolutional neural networks (LeCun et al., 1998) have also been successfully applied to various NLP tasks (Kalchbrenner et al., 2014; Kim, 2014; Kim et al., 2015; Zhang et al., 2015; Gao et al., 2014). As models, they are different from LSTMs since the temporal convolution operation Corpus Training Dev Test # of unique questions Avg length of title Avg length of body # of unique questions # of user-marked pairs # of query questions # of annotated pairs Avg # of positive pairs per query # of query questions # of annotated pairs Avg # of positive pairs per query 167,765 6.7 59.7 12,584 16,391 200 200×20 5.8 200 200×20 5.5 Table 1: Various statistics from our Training, Dev, and Test sets derived from the Sept. 2014 Stack Exchange AskUbu"
N16-1153,D15-1180,1,0.218935,"Missing"
N16-1153,P11-1143,0,0.02256,"Missing"
N16-1153,S15-2047,1,0.46169,"Missing"
N16-1153,S16-1083,1,0.108575,"Missing"
N16-1153,D15-1044,0,0.015101,"s. Gating plays a similar role in LSTMs (Hochreiter and Schmidhuber, 1997), though LSTMs do not reach the same level of performance in our setting. Moreover, we counter the scattered annotations available from user-driven associations by training the model largely based on the entire unannotated corpus. The encoder is coupled with a decoder and trained to reproduce the title from the noisy question body. The methodology is reminiscent of recent encoder-decoder networks in machine translation and document summarization (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Rush et al., 2015). The resulting encoder is subsequently fine-tuned discriminatively on the basis of limited annotations yielding an additional performance boost. 3 http://askubuntu.com/ 1280 We evaluate our model on the AskUbuntu corpus from Stack Exchange used in prior work (dos Santos et al., 2015). During training, we directly utilize noisy pairs readily available in the forum, but to have a realistic evaluation of the system performance, we manually annotate 8K pairs of questions. This clean data is used in two splits, one for development and hyper parameter tuning and another for testing. We evaluate our"
N16-1153,P15-2116,0,0.0315029,"Missing"
N16-1153,P15-1025,0,0.0509211,"Missing"
N16-1156,W13-3520,0,0.0186408,", adjective, adverb, pronoun, determiner, adposition, numeral, conjunction, sentence conjunction, particle, punctuation mark, and a catch-all tag X. Note that this universal tagset contains two more tags than the traditional universal tagset proposed by Petrov et al. (2011): auxiliary verb and sentence conjunction. We follow the standard split of the treebanks for every language. For each target language, we use the sentences in the training set as unlabeled data, and evaluate on the testing set. Word Embeddings To induce monolingual word embeddings, we use the processed Wikipedia text dumps (Al-Rfou et al., 2013) for each language. 3 Examples of symbol mark include “-”, “/” etc. Language Tokens (106 ) English 1,888 Danish 44 German 687 Spanish 399 Finnish 66 Hungarian 89 Indonesian 41 Table 1: Number of tokens of the Wikipedia dumps used for inducing word embeddings. While Wikipedia texts may contain parallel articles, we show in Table 1 that the amount of text varies significantly across languages. Smith et al. (2010) also demonstrated that parallel information in Wikipedia is very noisy. Therefore, direct translations are difficult to get from these texts. We use the word2vec tool with the skip-gram"
N16-1156,N10-1083,0,0.0164184,"Missing"
N16-1156,W06-2920,0,0.0135676,"oun. More specifically, the goal is to predict word ordering preferences such as whether an adjective comes before a noun (as in English) or after a noun (as in Spanish). We collect the true ordering preferences from “The World Atlas of Language Structure (WALS)” (Dryer et al., 2005). To make predictions, we train a multiclass support vector machine (SVM) classifier (Tsochantaridis et al., 2004) on a multilingual corpus using bigrams and trigrams of POS tags as features. The training data for SVM comes from a combination of the Universal Dependencies Treebanks, CoNLL-X, and CoNLL-07 datasets (Buchholz and Marsi, 2006; Nilsson et al., 2007), excluding all sentences in the target language. We train one classifier for each typological property, and make predictions for each of the six target languages. For evaluation, we directly report the overall accuracy on all 30 test cases (six languages combined with five typological prop90 70 window=1 window=5 80 Accuracy Accuracy 70 60 50 65 60 40 Direct Transfer Transfer+EM Prototype 30 20 10 20 50 100 150 200 500 55 1000 # Translation Pairs or Prototypes Figure 2: Accuracy of our models and the prototype baseline as a function of the amount of supervision, in Germa"
N16-1156,W11-2208,0,0.0366213,"Missing"
N16-1156,P11-1061,0,0.0275008,"Missing"
N16-1156,I13-1177,0,0.0252514,"Missing"
N16-1156,D14-1096,0,0.0662188,"Missing"
N16-1156,D12-1001,0,0.0321311,"Moreover, emission probabilities are hand-engineered to capture language-specific morphological properties. In contrast, our method does not require any languagespecific knowledge on the target side. Multilingual Word Embeddings There is an expansive body of research on learning multilingual word embeddings (Gouws et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015; Lauly et al., 2014; Luong et al., 2015). Previous work has shown its effectiveness across a wide range of multilingual transfer tasks including tagging (Kim et al., 2015), syntactic parsing (Xiao and Guo, 2014; Guo et al., 2015; Durrett et al., 2012), and machine translation (Zou et al., 2013; Mikolov et al., 2013b). However, these approaches commonly require parallel sentences or bilingual lexicon to learn multilingual embeddings. Vulic and Moens (2015) have alleviated the requirements by inducing multilingual word embeddings directly from a document-aligned corpus such as a set of Wikipedia pages on the same theme but in different languages. However, they still used about ten thousands aligned documents as parallel supervision. Our work demonstrates that useful multilingual embeddings can be learned with a minimal amount of parallel sup"
N16-1156,E14-1049,0,0.27712,". The feasibility of POS tagging transfer without parallel data has been shown by Hana et al. (2004). The transfer is performed between typologically similar languages, which enables the model to directly transfer the transition probabilities from source to the target. Moreover, emission probabilities are hand-engineered to capture language-specific morphological properties. In contrast, our method does not require any languagespecific knowledge on the target side. Multilingual Word Embeddings There is an expansive body of research on learning multilingual word embeddings (Gouws et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015; Lauly et al., 2014; Luong et al., 2015). Previous work has shown its effectiveness across a wide range of multilingual transfer tasks including tagging (Kim et al., 2015), syntactic parsing (Xiao and Guo, 2014; Guo et al., 2015; Durrett et al., 2012), and machine translation (Zou et al., 2013; Mikolov et al., 2013b). However, these approaches commonly require parallel sentences or bilingual lexicon to learn multilingual embeddings. Vulic and Moens (2015) have alleviated the requirements by inducing multilingual word embeddings directly from a document-aligned corpus such as"
N16-1156,P15-1119,0,0.198896,"ce to the target. Moreover, emission probabilities are hand-engineered to capture language-specific morphological properties. In contrast, our method does not require any languagespecific knowledge on the target side. Multilingual Word Embeddings There is an expansive body of research on learning multilingual word embeddings (Gouws et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015; Lauly et al., 2014; Luong et al., 2015). Previous work has shown its effectiveness across a wide range of multilingual transfer tasks including tagging (Kim et al., 2015), syntactic parsing (Xiao and Guo, 2014; Guo et al., 2015; Durrett et al., 2012), and machine translation (Zou et al., 2013; Mikolov et al., 2013b). However, these approaches commonly require parallel sentences or bilingual lexicon to learn multilingual embeddings. Vulic and Moens (2015) have alleviated the requirements by inducing multilingual word embeddings directly from a document-aligned corpus such as a set of Wikipedia pages on the same theme but in different languages. However, they still used about ten thousands aligned documents as parallel supervision. Our work demonstrates that useful multilingual embeddings can be learned with a minimal"
N16-1156,N06-1041,0,0.0150329,"sses, such as punctuation marks, determiners and prepositions. We find translations using Wiktionary.4 Model Variants Our model varies along two dimensions. On one dimension, we use two different methods for inducing multilingual word embeddings: Pseudoinverse and Isometric alignment as described in Section 3.1. On the other dimension, we experiment with two different multilingual transfer models. We use Direct Transfer to denote our direct transfer model, and Transfer+EM for our unsupervised model trained in the target language. Baselines We also compare against the prototypedriven method of Haghighi and Klein (2006). Specifically, we use the publicly available implementation provided by the authors.5 Note that their model requires at least one prototype for each POS category. Therefore, we select 14 prototypes (the most frequent word from each category) for the baseline, while our method only uses ten translation pairs. 4 https://www.wiktionary.org/ http://code.google.com/p/ prototype-sequence-toolkit/ Evaluation Unlike other unsupervised methods, all models in our experiments can identify the label for each POS tag because of knowledge from either the source languages or prototypes. Therefore, we direct"
N16-1156,W04-3229,0,0.132416,"ese approaches assume access to a large amount of parallel sentences to facilitate multilingual transfer. In our work, we focus on a more challenging scenario, in which we do not assume access to parallel sentences. Instead of projecting tag information via word alignment, the transfer in our model is driven by mapping multilingual embedding spaces. Kim et al. (2015) also use latent word representations for multilingual transfer. However, similarly to prior work, this representation is learned using parallel data. The feasibility of POS tagging transfer without parallel data has been shown by Hana et al. (2004). The transfer is performed between typologically similar languages, which enables the model to directly transfer the transition probabilities from source to the target. Moreover, emission probabilities are hand-engineered to capture language-specific morphological properties. In contrast, our method does not require any languagespecific knowledge on the target side. Multilingual Word Embeddings There is an expansive body of research on learning multilingual word embeddings (Gouws et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015; Lauly et al., 2014; Luong et al., 2015). Previous work has"
N16-1156,D15-1003,0,0.017954,"Missing"
N16-1156,D15-1150,0,0.0296533,"ainly focused on the tag projection method (Yarowsky et al., 2001; Wisniewski et al., 2014; Duong et al., 2013; Duong et al., 2014; T¨ackstr¨om et al., 2013; Das and Petrov, 2011; Snyder et al., 2008; Naseem et al., 2009; Chen et al., 2011). All these approaches assume access to a large amount of parallel sentences to facilitate multilingual transfer. In our work, we focus on a more challenging scenario, in which we do not assume access to parallel sentences. Instead of projecting tag information via word alignment, the transfer in our model is driven by mapping multilingual embedding spaces. Kim et al. (2015) also use latent word representations for multilingual transfer. However, similarly to prior work, this representation is learned using parallel data. The feasibility of POS tagging transfer without parallel data has been shown by Hana et al. (2004). The transfer is performed between typologically similar languages, which enables the model to directly transfer the transition probabilities from source to the target. Moreover, emission probabilities are hand-engineered to capture language-specific morphological properties. In contrast, our method does not require any languagespecific knowledge o"
N16-1156,2005.mtsummit-papers.11,0,0.0723962,"Missing"
N16-1156,N15-1144,0,0.0206873,"Missing"
N16-1156,N15-1028,0,0.0315771,"tagging transfer without parallel data has been shown by Hana et al. (2004). The transfer is performed between typologically similar languages, which enables the model to directly transfer the transition probabilities from source to the target. Moreover, emission probabilities are hand-engineered to capture language-specific morphological properties. In contrast, our method does not require any languagespecific knowledge on the target side. Multilingual Word Embeddings There is an expansive body of research on learning multilingual word embeddings (Gouws et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015; Lauly et al., 2014; Luong et al., 2015). Previous work has shown its effectiveness across a wide range of multilingual transfer tasks including tagging (Kim et al., 2015), syntactic parsing (Xiao and Guo, 2014; Guo et al., 2015; Durrett et al., 2012), and machine translation (Zou et al., 2013; Mikolov et al., 2013b). However, these approaches commonly require parallel sentences or bilingual lexicon to learn multilingual embeddings. Vulic and Moens (2015) have alleviated the requirements by inducing multilingual word embeddings directly from a document-aligned corpus such as a set of Wikipedi"
N16-1156,W15-1521,0,0.0754935,"ta has been shown by Hana et al. (2004). The transfer is performed between typologically similar languages, which enables the model to directly transfer the transition probabilities from source to the target. Moreover, emission probabilities are hand-engineered to capture language-specific morphological properties. In contrast, our method does not require any languagespecific knowledge on the target side. Multilingual Word Embeddings There is an expansive body of research on learning multilingual word embeddings (Gouws et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015; Lauly et al., 2014; Luong et al., 2015). Previous work has shown its effectiveness across a wide range of multilingual transfer tasks including tagging (Kim et al., 2015), syntactic parsing (Xiao and Guo, 2014; Guo et al., 2015; Durrett et al., 2012), and machine translation (Zou et al., 2013; Mikolov et al., 2013b). However, these approaches commonly require parallel sentences or bilingual lexicon to learn multilingual embeddings. Vulic and Moens (2015) have alleviated the requirements by inducing multilingual word embeddings directly from a document-aligned corpus such as a set of Wikipedia pages on the same theme but in differen"
N16-1156,D11-1006,0,0.0118663,"e-to-one and tags fully preserved in translation. Other cues are necessary. The few translation pairs provide just enough information to obtain a coarse global alignment between the source and target language embeddings. We limit the initial linear transformation between embeddings to isometric (orthonormal) mappings so as to preserve norms and angles (e.g., cosine similarities) between words. Once the embeddings are aligned, any source language model expressed in terms of embeddings can be mapped to a target language model. The approach is akin to direct transfer commonly applied in parsing (McDonald et al., 2011; Zeman and Resnik, 2008) though often with more information. We use the term “direct transfer” to mean the process where no further adjustment is performed beyond the immediate mapping via (coarsely) aligned embeddings. Direct transfer is insufficient between languages that are syntactically (even moderately) divergent. Instead, we use the directly transferred model to initialize and regularize an unsupervised tagger. Specifically, we employ a feature-based HMM (Berg-Kirkpatrick et al., 2010) tagger for both the source and target languages with two important modifications. The emission probab"
N16-1156,P13-2017,0,0.0223491,"Missing"
N16-1156,D14-1162,0,0.108721,"Missing"
N16-1156,petrov-etal-2012-universal,0,0.0218246,"Missing"
N16-1156,N10-1063,0,0.00827438,"the sentences in the training set as unlabeled data, and evaluate on the testing set. Word Embeddings To induce monolingual word embeddings, we use the processed Wikipedia text dumps (Al-Rfou et al., 2013) for each language. 3 Examples of symbol mark include “-”, “/” etc. Language Tokens (106 ) English 1,888 Danish 44 German 687 Spanish 399 Finnish 66 Hungarian 89 Indonesian 41 Table 1: Number of tokens of the Wikipedia dumps used for inducing word embeddings. While Wikipedia texts may contain parallel articles, we show in Table 1 that the amount of text varies significantly across languages. Smith et al. (2010) also demonstrated that parallel information in Wikipedia is very noisy. Therefore, direct translations are difficult to get from these texts. We use the word2vec tool with the skip-gram learning scheme (Mikolov et al., 2013a). In our experiments we use d = 20 for the dimension of word embeddings and w = 1 for the context window size of the skip-gram, which yields the best overall performance for our model. In our analysis, we also explore the impact of embedding dimension and window size. Word Translation Pairs For each target language, we collect English translations for the top ten most fre"
N16-1156,D08-1109,1,0.82441,"Missing"
N16-1156,Q13-1001,0,0.034462,"Missing"
N16-1156,P15-2118,0,0.0790889,"ngual Word Embeddings There is an expansive body of research on learning multilingual word embeddings (Gouws et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015; Lauly et al., 2014; Luong et al., 2015). Previous work has shown its effectiveness across a wide range of multilingual transfer tasks including tagging (Kim et al., 2015), syntactic parsing (Xiao and Guo, 2014; Guo et al., 2015; Durrett et al., 2012), and machine translation (Zou et al., 2013; Mikolov et al., 2013b). However, these approaches commonly require parallel sentences or bilingual lexicon to learn multilingual embeddings. Vulic and Moens (2015) have alleviated the requirements by inducing multilingual word embeddings directly from a document-aligned corpus such as a set of Wikipedia pages on the same theme but in different languages. However, they still used about ten thousands aligned documents as parallel supervision. Our work demonstrates that useful multilingual embeddings can be learned with a minimal amount of parallel supervision. 3 Multilingual POS Tagger Our method is designed to operate in the regime where there are no parallel sentences or target annotations. We assume only a few, in our case ten, word translation pairs."
N16-1156,D14-1187,0,0.0392456,"Missing"
N16-1156,W14-1613,0,0.0454515,"babilities from source to the target. Moreover, emission probabilities are hand-engineered to capture language-specific morphological properties. In contrast, our method does not require any languagespecific knowledge on the target side. Multilingual Word Embeddings There is an expansive body of research on learning multilingual word embeddings (Gouws et al., 2014; Faruqui and Dyer, 2014; Lu et al., 2015; Lauly et al., 2014; Luong et al., 2015). Previous work has shown its effectiveness across a wide range of multilingual transfer tasks including tagging (Kim et al., 2015), syntactic parsing (Xiao and Guo, 2014; Guo et al., 2015; Durrett et al., 2012), and machine translation (Zou et al., 2013; Mikolov et al., 2013b). However, these approaches commonly require parallel sentences or bilingual lexicon to learn multilingual embeddings. Vulic and Moens (2015) have alleviated the requirements by inducing multilingual word embeddings directly from a document-aligned corpus such as a set of Wikipedia pages on the same theme but in different languages. However, they still used about ten thousands aligned documents as parallel supervision. Our work demonstrates that useful multilingual embeddings can be lear"
N16-1156,H01-1035,0,0.24104,"Missing"
N16-1156,D13-1141,0,\N,Missing
N16-1156,I08-3008,0,\N,Missing
N16-1156,L16-1262,0,\N,Missing
N19-1082,P14-1016,0,0.0205677,"skovec and Mcauley, 2012), i.e. when we extract information about one user, we consider the subgraph formed by the user and his/her direct neighbors. Each node corresponds to a Twitter user, who is represented by the set of posted tweets.7 Edges are defined by the followedby link, under the assumption that connected users are more likely to come from the same university or company. An example of the social media graph is reported in the appendices. Social media information extraction refers to the task of extracting information from users’ posts in online social networks (Benson et al., 2011; Li et al., 2014). In this paper, we aim at extracting education and job information from users’ tweets. Given a set of tweets posted by a user, the goal is to extract mentions of the organizations to which they belong. The fact that the tweets are short, highly contextualized and show special linguistic features makes this task particularly challenging. Dataset We construct two datasets, E DUCA TION and J OB , from the Twitter corpus released by Li et al. (2014). The original corpus contains millions of tweets generated by ≈ 10 thousand users, where the education and job mentions are annotated using distant s"
N19-1082,P13-1008,0,0.0602827,"Missing"
N19-1082,P11-1040,1,0.723765,"h as ego-networks (Leskovec and Mcauley, 2012), i.e. when we extract information about one user, we consider the subgraph formed by the user and his/her direct neighbors. Each node corresponds to a Twitter user, who is represented by the set of posted tweets.7 Edges are defined by the followedby link, under the assumption that connected users are more likely to come from the same university or company. An example of the social media graph is reported in the appendices. Social media information extraction refers to the task of extracting information from users’ posts in online social networks (Benson et al., 2011; Li et al., 2014). In this paper, we aim at extracting education and job information from users’ tweets. Given a set of tweets posted by a user, the goal is to extract mentions of the organizations to which they belong. The fact that the tweets are short, highly contextualized and show special linguistic features makes this task particularly challenging. Dataset We construct two datasets, E DUCA TION and J OB , from the Twitter corpus released by Li et al. (2014). The original corpus contains millions of tweets generated by ≈ 10 thousand users, where the education and job mentions are annotat"
N19-1082,P16-1101,0,0.0494878,"Task 2 and Task 3. The encoder and decoder BiLSTMs have the same dimension as the graph convolution layer. In Task 3, we concatenate a positional encoding to each text box’s representation by transforming its bounding box coordinates to a vector of length 32, and then applying a tanh activation. Baseline and Our Method We implement a two-layer BiLSTM with a conditional random fields (CRF) tagger as the sequential baseline (SeqIE). This architecture and its variants have been extensively studied and demonstrated to be successful in previous work on information extraction (Lample et al., 2016; Ma and Hovy, 2016). In the textual IE task (Task 1), our baseline is shown to obtain competitive results with the state-of-the-art method in the CONLL03 dataset. In the visual IE task (Task 3), in order to further increase the competitiveness of the baseline, we sequentially concatenate the horizontally aligned text boxes, therefore fully modeling the horizontal edges of the graph. Our baseline shares the same encoder and decoder architecture with GraphIE, but without the graph module. Both architectures have similar 6 6.1 Results Task 1: Textual Information Extraction Table 4 describes the NER accuracy on the"
N19-1082,P05-1045,0,0.155504,"involved in a wide range of duties for Washington ’s request … Figure 1: Example of the entity extraction task with an ambiguous entity mention (i.e. “...for Washington’s request...”). Aside from the sentential forward and backward edges (green, solid) which aggregate local contextual information, non-local relations — such as the co-referent edges (red, dashed) and the identicalmention edges (blue, dotted) — provide additional valuable information to reduce tagging ambiguity (i.e. P ERSON or L OCATION). Best viewed in color. Introduction the output space in a structured prediction framework (Finkel et al., 2005; Reichart and Barzilay, 2012; Hu et al., 2016). Such approaches, however, mostly overlook the richer set of structural relations in the input space. With reference to the example in Figure 1, the co-referent dependencies would not be readily exploited by simply constraining the output space, as they would not necessarily be labeled as entities (e.g. pronouns). In the attempt to capture non-local dependencies in the input space, alternative approaches define a graph that outlines the input structure and engineer features to describe it (Quirk and Poon, 2017). Designing effective features is ho"
N19-1082,P09-1113,0,0.0420489,"per, we aim at extracting education and job information from users’ tweets. Given a set of tweets posted by a user, the goal is to extract mentions of the organizations to which they belong. The fact that the tweets are short, highly contextualized and show special linguistic features makes this task particularly challenging. Dataset We construct two datasets, E DUCA TION and J OB , from the Twitter corpus released by Li et al. (2014). The original corpus contains millions of tweets generated by ≈ 10 thousand users, where the education and job mentions are annotated using distant supervision (Mintz et al., 2009). We sample the tweets from each user, maintaining the ratio between positive and negative posts.6 The obtained E DUCATION dataset consists of 443, 476 tweets generated by 7, 208 users, and the J OB dataset contains 176, 043 tweets generated by 1, 772 users. Dataset statistics are reported in Table 3. 5.3 Task 3: Visual Information Extraction Visual information extraction refers to the extraction of attribute values from documents formatted in various layouts. Examples include invoices and forms, whose format can be exploited to infer valuable information to support extraction. Dataset The cor"
N19-1082,P16-1105,0,0.216366,"Missing"
N19-1082,Q17-1008,0,0.101661,"Missing"
N19-1082,D14-1162,0,0.0835136,"IE with sentence-level graph module (cf. Figure 2(b)). 5.5 Implementation Details The models are trained with Adam (Kingma and Ba, 2014) to minimize the CRF objective. For regularization, we choose dropout with a ratio of 0.1 on both the input word representation and the hidden layer of the decoder. The learning rate is set to 0.001. We use the development set for early-stopping and the selection of the best performing hyperparameters. For CharCNN, we use 64-dimensional character embeddings and 64 filters of width 2 to 4 (Kim et al., 2016). The 100dimensional pretrained GloVe word embeddings (Pennington et al., 2014) are used in Task 1 and 2, and 64-dimensional randomly initialized word embeddings are used in Task 3. We use a twolayer GCN in Task 1, and a one-layer GCN in Task 2 and Task 3. The encoder and decoder BiLSTMs have the same dimension as the graph convolution layer. In Task 3, we concatenate a positional encoding to each text box’s representation by transforming its bounding box coordinates to a vector of length 32, and then applying a tanh activation. Baseline and Our Method We implement a two-layer BiLSTM with a conditional random fields (CRF) tagger as the sequential baseline (SeqIE). This a"
N19-1082,N18-1202,0,0.0617987,"Missing"
N19-1082,E17-1110,0,0.0223399,"in a structured prediction framework (Finkel et al., 2005; Reichart and Barzilay, 2012; Hu et al., 2016). Such approaches, however, mostly overlook the richer set of structural relations in the input space. With reference to the example in Figure 1, the co-referent dependencies would not be readily exploited by simply constraining the output space, as they would not necessarily be labeled as entities (e.g. pronouns). In the attempt to capture non-local dependencies in the input space, alternative approaches define a graph that outlines the input structure and engineer features to describe it (Quirk and Poon, 2017). Designing effective features is however challenging, arbitrary and time consuming, especially when the underlying structure is complex. Moreover, these approaches have limited capacity of capturing node interactions informed by the graph structure. Most modern Information Extraction (IE) systems are implemented as sequential taggers. While such models effectively capture relations in the local context, they have limited capability of exploiting non-local and non-sequential dependencies. In many applications, however, such dependencies can greatly reduce tagging ambiguity, thereby improving o"
N19-1082,N16-1030,0,0.365853,"representation, i.e. gi = Enc(si ), and conducts graph convolution on every node, propagating information between its neighbors, and integrating such information into a new hidden representation. Specifically, each layer of 4.3 Decoder To support tagging, the learned representation is propagated to the decoder. 3 We choose this simple normalization strategy instead of the two-sided normalization in Kipf and Welling (2016), as it performs better in the experiments. The same strategy is also adopted by Zhang et al. (2018). 754 (i) In our work, the decoder is instantiated as a BiLSTM+CRF tagger (Lample et al., 2016). The output representation of the graph module, GCN(si ), is split into two vectors of the same length, which are used as the initial hidden states for the forward and backward LSTMs, respectively. In this way, the graph contextual information is propagated to each word through the LSTM. Specifically, we have   (i) (i) z1:k = RNN h1:k ; GCN(si ), Θdec , (5) 5 (i) where h1:k are the output hidden states of the encoder, GCN(si ) represents the initial state, and Θdec is the decoder parameters. A simpler way to incorporate the graph representation into the decoder is concatenating with its inp"
N19-1082,N12-1008,1,0.865342,"nge of duties for Washington ’s request … Figure 1: Example of the entity extraction task with an ambiguous entity mention (i.e. “...for Washington’s request...”). Aside from the sentential forward and backward edges (green, solid) which aggregate local contextual information, non-local relations — such as the co-referent edges (red, dashed) and the identicalmention edges (blue, dotted) — provide additional valuable information to reduce tagging ambiguity (i.e. P ERSON or L OCATION). Best viewed in color. Introduction the output space in a structured prediction framework (Finkel et al., 2005; Reichart and Barzilay, 2012; Hu et al., 2016). Such approaches, however, mostly overlook the richer set of structural relations in the input space. With reference to the example in Figure 1, the co-referent dependencies would not be readily exploited by simply constraining the output space, as they would not necessarily be labeled as entities (e.g. pronouns). In the attempt to capture non-local dependencies in the input space, alternative approaches define a graph that outlines the input structure and engineer features to describe it (Quirk and Poon, 2017). Designing effective features is however challenging, arbitrary"
N19-1082,W08-0602,0,0.0979603,"Missing"
N19-1082,R11-1004,0,0.052394,"Missing"
N19-1082,P15-1150,0,0.140715,"Missing"
N19-1082,W03-0419,0,0.319986,"Missing"
N19-1082,P18-2038,0,0.0295615,"Missing"
N19-1082,D18-1244,0,0.167599,"context, which — instead of being used directly for classification — is projected to the decoder to enrich local information and perform sequence tagging. A handful of other information extraction approaches have used graph-based neural networks. Miwa and Bansal (2016) applied Tree LSTM (Tai et al., 2015) to jointly represent sequences and dependency trees for entity and relation extraction. On the same line of work, Peng et al. (2017) and Song et al. (2018) introduced Graph LSTM, which extended the traditional LSTM to graphs by enabling a varied number of incoming edges at each memory cell. Zhang et al. (2018) exploited graph convolutions to pool information over pruned dependency trees, outperforming existing sequence and dependency-based neural models in a relation extraction task. These studies differ from ours in several respects. First, they can only model wordlevel graphs, whereas our framework can learn non-local context either from word- or sentencelevel graphs, using it to reduce ambiguity during tagging at the word level. Second, all these studies achieved improvements only when using dependency trees. We extend the graph-based approach to validate the benefits of using other types of rel"
N19-1162,W18-2501,0,0.0575212,"Missing"
N19-1162,D18-1217,0,0.127924,"i , ei,c ) A LL W ORDS H OMONYMS 0.85 (±0.09) 0.18 (±0.04) 0.21 (±0.04) Table 1: Average cosine distances between pairs of embedding anchors (left column) and between contextualized embeddings of words to their corresponding anchor. The right column includes these distances only for homonyms, whereas the center column is averaged across all words. Only alphabetic words with at least 100 occurrences were included. In all of the above cases, token-level embeddings are used. Inspired by strong results of using contextualized embeddings in monolingual parsing (Che et al., 2018; Wang et al., 2018; Clark et al., 2018), we aim to utilize them in the multilingual transfer case. Our results demonstrate that richer representation of lexical space does lead to significant performance gains. 3 Figure 1: A two dimensional PCA showing examples of contextual representations for four Spanish words. Their corresponding anchors are presented as a star in the same color. (best viewed in color) • Embedding Alignment: Given an embedding esi,c in s, we want to generate an embedding es→t i,c in the target language space, using a linear mapping W s→t . Formally, our alignment is always of the following form: Aligning Contex"
N19-1162,L18-1550,0,0.116496,"aligning monolingual embedding spaces. When a bilingual dictionary is provided, their approach is similar to those of (Smith et al., 2017; Artetxe et al., 2017). MUSE extends these methods to the unsupervised case by constructing a synthetic dictionary. The resulting alignment achieves strong performance in a range of NLP tasks, from sequence labeling (Lin et al., 2018) to natural language inference (Conneau et al., 2018b) and machine translation (Lample et al., 2018; Qi et al., 2018). Recent work further improves the performance on both the supervised (Joulin et al., 2018) and unsupervised (Grave et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018) settings for context-independent embeddings. While MUSE operates over token based embeddings, we are interested in aligning contextual embeddings, which have shown their benefits in several monolingual applications (Peters et al., 2018; McCann et al., 2017; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). However, this expansion introduces new challenges which we address in this paper. In a concurrent study, Aldarmaki and Diab (2019) introduced an alignment that is based only on word pairs in the same context, using"
N19-1162,D18-1269,0,0.350807,"se such information. Our results also demonstrate the benefits of this approach for few-shot learning, i.e. processing languages with limited data. Specifically, on the Kazakh tree-bank from the recent CoNLL 2018 shared task with only 38 trees for training, the model yields 5 LAS points gain over the top result (Smith et al., 2018a). 2 Related work Multilingual Embeddings The topic of crosslingual embedding alignment is an active area of research (Mikolov et al., 2013; Xing et al., 2015; Dinu and Baroni, 2014; Lazaridou et al., 2015; Zhang et al., 2017). Our work most closely relates to MUSE (Conneau et al., 2018a), which constructs a multilingual space by aligning monolingual embedding spaces. When a bilingual dictionary is provided, their approach is similar to those of (Smith et al., 2017; Artetxe et al., 2017). MUSE extends these methods to the unsupervised case by constructing a synthetic dictionary. The resulting alignment achieves strong performance in a range of NLP tasks, from sequence labeling (Lin et al., 2018) to natural language inference (Conneau et al., 2018b) and machine translation (Lample et al., 2018; Qi et al., 2018). Recent work further improves the performance on both the supervi"
N19-1162,N18-2084,0,0.0364527,"; Zhang et al., 2017). Our work most closely relates to MUSE (Conneau et al., 2018a), which constructs a multilingual space by aligning monolingual embedding spaces. When a bilingual dictionary is provided, their approach is similar to those of (Smith et al., 2017; Artetxe et al., 2017). MUSE extends these methods to the unsupervised case by constructing a synthetic dictionary. The resulting alignment achieves strong performance in a range of NLP tasks, from sequence labeling (Lin et al., 2018) to natural language inference (Conneau et al., 2018b) and machine translation (Lample et al., 2018; Qi et al., 2018). Recent work further improves the performance on both the supervised (Joulin et al., 2018) and unsupervised (Grave et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018) settings for context-independent embeddings. While MUSE operates over token based embeddings, we are interested in aligning contextual embeddings, which have shown their benefits in several monolingual applications (Peters et al., 2018; McCann et al., 2017; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). However, this expansion introduces new challenges which we address in this paper. In a"
N19-1162,K18-2019,0,0.084907,"Missing"
N19-1162,K18-2011,0,0.0849581,"Missing"
N19-1162,D18-1291,0,0.060176,"Missing"
N19-1162,K17-3009,0,0.0729883,"Missing"
N19-1162,tiedemann-2012-parallel,0,0.0669551,"Missing"
N19-1162,W15-2137,0,0.0423654,"ss-lingual word embeddings by replacing the predicted word with its translation. To utilize a dictionary in the contextualized case, we include a soft constraint that pushes those translations to be similar in their context-independent representation. A similar style of regularization was shown to be effective for cross-domain transfer of word embeddings (Yang et al., 2017). Multilingual Parsing In early work on multilingual parsing, transfer was commonly implemented using delexicalized representation such as part-ofspeech tags (McDonald et al., 2011; Petrov et al., 2012; Naseem et al., 2012; Tiedemann, 2015). Another approach for cross-lingual parsing includes annotation projection and treebank translation (Xiao and Guo, 2015; Wang and Eisner, 2016; Tiedemann, 2017), which mostly require some source of supervision. Advancements in multilingual word representations opened a possibility of lexicalized transfer. Some of these approaches start by aligning monolingual embedding spaces (Zhang and Barzilay, 2015; Guo et al., 2015, 2016; Ammar et al., 2016), and using resulting word embeddings as word representations instead of universal tags. Other approaches are learning customized multilingual syntact"
N19-1162,W17-1216,0,0.0168806,"that pushes those translations to be similar in their context-independent representation. A similar style of regularization was shown to be effective for cross-domain transfer of word embeddings (Yang et al., 2017). Multilingual Parsing In early work on multilingual parsing, transfer was commonly implemented using delexicalized representation such as part-ofspeech tags (McDonald et al., 2011; Petrov et al., 2012; Naseem et al., 2012; Tiedemann, 2015). Another approach for cross-lingual parsing includes annotation projection and treebank translation (Xiao and Guo, 2015; Wang and Eisner, 2016; Tiedemann, 2017), which mostly require some source of supervision. Advancements in multilingual word representations opened a possibility of lexicalized transfer. Some of these approaches start by aligning monolingual embedding spaces (Zhang and Barzilay, 2015; Guo et al., 2015, 2016; Ammar et al., 2016), and using resulting word embeddings as word representations instead of universal tags. Other approaches are learning customized multilingual syntactic embeddings bootstrapping from universal POS tags (Duong et al., 2015). While some models also learn a language embedding (Ammar et al., 2016; de Lhoneux et al"
N19-1162,Q16-1035,0,0.0665857,"clude a soft constraint that pushes those translations to be similar in their context-independent representation. A similar style of regularization was shown to be effective for cross-domain transfer of word embeddings (Yang et al., 2017). Multilingual Parsing In early work on multilingual parsing, transfer was commonly implemented using delexicalized representation such as part-ofspeech tags (McDonald et al., 2011; Petrov et al., 2012; Naseem et al., 2012; Tiedemann, 2015). Another approach for cross-lingual parsing includes annotation projection and treebank translation (Xiao and Guo, 2015; Wang and Eisner, 2016; Tiedemann, 2017), which mostly require some source of supervision. Advancements in multilingual word representations opened a possibility of lexicalized transfer. Some of these approaches start by aligning monolingual embedding spaces (Zhang and Barzilay, 2015; Guo et al., 2015, 2016; Ammar et al., 2016), and using resulting word embeddings as word representations instead of universal tags. Other approaches are learning customized multilingual syntactic embeddings bootstrapping from universal POS tags (Duong et al., 2015). While some models also learn a language embedding (Ammar et al., 2016"
N19-1162,D18-1311,0,0.138692,"(¯ ei , e Dcos (¯ ei , ei,c ) A LL W ORDS H OMONYMS 0.85 (±0.09) 0.18 (±0.04) 0.21 (±0.04) Table 1: Average cosine distances between pairs of embedding anchors (left column) and between contextualized embeddings of words to their corresponding anchor. The right column includes these distances only for homonyms, whereas the center column is averaged across all words. Only alphabetic words with at least 100 occurrences were included. In all of the above cases, token-level embeddings are used. Inspired by strong results of using contextualized embeddings in monolingual parsing (Che et al., 2018; Wang et al., 2018; Clark et al., 2018), we aim to utilize them in the multilingual transfer case. Our results demonstrate that richer representation of lexical space does lead to significant performance gains. 3 Figure 1: A two dimensional PCA showing examples of contextual representations for four Spanish words. Their corresponding anchors are presented as a star in the same color. (best viewed in color) • Embedding Alignment: Given an embedding esi,c in s, we want to generate an embedding es→t i,c in the target language space, using a linear mapping W s→t . Formally, our alignment is always of the following"
N19-1162,W14-1613,0,0.0834496,"lingual applications (Peters et al., 2018; McCann et al., 2017; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). However, this expansion introduces new challenges which we address in this paper. In a concurrent study, Aldarmaki and Diab (2019) introduced an alignment that is based only on word pairs in the same context, using parallel sentences. Our method achieves better word translations without relying on such supervision. Our work also relates to prior approaches that utilize bilingual dictionaries to improve embeddings that were trained on small datasets. For instance, Xiao and Guo (2014) represent word pairs as a mutual vector, while Adams et al. (2017) jointly train cross-lingual word embeddings by replacing the predicted word with its translation. To utilize a dictionary in the contextualized case, we include a soft constraint that pushes those translations to be similar in their context-independent representation. A similar style of regularization was shown to be effective for cross-domain transfer of word embeddings (Yang et al., 2017). Multilingual Parsing In early work on multilingual parsing, transfer was commonly implemented using delexicalized representation such as"
N19-1162,K15-1008,0,0.0260541,"tualized case, we include a soft constraint that pushes those translations to be similar in their context-independent representation. A similar style of regularization was shown to be effective for cross-domain transfer of word embeddings (Yang et al., 2017). Multilingual Parsing In early work on multilingual parsing, transfer was commonly implemented using delexicalized representation such as part-ofspeech tags (McDonald et al., 2011; Petrov et al., 2012; Naseem et al., 2012; Tiedemann, 2015). Another approach for cross-lingual parsing includes annotation projection and treebank translation (Xiao and Guo, 2015; Wang and Eisner, 2016; Tiedemann, 2017), which mostly require some source of supervision. Advancements in multilingual word representations opened a possibility of lexicalized transfer. Some of these approaches start by aligning monolingual embedding spaces (Zhang and Barzilay, 2015; Guo et al., 2015, 2016; Ammar et al., 2016), and using resulting word embeddings as word representations instead of universal tags. Other approaches are learning customized multilingual syntactic embeddings bootstrapping from universal POS tags (Duong et al., 2015). While some models also learn a language embedd"
N19-1162,N15-1104,0,0.19535,"Missing"
N19-1162,D17-1312,0,0.0293487,"relates to prior approaches that utilize bilingual dictionaries to improve embeddings that were trained on small datasets. For instance, Xiao and Guo (2014) represent word pairs as a mutual vector, while Adams et al. (2017) jointly train cross-lingual word embeddings by replacing the predicted word with its translation. To utilize a dictionary in the contextualized case, we include a soft constraint that pushes those translations to be similar in their context-independent representation. A similar style of regularization was shown to be effective for cross-domain transfer of word embeddings (Yang et al., 2017). Multilingual Parsing In early work on multilingual parsing, transfer was commonly implemented using delexicalized representation such as part-ofspeech tags (McDonald et al., 2011; Petrov et al., 2012; Naseem et al., 2012; Tiedemann, 2015). Another approach for cross-lingual parsing includes annotation projection and treebank translation (Xiao and Guo, 2015; Wang and Eisner, 2016; Tiedemann, 2017), which mostly require some source of supervision. Advancements in multilingual word representations opened a possibility of lexicalized transfer. Some of these approaches start by aligning monolingu"
N19-1162,K18-2001,0,0.0508426,"Missing"
N19-1162,P17-1179,0,0.0684352,"el performs on par with context-independent models that do use such information. Our results also demonstrate the benefits of this approach for few-shot learning, i.e. processing languages with limited data. Specifically, on the Kazakh tree-bank from the recent CoNLL 2018 shared task with only 38 trees for training, the model yields 5 LAS points gain over the top result (Smith et al., 2018a). 2 Related work Multilingual Embeddings The topic of crosslingual embedding alignment is an active area of research (Mikolov et al., 2013; Xing et al., 2015; Dinu and Baroni, 2014; Lazaridou et al., 2015; Zhang et al., 2017). Our work most closely relates to MUSE (Conneau et al., 2018a), which constructs a multilingual space by aligning monolingual embedding spaces. When a bilingual dictionary is provided, their approach is similar to those of (Smith et al., 2017; Artetxe et al., 2017). MUSE extends these methods to the unsupervised case by constructing a synthetic dictionary. The resulting alignment achieves strong performance in a range of NLP tasks, from sequence labeling (Lin et al., 2018) to natural language inference (Conneau et al., 2018b) and machine translation (Lample et al., 2018; Qi et al., 2018). Rec"
N19-1162,D15-1213,1,0.856634,"g In early work on multilingual parsing, transfer was commonly implemented using delexicalized representation such as part-ofspeech tags (McDonald et al., 2011; Petrov et al., 2012; Naseem et al., 2012; Tiedemann, 2015). Another approach for cross-lingual parsing includes annotation projection and treebank translation (Xiao and Guo, 2015; Wang and Eisner, 2016; Tiedemann, 2017), which mostly require some source of supervision. Advancements in multilingual word representations opened a possibility of lexicalized transfer. Some of these approaches start by aligning monolingual embedding spaces (Zhang and Barzilay, 2015; Guo et al., 2015, 2016; Ammar et al., 2016), and using resulting word embeddings as word representations instead of universal tags. Other approaches are learning customized multilingual syntactic embeddings bootstrapping from universal POS tags (Duong et al., 2015). While some models also learn a language embedding (Ammar et al., 2016; de Lhoneux et al., 2018), it is unfeasible in a zero-shot scenario. 1600 ¯j ) Dcos (¯ ei , e Dcos (¯ ei , ei,c ) A LL W ORDS H OMONYMS 0.85 (±0.09) 0.18 (±0.04) 0.21 (±0.04) Table 1: Average cosine distances between pairs of embedding anchors (left column) and"
N19-1371,D15-1075,0,0.248725,"el is provided with a prompt that specifies an intervention, a comparator, and an outcome, along with a fulltext article. The model is then to infer the reported findings with respect to this prompt (Figure 1). From a healthcare perspective, this inference task is an essential step for automating extraction of actionable evidence from trial reports. 3705 Proceedings of NAACL-HLT 2019, pages 3705–3717 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics From an NLP standpoint, the proposed task can be seen as an instance of natural language inference (Bowman et al., 2015), viewing the article and prompt as the premise and hypothesis, respectively. However, the problem differs in a few important ways from existing NLP formulations. First, the inputs: prompts are brief (∼13.5 words on average), but articles are long (∼4200 words). Further, only a few snippets of the article will be relevant to the label for a given prompt. Second, prompts in this domain are structured, and include only a few types of key information: interventions, comparators, and outcomes. Methods that exploit this regularity are likely to be more accurate than generic inference algorithms. An"
N19-1371,P17-1020,0,0.0489168,"Missing"
N19-1371,C00-1043,0,0.344392,"Missing"
N19-1371,D15-1050,0,0.0254011,"uced due to shifting objectives. 3712 0.95 0.90 Evidence token AUC 0.85 0.80 0.75 0.70 0.65 0.60 0.55 cond-attn + pretraining cond-attn attn + pretraining attn 40 20 0 Epoch 20 40 Figure 6: Validation evidence token AUCs during training. ‘pretraining’ epochs are depicted as ‘negative’ for the two explicitly supervised attention variants. Note that we use early stopping, so not all models run for the same number of epochs. 7 Related Work The proposed task is situated at the intersection of information extraction (Cardie, 1997), natural language inference (Bowman et al., 2015), evidence mining (Rinott et al., 2015) and question answering (Harabagiu et al., 2000; Hovy et al., 2000). However, our focus on inferring results from lengthy clinical trial reports pertaining to particular prompts constitutes a unique problem, as discussed in the Introduction. Prior systems have attempted to extract information from articles describing RCTs. For example, ExaCT (Kiritchenko et al., 2010) attempts to extract variables describing clinical trials from articles, and ACRES (Summerscales et al., 2011) ingests extracts key variables from abstracts. Blake and Lucic (2015; 2012) considered the problem of automatically ext"
N19-1371,N16-1174,0,0.0513476,"Missing"
N19-1371,N07-1033,0,0.21953,"icle and ICO frame into a vector [a; i; c; o] which is then passed through a feedforward network with a single hidden layer to allow interactions between the prompt and article text.4 As discussed in detail below, we experiment with a variety of attention mechanisms imposed over article tokens. 4.2 Finding the Evidence Exploiting the spans of evidence marked as supporting assessments should improve the predictive performance of models. An additional advantage of modeling this explicitly is that models will then be able to provide rationales for decisions (Lei et al., 2016; Zhang et al., 2016; Zaidan et al., 2007), i.e., snippets of text that support predictions. We therefore experiment with model variants that classify input tokens as being relevant evidence (or not) prior to performing inference. We consider both pipeline and joint instantiations of such models. In the former type, the model first identifies spans in the text and then passes these forward to an independent component that makes predictions on the basis of these. In models of the latter type, evidence span tagging and document-level inference is performed 4 We use a linear hidden layer; experiments adding a nonlinearity (ReLU) did not"
N19-1371,D16-1076,1,0.877833,"nate the encoded article and ICO frame into a vector [a; i; c; o] which is then passed through a feedforward network with a single hidden layer to allow interactions between the prompt and article text.4 As discussed in detail below, we experiment with a variety of attention mechanisms imposed over article tokens. 4.2 Finding the Evidence Exploiting the spans of evidence marked as supporting assessments should improve the predictive performance of models. An additional advantage of modeling this explicitly is that models will then be able to provide rationales for decisions (Lei et al., 2016; Zhang et al., 2016; Zaidan et al., 2007), i.e., snippets of text that support predictions. We therefore experiment with model variants that classify input tokens as being relevant evidence (or not) prior to performing inference. We consider both pipeline and joint instantiations of such models. In the former type, the model first identifies spans in the text and then passes these forward to an independent component that makes predictions on the basis of these. In models of the latter type, evidence span tagging and document-level inference is performed 4 We use a linear hidden layer; experiments adding a nonlin"
N19-1371,D16-1011,1,\N,Missing
N19-1371,D18-1546,0,\N,Missing
N19-1371,W12-4301,0,\N,Missing
P01-1008,W99-0625,0,\N,Missing
P01-1008,J93-1004,0,\N,Missing
P01-1008,W97-0703,1,\N,Missing
P01-1008,N01-1009,0,\N,Missing
P01-1008,J93-2003,0,\N,Missing
P01-1008,P97-1004,0,\N,Missing
P01-1008,P95-1026,0,\N,Missing
P01-1008,P98-1116,0,\N,Missing
P01-1008,C98-1112,0,\N,Missing
P01-1008,P98-2127,0,\N,Missing
P01-1008,C98-2122,0,\N,Missing
P01-1008,P93-1023,1,\N,Missing
P01-1008,W99-0613,0,\N,Missing
P01-1008,P93-1024,0,\N,Missing
P05-1018,N04-1015,1,0.70992,"nging from collaborative filtering (Joachims, 2002a) to parsing (Toutanova et al., 2004). In our ranking experiments, we use Joachims’ (2002a) SVMlight package for training and testing with all parameters set to their default values. 4 Evaluation Set-Up In this section we describe two evaluation tasks that assess the merits of the coherence modeling framework introduced above. We also give details regarding our data collection, and parameter estimation. Finally, we introduce the baseline method used for comparison with our approach. 4.1 Text Ordering Text structuring algorithms (Lapata, 2003; Barzilay and Lee, 2004; Karamanis et al., 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information-bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. Since local coherence is a key property of any well-formed text, our model can be used to rank alternative sentence orderings. We do not assume that local coherence is sufficient to uniquely determine the best ordering — other constraints clearly play a role here. However,"
P05-1018,P97-1003,0,0.034898,"continues to show [increased earnings] O despite [the trial] X . Table 2: Summary augmented with syntactic annotations for grid computation. we employ a state-of-the-art noun phrase coreference resolution system (Ng and Cardie, 2002) trained on the MUC (6–7) data sets. The system decides whether two NPs are coreferent by exploiting a wealth of features that fall broadly into four categories: lexical, grammatical, semantic and positional. Once we have identified entity classes, the next step is to fill out grid entries with relevant syntactic information. We employ a robust statistical parser (Collins, 1997) to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified. Passive verbs are recognized using a small set of patterns, and the underlying deep grammatical role for arguments involved in the passive construction is entered in the grid (see the grid cell o for Microsoft, Sentence 2, Table 2). 143 When a noun is attested more than once with a different grammatical role in the same sentence, we default to the role with the highest grammatical ranking: subjects are ranked higher than objects, which"
P05-1018,J95-2003,0,0.998158,"which is inspired by Centering Theory and can be computed automatically from raw text. We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model. 1 Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson, 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza, 1990; Kibble and Power, 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale, 2000). Furthermore, coherence constraints are"
P05-1018,J04-4001,0,0.287963,"g problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model. 1 Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson, 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza, 1990; Kibble and Power, 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale, 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides, 2003) which are hard to implement in a robust application. This paper f"
P05-1018,P03-1069,1,0.815505,"rious tasks ranging from collaborative filtering (Joachims, 2002a) to parsing (Toutanova et al., 2004). In our ranking experiments, we use Joachims’ (2002a) SVMlight package for training and testing with all parameters set to their default values. 4 Evaluation Set-Up In this section we describe two evaluation tasks that assess the merits of the coherence modeling framework introduced above. We also give details regarding our data collection, and parameter estimation. Finally, we introduce the baseline method used for comparison with our approach. 4.1 Text Ordering Text structuring algorithms (Lapata, 2003; Barzilay and Lee, 2004; Karamanis et al., 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information-bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. Since local coherence is a key property of any well-formed text, our model can be used to rank alternative sentence orderings. We do not assume that local coherence is sufficient to uniquely determine the best ordering — other constraints clearly pl"
P05-1018,N03-1020,0,0.0592586,"Missing"
P05-1018,P00-1052,0,0.020558,"intuition by introducing constraints on the distribution of discourse entities in coherent text. These constraints are formulated in terms of focus, the most salient entity in a discourse segment, and transition of focus between adjacent sentences. The theory also establishes constraints on the linguistic realization of focus, suggesting that it is more likely to appear in prominent syntactic positions (such as subject or object), and to be referred to with anaphoric expressions. A great deal of research has attempted to translate principles of Centering Theory into a robust coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004; Karamanis et al., 2004). Such a translation is challenging in several respects: one has to specify the “free parameters” of the system (Poesio et al., 2004) and to determine ways of combining the effects of various constraints. A common methodology that has emerged in this research is to develop and evaluate coherence metrics on manually annotated corpora. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with transition information, and show that the distribution of transitions correlates with human grades. Karamanis et al. (2004) use a similar met"
P05-1018,J91-1002,0,0.310426,"salient entities and the rest, collecting statistics for each group separately. We identify salient entities based on their S O X – – – –– – X XX XO – XS OX O OO – OS S SO SX SS d1 d2 d3 0 0 0 .03 0 0 0 .02 .07 0 0 .12 .02 .02 .05 .25 0 0 0 .02 0 .07 0 .02 0 0 .06 .04 0 0 0 .36 .02 0 0 .03 0 0 0 .06 0 0 0 .05 .03 .07 .07 .29 Table 3: Example of a feature-vector document representation using all transitions of length two given syntactic categories: S, O, X, and –. frequency,1 following the widely accepted view that the occurrence frequency of an entity correlates with its discourse prominence (Morris and Hirst, 1991; Grosz et al., 1995). Ranking We view coherence assessment as a ranking learning problem. The ranker takes as input a set of alternative renderings of the same document and ranks them based on their degree of local coherence. Examples of such renderings include a set of different sentence orderings of the same text and a set of summaries produced by different systems for the same document. Ranking is more suitable than classification for our purposes since in text generation, a system needs a scoring function to compare among alternative renderings. Furthermore, it is clear that coherence ass"
P05-1018,P02-1014,0,0.0551969,". 3 [The case] S revolves around [evidence] O of [Microsoft] S aggressively pressuring [Netscape] O into merging [browser software] O . 4 [Microsoft]S claims [its tactics] S are commonplace and good economically. 5 [The government] S may file [a civil suit]O ruling that [conspiracy] S to curb [competition] O through [collusion]X is [a violation of the Sherman Act] O . 6 [Microsoft]S continues to show [increased earnings] O despite [the trial] X . Table 2: Summary augmented with syntactic annotations for grid computation. we employ a state-of-the-art noun phrase coreference resolution system (Ng and Cardie, 2002) trained on the MUC (6–7) data sets. The system decides whether two NPs are coreferent by exploiting a wealth of features that fall broadly into four categories: lexical, grammatical, semantic and positional. Once we have identified entity classes, the next step is to fill out grid entries with relevant syntactic information. We employ a robust statistical parser (Collins, 1997) to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified. Passive verbs are recognized using a small set of patterns,"
P05-1018,P02-1040,0,0.105341,"th naturally occurring coherence violations as perceived by human readers. A representative example of such texts are automatically generated summaries which often contain sentences taken out of context and thus display problems with respect to local coherence (e.g., dangling anaphors, thematically unrelated sentences). A model that exhibits high agreement with human judges not only accurately captures the coherence properties of the summaries in question, but ultimately holds promise for the automatic evaluation of machine-generated texts. Existing automatic evaluation measures such as BLEU (Papineni et al., 2002) and ROUGE (Lin 2 The collections are available from http://www.csail. mit.edu/regina/coherence/. 145 and Hovy, 2003), are not designed for the coherence assessment task, since they focus on content similarity between system output and reference texts. Data Our evaluation was based on materials from the Document Understanding Conference (DUC, 2003), which include multi-document summaries produced by human writers and by automatic summarization systems. In order to learn a ranking, we require a set of summaries, each of which have been rated in terms of coherence. We therefore elicited judgment"
P05-1018,J04-3003,0,0.047824,"ically from raw text. Second, we learn patterns of entity distribution from a corpus, without attempting to directly implement or refine Centering constraints. 2 Related Work In this section we introduce our entity-based representation of discourse. We describe how it can be computed and how entity transition patterns can be extracted. The latter constitute a rich feature space on which probabilistic inference is performed. Local coherence has been extensively studied within the modeling framework put forward by Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004; Kibble and Power, 2004). One of the main assumptions underlying Centering is that a text segment which foregrounds a single entity is perceived to be more coherent than a segment in which multiple entities are discussed. The theory formalizes this intuition by introducing constraints on the distribution of discourse entities in coherent text. These constraints are formulated in terms of focus, the most salient entity in a discourse segment, and transition of focus between adjacent sentences. The theory also establishes constraints on the linguistic realization of focus, suggesting that it is"
P05-1018,J99-3001,0,0.0139369,"can be computed automatically from raw text. Second, we learn patterns of entity distribution from a corpus, without attempting to directly implement or refine Centering constraints. 2 Related Work In this section we introduce our entity-based representation of discourse. We describe how it can be computed and how entity transition patterns can be extracted. The latter constitute a rich feature space on which probabilistic inference is performed. Local coherence has been extensively studied within the modeling framework put forward by Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004; Kibble and Power, 2004). One of the main assumptions underlying Centering is that a text segment which foregrounds a single entity is perceived to be more coherent than a segment in which multiple entities are discussed. The theory formalizes this intuition by introducing constraints on the distribution of discourse entities in coherent text. These constraints are formulated in terms of focus, the most salient entity in a discourse segment, and transition of focus between adjacent sentences. The theory also establishes constraints on the linguistic realization of focus,"
P05-1018,W04-3222,0,0.0209531,"procedure is to find a parameter vector ~w that yields a “ranking score” function ~w · Φ(xi j ), which minimizes the number of violations of pairwise rankings provided in the training set. Thus, the ideal ~w would satisfy the condition ~w · (Φ(xi j ) − Φ(xik )) &gt; 0 ∀ j, i, k such that j &gt; k. The problem is typically treated as a Support Vector Machine constraint optimization problem, and can be solved using the search technique described in Joachims (2002a). This approach has been shown to be highly effective in various tasks ranging from collaborative filtering (Joachims, 2002a) to parsing (Toutanova et al., 2004). In our ranking experiments, we use Joachims’ (2002a) SVMlight package for training and testing with all parameters set to their default values. 4 Evaluation Set-Up In this section we describe two evaluation tasks that assess the merits of the coherence modeling framework introduced above. We also give details regarding our data collection, and parameter estimation. Finally, we introduce the baseline method used for comparison with our approach. 4.1 Text Ordering Text structuring algorithms (Lapata, 2003; Barzilay and Lee, 2004; Karamanis et al., 2004) are commonly evaluated by their performa"
P05-1018,W98-1411,0,\N,Missing
P05-1018,A00-2018,0,\N,Missing
P05-1018,J94-2003,0,\N,Missing
P05-1018,M95-1005,0,\N,Missing
P05-1018,P04-1050,0,\N,Missing
P05-1018,W03-1004,1,\N,Missing
P05-1018,N01-1003,0,\N,Missing
P05-1018,H01-1046,0,\N,Missing
P05-1018,P95-1034,0,\N,Missing
P05-1018,P98-1116,0,\N,Missing
P05-1018,C98-1112,0,\N,Missing
P05-1018,briscoe-carroll-2002-robust,0,\N,Missing
P05-1018,P87-1022,0,\N,Missing
P05-1018,P04-1051,0,\N,Missing
P05-1018,P05-1065,0,\N,Missing
P05-1018,J01-4004,0,\N,Missing
P05-1018,P86-1031,0,\N,Missing
P05-1018,P96-1036,0,\N,Missing
P05-1018,C69-7001,0,\N,Missing
P05-1018,C69-6902,0,\N,Missing
P05-1018,N01-1025,0,\N,Missing
P06-1004,W01-0514,0,0.747996,"here si are vectors of word counts, and α is a parameter that controls the degree of smoothing. In the formulation above we use sentences as our nodes. However, we can also represent graph nodes with non-overlapping blocks of words of fixed length. This is desirable, since the lecture transcripts lack sentence boundary markers, and short utterances can skew the cosine similarity scores. The optimal length of the block is tuned on a heldout development set. Lexical Weighting Previous research has shown that weighting schemes play an important role in segmentation performance (Ji and Zha, 2003; Choi et al., 2001). Of particular concern are words that may not be common in general English discourse but that occur throughout the text for a particular lecture or subject. For example, in a lecture about support vector machines, the occurrence of the term “SVM” is not going to convey a lot of information about the distribution of 5.2 Corpora We evaluate our segmentation algorithm on three sets of data. Two of the datasets we use are new segmentation collections that we have compiled for this study,1 and the remaining set includes a standard collection previously used for evaluation of segmentation algorithm"
P06-1004,A00-2004,0,0.820084,"re broad, encompassing information retrieval, question-answering and summarization. Not surprisingly, text segmentation has been extensively investigated over the last decade. Following the first unsupervised segmentation approach by Hearst (1994), most algorithms assume that variations in lexical distribution indicate topic changes. When documents exhibit sharp variations in lexical distribution, these algorithms are likely to detect segment boundaries accurately. For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000). The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle. This is evident in the performance of existing unsupervised algorithms on less 25 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 25–32, c Sydney, July 2006. 2006 Association for Computational Linguistics erty of the algorithm is its robustness to noise: the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output. 2 Previous Work Most unsupervised algorithms a"
P06-1004,P93-1001,0,0.0170957,"cond, the computational techniques for finding the optimal partitioning are also quite different. Since the minimization of the normalized cut is N P -complete in the general case, researchers in vision have to approximate this computation. Fortunately, we can find an exact solution due to the linearity constraint on text segmentation. 3 Figure 1: Sentence similarity plot for a Physics lecture, with vertical lines indicating true segment boundaries. Figure 1 illustrates these properties in a lecture transcript from an undergraduate Physics class. We use the text Dotplotting representation by (Church, 1993) and plot the cosine similarity scores between every pair of sentences in the text. The intensity of a point (i, j) on the plot indicates the degree to which the i-th sentence in the text is similar to the j-th sentence. The true segment boundaries are denoted by vertical lines. This similarity plot reveals a block structure where true boundaries delimit blocks of text with high inter-sentential similarity. Sentences found in different blocks, on the other hand, tend to exhibit low similarity. u1 u2 u3 un Figure 2: Graph-based Representation of Text Formalizing the Objective Whereas previous u"
P06-1004,P03-1071,0,0.837815,"iterion. In our case, the true positive rate is the fraction of boundaries correctly classified, and the false positive rate is the fraction of non-boundary positions incorrectly classified as boundaries. In computing the true and false positive rates, we vary the threshold distance to the true boundary within which a hypothesized boundary is considered correct. Larger areas under the ROC curve of a classifier indicate better discriminative performance. 5.4 Human Segmentation Study Spoken lectures are very different in style from other corpora used in human segmentation studies (Hearst, 1994; Galley et al., 2003). We are interested in analyzing human performance on a corpus of lecture transcripts with much longer texts and a less clear-cut concept of a sub-topic. We define a segment to be a sub-topic that signals a prominent shift in subject matter. Disregarding this sub-topic change would impair the high-level understanding of the structure and the content of the lecture. As part of our human segmentation analysis, we asked three annotators to segment the Physics lecture corpus. These annotators had taken the class in the past and were familiar with the subject matter under consideration. We wrote a"
P06-1004,2005.sigdial-1.13,0,0.0466991,"less clear-cut concept of a sub-topic. We define a segment to be a sub-topic that signals a prominent shift in subject matter. Disregarding this sub-topic change would impair the high-level understanding of the structure and the content of the lecture. As part of our human segmentation analysis, we asked three annotators to segment the Physics lecture corpus. These annotators had taken the class in the past and were familiar with the subject matter under consideration. We wrote a detailed instruction manual for the task, with annotation guidelines for the most part following the model used by Gruenstein et al. (2005). The annotators were instructed to segment at a level of granularity 2 A speaker-dependent model of the lecturer was trained on 38 hours of lectures from other courses using the SUMMIT segment-based Speech Recognizer (Glass, 2003). 29 M EAN S EG . C OUNT M EAN S EG . L ENGTH S EG . L ENGTH DEV. O 6.6 69.4 39.6 A 8.9 51.5 37.4 B 18.4 24.9 34.5 C 13.8 33.2 39.4 PK WD Table 2: Annotator Segmentation Statistics for the first ten Physics lectures. R EF /H YP O A B C O 0 0.219 0.314 0.260 A 0.243 0 0.337 0.296 B 0.418 0.400 0 0.370 E DGE C UTOFF 10 25 50 100 200 P HYSICS (M ANUAL ) 0.394 0.373 0.34"
P06-1004,P94-1002,0,0.524256,"ntroduction The development of computational models of text structure is a central concern in natural language processing. Text segmentation is an important instance of such work. The task is to partition a text into a linear sequence of topically coherent segments and thereby induce a content structure of the text. The applications of the derived representation are broad, encompassing information retrieval, question-answering and summarization. Not surprisingly, text segmentation has been extensively investigated over the last decade. Following the first unsupervised segmentation approach by Hearst (1994), most algorithms assume that variations in lexical distribution indicate topic changes. When documents exhibit sharp variations in lexical distribution, these algorithms are likely to detect segment boundaries accurately. For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000). The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle. This is evident in the performance of existing unsupervised algorithms on less 25 Proceedings of the 21st Internation"
P06-1004,E03-1058,0,0.698165,"uly 2006. 2006 Association for Computational Linguistics erty of the algorithm is its robustness to noise: the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output. 2 Previous Work Most unsupervised algorithms assume that fragments of text with homogeneous lexical distribution correspond to topically coherent segments. Previous research has analyzed various facets of lexical distribution, including lexical weighting, similarity computation, and smoothing (Hearst, 1994; Utiyama and Isahara, 2001; Choi, 2000; Reynar, 1998; Kehagias et al., 2003; Ji and Zha, 2003). The focus of our work, however, is on an orthogonal yet fundamental aspect of this analysis — the impact of long-range cohesion dependencies on segmentation performance. In contrast to previous approaches, the homogeneity of a segment is determined not only by the similarity of its words, but also by their relation to words in other segments of the text. We show that optimizing our global objective enables us to detect subtle topical changes. Graph-Theoretic Approaches in Vision Segmentation Our work is inspired by minimum-cutbased segmentation algorithms developed for image analysis. Shi an"
P06-1004,J02-1002,0,0.49274,"synthetic corpus created by Choi (2000) which is commonly used in the evaluation of segmentation algorithms. This corpus consists of a set of concatenated segments randomly sampled from the Brown corpus. The length of the segments in this corpus ranges from three to eleven sentences. It is important to note that the lexical transitions in these concatenated texts are very sharp, since the segments come from texts written in widely varying language styles on completely different topics. 5.3 Evaluation Metric We use the Pk and WindowDiff measures to evaluate our system (Beeferman et al., 1999; Pevzner and Hearst, 2002). The Pk measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified. The WindowDiff metric is a variant of the Pk measure, which penalizes false positives on an equal basis with near misses. Both of these metrics are defined with respect to the average segment length of texts and exhibit high variability on real data. We follow Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately. We also plot the Receiver Operating Characteristic (ROC) curve to gauge perf"
P06-1004,P01-1064,0,0.812707,"nal Linguistics and 44th Annual Meeting of the ACL, pages 25–32, c Sydney, July 2006. 2006 Association for Computational Linguistics erty of the algorithm is its robustness to noise: the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output. 2 Previous Work Most unsupervised algorithms assume that fragments of text with homogeneous lexical distribution correspond to topically coherent segments. Previous research has analyzed various facets of lexical distribution, including lexical weighting, similarity computation, and smoothing (Hearst, 1994; Utiyama and Isahara, 2001; Choi, 2000; Reynar, 1998; Kehagias et al., 2003; Ji and Zha, 2003). The focus of our work, however, is on an orthogonal yet fundamental aspect of this analysis — the impact of long-range cohesion dependencies on segmentation performance. In contrast to previous approaches, the homogeneity of a segment is determined not only by the similarity of its words, but also by their relation to words in other segments of the text. We show that optimizing our global objective enables us to detect subtle topical changes. Graph-Theoretic Approaches in Vision Segmentation Our work is inspired by minimum-c"
P06-1004,J97-1005,0,\N,Missing
P06-1004,P06-1003,0,\N,Missing
P06-1004,P94-1050,0,\N,Missing
P06-1004,P93-1041,0,\N,Missing
P06-1004,W05-0405,0,\N,Missing
P06-1004,J86-3001,0,\N,Missing
P07-1064,W01-0514,0,0.484523,"ure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment boundaries by identifying homogeneous regions within a similarity matrix that encodes pairwise similarity between textual units, such as sentences. Our segmentation algorithm operates over a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data Our work is related"
P07-1064,P03-1071,0,0.109752,"r applications where a speech recognizer is not available, or its output has a high word error rate. 1 Introduction An important practical application of topic segmentation is the analysis of spoken data. Paragraph breaks, section markers and other structural cues common in written documents are entirely missing in spoken data. Insertion of these structural markers can benefit multiple speech processing applications, including audio browsing, retrieval, and summarization. Not surprisingly, a variety of methods for topic segmentation have been developed in the 504 past (Beeferman et al., 1999; Galley et al., 2003; Dielmann and Renals, 2005). These methods typically assume that a segmentation algorithm has access not only to acoustic input, but also to its transcript. This assumption is natural for applications where the transcript has to be computed as part of the system output, or it is readily available from other system components. However, for some domains and languages, the transcripts may not be available, or the recognition performance may not be adequate to achieve reliable segmentation. In order to process such data, we need a method for topic segmentation that does not require transcribed in"
P07-1064,P94-1002,0,0.038429,"scourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment boundaries by identifying homogeneous regions within a similarity matrix that encodes pairwise similarity between textual units, such as sentences. Our segmentation algorithm operates over a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data"
P07-1064,P96-1038,0,0.0266596,"cripts are unavailable or highly errorful. 2 Related Work Speech-based Topic Segmentation A variety of supervised and unsupervised methods have been employed to segment speech input. Some of these algorithms have been originally developed for processing written text (Beeferman et al., 1999). Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005). In parallel, researchers ex505 tensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000). However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text. At the same time, our work is closely related to unsupervised approaches for text segmentation. The central assumption here is that sharp changes in lexical distribution signal the presence of topic boundaries (Hearst, 1994; Choi et al., 2001). These approaches determine segment bounda"
P07-1064,E03-1058,0,0.316955,"these path-free blocks dilute segment homogeneity. This is problematic because it is not always possible to tell whether a sudden shift in scores signifies a transition or if it is just an artifact of irregularities in acoustic matching. Without additional matrix processing, these irregularities will lead the system astray. We further refine the acoustic comparison matrix using anisotropic diffusion. This technique has been developed for enhancing edge detection accuracy in image processing (Perona and Malik, 1990), and has been shown to be an effective smoothing method in text segmentation (Ji and Zha, 2003). When applied to a comparison matrix, anisotropic diffusion reduces score variability within homogeneous reMatrix Smoothing Equipped with a block distortion measure, we can now construct an acoustic comparison matrix. In principle, this matrix can be processed employing standard methods developed 2 We converted the original comparison distortion matrix to for text segmentation. However, as Figure 1 illus- the similarity matrix by subtracting the component distortions trates, the structure of the acoustic matrix is quite from the maximum alignment distortion score. 508 gions of the matrix and"
P07-1064,P06-1004,1,0.705878,"we can observe that the boundary structure in the diffused comparison matrix becomes more salient and corresponds more closely to the reference segmentation. 3.3 Matrix Partitioning Given a target number of segments k, the goal of the partitioning step is to divide a matrix into k square submatrices along the diagonal. This process is guided by an optimization function that maximizes the homogeneity within a segment or minimizes the homogeneity across segments. This optimization problem can be solved using one of many unsupervised segmentation approaches (Choi et al., 2001; Ji and Zha, 2003; Malioutov and Barzilay, 2006). In our implementation, we employ the minimumcut segmentation algorithm (Shi and Malik, 2000; Malioutov and Barzilay, 2006). In this graphtheoretic framework, segmentation is cast as a problem of partitioning a weighted undirected graph that minimizes the normalized-cut criterion. The minimum-cut method achieves robust analysis by jointly considering all possible partitionings of a document, moving beyond localized decisions. This allows us to aggregate comparisons from multiple locations, thereby compensating for the noise of individual matches. 4 Evaluation Set-Up Data We use a publicly ava"
P07-1064,J02-1002,0,0.377253,"Missing"
P07-1064,P01-1064,0,0.0639328,"Missing"
P07-1064,J01-3002,0,0.0210106,"er a distortion matrix, but the unit of comparison is the speech signal over a time interval. This change in representation gives rise to multiple challenges related to the inherent noise of acoustic matching, and requires the development of new methods for signal discretization, interval comparison and matrix analysis. Pattern Induction in Acoustic Data Our work is related to research on unsupervised lexical acquisition from continuous speech. These methods aim to infer vocabulary from unsegmented audio streams by analyzing regularities in pattern distribution (de Marcken, 1996; Brent, 1999; Venkataraman, 2001). Traditionally, the speech signal is first converted into a string-like representation such as phonemes and syllables using a phonetic recognizer. Park and Glass (2006) have recently shown the feasibility of an audio-based approach for word discovery. They induce the vocabulary from the audio stream directly, avoiding the need for phonetic transcription. Their method can accurately discover words which appear with high frequency in the audio stream. While the results obtained by Park and Glass inspire our approach, we cannot directly use their output as proxies for words in topic segmentation"
P07-1069,P00-1041,0,0.037785,"on. Our work, however, is closer to domainindependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation. Moreover, extraction for each segment is performed in isolation, and global constraints on the summary are not enforced. Finally, our work is also related to research on title generation (Banko et al., 2000; Jin and Hauptmann, 2001; Dorr et al., 2003). Since work in this area focuses on generating titles for one article at a time (e.g., newspaper reports), the issue of hierarchical generation, which is unique to our task, does not arise. However, this is not the only novel aspect of the proposed approach. Our model learns title generation in a fully discriminative framework, in contrast to the commonly used noisy-channel model. Thus, instead of independently modeling the selection and grammaticality constraints, we learn both types of features in a single framework. This joint training regime su"
P07-1069,P04-1015,0,0.650561,"arge: for the local model, it is exponential in the length of the segment title, and for the global model it is exponential in the size of the tree. Therefore, we construct the output for each of these models incrementally using beam search. The algorithm maintains the most promising partial output structures, which are extended at every iteration. The model incorporates this decoding procedure into the training process, thereby learning model parameters best suited for the specific decoding algorithm. Similar models have been successfully applied in the past to other tasks including parsing (Collins and Roark, 2004), chunking (Daum´e and Marcu, 2005), and machine translation (Cowan et al., 2006). 4.1 Model Structure The model takes as input a tree of text segments S. Each segment s ∈ S and its title z are represented as a local feature vector Φloc (s, z). Each component of this vector stores a numerical value. This feature vector can track any feature of the segment s together with its title z. For instance, the ith component of this vector may indicate whether the bigram (z[j]z[j + 1]) occurs in s, where z[j] is the j th word in z:  1 if (z[j]z[j + 1]) ∈ s (Φloc (s, z))i = 0 otherwise In addition, our"
P07-1069,W06-1628,0,0.0287347,"or the global model it is exponential in the size of the tree. Therefore, we construct the output for each of these models incrementally using beam search. The algorithm maintains the most promising partial output structures, which are extended at every iteration. The model incorporates this decoding procedure into the training process, thereby learning model parameters best suited for the specific decoding algorithm. Similar models have been successfully applied in the past to other tasks including parsing (Collins and Roark, 2004), chunking (Daum´e and Marcu, 2005), and machine translation (Cowan et al., 2006). 4.1 Model Structure The model takes as input a tree of text segments S. Each segment s ∈ S and its title z are represented as a local feature vector Φloc (s, z). Each component of this vector stores a numerical value. This feature vector can track any feature of the segment s together with its title z. For instance, the ith component of this vector may indicate whether the bigram (z[j]z[j + 1]) occurs in s, where z[j] is the j th word in z:  1 if (z[j]z[j + 1]) ∈ s (Φloc (s, z))i = 0 otherwise In addition, our model captures dependencies among multiple titles that appear in the same tableof"
P07-1069,W03-0501,0,0.0247879,"ependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation. Moreover, extraction for each segment is performed in isolation, and global constraints on the summary are not enforced. Finally, our work is also related to research on title generation (Banko et al., 2000; Jin and Hauptmann, 2001; Dorr et al., 2003). Since work in this area focuses on generating titles for one article at a time (e.g., newspaper reports), the issue of hierarchical generation, which is unique to our task, does not arise. However, this is not the only novel aspect of the proposed approach. Our model learns title generation in a fully discriminative framework, in contrast to the commonly used noisy-channel model. Thus, instead of independently modeling the selection and grammaticality constraints, we learn both types of features in a single framework. This joint training regime supports greater flexibility in modeling featur"
P07-1069,P94-1002,0,0.205186,"mmary could serve as an effective navigation tool for understanding a long, unstructured transcript for an academic lecture or a meeting. Given a text, our goal is to generate a tree wherein a node represents a segment of text and a title that summarizes its content. This process involves two tasks: the hierarchical segmentation of the text, and the generation of informative titles for each segment. The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). In this paper, we take the former approach. As for the second task, a naive approach would be to employ existing methods of title generation to each segment, and combine the results into a tree structure. However, the latter approach cannot guarantee that the generated table-of-contents forms a coherent representation of the entire text. Since titles of different segments are generated in isolation, some of the generated titles may be repetitive. Even nonrepetitive titles may not provide sufficient information to discriminate between the content of one segProceedings of the 45th Annual Meeti"
P07-1069,H01-1011,0,0.123392,"r, is closer to domainindependent methods for summarizing long texts. Typically, these approaches employ topic segmentation to identify a list of topics described in a document, and then produce a summary for each part (Boguraev and Neff, 2000; Angheluta et al., 2002). In contrast to our method, these approaches perform either sentence or phrase extraction, rather than summary generation. Moreover, extraction for each segment is performed in isolation, and global constraints on the summary are not enforced. Finally, our work is also related to research on title generation (Banko et al., 2000; Jin and Hauptmann, 2001; Dorr et al., 2003). Since work in this area focuses on generating titles for one article at a time (e.g., newspaper reports), the issue of hierarchical generation, which is unique to our task, does not arise. However, this is not the only novel aspect of the proposed approach. Our model learns title generation in a fully discriminative framework, in contrast to the commonly used noisy-channel model. Thus, instead of independently modeling the selection and grammaticality constraints, we learn both types of features in a single framework. This joint training regime supports greater flexibilit"
P07-1069,I05-2027,0,0.0371369,"Missing"
P07-1069,J02-4002,0,\N,Missing
P08-1031,P05-1045,0,0.0324168,"awn from a Beta distribution with prior λ0 . We have zd,n ∼ ηd if cd,n = 1, and zd,n ∼ φd otherwise. Finally, the word wd,n is drawn from the multinomial θzd,n , where zd,n indexes a topic-specific language model. Each of the K language models θk is drawn from a symmetric Dirichlet prior θ0 . 5 Posterior Sampling Ultimately, we need to compute the model’s posterior distribution given the training data. Doing so analytically is intractable due to the complexity of the model, but sampling-based techniques can be used to estimate the posterior. We employ Gibbs sampling, previously used in NLP by Finkel et al. (2005) and Goldwater et al. (2006), among others. This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model (Gelman et al., 2004). We now present sampling equations for each of the hidden variables in Figure 2. The prior over keyphrase clusters ψ is sampled based on hyperprior ψ0 and keyphrase cluster assignments x. We write p(ψ |. . .) to mean the probability conditioned on all the other variables. p(ψ |. . .) ∝ p(ψ |ψ0 )p(x |ψ)"
P08-1031,P06-1085,0,0.00469492,"ion with prior λ0 . We have zd,n ∼ ηd if cd,n = 1, and zd,n ∼ φd otherwise. Finally, the word wd,n is drawn from the multinomial θzd,n , where zd,n indexes a topic-specific language model. Each of the K language models θk is drawn from a symmetric Dirichlet prior θ0 . 5 Posterior Sampling Ultimately, we need to compute the model’s posterior distribution given the training data. Doing so analytically is intractable due to the complexity of the model, but sampling-based techniques can be used to estimate the posterior. We employ Gibbs sampling, previously used in NLP by Finkel et al. (2005) and Goldwater et al. (2006), among others. This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model (Gelman et al., 2004). We now present sampling equations for each of the hidden variables in Figure 2. The prior over keyphrase clusters ψ is sampled based on hyperprior ψ0 and keyphrase cluster assignments x. We write p(ψ |. . .) to mean the probability conditioned on all the other variables. p(ψ |. . .) ∝ p(ψ |ψ0 )p(x |ψ), = p(ψ |ψ0 ) L Y p(xℓ |ψ) ℓ"
P08-1031,P06-2063,0,0.0598858,"ments are annotated in a noisy manner. In this work, we apply our method to a collection of reviews in two categories: restaurants and cell phones. The training data consists of review text and the associated pros/cons lists. We then evaluate the ability of our model to predict review properties when the pros/cons list is hidden. Across a variety of evaluation scenarios, our algorithm consistently outperforms alternative strategies by a wide margin. 2 Related Work Review Analysis Our approach relates to previous work on property extraction from reviews (Popescu et al., 2005; Hu and Liu, 2004; Kim and Hovy, 2006). These methods extract lists of phrases, which are analogous to the keyphrases we use as input to our algorithm. However, our approach is distinguished in two ways: first, we are able to predict keyphrases beyond those that appear verbatim in the text. Second, our approach learns the relationships between keyphrases, allowing us to draw direct comparisons between reviews. Bayesian Topic Modeling One aspect of our model views properties as distributions over words in the document. This approach is inspired by methods in the topic modeling literature, such as Latent Dirichlet Allocation (LDA) ("
P08-1031,H05-2017,0,0.410414,"applicable to many scenarios where documents are annotated in a noisy manner. In this work, we apply our method to a collection of reviews in two categories: restaurants and cell phones. The training data consists of review text and the associated pros/cons lists. We then evaluate the ability of our model to predict review properties when the pros/cons list is hidden. Across a variety of evaluation scenarios, our algorithm consistently outperforms alternative strategies by a wide margin. 2 Related Work Review Analysis Our approach relates to previous work on property extraction from reviews (Popescu et al., 2005; Hu and Liu, 2004; Kim and Hovy, 2006). These methods extract lists of phrases, which are analogous to the keyphrases we use as input to our algorithm. However, our approach is distinguished in two ways: first, we are able to predict keyphrases beyond those that appear verbatim in the text. Second, our approach learns the relationships between keyphrases, allowing us to draw direct comparisons between reviews. Bayesian Topic Modeling One aspect of our model views properties as distributions over words in the document. This approach is inspired by methods in the topic modeling literature, such"
P08-1031,P08-1036,0,0.187105,"topic modeling literature, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), where topics are treated as hidden variables that govern the distribution of words in a text. Our algorithm extends this notion by biasing the induced hidden topics toward a clustering of known keyphrases. Tying these two information sources together enhances the robustness of the hidden topics, thereby increasing 264 the chance that the induced structure corresponds to semantically meaningful properties. Recent work has examined coupling topic models with explicit supervision (Blei and McAuliffe, 2007; Titov and McDonald, 2008). However, such approaches assume that the documents are labeled within a predefined annotation structure, e.g., the properties of food, ambiance, and service for restaurants. In contrast, we address free-text annotations created by end users, without known semantic properties. Rather than requiring a predefined annotation structure, our model infers one from the data. 3 Problem Formulation We formulate our problem as follows. We assume a dataset composed of documents with associated keyphrases. Each document may be marked with multiple keyphrases that express unseen semantic properties. Acros"
P08-1031,J98-3005,0,\N,Missing
P08-1031,W05-1612,0,\N,Missing
P08-1031,H01-1054,0,\N,Missing
P08-1031,C00-2137,0,\N,Missing
P08-1031,D07-1109,0,\N,Missing
P08-1031,P01-1008,1,\N,Missing
P08-1031,P06-1003,0,\N,Missing
P08-1031,W00-0403,0,\N,Missing
P08-1031,J93-3001,0,\N,Missing
P08-1031,W05-0908,0,\N,Missing
P08-1031,N07-1038,1,\N,Missing
P08-1031,J06-4012,0,\N,Missing
P08-1084,P06-1084,0,0.00774535,"es the performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon. A locally optimal segmen"
P08-1084,D07-1023,0,0.0303016,"gger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon. A locally optimal segmentation is identified using a task-specific greed"
P08-1084,P02-1033,0,0.045504,"Missing"
P08-1084,feldman-etal-2006-cross,0,0.0299148,"Missing"
P08-1084,J01-2001,0,0.814261,"ech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon."
P08-1084,P06-1085,0,0.0125948,") and abstract (k) morphemes are drawn from a Poisson distribution. (b) Stray morphemes are then drawn from E and F (language-specific distributions) and abstract morphemes are drawn from A. (c) The resulting morphemes are ordered. (d) Finally, some of the contiguous morphemes are fused into words. controlled by the prior α. Nevertheless, some nonzero probability is reserved for every possible string. We note that these single-language morpheme distributions also serve as monolingual segmentation models, and similar models have been successfully applied to the task of word boundary detection (Goldwater et al., 2006). Abstract Morpheme Distribution To model the connections between morphemes across languages, we further define a model for bilingual morpheme pairs, or abstract morphemes. This model assigns probabilities to all pairs of morphemes – that is, all pairs of finite strings from the respective alphabets – (e, f ). Intuitively, we wish to assign high probability to pairs of morphemes that play similar syntactic or semantic roles (e.g. (fy, b-) for “in” in Arabic and Hebrew). These morpheme pairs can thus be viewed as representing abstract morphemes. As with the stray morpheme models, we wish to def"
P08-1084,P05-1071,0,0.035212,"Missing"
P08-1084,W04-3229,0,0.0133278,"g annotations provided by an English stemmer. An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006). While their approach does not require a parallel corpus it does assume the availability of annotations in one language. Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language. The key assumption here is that certain distributional properties are invariant across languages from the same language families. An example of such a property is the distribution of part-of-speech bigrams. Hana et al., (2004) demonstrate that adding such statistics from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morpholo"
P08-1084,P06-1146,0,0.0308927,"Missing"
P08-1084,P03-1050,0,0.037236,"t benefit. 2 Related Work Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006). Most of this research assumes that one language has annotations for the task of interest. Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language. In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer. An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006). While their approach does not require a parallel corpus it does assume the availability of annotations in one language. Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language. The key assumption here is that certain distributional properties are invariant across languages from the same language families. An example of such a pr"
P08-1084,W00-0712,0,0.0586059,"istics from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditione"
P08-1084,H05-1107,0,0.0772875,"Missing"
P08-1084,H01-1035,0,\N,Missing
P08-1097,P01-1016,0,0.0270445,"t, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriented dialogue (Cassell et al., 2001). These findings are exploited to generate realistic conversational “grounding” behavior in an animated agent. The semantic content of gesture was leveraged – again, for gesture generation – in (Kopp et al., 2007), which presents an animated agent that is capable of augmenting navigation directions with gestures that describe the physical properties of landmarks along the route. Both systems generate plausible and human-like gestural behavior; we address the converse problem of interpreting such gestures. In this vein, hand-coded gesture features have been used to improve sentence segmentation"
P08-1097,P07-1045,1,0.363629,"ures. In this vein, hand-coded gesture features have been used to improve sentence segmentation, show853 ing that sentence boundaries are unlikely to overlap gestures that are in progress (Chen et al., 2006). Features that capture the start and end of gestures are shown to improve sentence segmentation beyond lexical and prosodic features alone. This idea of gestural features as a sort of visual punctuation has parallels in the literature on prosody, which we discuss in the next subsection. Finally, ambiguous noun phrases can be resolved by examining the similarity of co-articulated gestures (Eisenstein and Davis, 2007). While noun phrase coreference can be viewed as a discourse processing task, we address the higher-level discourse phenomenon of topic segmentation. In addition, this prior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation ha"
P08-1097,J86-3001,0,0.625896,"Missing"
P08-1097,P94-1002,0,0.100677,"tein, Regina Barzilay and Randall Davis Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology 77 Massachusetts Ave., Cambridge MA 02139 {jacobe, regina, davis}@csail.mit.edu Abstract method builds on the idea that coherent discourse segments are characterized by gestural cohesion; in other words, that such segments exhibit homogeneous gestural patterns. Lexical cohesion (Halliday and Hasan, 1976) forms the backbone of many verbal segmentation algorithms, on the theory that segmentation boundaries should be placed where the distribution of words changes (Hearst, 1994). With gestural cohesion, we explore whether the same idea holds for gesture features. This paper explores the relationship between discourse segmentation and coverbal gesture. Introducing the idea of gestural cohesion, we show that coherent topic segments are characterized by homogeneous gestural forms and that changes in the distribution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manua"
P08-1097,H93-1016,0,0.168793,"Missing"
P08-1097,P98-1102,0,0.0451873,"structure. Applying our algorithm to a dataset of faceto-face dialogues, we find that gesture communicates unique information, improving segmentation performance over lexical features alone. The positive impact of gesture is most pronounced when automatically-recognized speech transcripts are used, but gestures improve performance by a significant margin even in combination with manual transcripts. 2 Related Work Gesture and discourse Much of the work on gesture in natural language processing has focused on multimodal dialogue systems in which the gestures and speech may be constrained, e.g. (Johnston, 1998). In contrast, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriente"
P08-1097,P06-1004,1,0.866223,"setting for ψ is found via a gradient-based search. This setting is then used to generate another segmentation, and the process is iterated until convergence, as in hard expectationmaximization. The Dirichlet priors on the language models are symmetric, and are chosen via crossvalidation. Sampling or gradient-based techniques may be used to estimate these parameters, but this is left for future work. Relation to other segmentation models Other cohesion-based techniques have typically focused on hand-crafted similarity metrics between sentences, such as cosine similarity (Galley et al., 2003; Malioutov and Barzilay, 2006). In contrast, the model described here is probabilistically motivated, maximizing the joint probability of the segmentation with the observed words and gestures. Our objective criterion is similar in form to that of Utiyama and Isahara (2001); however, in contrast to this prior work, our criterion is justified by a Bayesian approach. Also, while the smoothing in our approach arises naturally from the symmetric Dirichlet prior, Utiyama and Isahara apply Laplace’s rule and add pseudo-counts of one in all cases. Such an approach would be incapable of flexibly balancing the contributions of each"
P08-1097,J97-1005,0,0.0545908,"bution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manual and automaticallyrecognized speech transcripts. 1 Introduction When people communicate face-to-face, discourse cues are expressed simultaneously through multiple channels. Previous research has extensively studied how discourse cues correlate with lexico-syntactic and prosodic features (Hearst, 1994; Hirschberg and Nakatani, 1998; Passonneau and Litman, 1997); this work informs various text and speech processing applications, such as automatic summarization and segmentation. Gesture is another communicative modality that frequently accompanies speech, yet it has not been exploited for computational discourse analysis. This paper empirically demonstrates that gesture correlates with discourse structure. In particular, we show that automatically-extracted visual features can be combined with lexical cues in a statistical model to predict topic segmentation, a frequently studied form of discourse structure. Our The motivation for this approach comes"
P08-1097,J02-1002,0,0.025458,"lion words. The entire ICSI meeting corpus contains roughly 600,000 words, although only one third of this dataset was annotated for segmentation (Galley et al., 2003). The physics lecture corpus that was mentioned above contains 232,000 words (Malioutov and Barzilay, 2006). The task considered in this section is thus more difficult than much of the previous discourse segmentation work on two dimensions: there is less training data, and a finer-grained segmentation is required. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. These metrics are penalties, so lower values indicate better segmentations. The Pk metric expresses the probability that any randomly chosen pair of sentences is incorrectly segmented, if they are k sentences apart (Beeferman et al., 1999). Following tradition, k is set to half of the mean segMethod 1. gesture only 2. ASR only 3. ASR + gesture 4. transcript only 5. transcript + gesture 6. random 7. equal-width Pk .486 .462 .388 .382 .332 .473 .508 nificant for both Pk (t(14) = 2.00, p < .05) and WD (t(14) = 1.94, p < .05). WD .502 .476 .401 .397 .349 .526 .515 Table 1: For each method"
P08-1097,P90-1002,0,0.169566,"of topic segmentation. In addition, this prior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation has primarily focused on prosody, under the assumption that a key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between ges"
P08-1097,J01-1002,0,0.0204685,"key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between gesture and discourse structure is a relatively unexplored area, at least with respect to computational approaches. One conclusion that emerges from our analysis is that gesture may signal discourse structure in a different way than prosody does: while specific prosodic markers characterize segment boundaries, gesture predicts segmentation through intrasegmental cohesion. The combination of these two modalities is an exciting direction for future research. 3 Visual Features for Discourse Analysis This section describes the process o"
P08-1097,P01-1064,0,0.238092,"ferent priors for the verbal and gestural language models allows us to weight these modalities in a Bayesian framework. Finally, we model the probability of the segmentation z by Q considering the durations of each segment: p(z) = i p(dur(i)|ψ). A negativebinomial distribution with parameter ψ is applied to discourage extremely short or long segments. Inference Crucially, both the likelihood (equation 1) and the prior (equation 2) factor into a product across the segments. This factorization enables the optimal segmentation to be found using a dynamic program, similar to those demonstrated by Utiyama and Isahara (2001) and Malioutov and 856 Barzilay (2006). For each set of segmentation points z, the associated language models are set to their posterior expectations, e.g., θi = E[θ|{xt : zt = i}, α]. The Dirichlet prior is conjugate to the multinomial, so this expectation can be computed in closed form: θi,j = n(i, j) + α , N (i) + W α (3) where n(i, j) is the count of word j in segment i and N (i) is the total number of words in segment i (Bernardo and Smith, 2000). The symmetric Dirichlet prior α acts as a smoothing pseudo-count. In the multimodal context, the priors act to control the weight of each modal"
P08-1097,P03-1071,0,\N,Missing
P08-1097,P06-1003,0,\N,Missing
P08-1097,C98-1099,0,\N,Missing
P08-1097,P07-1094,0,\N,Missing
P09-1009,P02-1017,0,0.718158,"goal is to explore the benefits of multilingual grammar induction in a fully unsupervised setting. We finally note a recent paper which uses parameter tying to improve unsupervised dependency parse induction (Cohen and Smith, 2009). While the primary performance gains occur when tying related parameters within a language, some additional benefit is observed through bilingual tying, even in the absence of a parallel corpus. Related Work The unsupervised grammar induction task has been studied extensively, mostly in a monolingual setting (Charniak and Carroll, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Seginer, 2007). While PCFGs perform poorly on this task, the CCM model (Klein and Manning, 2002) has achieved large gains in performance and is among the state-of-the-art probabilistic models for unsupervised constituency parsing. We therefore use CCM as our basic model of monolingual syntax. While there has been some previous work on bilingual CFG parsing, it has mainly focused on improving MT systems rather than monolingual parsing accuracy. Research in this direction was pioneered by (Wu, 1997), who developed Inversion Transduction Grammars to capture crosslingual grammar variations such"
P09-1009,P04-1060,0,0.770386,"Missing"
P09-1009,D08-1092,0,0.293689,"ce falls short of state-of-the-art monolingual models such as the CCM. More recently, there has been a body of work attempting to improve parsing performance by exploiting syntactically annotated parallel data. In one strand of this work, annotations are assumed only in a resource-rich language and are projected onto a resource-poor language using the parallel data (Hwa et al., 2005; Xi and Hwa, 2005). In another strand of work, syntactic annotations are assumed on both sides of the parallel data, and a model is trained to exploit the parallel data at test time as well (Smith and Smith, 2004; Burkett and Klein, 2008). In contrast to this work, our goal is to explore the benefits of multilingual grammar induction in a fully unsupervised setting. We finally note a recent paper which uses parameter tying to improve unsupervised dependency parse induction (Cohen and Smith, 2009). While the primary performance gains occur when tying related parameters within a language, some additional benefit is observed through bilingual tying, even in the absence of a parallel corpus. Related Work The unsupervised grammar induction task has been studied extensively, mostly in a monolingual setting (Charniak and Carroll, 199"
P09-1009,N03-1021,0,0.0191491,"in performance and is among the state-of-the-art probabilistic models for unsupervised constituency parsing. We therefore use CCM as our basic model of monolingual syntax. While there has been some previous work on bilingual CFG parsing, it has mainly focused on improving MT systems rather than monolingual parsing accuracy. Research in this direction was pioneered by (Wu, 1997), who developed Inversion Transduction Grammars to capture crosslingual grammar variations such as phrase reorderings. More general formalisms for the same purpose were later developed (Wu and Wong, 1998; Chiang, 2005; Melamed, 2003; Eisner, 3 Model We propose an unsupervised Bayesian model for learning bilingual syntactic structure using parallel corpora. Our key premise is that difficult-tolearn syntactic structures of one language may correspond to simpler or less uncertain structures in the other language. We treat the part-of-speech tag sequences of parallel sentences, as well as their 74 (i) (ii) (iii) Figure 1: A pair of trees (i) and two possible alignment trees. In (ii), no empty spaces are inserted, but the order of one of the original tree’s siblings has been reversed. In (iii), only two pairs of nodes have be"
P09-1009,J03-1002,0,0.0104492,"learn syntactic structures of one language may correspond to simpler or less uncertain structures in the other language. We treat the part-of-speech tag sequences of parallel sentences, as well as their 74 (i) (ii) (iii) Figure 1: A pair of trees (i) and two possible alignment trees. In (ii), no empty spaces are inserted, but the order of one of the original tree’s siblings has been reversed. In (iii), only two pairs of nodes have been aligned (indicated by arrows) and many empty spaces inserted. 3.2 word-level alignments, as observed data. We obtain these word-level alignments using GIZA ++ (Och and Ney, 2003). As our basic model of syntactic structure, we adopt the Constituent-Context Model (CCM) of Klein and Manning (2002). Under this model, the part-of-speech sequence of each span in a sentence is generated either as a constituent yield — if it is dominated by a node in the tree — or otherwise as a distituent yield. For example, in the bracketed sentence [John/NNP [climbed/VB [the/DT tree/NN]]], the sequence VB DT NN is generated as a constituent yield, since it constitutes a complete bracket in the tree. On the other hand, the sequence VB DT is generated as a distituent, since it does not. Besi"
P09-1009,P05-1033,0,0.0510595,"ed large gains in performance and is among the state-of-the-art probabilistic models for unsupervised constituency parsing. We therefore use CCM as our basic model of monolingual syntax. While there has been some previous work on bilingual CFG parsing, it has mainly focused on improving MT systems rather than monolingual parsing accuracy. Research in this direction was pioneered by (Wu, 1997), who developed Inversion Transduction Grammars to capture crosslingual grammar variations such as phrase reorderings. More general formalisms for the same purpose were later developed (Wu and Wong, 1998; Chiang, 2005; Melamed, 2003; Eisner, 3 Model We propose an unsupervised Bayesian model for learning bilingual syntactic structure using parallel corpora. Our key premise is that difficult-tolearn syntactic structures of one language may correspond to simpler or less uncertain structures in the other language. We treat the part-of-speech tag sequences of parallel sentences, as well as their 74 (i) (ii) (iii) Figure 1: A pair of trees (i) and two possible alignment trees. In (ii), no empty spaces are inserted, but the order of one of the original tree’s siblings has been reversed. In (iii), only two pairs o"
P09-1009,P07-1049,0,0.0492605,"nefits of multilingual grammar induction in a fully unsupervised setting. We finally note a recent paper which uses parameter tying to improve unsupervised dependency parse induction (Cohen and Smith, 2009). While the primary performance gains occur when tying related parameters within a language, some additional benefit is observed through bilingual tying, even in the absence of a parallel corpus. Related Work The unsupervised grammar induction task has been studied extensively, mostly in a monolingual setting (Charniak and Carroll, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Seginer, 2007). While PCFGs perform poorly on this task, the CCM model (Klein and Manning, 2002) has achieved large gains in performance and is among the state-of-the-art probabilistic models for unsupervised constituency parsing. We therefore use CCM as our basic model of monolingual syntax. While there has been some previous work on bilingual CFG parsing, it has mainly focused on improving MT systems rather than monolingual parsing accuracy. Research in this direction was pioneered by (Wu, 1997), who developed Inversion Transduction Grammars to capture crosslingual grammar variations such as phrase reorde"
P09-1009,W04-3207,0,0.273102,"G. Still, the performance falls short of state-of-the-art monolingual models such as the CCM. More recently, there has been a body of work attempting to improve parsing performance by exploiting syntactically annotated parallel data. In one strand of this work, annotations are assumed only in a resource-rich language and are projected onto a resource-poor language using the parallel data (Hwa et al., 2005; Xi and Hwa, 2005). In another strand of work, syntactic annotations are assumed on both sides of the parallel data, and a model is trained to exploit the parallel data at test time as well (Smith and Smith, 2004; Burkett and Klein, 2008). In contrast to this work, our goal is to explore the benefits of multilingual grammar induction in a fully unsupervised setting. We finally note a recent paper which uses parameter tying to improve unsupervised dependency parse induction (Cohen and Smith, 2009). While the primary performance gains occur when tying related parameters within a language, some additional benefit is observed through bilingual tying, even in the absence of a parallel corpus. Related Work The unsupervised grammar induction task has been studied extensively, mostly in a monolingual setting"
P09-1009,N09-1009,0,0.166926,"ly in a resource-rich language and are projected onto a resource-poor language using the parallel data (Hwa et al., 2005; Xi and Hwa, 2005). In another strand of work, syntactic annotations are assumed on both sides of the parallel data, and a model is trained to exploit the parallel data at test time as well (Smith and Smith, 2004; Burkett and Klein, 2008). In contrast to this work, our goal is to explore the benefits of multilingual grammar induction in a fully unsupervised setting. We finally note a recent paper which uses parameter tying to improve unsupervised dependency parse induction (Cohen and Smith, 2009). While the primary performance gains occur when tying related parameters within a language, some additional benefit is observed through bilingual tying, even in the absence of a parallel corpus. Related Work The unsupervised grammar induction task has been studied extensively, mostly in a monolingual setting (Charniak and Carroll, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Seginer, 2007). While PCFGs perform poorly on this task, the CCM model (Klein and Manning, 2002) has achieved large gains in performance and is among the state-of-the-art probabilistic models for unsupervis"
P09-1009,P08-1084,1,0.360973,"CM model, a strong monolingual baseline. On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure. 1 1 Introduction In this paper we investigate the task of unsupervised constituency parsing when bilingual parallel text is available. Our goal is to improve parsing performance on monolingual test data for each language by using unsupervised bilingual cues at training time. Multilingual learning has been successful for other linguistic induction tasks such as lexicon acquisition, morphological segmentation, and part-of-speech tagging (Genzel, 2005; Snyder and Barzilay, 2008; Snyder et al., 2008; Snyder 1 Code and the outputs of our experiments are available at http://groups.csail.mit.edu/rbg/code/multiling induction. 73 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 73–81, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP lingual constituent, a sequence of part-of-speech tags is drawn from a language-specific distribution. For each pair of coupled bilingual constituents, a pair of part-of-speech sequences are drawn jointly from a cross-lingual distribution. Word-level alignments are then drawn based on the tree al"
P09-1009,P03-2041,0,0.159197,"Missing"
P09-1009,D08-1109,1,0.847525,"ual baseline. On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure. 1 1 Introduction In this paper we investigate the task of unsupervised constituency parsing when bilingual parallel text is available. Our goal is to improve parsing performance on monolingual test data for each language by using unsupervised bilingual cues at training time. Multilingual learning has been successful for other linguistic induction tasks such as lexicon acquisition, morphological segmentation, and part-of-speech tagging (Genzel, 2005; Snyder and Barzilay, 2008; Snyder et al., 2008; Snyder 1 Code and the outputs of our experiments are available at http://groups.csail.mit.edu/rbg/code/multiling induction. 73 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 73–81, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP lingual constituent, a sequence of part-of-speech tags is drawn from a language-specific distribution. For each pair of coupled bilingual constituents, a pair of part-of-speech sequences are drawn jointly from a cross-lingual distribution. Word-level alignments are then drawn based on the tree alignment. Finally, par"
P09-1009,H05-1110,0,0.0230735,"ins over the CCM model, a strong monolingual baseline. On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure. 1 1 Introduction In this paper we investigate the task of unsupervised constituency parsing when bilingual parallel text is available. Our goal is to improve parsing performance on monolingual test data for each language by using unsupervised bilingual cues at training time. Multilingual learning has been successful for other linguistic induction tasks such as lexicon acquisition, morphological segmentation, and part-of-speech tagging (Genzel, 2005; Snyder and Barzilay, 2008; Snyder et al., 2008; Snyder 1 Code and the outputs of our experiments are available at http://groups.csail.mit.edu/rbg/code/multiling induction. 73 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 73–81, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP lingual constituent, a sequence of part-of-speech tags is drawn from a language-specific distribution. For each pair of coupled bilingual constituents, a pair of part-of-speech sequences are drawn jointly from a cross-lingual distribution. Word-level alignments are then"
P09-1009,N09-1010,1,0.889248,"Missing"
P09-1009,P98-2230,0,0.0474176,"g, 2002) has achieved large gains in performance and is among the state-of-the-art probabilistic models for unsupervised constituency parsing. We therefore use CCM as our basic model of monolingual syntax. While there has been some previous work on bilingual CFG parsing, it has mainly focused on improving MT systems rather than monolingual parsing accuracy. Research in this direction was pioneered by (Wu, 1997), who developed Inversion Transduction Grammars to capture crosslingual grammar variations such as phrase reorderings. More general formalisms for the same purpose were later developed (Wu and Wong, 1998; Chiang, 2005; Melamed, 2003; Eisner, 3 Model We propose an unsupervised Bayesian model for learning bilingual syntactic structure using parallel corpora. Our key premise is that difficult-tolearn syntactic structures of one language may correspond to simpler or less uncertain structures in the other language. We treat the part-of-speech tag sequences of parallel sentences, as well as their 74 (i) (ii) (iii) Figure 1: A pair of trees (i) and two possible alignment trees. In (ii), no empty spaces are inserted, but the order of one of the original tree’s siblings has been reversed. In (iii), on"
P09-1009,J97-3002,0,0.395917,"n a monolingual setting (Charniak and Carroll, 1992; Stolcke and Omohundro, 1994; Klein and Manning, 2002; Seginer, 2007). While PCFGs perform poorly on this task, the CCM model (Klein and Manning, 2002) has achieved large gains in performance and is among the state-of-the-art probabilistic models for unsupervised constituency parsing. We therefore use CCM as our basic model of monolingual syntax. While there has been some previous work on bilingual CFG parsing, it has mainly focused on improving MT systems rather than monolingual parsing accuracy. Research in this direction was pioneered by (Wu, 1997), who developed Inversion Transduction Grammars to capture crosslingual grammar variations such as phrase reorderings. More general formalisms for the same purpose were later developed (Wu and Wong, 1998; Chiang, 2005; Melamed, 2003; Eisner, 3 Model We propose an unsupervised Bayesian model for learning bilingual syntactic structure using parallel corpora. Our key premise is that difficult-tolearn syntactic structures of one language may correspond to simpler or less uncertain structures in the other language. We treat the part-of-speech tag sequences of parallel sentences, as well as their 74"
P09-1009,H05-1107,0,0.11398,"languages are inconsistent with GIZA ++ wordlevel alignments. By incorporating these constraints into the EM algorithm he was able to improve performance over a monolingual unsupervised PCFG. Still, the performance falls short of state-of-the-art monolingual models such as the CCM. More recently, there has been a body of work attempting to improve parsing performance by exploiting syntactically annotated parallel data. In one strand of this work, annotations are assumed only in a resource-rich language and are projected onto a resource-poor language using the parallel data (Hwa et al., 2005; Xi and Hwa, 2005). In another strand of work, syntactic annotations are assumed on both sides of the parallel data, and a model is trained to exploit the parallel data at test time as well (Smith and Smith, 2004; Burkett and Klein, 2008). In contrast to this work, our goal is to explore the benefits of multilingual grammar induction in a fully unsupervised setting. We finally note a recent paper which uses parameter tying to improve unsupervised dependency parse induction (Cohen and Smith, 2009). While the primary performance gains occur when tying related parameters within a language, some additional benefit"
P09-1009,N07-1018,0,0.0132057,"d word alignments. For example, suppose the constituent my We define the distribution P (T1 , T2 , A) to be uniform over all pairs of binary trees and their alignments. 2. For each node in A of the form (n1 , λ) (i.e. nodes in T1 left unaligned by A), draw (i) a constituent yield according to π1C , 76 (ii) a constituent context according to φC 1, (iii) a Giza-score according to Gznode . all other sentences. The samples form a Markov chain which is guaranteed to converge to the true joint distribution over all sentences. In the monolingual setting, there is a wellknown tree sampling algorithm (Johnson et al., 2007). This algorithm proceeds in top-down fashion by sampling individual split points using the marginal probabilities of all possible subtrees. These marginals can be efficiently pre-computed and form the “inside” table of the famous InsideOutside algorithm. However, in our setting, trees come in pairs, and their joint probability crucially depends on their alignment. For the ith parallel sentence, we wish to jointly sample the pair of trees (T1 , T2 )i together with their alignment Ai . To do so directly would involve simultaneously marginalizing over all possible subtrees as well as all possibl"
P09-1009,P05-1059,0,0.0218326,"Missing"
P09-1009,C98-2225,0,\N,Missing
P09-1010,H89-1033,0,0.71984,"ons. Reinforcement learning is a natural framework for building models using validation from an environment (Sutton and Barto, 1998). We assume that supervision is provided in the form of a reward function that defines the quality of executed actions. During training, the learner repeatedly constructs action sequences for a set of given documents, executes those actions, and observes the resulting reward. The learner’s goal is to estimate a Introduction The problem of interpreting instructions written in natural language has been widely studied since the early days of artificial intelligence (Winograd, 1972; Di Eugenio, 1992). Mapping instructions to a sequence of executable actions would enable the automation of tasks that currently require human participation. Examples include configuring software based on how-to guides and operating simulators using instruction manuals. In this paper, we present a reinforcement learning framework for inducing mappings from text to actions without the need for annotated training examples. For concreteness, consider instructions from a Windows troubleshooting guide on deleting temporary folders, shown in Figure 1. We aim to map 1 Code, data, and annotations use"
P09-1010,P92-1016,0,0.509104,"Missing"
P09-1010,W05-0614,0,0.147141,"(a0 , . . . , an−1 ). Actions are predicted and executed sequentially.2 An action a = (c, R, W 0 ) encompasses a command c, the command’s parameters R, and the words W 0 specifying c and R. Elements of R refer to objects available in the environment state, as described below. Some parameters can also refer to words in document d. Additionally, to account for words that do not describe any actions, c can be a null command. Related Work Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 2001; Oates, 2001). Instances of such approaches include work on inferring the meaning of words from video data (Roy and Pentland, 2002; Barnard and Forsyth, 2001), and interpreting the commentary of a simulated soccer game (Chen and Mooney, 2008). Most of these approaches assume some form of parallel data, and learn perceptual cooccurrence patterns. In contrast, our emphasis is on learning language by proactively interacting with an external environment. The Environment The environment state E specifies the set of objects available for interaction, and their p"
P09-1010,C00-1073,0,0.0282889,"e E specifies the set of objects available for interaction, and their properties. In Figure 2, E is shown on the right. The environment state E changes in response to the execution of command c with parameters R according to a transition distribution p(E 0 |E, c, R). This distribution is a priori unknown to the learner. As we will see in Section 5, our approach avoids having to directly estimate this distribution. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. State To predict actions sequentially, we need to track the state of the document-to-actions mapping over tim"
P09-1010,P00-1013,0,0.0170029,"e environment state E specifies the set of objects available for interaction, and their properties. In Figure 2, E is shown on the right. The environment state E changes in response to the execution of command c with parameters R according to a transition distribution p(E 0 |E, c, R). This distribution is a priori unknown to the learner. As we will see in Section 5, our approach avoids having to directly estimate this distribution. Reinforcement Learning for Language Processing Reinforcement learning has been previously applied to the problem of dialogue management (Scheffler and Young, 2002; Roy et al., 2000; Litman et al., 2000; Singh et al., 1999). These systems converse with a human user by taking actions that emit natural language utterances. The reinforcement learning state space encodes information about the goals of the user and what they say at each time step. The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user. Reinforcement learning is applied very differently in dialogue systems compared to our setup. State To predict actions sequentially, we need to track the state of the document-to-act"
P09-1010,E12-1061,0,\N,Missing
P09-1024,W03-1004,1,0.775109,"erage is consistent with existing human-authored documents. We aim to derive these templates by discovering common patterns in the organization of documents in a domain of interest. There has been a sizable amount of research on structure induction ranging from linear segmentation (Hearst, 1994) to content modeling (Barzilay and Lee, 2004). At the core of these methods is the assumption that fragments of text conveying similar information have similar word distribution patterns. Therefore, often a simple segment clustering across domain texts can identify strong patterns in content structure (Barzilay and Elhadad, 2003). Clusters containing fragments from many documents are indicative of topics that are essential for a comprehensive summary. Given the simplicity and robustness of this approach, we utilize it for template induction. We cluster all section headings hi1 . . . him from all documents di using a repeated bisectioning algorithm (Zhao et al., 2005). As a similarity function, we use cosine similarity weighted with TF*IDF. We eliminate any clusters with low internal similarity (i.e., smaller than 0.5), as we assume these are “miscellaneous” clusters that will not yield unified topics. 1. Preprocessing"
P09-1024,W05-0618,0,0.0279462,"edundancy between topics, we formulate an optimization problem using excerpt rankings to create the final article. Given k topics, we would like to select one excerpt ejl for each topic tj , such that the rank is minimized; that is, scorej (ejl ) is high. To select the optimal excerpts, we employ integer linear programming (ILP). This framework is 211 Feature UNI wordi POS wordi BI wordi wordi+1 SENT EXCL QUES WORD NAME DATE PROP PRON NUM FIRST word1 FIRST word1 word2 SIMS commonly used in generation and summarization applications where the selection process is driven by multiple constraints (Marciniak and Strube, 2005; Clarke and Lapata, 2007). We represent excerpts included in the output using a set of indicator variables, xjl . For each excerpt ejl , the corresponding indicator variable xjl = 1 if the excerpt is included in the final document, and xjl = 0 otherwise. Our objective is to minimize the ranks of the excerpts selected for the final document: min k X r X l · xjl Table 1: Features employed in the ranking model. ∗ j=1 l=1 † ‡ We augment this formulation with two types of constraints. Exclusivity Constraints We want to ensure that exactly one indicator xjl is nonzero for each topic tj . These cons"
P09-1024,N04-1015,1,0.776626,"topical structure of documents in one domain. For instance, the template for articles about actors consists of four topics t1 . . . t4 : biography, early life, career, and personal life. Using this template to create the biography of a new actor will ensure that its information coverage is consistent with existing human-authored documents. We aim to derive these templates by discovering common patterns in the organization of documents in a domain of interest. There has been a sizable amount of research on structure induction ranging from linear segmentation (Hearst, 1994) to content modeling (Barzilay and Lee, 2004). At the core of these methods is the assumption that fragments of text conveying similar information have similar word distribution patterns. Therefore, often a simple segment clustering across domain texts can identify strong patterns in content structure (Barzilay and Elhadad, 2003). Clusters containing fragments from many documents are indicative of topics that are essential for a comprehensive summary. Given the simplicity and robustness of this approach, we utilize it for template induction. We cluster all section headings hi1 . . . him from all documents di using a repeated bisectioning"
P09-1024,P99-1071,1,0.433232,"ative they are of each individual topic. For each topic tj , we induce a ranking of the excerpts ej1 . . . ejr by mapping each excerpt ejl to a score: Selection Model Our selection model takes the content template t1 . . . tk and the candidate excerpts ej1 . . . ejr for each topic tj produced in the previous steps. It then selects a series of k excerpts, one from each topic, to create a coherent summary. One possible approach is to perform individual selections from each set of excerpts ej1 . . . ejr and then combine the results. This strategy is commonly used in multi-document summarization (Barzilay et al., 1999; Goldstein et al., 2000; Radev et al., 2000), where the combination step eliminates the redundancy across selected excerpts. However, separating the two steps may not be optimal for this task — the balance between coverage and redundancy is harder to achieve when a multi-paragraph summary is generated. In addition, a more discriminative selection strategy scorej (ejl ) = φ(ejl ) · wj Candidates for each topic are ranked from highest to lowest score. After this procedure, the position l of excerpt ejl within the topic-specific candidate vector is the excerpt’s rank. Optimizing the Global Objec"
P09-1024,D08-1080,0,0.0233116,"rovides a detailed template for what information should be included in the output and how this information should be organized (Reiter and Dale, 2000). In text-to-text generation, such templates for information organization are not available; sentences are selected based on their salience properties (Mani and Maybury, 1999). While this strategy is robust and portable across Our work also relates to a large body of recent work that uses Wikipedia material. Instances of this work include information extraction, ontology induction and resource acquisition (Wu and Weld, 2007; Biadsy et al., 2008; Nastase, 2008; Nastase and Strube, 2008). Our focus is on a different task — generation of new overview articles that follow the structure of Wikipedia articles. 209 3 Method quality of a given excerpt. Using the perceptron framework augmented with an ILP formulation for global optimization, the system is trained to select the best excerpt for each document di and each topic tj . For training, we assume the best excerpt is the original human-authored text sij . The goal of our system is to produce a comprehensive overview article given a title – e.g., Cancer. We assume that relevant information on the subj"
P09-1024,P08-1092,0,0.251215,"ains, output summaries often suffer from coherence and coverage problems. In between these two approaches is work on domain-specific text-to-text generation. Instances of these tasks are biography generation in summarization and answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). • Joint parameter estimation for content selection: Parameters are learned jointly for all topics in the template. This procedure optimizes both local relevance of information for each topic and global coherence across the"
P09-1024,W00-0403,0,0.0194534,"ch topic tj , we induce a ranking of the excerpts ej1 . . . ejr by mapping each excerpt ejl to a score: Selection Model Our selection model takes the content template t1 . . . tk and the candidate excerpts ej1 . . . ejr for each topic tj produced in the previous steps. It then selects a series of k excerpts, one from each topic, to create a coherent summary. One possible approach is to perform individual selections from each set of excerpts ej1 . . . ejr and then combine the results. This strategy is commonly used in multi-document summarization (Barzilay et al., 1999; Goldstein et al., 2000; Radev et al., 2000), where the combination step eliminates the redundancy across selected excerpts. However, separating the two steps may not be optimal for this task — the balance between coverage and redundancy is harder to achieve when a multi-paragraph summary is generated. In addition, a more discriminative selection strategy scorej (ejl ) = φ(ejl ) · wj Candidates for each topic are ranked from highest to lowest score. After this procedure, the position l of excerpt ejl within the topic-specific candidate vector is the excerpt’s rank. Optimizing the Global Objective To avoid redundancy between topics, we f"
P09-1024,D07-1001,0,0.0191016,"formulate an optimization problem using excerpt rankings to create the final article. Given k topics, we would like to select one excerpt ejl for each topic tj , such that the rank is minimized; that is, scorej (ejl ) is high. To select the optimal excerpts, we employ integer linear programming (ILP). This framework is 211 Feature UNI wordi POS wordi BI wordi wordi+1 SENT EXCL QUES WORD NAME DATE PROP PRON NUM FIRST word1 FIRST word1 word2 SIMS commonly used in generation and summarization applications where the selection process is driven by multiple constraints (Marciniak and Strube, 2005; Clarke and Lapata, 2007). We represent excerpts included in the output using a set of indicator variables, xjl . For each excerpt ejl , the corresponding indicator variable xjl = 1 if the excerpt is included in the final document, and xjl = 0 otherwise. Our objective is to minimize the ranks of the excerpts selected for the final document: min k X r X l · xjl Table 1: Features employed in the ranking model. ∗ j=1 l=1 † ‡ We augment this formulation with two types of constraints. Exclusivity Constraints We want to ensure that exactly one indicator xjl is nonzero for each topic tj . These constraints are formulated as"
P09-1024,N07-1038,1,0.55383,"where such mark-up is not available, one can employ topical segmentation algorithms as an additional preprocessing step. 210 is needed when candidate excerpts are drawn directly from the web, as they may be contaminated with noise. We propose a novel joint training algorithm that learns selection criteria for all the topics simultaneously. This approach enables us to maximize both local fit and global coherence. We implement this algorithm using the perceptron framework, as it can be easily modified for structured prediction while preserving convergence guarantees (Daum´e III and Marcu, 2005; Snyder and Barzilay, 2007). In this section, we first describe the structure and decoding procedure of our model. We then present an algorithm to jointly learn the parameters of all topic models. We determine the average number of sections k over all documents in our training set, then select the k largest section clusters as topics. We order these topics as t1 . . . tk using a majority ordering algorithm (Cohen et al., 1998). This algorithm finds a total order among clusters that is consistent with a maximal number of pairwise relationships observed in our data set. Each topic tj is identified by the most frequent hea"
P09-1024,H05-1013,0,0.0472063,"Missing"
P09-1024,J07-1005,0,0.0275946,"Missing"
P09-1024,H05-1015,0,0.0321005,"ation in summarization and answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). • Joint parameter estimation for content selection: Parameters are learned jointly for all topics in the template. This procedure optimizes both local relevance of information for each topic and global coherence across the entire article. We evaluate our approach by creating articles in two domains: Actors and Diseases. For a data set, we use Wikipedia, which contains articles similar to those we wish to produce in terms of le"
P09-1024,W04-3256,0,0.211329,"ternet sources. domains, output summaries often suffer from coherence and coverage problems. In between these two approaches is work on domain-specific text-to-text generation. Instances of these tasks are biography generation in summarization and answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). • Joint parameter estimation for content selection: Parameters are learned jointly for all topics in the template. This procedure optimizes both local relevance of information for each topic and global"
P09-1024,P06-2027,0,0.0472721,"answering definition requests in question-answering. In contrast to a generic summarizer, these applications aim to characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). • Joint parameter estimation for content selection: Parameters are learned jointly for all topics in the template. This procedure optimizes both local relevance of information for each topic and global coherence across the entire article. We evaluate our approach by creating articles in two domains: Actors and Diseases. For a data set, we use Wikipedia, which contains articles similar to those we wish to produce in terms of length and breadth. An adv"
P09-1024,C04-1093,0,0.195773,"characterize the types of information that are essential in a given domain. This characterization varies greatly in granularity. For instance, some approaches coarsely discriminate between biographical and non-biographical information (Zhou et al., 2004; Biadsy et al., 2008), while others go beyond binary distinction by identifying atomic events – e.g., occupation and marital status – that are typically included in a biography (Weischedel et al., 2004; Filatova and Prager, 2005; Filatova et al., 2006). Commonly, such templates are specified manually and are hard-coded for a particular domain (Fujii and Ishikawa, 2004; Weischedel et al., 2004). • Joint parameter estimation for content selection: Parameters are learned jointly for all topics in the template. This procedure optimizes both local relevance of information for each topic and global coherence across the entire article. We evaluate our approach by creating articles in two domains: Actors and Diseases. For a data set, we use Wikipedia, which contains articles similar to those we wish to produce in terms of length and breadth. An advantage of this data set is that Wikipedia articles explicitly delineate topical sections, facilitating structural anal"
P09-1024,W00-0405,0,0.0385656,"individual topic. For each topic tj , we induce a ranking of the excerpts ej1 . . . ejr by mapping each excerpt ejl to a score: Selection Model Our selection model takes the content template t1 . . . tk and the candidate excerpts ej1 . . . ejr for each topic tj produced in the previous steps. It then selects a series of k excerpts, one from each topic, to create a coherent summary. One possible approach is to perform individual selections from each set of excerpts ej1 . . . ejr and then combine the results. This strategy is commonly used in multi-document summarization (Barzilay et al., 1999; Goldstein et al., 2000; Radev et al., 2000), where the combination step eliminates the redundancy across selected excerpts. However, separating the two steps may not be optimal for this task — the balance between coverage and redundancy is harder to achieve when a multi-paragraph summary is generated. In addition, a more discriminative selection strategy scorej (ejl ) = φ(ejl ) · wj Candidates for each topic are ranked from highest to lowest score. After this procedure, the position l of excerpt ejl within the topic-specific candidate vector is the excerpt’s rank. Optimizing the Global Objective To avoid redundancy"
P09-1024,P94-1002,0,0.117755,"n A content template specifies the topical structure of documents in one domain. For instance, the template for articles about actors consists of four topics t1 . . . t4 : biography, early life, career, and personal life. Using this template to create the biography of a new actor will ensure that its information coverage is consistent with existing human-authored documents. We aim to derive these templates by discovering common patterns in the organization of documents in a domain of interest. There has been a sizable amount of research on structure induction ranging from linear segmentation (Hearst, 1994) to content modeling (Barzilay and Lee, 2004). At the core of these methods is the assumption that fragments of text conveying similar information have similar word distribution patterns. Therefore, often a simple segment clustering across domain texts can identify strong patterns in content structure (Barzilay and Elhadad, 2003). Clusters containing fragments from many documents are indicative of topics that are essential for a comprehensive summary. Given the simplicity and robustness of this approach, we utilize it for template induction. We cluster all section headings hi1 . . . him from a"
P09-1024,W04-1013,0,0.0399226,"yond those in the template; in these cases, the Oracle system produces a longer article than our algorithm. Table 2 shows the average number of excerpts selected and sources used in articles created by our full model and each baseline. Automatic Evaluation To assess the quality of the resulting overview articles, we compare them with the original human-authored articles. We use ROUGE, an evaluation metric employed at the Document Understanding Conferences (DUC), which assumes that proximity to human-authored text is an indicator of summary quality. We use the publicly available ROUGE toolkit (Lin, 2004) to compute recall, precision, and F-score for ROUGE-1. We use the Wilcoxon Signed Rank Test to determine statistical significance. Analysis of Human Edits In addition to our automatic evaluation, we perform a study of reactions to system-produced articles by the general public. To achieve this goal, we insert automatically created articles4 into Wikipedia itself and examine the feedback of Wikipedia editors. Selection of specific articles is constrained by the need to find topics which are currently of “stub” status that have enough information available on the Internet to construct a valid a"
P09-1024,P02-1062,0,\N,Missing
P10-1107,D07-1093,0,0.0965076,"Missing"
P10-1107,W97-0119,0,0.0342961,"on that is unknown in a typical deciphering scenario (while being readily available for living languages). For instance, some methods employ a hand-coded similarity function (Kondrak, 2001), while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constr"
P10-1107,P08-1088,0,0.136172,"being readily available for living languages). For instance, some methods employ a hand-coded similarity function (Kondrak, 2001), while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered in this paper. More imp"
P10-1107,P06-2065,1,0.810742,"pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered in this paper. More importantly, this method had not been applied to archaeological data. While lost languages are gaining increasing interest in the NLP community (Knight and Sproat, 2009), there have been no successful attempts of"
P10-1107,W02-0902,1,0.558414,"phering scenario (while being readily available for living languages). For instance, some methods employ a hand-coded similarity function (Kondrak, 2001), while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered"
P10-1107,N01-1014,0,0.147769,"Missing"
P10-1107,J94-3004,0,0.226293,"Missing"
P10-1107,P99-1067,0,0.0646936,"typical deciphering scenario (while being readily available for living languages). For instance, some methods employ a hand-coded similarity function (Kondrak, 2001), while others assume knowledge of the phonetic mapping or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints simila"
P10-1107,N09-4008,1,0.804974,"language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered in this paper. More importantly, this method had not been applied to archaeological data. While lost languages are gaining increasing interest in the NLP community (Knight and Sproat, 2009), there have been no successful attempts of their automatic decipherment. 3 Background on Ugaritic Manual Decipherment of Ugaritic Ugaritic tablets were first found in Syria in 1929 (Smith, 1955; Watson and Wyatt, 1999). At the time, the cuneiform writing on the tablets was of an unknown type. Charles Virolleaud, who lead the initial decipherment effort, recognized that the script was likely alphabetic, since the inscribed words consisted of only thirty distinct symbols. The location of the tablets discovery further suggested that Ugaritic was likely to have been a Semitic language from the We"
P10-1107,W99-0906,1,0.648885,"g or require parallel cognate pairs to learn a similarity function (Bouchard et al., 2007). A second related line of work is lexicon induction from non-parallel corpora. While this research has similar goals, it typically builds on information or resources unavailable for ancient texts, such as comparable corpora, a seed lexicon, and cognate information (Fung and McKeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). Moreover, distributional methods that rely on co-occurrence analysis operate over large corpora, which are typically unavailable for a lost language. Finally, Knight and Yamada (1999) and Knight et al. (2006) describe a computational HMMbased method for deciphering an unknown script that represents a known spoken language. This method “makes the text speak” by gleaning character-to-sound mappings from non-parallel character and sound sequences. It does not relate words in different languages, thus it cannot encode deciphering constraints similar to the ones considered in this paper. More importantly, this method had not been applied to archaeological data. While lost languages are gaining increasing interest in the NLP community (Knight and Sproat, 2009), there have been n"
P10-1107,H01-1035,0,\N,Missing
P10-1129,P09-1010,1,0.88521,"n word spans Wa , and translating each instruction into the sequence ~c of one or more commands it describes. During learning, the correct output command sequence is not provided to the algorithm. instructions enables us to bias exploration toward transitions relevant for language learning. This approach yields superior performance compared to a policy that relies on an environment model constructed via random exploration. 2 Related Work Interpreting Instructions Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al. (2009) (see Section 4 for more detail). Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. This assumption of a direct correspondence between the text and the environment is not unique to that paper, being inherent in other work on grounded language learning (Siskind, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than l"
P10-1129,C92-4181,0,0.700947,"Missing"
P10-1129,P92-1016,0,0.295405,"Missing"
P10-1129,D09-1100,0,0.0843068,"learning algorithm for mapping text instructions to commands developed by Branavan et al. (2009) (see Section 4 for more detail). Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. This assumption of a direct correspondence between the text and the environment is not unique to that paper, being inherent in other work on grounded language learning (Siskind, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. For example, their method can learn the rules of a card game given instructions for how to play. Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al., 1995; MacMahon et al., 2006). Not surprisingly, automatic approaches for processing such instructions have relied on hand-engineered world kno"
P10-1129,W05-0614,0,0.0542742,"via random exploration. 2 Related Work Interpreting Instructions Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al. (2009) (see Section 4 for more detail). Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. This assumption of a direct correspondence between the text and the environment is not unique to that paper, being inherent in other work on grounded language learning (Siskind, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. For example, their method can learn the rules of a card game given instructions for how to play. Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al., 1995; MacMahon et a"
P10-1129,E09-1058,0,0.0108413,"n of relevant environment knowledge. Reinforcement Learning Our work combines ideas of two traditionally disparate approaches to reinforcement learning (Sutton and Barto, 1998). The first approach, model-based learning, constructs a model of the environment in which the learner operates (e.g., modeling location, velocity, and acceleration in robot navigation). It then computes a policy directly from the rich information represented in the induced environment model. In the NLP literature, model-based reinforcement learning techniques are commonly used for dialog management (Singh et al., 2002; Lemon and Konstas, 2009; Schatzmann and Young, 2009). However, if the environment cannot be accurately approximated by a compact representation, these methods perform poorly (Boyan and Moore, 1995; Jong and Stone, 2007). Our instruction interpretation task falls into this latter category,2 rendering standard model-based learning ineffective. The second approach – model-free methods such as policy learning – aims to select the opti2 For example, in the Windows GUI domain, clicking on the File menu will result in a different submenu depending on the application. Thus it is impossible to predict the effects of a previo"
P10-1129,P09-1011,0,0.123719,"Interpreting Instructions Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al. (2009) (see Section 4 for more detail). Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. This assumption of a direct correspondence between the text and the environment is not unique to that paper, being inherent in other work on grounded language learning (Siskind, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. For example, their method can learn the rules of a card game given instructions for how to play. Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al., 1995; MacMahon et al., 2006). Not surprisingly, autom"
P10-1129,H89-1033,0,0.197931,"d, 2001; Oates, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al., 2009; Matuszek et al., 2010). A notable exception is the approach of Eisenstein et al. (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. For example, their method can learn the rules of a card game given instructions for how to play. Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al., 1995; MacMahon et al., 2006). Not surprisingly, automatic approaches for processing such instructions have relied on hand-engineered world knowledge to reason about the preconditions and effects of environment commands. The assumption of a fully specified environment model is also common in work on semantics in the linguistics literature (Lascarides and Asher, 2004). While our approach learns to analyze instructions in a goaldirected manner, it does not require manual specification of relevant environment knowledge. Reinforcement Learning Our work combines id"
P11-1028,P09-1010,1,0.832745,"Missing"
P11-1028,P10-1129,1,0.850517,"Missing"
P11-1028,de-marneffe-etal-2006-generating,0,0.00295183,"Missing"
P11-1028,D09-1100,0,0.152738,"ruction text where input documents specify a set of actions to be executed in the environment. In contrast, game manuals provide high-level advice but do not directly describe the correct actions for every potential game state. Moreover, these documents are long, and use rich vocabularies with complex grammatical constructions. We do not aim to perform a comprehensive interpretation of such documents. Rather, our focus is on language analysis that is sufficiently detailed to help the underlying control task. The area of language analysis situated in a game domain has been studied in the past (Eisenstein et al., 2009). Their method, however, is different both in terms of the target interpretation task, and the supervision signal it learns from. They aim to learn the rules of a given game, such as which moves are valid, given documents describing the rules. Our goal is more open ended, in that we aim to learn winning game strategies. Furthermore, Eisenstein et al. (2009) rely on a different source of supervision – game traces collected a priori. For complex games, like the one considered in this paper, collecting such game traces is prohibitively expensive. Therefore our approach learns by actively playing"
P11-1028,W05-0614,0,0.166114,"Missing"
P11-1028,J93-2004,0,0.0427494,"Missing"
P11-1028,P10-1083,0,0.519352,"Missing"
P11-1028,W10-2903,0,\N,Missing
P11-1028,E12-1061,0,\N,Missing
P11-1028,P09-1011,0,\N,Missing
P11-1028,P09-1110,0,\N,Missing
P11-1028,P11-1060,0,\N,Missing
P11-1028,P11-1149,0,\N,Missing
P11-1036,E06-1039,0,0.00904292,"mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization methods (Seki et al., 2006), while others propose new methods targeted for opinion text (Liu et al., 2005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches analyze review documents using probabilistic topic models (Lu and Zhai, 2008; Titov and McDonald, 2008; M"
P11-1036,P06-2063,0,0.0164182,"greement amongst user snippets. This tests our model’s capacity to jointly identify properties and assess attributes. 2 Related Work Our work on review aggregation has connections to three lines of work in text analysis. First, our work relates to research on extraction of product properties with associated sentiment from review text (Hu and Liu, 2004; Liu et al., 2005a; Popescu et al., 2005). These methods identify relevant information in a document using a wide range of methods such as association mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization methods (Seki et al., 2006), while others propose new methods targeted for opinion text (Liu et al., 2005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive"
P11-1036,H05-2017,0,0.0787257,"that learning to identify attributes in the context of other product reviews yields significant gains. Finally, we evaluate our model on its ability to identify product properties for which there is significant sentiment disagreement amongst user snippets. This tests our model’s capacity to jointly identify properties and assess attributes. 2 Related Work Our work on review aggregation has connections to three lines of work in text analysis. First, our work relates to research on extraction of product properties with associated sentiment from review text (Hu and Liu, 2004; Liu et al., 2005a; Popescu et al., 2005). These methods identify relevant information in a document using a wide range of methods such as association mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization"
P11-1036,D10-1037,1,0.870619,"work is to provide a mechanism for review content aggregation that goes beyond numerical scores. Specifically, we are interested in identifying fine-grained product properties across reviews (e.g., battery life for electronics or pizza for restaurants) as well as capturing attributes of these properties, namely aggregate user sentiment. For this task, we assume as input a set of product review snippets (i.e., standalone phrases such as “battery life is the best I’ve found”) rather than complete reviews. There are many techniques for extracting this type of snippet in existing work; we use the Sauper et al. (2010) system. 350 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 350–358, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics At first glance, this task can be solved using existing methods for review analysis. These methods can effectively extract product properties from individual snippets along with their corresponding sentiment. While the resulting property-attribute pairs form a useful abstraction for cross-review analysis, in practice direct comparison of these pairs is challenging. Consider, for instance, the tw"
P11-1036,P08-1036,0,0.102107,"005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches analyze review documents using probabilistic topic models (Lu and Zhai, 2008; Titov and McDonald, 2008; Mei et al., 2007). While some of these methods focus primarily on modeling ratable aspects (Titov and McDonald, 2008), others explicitly capture the mixture of topics and sentiments (Mei et al., 2007). These approaches are capable of identifying latent topics in the collection in opinion text (e.g., weblogs) as well as associated sentiment. While our model captures similar high-level intuition, it analyzes fine-grained properties expressed at the snippet level, rather than document-level sentiment. Delivering analysis at such a fine granularity requires a new technique. 3 One of the goals of"
P11-1036,M95-1005,0,0.00872765,"h were provided to two workers, for a total output of 210 generated clusters. Baseline The baseline for this task is a clustering algorithm weighted by TF*IDF over the data set as implemented by the publicly available CLUTO package.3 This baseline will put a strong connection between things which are lexically similar. Because our model only uses property words to tie together clusters, it may miss correlations between words which are not correctly identified as property words. The baseline is allowed 10 property clusters per restaurant. We use the MUC cluster evaluation metric for this task (Vilain et al., 1995). This metric measures the number of cluster merges and splits required to recreate the gold clusters given the model’s output. 3 Available at http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview with agglomerative clustering, using the cosine similarity distance metric. Baseline Our model Precision 80.2 72.2 Recall 61.1 79.1 F1 69.3 75.5 Table 2: Results using the MUC metric on the cluster prediction task. Note that while the precision of the baseline is higher, the recall and overall F1 of our model outweighs that. While MUC has a deficiency in that putting everything into a single cluster"
P11-1036,H05-1043,0,\N,Missing
P11-1040,P07-1073,0,0.0130902,"e. This idea of consensus-based extraction is also central to our method. However, we incorporate this idea into our model by simultaneously clustering output and labeling documents rather than performing the two tasks in serial fashion. Another important difference is inherent in the input data we are processing: it is not clear a priori which extraction decisions should agree with each other. Identifying messages that re390 fer to the same event is a large part of our challenge. Our work also relates to recent approaches for relation extraction with distant supervision (Mintz et al., 2009b; Bunescu and Mooney, 2007; Yao et al., 2010a). These approaches assume a database and a collection of documents that verbalize some of the database relations. In contrast to traditional supervised IE approaches, these methods do not assume that relation instantiations are annotated in the input documents. For instance, the method of Mintz et al. (2009b) induces the mapping automatically by bootstrapping from sentences that directly match record entries. These mappings are used to learn a classifier for relation extraction. Yao et al. (2010a) further refine this approach by constraining predicted relations to be consis"
P11-1040,D10-1124,0,0.020902,"Missing"
P11-1040,P04-1053,0,0.0267629,"Missing"
P11-1040,P05-1060,0,0.12554,"n evaluated manually, significantly outperforming several baselines. 2 Related Work A large number of information extraction approaches exploit redundancy in text collections to improve their accuracy and reduce the need for manually annotated data (Agichtein and Gravano, 2000; Yangarber et al., 2000; Zhu et al., 2009; Mintz et al., 2009a; Yao et al., 2010b; Hasegawa et al., 2004; Shinyama and Sekine, 2006). Our work most closely relates to methods for multi-document information extraction which utilize redundancy in input data to increase the accuracy of the extraction process. For instance, Mann and Yarowsky (2005) explore methods for fusing extracted information across multiple documents by performing extraction on each document independently and then merging extracted relations by majority vote. This idea of consensus-based extraction is also central to our method. However, we incorporate this idea into our model by simultaneously clustering output and labeling documents rather than performing the two tasks in serial fashion. Another important difference is inherent in the input data we are processing: it is not clear a priori which extraction decisions should agree with each other. Identifying messag"
P11-1040,P09-1113,0,0.457653,"priori which extraction decisions should agree with each other. Identifying messages that re390 fer to the same event is a large part of our challenge. Our work also relates to recent approaches for relation extraction with distant supervision (Mintz et al., 2009b; Bunescu and Mooney, 2007; Yao et al., 2010a). These approaches assume a database and a collection of documents that verbalize some of the database relations. In contrast to traditional supervised IE approaches, these methods do not assume that relation instantiations are annotated in the input documents. For instance, the method of Mintz et al. (2009b) induces the mapping automatically by bootstrapping from sentences that directly match record entries. These mappings are used to learn a classifier for relation extraction. Yao et al. (2010a) further refine this approach by constraining predicted relations to be consistent with entity types assignment. To capture the complex dependencies among assignments, Yao et al. (2010a) use a factor graph representation. Despite the apparent similarity in model structure, the two approaches deal with various types of uncertainties. The key challenge for our method is modeling message to record alignmen"
P11-1040,N06-1039,0,0.0682047,"Missing"
P11-1040,C00-2136,0,0.216336,"Missing"
P11-1040,D10-1099,0,0.0357466,"for relation extraction with distant supervision (Mintz et al., 2009b; Bunescu and Mooney, 2007; Yao et al., 2010a). These approaches assume a database and a collection of documents that verbalize some of the database relations. In contrast to traditional supervised IE approaches, these methods do not assume that relation instantiations are annotated in the input documents. For instance, the method of Mintz et al. (2009b) induces the mapping automatically by bootstrapping from sentences that directly match record entries. These mappings are used to learn a classifier for relation extraction. Yao et al. (2010a) further refine this approach by constraining predicted relations to be consistent with entity types assignment. To capture the complex dependencies among assignments, Yao et al. (2010a) use a factor graph representation. Despite the apparent similarity in model structure, the two approaches deal with various types of uncertainties. The key challenge for our method is modeling message to record alignment which is not an issue in the previous set up. Finally, our work fits into a broader area of text processing methods designed for social-media streams. Examples of such approaches include met"
P11-1040,N10-1020,0,\N,Missing
P11-1054,P08-1004,0,0.246349,"Missing"
P11-1054,N04-1015,1,0.713077,"lation instance. Specifically, we require that every token of the corpus occurs at most once as a word in a relation’s argument in expectation. On the other hand, a single word can sometimes be evocative of multiple relations (e.g., “occurred” signals both date and time in “occurred on Friday at 3pm”). Thus, we allow each word to serve as an indicator more than once, arbitrarily fixing the limit at two. 6 Experimental Setup Datasets and Metrics We evaluate on two datasets, financial market reports and newswire articles about earthquakes, previously used in work on high-level content analysis (Barzilay and Lee, 2004; Lapata, 2006). Finance articles chronicle daily market movements of currencies and stock indexes, and earthquake articles document specific earthquakes. Constituent parses are obtained automatically using the Stanford parser (Klein and Manning, 2003) and then converted to dependency parses using the PennConvertor tool (Johansson and Nugues, 2007). We manually annotated relations for both corpora, selecting relation types that occurred frequently in each domain. We found 15 types for finance and 9 for earthquake. Corpus statistics are summarized below, and example relation types are shown in"
P11-1054,D09-1014,0,0.054875,"ramming to impose global declarative constraints on the output from a set of classifiers trained on local features. Chang et al. (2007) propose an objective function for semi-supervised extraction that balances likelihood of labeled instances and constraint violation on unlabeled instances. Recent work has also explored how certain kinds of supervision can be formulated as constraints on model posteriors. Such constraints are not declarative, but instead based on annotations of words’ majority relation labels (Mann and McCallum, 2008) and pre-existing databases with the desired output schema (Bellare and McCallum, 2009). In contrast to previous work, our approach explores a different class of constraints that does not rely on supervision that is specific to particular relation types and their instances. 3 Model Our work performs in-domain relation discovery by leveraging regularities in relation expression at the lexical, syntactic, and discourse levels. These regularities are captured via two components: a probabilistic model that explains how documents are generated from latent relation variables and a technique ments but irrelevant for indicators).4 3.2 is_verb earthquake hit 0 1 0 1 0 1 0 0 0 has_proper"
P11-1054,P07-1073,0,0.0215287,"successfully identify domain-relevant relations. We also study the importance and effectiveness of the declaratively-specified constraints. In particular, we find that a small set of declarative constraints are effective across domains, while additional domainspecific constraints yield further benefits. 2 Related Work Extraction with Reduced Supervision Recent research in information extraction has taken large steps toward reducing the need for labeled data. Examples include using bootstrapping to amplify small seed sets of example outputs (Agichtein and Gravano, 2000; Yangarber et al., 2000; Bunescu and Mooney, 2007; Zhu et al., 2009), leveraging existing databases that overlap with the text (Mintz et al., 2009; Yao et al., 2010), and learning general domain-independent knowledge bases by exploiting redundancies in large web and news corpora (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Banko et al., 2007; Yates and Etzioni, 2009). Our approach is distinct in both the supervision and data we operate over. First, in contrast to bootstrapping and database matching approaches, we learn from meta-qualities, such as low variability in syntactic patterns, that characterize a good relation. 2 We do not use"
P11-1054,P07-1036,0,0.0522458,"et al., 2005) and contexts (Chen et al., 2005; Rosenfeld and Feldman, 2007). Our approach incorporates a broader range of constraints and balances constraints with underlying patterns learned from the data, thereby requiring more sophisticated machinery for modeling and inference. Extraction with Constraints Previous work has recognized the appeal of applying declarative constraints to extraction. In a supervised setting, Roth and Yih (2004) induce relations by using linear programming to impose global declarative constraints on the output from a set of classifiers trained on local features. Chang et al. (2007) propose an objective function for semi-supervised extraction that balances likelihood of labeled instances and constraint violation on unlabeled instances. Recent work has also explored how certain kinds of supervision can be formulated as constraints on model posteriors. Such constraints are not declarative, but instead based on annotations of words’ majority relation labels (Mann and McCallum, 2008) and pre-existing databases with the desired output schema (Bellare and McCallum, 2009). In contrast to previous work, our approach explores a different class of constraints that does not rely on"
P11-1054,I05-1035,0,0.0160396,"work that builds general relation databases from heterogeneous corpora, our focus is on learning the relations salient in a single domain. Our setup is more germane to specialized domains expressing information not broadly available on the web. Earlier work in unsupervised information extraction has also leveraged meta-knowledge independent of specific relation types, such as declarativelyspecified syntactic patterns (Riloff, 1996), frequent dependency subtree patterns (Sudo et al., 2003), and automatic clusterings of syntactic patterns (Lin and Pantel, 2001; Zhang et al., 2005) and contexts (Chen et al., 2005; Rosenfeld and Feldman, 2007). Our approach incorporates a broader range of constraints and balances constraints with underlying patterns learned from the data, thereby requiring more sophisticated machinery for modeling and inference. Extraction with Constraints Previous work has recognized the appeal of applying declarative constraints to extraction. In a supervised setting, Roth and Yih (2004) induce relations by using linear programming to impose global declarative constraints on the output from a set of classifiers trained on local features. Chang et al. (2007) propose an objective funct"
P11-1054,W08-1301,0,0.100777,"Missing"
P11-1054,P04-1053,0,0.155566,"mainspecific constraints yield further benefits. 2 Related Work Extraction with Reduced Supervision Recent research in information extraction has taken large steps toward reducing the need for labeled data. Examples include using bootstrapping to amplify small seed sets of example outputs (Agichtein and Gravano, 2000; Yangarber et al., 2000; Bunescu and Mooney, 2007; Zhu et al., 2009), leveraging existing databases that overlap with the text (Mintz et al., 2009; Yao et al., 2010), and learning general domain-independent knowledge bases by exploiting redundancies in large web and news corpora (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Banko et al., 2007; Yates and Etzioni, 2009). Our approach is distinct in both the supervision and data we operate over. First, in contrast to bootstrapping and database matching approaches, we learn from meta-qualities, such as low variability in syntactic patterns, that characterize a good relation. 2 We do not use the word “argument” in the syntactic sense— a relation’s argument may or may not be the syntactic dependency argument of its indicator. 531 We hypothesize that these properties hold across relations in different domains. Second, in contrast to work tha"
P11-1054,W07-2416,0,0.011315,"ndicator more than once, arbitrarily fixing the limit at two. 6 Experimental Setup Datasets and Metrics We evaluate on two datasets, financial market reports and newswire articles about earthquakes, previously used in work on high-level content analysis (Barzilay and Lee, 2004; Lapata, 2006). Finance articles chronicle daily market movements of currencies and stock indexes, and earthquake articles document specific earthquakes. Constituent parses are obtained automatically using the Stanford parser (Klein and Manning, 2003) and then converted to dependency parses using the PennConvertor tool (Johansson and Nugues, 2007). We manually annotated relations for both corpora, selecting relation types that occurred frequently in each domain. We found 15 types for finance and 9 for earthquake. Corpus statistics are summarized below, and example relation types are shown in Table 2. Finance Earthquake Docs 100 200 Sent/Doc 12.1 9.3 Tok/Doc 262.9 210.3 Vocab 2918 3155 In our task, annotation conventions for desired output relations can greatly impact token-level performance, and the model cannot learn to fit a particular convention by looking at example data. For example, earthquakes times are frequently reported in bo"
P11-1054,D07-1031,0,0.0550634,"Missing"
P11-1054,P03-1054,0,0.00676329,"s both date and time in “occurred on Friday at 3pm”). Thus, we allow each word to serve as an indicator more than once, arbitrarily fixing the limit at two. 6 Experimental Setup Datasets and Metrics We evaluate on two datasets, financial market reports and newswire articles about earthquakes, previously used in work on high-level content analysis (Barzilay and Lee, 2004; Lapata, 2006). Finance articles chronicle daily market movements of currencies and stock indexes, and earthquake articles document specific earthquakes. Constituent parses are obtained automatically using the Stanford parser (Klein and Manning, 2003) and then converted to dependency parses using the PennConvertor tool (Johansson and Nugues, 2007). We manually annotated relations for both corpora, selecting relation types that occurred frequently in each domain. We found 15 types for finance and 9 for earthquake. Corpus statistics are summarized below, and example relation types are shown in Table 2. Finance Earthquake Docs 100 200 Sent/Doc 12.1 9.3 Tok/Doc 262.9 210.3 Vocab 2918 3155 In our task, annotation conventions for desired output relations can greatly impact token-level performance, and the model cannot learn to fit a particular c"
P11-1054,J06-4002,0,0.0293163,"cally, we require that every token of the corpus occurs at most once as a word in a relation’s argument in expectation. On the other hand, a single word can sometimes be evocative of multiple relations (e.g., “occurred” signals both date and time in “occurred on Friday at 3pm”). Thus, we allow each word to serve as an indicator more than once, arbitrarily fixing the limit at two. 6 Experimental Setup Datasets and Metrics We evaluate on two datasets, financial market reports and newswire articles about earthquakes, previously used in work on high-level content analysis (Barzilay and Lee, 2004; Lapata, 2006). Finance articles chronicle daily market movements of currencies and stock indexes, and earthquake articles document specific earthquakes. Constituent parses are obtained automatically using the Stanford parser (Klein and Manning, 2003) and then converted to dependency parses using the PennConvertor tool (Johansson and Nugues, 2007). We manually annotated relations for both corpora, selecting relation types that occurred frequently in each domain. We found 15 types for finance and 9 for earthquake. Corpus statistics are summarized below, and example relation types are shown in Table 2. Financ"
P11-1054,P08-1099,0,0.0219499,". In a supervised setting, Roth and Yih (2004) induce relations by using linear programming to impose global declarative constraints on the output from a set of classifiers trained on local features. Chang et al. (2007) propose an objective function for semi-supervised extraction that balances likelihood of labeled instances and constraint violation on unlabeled instances. Recent work has also explored how certain kinds of supervision can be formulated as constraints on model posteriors. Such constraints are not declarative, but instead based on annotations of words’ majority relation labels (Mann and McCallum, 2008) and pre-existing databases with the desired output schema (Bellare and McCallum, 2009). In contrast to previous work, our approach explores a different class of constraints that does not rely on supervision that is specific to particular relation types and their instances. 3 Model Our work performs in-domain relation discovery by leveraging regularities in relation expression at the lexical, syntactic, and discourse levels. These regularities are captured via two components: a probabilistic model that explains how documents are generated from latent relation variables and a technique ments bu"
P11-1054,P09-1113,0,0.0780842,"declaratively-specified constraints. In particular, we find that a small set of declarative constraints are effective across domains, while additional domainspecific constraints yield further benefits. 2 Related Work Extraction with Reduced Supervision Recent research in information extraction has taken large steps toward reducing the need for labeled data. Examples include using bootstrapping to amplify small seed sets of example outputs (Agichtein and Gravano, 2000; Yangarber et al., 2000; Bunescu and Mooney, 2007; Zhu et al., 2009), leveraging existing databases that overlap with the text (Mintz et al., 2009; Yao et al., 2010), and learning general domain-independent knowledge bases by exploiting redundancies in large web and news corpora (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Banko et al., 2007; Yates and Etzioni, 2009). Our approach is distinct in both the supervision and data we operate over. First, in contrast to bootstrapping and database matching approaches, we learn from meta-qualities, such as low variability in syntactic patterns, that characterize a good relation. 2 We do not use the word “argument” in the syntactic sense— a relation’s argument may or may not be the syntacti"
P11-1054,D09-1001,0,0.0483698,"l, we set the number of clusters K to the true number of relation types. Mallows Topic Model (MTM): Another technique for grouping similar sentences is the Mallows-based topic model of Chen et al. (2009). The datasets we consider here exhibit high-level regularities in content organization, so we expect that a topic model with global constraints could identify plausible clusters of relation-bearing sentences. Again, K is set to the true number of relation types. Unsupervised Semantic Parsing (USP): Our final unsupervised comparison is to USP, an unsupervised deep semantic parser introduced by Poon and Domingos (2009). USP induces a lambda calculus representation of an entire corpus and was shown to be competitive with open information extraction approaches (Lin and Pantel, 2001; Banko et al., 2007). We give USP the required Stanford dependency format as input (de Marneffe and Manning, 2008). We find that the results are sensitive to the cluster granularity prior, so we tune this parameter and report the best-performing runs. We recognize that USP targets a different output representation than ours: a hierarchical semantic structure over the entirety of a dependency-parsed text. In contrast, we focus on di"
P11-1054,W04-2401,0,0.0413542,"ied syntactic patterns (Riloff, 1996), frequent dependency subtree patterns (Sudo et al., 2003), and automatic clusterings of syntactic patterns (Lin and Pantel, 2001; Zhang et al., 2005) and contexts (Chen et al., 2005; Rosenfeld and Feldman, 2007). Our approach incorporates a broader range of constraints and balances constraints with underlying patterns learned from the data, thereby requiring more sophisticated machinery for modeling and inference. Extraction with Constraints Previous work has recognized the appeal of applying declarative constraints to extraction. In a supervised setting, Roth and Yih (2004) induce relations by using linear programming to impose global declarative constraints on the output from a set of classifiers trained on local features. Chang et al. (2007) propose an objective function for semi-supervised extraction that balances likelihood of labeled instances and constraint violation on unlabeled instances. Recent work has also explored how certain kinds of supervision can be formulated as constraints on model posteriors. Such constraints are not declarative, but instead based on annotations of words’ majority relation labels (Mann and McCallum, 2008) and pre-existing data"
P11-1054,N06-1039,0,0.0745348,"s yield further benefits. 2 Related Work Extraction with Reduced Supervision Recent research in information extraction has taken large steps toward reducing the need for labeled data. Examples include using bootstrapping to amplify small seed sets of example outputs (Agichtein and Gravano, 2000; Yangarber et al., 2000; Bunescu and Mooney, 2007; Zhu et al., 2009), leveraging existing databases that overlap with the text (Mintz et al., 2009; Yao et al., 2010), and learning general domain-independent knowledge bases by exploiting redundancies in large web and news corpora (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Banko et al., 2007; Yates and Etzioni, 2009). Our approach is distinct in both the supervision and data we operate over. First, in contrast to bootstrapping and database matching approaches, we learn from meta-qualities, such as low variability in syntactic patterns, that characterize a good relation. 2 We do not use the word “argument” in the syntactic sense— a relation’s argument may or may not be the syntactic dependency argument of its indicator. 531 We hypothesize that these properties hold across relations in different domains. Second, in contrast to work that builds general relation d"
P11-1054,P03-1029,0,0.130873,"f its indicator. 531 We hypothesize that these properties hold across relations in different domains. Second, in contrast to work that builds general relation databases from heterogeneous corpora, our focus is on learning the relations salient in a single domain. Our setup is more germane to specialized domains expressing information not broadly available on the web. Earlier work in unsupervised information extraction has also leveraged meta-knowledge independent of specific relation types, such as declarativelyspecified syntactic patterns (Riloff, 1996), frequent dependency subtree patterns (Sudo et al., 2003), and automatic clusterings of syntactic patterns (Lin and Pantel, 2001; Zhang et al., 2005) and contexts (Chen et al., 2005; Rosenfeld and Feldman, 2007). Our approach incorporates a broader range of constraints and balances constraints with underlying patterns learned from the data, thereby requiring more sophisticated machinery for modeling and inference. Extraction with Constraints Previous work has recognized the appeal of applying declarative constraints to extraction. In a supervised setting, Roth and Yih (2004) induce relations by using linear programming to impose global declarative c"
P11-1054,C00-2136,0,0.087661,"demonstrate that we can successfully identify domain-relevant relations. We also study the importance and effectiveness of the declaratively-specified constraints. In particular, we find that a small set of declarative constraints are effective across domains, while additional domainspecific constraints yield further benefits. 2 Related Work Extraction with Reduced Supervision Recent research in information extraction has taken large steps toward reducing the need for labeled data. Examples include using bootstrapping to amplify small seed sets of example outputs (Agichtein and Gravano, 2000; Yangarber et al., 2000; Bunescu and Mooney, 2007; Zhu et al., 2009), leveraging existing databases that overlap with the text (Mintz et al., 2009; Yao et al., 2010), and learning general domain-independent knowledge bases by exploiting redundancies in large web and news corpora (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Banko et al., 2007; Yates and Etzioni, 2009). Our approach is distinct in both the supervision and data we operate over. First, in contrast to bootstrapping and database matching approaches, we learn from meta-qualities, such as low variability in syntactic patterns, that characterize a good"
P11-1054,D10-1099,0,0.0403325,"ied constraints. In particular, we find that a small set of declarative constraints are effective across domains, while additional domainspecific constraints yield further benefits. 2 Related Work Extraction with Reduced Supervision Recent research in information extraction has taken large steps toward reducing the need for labeled data. Examples include using bootstrapping to amplify small seed sets of example outputs (Agichtein and Gravano, 2000; Yangarber et al., 2000; Bunescu and Mooney, 2007; Zhu et al., 2009), leveraging existing databases that overlap with the text (Mintz et al., 2009; Yao et al., 2010), and learning general domain-independent knowledge bases by exploiting redundancies in large web and news corpora (Hasegawa et al., 2004; Shinyama and Sekine, 2006; Banko et al., 2007; Yates and Etzioni, 2009). Our approach is distinct in both the supervision and data we operate over. First, in contrast to bootstrapping and database matching approaches, we learn from meta-qualities, such as low variability in syntactic patterns, that characterize a good relation. 2 We do not use the word “argument” in the syntactic sense— a relation’s argument may or may not be the syntactic dependency argume"
P11-1054,I05-1034,0,0.0258504,"t domains. Second, in contrast to work that builds general relation databases from heterogeneous corpora, our focus is on learning the relations salient in a single domain. Our setup is more germane to specialized domains expressing information not broadly available on the web. Earlier work in unsupervised information extraction has also leveraged meta-knowledge independent of specific relation types, such as declarativelyspecified syntactic patterns (Riloff, 1996), frequent dependency subtree patterns (Sudo et al., 2003), and automatic clusterings of syntactic patterns (Lin and Pantel, 2001; Zhang et al., 2005) and contexts (Chen et al., 2005; Rosenfeld and Feldman, 2007). Our approach incorporates a broader range of constraints and balances constraints with underlying patterns learned from the data, thereby requiring more sophisticated machinery for modeling and inference. Extraction with Constraints Previous work has recognized the appeal of applying declarative constraints to extraction. In a supervised setting, Roth and Yih (2004) induce relations by using linear programming to impose global declarative constraints on the output from a set of classifiers trained on local features. Chang et al. ("
P12-1014,blanco-etal-2008-causal,0,0.0326497,"hes that of an oracle planner which uses manually-annotated preconditions. 2 Related Work Extracting Event Semantics from Text The task of extracting preconditions and effects has previously been addressed in the context of lexical semantics (Sil et al., 2010; Sil and Yates, 2011). These approaches combine large-scale distributional techniques with supervised learning to identify desired semantic relations in text. Such combined approaches have also been shown to be effective for identifying other relationships between events, such as causality (Girju and Moldovan, 2002; Chang and Choi, 2006; Blanco et al., 2008; Beamer and Girju, 2009; Do et al., 2011). Similar to these methods, our algorithm capitalizes on surface linguistic cues to learn preconditions from text. However, our only source of supervision is the feedback provided by the planning task which utilizes the predictions. Additionally, we not only identify these relations in text, but also show they are valuable in performing an external task. Learning Semantics via Language Grounding Our work fits into the broad area of grounded language acquisition, where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskin"
P12-1014,P09-1010,1,0.711922,"italizes on surface linguistic cues to learn preconditions from text. However, our only source of supervision is the feedback provided by the planning task which utilizes the predictions. Additionally, we not only identify these relations in text, but also show they are valuable in performing an external task. Learning Semantics via Language Grounding Our work fits into the broad area of grounded language acquisition, where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Liang et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to the reinforcement learning approaches that learn language by interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010; Branavan et al., 2011). 4 The state-of-the-art baseline used in the 2008 International Planning Competition. http://ipc.informatik.uni-freiburg.de/ Text (input): 3 A pickaxe, which is used to harvest stone, can be made from wood. Precondition Relations: wood pickaxe pickaxe stone Plan Subgoal Sequence: stone (goal) pi"
P12-1014,P10-1129,1,0.81139,"lso show they are valuable in performing an external task. Learning Semantics via Language Grounding Our work fits into the broad area of grounded language acquisition, where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Liang et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to the reinforcement learning approaches that learn language by interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010; Branavan et al., 2011). 4 The state-of-the-art baseline used in the 2008 International Planning Competition. http://ipc.informatik.uni-freiburg.de/ Text (input): 3 A pickaxe, which is used to harvest stone, can be made from wood. Precondition Relations: wood pickaxe pickaxe stone Plan Subgoal Sequence: stone (goal) pickaxe (subgoal 2) wood (subgoal 1) initial state Figure 2: A high-level plan showing two subgoals in a precondition relation. The corresponding sentence is shown above. The key distinction of our work is the use of grounding to learn abstract pragmatic"
P12-1014,P11-1028,1,0.735845,"nal task. Learning Semantics via Language Grounding Our work fits into the broad area of grounded language acquisition, where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Liang et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to the reinforcement learning approaches that learn language by interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010; Branavan et al., 2011). 4 The state-of-the-art baseline used in the 2008 International Planning Competition. http://ipc.informatik.uni-freiburg.de/ Text (input): 3 A pickaxe, which is used to harvest stone, can be made from wood. Precondition Relations: wood pickaxe pickaxe stone Plan Subgoal Sequence: stone (goal) pickaxe (subgoal 2) wood (subgoal 1) initial state Figure 2: A high-level plan showing two subgoals in a precondition relation. The corresponding sentence is shown above. The key distinction of our work is the use of grounding to learn abstract pragmatic relations, i.e. to learn linguistic patterns that"
P12-1014,de-marneffe-etal-2006-generating,0,0.0404353,"Missing"
P12-1014,D11-1027,0,0.0666043,"ally-annotated preconditions. 2 Related Work Extracting Event Semantics from Text The task of extracting preconditions and effects has previously been addressed in the context of lexical semantics (Sil et al., 2010; Sil and Yates, 2011). These approaches combine large-scale distributional techniques with supervised learning to identify desired semantic relations in text. Such combined approaches have also been shown to be effective for identifying other relationships between events, such as causality (Girju and Moldovan, 2002; Chang and Choi, 2006; Blanco et al., 2008; Beamer and Girju, 2009; Do et al., 2011). Similar to these methods, our algorithm capitalizes on surface linguistic cues to learn preconditions from text. However, our only source of supervision is the feedback provided by the planning task which utilizes the predictions. Additionally, we not only identify these relations in text, but also show they are valuable in performing an external task. Learning Semantics via Language Grounding Our work fits into the broad area of grounded language acquisition, where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman"
P12-1014,W05-0614,0,0.0803925,"al., 2011). Similar to these methods, our algorithm capitalizes on surface linguistic cues to learn preconditions from text. However, our only source of supervision is the feedback provided by the planning task which utilizes the predictions. Additionally, we not only identify these relations in text, but also show they are valuable in performing an external task. Learning Semantics via Language Grounding Our work fits into the broad area of grounded language acquisition, where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Liang et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to the reinforcement learning approaches that learn language by interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010; Branavan et al., 2011). 4 The state-of-the-art baseline used in the 2008 International Planning Competition. http://ipc.informatik.uni-freiburg.de/ Text (input): 3 A pickaxe, which is used to harvest stone, can be made from wood. Precondition Relations: wood pickaxe"
P12-1014,P09-1011,0,0.0265765,"guistic cues to learn preconditions from text. However, our only source of supervision is the feedback provided by the planning task which utilizes the predictions. Additionally, we not only identify these relations in text, but also show they are valuable in performing an external task. Learning Semantics via Language Grounding Our work fits into the broad area of grounded language acquisition, where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Liang et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to the reinforcement learning approaches that learn language by interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010; Branavan et al., 2011). 4 The state-of-the-art baseline used in the 2008 International Planning Competition. http://ipc.informatik.uni-freiburg.de/ Text (input): 3 A pickaxe, which is used to harvest stone, can be made from wood. Precondition Relations: wood pickaxe pickaxe stone Plan Subgoal Sequence: stone (goal) pickaxe (subgoal 2) wo"
P12-1014,R11-1001,0,0.0974412,"Missing"
P12-1014,P10-1083,0,0.0501888,"n preconditions from text. However, our only source of supervision is the feedback provided by the planning task which utilizes the predictions. Additionally, we not only identify these relations in text, but also show they are valuable in performing an external task. Learning Semantics via Language Grounding Our work fits into the broad area of grounded language acquisition, where the goal is to learn linguistic analysis from a situated context (Oates, 2001; Siskind, 2001; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008a; Mooney, 2008b; Branavan et al., 2009; Liang et al., 2009; Vogel and Jurafsky, 2010). Within this line of work, we are most closely related to the reinforcement learning approaches that learn language by interacting with an external environment (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010; Branavan et al., 2011). 4 The state-of-the-art baseline used in the 2008 International Planning Competition. http://ipc.informatik.uni-freiburg.de/ Text (input): 3 A pickaxe, which is used to harvest stone, can be made from wood. Precondition Relations: wood pickaxe pickaxe stone Plan Subgoal Sequence: stone (goal) pickaxe (subgoal 2) wood (subgoal 1) initial stat"
P12-1014,E12-1061,0,\N,Missing
P12-1066,P10-1131,0,0.0395256,"8; McDonald et al., 2011; Søgaard, 2011). Since many unlexicalized dependencies are preserved across languages, these approaches are shown to be effective for related languages. For instance, when applied to the language pairs within the Indo-European family, such parsers outperform unsupervised monolingual techniques by a significant margin. The challenge, however, is to enable dependency transfer for target languages that exhibit structural differences from source languages. In such cases, the extent of multilingual transfer is determined by the relation between source and target languages. Berg-Kirkpatrick and Klein (2010) define such a relation in terms of phylogenetic trees, and use this distance to selectively tie the parameters of monolingual syntactic models. Cohen et al. (2011) do not use a predefined linguistic hierarchy of language relations, but instead learn the contribution of source languages to the training mixture based on the likelihood of the target language. Søgaard (2011) proposes a different measure of language relatedness based on perplexity between POS sequences of source and target languages. Using this measure, he selects a subset of training source sentences that are closer to the target"
P12-1066,D08-1092,0,0.0582917,"Missing"
P12-1066,D11-1005,0,0.193962,"For instance, when applied to the language pairs within the Indo-European family, such parsers outperform unsupervised monolingual techniques by a significant margin. The challenge, however, is to enable dependency transfer for target languages that exhibit structural differences from source languages. In such cases, the extent of multilingual transfer is determined by the relation between source and target languages. Berg-Kirkpatrick and Klein (2010) define such a relation in terms of phylogenetic trees, and use this distance to selectively tie the parameters of monolingual syntactic models. Cohen et al. (2011) do not use a predefined linguistic hierarchy of language relations, but instead learn the contribution of source languages to the training mixture based on the likelihood of the target language. Søgaard (2011) proposes a different measure of language relatedness based on perplexity between POS sequences of source and target languages. Using this measure, he selects a subset of training source sentences that are closer to the target language. While all of the above techniques demonstrate gains from modeling language relatedness, they still underperform when the source and target languages are"
P12-1066,P04-1061,0,0.0979351,"Missing"
P12-1066,P04-1060,0,0.0923324,"Missing"
P12-1066,D11-1006,0,0.818995,", 2005; Burkett and Klein, 2008; Snyder et al., 2009). However, recent work in multilingual parsing has demonstrated the feasibility of transfer in the absence of parallel data. As a main source of guidance, these methods rely on the commonalities in dependency structure across languages. For instance, Naseem et al. (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. An alternative approach is to directly employ a non-lexicalized 630 parser trained on one language to process a target language (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). Since many unlexicalized dependencies are preserved across languages, these approaches are shown to be effective for related languages. For instance, when applied to the language pairs within the Indo-European family, such parsers outperform unsupervised monolingual techniques by a significant margin. The challenge, however, is to enable dependency transfer for target languages that exhibit structural differences from source languages. In such cases, the extent of multilingual transfer is determined by the relation between source and target languages. Berg-Kirkpatrick and Kle"
P12-1066,D10-1120,1,0.93215,"of observed typological information, a set of automatically induced latent features can effectively work as a proxy for typology. 2 Related Work Traditionally, parallel corpora have been a mainstay of multilingual parsing (Wu, 1997; Kuhn, 2004; Smith and Smith, 2004; Hwa et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008; Snyder et al., 2009). However, recent work in multilingual parsing has demonstrated the feasibility of transfer in the absence of parallel data. As a main source of guidance, these methods rely on the commonalities in dependency structure across languages. For instance, Naseem et al. (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. An alternative approach is to directly employ a non-lexicalized 630 parser trained on one language to process a target language (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). Since many unlexicalized dependencies are preserved across languages, these approaches are shown to be effective for related languages. For instance, when applied to the language pairs within the Indo-European family, such parsers outperform unsupervised monolingual techniques by a si"
P12-1066,W04-3207,0,0.0694402,"Missing"
P12-1066,P09-1009,1,0.903808,"Missing"
P12-1066,P11-2120,0,0.319793,"in, 2008; Snyder et al., 2009). However, recent work in multilingual parsing has demonstrated the feasibility of transfer in the absence of parallel data. As a main source of guidance, these methods rely on the commonalities in dependency structure across languages. For instance, Naseem et al. (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. An alternative approach is to directly employ a non-lexicalized 630 parser trained on one language to process a target language (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). Since many unlexicalized dependencies are preserved across languages, these approaches are shown to be effective for related languages. For instance, when applied to the language pairs within the Indo-European family, such parsers outperform unsupervised monolingual techniques by a significant margin. The challenge, however, is to enable dependency transfer for target languages that exhibit structural differences from source languages. In such cases, the extent of multilingual transfer is determined by the relation between source and target languages. Berg-Kirkpatrick and Klein (2010) define"
P12-1066,J97-3002,0,0.143665,"Missing"
P12-1066,H05-1107,0,0.0748513,"Missing"
P12-1066,I08-3008,0,0.822024,"et al., 2005; Xi and Hwa, 2005; Burkett and Klein, 2008; Snyder et al., 2009). However, recent work in multilingual parsing has demonstrated the feasibility of transfer in the absence of parallel data. As a main source of guidance, these methods rely on the commonalities in dependency structure across languages. For instance, Naseem et al. (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. An alternative approach is to directly employ a non-lexicalized 630 parser trained on one language to process a target language (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011). Since many unlexicalized dependencies are preserved across languages, these approaches are shown to be effective for related languages. For instance, when applied to the language pairs within the Indo-European family, such parsers outperform unsupervised monolingual techniques by a significant margin. The challenge, however, is to enable dependency transfer for target languages that exhibit structural differences from source languages. In such cases, the extent of multilingual transfer is determined by the relation between source and target languages. B"
P12-1066,W06-2920,0,\N,Missing
P12-1066,D07-1096,0,\N,Missing
P12-2063,P08-1087,0,0.0370559,"ociation for Computational Linguistics, pages 322–327, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2 Related Work 3 Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007"
P12-2063,P11-1004,0,0.0277945,"or pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the word baseline partially due to the insufficient a"
P12-2063,P05-1071,0,0.174571,"Akın (2010) and Mermer and Saraclar (2011) attempts to integrate morphology and MT more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor for Turkish-toEnglish MT. However, the bilingual strategy shows no gain over the monolingual version, and neither version is competitive for MT with a supervised Turkish morphological segmenter (Oflazer, 1993). By contrast, the unsupervised analyzer we report on here yields MSA-to-English MT performance that equals or exceed the performance obtained with a leading supervised MSA segmenter, MADA (Habash and Rambow, 2005). 323 Review of Lee Unsupervised Segmenter The segmenter of Lee et al. (2011) is a probabilistic model operating at word-type level. It is divided into four sub-model levels. Model 1 prefers small affix lexicons, and assumes that morphemes are drawn independently. Model 2 generates a latent POS tag for each word type, conditioning the word’s affixes on the tag, thereby encouraging compatible affixes to be generated together. Model 3 incorporates token-level contextual information, by generating word tokens with a type-level Hidden Markov Model (HMM). Finally, Model 4 models morphosyntactic agr"
P12-2063,P08-2015,0,0.0734444,"Missing"
P12-2063,N09-1036,0,0.0737937,"Missing"
P12-2063,D07-1091,0,0.0651734,"Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322–327, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2 Related Work 3 Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers"
P12-2063,W04-3250,0,0.0937356,"Missing"
P12-2063,W11-0301,1,0.747344,"ervised MSA segmenters on dialect establishes a lower bound, which the unsupervised segmenter must exceed if it is to be useful for dialect. And comparing the gain for supervised and unsupervised segmenters on MSA tells us how useful the unsupervised segmenter is, relative to the ideal case in which a supervised segmenter is available. In this paper, we show that an unsupervised segmenter can in fact rival or surpass supervised MSA segmenters on MSA itself, while at the same time providing superior performance on dialect. Specifically, we compare the state-of-the-art morphological analyzer of Lee et al. (2011) with two leading supervised analyzers for MSA, MADA and Sakhr1 , each serving as an alternative preprocessor for a state-ofthe-art statistical MT system (Shen et al., 2008). We measure MSA performance on NIST MT-08 (NIST, 2010), and dialect performance on a Levantine dialect web corpus (Zbib et al., 2012b). To improve performance, we apply maximum marginal decoding (Johnson and Goldwater, 2009) (MM) to combine multiple runs of the Lee segmenter, and show that this dramatically reduces the variance and noise in the segmenter output, while yielding an improved segmentation accuracy that exceeds"
P12-2063,D10-1015,0,0.0715232,"Missing"
P12-2063,P10-3006,0,0.0188029,"babilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the word baseline partially due to the insufficient accuracy of the automatic morphological analyzer. The work of Mermer and Akın (2010) and Mermer and Saraclar (2011) attempts to integrate morphology and MT more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor for Turkish-toEnglish MT. However, the bilingual"
P12-2063,P11-1130,0,0.012922,"e unsegmented baseline. 1 http://www.sakhr.com/Default.aspx 322 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 322–327, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2 Related Work 3 Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approa"
P12-2063,P11-1090,0,0.0911202,"Missing"
P12-2063,E93-1066,0,0.114741,"n existing MT system at the level of morphemes. The system does not outperform the word baseline partially due to the insufficient accuracy of the automatic morphological analyzer. The work of Mermer and Akın (2010) and Mermer and Saraclar (2011) attempts to integrate morphology and MT more closely than we do, by incorporating bilingual alignment probabilities into a Gibbs-sampled version of Morfessor for Turkish-toEnglish MT. However, the bilingual strategy shows no gain over the monolingual version, and neither version is competitive for MT with a supervised Turkish morphological segmenter (Oflazer, 1993). By contrast, the unsupervised analyzer we report on here yields MSA-to-English MT performance that equals or exceed the performance obtained with a leading supervised MSA segmenter, MADA (Habash and Rambow, 2005). 323 Review of Lee Unsupervised Segmenter The segmenter of Lee et al. (2011) is a probabilistic model operating at word-type level. It is divided into four sub-model levels. Model 1 prefers small affix lexicons, and assumes that morphemes are drawn independently. Model 2 generates a latent POS tag for each word type, conditioning the word’s affixes on the tag, thereby encouraging co"
P12-2063,N09-1024,0,0.0980242,"2 runs of MM on the full MT-08 data set. We give the average segmentation recall, precision, F1-measure, and exact-match accuracy between outputs, at word-type and word-token levels. Unique prefixes Unique suffixes Top-95 prefixes Top-95 suffixes ATB GS 17 41 7 14 GS 130 261 7 26 Model M1 M2 M3 M4 Acc 74.5 86.7 93.9 95.1 MT-08 MM Morf 93 287 216 241 6 6 19 19 Mean 80.1 81.4 81.4 86.2 Min 79.0 80.2 80.1 85.4 Max 81.5 83.0 82.8 87.2 MM 81.8 82.0 83.2 88.2 Table 3: Segmentation F-scores on ATB dataset for Lee segmenter, shown for each Model level M1–M4 on the Arabic segmentation dataset used by (Poon et al., 2009): We give the mean, minimum, and maximum F-scores for 25 independent runs of Gibbs sampling, together with the F-score from running MM over that same set of runs. 5 MT Evaluation 5.1 Experimental Design Table 2: Affix statistics of unsupervised segmenters. For the ATB lexicon, we show statistics for the Lee segmenter with regular Gibbs sampling (GS). For the MT08 lexicon, we also show the output of the Lee segmenter with maximum marginal decoding (MM). In addition, we show statistics for Morfessor. induces 130 prefixes and 261 suffixes for MT-08 (statistics for Morfessor are similar). This phe"
P12-2063,P06-1001,0,0.0825384,"approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al., 2007; Creutz and Lagus, 2007; Clifton and Sarkar, 2011; Mermer and Akın, 2010; Mermer and Saraclar, 2011). Virpioja et al. (2007) apply the unsupervised morphological segmenter Morfessor (Creutz and Lagus, 2007), and apply an existing MT system at the level of morphemes. The system does not outperform the wor"
P12-2063,P08-1066,0,0.0287875,"Missing"
P12-2063,2007.mtsummit-papers.65,0,0.117346,"Missing"
P12-2063,E06-1006,0,0.028098,"Annual Meeting of the Association for Computational Linguistics, pages 322–327, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2 Related Work 3 Machine translation systems that process highly inflected languages often incorporate morphological analysis. Some of these approaches rely on morphological analysis for pre- and post-processing, while others modify the core of a translation system to incorporate morphological information (Habash, 2008; Luong et al., 2010; Nakov and Ng, 2011). For instance, factored translation Models (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) parametrize translation probabilities as factors encoding morphological features. The approach we have taken in this paper is an instance of a segmented MT model, which divides the input into morphemes and uses the derived morphemes as a unit of translation (Sadat and Habash, 2006; Badr et al., 2008; Clifton and Sarkar, 2011). This is a mainstream architecture that has been shown to be effective when translating from a morphologically rich language. A number of recent approaches have explored the use of unsupervised morphological analyzers for MT (Virpioja et al.,"
P12-2063,N12-1006,1,0.807234,"by Johnson and Goldwater (2009), but also for its potential to provide more stable and less noisy results. 324 MT System. Our experiments were performed using a state-of-the-art, hierarchical string-todependency-tree MT system, described in Shen et al. (2008). Morphological Analyzers. We compare the Lee segmenter with the supervised MSA segmenter MADA, using its “D3” scheme. We also compare with Sakhr, an intensively-engineered, supervised MSA segmenter which applies multiple NLP technologies to the segmentation problem, and which has given the best results for our MT system in previous work (Zbib et al., 2012a). We also compare with Morfessor. MT experiments. We apply the appropriate segmenter to split words into morphemes, which we then treat as words for alignment and decoding. Following Lee et al. (2011), we segment the test and training sets jointly, estimating separate translation models for each segmenter/dataset combination. Training and Test Corpora. Our “Full MSA” corpus is the NIST MT-08 Constrained Data Track Arabic training corpus (35M total, 336K unique words); our “Small MSA” corpus is a 1.3M-word subset. Both are tested on the MT-08 evaluation set. For dialect, we use a Levantine di"
P12-2063,P08-2039,0,\N,Missing
P12-2063,P08-1000,0,\N,Missing
P13-1029,W06-1615,0,0.0761468,"les Figure 1: Derivation trees for CFG as well as CCG, HPSG and LFG formalisms. also observed on CCG and LFG formalisms. 2 Related Work Our work belongs to a broader class of research on transfer learning in parsing. This area has garnered significant attention due to the expense associated with obtaining syntactic annotations. Transfer learning in parsing has been applied in different contexts, such as multilingual learning (Snyder et al., 2009; Hwa et al., 2005; McDonald et al., 2006; McDonald et al., 2011; Jiang and Liu, 2009), domain adaptation (McClosky et al., 2010; Dredze et al., 2007; Blitzer et al., 2006), and crossformalism transfer (Hockenmaier and Steedman, 2002; Miyao et al., 2005; Cahill et al., 2002; Riezler et al., 2002; Chen and Shanker, 2005; Candito et al., 2010). There have been several attempts to map annotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the two formalisms. For instance, mappings may specify how to convert traces and functional tags in Penn Treebank to the f-structure in LFG (Cahill, 2004). These conversion rule"
P13-1029,P04-1041,0,0.0672433,"Missing"
P13-1029,candito-etal-2010-statistical,0,0.0604021,"Missing"
P13-1029,P05-1022,0,0.198679,"metric is commonly used to measure parsing quality for the formalisms considered in this paper. The detailed definition of this measure as applied for each formalism is provided in (Clark and Curran, 2003; Miyao and Tsujii, 2008; Cahill et al., 2004). For CCG, we use the evaluation script from the C&C tools.5 For HPSG, we evaluate all types of dependencies, including punctuations. For LFG, we consider the preds-only dependencies, which are the dependencies between pairs of words. Secondly, we also evaluate using unlabeled PARSEVAL, a standard measure for PCFG parsing (Petrov and Klein, 2007; Charniak and Johnson, 2005; Charniak, 2000; Collins, 1997). The dependency F-score captures both the target5 8 Experiment and Analysis Impact of Coarse Annotations on Target Formalism: To analyze the effectiveness of annotation transfer, we fix the number of annotated trees in the target formalism and vary the amount of coarse annotations available to the algorithm during training. In particular, we use 500 sentences with formalism-specific annotations, and vary the number of CFG trees from zero to 15,000. As Figure 4 shows, CFG data boosts parsing accuracy for all the target formalisms. For instance, there is a gain o"
P13-1029,J07-4004,0,0.1221,"G ) (3) Thus, we assume that both pjoint and pCF G have the same dependence on the fCF G features. The Likelihood Objective: Given the models above, it is natural to use maximum likelihood to find the optimal parameters. To do this, we define the following regularized likelihood function: L(θ) = i=1 M X i=1 Supertagging When parsing a target formalism tree, one needs to associate each word with a lexical entry. However, since the number of candidates is typically more than one thousand, the size of the chart explodes. One effective way of reducing the number of candidates is via supertagging (Clark and Curran, 2007). A supertagger is used for selecting a small set of lexical entry candidates for each word in the sentence. We use the tagger in (Clark and Curran, 2007) as a general suppertagger for all the grammars considered. The only difference is that we use different lexical entries in different grammars. pCF G (yCF G |S; θCF G ) = N X Implementation 5.2 Feature Forest Pruning In the BFGS algorithm (see Section 4), feature expectation is computed using the inside-outside algorithm. To perform this dynamic programming efficiently, we first need to build the packed chart, namely the feature forest (Miyao"
P13-1029,P97-1003,0,0.503639,"Missing"
P13-1029,W06-2932,0,0.0355159,"↑SBJ!=↓ ↑=↓ ↑OBJ!=↓ [Pron.I] [↑SBJ,↑OBJ] [N.3pl] I eat HPSG subj_head ↑=↓ NP apples apples [N.no3sg] [N<V.bse&gt;N] I eat [N.3pl] apples Figure 1: Derivation trees for CFG as well as CCG, HPSG and LFG formalisms. also observed on CCG and LFG formalisms. 2 Related Work Our work belongs to a broader class of research on transfer learning in parsing. This area has garnered significant attention due to the expense associated with obtaining syntactic annotations. Transfer learning in parsing has been applied in different contexts, such as multilingual learning (Snyder et al., 2009; Hwa et al., 2005; McDonald et al., 2006; McDonald et al., 2011; Jiang and Liu, 2009), domain adaptation (McClosky et al., 2010; Dredze et al., 2007; Blitzer et al., 2006), and crossformalism transfer (Hockenmaier and Steedman, 2002; Miyao et al., 2005; Cahill et al., 2002; Riezler et al., 2002; Chen and Shanker, 2005; Candito et al., 2010). There have been several attempts to map annotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the two formalisms. For instance, mappings may"
P13-1029,J03-4003,0,0.0688066,"parallelized, using the Message Passing Interface package (Gropp et al., 1999). 4 An alternative approach would be to marginalize over yCF G and maximize over yCCG . However, this is a harder computational problem. where pCCG and pCF G are defined in Equations 2 and 3 respectively. The last term is the l2 -norm regularization. Our goal is then to find a θ that maximizes L(θ). 294 5.4 structures. However, a common problem for lexicalized grammars is that the forest size is too large. In CFG, the forest is pruned according to the inside probability of a simple generative PCFG model and a prior (Collins, 2003). The basic idea is to prune the trees with lower probability. For the target formalism, a common practice is to prune the forest using the supertagger (Clark and Curran, 2007; Miyao, 2006). In our implementation, we applied all pruning techniques, because the forest is a combination of CFG and target grammar formalisms (e.g., CCG or HPSG). 5.3 We introduce our model in the context of CCG, but the model can easily be generalized to other constituency-based grammars, such as HPSG and LFG. In a derivation tree, the formalism-specific information is mainly encoded in the lexical entries and the a"
P13-1029,D11-1006,0,0.0271025,"ron.I] [↑SBJ,↑OBJ] [N.3pl] I eat HPSG subj_head ↑=↓ NP apples apples [N.no3sg] [N<V.bse&gt;N] I eat [N.3pl] apples Figure 1: Derivation trees for CFG as well as CCG, HPSG and LFG formalisms. also observed on CCG and LFG formalisms. 2 Related Work Our work belongs to a broader class of research on transfer learning in parsing. This area has garnered significant attention due to the expense associated with obtaining syntactic annotations. Transfer learning in parsing has been applied in different contexts, such as multilingual learning (Snyder et al., 2009; Hwa et al., 2005; McDonald et al., 2006; McDonald et al., 2011; Jiang and Liu, 2009), domain adaptation (McClosky et al., 2010; Dredze et al., 2007; Blitzer et al., 2006), and crossformalism transfer (Hockenmaier and Steedman, 2002; Miyao et al., 2005; Cahill et al., 2002; Riezler et al., 2002; Chen and Shanker, 2005; Candito et al., 2010). There have been several attempts to map annotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the two formalisms. For instance, mappings may specify how to convert"
P13-1029,J08-1002,0,0.293327,"nnotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the two formalisms. For instance, mappings may specify how to convert traces and functional tags in Penn Treebank to the f-structure in LFG (Cahill, 2004). These conversion rules are typically utilized in two ways: (1) to create a new treebank which is consequently used to train a parser for the target formalism (Hockenmaier and Steedman, 2002; Clark and Curran, 2003; Miyao et al., 2005; Miyao and Tsujii, 2008), (2) to translate the output of a CFG parser into the target formalism (Cahill et al., 2002). The design of these rules is a major linguistic and computational undertaking, which requires multiple iterations over the data to increase coverage (Miyao et al., 2005; Oepen et al., 2004). By nature, the mapping rules are formalism spe2 While the Penn Treebank-2 contains richer annotations, we decided to use the Penn Treebank-1 to demonstrate the feasibility of transfer from coarse annotations. 292 cific and therefore not transferable. Moreover, frequently designing such mappings involves modificat"
P13-1029,hockenmaier-steedman-2002-acquiring,0,0.0945,"Missing"
P13-1029,W09-3803,0,0.0173238,"pl] I eat HPSG subj_head ↑=↓ NP apples apples [N.no3sg] [N<V.bse&gt;N] I eat [N.3pl] apples Figure 1: Derivation trees for CFG as well as CCG, HPSG and LFG formalisms. also observed on CCG and LFG formalisms. 2 Related Work Our work belongs to a broader class of research on transfer learning in parsing. This area has garnered significant attention due to the expense associated with obtaining syntactic annotations. Transfer learning in parsing has been applied in different contexts, such as multilingual learning (Snyder et al., 2009; Hwa et al., 2005; McDonald et al., 2006; McDonald et al., 2011; Jiang and Liu, 2009), domain adaptation (McClosky et al., 2010; Dredze et al., 2007; Blitzer et al., 2006), and crossformalism transfer (Hockenmaier and Steedman, 2002; Miyao et al., 2005; Cahill et al., 2002; Riezler et al., 2002; Chen and Shanker, 2005; Candito et al., 2010). There have been several attempts to map annotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the two formalisms. For instance, mappings may specify how to convert traces and functional"
P13-1029,N07-1051,0,0.121309,"dependency F-score. This metric is commonly used to measure parsing quality for the formalisms considered in this paper. The detailed definition of this measure as applied for each formalism is provided in (Clark and Curran, 2003; Miyao and Tsujii, 2008; Cahill et al., 2004). For CCG, we use the evaluation script from the C&C tools.5 For HPSG, we evaluate all types of dependencies, including punctuations. For LFG, we consider the preds-only dependencies, which are the dependencies between pairs of words. Secondly, we also evaluate using unlabeled PARSEVAL, a standard measure for PCFG parsing (Petrov and Klein, 2007; Charniak and Johnson, 2005; Charniak, 2000; Collins, 1997). The dependency F-score captures both the target5 8 Experiment and Analysis Impact of Coarse Annotations on Target Formalism: To analyze the effectiveness of annotation transfer, we fix the number of annotated trees in the target formalism and vary the amount of coarse annotations available to the algorithm during training. In particular, we use 500 sentences with formalism-specific annotations, and vary the number of CFG trees from zero to 15,000. As Figure 4 shows, CFG data boosts parsing accuracy for all the target formalisms. For"
P13-1029,N04-1013,0,0.0194339,"Parseval 65 0 1000 3000 7000 11000 15000 (c) LFG Figure 4: Model performance with 500 target formalism trees and different numbers of CFG trees, evaluated using labeled/unlabeled dependency F-score and unlabeled PARSEVAL. crafted rules (Hockenmaier and Steedman, 2002; Miyao et al., 2005). Table 3 shows which sections of the treebanks were used in training, testing and development for both formalisms. Our LFG training dataset was constructed in a similar fashion (Cahill et al., 2002). However, we choose to use PARC700 as our LFG tesing and development datasets, following the previous work by (Kaplan et al., 2004). It contains 700 manually annotated sentences that are randomly selected from Penn Treebank Section 23. The split of PARC700 follows the setting in (Kaplan et al., 2004). Since our model does not assume parallel data, we use distinct sentences in the source and target treebanks. Following previous work (Hockenmaier, 2003; Miyao and Tsujii, 2008), we only consider sentences not exceeding 40 words, except on PARC700 where all sentences are used. grammar labels and tree-structural relations. The unlabeled PARSEVAL is used as an auxiliary measure that enables us to separate these two aspects by f"
P13-1029,W03-2401,0,0.0382263,"l,r i hr, d, c, syl,r , hll,r i, hr, d, c, syl,r i hr, c, spl,r , syl,r , hll,r i, hr, c, spl,r , syl,r i 7 Evaluation Setup Grammar CCG HPSG LFG Train Sec. 02-21 Dev. Test Sec. 00 Sec. 23 140 sents. in PARC700 560 sents. in PARC700 Table 3: Training/Dev./Test split on WSJ sections and PARC700 for different grammar formalisms. Datasets: As a source of coarse annotations, we use the Penn Treebank-1 (Marcus et al., 1993). In addition, for CCG, HPSG and LFG, we rely on formalism-specific corpora developed in prior research (Hockenmaier and Steedman, 2002; Miyao et al., 2005; Cahill et al., 2002; King et al., 2003). All of these corpora were derived via conversion of Penn Treebank to the target formalisms. In particular, our CCG and HPSG datasets were converted from the Penn Treebank based on handTable 2: Binary feature templates used in f (y, S). Unary and root features follow a similar pattern. In order to apply the same feature templates to other target formalisms, we only need to assign the atomic features r and hl with the formalismspecific values. We do not need extra engineering work on redesigning the feature templates. 296 88 86 80 86 84 84 82 82 80 80 78 76 78 Labeled Dep Unlabeled Dep Unlabel"
P13-1029,P02-1035,0,0.209244,"lated Work Our work belongs to a broader class of research on transfer learning in parsing. This area has garnered significant attention due to the expense associated with obtaining syntactic annotations. Transfer learning in parsing has been applied in different contexts, such as multilingual learning (Snyder et al., 2009; Hwa et al., 2005; McDonald et al., 2006; McDonald et al., 2011; Jiang and Liu, 2009), domain adaptation (McClosky et al., 2010; Dredze et al., 2007; Blitzer et al., 2006), and crossformalism transfer (Hockenmaier and Steedman, 2002; Miyao et al., 2005; Cahill et al., 2002; Riezler et al., 2002; Chen and Shanker, 2005; Candito et al., 2010). There have been several attempts to map annotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the two formalisms. For instance, mappings may specify how to convert traces and functional tags in Penn Treebank to the f-structure in LFG (Cahill, 2004). These conversion rules are typically utilized in two ways: (1) to create a new treebank which is consequently used to train a parser for the targ"
P13-1029,J93-2004,0,0.0482855,"r , hpl,r , hll,r i hr, c, spl,r , hwl,r , hpl,r , i, hr, c, spl,r , hwl,r , hll,r i hr, d, c, hpl,r , hll,r i, hr, d, c, hpl,r i, hr, d, c, hll,r i hr, c, hpl,r , hll,r i, hr, c, hpl,r i, hr, c, hll,r i hr, d, c, syl,r , hll,r i, hr, d, c, syl,r i hr, c, spl,r , syl,r , hll,r i, hr, c, spl,r , syl,r i 7 Evaluation Setup Grammar CCG HPSG LFG Train Sec. 02-21 Dev. Test Sec. 00 Sec. 23 140 sents. in PARC700 560 sents. in PARC700 Table 3: Training/Dev./Test split on WSJ sections and PARC700 for different grammar formalisms. Datasets: As a source of coarse annotations, we use the Penn Treebank-1 (Marcus et al., 1993). In addition, for CCG, HPSG and LFG, we rely on formalism-specific corpora developed in prior research (Hockenmaier and Steedman, 2002; Miyao et al., 2005; Cahill et al., 2002; King et al., 2003). All of these corpora were derived via conversion of Penn Treebank to the target formalisms. In particular, our CCG and HPSG datasets were converted from the Penn Treebank based on handTable 2: Binary feature templates used in f (y, S). Unary and root features follow a similar pattern. In order to apply the same feature templates to other target formalisms, we only need to assign the atomic features"
P13-1029,P09-1009,1,0.829804,"I ROOT LFG (S[dcl]NP)/NP eat head_comp ↑SBJ!=↓ ↑=↓ ↑OBJ!=↓ [Pron.I] [↑SBJ,↑OBJ] [N.3pl] I eat HPSG subj_head ↑=↓ NP apples apples [N.no3sg] [N<V.bse&gt;N] I eat [N.3pl] apples Figure 1: Derivation trees for CFG as well as CCG, HPSG and LFG formalisms. also observed on CCG and LFG formalisms. 2 Related Work Our work belongs to a broader class of research on transfer learning in parsing. This area has garnered significant attention due to the expense associated with obtaining syntactic annotations. Transfer learning in parsing has been applied in different contexts, such as multilingual learning (Snyder et al., 2009; Hwa et al., 2005; McDonald et al., 2006; McDonald et al., 2011; Jiang and Liu, 2009), domain adaptation (McClosky et al., 2010; Dredze et al., 2007; Blitzer et al., 2006), and crossformalism transfer (Hockenmaier and Steedman, 2002; Miyao et al., 2005; Cahill et al., 2002; Riezler et al., 2002; Chen and Shanker, 2005; Candito et al., 2010). There have been several attempts to map annotations in coarse grammars like CFG to annotations in richer grammar, like HPSG, LFG, or CCG. Traditional approaches in this area typically rely on manually specified rules that encode the relation between the t"
P13-1029,P11-1069,0,0.0303395,"Missing"
P13-1029,A00-2018,0,\N,Missing
P13-1029,W03-1013,0,\N,Missing
P13-1029,D07-1112,0,\N,Missing
P13-1029,N10-1004,0,\N,Missing
P13-1127,P09-1010,1,0.303462,"ultimately unsuccessful versions of standard formal programming languages. In recent years 1 The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates 1294 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1294–1303, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics data structures for holding the data. The need to automate this task arises because input format specifications are almost always described in natural languages, with these specifica"
P13-1127,P10-1129,1,0.828137,"l versions of standard formal programming languages. In recent years 1 The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates 1294 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1294–1303, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics data structures for holding the data. The need to automate this task arises because input format specifications are almost always described in natural languages, with these specifications then manually tran"
P13-1127,W10-2903,0,0.289106,"on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy feedback. While previous approaches rely on the feedback to train a discriminative prediction model, our approach models a generative process to guide structure predictions when the feedback is noisy or unavailable. NLP in Software Engineering Researchers have recently developed a number of approaches that apply natural language p"
P13-1127,P11-1144,0,0.0103732,"ion tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is"
P13-1127,N10-1138,0,0.0194067,"rom the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time."
P13-1127,P11-1149,0,0.0208324,"efinition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to pre"
P13-1127,N07-1018,0,0.0667703,"Missing"
P13-1127,P09-1011,0,0.0295194,"m with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et al. (2009) formalize the task as a sequence of decisions and learns from local rewards in a Reinforcement Learning framework, our model learns to predict the whole structure at a time. Another difference is the way our model incorporates the noisy feedback. While previou"
P13-1127,P11-1060,0,0.221196,"er science was founded. Early attempts to solve this problem produced what were essentially verbose, clumsy, and ultimately unsuccessful versions of standard formal programming languages. In recent years 1 The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates 1294 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1294–1303, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics data structures for holding the data. The need to automate this task"
P13-1127,D09-1001,0,0.199457,"since the field of computer science was founded. Early attempts to solve this problem produced what were essentially verbose, clumsy, and ultimately unsuccessful versions of standard formal programming languages. In recent years 1 The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates 1294 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1294–1303, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics data structures for holding the data. The need"
P13-1127,W00-1317,0,0.0131852,"nput format structure (we omit the background phrases in this tree in order to give a clear view of the input format structure); and (c) formal definition of the input format constructed from the specification tree, represented as a context-free grammar in Backus-Naur Form with additional size constraints. specification. 2 Related Work Learning Meaning Representation from Text Mapping sentences into structural meaning representations is an active and extensively studied task in NLP. Examples of meaning representations considered in prior research include logical forms based on database query (Tang and Mooney, 2000; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Wong and Mooney, 2007; Poon and Domingos, 2009; Liang et al., 2011; Goldwasser et al., 2011), semantic frames (Das et al., 2010; Das and Smith, 2011) and database records (Chen and Mooney, 2008; Liang et al., 2009). Learning Semantics from Feedback Our approach is related to recent research on learning from indirect supervision. Examples include leveraging feedback available via responses from a virtual world (Branavan et al., 2009) or from executing predicted database queries (Chang et al., 2010; Clarke et al., 2010). While Branavan et a"
P13-1127,P07-1121,0,0.0398509,"e specifications into executable code has been around since the field of computer science was founded. Early attempts to solve this problem produced what were essentially verbose, clumsy, and ultimately unsuccessful versions of standard formal programming languages. In recent years 1 The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates 1294 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1294–1303, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Lin"
P13-1127,P09-1110,0,0.0277253,"xecutable code has been around since the field of computer science was founded. Early attempts to solve this problem produced what were essentially verbose, clumsy, and ultimately unsuccessful versions of standard formal programming languages. In recent years 1 The code, data, and experimental setup for this research are available at http://groups.csail.mit.edu/rbg/code/nl2p however, researchers have had success addressing specific aspects of this problem. Recent advances in this area include the successful translation of natural language commands into database queries (Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Poon and Domingos, 2009; Liang et al., 2011) and the successful mapping of natural language instructions into Windows command sequences (Branavan et al., 2009; Branavan et al., 2010). In this paper we explore a different aspect of this general problem: the translation of natural language input specifications into executable code that correctly parses the input data and generates 1294 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1294–1303, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics data structures for ho"
P14-1019,P08-1067,0,0.0940694,"Missing"
P14-1019,P10-1001,0,0.126867,"Missing"
P14-1019,W13-4916,0,0.0442721,"Missing"
P14-1019,D10-1125,1,0.958627,"cial, even at the cost of using approximate inference and sacrificing optimality. The first successful approach in this arena was reranking (Collins, 2000; Charniak and Johnson, 2005) on constituency parsing. Reranking can be combined with an arbitrary scoring function, and thus can easily incorporate global features over the entire parse tree. Its main disadvantage is that the output parse can only be one of the few parses passed to the reranker. Recent work has focused on more powerful inference mechanisms that consider the full search space (Zhang and McDonald, 2012; Rush and Petrov, 2012; Koo et al., 2010; Huang, 2008). For instance, Nakagawa (2007) deals with tractability issues by using sampling to approximate marginals. Another example is the dual decomposition (DD) framework (Koo et al., 2010; Martins et al., 2011). The idea in DD is to decompose the hard maximization problem into smaller parts that can be efficiently maximized and enforce agreement among these via Lagrange multipliers. The method is essentially equivalent to linear programming relaxation approaches (Martins et al., 2009; Sontag et al., 2011), and also similar in spirit to ILP approaches (Punyakanok et al., 2004). A natura"
P14-1019,C10-1011,0,0.0870043,"Missing"
P14-1019,W06-2920,0,0.264203,"Missing"
P14-1019,P09-1039,0,0.0392755,"Missing"
P14-1019,P05-1022,0,0.254207,"Missing"
P14-1019,D10-1004,0,0.0898295,"Missing"
P14-1019,D11-1022,0,0.41667,"nking can be combined with an arbitrary scoring function, and thus can easily incorporate global features over the entire parse tree. Its main disadvantage is that the output parse can only be one of the few parses passed to the reranker. Recent work has focused on more powerful inference mechanisms that consider the full search space (Zhang and McDonald, 2012; Rush and Petrov, 2012; Koo et al., 2010; Huang, 2008). For instance, Nakagawa (2007) deals with tractability issues by using sampling to approximate marginals. Another example is the dual decomposition (DD) framework (Koo et al., 2010; Martins et al., 2011). The idea in DD is to decompose the hard maximization problem into smaller parts that can be efficiently maximized and enforce agreement among these via Lagrange multipliers. The method is essentially equivalent to linear programming relaxation approaches (Martins et al., 2009; Sontag et al., 2011), and also similar in spirit to ILP approaches (Punyakanok et al., 2004). A natural approach to approximate global inference is via search. For instance, a transitionbased parsing system (Zhang and Nivre, 2011) incrementally constructs a parsing structure using greedy beam-search. Other approaches o"
P14-1019,P13-2109,0,0.599953,"ence and whether the span is followed by the punctuation. Features First- to Third-Order Features The feature templates of first- to third-order features are mainly drawn from previous work on graphbased parsing (McDonald and Pereira, 2006), transition-based parsing (Nivre et al., 2006) and dual decomposition-based parsing (Martins et al., 2011). As shown in Figure 5, the arc is the basic structure for first-order features. We also define features based on consecutive sibling, grandparent, arbitrary sibling, head bigram, grand-sibling and tri-siblings, which are also used in the Turbo parser (Martins et al., 2013). In addition to these first- to third-order structures, we also consider grand-grandparent and sibling-grandchild structures. There are two types of sibling-grandchild structures: (1) inner-sibling when the sibling is between the head and the modifier and (2) outersibling for the other cases. • Neighbors The POS tags of the neighboring words to the left and right of each span, together with the binned span length and the POS tag at the span root. • Valency We consider valency features for each POS tag. Specifically, we add two types of valency information: (1) the binned number of non-punctua"
P14-1019,N10-1115,0,0.076856,"Missing"
P14-1019,W13-4910,0,0.19723,"er distribution is not well-defined and we only employ Gibbs sampling for simplicity. On the CATiB dataset, we restrict the sample trees to always be projective as described in Section 3.2.1. However, we do not impose this constraint for the CoNLL datasets. Table 1: POS tag feature templates. ti and wi denotes the POS tag and the word at the current position. ti−x and ti+x denote the left and right context tags, and similarly for words. contain non-projective dependency trees. We use all sentences in CoNLL datasets during training and testing. We also use the Columbia Arabic Treebank (CATiB) (Marton et al., 2013). CATiB mostly includes projective trees. The trees are annotated with both gold and predicted versions of POS tags and morphology information. Following Marton et al. (2013), for this dataset we use 12 core POS tags, word lemmas, determiner features, rationality features and functional genders and numbers. Some CATiB sentences exceed 200 tokens. For efficiency, we limit the sentence length to 70 tokens in training and development sets. However, we do not impose this constraint during testing. We handle long sentences during testing by applying a simple split-merge strategy. We split the sente"
P14-1019,W11-2130,0,0.0204741,"Missing"
P14-1019,E06-1011,0,0.842183,"nction s(x, y) as s(x, y) = θ · f (x, y) Sample Bernouli variable Z with P [Z = 1] = α. if Z = 0 then y t+1 ← y t else y t+1 ← y 0 t←t+1 T ←c·T until convergence return y ∗ (1) Figure 1: Sampling-based algorithm for decoding (i.e., approximately maximizing s(x, y)). where f (x, y) is the feature vector associated with tree y for sentence x. We do not make any assumptions about how the feature function decomposes. In contrast, most state-of-the-art parsers operate under the assumption that the feature function decomposes into a sum of simpler terms. For example, in the second-order MST parser (McDonald and Pereira, 2006), all the feature terms involve arcs or consecutive siblings. Similarly, parsers based on dual decomposition (Martins et al., 2011; Koo et al., 2010) assume that s(x, y) decomposes into a sum of terms where each term can be maximized over y efficiently. 3.2 We follow here a Metropolis-Hastings sampling algorithm (e.g., see Andrieu et al. (2003)) and explore different alternative proposal distributions q(y 0 |x, y, θ, T ). The distribution q governs the small steps that are taken in generating a sequence of structures. The target distribution p folds into the procedure by defining the probabili"
P14-1019,N03-1033,0,0.0480007,"difiers. • Non-projective Arcs A flag indicating if a dependency is projective or not (i.e. if it spans a word that does not descend from its head) (Martins et al., 2011). This flag is also combined with the POS tags or the lexical words of the head and the modifier. Global Features We used feature shown promising in prior reranking work Charniak and Johnson (2005), Collins (2000) and Huang (2008). POS Tag Features In the joint POS correction scenario, we also add additional features specifically for POS prediction. The feature templates are inspired by previous feature-rich POS tagging work (Toutanova et al., 2003). However, we are free to add higher order features because we do not rely on dynamic programming decoding. In our work we use feature templates up to 5-gram. Table 1 summarizes all POS tag feature templates. • Right Branch This feature enables the model to prefer right or left-branching trees. It counts the number of words on the path from the root node to the right-most non-punctuation word, normalized by the length of the sentence. • Coordination In a coordinate structure, the two adjacent conjuncts usually agree with each other on POS tags and their span lengths. For instance, in cats and"
P14-1019,H05-1066,0,0.517229,"Missing"
P14-1019,W06-2932,0,0.120456,"Missing"
P14-1019,D07-1100,0,0.288652,"Missing"
P14-1019,W06-2933,0,0.0279659,"n example of PP attachment with coordination. The arguments should be knife and fork, not and. m m +1 h! h tri-siblings! s eat! m head bigram! arbitrary sibling! h h • Span Length This feature captures the distribution of the binned span length of each POS tag. It also includes flags of whether the span reaches the end of the sentence and whether the span is followed by the punctuation. Features First- to Third-Order Features The feature templates of first- to third-order features are mainly drawn from previous work on graphbased parsing (McDonald and Pereira, 2006), transition-based parsing (Nivre et al., 2006) and dual decomposition-based parsing (Martins et al., 2011). As shown in Figure 5, the arc is the basic structure for first-order features. We also define features based on consecutive sibling, grandparent, arbitrary sibling, head bigram, grand-sibling and tri-siblings, which are also used in the Turbo parser (Martins et al., 2013). In addition to these first- to third-order structures, we also consider grand-grandparent and sibling-grandchild structures. There are two types of sibling-grandchild structures: (1) inner-sibling when the sibling is between the head and the modifier and (2) outer"
P14-1019,D12-1030,0,0.0990703,"Missing"
P14-1019,C04-1197,0,0.0608405,"Missing"
P14-1019,P11-2033,0,0.107367,"Missing"
P14-1019,N12-1054,0,0.0459334,"eatures could be beneficial, even at the cost of using approximate inference and sacrificing optimality. The first successful approach in this arena was reranking (Collins, 2000; Charniak and Johnson, 2005) on constituency parsing. Reranking can be combined with an arbitrary scoring function, and thus can easily incorporate global features over the entire parse tree. Its main disadvantage is that the output parse can only be one of the few parses passed to the reranker. Recent work has focused on more powerful inference mechanisms that consider the full search space (Zhang and McDonald, 2012; Rush and Petrov, 2012; Koo et al., 2010; Huang, 2008). For instance, Nakagawa (2007) deals with tractability issues by using sampling to approximate marginals. Another example is the dual decomposition (DD) framework (Koo et al., 2010; Martins et al., 2011). The idea in DD is to decompose the hard maximization problem into smaller parts that can be efficiently maximized and enforce agreement among these via Lagrange multipliers. The method is essentially equivalent to linear programming relaxation approaches (Martins et al., 2009; Sontag et al., 2011), and also similar in spirit to ILP approaches (Punyakanok et al"
P14-1019,D13-1093,0,0.201977,"Missing"
P14-1019,W08-2121,0,0.180218,"Missing"
P14-1019,W13-4917,0,\N,Missing
P14-1026,de-marneffe-etal-2006-generating,0,0.0419568,"Missing"
P14-1026,D11-1039,1,0.809714,"Missing"
P14-1026,P08-1030,0,0.012397,"Missing"
P14-1026,D13-1160,0,0.0581256,"sed to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated equations An amusement park sells 2 kinds of tickets. Tickets for"
P14-1026,D12-1040,0,0.0271902,"Missing"
P14-1026,P09-1010,1,0.896819,"ion (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to generate the global semantic representations common in domains such as algebra word problems. pairs (“$1.50”, “children”) and (“$4”,“adults”) both surround the word “cost,” suggesting an output equation with a sum of two constant-variable products. We consider learning with two different levels of supervision. In the first scenario, we assume access to each problem’s numeric solution (see Figure 1) for most of the data, along with a s"
P14-1026,N13-1103,1,0.56024,"Missing"
P14-1026,P10-1129,1,0.377659,"rning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automaticall"
P14-1026,D10-1119,1,0.608985,"n, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learnin"
P14-1026,P13-1042,0,0.0113296,"supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated equations An amusement"
P14-1026,D13-1161,1,0.712521,"Missing"
P14-1026,S13-1045,0,0.00788108,"supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated equations An amusement"
P14-1026,P13-1127,1,0.848031,"Missing"
P14-1026,P11-1098,0,0.00930203,"e performance when trained exclusively on answers. 2 Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Reichart and Barzilay, 2012). However, some recent work has studied the automatic induction of the set of possible templates from data (Chambers and Jurafsky, 2011; Ritter et al., 2012). In our approach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body"
P14-1026,W04-0902,0,0.4035,"d Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated equations An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50 . Adult tickets cost $ 4 . On a certain day, 278 people entered the park. On that same day the admission fees collected totaled $ 792 . How many children were admitted on that day? How many adults were admitted? u11 + u12 − n1 = 0 n2 × u21 + n3 × u22 − n4 = 0 x + y − 278 = 0 1.5x + 4y − 792 ="
P14-1026,P07-1075,0,0.0442321,"Missing"
P14-1026,P12-1045,0,0.0140584,"Missing"
P14-1026,W10-2903,0,0.045354,"iven varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word problem Aligned template Instantiated e"
P14-1026,N12-1008,1,0.824517,"Missing"
P14-1026,P10-1083,0,0.00836505,"pping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of"
P14-1026,N06-1056,0,0.0645808,"ain areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee an"
P14-1026,P09-1110,1,0.72552,"Missing"
P14-1026,Q13-1005,1,\N,Missing
P14-1130,W13-4907,0,0.0168217,"presentations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces (Collobert an"
P14-1130,W06-2920,0,0.133205,"ot include any overall regularization. In other words, keeping updating the model may lead to large parameter values and over-fitting. To counter this effect, we use parameter averaging as used in the MST and Turbo parsers. The final parameters are those averaged across all the iterations (cf. (Collins, 2002)). For simplicity, in our algorithm we average U , V , W and θ separately, which works well empirically. 5 Experimental Setup Datasets We test our dependency model on 14 languages, including the English dataset from CoNLL 2008 shared tasks and all 13 datasets from CoNLL 2006 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). These datasets include manually annotated dependency trees, POS tags and morphological information. Following standard practices, we encode this information as features. Methods We compare our model to MST and Turbo parsers on non-projective dependency parsing. For our parser, we train both a first-order parsing model (as described in Section 3 and 4) as well as a third-order model. The third order parser simply adds high-order features, those typically used in MST and Turbo parsers, into our sθ (x, y) = hθ, φ(x, y)i scoring component. The decoding algorithm for the t"
P14-1130,W13-4909,0,0.0630158,"Missing"
P14-1130,P12-1024,0,0.023343,"decompose the parameter matrix as a sum of lowrank and sparse matrices (Tao and Yuan, 2011; Zhou and Tao, 2011). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace (Waters et al., 2011; Chandrasekaran et al., 2011). We follow this decomposition while extending the parameter matrix into a tensor. Tensors are multi-way generalizations of matrices and possess an analogous notion of rank. Tensors are increasingly used as tools in spectral estimation (Hsu and Kakade, 2013), including in parsing (Cohen et al., 2012) and other NLP problems (de Cruys et al., 2013), where the goal is to avoid local optima in maximum likelihood estimation. In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associated parameter tensor. The explicit representation sidesteps inherent complexity problems associated with the tensor rank (Hillar and Lim, 2009). Our parameters are divided into a sparse set corresponding to manually chosen MST or Turbo parser features and a larger set governed by a low-rank tensor. 3 Problem Formulation We will commence he"
P14-1130,W02-1001,0,0.0599737,"of this matrix are assigned to V (i, :) and W (i, :) respectively. In our implementation, we run one epoch of our model without low-rank parameters and initialize the tensor A. Parameter Averaging The passive-aggressive algorithm regularizes the increments (e.g. ∆θ and ∆U ) during each update but does not include any overall regularization. In other words, keeping updating the model may lead to large parameter values and over-fitting. To counter this effect, we use parameter averaging as used in the MST and Turbo parsers. The final parameters are those averaged across all the iterations (cf. (Collins, 2002)). For simplicity, in our algorithm we average U , V , W and θ separately, which works well empirically. 5 Experimental Setup Datasets We test our dependency model on 14 languages, including the English dataset from CoNLL 2008 shared tasks and all 13 datasets from CoNLL 2006 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). These datasets include manually annotated dependency trees, POS tags and morphological information. Following standard practices, we encode this information as features. Methods We compare our model to MST and Turbo parsers on non-projective dependency parsing"
P14-1130,N13-1134,0,0.0416926,"Missing"
P14-1130,P10-1001,0,0.0955467,"can be further complemented with auxiliary information about words 1 Our code is available at https://github.com/ taolei87/RBGParser. participating in an arc, such as continuous vector representations of words. The exploding dimensionality of rich feature vectors must then be balanced with the difficulty of effectively learning the associated parameters from limited training data. A predominant way to counter the high dimensionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features (McDonald et al., 2005a; Koo and Collins, 2010; Martins et al., 2013). Direct manual selection may be problematic for two reasons. First, features may lack clear linguistic interpretation as in distributional features or continuous vector embeddings of words. Second, designing a small subset of templates (and features) is challenging when the relevant linguistic information is distributed across the features. For instance, morphological properties are closely tied to part-of-speech tags, which in turn relate to positional features. These features are not redundant. Therefore, we may suffer a performance loss if we select only a small subs"
P14-1130,D10-1125,1,0.885211,"1 (Ma13) 93.72 (Ma11) 93.03 (Ko10) 86.95 (Ma11) 87.96 (Zh13) 91.62 (Zh13) 77.55 (Ko10) 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter γ = 0.3. To remove the tensor in our model, we ran experiments with γ = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors φh and φm . For each word in the sentence, we add its own word vector as wel"
P14-1130,D13-1196,0,0.0147095,"Missing"
P14-1130,D10-1004,0,0.056672,"Missing"
P14-1130,D11-1022,0,0.0726598,"Missing"
P14-1130,D11-1139,0,0.133438,"inally, we demonstrate that the model can successfully leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of"
P14-1130,P13-2109,0,0.751917,"nted with auxiliary information about words 1 Our code is available at https://github.com/ taolei87/RBGParser. participating in an arc, such as continuous vector representations of words. The exploding dimensionality of rich feature vectors must then be balanced with the difficulty of effectively learning the associated parameters from limited training data. A predominant way to counter the high dimensionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features (McDonald et al., 2005a; Koo and Collins, 2010; Martins et al., 2013). Direct manual selection may be problematic for two reasons. First, features may lack clear linguistic interpretation as in distributional features or continuous vector embeddings of words. Second, designing a small subset of templates (and features) is challenging when the relevant linguistic information is distributed across the features. For instance, morphological properties are closely tied to part-of-speech tags, which in turn relate to positional features. These features are not redundant. Therefore, we may suffer a performance loss if we select only a small subset of the features. On"
P14-1130,W10-1402,0,0.0351861,"Missing"
P14-1130,P11-1159,0,0.0193241,"Missing"
P14-1130,P05-1012,0,0.856863,"d their cross products, can be further complemented with auxiliary information about words 1 Our code is available at https://github.com/ taolei87/RBGParser. participating in an arc, such as continuous vector representations of words. The exploding dimensionality of rich feature vectors must then be balanced with the difficulty of effectively learning the associated parameters from limited training data. A predominant way to counter the high dimensionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features (McDonald et al., 2005a; Koo and Collins, 2010; Martins et al., 2013). Direct manual selection may be problematic for two reasons. First, features may lack clear linguistic interpretation as in distributional features or continuous vector embeddings of words. Second, designing a small subset of templates (and features) is challenging when the relevant linguistic information is distributed across the features. For instance, morphological properties are closely tied to part-of-speech tags, which in turn relate to positional features. These features are not redundant. Therefore, we may suffer a performance loss if we"
P14-1130,H05-1066,0,0.495423,"Missing"
P14-1130,W06-2932,0,0.0395933,"1.12 (Ma11) 94.02 (Zh13) 91.89 (Ma10) 90.32 (Ma13) 92.00 (Zh13) 86.19 (Ma13) 93.22 (Ma13) 92.41 (Ma13) 93.72 (Ma11) 93.03 (Ko10) 86.95 (Ma11) 87.96 (Zh13) 91.62 (Zh13) 77.55 (Ko10) 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter γ = 0.3. To remove the tensor in our model, we ran experiments with γ = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into fea"
P14-1130,W08-2121,0,0.22491,"Missing"
P14-1130,P10-1040,0,0.0448101,"dard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces (Collobert and Weston, 2008; Turian et al., 2010; Dhillon et al., 2011; Mikolov et al., 2013). Traditionally, these vector representations have been derived primarily from co-occurrences of words within sentences, ignoring syntactic roles of the co-occurring words. Nevertheless, any such word-level representation can be used to offset inherent sparsity problems associated with full lexicalization (Cirik and S¸ensoy, 2013). In this sense they perform a role similar to POS tags. Word-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vectors directly as featu"
P14-1130,C10-1093,0,0.0328904,"ly leverage word vector representations, in contrast to the baselines. 2 Related Work Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering (Lazaridou et al., 2013; Marton et al., 2010; Marton et al., 2011). While in most state-of-the-art parsers, features are selected manually (McDonald et al., 2005a; McDonald et al., 2005b; Koo and Collins, 2010; Martins et al., 2013; Zhang and McDonald, 2012a; Rush and Petrov, 2012a), automatic feature selection methods are gaining popularity (Martins et al., 2011b; Ballesteros and Nivre, 2012; Nilsson and Nugues, 2010; Ballesteros, 2013). Following standard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promising in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features. Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector"
P14-1130,W06-2933,0,0.0356237,"8.73 Best Published 81.12 (Ma11) 94.02 (Zh13) 91.89 (Ma10) 90.32 (Ma13) 92.00 (Zh13) 86.19 (Ma13) 93.22 (Ma13) 92.41 (Ma13) 93.72 (Ma11) 93.03 (Ko10) 86.95 (Ma11) 87.96 (Zh13) 91.62 (Zh13) 77.55 (Ko10) 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter γ = 0.3. To remove the tensor in our model, we ran experiments with γ = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as"
P14-1130,D12-1030,0,0.134983,"11) 87.96 (Zh13) 91.62 (Zh13) 77.55 (Ko10) 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter γ = 0.3. To remove the tensor in our model, we ran experiments with γ = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors φh and φm . For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We sh"
P14-1130,N12-1054,0,0.0677267,"1) 93.03 (Ko10) 86.95 (Ma11) 87.96 (Zh13) 91.62 (Zh13) 77.55 (Ko10) 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter γ = 0.3. To remove the tensor in our model, we ran experiments with γ = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors φh and φm . For each word in the sentence, we add its own word vector as well as the vectors of its"
P14-1130,D13-1093,0,0.0450642,"7.55 (Ko10) 89.43 Table 2: First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyperparameter γ = 0.3. To remove the tensor in our model, we ran experiments with γ = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013). and S¸ensoy, 2013), learned from raw data (Globerson et al., 2007; Maron et al., 2010). Three languages in our dataset – English, German and Swedish – have corresponding word vectors in this collection.5 The dimensionality of this representation varies by language: English has 50 dimensional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors φh and φm . For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We should note that since our m"
P14-1130,P14-1019,1,0.825297,"datasets include manually annotated dependency trees, POS tags and morphological information. Following standard practices, we encode this information as features. Methods We compare our model to MST and Turbo parsers on non-projective dependency parsing. For our parser, we train both a first-order parsing model (as described in Section 3 and 4) as well as a third-order model. The third order parser simply adds high-order features, those typically used in MST and Turbo parsers, into our sθ (x, y) = hθ, φ(x, y)i scoring component. The decoding algorithm for the third-order parsing is based on (Zhang et al., 2014). For the Turbo parser, we directly compare with the recent published results in (Martins et al., 2013). For the MST parser, we train and test using the most recent version of the code.4 In addition, we implemented two additional baselines, NT-1st (first order) and NT-3rd (third order), corresponding to our model without the tensor component. Features For the arc feature vector φh→m , we use the same set of feature templates as MST v0.5.1. For head/modifier vector φh and φm , we show the complete set of feature templates used by our model in Table 1. Finally, we use a similar set of feature te"
P14-1130,P13-1045,0,0.048834,"space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding nonsparse vectors directly as features, including their combinations, can significantly increase the number of active features for scoring syntactic structures (e.g., dependency arc). Because of this issue, Cirik and S¸ensoy (2013) used word vectors only as unigram features (without combinations) as part of a shift reduce parser (Nivre et al., 2007). The improvement on the overall parsing performance was marginal. Another application of word vectors is compositional vector grammar (Socher et al., 2013). While this method learns to map word combinations into vectors, it builds on existing word-level vector representations. In contrast, we represent words as vectors in a manner that is directly optimized for parsing. This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance. Dimensionality Reduction Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-tas"
P14-1130,E12-2012,0,\N,Missing
P15-1121,D14-1159,0,0.0614444,"Missing"
P15-1121,W04-2504,0,0.162781,"Missing"
P15-1121,P12-1007,0,0.0214314,"Missing"
P15-1121,P14-1048,0,0.0726329,"Missing"
P15-1121,D14-1070,0,0.0301932,"Missing"
P15-1121,P14-1092,0,0.0738272,"Missing"
P15-1121,D09-1036,0,0.10247,"Missing"
P15-1121,P14-5010,0,0.00362354,"Missing"
P15-1121,W12-1614,0,0.0508699,"Missing"
P15-1121,C08-2022,0,0.0188952,"Missing"
P15-1121,prasad-etal-2008-penn,0,0.10538,"Missing"
P15-1121,D13-1020,0,0.336452,"Missing"
P15-1121,R11-1063,0,0.101041,"Missing"
P15-1121,J05-2005,0,0.0170998,"Missing"
P19-1303,D11-1029,0,0.792362,"age model can partially compensate for the lack of explicit parallel supervision. Unfortunately, these methods cannot be applied to ancient texts due to the scarcity of available data. Decoding of Ancient Texts (Snyder et al., 2010) were the first to demonstrate the feasibility of automatic decipherment of a dead language using non-parallel data. The success of their approach can be attributed to cleverly designed Bayesian model that structurally incorporated powerful linguistic constraints. This includes customized priors for alphabet matching, incorporation of morphological structure, etc. (Berg-Kirkpatrick and Klein, 2011) proposed an alternative decipherment approach based on a relatively simple model paired with sophisticated inference algorithm. While their model performed well in a noise-free scenario when matching vocabularies only contain cognates, it has not been shown successful in a full decipherment scenario. Our approach outperforms these models in both scenarios. Moreover, we have demonstrated that the 3147 same architecture deciphers two distinct ancient languages Ugaritic and Linear B. The latter result is particularly important given that Linear B is a syllabic language. 3 Approach The main chall"
P19-1303,D13-1087,0,0.0329193,"Missing"
P19-1303,D18-1102,0,0.12339,"on cognate datasets used in previous work (BergKirkpatrick and Klein, 2013). 2 Related Work Decoding of Ciphered Texts Early work on decipherment was primarily focused on man-made ciphers, such as substitution ciphers. Most of these approaches are based on EM algorithms which are further adjusted for target decipherment scenarios. These adjustments are informed by assumptions about ciphers used to produce the data (Knight and Yamada, 1999; Knight et al., 2006; Ravi and Knight, 2011; Pourdamghani and Knight, 2017). Besides the commonly used EM algorithm, (Nuhn et al., 2013; Hauer et al., 2014; Kambhatla et al., 2018) also tackles substitution decipherment and formulate this problem as a heuristic search procedure, with guidance provided by an external language model (LM) for candidate rescoring. So far, techniques developed for man-made ciphers have not been shown successful in deciphering archaeological data. This can be attributed to the inherent complexity associated with processes behind language evolution of related languages. Nonparallel Machine Translation Advancements in distributed representations kindled exciting developments in this field, including translations at both the lexical and the sent"
P19-1303,P06-2065,0,0.190506,"Missing"
P19-1303,W99-0906,0,0.807995,"Missing"
P19-1303,D18-1062,0,0.0565101,"Missing"
P19-1303,D18-1063,0,0.138303,"logical data. This can be attributed to the inherent complexity associated with processes behind language evolution of related languages. Nonparallel Machine Translation Advancements in distributed representations kindled exciting developments in this field, including translations at both the lexical and the sentence level. Lexical translation is primarily formulated as alignment of monolingual embedding spaces into a crosslingual representation using adversarial training (Conneau et al., 2017), VAE (Dou et al., 2018), CCA (Haghighi et al., 2008; Faruqui and Dyer, 2014) or mutual information (Mukherjee et al., 2018). The constructed monolingual embedding spaces are usually of high quality due to the large amount of monolingual data available. The improved quality of distributed representations has similarly strong impact on non-parallel translation systems that operate at the sentence level (Pourdamghani and Knight, 2017). In that case, access to a powerful language model can partially compensate for the lack of explicit parallel supervision. Unfortunately, these methods cannot be applied to ancient texts due to the scarcity of available data. Decoding of Ancient Texts (Snyder et al., 2010) were the firs"
P19-1303,E14-1049,0,0.018086,"ot been shown successful in deciphering archaeological data. This can be attributed to the inherent complexity associated with processes behind language evolution of related languages. Nonparallel Machine Translation Advancements in distributed representations kindled exciting developments in this field, including translations at both the lexical and the sentence level. Lexical translation is primarily formulated as alignment of monolingual embedding spaces into a crosslingual representation using adversarial training (Conneau et al., 2017), VAE (Dou et al., 2018), CCA (Haghighi et al., 2008; Faruqui and Dyer, 2014) or mutual information (Mukherjee et al., 2018). The constructed monolingual embedding spaces are usually of high quality due to the large amount of monolingual data available. The improved quality of distributed representations has similarly strong impact on non-parallel translation systems that operate at the sentence level (Pourdamghani and Knight, 2017). In that case, access to a powerful language model can partially compensate for the lack of explicit parallel supervision. Unfortunately, these methods cannot be applied to ancient texts due to the scarcity of available data. Decoding of An"
P19-1303,N18-1032,0,0.0320717,"ions linking weighted sum of input embeddings and softmax. Inputs to the encoder and decoder are the lost and known languages respectively. See Sec. 3.2 for details. characters in the lost and the known languages, respectively. Embedding matrices for both languages are computed by (Linear B) (Greek) Ex = Wx U, Ey = Wy U. This formulation reflects the principle underlying crosslingual embeddings such as MUSE (Conneau et al., 2017). Along a similar line, previous work has demonstrated the effectiveness of using universal word embeddings, in the context of lowresource neural machine translation (Gu et al., 2018). Residual connection Character alignment is mostly local in nature, but this fact is not reflected by how the next character is predicted by the model. Specifically, the prediction is made ˜ which is a nonlinbased on the context vector h, ear function of the hidden states of the encoder and ˜ captures a much wider the decoder. As a result, h context due to the nature of a recurrent neural network. To address this issue and directly improve the quality of character alignment, we add a residual connection from the encoder embedding layer to the decoder projection layer. Specifically, letting α"
P19-1303,P08-1088,0,0.0317591,"man-made ciphers have not been shown successful in deciphering archaeological data. This can be attributed to the inherent complexity associated with processes behind language evolution of related languages. Nonparallel Machine Translation Advancements in distributed representations kindled exciting developments in this field, including translations at both the lexical and the sentence level. Lexical translation is primarily formulated as alignment of monolingual embedding spaces into a crosslingual representation using adversarial training (Conneau et al., 2017), VAE (Dou et al., 2018), CCA (Haghighi et al., 2008; Faruqui and Dyer, 2014) or mutual information (Mukherjee et al., 2018). The constructed monolingual embedding spaces are usually of high quality due to the large amount of monolingual data available. The improved quality of distributed representations has similarly strong impact on non-parallel translation systems that operate at the sentence level (Pourdamghani and Knight, 2017). In that case, access to a powerful language model can partially compensate for the lack of explicit parallel supervision. Unfortunately, these methods cannot be applied to ancient texts due to the scarcity of avail"
P19-1303,P10-1105,0,0.0255032,"tialize the parameters of the neural model and reset the state of the optimizer after each iteration. Empirically, we found that our neural network can easily converge to a suboptimal local minimum given a poor global word-level alignment. Resetting the model parameters periodically helps with limiting the negative effect caused by such alignments. 5 Experiments Datasets We evaluate our system on the following datasets: • ROMANCE: Cognate detection between three Romance languages. It contains phonetic transcriptions of cognates in Italian, Spanish and Portuguese. This dataset has been used by Hall and Klein (2010) and Berg-Kirkpatrick and Klein (2011). Data statistics are summarized in Table 1. 2 For instance, k, K and T encode “ka”, “ke” and “te”, respectively. 3 https://archive.org/details/ LinearBLexicon/page/n5 3151 Dataset UGARITIC Linear B Linear B/names ROMANCE #Cognates #Tokens (lost/known) #Symbols (lost/known) 2214 919 455 583 7353/41263 919/919 919/455 583/583 30/23 70/28 70/28 25/31/28 (Es/It/Pt) Table 1: Statistics of datasets used in our experiments. Systems We report numbers for the following systems: • Bayesian: the Bayesian model by Snyder et al. (2010) that automatically deciphered Ug"
P19-1303,C14-1218,0,0.0151992,"uperior performance on cognate datasets used in previous work (BergKirkpatrick and Klein, 2013). 2 Related Work Decoding of Ciphered Texts Early work on decipherment was primarily focused on man-made ciphers, such as substitution ciphers. Most of these approaches are based on EM algorithms which are further adjusted for target decipherment scenarios. These adjustments are informed by assumptions about ciphers used to produce the data (Knight and Yamada, 1999; Knight et al., 2006; Ravi and Knight, 2011; Pourdamghani and Knight, 2017). Besides the commonly used EM algorithm, (Nuhn et al., 2013; Hauer et al., 2014; Kambhatla et al., 2018) also tackles substitution decipherment and formulate this problem as a heuristic search procedure, with guidance provided by an external language model (LM) for candidate rescoring. So far, techniques developed for man-made ciphers have not been shown successful in deciphering archaeological data. This can be attributed to the inherent complexity associated with processes behind language evolution of related languages. Nonparallel Machine Translation Advancements in distributed representations kindled exciting developments in this field, including translations at both"
P19-1303,N18-1031,0,0.0276269,"As a result, h context due to the nature of a recurrent neural network. To address this issue and directly improve the quality of character alignment, we add a residual connection from the encoder embedding layer to the decoder projection layer. Specifically, letting α be the predicted attention weights, we compute X c= αi Ex (i), i ˆ = c ⊕ h, ˜ h ✔ ✖ ✔ ✔ ✔ ✖ Figure 2: An example of alignment between a Linear B word and Greek word. 4 and 6 denote correct and wrong alignment positions respectively. The misalignment between E and ν incurs a deletion error; 1 and ζ incurs an insertion error. by Nguyen and Chiang (2018) to refine the quality of lexical translations in NMT. Monotonic alignment regularization We design a regularization term that guides the model towards monotonic rewriting. Specifically, we penalizes the model whenever insertions or deletions occur. More concretely, for each word in the lost language xi , we first compute the alignment probability Pr(ati |xi ) over the input sequence at decoder time step t, predicted by the attention mechanism. Then we compute the expected alignment position as X pti = k · Pr(ati = k|xi ), k (3) where Ex (i) is the encoder character embedding at position i, an"
P19-1303,P13-1154,0,0.015438,"he model achieves superior performance on cognate datasets used in previous work (BergKirkpatrick and Klein, 2013). 2 Related Work Decoding of Ciphered Texts Early work on decipherment was primarily focused on man-made ciphers, such as substitution ciphers. Most of these approaches are based on EM algorithms which are further adjusted for target decipherment scenarios. These adjustments are informed by assumptions about ciphers used to produce the data (Knight and Yamada, 1999; Knight et al., 2006; Ravi and Knight, 2011; Pourdamghani and Knight, 2017). Besides the commonly used EM algorithm, (Nuhn et al., 2013; Hauer et al., 2014; Kambhatla et al., 2018) also tackles substitution decipherment and formulate this problem as a heuristic search procedure, with guidance provided by an external language model (LM) for candidate rescoring. So far, techniques developed for man-made ciphers have not been shown successful in deciphering archaeological data. This can be attributed to the inherent complexity associated with processes behind language evolution of related languages. Nonparallel Machine Translation Advancements in distributed representations kindled exciting developments in this field, including"
P19-1303,D17-1266,0,0.33495,"equivalents in the decipherment scenario. Finally, we demonstrate that the model achieves superior performance on cognate datasets used in previous work (BergKirkpatrick and Klein, 2013). 2 Related Work Decoding of Ciphered Texts Early work on decipherment was primarily focused on man-made ciphers, such as substitution ciphers. Most of these approaches are based on EM algorithms which are further adjusted for target decipherment scenarios. These adjustments are informed by assumptions about ciphers used to produce the data (Knight and Yamada, 1999; Knight et al., 2006; Ravi and Knight, 2011; Pourdamghani and Knight, 2017). Besides the commonly used EM algorithm, (Nuhn et al., 2013; Hauer et al., 2014; Kambhatla et al., 2018) also tackles substitution decipherment and formulate this problem as a heuristic search procedure, with guidance provided by an external language model (LM) for candidate rescoring. So far, techniques developed for man-made ciphers have not been shown successful in deciphering archaeological data. This can be attributed to the inherent complexity associated with processes behind language evolution of related languages. Nonparallel Machine Translation Advancements in distributed representat"
P19-1303,P11-1002,0,0.0546861,"gnates into their Greek equivalents in the decipherment scenario. Finally, we demonstrate that the model achieves superior performance on cognate datasets used in previous work (BergKirkpatrick and Klein, 2013). 2 Related Work Decoding of Ciphered Texts Early work on decipherment was primarily focused on man-made ciphers, such as substitution ciphers. Most of these approaches are based on EM algorithms which are further adjusted for target decipherment scenarios. These adjustments are informed by assumptions about ciphers used to produce the data (Knight and Yamada, 1999; Knight et al., 2006; Ravi and Knight, 2011; Pourdamghani and Knight, 2017). Besides the commonly used EM algorithm, (Nuhn et al., 2013; Hauer et al., 2014; Kambhatla et al., 2018) also tackles substitution decipherment and formulate this problem as a heuristic search procedure, with guidance provided by an external language model (LM) for candidate rescoring. So far, techniques developed for man-made ciphers have not been shown successful in deciphering archaeological data. This can be attributed to the inherent complexity associated with processes behind language evolution of related languages. Nonparallel Machine Translation Advance"
P19-1303,P16-1159,0,0.0268356,"3) where Ex (i) is the encoder character embedding at position i, and c is the weighted character emˆ is subsequently used to predict the next bedding. h character. A similar strategy has also been adopted κ ν ω σ ο ς where k is any potential aligned position. The regularization term is subsequently defined as X Ω1 ({pti }) = (pti − pt−1 − 1)2 . (4) i 3149 t • fi,j : edges from xi to yj . y1 x1 y2 x2 ... S ... ... xN T d¯i,j = Ey∼Pr(y|xi ) d(y, yj ), yM where d(·, ·) is the edit distance function, and Pr(y|xi ) is given by the neural decipherment model. We use a sampling procedure proposed by Shen et al. (2016) to compute this expected distance. To provide a reasonable coverage of the cognatePpairs, we further specify the demand constraint j fj,t = D with a given hyperparameter D. We note that the edit distance cost plays an essential role of complementing the neural model. Specifically, neural seq2seq models are notoriously inadequate at capturing insertions and deletions, contributing to many issues of overgeneration or undergeneration in NMT (Tu et al., 2016). These problems are only accentuated due to a lack of supervision. Using edit distance in the flow setup helps alleviate this issue, since"
P19-1303,P10-1107,1,0.887714,"ion of neural methods that dominate modern machine translation. Even for human experts this translation scenario proved to be onerous: a typical decipherment spans over decades and requires encyclopedic domain knowledge, prohibitive manual effort and sheer luck (Robinson, 2002). Moreover, techniques applied for the decipherment of one lost language are rarely reusable for another language. As a result, every significant human decipherment is considered to be one of a kind, “the rarest category of achievement” (Pope, 1975). Prior work has demonstrated the feasibility of automatic decipherment. Snyder et al. (2010) 1 Code and all datasets are hosted in https:// github.com/j-luo93/NeuroDecipher. Regina Barzilay CSAIL, MIT regina@csail.mit.edu translated the ancient Semitic language Ugaritic into Hebrew. Since both languages are derived from the same proto-Semitic origin, the translation involved matching their alphabets at the character level and mapping cognates at the word level. The effectiveness of their approach stemmed from its ability to incorporate expansive linguistic knowledge, including expected morphological correspondences, the nature of alphabet-level alignment, etc. As with human decipherm"
P19-1303,P16-1008,0,0.0203895,"e d(·, ·) is the edit distance function, and Pr(y|xi ) is given by the neural decipherment model. We use a sampling procedure proposed by Shen et al. (2016) to compute this expected distance. To provide a reasonable coverage of the cognatePpairs, we further specify the demand constraint j fj,t = D with a given hyperparameter D. We note that the edit distance cost plays an essential role of complementing the neural model. Specifically, neural seq2seq models are notoriously inadequate at capturing insertions and deletions, contributing to many issues of overgeneration or undergeneration in NMT (Tu et al., 2016). These problems are only accentuated due to a lack of supervision. Using edit distance in the flow setup helps alleviate this issue, since a misstep of insertion or deletion by the neural model will still generate a string that resembles the ground truth in terms of edit distance. In other words, the edit distance based flow can still recover from the mistakes the neural model makes. Figure 3: Minimum-cost flow. S, T stands for source and sink respectively; xi , yj are the ith and j th word in X and Y. Each edge is associated with a flow fij and cost d¯ij . See Sec. 3.3 for details. Note that"
P99-1071,W97-0703,1,0.18292,"document summarization systems, then we will detail our sentence comparison technique, and describe the sentence generation component. We provide examples of generated summaries and conclude with a discussion of evaluation. 2 Related Work Automatic summarizers typically identify and extract the most important sentences from an input article. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Salton et al., 1991), symbolic techniques based on discourse structure (Marcu, 1997), and semantic relations between words (Barzilay and Elhadad, 1997). Extraction techniques can work only if summary sentences already appear in the article. Extraction cannot handle the task we address, because summarization of multiple documents requires information about similarities and differences across articles. While most of the summarization work has focused on single articles, a few initial projects have started to study multi-document summarization documents. In constrained domains, e.g., terrorism, a coherent summary of several articles can be generated, when a detailed semantic representation of the source text is available. For example, informati"
P99-1071,P96-1025,0,0.0195348,"opositions from an underlying knowledge base to form text content. A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: Content p l a n n i n g o p e r a t e s o v e r full sentences, producing s e n t e n c e fragm e n t s . Thus, content planning straddles the border between interpretation and generation. We preprocess the similar sentences using an existing shallow parser (Collins, 1996) and a mapping to predicateargument structure. The content planner finds an intersection of phrases by comparing the predicate-argument structures; through this process it selects the phrases that can adequately convey the common information of the theme. It also orders selected phrases and augments t h e m with 550 On 3th of September 1995, 120 hostages were released by Bosnian Serbs. Serbs were holding over 250 U.N. personnel. Bosnian serb leader Radovan Karadjic said he expected ""a sign of goodwill"" from the international community. U.S. F-16 fighter jet was shot down by Bosnian ! Serbs. El"
P99-1071,P97-1004,0,0.00607779,"Missing"
P99-1071,A97-1042,0,0.0830727,"fusion of similar information across multiple documents using language generation to produce a concise summary. We propose a m e t h o d for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for s u m m a r y generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while exMichael Elhadad Dept. of C o m p u t e r Science B e n - G u r i o n University Beer-Sheva, Israel tracting s o m e similar sentences could produce a s u m m a r y biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that shou"
P99-1071,W97-0713,0,0.158286,"nformation across multiple documents using language generation to produce a concise summary. We propose a m e t h o d for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for s u m m a r y generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while exMichael Elhadad Dept. of C o m p u t e r Science B e n - G u r i o n University Beer-Sheva, Israel tracting s o m e similar sentences could produce a s u m m a r y biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be include"
P99-1071,J98-3005,1,0.606113,"tic way. However, some of the rules can only be approximated to a certain degree. For example, identification of similarity based on semantic relations between words depends on the coverage of the thesaurus. We 553 identify word similarity using synonym relations from WordNet. Currently, paraphrasing using part of speech transformations is not supported by the system. All other paraphrase classes we identified are implemented in our algorithm for theme intersection. 3.3 T e m p o r a l O r d e r i n g A property that is unique to multi-document summarization is the effect of time perspective (Radev and McKeown, 1998). When reading an original text, it is possible to retrieve the correct temporal sequence of events which is usually available explicitly. However, when we p u t pieces of text from different sources together, we must provide the correct time perspective to the reader, including the order of events, the temporal distance between events and correct temporal references. In single-document summarization, one of the possible orderings of the extracted information is provided by the input document itself. However, in the case of multiple-document summarization, some events may not be described in t"
Q14-1043,P08-1037,0,0.501726,"ed on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995; Olteanu and Moldovan, 2005; ˇ Suster, 2012). In addition, the problem of PP attachment has also been addressed in the context of full parsing (Atterer and Sch¨utze, 2007; Agirre et al., 2008). For instance, Green (2009) engineered statesplit features for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, re"
Q14-1043,J07-4002,0,0.314307,"Missing"
Q14-1043,P14-2131,0,0.167967,"illustrates the resulting enriched vector. Similar dimensions are appended to vectors representing other words participating in the compositions. Our experiments show that such an extension significantly improves performance. 4.3 Syntactic word vectors In the standard Skip-gram model word vectors are trained from raw text using the linear context of neighboring words. We also consider an alternative method for creating word vectors by using the syntactic context of words. Such syntactic context is expected to be relevant for resolving PP attachments. Given a dependency-parsed text, we follow Bansal et al. (2014) and create a new corpus of tuples (l, g, p, c, l), for every word c, its parent p with dependency label l, and its grandparent g. Then we train an ordinary Skip-gram model on this corpus, but with a small window size of 2. Note that the label l appears on both ends so it contributes to the context of the word as well as its grandparent. We find that syntactic vectors yield significant performance gains compared to standard vectors.5 5 Experimental setup 5.1 Extracting PP attachments Instances of PP attachment decisions are extracted from standard treebanks. We use the CATiB dependency treeban"
Q14-1043,C94-2195,0,0.862621,"Missing"
Q14-1043,W95-0103,0,0.486292,"ation. 6.1 Alternative composition architectures In this section we analyze how different composition architectures (Section 3.2) contribute to the overall performance. To isolate the contribution of the architecture, we focus on standard (linear) word vectors, with no relearning or enriching. As Figure 3 shows, simpler models tend to perform worse than more complex ones. The best variants use different composition matrices based on the distance of the candidate head from the PP (HPCD, HPCDN). While the results shown are for 100-dimensional 14 Here we applied basic preprocessing similarly to (Collins and Brooks, 1995), converting 4-digit numbers to YEAR and other numbers to NUMBER; other tokens were lower-cased. 569 Figure 3: PP attachment accuracy of different architectures. (HC) uses only the candidate head and the child of the preposition; (HPC*) models use head, preposition, and child, with the following variants: (HPCT) ternary composition; (HPCL) local matrices for top and bottom compositions; (HPCN) context words; (HPCD) distance-dependent matrices; (HPCDN) combines HPCD+HPCN. vectors, similar trends are observed with lower dimensions, although the gaps between simple and complex models are then mor"
Q14-1043,P97-1003,0,0.237027,"Missing"
Q14-1043,P05-1071,0,0.056325,"dia for English8 and the arTenTen corpus for Arabic, containing web texts crawled in 2012 (Belinkov et al., 2013; Arts et al., 2014). Table 3 similar performance gains. 6 We used the Pennconverter tool: http://nlp.cs. lth.se/software/treebank-converter. 7 We used the word2vec tool: https://code.google. com/p/word2vec, with default settings. We experimented with word vectors of 25, 50, 100, and 200 dimensions, and found 100 to work best in most cases. 8 http://mattmahoney.net/dc/textdata. shows the comparable sizes of the datasets. The Arabic corpus has been tokenized and lemmatized with MADA (Habash and Rambow, 2005; Habash et al., 2005), a necessary procedure in order to separate some prepositions from their child words. In addition, lemmatization reduces vocabulary size and facilitates sharing information between different morphological variants that have the same meaning. For syntactic word vectors, we use the English vectors in (Bansal et al., 2014), which were trained from a parsed BLLIP corpus (minus PTB). For Arabic, we first convert the morphologically-processed arTenTen corpus to CoNLL format with the SPMRL shared-task scripts (Seddah et al., 2013). Then we parse the corpus with a baseline MST p"
Q14-1043,P09-2056,0,0.021064,"d create a new corpus of tuples (l, g, p, c, l), for every word c, its parent p with dependency label l, and its grandparent g. Then we train an ordinary Skip-gram model on this corpus, but with a small window size of 2. Note that the label l appears on both ends so it contributes to the context of the word as well as its grandparent. We find that syntactic vectors yield significant performance gains compared to standard vectors.5 5 Experimental setup 5.1 Extracting PP attachments Instances of PP attachment decisions are extracted from standard treebanks. We use the CATiB dependency treebank (Habash and Roth, 2009) for Arabic and a conversion of the Penn treebank (PTB) to dependency format for English.6 Standard train/dev/test splits are used: sections 2-21/22/23 of the PTB for English, and the split from the SPRML shared-task for Arabic (Seddah et al., 2013). As Table 2 shows, the datasets of the two languages are fairly similar in size, except for the much larger set of prepositions in the English data. Extracting instances of PP attachments from the treebanks is done in the following way. For each 5 We also experimented with another method for creating syntactic vectors by Levy and Goldberg (2014) an"
Q14-1043,P08-1068,0,0.518307,"train a new RNN model. 11 We use SVMRank: http://www.cs.cornell. edu/people/tj/svm_light/svm_rank.html. of the preposition. This feature was found useful ˇ in previous work on PP attachment (Suster, 2012). While this limits the contribution of the word vectors to the learned model to one dimension, attempts to use more dimensions in the SVM were unsuccessful.12 In contrast, the compositional models better capture the full dimensionality of the word vectors. A second type of features induced from raw data that we consider are Brown clusters, which were found to be useful in dependency parsing (Koo et al., 2008). Compared to distributed vectors, Brown clusters provide a more discrete representation that is easier to incorporate in the SVM. We create clusters from our unsupervised corpora using the Liang (2005) implementation of Brown’s algorithm, and add features in the spirit of (Koo et al., 2008). Specifically, we add full and prefixed bit strings for the head, preposition, and child, as well as bi-lexical versions for head-child pairs.13 Table 4 shows a summary of the SVM features. 6 Results Table 5 summarizes the results of our model and other systems. Our best results are obtained with the Head-"
Q14-1043,D12-1096,0,0.102964,"hows a summary of the SVM features. 6 Results Table 5 summarizes the results of our model and other systems. Our best results are obtained with the Head-Prep-Child-Dist (HPCD) model using syntactic vectors, enriching, and relearning. The full model outperforms both full-scale parsers and a dedicated SVM model. More advanced parsers do demonstrate higher accuracy on the PP attachment task, but our method outperforms them as well. Note that the self-trained reranking parser (Charniak-RS) performs especially well and quite better than the RNN parser. This trend is consistent with the results in (Kummerfeld et al., 2012; Socher et al., 2013). Our compositional architecture is effective in exploiting raw data: using only standard word vectors with no enriching, our HPCD (basic) model performs comparably to an SVM with access to all enriching features. Once we improve the representation, we outperform both the SVM and full parsers. In comparison, the contribution of raw data to the SVM, as either word vectors or Brown clusters, is rather limited. 12 For example, we tried adding all word vector dimensions as features, as well as element-wise products of the vectors representing the head and the child. 13 As in"
Q14-1043,P14-1130,1,0.926222,"ll as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9 We use gold POS tags in all systems and experiments. The only exception is the RNN parser, for which we use the built-in Englis"
Q14-1043,P14-2050,0,0.0442242,"ank (Habash and Roth, 2009) for Arabic and a conversion of the Penn treebank (PTB) to dependency format for English.6 Standard train/dev/test splits are used: sections 2-21/22/23 of the PTB for English, and the split from the SPRML shared-task for Arabic (Seddah et al., 2013). As Table 2 shows, the datasets of the two languages are fairly similar in size, except for the much larger set of prepositions in the English data. Extracting instances of PP attachments from the treebanks is done in the following way. For each 5 We also experimented with another method for creating syntactic vectors by Levy and Goldberg (2014) and observed 566 Total Candidates Vocab sizes All Heads Preps Children Arabic Train Test 42,387 3,917 4.5 4.3 English Train Test 35,359 1,951 3.7 3.6 8,230 8,225 13 4,222 11,429 10,395 72 5,504 2,944 2,936 10 1,424 2,440 2,133 46 983 Table 2: Statistics of extracted PP attachments, showing total sizes, average number of candidate heads, and vocabulary sizes. Corpus Tokens Types Arabic arTenTen 130M 43K English Wikipedia BLLIP 120M 43M 218K 317K Table 3: Statistics of Arabic and English corpora used for creating word vectors. preposition, we look for all possible candidate heads in a fixed pre"
Q14-1043,D10-1004,0,0.0154772,", we use part-of-speech information9 from the treebanks as well as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9 We use gold POS tags in all systems and experiments. The only excepti"
Q14-1043,P13-2109,0,0.026728,"h information9 from the treebanks as well as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9 We use gold POS tags in all systems and experiments. The only exception is the RNN parser, f"
Q14-1043,N06-1020,0,0.162558,"that have restricted access to word semantics. These considerations have motivated recent explorations in using distributed word representations for syntactic parsing (Cirik and S¸ensoy, 2013; Socher et al., 2013; Lei et al., 2014). Lowdimensional word embeddings help unveil semantic similarity between words, thereby alleviating the data sparsity problem associated with PP attachment. In this context, large amounts of raw data used to construct embeddings effectively enrich limited syntactic annotations. While these approaches show initial promise, they still lag behind self-trained parsers (McClosky et al., 2006). These parsers also utilize raw data but in a different way: self-trained parsers use it to get additional (noisy) annotations, without computing new word representations. These results suggest that embedding-based representations have not yet been utilized to their full potential. We show that embedding-based representations can indeed significantly improve PP attachment accuracy. We achieve this by using such representations within a compositional neural network architecture. The representations are initially learned from an unlabeled corpus, but are then further discriminatively trained to"
Q14-1043,P05-1012,0,0.0611958,"Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9 We use gold POS tags in all systems and experiments. The only exception is the RNN parser, for which we use the built-in English model in Stanford parser’s (version 3.4); 10 567 Source Treeban"
Q14-1043,mousser-2010-large,0,0.0247724,"sh vectors in (Bansal et al., 2014), which were trained from a parsed BLLIP corpus (minus PTB). For Arabic, we first convert the morphologically-processed arTenTen corpus to CoNLL format with the SPMRL shared-task scripts (Seddah et al., 2013). Then we parse the corpus with a baseline MST parser (Section 5.3) and create syntactic word vectors as described in Section 4.3. The Arabic syntactic vectors will be made available to the research community. For enriching word vectors, we use part-of-speech information9 from the treebanks as well as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al"
Q14-1043,nivre-etal-2006-maltparser,0,0.0680035,"l., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9 We use gold POS tags in all systems and experiments. The only exception is the RNN parser, for which we use the built-in English model in Stanford parser’s (version 3.4); 10 567 Source Treebank WordNet VerbNet Word Vectors Brown Clus"
Q14-1043,H05-1035,0,0.48423,"Our results 562 demonstrate that relearning the embeddings contributes to the model performance, across a range of configurations. We also notice that representations based on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995; Olteanu and Moldovan, 2005; ˇ Suster, 2012). In addition, the problem of PP attachment has also been addressed in the context of full parsing (Atterer and Sch¨utze, 2007; Agirre et al., 2008). For instance, Green (2009) engineered statesplit features for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources"
Q14-1043,H94-1048,0,0.859503,"Missing"
Q14-1043,rodriguez-etal-2008-arabic,0,0.0351991,"Missing"
Q14-1043,W13-4917,0,0.0479966,"Missing"
Q14-1043,P13-1045,0,0.68023,"ing WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from ˇ which a cosine similarity score is calculated (Suster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2010). In particular, Socher et al. (2013) obtain good parsing performance by building compositional representations from word vectors. However, to combat the computational complexity of the full parsing scenario, they rely on a probabilistic context-free grammar to prune search space. In contrast, focusing on PP attachment allows us to consider various neural network architectures that are more appropriate for this task, including ternary, binary, and distancedependent compositions. Furthermore, we investigate modifications to the original word vectors in several important directions: enriching word vectors with semantic and syntacti"
Q14-1043,W97-0109,0,0.461899,"s for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, researchers have explored using manually crafted resources (Stetina and Nagao, 1997; Gamallo et al., 2003; Olteanu and Moldovan, 2005; Medimi and Bhattacharyya, 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from ˇ which a cosine similarity score is calculated (Suster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our"
Q14-1043,P10-1040,0,0.0873257,"l learning rate η = 1.0 (Dyer, n.d.), and minibatch size of 500. Learned parameters are initialized similarly to previous work (Bengio and Glorot, 2010; Socher et al., 2013): composition matrices are set to W = 0.5[I I] + , where  ∼ U (− n1 , n1 ); bias terms b are set to zero; and the weight vector is set to w ∼ U (− √1n , √1n ). 4 Word vector representations Our approach assumes a vector representation for each word. Such representations have gained popularity in recent years, due to the ability to train them from large unlabeled datasets, and their ease of use in a wide variety of tasks (Turian et al., 2010). There are various approaches to training vector representations (Collobert and Weston, 2008; Bengio et al., 2009). Here we chose to focus on the Skip-gram method recently proposed by Mikolov et al. (2013a). The Skip-gram model maximizes the average log-probability of every word generating its context, which is modeled via a neural net architecture, but without the non-linearity. To improve efficiency, this probability is approximated by a hierarchical softmax (Mikolov et al., 2013b) with vocabulary words represented in a binary Huffman tree.3 In the simplest variant of our method, we train t"
Q14-1043,C02-1004,0,0.164705,"state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, researchers have explored using manually crafted resources (Stetina and Nagao, 1997; Gamallo et al., 2003; Olteanu and Moldovan, 2005; Medimi and Bhattacharyya, 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from ˇ which a cosine similarity score is calculated (Suster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2010). In particular, Socher et al. (2013) obtain good parsing performance by building compositional representations from word vectors. However, to combat the computational complexity of the"
Q14-1043,W13-4909,0,\N,Missing
Q15-1012,W02-0606,0,0.652473,"related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009). In these approaches, words are commonly modeled as concatenations of morphemes. 1 Code is available at https://github.com/ karthikncode/MorphoChain. This morpheme-centric view is well-suited for uncovering distributional properties of stems and affixes. But it is not well-equipped to capture semantic relatedness at the word level. In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002). Given two candidate words, the proximity is assessed using standard word-distributional measures such as mutual information. However, the fact that these models do not model morphemes directly greatly limits their performance. In this paper, we propose a model to integrate orthographic and semantic views. Our goal is to build a chain of derivations for a current word from its base form. For instance, given a word playfully, the corresponding chain is play → playful → playfully. The word play is a base form of this derivation as it cannot be reduced any further. Individual derivations are obt"
Q15-1012,D11-1057,0,0.140679,"of words (rather than morphemes) enables us to incorporate semantic and word-level features. Most recently, work by Sirts and Goldwater (2013) uses Adaptor Grammars for minimally supervised segmentation. By defining a morphological grammar consisting of zero or more prefixes, stems and suffixes, they induce segmentations over words in both unsupervised and semi-supervised settings. While their model (AGMorph) builds up a word by combining morphemes in the form of a parse tree, we operate at the word level and build up the final word via intermediate words in the chain. In other related work, Dreyer and Eisner (2011) tackle the problem of recovering morphological paradigms and inflectional principles. They use a Bayesian generative model with a log-linear framework, using expressive features, over pairs of strings. Their work, however, handles a different task from ours and requires a small amount of annotated data to seed the model. In this work, we make use of semantic information to help morphological analysis. Lee et al. (2011) present a model that takes advantage of syntactic context to perform better morphological segmentation. Stallard et al. (2012) improve on this approach using the technique of M"
Q15-1012,W04-0105,0,0.852943,"s or outperforms five state-of-the-art systems on Arabic, English and Turkish.1 1 Introduction Morphologically related words exhibit connections at multiple levels, ranging from orthographical patterns to semantic proximity. For instance, the words playing and played share the same stem, but also carry similar meaning. Ideally, all these complementary sources of information would be taken into account when learning morphological structures. Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009). In these approaches, words are commonly modeled as concatenations of morphemes. 1 Code is available at https://github.com/ karthikncode/MorphoChain. This morpheme-centric view is well-suited for uncovering distributional properties of stems and affixes. But it is not well-equipped to capture semantic relatedness at the word level. In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002). Given two candidate"
Q15-1012,W11-0301,1,0.956445,"by combining morphemes in the form of a parse tree, we operate at the word level and build up the final word via intermediate words in the chain. In other related work, Dreyer and Eisner (2011) tackle the problem of recovering morphological paradigms and inflectional principles. They use a Bayesian generative model with a log-linear framework, using expressive features, over pairs of strings. Their work, however, handles a different task from ours and requires a small amount of annotated data to seed the model. In this work, we make use of semantic information to help morphological analysis. Lee et al. (2011) present a model that takes advantage of syntactic context to perform better morphological segmentation. Stallard et al. (2012) improve on this approach using the technique of Maximum Marginal decoding to reduce noise. Their best system considers entire sentences, while our approach (and the morphological analyzers described above) operates at the vocabulary level without regarding sentence context. Hence, though their work is not directly comparable to ours, it presents an interesting orthogonal view to the problem. 3 Model 3.1 Definitions and Framework We use morphological chains to model wo"
Q15-1012,W13-3512,0,0.144562,"Missing"
Q15-1012,N09-1024,0,0.640293,"1 Introduction Morphologically related words exhibit connections at multiple levels, ranging from orthographical patterns to semantic proximity. For instance, the words playing and played share the same stem, but also carry similar meaning. Ideally, all these complementary sources of information would be taken into account when learning morphological structures. Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009). In these approaches, words are commonly modeled as concatenations of morphemes. 1 Code is available at https://github.com/ karthikncode/MorphoChain. This morpheme-centric view is well-suited for uncovering distributional properties of stems and affixes. But it is not well-equipped to capture semantic relatedness at the word level. In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002). Given two candidate words, the proximity is assessed using standard word-distributional mea"
Q15-1012,W00-0712,0,0.738284,"atterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009). In these approaches, words are commonly modeled as concatenations of morphemes. 1 Code is available at https://github.com/ karthikncode/MorphoChain. This morpheme-centric view is well-suited for uncovering distributional properties of stems and affixes. But it is not well-equipped to capture semantic relatedness at the word level. In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002). Given two candidate words, the proximity is assessed using standard word-distributional measures such as mutual information. However, the fact that these models do not model morphemes directly greatly limits their performance. In this paper, we propose a model to integrate orthographic and semantic views. Our goal is to build a chain of derivations for a current word from its base form. For instance, given a word playfully, the corresponding chain is play → playful → playfully. The word play is a base form of this derivation as it cannot be reduced any further. Individu"
Q15-1012,Q13-1021,0,0.625923,"entation sets is huge, learning and inference are quite involved. They use techniques like Contrastive Estimation, sampling and simulated annealing. In contrast, our formulation does not result in such a large search space. For each word, the number of parent candidates is bounded by its length multiplied by the number of possible transformations. Therefore, Contrastive Estimation can be implemented via enumeration, and does not require sampling. Moreover, operating at the level of words (rather than morphemes) enables us to incorporate semantic and word-level features. Most recently, work by Sirts and Goldwater (2013) uses Adaptor Grammars for minimally supervised segmentation. By defining a morphological grammar consisting of zero or more prefixes, stems and suffixes, they induce segmentations over words in both unsupervised and semi-supervised settings. While their model (AGMorph) builds up a word by combining morphemes in the form of a parse tree, we operate at the word level and build up the final word via intermediate words in the chain. In other related work, Dreyer and Eisner (2011) tackle the problem of recovering morphological paradigms and inflectional principles. They use a Bayesian generative m"
Q15-1012,P05-1044,0,0.531842,"een two words using the corresponding vector embeddings. At the orthographic level, features capture whether 157 Transactions of the Association for Computational Linguistics, vol. 3, pp. 157–167, 2015. Action Editor: Yuji Matsumoto. Submission batch: 9/2014; Revision batch: 12/2014; Revision batch 2/2015; Published 3/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. the words in the chain actually occur in the corpus, how affixes are reused, as well as how the words are altered during the addition of morphemes. We use Contrastive Estimation (Smith and Eisner, 2005) to efficiently learn this model in an unsupervised manner. Specifically, we require that each word has greater support among its bounded set of candidate parents than an artificially constructed neighboring word would. We evaluate our model on datasets in three languages: Arabic, English and Turkish. We compare our performance against five state-of-the-art unsupervised systems: Morfessor Baseline (Virpioja et al., 2013), Morfessor CatMAP (Creutz and Lagus, 2005), AGMorph (Sirts and Goldwater, 2013), the Lee Segmenter (Lee et al., 2011; Stallard et al., 2012) and the system of Poon et al. (200"
Q15-1012,P08-1084,1,0.898981,"bic, English and Turkish.1 1 Introduction Morphologically related words exhibit connections at multiple levels, ranging from orthographical patterns to semantic proximity. For instance, the words playing and played share the same stem, but also carry similar meaning. Ideally, all these complementary sources of information would be taken into account when learning morphological structures. Most state-of-the-art unsupervised approaches to morphological analysis are built primarily around orthographic patterns in morphologically-related words (Goldwater and Johnson, 2004; Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Poon et al., 2009). In these approaches, words are commonly modeled as concatenations of morphemes. 1 Code is available at https://github.com/ karthikncode/MorphoChain. This morpheme-centric view is well-suited for uncovering distributional properties of stems and affixes. But it is not well-equipped to capture semantic relatedness at the word level. In contrast, earlier approaches that capture semantic similarity in morphological variants operate solely at the word level (Schone and Jurafsky, 2000; Baroni et al., 2002). Given two candidate words, the proximity is assessed using standard wor"
Q15-1012,P12-2063,1,0.93129,"iate words in the chain. In other related work, Dreyer and Eisner (2011) tackle the problem of recovering morphological paradigms and inflectional principles. They use a Bayesian generative model with a log-linear framework, using expressive features, over pairs of strings. Their work, however, handles a different task from ours and requires a small amount of annotated data to seed the model. In this work, we make use of semantic information to help morphological analysis. Lee et al. (2011) present a model that takes advantage of syntactic context to perform better morphological segmentation. Stallard et al. (2012) improve on this approach using the technique of Maximum Marginal decoding to reduce noise. Their best system considers entire sentences, while our approach (and the morphological analyzers described above) operates at the vocabulary level without regarding sentence context. Hence, though their work is not directly comparable to ours, it presents an interesting orthogonal view to the problem. 3 Model 3.1 Definitions and Framework We use morphological chains to model words in the language. A morphological chain is a short sequence of words that starts from the base word and ends up in a morphol"
Q17-1025,W13-3520,0,0.0221783,"0,249 28,198 3.72 13.05 Turkish MC-10 617K MC-05:10 2531 BOUN 361M Arabic Gigaword 3.83M ATB 21085 Gigaword 1.22G German MC-10 2.34M Dsolve 15522 Wikipedia 589M Table 1: Data statistics: MC-10 = MorphoChallenge 2010 , MC:05-10 = aggregated from MorphoChallenge 2005-2010, BOUN = BOUN corpus (Sak et al., 2008), Gigaword = Arabic Gigaword corpus (Parker et al., 2011), ATB = Arabic Treebank (Maamouri et al., 2003). Duplicates in Arabic test set are filtered. Dsolve is the dataset released by W¨urzner and Jurish (2015), and for training German vectors, we use the pre-processed Wikipedia dump from (Al-Rfou et al., 2013). We also include a supervised counterpart, which uses the same set of features as NBJ-Imp but has access to gold segmentation during training (we perform 5-fold cross-validation using the same data). We obtain the gold standard parent-child pairs required for training from the segmented words in a straightforward fashion. Evaluation metric Following prior work (Virpioja et al., 2011), we evaluate all models using the standard boundary precision and recall (BPR). This measure assesses the accuracy of individual segmentation points, producing IR-style Precision, Recall and F1 scores. 358 Table"
Q17-1025,P11-1062,0,0.0312363,"al phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016). The graph induction methods vary widely depending on the task and the available supervision. The distinctive feature of our work is the use of global constraints to guide the learning of local, edge-level derivations. ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al., 2011). In all of these applications, the ILP formulation is used with a supervised classifier. Our work demonstrates that this framework continues to be effective in an unsupervised setting, providing strong guidance for a local, unsupervised classifier. 3 Model Our model considers a full morphological assignment for all the words in a language, representing it as a forest. Let F = (V, E) be a directed graph where each word corresponds to a node v ∈ V . A directed edge e = (vc , vp ) ∈ E encodes a single morphological derivation from a parent word vp to a child word vc . Edges also reflect the type"
Q17-1025,chrupala-etal-2008-learning,0,0.0776591,"Missing"
Q17-1025,K15-1017,0,0.0444877,"Missing"
Q17-1025,D09-1011,0,0.0270966,"t al., 2009; Sirts and Goldwater, 2013). These models are inherently limited in their ability to incorporate diverse features that are effectively utilized by local discriminative models. Our proposed approach attempts to combine the advantages of both approaches, by defining an objective that incorporates both levels of linguistic properties over the entire forest representation, and adopting an alternating training regime for optimization. Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016). The graph induction methods vary widely depending on the task and the available supervision. The distinctive feature of our work is the use of global constraints to guide the learning of local, edge-level derivations. ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al., 2011). In all of these"
Q17-1025,Q16-1001,0,0.0601958,"y limited in their ability to incorporate diverse features that are effectively utilized by local discriminative models. Our proposed approach attempts to combine the advantages of both approaches, by defining an objective that incorporates both levels of linguistic properties over the entire forest representation, and adopting an alternating training regime for optimization. Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016). The graph induction methods vary widely depending on the task and the available supervision. The distinctive feature of our work is the use of global constraints to guide the learning of local, edge-level derivations. ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al., 2011). In all of these applications, the ILP formulation is used with a supervised clas"
Q17-1025,W04-0105,0,0.473223,"ng the method described in Narasimhan et al. (2015). Now for the forest over the set of nodes V , the log-likelihood loss function is defined as: X L(V ; θ) = − log Pr(v) v∈V =− Xh v∈V − log log X X z∈C(v) exp(θ · φ(v, z)) X v 0 ∈N (v) z 0 ∈C(v 0 ) i exp(θ · φ(v 0 , z 0 )) , (3) This objective can be minimized by gradient descent. Space of Possible Candidates We only consider assignments where the parent word is strictly shorter than the child word to prevent cycles of length two or more. In addition to suffixation and prefixation, we also consider three types of transformations introduced in Goldwater and Johnson (2004): repetition, deletion, and modification. We also handle compounding, where two stems are combined to form a 356 new word (e.g., football). One of these stems carries the main semantic meaning of the compound and is considered to be the parent of the word. Note that stems are not considered affixes, so this does not affect the affix list. We allow parents to be words outside V , since many legitimate word forms might never appear in the corpus. For instance, if we have V = {painter, paints}, the optimal solution would add an unseen word paint to the forest, and choose edges (painter, paint) an"
Q17-1025,W11-0301,1,0.845503,"filtered out duplicate words, and we reran the baselines to obtain comparable results. Following Narasimhan et al. (2015), we reduce the noise by truncating the training word list to the top K frequent words. In addition, we train word vectors (Mikolov et al., 2013) to obtain cosine similarity features. Statistics for all datasets are summarized in Table 1. Baselines We compare our approach against the state-of-the-art unsupervised method of Narasimhan et al. (2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009). For this baseline, we report the results of the publicly available implementation of the technique (NBJ’15), as well as our own improved reimplementation (NBJ-Imp). Specifically in NBJ-Imp, we expanded the original algorithm to handle compounding, along with sibling features as described in Section 3.2, making it essentially an ablation of our model without ILP and alternating training. We employ grid search to find the optimal hyperparameter setting.6 5 Typically the model converges after 5 rounds K ∈ {2500, 5000, 10000}, number of automatically ex"
Q17-1025,P11-1090,0,0.0417199,"Missing"
Q17-1025,Q15-1012,1,0.836331,"ts We postulate that a valid assignment yields forests with the following properties: 1. Increased edge weights Edge weights reflect probabilities of single-step derivations based on the local features including orthographic patterns and semantic relatedness. This local information helps identify that the edge (painter, paint) should be preferred over (painter, pain), because −er is a valid suffix and paint is semantically closer to painter. 2. Minimized number of affixes Prior research has shown that local models tend to greatly overestimate the number of suffixes. For instance, the model of Narasimhan et al. (2015) produces 617 unique affixes when segmenting 10000 English words. Thus, we explicitly encourage the model towards assignments with the least number of affixes. 3. Minimized number of roots relatively to vocabulary size Similarly, the number of roots, and consequently the number of morphological families is markedly smaller than the size of the vocabulary. The first property is local in nature, while the last two are global and embody the principle of Minimum Description Length (MDL). Based on these properties, we formulate an objective function S(F ) over a forest F : P log Pr(e) |F | S(F ) ="
Q17-1025,D15-1108,0,0.0363469,"Missing"
Q17-1025,N09-1024,0,0.131747,"an the baselines to obtain comparable results. Following Narasimhan et al. (2015), we reduce the noise by truncating the training word list to the top K frequent words. In addition, we train word vectors (Mikolov et al., 2013) to obtain cosine similarity features. Statistics for all datasets are summarized in Table 1. Baselines We compare our approach against the state-of-the-art unsupervised method of Narasimhan et al. (2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009). For this baseline, we report the results of the publicly available implementation of the technique (NBJ’15), as well as our own improved reimplementation (NBJ-Imp). Specifically in NBJ-Imp, we expanded the original algorithm to handle compounding, along with sibling features as described in Section 3.2, making it essentially an ablation of our model without ILP and alternating training. We employ grid search to find the optimal hyperparameter setting.6 5 Typically the model converges after 5 rounds K ∈ {2500, 5000, 10000}, number of automatically extracted affixes ∈ {100, 200, 300, 400, 500}"
Q17-1025,W00-0712,0,0.087772,"roup paint, paints and pain into two clusters: {paint, paints} and {pain}. To derive clusters from the forest representation, we assume that all the words in the same tree form a cluster. Data To obtain gold information about morphological clusters, we use CELEX (Baayen et al., 1993). Data statistics are summarized in Table 2. We remove words without stems from CELEX.8 Baseline We compare our model against NBJ-Imp described above. We select the best variant of our model and the base model based on their respective performance on the segmentation task. Evaluation We use the metrics proposed by Schone and Jurafsky (2000). Specifically, let Xw 7 http://www.gurobi.com/ An example is aerodrome, where both aero- and drome are affixes. 8 Language #Words #Words (Test only) English Turkish German 1675 1759 1747 687 763 749 Language English Table 3: Data statistics for root detection task. Duplicate words are removed. and Yw be the clusters for word w in our predictions and gold standard, respectively. We compute the number of correct (C), inserted (I) and deleted (D) words for the clusters as follows: C= I= D= Turkish X |Xw ∩ Yw | |Yw | w∈W X |Xw  Yw | |Yw | w∈W X |Yw  Xw | |Yw | Arabic w∈W Then we compute precisi"
Q17-1025,Q13-1021,0,0.24062,"mmonly used log-linear formulation enables these models to consider a rich set of features ranging from orthographic patterns to semantic relatedness. However, these models generally bypass global constraints (Narasimhan et al., 2015) or require performing inference over very large spaces (Poon et al., 2009). As we show in our 354 analysis (Section 5), this omission negatively affects model performance. In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013). These models are inherently limited in their ability to incorporate diverse features that are effectively utilized by local discriminative models. Our proposed approach attempts to combine the advantages of both approaches, by defining an objective that incorporates both levels of linguistic properties over the entire forest representation, and adopting an alternating training regime for optimization. Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2"
Q17-1025,P05-1044,0,0.0592088,"asimhan et al., 2015), we model this probability using a log-linear model: Pr(w, z) ∝ exp(θ · φ(w, z)), (2) where θ is the set of parameters to be learned, and φ(w, z) is the feature vector extracted from w and z. Each candidate z is a tuple (string, label), where label refers to the label of the potential edge. As a result, the marginal probability is X Pr(w, z) Pr(w) = z∈C(w) =P P w0 ∈Σ∗ z∈C(w) exp(θ P · φ(w, z)) z 0 ∈C(w0 ) exp(θ · φ(w0 , z 0 )) , where Σ∗ is the set of all possible strings. Computing the sum in the denominator is infeasible. Instead, we make use of contrastive estimation (Smith and Eisner, 2005), substituting Σ∗ with a (limited) set of neighbor strings N (w) that are orthographically close to w. This technique distributes the probability mass among neighboring words and forces the model to identify meaningful discriminative features. We obtain N (w) by transposing characters in w, following the method described in Narasimhan et al. (2015). Now for the forest over the set of nodes V , the log-likelihood loss function is defined as: X L(V ; θ) = − log Pr(v) v∈V =− Xh v∈V − log log X X z∈C(v) exp(θ · φ(v, z)) X v 0 ∈N (v) z 0 ∈C(v 0 ) i exp(θ · φ(v 0 , z 0 )) , (3) This objective can be"
Q17-1025,P08-1084,1,0.852372,"and Toutanova, 2011; Narasimhan et al., 2015). A commonly used log-linear formulation enables these models to consider a rich set of features ranging from orthographic patterns to semantic relatedness. However, these models generally bypass global constraints (Narasimhan et al., 2015) or require performing inference over very large spaces (Poon et al., 2009). As we show in our 354 analysis (Section 5), this omission negatively affects model performance. In contrast, earlier work focuses on modeling global morphological assignment, using generative probabilistic models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008; Goldwater et al., 2009; Sirts and Goldwater, 2013). These models are inherently limited in their ability to incorporate diverse features that are effectively utilized by local discriminative models. Our proposed approach attempts to combine the advantages of both approaches, by defining an objective that incorporates both levels of linguistic properties over the entire forest representation, and adopting an alternating training regime for optimization. Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphologica"
Q17-1025,N15-1186,0,0.152706,"se models are inherently limited in their ability to incorporate diverse features that are effectively utilized by local discriminative models. Our proposed approach attempts to combine the advantages of both approaches, by defining an objective that incorporates both levels of linguistic properties over the entire forest representation, and adopting an alternating training regime for optimization. Graph-based representations in computational morphology Variants of a graph-based representation have been used to model various morphological phenomena (Dreyer and Eisner, 2009; Peng et al., 2015; Soricut and Och, 2015; Faruqui et al., 2016). The graph induction methods vary widely depending on the task and the available supervision. The distinctive feature of our work is the use of global constraints to guide the learning of local, edge-level derivations. ILP for capturing global properties Integer Linear Programming has been successfully employed to capture global constraints across multiple applications such as information extraction (Roth and Yih, 2001), sentence compression (Clarke and Lapata, 2008), and textual entailment (Berant et al., 2011). In all of these applications, the ILP formulation is used"
Q17-1025,P12-2063,1,0.856764,"icate words, and we reran the baselines to obtain comparable results. Following Narasimhan et al. (2015), we reduce the noise by truncating the training word list to the top K frequent words. In addition, we train word vectors (Mikolov et al., 2013) to obtain cosine similarity features. Statistics for all datasets are summarized in Table 1. Baselines We compare our approach against the state-of-the-art unsupervised method of Narasimhan et al. (2015) which outperforms a number of alternative approaches (Creutz and Lagus, 2005; Virpioja et al., 2013; Sirts and Goldwater, 2013; Lee et al., 2011; Stallard et al., 2012; Poon et al., 2009). For this baseline, we report the results of the publicly available implementation of the technique (NBJ’15), as well as our own improved reimplementation (NBJ-Imp). Specifically in NBJ-Imp, we expanded the original algorithm to handle compounding, along with sibling features as described in Section 3.2, making it essentially an ablation of our model without ILP and alternating training. We employ grid search to find the optimal hyperparameter setting.6 5 Typically the model converges after 5 rounds K ∈ {2500, 5000, 10000}, number of automatically extracted affixes ∈ {100,"
Q17-1036,P07-1056,0,0.218381,"Missing"
Q17-1036,S16-1044,0,0.05501,"Missing"
Q17-1036,P07-1036,0,0.0109493,"r transferring across domains, while our method can also handle aspect transfer. In addition, we introduce a reconstruction loss which results in more robust adversarial training. We believe that this formulation will benefit other applications of adversarial training, beyond the ones described in this paper. Semi-supervised Learning with Keywords In our work, we use a small set of keywords as a source of weak supervision for aspect-relevance scoring. This relates to prior work on utilizing prototypes and seed words in semi-supervised learning (Haghighi and Klein, 2006; Grenager et al., 2005; Chang et al., 2007; Mann and McCallum, 2010; Jagarlamudi et al., 2012; Li et al., 2012; Eisenstein, 2017). All these prior approaches utilize prototype annotations primarily targeting model bootstrapping but not for learning representations. In contrast, our model uses provided keywords to learn aspect-driven encoding of input examples. Attention Mechanism in NLP One may view our aspect-relevance scorer as a sentence-level “semi-supervised attention”, in which relevant sentences receive more attention during feature extraction. While traditional attention-based models typically induce attention in an unsupervis"
Q17-1036,D16-1053,0,0.0105832,"primarily targeting model bootstrapping but not for learning representations. In contrast, our model uses provided keywords to learn aspect-driven encoding of input examples. Attention Mechanism in NLP One may view our aspect-relevance scorer as a sentence-level “semi-supervised attention”, in which relevant sentences receive more attention during feature extraction. While traditional attention-based models typically induce attention in an unsupervised manner, they have to rely on a large amount of labeled data for the target task (Bahdanau et al., 2015; Rush et al., 2015; Chen et al., 2015; Cheng et al., 2016; Xu et al., 2015; Xu and Saenko, 2016; Yang et al., 2016; Martins and Astudillo, 2016; Lei et al., 2016). Unlike these methods, our approach assumes no label annotations in the target domain. Other researches have focused on utilizing human-provided rationales as “supervised attention” to improve prediction (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016; Brun et al., 2016). In contrast, our model only assumes access to a small set of keywords as a source of weak supervision. Moreover, all these prior approaches focus on in-domain classification. In this paper, however, we stu"
Q17-1036,P05-1046,0,0.0289668,"arning has been used for transferring across domains, while our method can also handle aspect transfer. In addition, we introduce a reconstruction loss which results in more robust adversarial training. We believe that this formulation will benefit other applications of adversarial training, beyond the ones described in this paper. Semi-supervised Learning with Keywords In our work, we use a small set of keywords as a source of weak supervision for aspect-relevance scoring. This relates to prior work on utilizing prototypes and seed words in semi-supervised learning (Haghighi and Klein, 2006; Grenager et al., 2005; Chang et al., 2007; Mann and McCallum, 2010; Jagarlamudi et al., 2012; Li et al., 2012; Eisenstein, 2017). All these prior approaches utilize prototype annotations primarily targeting model bootstrapping but not for learning representations. In contrast, our model uses provided keywords to learn aspect-driven encoding of input examples. Attention Mechanism in NLP One may view our aspect-relevance scorer as a sentence-level “semi-supervised attention”, in which relevant sentences receive more attention during feature extraction. While traditional attention-based models typically induce attent"
Q17-1036,N06-1041,0,0.0125907,"ter vision, adversarial learning has been used for transferring across domains, while our method can also handle aspect transfer. In addition, we introduce a reconstruction loss which results in more robust adversarial training. We believe that this formulation will benefit other applications of adversarial training, beyond the ones described in this paper. Semi-supervised Learning with Keywords In our work, we use a small set of keywords as a source of weak supervision for aspect-relevance scoring. This relates to prior work on utilizing prototypes and seed words in semi-supervised learning (Haghighi and Klein, 2006; Grenager et al., 2005; Chang et al., 2007; Mann and McCallum, 2010; Jagarlamudi et al., 2012; Li et al., 2012; Eisenstein, 2017). All these prior approaches utilize prototype annotations primarily targeting model bootstrapping but not for learning representations. In contrast, our model uses provided keywords to learn aspect-driven encoding of input examples. Attention Mechanism in NLP One may view our aspect-relevance scorer as a sentence-level “semi-supervised attention”, in which relevant sentences receive more attention during feature extraction. While traditional attention-based models"
Q17-1036,E12-1021,0,0.0394331,"Missing"
Q17-1036,D16-1011,1,0.0523594,"provided keywords to learn aspect-driven encoding of input examples. Attention Mechanism in NLP One may view our aspect-relevance scorer as a sentence-level “semi-supervised attention”, in which relevant sentences receive more attention during feature extraction. While traditional attention-based models typically induce attention in an unsupervised manner, they have to rely on a large amount of labeled data for the target task (Bahdanau et al., 2015; Rush et al., 2015; Chen et al., 2015; Cheng et al., 2016; Xu et al., 2015; Xu and Saenko, 2016; Yang et al., 2016; Martins and Astudillo, 2016; Lei et al., 2016). Unlike these methods, our approach assumes no label annotations in the target domain. Other researches have focused on utilizing human-provided rationales as “supervised attention” to improve prediction (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016; Brun et al., 2016). In contrast, our model only assumes access to a small set of keywords as a source of weak supervision. Moreover, all these prior approaches focus on in-domain classification. In this paper, however, we study the task in the context of domain adaptation. Multitask Learning Existing multitask learning methods f"
Q17-1036,D12-1127,0,0.0281627,"Missing"
Q17-1036,N15-1092,0,0.0194846,"7; Marshall et al., 2015; Zhang et al., 2016; Brun et al., 2016). In contrast, our model only assumes access to a small set of keywords as a source of weak supervision. Moreover, all these prior approaches focus on in-domain classification. In this paper, however, we study the task in the context of domain adaptation. Multitask Learning Existing multitask learning methods focus on the case where supervision is available for all tasks. A typical architecture involves using a shared encoder with a separate classifier for each task. (Caruana, 1998; Pan and Yang, 2010; Collobert and Weston, 2008; Liu et al., 2015; Bordes et al., 2012). In contrast, our work assumes labeled data only for the source aspect. We train a single classifier for both aspects by learning aspectinvariant representation that enables the transfer. 3 Problem Formulation We begin by formalizing aspect transfer with the idea of differentiating it from standard domain adaptation. In our setup, we have two classification tasks called the source and the target tasks. In contrast to source and target tasks in domain adaptation, both of these tasks are defined over the same set of examples (here documents, e.g., pathology reports). What"
Q17-1036,D15-1044,0,0.0114787,"proaches utilize prototype annotations primarily targeting model bootstrapping but not for learning representations. In contrast, our model uses provided keywords to learn aspect-driven encoding of input examples. Attention Mechanism in NLP One may view our aspect-relevance scorer as a sentence-level “semi-supervised attention”, in which relevant sentences receive more attention during feature extraction. While traditional attention-based models typically induce attention in an unsupervised manner, they have to rely on a large amount of labeled data for the target task (Bahdanau et al., 2015; Rush et al., 2015; Chen et al., 2015; Cheng et al., 2016; Xu et al., 2015; Xu and Saenko, 2016; Yang et al., 2016; Martins and Astudillo, 2016; Lei et al., 2016). Unlike these methods, our approach assumes no label annotations in the target domain. Other researches have focused on utilizing human-provided rationales as “supervised attention” to improve prediction (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016; Brun et al., 2016). In contrast, our model only assumes access to a small set of keywords as a source of weak supervision. Moreover, all these prior approaches focus on in-domain classif"
Q17-1036,N07-1033,0,0.049211,"more attention during feature extraction. While traditional attention-based models typically induce attention in an unsupervised manner, they have to rely on a large amount of labeled data for the target task (Bahdanau et al., 2015; Rush et al., 2015; Chen et al., 2015; Cheng et al., 2016; Xu et al., 2015; Xu and Saenko, 2016; Yang et al., 2016; Martins and Astudillo, 2016; Lei et al., 2016). Unlike these methods, our approach assumes no label annotations in the target domain. Other researches have focused on utilizing human-provided rationales as “supervised attention” to improve prediction (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016; Brun et al., 2016). In contrast, our model only assumes access to a small set of keywords as a source of weak supervision. Moreover, all these prior approaches focus on in-domain classification. In this paper, however, we study the task in the context of domain adaptation. Multitask Learning Existing multitask learning methods focus on the case where supervision is available for all tasks. A typical architecture involves using a shared encoder with a separate classifier for each task. (Caruana, 1998; Pan and Yang, 2010; Collobert and Weston, 2008; L"
Q17-1036,D16-1076,0,0.0220955,"ile traditional attention-based models typically induce attention in an unsupervised manner, they have to rely on a large amount of labeled data for the target task (Bahdanau et al., 2015; Rush et al., 2015; Chen et al., 2015; Cheng et al., 2016; Xu et al., 2015; Xu and Saenko, 2016; Yang et al., 2016; Martins and Astudillo, 2016; Lei et al., 2016). Unlike these methods, our approach assumes no label annotations in the target domain. Other researches have focused on utilizing human-provided rationales as “supervised attention” to improve prediction (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016; Brun et al., 2016). In contrast, our model only assumes access to a small set of keywords as a source of weak supervision. Moreover, all these prior approaches focus on in-domain classification. In this paper, however, we study the task in the context of domain adaptation. Multitask Learning Existing multitask learning methods focus on the case where supervision is available for all tasks. A typical architecture involves using a shared encoder with a separate classifier for each task. (Caruana, 1998; Pan and Yang, 2010; Collobert and Weston, 2008; Liu et al., 2015; Bordes et al., 2012). In c"
Q17-1036,P16-1031,0,0.0256076,"e abstract representations without pulling apart different aspects in the same example, and therefore are likely to fail on the aspect transfer problem. The majority of these prior methods first learn a task-independent representation, and then train a label predictor (e.g. SVM) on this representation in a separate step. For example, earlier researches employ a shared autoencoder (Glorot et al., 2011; Chopra et al., 2013) to learn a cross-domain representation. Chen et al. (2012) further improve and stabilize the representation learning by utilizing marginalized denoising autoencoders. Later, Zhou et al. (2016) propose to minimize domain-shift of the autoencoder in a linear data combination manner. Other researches have focused on learning transferable representations in an end-to-end fashion. Examples include using transduction learning for object recognition (Sener et al., 2016) and using residual transfer networks for image classification (Long et al., 2016). In contrast, we use adversarial training to encourage learning domaininvariant features in a more explicit way. Our approach offers another two advantages over prior work. First, we jointly optimize features with the final classification tas"
Q18-1004,D15-1138,0,0.197935,"onment to obtain accurate maps for navigation (Walter et al., 2013; Hemachandra et al., 2014). Most of these approaches typically assume access to detailed geometry or other forms of domain knowledge. In contrast to these knowledge-rich approaches, we are learning spatial reference via interaction with the environment, acquiring knowledge of the environment in the process. Instruction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path corresponding with provided instructions. In our setup, the agent is only driven by the final rewards when the goal is achieved. This weaker source of supervision motivates development of new techniques not considered in prior work. More recently, Misra et al. (2017) proposed a neural architecture for jointly mapping instructions and visual observations (pixels) to actions in the environment. Their model separately induces text and environment representations, which are concatenated into a singl"
Q18-1004,Q13-1005,0,0.189144,"sition information with spatial models of the environment to obtain accurate maps for navigation (Walter et al., 2013; Hemachandra et al., 2014). Most of these approaches typically assume access to detailed geometry or other forms of domain knowledge. In contrast to these knowledge-rich approaches, we are learning spatial reference via interaction with the environment, acquiring knowledge of the environment in the process. Instruction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path corresponding with provided instructions. In our setup, the agent is only driven by the final rewards when the goal is achieved. This weaker source of supervision motivates development of new techniques not considered in prior work. More recently, Misra et al. (2017) proposed a neural architecture for jointly mapping instructions and visual observations (pixels) to actions in the environment. Their model separately induces text and environment repr"
Q18-1004,N16-1089,0,0.489752,"annels, all with 3x3 kernels and padding of length 1 such that the output value map prediction is equal in size to the input observation. For each map, a reward of 3 is given for reaching the correct goal specified by human annotation and a reward of −1 is given for falling in a puddle cell. The only terminal state is when the agent is at the goal. Rewards are discounted by a factor of 0.95. We use Adam optimization (Kingma and Ba, 2015) for training all models. 6 Results We present empirical results on two different datasets - our annotated puddle world and an existing block navigation task (Bisk et al., 2016). 6.1 Puddle world navigation Comparison with the state-of-the-art We first investigate the ability of our model to learn solely from environment simulation. Figure 5 shows the discounted reward achieved by our model as well as the two baselines for both instruction types. In both experiments, our model is the only one of the Local Global Combined Policy Quality Distance Policy Quality Distance Policy Quality Distance UVFA (text) 0.56 4.71 0.62 6.28 0.61 5.06 CNN + LSTM 0.80 5.73 0.82 6.13 0.82 5.67 Our model 0.87 2.18 0.90 3.35 0.89 3.04 Table 2: Performance of models trained via reinforcemen"
Q18-1004,P11-1028,1,0.823047,"curring early in the task. It is likely that the language plays a less crucial role in specifying the subgoal position in the final steps of a task. As shown in Figure 9(a), it may be possible to narrow down candidate subgoal positions just by looking at a nearly-constructed highdefault metric here. However, estimating policy quality for environments substantially larger than those investigated here is a challenge in itself. level shape. In contrast, this would not be possible early in a task because most of the blocks will be randomly positioned. This finding is consistent with a result from Branavan et al. (2011), who reported that strategy game manuals were useful early in the game but became less essential further into play. It appears to be part of a larger trend that the marginal benefit of language in such grounding tasks can vary predictably between individual instructions. 7 Conclusions We have described a novel approach for grounded spatial reasoning. Combining the language representation in a spatially localized manner allows for increased precision of goal identification a nd improved performance on unseen environment configurations. Alongside our models, we present Puddle World Navigation,"
Q18-1004,P06-1094,0,0.105377,"Missing"
Q18-1004,P13-1022,0,0.0245676,"al models of the environment to obtain accurate maps for navigation (Walter et al., 2013; Hemachandra et al., 2014). Most of these approaches typically assume access to detailed geometry or other forms of domain knowledge. In contrast to these knowledge-rich approaches, we are learning spatial reference via interaction with the environment, acquiring knowledge of the environment in the process. Instruction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path corresponding with provided instructions. In our setup, the agent is only driven by the final rewards when the goal is achieved. This weaker source of supervision motivates development of new techniques not considered in prior work. More recently, Misra et al. (2017) proposed a neural architecture for jointly mapping instructions and visual observations (pixels) to actions in the environment. Their model separately induces text and environment representations, which are"
Q18-1004,D17-1106,0,0.646729,"t of interactive worlds. Specifically, we assume access to a simulated environment, in which an agent can take actions to interact with the world and is rewarded for reaching the location specified by the language instruction. This feedback is the only source of supervision the model uses for interpreting spatial references. The key modeling task here is to induce a representation that closely ties environment observations and linguistic expressions. In prior work, this issue was addressed by learning representations for each modality and then combining them, for instance, with concatenation (Misra et al., 2017). While this approach captures high-level correspondences between instructions and maps, it does not encode de49 Transactions of the Association for Computational Linguistics, vol. 6, pp. 49–61, 2018. Action Editor: Hal Daum´e III. Submission batch: 7/2017; Revision batch: 10/2017; Published 1/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tailed, lower-level mappings between specific positions on the map and their descriptions. As our experiments demonstrate, combining the language and environment representations in a spatially localized manner"
Q18-1004,W08-1109,0,0.0400581,"value function V (s, g) describing the expected reward from being in state s given goal g, capturing that state values are goal-dependent and that a single environment can offer many such goals. We also make use of such a generalized value function, although our goals are not observed directly as 2 We will use the terms goal specifications and instructions interchangeably. 3 A local reference for a non-unique object would be ambiguous, of course. Text instructions Prior work has investigated human usage of different types of referring expressions to describe spatial relations (Levinson, 2003; Viethen and Dale, 2008). In order to build a robust instruction following system, we examine several categories of spatial expressions that exhibit the wide range of natural language goal descriptions. Specifically, we consider instructions that utilize objects/entities present in the environment to describe a goal location. These instructions can be categorized into three groups: (a) Text referring to a specific entity (e.g., “Go to the circle”). 51 Figure 2: A schematic depiction of our model. Text instructions are represented as a vector h(t) and states as embeddings φ(s). A portion of the text representation is"
Q18-1004,P10-1083,0,0.121357,"ork in robotics has integrated text containing position information with spatial models of the environment to obtain accurate maps for navigation (Walter et al., 2013; Hemachandra et al., 2014). Most of these approaches typically assume access to detailed geometry or other forms of domain knowledge. In contrast to these knowledge-rich approaches, we are learning spatial reference via interaction with the environment, acquiring knowledge of the environment in the process. Instruction following Spatial reasoning is a common element in many papers on instruction following (MacMahon et al., 2006; Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013; Andreas and Klein, 2015). As a source of supervision, these methods assume access to demonstrations, which specify the path corresponding with provided instructions. In our setup, the agent is only driven by the final rewards when the goal is achieved. This weaker source of supervision motivates development of new techniques not considered in prior work. More recently, Misra et al. (2017) proposed a neural architecture for jointly mapping instructions and visual observations (pixels) to actions in the environment. Thei"
W02-1022,C00-1007,0,0.0767618,"Missing"
W02-1022,P01-1008,1,0.671208,"Missing"
W02-1022,J93-2003,0,0.00300923,"Missing"
W02-1022,P98-1116,0,0.091567,"Missing"
W02-1022,J00-2004,0,0.0216146,"Missing"
W02-1022,P00-1056,0,0.0659232,"Missing"
W02-1022,W00-0306,0,0.0156982,"Missing"
W02-1022,P02-1040,0,0.113723,"Missing"
W02-1022,A00-2026,0,0.016196,"Missing"
W02-1022,W99-0602,0,0.0133339,"Missing"
W02-1022,H94-1019,0,\N,Missing
W02-1022,C98-1112,0,\N,Missing
W03-1004,E99-1042,0,0.151173,"Missing"
W03-1004,P98-1069,0,0.0715488,"ing both the local similarity values and additional features. In the case of monolingual corpora, step (2) might seem unnecessary. Since the texts share the same language, it would be enough to choose for local similarity a function based on lexical cues only and select sentence pairs with high lexical similarity. Even a simple lexical function (e.g., one that counts word overlap) could produce an accurate alignment. 1 Sentence alignment for comparable multilingual corpora was not addressed in previous research. Comparable corpora have primarily been used to build bilingual lexical resources (Fung and Yee, 1998). · Petersburg served as the capital of Russia for 200 years. (A) · For two centuries Petersburg was the capital of the Russian Empire. · The city is also the country’s leading port and center of commerce. (B) · And yet, as with so much of the city, the port facilities are old and inefficient. Figure 1: Sentence pairs from our corpus sharing two content words. (A) is a matching pair, (B) is not. In MT, a weak similarity function is compensated for by searching for a globally optimal alignment, using dynamic programming or taking advantage of the geometric/positional or contextual properties of"
W03-1004,P91-1023,0,0.0176201,"erved as the capital of Russia for 200 years. (A) · For two centuries Petersburg was the capital of the Russian Empire. · The city is also the country’s leading port and center of commerce. (B) · And yet, as with so much of the city, the port facilities are old and inefficient. Figure 1: Sentence pairs from our corpus sharing two content words. (A) is a matching pair, (B) is not. In MT, a weak similarity function is compensated for by searching for a globally optimal alignment, using dynamic programming or taking advantage of the geometric/positional or contextual properties of the text pair (Gale and Church, 1991; Shemtov, 1993; Melamed, 1999). But these techniques operate on the assumptions that there are limited insertions and deletions between the texts and that the order of the information is roughly preserved from one text to another. Texts from comparable corpora, as opposed to parallel corpora, contain a great deal of “noise.” In Figure 2 which plots the manually identified alignment for a text pair in our corpus, only a small fraction of the sentences got aligned (35 out of 31 × 270 sentence pairs), which illustrates that there is no complete information overlap. Consider two texts written by"
W03-1004,W99-0625,0,0.155148,"sentence similarity using a cosine-based metric. Jing (2002) identifies phrases that were cut and pasted together using a Hidden Markov Model with features incorporating word identity and positioning within sentences, thereby providing an alignment of the document and its summary. However, both of these methods construct an alignment by looking at sentences one at a time, independently of the decisions made about other sentences. Because summaries often reuse original document text to a large extent, these methods achieve good results. In the context of multidocument summarization, SimFinder (Hatzivassiloglou et al., 1999) identifies sentences that convey similar information across input documents to select the summary content. Even though the input documents are about the same subject, they exhibit a great deal of lexical variability. To address this issue, SimFinder employs a complex similarity function, combining features that extend beyond a simple word count and include noun phrase, proper noun, and WordNet sense overlap. Since many documents are processed in parallel, clustering is used to combine pairwise alignments. In contrast to our approach, SimFinder does not take the context around sentences into a"
W03-1004,P94-1002,0,0.00771637,"tment.”3 If the task is to align a disease description written for physicians and a text describing the same disease for lay people, it is most likely that sentences within the topic “symptoms” in the expert version will map to sentences describing the symptoms in the lay version rather than those describing treatment options. If we can automatically identify the topic each paragraph conveys, we can decide more accurately whether two paragraphs are related and should be mapped for further processing. 2 Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). 3 We use the term topic differently than it is commonly used in the topic detection task– there, a “topic” would designate which disease is described. In the field of text generation, methods for representing the semantic structure of texts have been investigated through text schemata (McKeown, 1985) or rhetorical structures (Mann and Thompson, 1987). In our framework, we want to identify the different topics of the text, but we are not concerned with the relations holding between them or the order in which they typically appear. We propose to identify the topics typical to each collection i"
W03-1004,A00-2024,0,0.0232792,"automatically would provide a valuable resource for learning of text-totext rewriting rules. We incorporate context into the search for an optimal alignment in two complementary ways: learning rules for matching paragraphs using topic structure and further refining the matching through local alignment to find good sentence pairs. Evaluation shows that our alignment method outperforms state-of-the-art systems developed for the same task. 1 Introduction Text-to-text generation is an emerging area of research in NLP (Chandrasekar and Bangalore, 1997; Caroll et al., 1999; Knight and Marcu, 2000; Jing and McKeown, 2000). Unlike in traditional conceptto-text generation, text-to-text generation applications take a text as input and transform it into a new text satisfying specific constraints, such as length in summarization or style in text simplification. One exciting new research direction is the automatic induction of such transformation rules. This is a particularly promising direction given that there are naturally occurring examples of comparable texts that convey the same information yet are written in different styles. Presented with two such texts, one Noemie Elhadad Department of Computer Science Col"
W03-1004,J02-4006,0,0.066374,"l similarity, outperforms existing methods based on complex local similarity functions. In the next section, we provide an overview of existing work on monolingual sentence alignment. Section 3 describes our algorithm. In sections 4 and 5, we report on data collection and evaluation. 2 Related Work Most of the work in monolingual corpus alignment is in the context of summarization. In single document summarization, alignment between full documents and summaries written by humans is used to learn rules for text compression. Marcu (1999) computes sentence similarity using a cosine-based metric. Jing (2002) identifies phrases that were cut and pasted together using a Hidden Markov Model with features incorporating word identity and positioning within sentences, thereby providing an alignment of the document and its summary. However, both of these methods construct an alignment by looking at sentences one at a time, independently of the decisions made about other sentences. Because summaries often reuse original document text to a large extent, these methods achieve good results. In the context of multidocument summarization, SimFinder (Hatzivassiloglou et al., 1999) identifies sentences that con"
W03-1004,J99-1003,0,0.0195192,"years. (A) · For two centuries Petersburg was the capital of the Russian Empire. · The city is also the country’s leading port and center of commerce. (B) · And yet, as with so much of the city, the port facilities are old and inefficient. Figure 1: Sentence pairs from our corpus sharing two content words. (A) is a matching pair, (B) is not. In MT, a weak similarity function is compensated for by searching for a globally optimal alignment, using dynamic programming or taking advantage of the geometric/positional or contextual properties of the text pair (Gale and Church, 1991; Shemtov, 1993; Melamed, 1999). But these techniques operate on the assumptions that there are limited insertions and deletions between the texts and that the order of the information is roughly preserved from one text to another. Texts from comparable corpora, as opposed to parallel corpora, contain a great deal of “noise.” In Figure 2 which plots the manually identified alignment for a text pair in our corpus, only a small fraction of the sentences got aligned (35 out of 31 × 270 sentence pairs), which illustrates that there is no complete information overlap. Consider two texts written by different press agencies: while"
W03-1004,E93-1054,0,0.027937,"Russia for 200 years. (A) · For two centuries Petersburg was the capital of the Russian Empire. · The city is also the country’s leading port and center of commerce. (B) · And yet, as with so much of the city, the port facilities are old and inefficient. Figure 1: Sentence pairs from our corpus sharing two content words. (A) is a matching pair, (B) is not. In MT, a weak similarity function is compensated for by searching for a globally optimal alignment, using dynamic programming or taking advantage of the geometric/positional or contextual properties of the text pair (Gale and Church, 1991; Shemtov, 1993; Melamed, 1999). But these techniques operate on the assumptions that there are limited insertions and deletions between the texts and that the order of the information is roughly preserved from one text to another. Texts from comparable corpora, as opposed to parallel corpora, contain a great deal of “noise.” In Figure 2 which plots the manually identified alignment for a text pair in our corpus, only a small fraction of the sentences got aligned (35 out of 31 × 270 sentence pairs), which illustrates that there is no complete information overlap. Consider two texts written by different press"
W03-1004,J93-1004,0,\N,Missing
W03-1004,C98-1066,0,\N,Missing
W06-1623,A00-2018,0,0.0151996,"subset of relations covers a majority of annotated links (Pustejovsky et al., 2003). classification task. Given a set of candidate boundaries (e.g., sentence boundaries), our task is to select a subset of the boundaries that delineate temporal segment transitions. To implement this approach, we first identify a set of potential boundaries. Our analysis of the manually-annotated corpus reveals that boundaries can occur not only between sentences, but also within a sentence, at the boundary of syntactic clauses. We automatically segment sentences into clauses using a robust statistical parser (Charniak, 2000). Next, we encode each boundary as a vector of features. Given a set of annotated examples, we train a classifier 2 to predict boundaries based on the following feature set: Lexical Features Temporal expressions, such as tomorrow and earlier, are among the strongest markers of temporal discontinuity (Passonneau, 1988; Bestgen and Vonk, 1995). In addition to a well-studied set of domain-independent temporal markers, there are a variety of domain-specific temporal markers. For instance, the phrase initial hospital visit functions as a time anchor in the medical domain. To automatically extract t"
W06-1623,W01-1312,0,0.020639,"ed TDAG. Transitivity Constraints The key requirement on the edge assignment is the transitivity of the resulting graph. Transitivity also guarantees that the graph does not have cycles. We enforce transitivity by introducing the following constraint for every triple (i, j, k): Greedy Best-first Inference (BF) Our second inference strategy is also greedy. It aims to optimize the score of the graph. The score of the graph is computed by summing the scores of Ii→j + Ij→k − 1 ≤ Ii→k (4) If both indicator variables on the left side of the inequality are set to 1, then the indicator variable text (Wilson et al., 2001). 193 Hospital. A typical summary describes an admission status, previous diseases related to the current conditions and their treatments, family history, and the current course of treatment. For privacy protection, names and dates are removed from the summaries before publication. The average length of a summary is 47 sentences. The summaries are written in the past tense, and a typical summary does not include instances of the past perfect. The summaries do not follow a chronological order. The ordering of information in this domain is guided by stylistic conventions (i.e., symptoms are pres"
W06-1623,P94-1002,0,0.0158672,"rent and the previous sentence. However, many of these predictors are heavily context-dependent and, thus, cannot be considered independently. Instead of manually crafting complex rules controlling feature interaction, we opt to learn them from data. We model temporal segmentation as a binary 2 191 BoosTexter package (Schapire and Singer, 2000). identifying such transitions is relevant for temporal segmentation. We quantify the strength of a topic change by computing a cosine similarity between sentences bordering the proposed segmentation. This measure is commonly used in topic segmentation (Hearst, 1994) under the assumption that change in lexical distribution corresponds to topical change. Positional Features Some parts of the document are more likely to exhibit temporal change than others. This property is related to patterns in discourse organization of a document as a whole. For instance, a medical case summary first discusses various developments in the medical history of a patient and then focuses on his current conditions. As a result, the first part of the summary contains many short temporal segments. We encode positional features by recording the relative position of a sentence in a"
W06-1623,P92-1030,0,0.0499347,"e reduction1 of a TDAG is shown in Figure 1. Edges in a TDAG capture temporal precedence relations between segments. Because the graph encodes an order, cycles are prohibited. We do not require the graph to be fully connected — if the precedence relation between two nodes is not specified in the text, the corresponding nodes will not be connected. For instance, consider the segments S5 and S7 from Figure 1, which describe her previous tests and the history of eczema. Any 2 Related Work Temporal ordering has been extensively studied in computational linguistics (Passonneau, 1988; Webber, 1988; Hwang and Schubert, 1992; Lascarides and Asher, 1993; Lascarides and Oberlander, 1993). Prior research has investigated a variety of language mechanisms and knowledge sources that guide interpretation of temporal ordering, including tense, aspect, temporal adverbials, rhetorical relations and pragmatic constraints. In recent years, the availability of annotated corpora, such as TimeBank (Pustejovsky et al., 2003), has triggered the use of machinelearning methods for temporal analysis (Mani et al., 2003; Lapata and Lascarides, 2004; Boguraev and Ando, 2005). Typical tasks include identification of temporal anchors, li"
W06-1623,N04-1020,0,0.0864467,"has been extensively studied in computational linguistics (Passonneau, 1988; Webber, 1988; Hwang and Schubert, 1992; Lascarides and Asher, 1993; Lascarides and Oberlander, 1993). Prior research has investigated a variety of language mechanisms and knowledge sources that guide interpretation of temporal ordering, including tense, aspect, temporal adverbials, rhetorical relations and pragmatic constraints. In recent years, the availability of annotated corpora, such as TimeBank (Pustejovsky et al., 2003), has triggered the use of machinelearning methods for temporal analysis (Mani et al., 2003; Lapata and Lascarides, 2004; Boguraev and Ando, 2005). Typical tasks include identification of temporal anchors, linking events to times, and temporal ordering of events. Since this paper addresses temporal ordering, we focus our discussion on this task. Existing ordering approaches vary both in terms of the ordering unit — it can be a clause, a sentence or 1 The transitive reduction of a graph is the smallest graph with the same transitive closure. 190 S4 S2 S3 S7 S5 S10 S8 S11 S1 S2 S5 S7 S8 S13 S6 S1 S9 S12 S13 S14 A 32-year-old woman was admitted to the hospital because of left subcostal pain... The patient had been"
W06-1623,E93-1031,0,0.0643759,"a TDAG capture temporal precedence relations between segments. Because the graph encodes an order, cycles are prohibited. We do not require the graph to be fully connected — if the precedence relation between two nodes is not specified in the text, the corresponding nodes will not be connected. For instance, consider the segments S5 and S7 from Figure 1, which describe her previous tests and the history of eczema. Any 2 Related Work Temporal ordering has been extensively studied in computational linguistics (Passonneau, 1988; Webber, 1988; Hwang and Schubert, 1992; Lascarides and Asher, 1993; Lascarides and Oberlander, 1993). Prior research has investigated a variety of language mechanisms and knowledge sources that guide interpretation of temporal ordering, including tense, aspect, temporal adverbials, rhetorical relations and pragmatic constraints. In recent years, the availability of annotated corpora, such as TimeBank (Pustejovsky et al., 2003), has triggered the use of machinelearning methods for temporal analysis (Mani et al., 2003; Lapata and Lascarides, 2004; Boguraev and Ando, 2005). Typical tasks include identification of temporal anchors, linking events to times, and temporal ordering of events. Since"
W06-1623,N03-2019,0,0.0488939,"Temporal ordering has been extensively studied in computational linguistics (Passonneau, 1988; Webber, 1988; Hwang and Schubert, 1992; Lascarides and Asher, 1993; Lascarides and Oberlander, 1993). Prior research has investigated a variety of language mechanisms and knowledge sources that guide interpretation of temporal ordering, including tense, aspect, temporal adverbials, rhetorical relations and pragmatic constraints. In recent years, the availability of annotated corpora, such as TimeBank (Pustejovsky et al., 2003), has triggered the use of machinelearning methods for temporal analysis (Mani et al., 2003; Lapata and Lascarides, 2004; Boguraev and Ando, 2005). Typical tasks include identification of temporal anchors, linking events to times, and temporal ordering of events. Since this paper addresses temporal ordering, we focus our discussion on this task. Existing ordering approaches vary both in terms of the ordering unit — it can be a clause, a sentence or 1 The transitive reduction of a graph is the smallest graph with the same transitive closure. 190 S4 S2 S3 S7 S5 S10 S8 S11 S1 S2 S5 S7 S8 S13 S6 S1 S9 S12 S13 S14 A 32-year-old woman was admitted to the hospital because of left subcostal"
W06-1623,P87-1001,0,0.0655254,"Missing"
W06-1623,J88-2005,0,0.0954807,"AG). An example of the transitive reduction1 of a TDAG is shown in Figure 1. Edges in a TDAG capture temporal precedence relations between segments. Because the graph encodes an order, cycles are prohibited. We do not require the graph to be fully connected — if the precedence relation between two nodes is not specified in the text, the corresponding nodes will not be connected. For instance, consider the segments S5 and S7 from Figure 1, which describe her previous tests and the history of eczema. Any 2 Related Work Temporal ordering has been extensively studied in computational linguistics (Passonneau, 1988; Webber, 1988; Hwang and Schubert, 1992; Lascarides and Asher, 1993; Lascarides and Oberlander, 1993). Prior research has investigated a variety of language mechanisms and knowledge sources that guide interpretation of temporal ordering, including tense, aspect, temporal adverbials, rhetorical relations and pragmatic constraints. In recent years, the availability of annotated corpora, such as TimeBank (Pustejovsky et al., 2003), has triggered the use of machinelearning methods for temporal analysis (Mani et al., 2003; Lapata and Lascarides, 2004; Boguraev and Ando, 2005). Typical tasks includ"
W06-1623,P87-1021,0,0.270407,"Missing"
W06-1623,J88-2006,0,0.0805967,"the transitive reduction1 of a TDAG is shown in Figure 1. Edges in a TDAG capture temporal precedence relations between segments. Because the graph encodes an order, cycles are prohibited. We do not require the graph to be fully connected — if the precedence relation between two nodes is not specified in the text, the corresponding nodes will not be connected. For instance, consider the segments S5 and S7 from Figure 1, which describe her previous tests and the history of eczema. Any 2 Related Work Temporal ordering has been extensively studied in computational linguistics (Passonneau, 1988; Webber, 1988; Hwang and Schubert, 1992; Lascarides and Asher, 1993; Lascarides and Oberlander, 1993). Prior research has investigated a variety of language mechanisms and knowledge sources that guide interpretation of temporal ordering, including tense, aspect, temporal adverbials, rhetorical relations and pragmatic constraints. In recent years, the availability of annotated corpora, such as TimeBank (Pustejovsky et al., 2003), has triggered the use of machinelearning methods for temporal analysis (Mani et al., 2003; Lapata and Lascarides, 2004; Boguraev and Ando, 2005). Typical tasks include identificati"
W11-0301,P06-1084,0,0.00841042,"agus, 2007; Johnson, 2008), methods for incorporating rich features in unsupervised log-linear models (Poon et al., 2009) and the development of multilingual morphological segmenters (Snyder and Barzilay, 2008a). Our work most closely relates to approaches that aim to incorporate syntactic information into morphological analysis. Surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (Toutanova and Johnson, 2008; Habash and Rambow, 2005; Dasgupta and Ng, 2007; Adler and Elhadad, 2006). Toutanova and Cherry (2009) were the first to systematically study how to incorporate part-of-speech information into lemmatization and empirically demonstrate the benefits of this combination. While our high-level goal is similar, our respective problem formulations are distinct. Toutanova and Cherry (2009) have considered a semi-supervised setting where an initial morpholog2 ical dictionary and tagging lexicon are provided but the model also has access to unlabeled data. Since a lemmatizer and tagger trained in isolation may produce mutually inconsistent assignments, and their method emplo"
W11-0301,D07-1023,0,0.0514554,"al., 2006; Creutz and Lagus, 2007; Johnson, 2008), methods for incorporating rich features in unsupervised log-linear models (Poon et al., 2009) and the development of multilingual morphological segmenters (Snyder and Barzilay, 2008a). Our work most closely relates to approaches that aim to incorporate syntactic information into morphological analysis. Surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (Toutanova and Johnson, 2008; Habash and Rambow, 2005; Dasgupta and Ng, 2007; Adler and Elhadad, 2006). Toutanova and Cherry (2009) were the first to systematically study how to incorporate part-of-speech information into lemmatization and empirically demonstrate the benefits of this combination. While our high-level goal is similar, our respective problem formulations are distinct. Toutanova and Cherry (2009) have considered a semi-supervised setting where an initial morpholog2 ical dictionary and tagging lexicon are provided but the model also has access to unlabeled data. Since a lemmatizer and tagger trained in isolation may produce mutually inconsistent assignmen"
W11-0301,P06-1085,0,0.027478,"w,s, t, W , S, T , L, Θ, θ|γ, α, β) = P (L|γ) (a) P (W , S, T , Θ|L, γ, α) (b) Ppos (w, t, θ|W , S, T , L, α) (c) Pseg (s|W , S, T , L, β, α) (d) where γ, α, Θ, θ, β are hyperparameters and parameters whose roles we shall detail shortly. Our lexicon model captures the desirability of compact lexicon representation proposed by prior work by using parameters γ that favors small lexicons. Furthermore, if we set the number of syntactic categories in the segmentation model to one and exclude the token-based models, we recover a segmenter that is very similar to the unigram Dirichlet Process model (Goldwater et al., 2006; Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b). We shall elaborate on this point in Section 4. The segmentation model captures morphological consistency within syntactic categories (POS tag), whereas the Token-POS model captures POS tag dependencies between adjacent tokens. Lastly, the Token-Seg model encourages consistent segmentations between adjacent tokens that exhibit morphological agreement. Lexicon Model The design goal is to encourage morpheme types to be short and the set of affixes (i.e. prefixes and suffixes) to be much smaller than the set of stems. To achieve this, we f"
W11-0301,P05-1071,0,0.150798,"rmulations (Goldwater et al., 2006; Creutz and Lagus, 2007; Johnson, 2008), methods for incorporating rich features in unsupervised log-linear models (Poon et al., 2009) and the development of multilingual morphological segmenters (Snyder and Barzilay, 2008a). Our work most closely relates to approaches that aim to incorporate syntactic information into morphological analysis. Surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (Toutanova and Johnson, 2008; Habash and Rambow, 2005; Dasgupta and Ng, 2007; Adler and Elhadad, 2006). Toutanova and Cherry (2009) were the first to systematically study how to incorporate part-of-speech information into lemmatization and empirically demonstrate the benefits of this combination. While our high-level goal is similar, our respective problem formulations are distinct. Toutanova and Cherry (2009) have considered a semi-supervised setting where an initial morpholog2 ical dictionary and tagging lexicon are provided but the model also has access to unlabeled data. Since a lemmatizer and tagger trained in isolation may produce mutually"
W11-0301,W08-0704,0,0.24327,"Missing"
W11-0301,N09-1024,0,0.435863,"rdless of the initial state in theory, good initialization heuristics often speed up convergence in practice. We therefore train a series of models of increasing complexity (see section 6 for more details), each with 50 iterations of Gibbs sampling, and use the output of the preceding model to initialize the subsequent model. The initial model is initialized such that all words are not segmented. When POS tags are first introduced, they are initialized uniformly at random. 5 Experimental Setup Performance metrics To enable comparison with previous approaches, we adopt the evaluation set-up of Poon et al. (2009). They evaluate segmentation accuracy on a per token basis, using recall, precision and F1-score computed on segmentation points. We also follow a transductive testing scenario where the same (unlabeled) data is used for both training and testing the model. Data set We evaluate segmentation performance on the Penn Arabic Treebank (ATB).5 It consists of about 4,500 sentences of modern Arabic obtained from newswire articles. Following the preprocessing procedures of Poon et al. (2009) that exclude certain word types (such as abbreviations and digits), we obtain a corpus of 120,000 tokens and 20,"
W11-0301,P08-1084,1,0.949525,"han modeling syntactic categories. Overall, our results demonstrate that incorporating syntactic information is a promising direction for improving morphological analysis. 2 Related Work Research in unsupervised morphological segmentation has gained momentum in recent years bringing about significant developments to the area. These advances include novel Bayesian formulations (Goldwater et al., 2006; Creutz and Lagus, 2007; Johnson, 2008), methods for incorporating rich features in unsupervised log-linear models (Poon et al., 2009) and the development of multilingual morphological segmenters (Snyder and Barzilay, 2008a). Our work most closely relates to approaches that aim to incorporate syntactic information into morphological analysis. Surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (Toutanova and Johnson, 2008; Habash and Rambow, 2005; Dasgupta and Ng, 2007; Adler and Elhadad, 2006). Toutanova and Cherry (2009) were the first to systematically study how to incorporate part-of-speech information into lemmatization and empirically demonstrate the benefits of this co"
W11-0301,P09-1055,0,0.0111548,", methods for incorporating rich features in unsupervised log-linear models (Poon et al., 2009) and the development of multilingual morphological segmenters (Snyder and Barzilay, 2008a). Our work most closely relates to approaches that aim to incorporate syntactic information into morphological analysis. Surprisingly, the research in this area is relatively sparse, despite multiple results that demonstrate the connection between morphology and syntax in the context of part-of-speech tagging (Toutanova and Johnson, 2008; Habash and Rambow, 2005; Dasgupta and Ng, 2007; Adler and Elhadad, 2006). Toutanova and Cherry (2009) were the first to systematically study how to incorporate part-of-speech information into lemmatization and empirically demonstrate the benefits of this combination. While our high-level goal is similar, our respective problem formulations are distinct. Toutanova and Cherry (2009) have considered a semi-supervised setting where an initial morpholog2 ical dictionary and tagging lexicon are provided but the model also has access to unlabeled data. Since a lemmatizer and tagger trained in isolation may produce mutually inconsistent assignments, and their method employs a log-linear reranker to r"
W11-0301,N10-1082,0,\N,Missing
W11-0301,D10-1083,1,\N,Missing
W97-0703,J91-1002,0,\N,Missing
W97-0703,C94-1056,0,\N,Missing
W97-0703,P94-1002,0,\N,Missing
W97-0703,P92-1032,0,\N,Missing
W97-0703,W97-0704,0,\N,Missing
W97-0703,P93-1020,0,\N,Missing
W98-1409,J97-1004,0,0.0135396,"be considered onerous by the expert system developer, appearing unmotivated from the point of view of the core functionality of the system, namely reasoning (as opposed to explanation). Presumably, it is difficult for one and the same person to be a domain expert and a expert on communication in the domain. In the Rex approach, the obvious problem is that in order to generate an explanation, additional reasoning must be performed which in some sense is very similar to that done by the expert 2We do not consider explanation generation from data bases (for example, (McKeown, i985; Paris, 1988; Lester and Porter, 1997)) to be the same problem as expert system reasoning explanation (even though we may use some similar techniques). I n data base explanations, the knowledge to be communicated is static and its representation is given a p r i o r i as p a r t of the statement of the generation problem. In expert system explanations, the knowledge to be explained is generated dynamically, and the proper representation for this knowledge is part of the solution to the problem of expert system exp:anation, not its statement. 79 system itself (e.g., finding causal chains). This is redundant, and does not result in"
W98-1409,J88-3006,0,0.0305365,"S imposes may be considered onerous by the expert system developer, appearing unmotivated from the point of view of the core functionality of the system, namely reasoning (as opposed to explanation). Presumably, it is difficult for one and the same person to be a domain expert and a expert on communication in the domain. In the Rex approach, the obvious problem is that in order to generate an explanation, additional reasoning must be performed which in some sense is very similar to that done by the expert 2We do not consider explanation generation from data bases (for example, (McKeown, i985; Paris, 1988; Lester and Porter, 1997)) to be the same problem as expert system reasoning explanation (even though we may use some similar techniques). I n data base explanations, the knowledge to be communicated is static and its representation is given a p r i o r i as p a r t of the statement of the generation problem. In expert system explanations, the knowledge to be explained is generated dynamically, and the proper representation for this knowledge is part of the solution to the problem of expert system exp:anation, not its statement. 79 system itself (e.g., finding causal chains). This is redundan"
