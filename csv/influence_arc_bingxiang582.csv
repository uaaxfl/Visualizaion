2009.iwslt-papers.2,W06-3123,0,0.227225,"Missing"
2009.iwslt-papers.2,W07-0403,0,0.0643152,"search space of this method is huge, so Birch et - 136 - Proceedings of IWSLT 2009, Tokyo - Japan linking sub-translations generated chunk-by-chunk: X → hX1 X2 , X1 X2 i. (5) X is also the start symbol. All rules in R are paired with statistical parameters (i.e., weighted SCFG), which combines with other features to form the models using a log-linear framework. The decoder tries to maximize: P (D) ∝ PLM (e)λLM × Q Q i Figure 1: Enriching SCFG rules from bilingual chart parsing. al. [5] reduce it by using only concepts that match the highconfidence GIZA++ alignments. Similarly, Cherry and Lin [6] use ITG for pruning. May and Knight [7] use EM algorithm to train tree-tostring rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou [8] use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, wo"
2009.iwslt-papers.2,D07-1038,0,0.0916274,"Birch et - 136 - Proceedings of IWSLT 2009, Tokyo - Japan linking sub-translations generated chunk-by-chunk: X → hX1 X2 , X1 X2 i. (5) X is also the start symbol. All rules in R are paired with statistical parameters (i.e., weighted SCFG), which combines with other features to form the models using a log-linear framework. The decoder tries to maximize: P (D) ∝ PLM (e)λLM × Q Q i Figure 1: Enriching SCFG rules from bilingual chart parsing. al. [5] reduce it by using only concepts that match the highconfidence GIZA++ alignments. Similarly, Cherry and Lin [6] use ITG for pruning. May and Knight [7] use EM algorithm to train tree-tostring rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou [8] use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, word alignments are used to define all rul"
2009.iwslt-papers.2,P06-1121,0,0.107956,"e)λLM × Q Q i Figure 1: Enriching SCFG rules from bilingual chart parsing. al. [5] reduce it by using only concepts that match the highconfidence GIZA++ alignments. Similarly, Cherry and Lin [6] use ITG for pruning. May and Knight [7] use EM algorithm to train tree-tostring rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou [8] use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, word alignments are used to define all rules, our method proposes new rules independently of word alignments. 1.2. Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model [2, 10, 11] based on Synchronous Context-free Grammar. The rules have form X → hγ, α, ∼i, (4) φi (X →&lt; γ, α &gt;)λi , (6) where the set of φi (X →&lt; γ, α &gt;) are features defined over given production rule, and PL"
2009.iwslt-papers.2,P05-1033,0,0.397447,"conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, word alignments are used to define all rules, our method proposes new rules independently of word alignments. 1.2. Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model [2, 10, 11] based on Synchronous Context-free Grammar. The rules have form X → hγ, α, ∼i, (4) φi (X →&lt; γ, α &gt;)λi , (6) where the set of φi (X →&lt; γ, α &gt;) are features defined over given production rule, and PLM (e) is the language model score on hypothesized output, the λi is the feature weight. The baseline model follows Chiang’s hierarchical model [2]: conditional probabilities P (γ|α) and P (α|γ); lexical weights [13] Pw (γ|α) and Pw (α|γ); word counts |e|; rule counts |D|; abstraction penalty (to account for the accumulated number of non-terminals in D); target n-gram language model PLM (e); and the g"
2009.iwslt-papers.2,W08-0403,1,0.916574,"conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, word alignments are used to define all rules, our method proposes new rules independently of word alignments. 1.2. Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model [2, 10, 11] based on Synchronous Context-free Grammar. The rules have form X → hγ, α, ∼i, (4) φi (X →&lt; γ, α &gt;)λi , (6) where the set of φi (X →&lt; γ, α &gt;) are features defined over given production rule, and PLM (e) is the language model score on hypothesized output, the λi is the feature weight. The baseline model follows Chiang’s hierarchical model [2]: conditional probabilities P (γ|α) and P (α|γ); lexical weights [13] Pw (γ|α) and Pw (α|γ); word counts |e|; rule counts |D|; abstraction penalty (to account for the accumulated number of non-terminals in D); target n-gram language model PLM (e); and the g"
2009.iwslt-papers.2,P00-1056,0,0.217366,"r, γ and α are source and target strings with both terminals and nonterminals, subject to the constraint that there is always a oneto-one correspondence ∼ between those non-terminals. The ∼ is often represented by co-indexing corresponding nonterminals. Rules with terminals only are called phrasal rules, while rules with non-terminals are abstract rules. We limit the number of non-terminals in each rule to no more than two, thus ensuring the rank of SCFG is two. The set of rules, denoted as R, are automatically extracted from a parallel corpus [2, 10] with word-alignments obtained from GIZA++ [12]. Finally, an implicit glue rule is embedded with decoder to allow for translations that can be achieved by sequentially X→&lt;γ,α&gt;∈D 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. - 137 - RSpans := precompute(R, e, f ) for i, j, k, l in bottom-up order, such that 1 ≤ i ≤ j ≤ M, 1≤k≤l≤N for ρ ∈ RSpans(i, j, k, l) switch ρ.n case 0: tijkl .push(ρ) case 1: if (f illed(tρ.bp1 )) tijkl .push(ρ) case 2: if (f illed(tρ.bp1 )&f illed(tρ.bp2 )) tijkl .push(ρ) Figure 2: Bilingual chart parser for SCFG. Proceedings of IWSLT 2009, Tokyo - Japan The chart T is a set of cells tijkl such that 1 ≤ i ≤ j ≤ M and 1 ≤ k"
2009.iwslt-papers.2,N03-1017,0,0.0603151,"ed to define all rules, our method proposes new rules independently of word alignments. 1.2. Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model [2, 10, 11] based on Synchronous Context-free Grammar. The rules have form X → hγ, α, ∼i, (4) φi (X →&lt; γ, α &gt;)λi , (6) where the set of φi (X →&lt; γ, α &gt;) are features defined over given production rule, and PLM (e) is the language model score on hypothesized output, the λi is the feature weight. The baseline model follows Chiang’s hierarchical model [2]: conditional probabilities P (γ|α) and P (α|γ); lexical weights [13] Pw (γ|α) and Pw (α|γ); word counts |e|; rule counts |D|; abstraction penalty (to account for the accumulated number of non-terminals in D); target n-gram language model PLM (e); and the glue rule penalty to learn preference of non-terminal rewriting over serial combination through Eq. (5). We note, however, that these parameters are often poorly estimated due to the scarceness of data and the usage of inaccurate heuristics. We try to alleviate this problem by EM training in Section 3. 2. Bilingual chart parsing In this section, we describe the algorithm for bilingual parsing and present our i"
2009.iwslt-papers.2,2001.mtsummit-papers.68,0,0.0430334,"by EM, and uses rule arithmetic to propose rules that better explain the training data. The modular architecture allows for different training setups. In the first setup (EMcosts), only rule probabilities/costs of abstract rules were estimated by 10 iterations of EM. In the second setup (EMpropose), the new rules were proposed after each iteration of EM. In the third setup (EM-propose&costs), the proposed rules were merged with the baseline and ITG rules and the rule costs were estimated by EM. In the following, we are presenting more details about the three setups and present the BLEU scores [14] in the Table 2. 6.3. Baseline The baseline in our experiments is a formal syntax-based translation model [11]. We train GIZA++ [15] word alignments on the sentence-aligned data, and extract phrase pairs using heuristics grow-diag-final [16]. The phrases were up to 6 and 8 words long on the source and target sides, respectively. The method of extracting abstract rules is similar to [2]. The log-linear model combines 9 features, as described in Section 1.2. 6.4. Using BCP and EM to estimate rule costs In the first experiment, we were trying to better estimate features of the baseline abstract r"
2009.iwslt-papers.2,2005.mtsummit-papers.11,0,0.0615178,"istics usually apply constraints, such as limitations of the phrase length or non-terminal span, sometimes too restrictive to extract some good rules. Another reason is the deterministic nature of those heuristics that does not allow to recover from errors in the word alignment. In this work, we learn rules for hierarchical phrase based MT systems directly from the parallel data. The main contribution of this paper is a new method for proposing translation rules which is independent of bilingual word alignments. Let us have an example of a German-English sentence pair from the Europarl corpus [1]. (1) make) are swapped. It would be nice to generate rules that can handle long distance reorderings, still with a reasonably low number of terminals, for example: X → hbesteht darin, is i X → hX1 zu X2 , to X2 X1 i to get the rule (2). Our approach, as shown in Figure 1, consists of bilingual chart parsing (BCP) of the training data, combining rules found in the chart using a rule arithmetic to propose new rules, and using EM to estimate rule probabilities. The paper is structured as follows: In Section 1, we explain our main motivation, summarize previous work, and briefly introduce the for"
2009.iwslt-papers.2,W99-0604,0,0.255074,"t training setups. In the first setup (EMcosts), only rule probabilities/costs of abstract rules were estimated by 10 iterations of EM. In the second setup (EMpropose), the new rules were proposed after each iteration of EM. In the third setup (EM-propose&costs), the proposed rules were merged with the baseline and ITG rules and the rule costs were estimated by EM. In the following, we are presenting more details about the three setups and present the BLEU scores [14] in the Table 2. 6.3. Baseline The baseline in our experiments is a formal syntax-based translation model [11]. We train GIZA++ [15] word alignments on the sentence-aligned data, and extract phrase pairs using heuristics grow-diag-final [16]. The phrases were up to 6 and 8 words long on the source and target sides, respectively. The method of extracting abstract rules is similar to [2]. The log-linear model combines 9 features, as described in Section 1.2. 6.4. Using BCP and EM to estimate rule costs In the first experiment, we were trying to better estimate features of the baseline abstract rules. As discussed in Section 4, to increase the parsability of the corpus we had to provide additional ITG rules. We added all word"
2009.iwslt-papers.2,J07-2003,0,0.279565,"nd for this method, and initial experimental results on German-English translations of Europarl data. 1. Introduction (2) GER: die herausforderung besteht darin diese systeme zu den besten der welt zu machen ENG: the challenge is to make the system the very best We can see that the pairs of long sequences (diese systeme ... der welt, the system ... best) and (zu machen, to X → hbesteht darin X1 zu X2 , is to X2 X1 i, There are 127 sentence pairs out of 300K of the training data that contain this pattern, but this rule was not extracted into the baseline ruleset using the conventional approach [2]: either because of word alignment errors, or because the maximum span for rule extraction is lower than 11 words. We want to learn new rules by combining existing rule usages. Thus we might combine: (3) Statistical machine translation has dramatically improved over the last few decades. Phrase-based and syntax-based systems are probably the most commonly adopted approaches. Although they implement various modeling techniques to improve performance on different languages, domains, or user scenarios, they usually share the same basic pattern for generating rules: starting with word alignments,"
2009.iwslt-papers.2,P07-2045,0,0.00951543,"ed by 10 iterations of EM. In the second setup (EMpropose), the new rules were proposed after each iteration of EM. In the third setup (EM-propose&costs), the proposed rules were merged with the baseline and ITG rules and the rule costs were estimated by EM. In the following, we are presenting more details about the three setups and present the BLEU scores [14] in the Table 2. 6.3. Baseline The baseline in our experiments is a formal syntax-based translation model [11]. We train GIZA++ [15] word alignments on the sentence-aligned data, and extract phrase pairs using heuristics grow-diag-final [16]. The phrases were up to 6 and 8 words long on the source and target sides, respectively. The method of extracting abstract rules is similar to [2]. The log-linear model combines 9 features, as described in Section 1.2. 6.4. Using BCP and EM to estimate rule costs In the first experiment, we were trying to better estimate features of the baseline abstract rules. As discussed in Section 4, to increase the parsability of the corpus we had to provide additional ITG rules. We added all word pairs that had an entry in at least one of the GIZA++ tables of lexical translation probabilities, but were"
2009.iwslt-papers.2,P02-1040,0,\N,Missing
2009.iwslt-papers.2,2006.amta-papers.2,0,\N,Missing
2020.acl-main.413,P19-1620,0,0.0523292,", XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervised reading comprehension consists of three components: a questi"
2020.acl-main.413,N18-2092,0,0.304428,"models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervised reading comprehens"
2020.acl-main.413,Q19-1026,0,0.0532274,"named entity, achieving stateof-the-art performance on SQuAD for unsupervised QA. 1 Figure 1: Question Generation Pipeline: the original context sentence containing a given answer is used as a query to retrieve a related sentence containing matching entities, which is input into our question-style converter to create QA training data. Introduction Question Answering aims to answer a question based on a given knowledge source. Recent advances have driven the performance of QA systems to above or near-human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) thanks to pretrained language models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data au"
2020.acl-main.413,P19-1484,0,0.13018,"escale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervised reading comprehension consists of three components: a question, a context, and an answer. For a given dataset domain, a collection of documents can usually be easily obtained,"
2020.acl-main.413,2021.ccl-1.108,0,0.155927,"Missing"
2020.acl-main.413,D16-1264,0,0.0663435,"nerating questions for (context, answer) pairs but shows little improvement over applying a much simpler question generator which drops, permutates and masks words. We improve upon this paper by proposing a simple, intuitive, retrieval and template-based question generation approach, illustrated in Figure 1. The idea is to retrieve a sentence from the corpus similar to the current context, and then generate a question based on that sentence. Having created a question for all (context, answer) pairs, we then fine-tune a pretrained BERT model on this data and evaluate on the SQuAD v1.1 dataset (Rajpurkar et al., 2016). Our contributions are as follows: we introduce a retrieval, template-based framework which achieves state-of-the-art results on SQuAD for unsupervised models, particularly when the answer is a named entity. We perform ablation studies to determine the effect of components in template question generation. We are releasing our synthetic training data and code.1 2 Unsupervised QA Approach We focus on creating high-quality, non-trivial questions which will allow the model to learn to extract the proper answer from a context-question pair. Sentence Retrieval: A standard cloze question can be obta"
2020.acl-main.413,C18-1073,0,0.163828,"evlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervised reading comprehension consists of thr"
2020.acl-main.413,P17-1096,0,0.0188485,"retrained language models such as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fine-tuning these language models, however, requires largescale data for fine-tuning. Creating a dataset for every new domain is extremely costly and practically infeasible. The ability to apply QA models on outof-domain data in an efficient manner is thus very 1 2 Equal contribution Work done during internship at the AWS AI Labs desirable. This problem may be approached with domain adaptation or transfer learning techniques (Chung et al., 2018) as well as data augmentation (Yang et al., 2017; Dhingra et al., 2018; Wang et al., 2018; Alberti et al., 2019). However, here we expand upon the recently introduced task of unsupervised question answering (Lewis et al., 2019) to examine the extent to which synthetic training data alone can be used to train a QA model. In particular, we focus on the machine reading comprehension setting in which the context is a given paragraph, and the QA model can only access this paragraph to answer a question. Furthermore, we work on extractive QA, where the answer is assumed to be a contiguous sub-string of the context. A training instance for supervi"
2020.acl-main.413,N18-1143,0,\N,Missing
2020.acl-main.413,N19-1423,0,\N,Missing
2020.emnlp-main.134,N19-1423,0,0.0348657,"set to 5 in the case of LU L. When fine-tuning with RLL loss (Eq. 5), we use a batch size of 8. During training, when processing a question we randomly sample 15 negative passages from the set of negative passages of the question. However, only the negative passage with the highest score is used to update the model. Early experiments demonstrated that this strategy performs similarly to the usual pairwise approach. 3.3 Ranking Results In Table 2 we present the experimental results for our proposed generative approach and four state-ofthe-art discriminative baselines, which are based on BERT (Devlin et al., 2019) and BART. Both BERTSel (Li et al., 2019) and BERT-PR (Xu et al., 2019) fine-tuned BERT-base using a ranking loss on the score computed with [CLS] token. We trained a BERT-large model using [CLS]-based scoring + ranking loss (rows 3). We additionally trained a discriminative version of BART-large (row 4) where the input for the encoder and the decoder are the passage and the question, respectively. As it is normally adopted in BART for classification (Lewis et al., 2019), we take the representation generated by the decoder for the last token and use it to create a score by applying a linear la"
2020.emnlp-main.134,P18-1082,0,0.0232167,"Question and Passage Generation A good side effect of using generative models to perform ranking is that we can use the trained model to generate new questions given a passage and vice-versa (depending on the conditioning context used for fine-tuning). This type of synthetically generated data could be used as additional training data to improve discriminative models such as BERT-PR (Xu et al., 2019). In Tables 4 and 5, we present some examples of questions and passages, respectively, that were generated using our fine-tuned GPT2-largeLU L LM. In both cases we use a mixture of top k-sampling (Fan et al., 2018) and nucleus sampling (Holtzman et al., 2019) to generate the samples. Please note that the passages in Table 4 were extracted from the test set and are not present in the training set. The same applies for the question used in Table 5. In Table 4, we can see that the generated questions are very fluent and, for most questions (except for the ones in italic), the input passage contains the answer for the question. In Table 5, we can observe that the generated passages are quite related to the input question. However, the content is normally not factual and contains inconsistencies and some rep"
2020.emnlp-main.134,2020.findings-emnlp.63,0,0.0672782,"ranking). In this task, given an input question and a set of candidate passages, the goal is to rank the candidate passages so that passages containing the correct answer appear at the top of the ranked list. Considerable body of work exists on the use of NNs for this task (Feng et al., 2015; Severyn and Moschitti, 2015; Tan et al., 2016; dos Santos et al., 2016; Rao et al., 2016; Wang et al., 2017), where the most recent ones use BERT-based models that perform discrimination based on the special [CLS] token (Nogueira and Cho, 2019; Li et al., 2019; Xu et al., 2019). A contemporaneous work by Nogueira et al. (2020) also proposes a generative approach for the passage ranking task. However, while their approach decides the relevance of a passage by generating a single keyword (e.g. true or false), our method 1722 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1722–1727, c November 16–20, 2020. 2020 Association for Computational Linguistics Figure 1: Illustration of the inference step of our ranking by generation approach. Each candidate passage ak is ranked based on the likelihood of generating the question q conditioned on the passage, pθ (q|ak ). uses the c"
2020.emnlp-main.134,P15-2114,1,0.817555,"for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR. 1 Introduction Most recent approaches for ranking tasks in Information Retrieval (IR) such as passage ranking and retrieval of semantically related questions have focused primarily on discriminative methods using neural networks that learn a similarity function to compare questions and candidate answers (Severyn and Moschitti, 2015; dos Santos et al., 2015; Tan et al., 2016; Tay et al., 2017, 2018). On the other hand, classical literature on probabilistic models for IR showed that language modeling, a type of simple generative model, can be effective for document ranking (Zhai, 2008; Lafferty and Zhai, 2001; Ponte and Croft, 1998). The key idea consists of first training a unique language model lmi for each candidate document di , then using the likelihood of generating the input query using lmi , denoted by P (q|lmi ), as the ranking score for document di . Recent advances in neural language models (NLMs) have led to impressive improvements in"
2020.emnlp-main.134,D15-1237,0,0.0955737,"Missing"
2020.emnlp-main.27,D19-1539,0,0.272045,"ure 2: Comparison between our generative-style sequence labeling model (top) and the conventional token-level classification model (bottom). late the F1 score for sequence labeling or accuracy for sentence classification. abstractive summarization (Rush et al., 2015), generative question answering (Dong et al., 2019), to name a few. However, the sequence-to-sequence framework is often not a method of choice when it comes to sequence labeling. Most models for sequence labeling use the token-level classification framework, where the model predicts a label for each element in the input sequence (Baevski et al., 2019; Li et al., 2019b; Chen et al., 2019). While select prior work adopts the sequence-to-sequence method for sequence labeling (Chen and Moschitti, 2018), this approach is not widely in use due to the difficulty of fixing the output length, output space, and alignment with the original sequence. Natural Labels Labels are associated to real-world concepts that can be described through natural words. These words have rich information, but are often ignored in traditional discriminative approaches. In contrast, our model naturally incorporate label semantics directly through the generation-asclassi"
2020.emnlp-main.27,C18-1251,0,0.0620944,"Missing"
2020.emnlp-main.27,C18-1185,0,0.0632458,"Missing"
2020.emnlp-main.27,I17-2016,0,0.0225681,"“person” was learned with other uninformative words. 3 Related Work Sequence to sequence learning has various applications including machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), language modeling (Radford et al., 2018; Raffel et al., 2019), Multi-task and multi-domain learning often benefit sequence labeling performance (Changpinyo et al., 2018). The archetypal multi-task setup jointly trains on a target dataset and one or more auxiliary datasets. In the cross lingual setting, these auxiliary datasets typically represent high-resource languages (Schuster et al., 2018; Cotterell and Duh, 2017). While in a monolingual scenario, the auxiliary datasets commonly represent similar, highresource tasks. Examples of similar multi-task pairs include NER and slot labeling (Louvan and Magnini, 2019) as well as dialogue state tracking and language understanding (Rastogi et al., 2018). A recent series of works frame natural language processing tasks, such as translation, question answering, and sentence classification, as conditional sequence generation problems (Raffel et al., 2019; Radford et al., 2019; Brown et al., 2020). By unifying the model output space across tasks to consist of natural"
2020.emnlp-main.27,N19-1423,0,0.173648,"blems (Raffel et al., 2019; Radford et al., 2019; Brown et al., 2020). By unifying the model output space across tasks to consist of natural language symbols, these approaches reduce the gap between language model pre-training tasks and downstream tasks. Moreover, this framework allows acquisition of new tasks without any architectural change. The GPT-3 model (Brown 377 SL/IC Bi-Model (Wang et al., 2018) Joint BERT (Chen et al., 2019) ELMO+BiLSTM (Siddhant et al., 2019) NER Task & Dataset Cloze-CNN (Baevski et al., 2019) BERT-MRC (Li et al., 2019a) BERT-MRC + DSC (Li et al., 2019b) BERT Base (Devlin et al., 2019) Ours: Individual Ours: SNIPS+ATIS Ours: CoNLL+Ontonotes Ours: SNIPS+ATIS+CoNLL+Ontonotes Intent Clas. SNIPS 98.60 99.29 ATIS 98.99 97.50 97.42 Slot Labeling SNIPS 97.00 93.90 99.00 99.29 96.86 97.20 97.43 97.21 99.14 97.08 96.82 ATIS 96.89 96.10 95.62 96.13 95.83 96.65 CoNLL Onto 93.50 93.04 93.33 92.40 91.11 92.07 88.95 90.70 90.24 91.48 91.48 89.52 89.67 Table 1: Results of our models trained on combinations of datasets. Results for Ours: individual are from models trained on a single respective dataset. We underline scores of our models that exceed previous state-of-the-art results in each"
2020.emnlp-main.27,H90-1021,0,0.613214,"Missing"
2020.emnlp-main.27,2020.acl-main.128,0,0.375059,"task, and evaluates on Q. 4.5.2 Few-Shot Baselines TransferBERT trains a token-level classification model by fine-tuning. Matching Net (MN) + BERT Vinyals et al. (2016) Given a word xi , the model classifies by finding the most similar word xSj in the support set and predicts yjS as the label of xi . The model also adapts the backbone model with episodic training. Warm Proto Zero (WPZ) + BERT Fritzler et al. (2019) uses token-level prototypical network (Snell et al., 2017), which classifies by comparing a word xi to each class centroid rather than individual sample embeddings. L-TapNet + CDT Hou et al. (2020) uses a CRF framework and leverages label semantics in representing labels to calculate emission scores and a collapsed dependency transfer method to calculate transition scores. We note that all baselines except for TransferBERT uses episodic meta-training whereas TransferBERT uses fine-tuning. All baseline results are taken from Hou et al. (2020). Our model performs fine-tuning with the generation framework. The major difference between our model and a token-level classification model such as TransferBERT is that we do not require a new classifier for every novel task during the finetuning o"
2020.emnlp-main.27,2021.ccl-1.108,0,0.0958691,"Missing"
2020.emnlp-main.27,W19-5911,0,0.0170383,"15), language modeling (Radford et al., 2018; Raffel et al., 2019), Multi-task and multi-domain learning often benefit sequence labeling performance (Changpinyo et al., 2018). The archetypal multi-task setup jointly trains on a target dataset and one or more auxiliary datasets. In the cross lingual setting, these auxiliary datasets typically represent high-resource languages (Schuster et al., 2018; Cotterell and Duh, 2017). While in a monolingual scenario, the auxiliary datasets commonly represent similar, highresource tasks. Examples of similar multi-task pairs include NER and slot labeling (Louvan and Magnini, 2019) as well as dialogue state tracking and language understanding (Rastogi et al., 2018). A recent series of works frame natural language processing tasks, such as translation, question answering, and sentence classification, as conditional sequence generation problems (Raffel et al., 2019; Radford et al., 2019; Brown et al., 2020). By unifying the model output space across tasks to consist of natural language symbols, these approaches reduce the gap between language model pre-training tasks and downstream tasks. Moreover, this framework allows acquisition of new tasks without any architectural c"
2020.emnlp-main.27,N18-1202,0,0.0303895,"NER on Ontonotes and CoNLL datasets; and slot labeling (SL) and in378 tent classification (IC) on SNIPS and ATIS dialog datasets. For comparison, we provide baseline results from the following models: SL and IC: Bi-Model (Wang et al., 2018) uses two correlated bidirectional LSTMs to perform both IC and SL. Joint BERT (Chen et al., 2019) performs joint IC and SL with a sequential classifier on top of BERT, where the classification for the start-of-sentence token corresponds to intent class. ELMO+Bi-LSTM (Siddhant et al., 2019) uses a Bi-LSTM with CRF as a classifier on top of pre-trained ELMO (Peters et al., 2018). NER: Cloze-CNN (Baevski et al., 2019) finetunes a Bi-LSTM with CRF model Peters et al. (2018) on a pre-trained model with a cloze-style word reconstruction task. BERT MRC (Li et al., 2019a) performs sequence labeling in a question answering model to predict the slot label span. BERT MRC + Dice Loss (Li et al., 2019b) improves upon BERT MRC with a dice loss shown to be suitable for data with imbalanced labels. BERT (Devlin et al., 2019) refers to a token-level classification with BERT pre-trained model. Note that the results for BERT with Ontonotes are from our own implementation. In Table 1,"
2020.emnlp-main.27,W13-3516,0,0.0472795,"Missing"
2020.emnlp-main.27,W18-5045,0,0.0193621,"domain learning often benefit sequence labeling performance (Changpinyo et al., 2018). The archetypal multi-task setup jointly trains on a target dataset and one or more auxiliary datasets. In the cross lingual setting, these auxiliary datasets typically represent high-resource languages (Schuster et al., 2018; Cotterell and Duh, 2017). While in a monolingual scenario, the auxiliary datasets commonly represent similar, highresource tasks. Examples of similar multi-task pairs include NER and slot labeling (Louvan and Magnini, 2019) as well as dialogue state tracking and language understanding (Rastogi et al., 2018). A recent series of works frame natural language processing tasks, such as translation, question answering, and sentence classification, as conditional sequence generation problems (Raffel et al., 2019; Radford et al., 2019; Brown et al., 2020). By unifying the model output space across tasks to consist of natural language symbols, these approaches reduce the gap between language model pre-training tasks and downstream tasks. Moreover, this framework allows acquisition of new tasks without any architectural change. The GPT-3 model (Brown 377 SL/IC Bi-Model (Wang et al., 2018) Joint BERT (Chen"
2020.emnlp-main.27,D15-1044,0,0.0368736,"gs Layer Transformers Task 1: Add Kent James to the Disney soundtrack. O B-artist I-artist O O B-playlist Task 2: He is John Wethy from NBC News O O O Token-level classifier for task 1 B-person Kent James to O B-org I-org Transformers Transformers Add I-person Token-level classifier for task 2 the Disney soundtrack He is John Wethy from NBC News Figure 2: Comparison between our generative-style sequence labeling model (top) and the conventional token-level classification model (bottom). late the F1 score for sequence labeling or accuracy for sentence classification. abstractive summarization (Rush et al., 2015), generative question answering (Dong et al., 2019), to name a few. However, the sequence-to-sequence framework is often not a method of choice when it comes to sequence labeling. Most models for sequence labeling use the token-level classification framework, where the model predicts a label for each element in the input sequence (Baevski et al., 2019; Li et al., 2019b; Chen et al., 2019). While select prior work adopts the sequence-to-sequence method for sequence labeling (Chen and Moschitti, 2018), this approach is not widely in use due to the difficulty of fixing the output length, output s"
2020.emnlp-main.27,W03-0419,0,0.571736,"Missing"
2020.emnlp-main.27,P19-1547,0,0.022385,"tics, in the form of label names such as departure city, example values like San Francisco, and descriptions like “the city from which the user would like to depart on the airline”. Label semantics provide contextual signals that can improve model performance in multi-task and low-resource scenarios. Multiple works show that conditioning input representations on slot description embeddings improves multidomain slot labeling performance (Bapna et al., 2017; Lee and Jha, 2019). Embedding example slot values in addition to slot descriptions yields further improvements in zero-shot slot labeling (Shah et al., 2019). In contrast to our work, these approaches train slot description and slot value embedding matrices, whereas our framework can incorporate these signals as natural language input without changing the network architecture. 4 4.1 Experimental Setup and Results Data Datasets We use popular benchmark data SNIPS (Coucke et al., 2018) and ATIS (Hemphill et al., 1990) for slot labeling and intent classification. SNIPS is an SLU benchmark with 7 intents and 39 distinct types of slots, while ATIS is a benchmark for the air travel domain (see appendix A.4 for details). We also evaluate our approach on"
2020.emnlp-main.27,N18-2050,0,0.0957999,"nderstanding (Rastogi et al., 2018). A recent series of works frame natural language processing tasks, such as translation, question answering, and sentence classification, as conditional sequence generation problems (Raffel et al., 2019; Radford et al., 2019; Brown et al., 2020). By unifying the model output space across tasks to consist of natural language symbols, these approaches reduce the gap between language model pre-training tasks and downstream tasks. Moreover, this framework allows acquisition of new tasks without any architectural change. The GPT-3 model (Brown 377 SL/IC Bi-Model (Wang et al., 2018) Joint BERT (Chen et al., 2019) ELMO+BiLSTM (Siddhant et al., 2019) NER Task & Dataset Cloze-CNN (Baevski et al., 2019) BERT-MRC (Li et al., 2019a) BERT-MRC + DSC (Li et al., 2019b) BERT Base (Devlin et al., 2019) Ours: Individual Ours: SNIPS+ATIS Ours: CoNLL+Ontonotes Ours: SNIPS+ATIS+CoNLL+Ontonotes Intent Clas. SNIPS 98.60 99.29 ATIS 98.99 97.50 97.42 Slot Labeling SNIPS 97.00 93.90 99.00 99.29 96.86 97.20 97.43 97.21 99.14 97.08 96.82 ATIS 96.89 96.10 95.62 96.13 95.83 96.65 CoNLL Onto 93.50 93.04 93.33 92.40 91.11 92.07 88.95 90.70 90.24 91.48 91.48 89.52 89.67 Table 1: Results of our mod"
2020.emnlp-main.439,P19-1620,0,0.0985011,"nding research goal (Mitkov and Ha, 2003; Rus et al., 2010). Although many past works have proposed different strategies for question generation, they have limited or no success in improving the downstream QA task (Du et al., 2017; Sun et al., 2018; Song et al., 2018; Klein and Nabi, 2019; Wang et al., 2020; Ma et al., 2020; Chen et al., 2020; Tuan et al., 2019). Some recent approaches for synthetic QA data generation based on large pretrained language models (LM) have started to demonstrate success in improving the downstream Reading Comprehension (RC) task with automatically generated data (Alberti et al., 2019; Puri et al., 2020). However, these approaches typically consist of multi-stage systems that use three modules: span/answer detector, question generator and question filtering. ∗ *equal contribution. † Siamak Shakeri is currently with Google. The work was done when he was at AWS AI. Given an input passage, the span detector is responsible for extracting spans that will serve as answers for which questions will be generated. This module normally combines a pretrained QA model with handcrafted heuristics. The question generator is a large LM fine-tuned for the task of conditional generation of"
2020.emnlp-main.439,P17-1123,0,0.275737,"or a separate filtering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. 1 Introduction Improving question answering (QA) systems through automatically generated synthetic data is a long standing research goal (Mitkov and Ha, 2003; Rus et al., 2010). Although many past works have proposed different strategies for question generation, they have limited or no success in improving the downstream QA task (Du et al., 2017; Sun et al., 2018; Song et al., 2018; Klein and Nabi, 2019; Wang et al., 2020; Ma et al., 2020; Chen et al., 2020; Tuan et al., 2019). Some recent approaches for synthetic QA data generation based on large pretrained language models (LM) have started to demonstrate success in improving the downstream Reading Comprehension (RC) task with automatically generated data (Alberti et al., 2019; Puri et al., 2020). However, these approaches typically consist of multi-stage systems that use three modules: span/answer detector, question generator and question filtering. ∗ *equal contribution. † Siamak"
2020.emnlp-main.439,D17-1090,0,0.0452821,"for QAGen and QAGen2S because question quality would have a dominant 5448 3 Related Work Question generation (QG) has been extensively studied from the early heuristic-based methods (Mitkov and Ha, 2003; Rus et al., 2010) to the recent neural-base approaches. However, most work (Du et al., 2017; Sun et al., 2018; Zhao et al., 2018; Kumar et al., 2019; Wang et al., 2020; Ma et al., 2020; Tuan et al., 2019; Chen et al., 2020) only takes QG as a stand-alone task, and evaluates the quality of generated questions with either automatic metrics such as BLEU, or human evaluation. Tang et al. (2017), Duan et al. (2017) and Sachan and Xing (2018) verified that generated questions can improve the downstream answer sentence selection tasks. Song et al. (2018) and Klein and Nabi (2019) leveraged QG to augment the training set for machine reading comprehend tasks. However, they only got improvement when only a small amount of human labeled data is available. Recently, with the help of large pre-trained language models, Alberti et al. (2019) and Puri et al. (2020) have been able to improve the performance of RC models using generated questions. However, they need two extra BERT models to identify high-quality ans"
2020.emnlp-main.439,D19-5801,0,0.0761039,"the language model on the target domains. 4 4.1 Experimental Setup and Results Datasets We used SQuAD 1.1 dataset (Rajpurkar et al., 2016) to train the generative models as well as in-domain supervised data for the downstream RC task in this work. We used the default train and dev splits, which contain 87,599 and 10,570 (q, a) pairs, respectively. Similar to (Nishida et al., 2019), we selected the following four datasets as target domains: Natural Questions (Kwiatkowski et al., 2019), which consist of Google search questions and the annotated answers from Wikipedia. We used MRQA Shared Task (Fisch et al., 2019) preprocessed training and dev sets, which consist of 104,071 and 12,836 (q, a) pairs, respectively. The training set passages were used as the unlabeled target domain corpus, while the evaluations were performed on the dev set. NewsQA (Hermann et al., 2015), which consists of question and answer pairs from CNN news articles. We used the dev set from the MRQA Shared Task, which removes unanswerable questions and those without annotator agreement. We prefer this version as we focus only on the generation of answerable questions. The dev set consists of 4,212 (q, a) pairs. Passages from CNN/Dail"
2020.emnlp-main.439,N18-1058,0,0.0167852,"because question quality would have a dominant 5448 3 Related Work Question generation (QG) has been extensively studied from the early heuristic-based methods (Mitkov and Ha, 2003; Rus et al., 2010) to the recent neural-base approaches. However, most work (Du et al., 2017; Sun et al., 2018; Zhao et al., 2018; Kumar et al., 2019; Wang et al., 2020; Ma et al., 2020; Tuan et al., 2019; Chen et al., 2020) only takes QG as a stand-alone task, and evaluates the quality of generated questions with either automatic metrics such as BLEU, or human evaluation. Tang et al. (2017), Duan et al. (2017) and Sachan and Xing (2018) verified that generated questions can improve the downstream answer sentence selection tasks. Song et al. (2018) and Klein and Nabi (2019) leveraged QG to augment the training set for machine reading comprehend tasks. However, they only got improvement when only a small amount of human labeled data is available. Recently, with the help of large pre-trained language models, Alberti et al. (2019) and Puri et al. (2020) have been able to improve the performance of RC models using generated questions. However, they need two extra BERT models to identify high-quality answer spans, and filter out l"
2020.emnlp-main.439,P18-1156,0,0.0454568,"er pairs from CNN news articles. We used the dev set from the MRQA Shared Task, which removes unanswerable questions and those without annotator agreement. We prefer this version as we focus only on the generation of answerable questions. The dev set consists of 4,212 (q, a) pairs. Passages from CNN/Daily Mail corpus of Hermann et al. (2015) are used as unlabeled target domain corpus. BioASQ (Tsatsaronis et al., 2015): we employed MRQA shared task version of BioASQ, which consists of a dev set with 1,504 samples. We collected PubMed abstracts to use as target domain unlabeled passages. DuoRC (Saha et al., 2018) contains questionanswer pairs from movie plots which are extracted from both Wikipedia and IMDB. ParaphraseRC task of DuoRC dataset was used in our evaluations, consisting of 13,111 pairs. We crawled IMDB movie plots to use as the unlabeled target domain corpus. 4.2 Experimental Setup We used Pytorch (Paszke et al., 2019) and Transformers (Wolf et al., 2019) to develop the models and perform experiments. Generative models are trained on SQuAD 1.1 for 5 epochs, and the best model is selected based on the cross entropy loss on the SQuAD dev set. AdamW (Loshchilov and Hutter, 2017) optimizer wit"
2020.emnlp-main.439,N18-2090,1,0.925526,"generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. 1 Introduction Improving question answering (QA) systems through automatically generated synthetic data is a long standing research goal (Mitkov and Ha, 2003; Rus et al., 2010). Although many past works have proposed different strategies for question generation, they have limited or no success in improving the downstream QA task (Du et al., 2017; Sun et al., 2018; Song et al., 2018; Klein and Nabi, 2019; Wang et al., 2020; Ma et al., 2020; Chen et al., 2020; Tuan et al., 2019). Some recent approaches for synthetic QA data generation based on large pretrained language models (LM) have started to demonstrate success in improving the downstream Reading Comprehension (RC) task with automatically generated data (Alberti et al., 2019; Puri et al., 2020). However, these approaches typically consist of multi-stage systems that use three modules: span/answer detector, question generator and question filtering. ∗ *equal contribution. † Siamak Shakeri is currently with Google. The"
2020.emnlp-main.439,D18-1424,0,0.0268579,"ncluded in Appendix B.3. We speculate this is due to average pooling encouraging longer question-answers, which could be of lower quality than shorter question-answer pairs. Where Nq and Na indicate the lengths of generated question and answer, respectively. We use answer-only scores for QAGen and QAGen2S because question quality would have a dominant 5448 3 Related Work Question generation (QG) has been extensively studied from the early heuristic-based methods (Mitkov and Ha, 2003; Rus et al., 2010) to the recent neural-base approaches. However, most work (Du et al., 2017; Sun et al., 2018; Zhao et al., 2018; Kumar et al., 2019; Wang et al., 2020; Ma et al., 2020; Tuan et al., 2019; Chen et al., 2020) only takes QG as a stand-alone task, and evaluates the quality of generated questions with either automatic metrics such as BLEU, or human evaluation. Tang et al. (2017), Duan et al. (2017) and Sachan and Xing (2018) verified that generated questions can improve the downstream answer sentence selection tasks. Song et al. (2018) and Klein and Nabi (2019) leveraged QG to augment the training set for machine reading comprehend tasks. However, they only got improvement when only a small amount of human"
2020.emnlp-main.439,D18-1427,0,0.233325,"tering model. Our generator is trained by finetuning a pretrained LM using maximum likelihood estimation. The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods. 1 Introduction Improving question answering (QA) systems through automatically generated synthetic data is a long standing research goal (Mitkov and Ha, 2003; Rus et al., 2010). Although many past works have proposed different strategies for question generation, they have limited or no success in improving the downstream QA task (Du et al., 2017; Sun et al., 2018; Song et al., 2018; Klein and Nabi, 2019; Wang et al., 2020; Ma et al., 2020; Chen et al., 2020; Tuan et al., 2019). Some recent approaches for synthetic QA data generation based on large pretrained language models (LM) have started to demonstrate success in improving the downstream Reading Comprehension (RC) task with automatically generated data (Alberti et al., 2019; Puri et al., 2020). However, these approaches typically consist of multi-stage systems that use three modules: span/answer detector, question generator and question filtering. ∗ *equal contribution. † Siamak Shakeri is current"
2020.findings-emnlp.298,P19-1285,0,0.0743784,"Missing"
2020.findings-emnlp.298,W18-5446,0,0.0289867,"Missing"
2020.findings-emnlp.298,P19-1441,0,0.129897,"whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative position embedding method is reasonably generalized and robust from the inductive perspective. Finally, we show that our proposed method can be adopted as a near drop-in replacement for improving the accuracy of large models with a small computational budget. 1 Introduction The introduction of BERT (Devlin et al., 2018) has lead to new state-of-the-art results on various downstream tasks such as question answering and passage ranking. Variations of BERT, including RoBERTa (Liu et al., 2019b), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) and T5 (Raffel et al., 2019) have been proposed. At its core, BERT is non-recurrent and based on self-attention; in order to model the dependency between elements at different positions in the sequence, BERT relies on position embeddings. With BERT, the input embeddings are the sum of the token embeddings, segment embeddings, and position embeddings. The position embedding encodes the absolute positions from 1 to maximum sequence length (usually 512). That is, each position has a learnable embedding 1. We argue that the relative position"
2020.findings-emnlp.298,2021.ccl-1.108,0,0.145482,"Missing"
2020.findings-emnlp.298,D16-1244,0,0.0651909,"Missing"
2020.findings-emnlp.298,D16-1264,0,0.0364048,"Missing"
2020.findings-emnlp.298,N18-2074,0,0.0629364,"tion attends to another token at a different position. Recent work suggested removing the next sentence prediction (NSP) loss with training conducted solely on individual chunks of text (Liu et al., 2019a). In this setup, the notion of absolute positions can be arbitrary depending on chunk start positions. Therefore, the association of a token to an absolute position is not well justified. Indeed, what really matters is the relative position or distance between two tokens ti and tj , which is j − i. This phenomena has been realized and the relative position representation has been proposed in Shaw et al. (2018); Huang et al. (2018), in the context of encoder decoder machine translation and music generation respectively. Shaw et al. (2018) has been modified in transformer-XL (Dai et al., 2019) and adopted in XLNet (Yang et al., 2019). The relative position embedding in (Shaw et al., 2018) has been proven to be effective and thus it is adopted in (Raffel et al., 2019; Song et al., 2020). In this paper, we review the absolute position embedding from Devlin et al. (2018) and the relative position embeddings in Shaw et al. (2018); Dai et al. (2019). Our contributions are as follows. Transformer architect"
2020.findings-emnlp.315,Q18-1039,0,0.150352,"very costly (Sun and Saenko, 2014; Vazquez et al., 2013; Stark et al., 2010; Keung et al., 2019). For example, we often have sufficient labeled data for English, while very limited or even no labeled data are available for many other languages. Successfully transferring knowledge learned from the English domain to other languages is of great interest in solving many tasks in natural language processing. Many recent successes in unsupervised domain adaptation have been achieved by learning domain invariant features that are simultaneously being discriminative to the task in the source domain (Chen et al., 2018; Ganin and Lempitsky, 2014; Ganin et al., 2016; Tzeng et al., 2017). Following this line, Keung et al. (2019) propose a language-adversarial training approach for cross-lingual document classification and NER. They leverage the benefit of contextualized word embeddings by using multilingual BERT (Devlin et al., 2019) as the feature generator, and adopt the GAN framework (Goodfellow et al., 2014) to align the features from the two domains. Keung et al. (2019) show significant improvement over the baseline where the pretrained multilingual BERT is finetuned on the English data alone and testing"
2020.findings-emnlp.315,N19-1423,0,0.319014,"other languages is of great interest in solving many tasks in natural language processing. Many recent successes in unsupervised domain adaptation have been achieved by learning domain invariant features that are simultaneously being discriminative to the task in the source domain (Chen et al., 2018; Ganin and Lempitsky, 2014; Ganin et al., 2016; Tzeng et al., 2017). Following this line, Keung et al. (2019) propose a language-adversarial training approach for cross-lingual document classification and NER. They leverage the benefit of contextualized word embeddings by using multilingual BERT (Devlin et al., 2019) as the feature generator, and adopt the GAN framework (Goodfellow et al., 2014) to align the features from the two domains. Keung et al. (2019) show significant improvement over the baseline where the pretrained multilingual BERT is finetuned on the English data alone and testing on the same tasks in other languages. However, Keung et al. (2019), as well as the works mentioned above, are inspired by the pioneering work of Ben-David et al. (2010), which only rigorously studies domain adaptation in the setting of binary classification; there is a lack of theoretical guarantees when it comes to"
2020.findings-emnlp.315,D19-1138,0,0.0229207,"Missing"
2020.findings-emnlp.315,N18-1202,0,0.00882261,"eas the crossentropy loss often leads to poor margins (Liu et al., 2016; Elsayed et al., 2018). To tackle this problem, we augment the cross-entropy loss with Virtual Adversarial Training (VAT) (Miyato et al., 2018). As shown in Zhang et al. (2019a), the local consistency regularization introduced by VAT is capable of promoting large classification margin by optimizing the classification boundary error. This is further demonstrated in Section 4 that the incorporation of VAT leads to remarkable improvement over Zhang et al. (2019b). Although the pretrained language models (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2019) have provided a good foundation for many downstream tasks, to leverage them for unsupervised domain adaptation, we need to tackle the potential overfitting problem, especially when we only have limited labeled data in the source domain but can require many training iterations to minimize the domain discrepancy. As shown in Section 4, VAT can efficiently prevent overfitting in the source domain, and hence significantly improve the generalization in the target domain. This matches the theoretical insights (Ben-David et al., 2010; Zhang et al., 2019b) that the generalizati"
2020.findings-emnlp.315,W03-0419,0,0.120011,"Missing"
2020.findings-emnlp.315,L18-1560,0,0.154376,"ψ(xs + δ)))] . δ;kδk≤ε This term regularizes the predictions being consistent within the ε norm ball of each input. As indicated in Zhang et al. (2019a), the local consistency regularization described in (6) can effectively 3530 promote large margin by optimizing the classification boundary error. As demonstrated in Miyato et al. (2018), the maximization in (6) can be well approximated by a pair of forward- and backwardpropagations. Note that, the input is discrete for the language data, hence we apply VAT to the embedding space and consider the following, tion, where we use the MLDoc corpus (Schwenk and Li, 2018); and named entity recognition, where we use the CoNLL 2002/2003 NER corpus (Tjong Kim Sang, 2002; Sang and De Meulder, 2003). We compare our regularized MDD approach against both Keung et al. (2019) and the baseline. For the baseline, we train the model on the English corpus only, while evaluating on the corpus of the other languages. We also do an ablation study to demone (7) RS strate VAT can yield remarkable performance boost := max KL [σ(f (ψ(e[xs ])))kσ(f (ψ(e[xs ] + δ)))] for all three approaches evaluated in this section. δ;kδk≤ε We implement all three approaches in PyTorch We use e[xs"
2020.findings-emnlp.315,W02-2024,0,0.124814,"input. As indicated in Zhang et al. (2019a), the local consistency regularization described in (6) can effectively 3530 promote large margin by optimizing the classification boundary error. As demonstrated in Miyato et al. (2018), the maximization in (6) can be well approximated by a pair of forward- and backwardpropagations. Note that, the input is discrete for the language data, hence we apply VAT to the embedding space and consider the following, tion, where we use the MLDoc corpus (Schwenk and Li, 2018); and named entity recognition, where we use the CoNLL 2002/2003 NER corpus (Tjong Kim Sang, 2002; Sang and De Meulder, 2003). We compare our regularized MDD approach against both Keung et al. (2019) and the baseline. For the baseline, we train the model on the English corpus only, while evaluating on the corpus of the other languages. We also do an ablation study to demone (7) RS strate VAT can yield remarkable performance boost := max KL [σ(f (ψ(e[xs ])))kσ(f (ψ(e[xs ] + δ)))] for all three approaches evaluated in this section. δ;kδk≤ε We implement all three approaches in PyTorch We use e[xs ] to denote the embedding of the dis(Paszke et al., 2017) with the HuggingFace library crete inp"
2021.acl-long.253,P19-1620,0,0.0606793,"Missing"
2021.acl-long.253,P17-1171,0,0.153306,"our R EFUEL as well as several baseline models. We release source code for our models and experiments at https://github. com/amzn/refuel-open-domain-qa. 1 Figure 1: An example from the A MBIG QA (Min et al., 2020) dataset. The Prompt Question is gathered from Google search queries and has three interpretations upon reading Wikipedia. Disambiguated QA Pairs are the full set of acceptable answers, paired with the disambiguated rewriting of the prompt question. Introduction Open-domain Question Answering (QA) is the task of answering questions using a collection of passages with diverse topics (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020). Open-domain questions are highly likely to be ambiguous because people may not have the knowledge of relevant topics when formulating them. For example, in Figure 1, the prompt question “What’s the most ∗ Work done during an internship at AWS AI. points scored in an NBA game?” is ambiguous because the score in this question could be interpreted as the combined score in a game (Q1 A1 ), score from a single team (Q2 A2 ), or score from an individual player (Q3 A3 ). Therefore, a system needs to adaptively predict a single answer, or a set of equally p"
2021.acl-long.253,N19-1423,0,0.0246729,"n the first prediction pass, which we further refine using a conditional-probability-based filtering approach (Sec. 2.3). 2.1 Passage Retrieval & Reranking We use Dense Passage Retriever (DPR) (Karpukhin et al., 2020) for retrieval. First, we split all Wikipedia pages into 100-token passages, resulting in 24M passages in total. Then DPR maps all passages into d-dimensional vectors, computes the representation of the prompt question, and retrieves N passages whose vectors are closest to the question vector (we use N=1000). After retrieving N passages for the prompt question, we fine-tune BERT (Devlin et al., 2019) to rerank these passages. Taking the concatenation of the prompt question and each passage as input, the reranker allows a token-level cross-attention between the prompt question and passages. The relevance score is then derived by taking the [CLS] vector of the input sequence into a linear layer. After reranking, the QA pair generation model takes the top K passages as inputs (we use K=100). 2.2 Single Pass QA Pair Generation The single pass QA pair generation step includes an Answer Prediction module and a Question Disambiguation module. Firstly, taking the reranked passages and the prompt"
2021.acl-long.253,P17-1147,0,0.0259921,"tors to search for, navigate and read multiple Wikipedia pages to find as many interpretations as possible. As a result, each question is annotated with either a single answer or multiple disambiguated QA pairs, depending on how many interpretations can be found. The train, development, and test (not public) dataset sizes are 10036, 2002, 2004, respectively 1 . On average, there are 2.1 distinct answers per question in A MBIG QA. To test the generalization ability of R EFUEL on any possibly ambiguous questions, we additionally evaluate it on two open-domain QA datasets: NQ- OPEN and TriviaQA (Joshi et al., 2017). Implementation Details are in Appendix A. We release source code for our models and experiments at https: //github.com/amzn/refuel-open-domain-qa. Evaluation Metrics. Let (q1 , a1 ), ..., (qm , am ) be m QA pair predictions, (ˆ q1 , a ˆ1 ), ..., (ˆ qn , a ˆn ) be n gold QA pairs, each predicted QA pair (qi , ai ) is evaluated in order by a correctness score towards all gold QA pairs: ci = 1(ai =ˆ aj )f (qi , qˆj ), where f (qi , qˆj ) is a similarity function for questions. (ˆ qj , a ˆj ) will not be further used to evaluate 1 Leaderboard: https://nlp.cs.washington. edu/ambigqa/leaderboard.h"
2021.acl-long.253,2020.emnlp-main.550,0,0.121133,"ine models. We release source code for our models and experiments at https://github. com/amzn/refuel-open-domain-qa. 1 Figure 1: An example from the A MBIG QA (Min et al., 2020) dataset. The Prompt Question is gathered from Google search queries and has three interpretations upon reading Wikipedia. Disambiguated QA Pairs are the full set of acceptable answers, paired with the disambiguated rewriting of the prompt question. Introduction Open-domain Question Answering (QA) is the task of answering questions using a collection of passages with diverse topics (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020). Open-domain questions are highly likely to be ambiguous because people may not have the knowledge of relevant topics when formulating them. For example, in Figure 1, the prompt question “What’s the most ∗ Work done during an internship at AWS AI. points scored in an NBA game?” is ambiguous because the score in this question could be interpreted as the combined score in a game (Q1 A1 ), score from a single team (Q2 A2 ), or score from an individual player (Q3 A3 ). Therefore, a system needs to adaptively predict a single answer, or a set of equally plausible answers when the question has mult"
2021.acl-long.253,P19-1612,0,0.0714291,"EFUEL (w/o RTP) R EFUEL (w/o RTP) Table 2: Dev. set results of A MBIG QA as a function of the number of retrieval/reranking (N) and QA input (K) passages. #QAs: the average number of predicted QA pairs per prompt question. *: our replicated results. NQ- OPEN Model TriviaQA EM Oracle EM EM Oracle EM ORQA (supervised) HardEM (supervised) DPR (supervised) RAG (supervised) 33.3 28.1 41.5 44.5 - 45.0 50.9 57.9 56.8 - R EFUEL w/o RTP (NFT) R EFUEL (NFT) 35.4 37.3 45.2 48.9 48.2 49.8 52.9 54.3 Table 3: Results on NQ- OPEN and TriviaQA test set. RTP: Round-Trip Prediction. NFT: No Fine-Tuning. ORQA (Lee et al., 2019), HardEM (Min et al., 2019), RAG (Lewis et al., 2020b). Experimental Results Main Results. Performance on the dev. and hidden test set of A MBIG QA is shown in Table 1. Even without having round-trip prediction, R E FUEL (w/o RTP) outperforms S PAN S EQ G EN on both the answer prediction subtask and question disambiguation subtask by a large margin. Moreover, the round-trip prediction indeed further improves the performance by finding more and better QA pairs, going from 1.55 to 1.72 pairs per prompt question on the dev. set. A comprehensive analysis on the round-trip prediction is discussed i"
2021.acl-long.253,2020.acl-main.703,0,0.332659,"tion is ambiguous or not. If multiple answers are predicted, the second subtask, Question Disambiguation, requires generating a disambiguated question for each of the plausible answers. They propose S PAN S EQ G EN, which first retrieves and reranks 3263 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3263–3276 August 1–6, 2021. ©2021 Association for Computational Linguistics passages using the prompt question, and then adopts a BART pre-trained sequence-to-sequence model (Lewis et al., 2020a) to generate all plausible answers, conditioned on the concatenation of the prompt question and top 8 passages. For the question disambiguation subtask, based on BART, they first pre-train a question generation model on NQ- OPEN (Kwiatkowski et al., 2019), a large-scale open-domain QA dataset, to generate the question given the answer and top 8 passages. Then they fine-tune it as a question disambiguation model to generate the disambiguated question conditioned on the prompt question, answer, and passages. There are three main drawbacks to S PAN S E Q G EN . Firstly, a complete coverage of a"
2021.acl-long.253,D19-1284,0,0.0226563,"Missing"
2021.acl-long.253,W19-4805,1,0.890201,"Missing"
2021.acl-long.253,N19-4013,0,0.0371484,"Missing"
2021.acl-long.253,2020.emnlp-main.466,0,0.147729,"s, and then verify and filter out the incorrect questionanswer pairs to arrive at the final disambiguated output. Our model, named R EFUEL, achieves a new state-of-the-art performance on the A MBIG QA dataset, and shows competitive performance on NQ- OPEN and TriviaQA. The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our R EFUEL as well as several baseline models. We release source code for our models and experiments at https://github. com/amzn/refuel-open-domain-qa. 1 Figure 1: An example from the A MBIG QA (Min et al., 2020) dataset. The Prompt Question is gathered from Google search queries and has three interpretations upon reading Wikipedia. Disambiguated QA Pairs are the full set of acceptable answers, paired with the disambiguated rewriting of the prompt question. Introduction Open-domain Question Answering (QA) is the task of answering questions using a collection of passages with diverse topics (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020). Open-domain questions are highly likely to be ambiguous because people may not have the knowledge of relevant topics when formulating them. For example,"
2021.acl-long.253,P02-1040,0,0.109528,"d QA pairs as it is used for (qi , ai ). The overall correctness is calculated by F1 between predictions and references, Pm Pm 2Pf Rf ci i=1 ci Pf = , Rf = i=1 , F1f = . m n Pf + Rf All examples are evaluated for the answer prediction subtask, in which f function always yields 1. This metric is denoted as F1ans (all). For the subset of examples with multiple gold QA pairs, both answer prediction subtask and question disambiguation subtask are evaluated. The answer prediction metric only computed on this subset is denoted as F1ans (multi). To evaluate question disambiguation performance, BLEU (Papineni et al., 2002) and EDIT-F1 is used for the function f , denoted as F1BLEU and F1EDIT-F1 , respectively. EDIT-F1 compute the F1 score of added and deleted unigrams from the prompt question to the predicted disambiguated question towards references. 4.2 N K #QAs F1ans F1EDIT-F1 100 100 100 100 1000 ≈8 ≈8 8 100 100 1.17 1.14 1.42 1.54 1.55 39.7 41.7 44.7 45.4 48.4 7.2 7.1 10.0 10.7 11.2 Model S PAN S EQ G EN S PAN S EQ G EN* R EFUEL (w/o RTP) R EFUEL (w/o RTP) R EFUEL (w/o RTP) Table 2: Dev. set results of A MBIG QA as a function of the number of retrieval/reranking (N) and QA input (K) passages. #QAs: the ave"
2021.acl-long.253,2020.emnlp-main.468,0,0.0395981,"Missing"
2021.acl-long.253,D16-1264,0,0.134347,"Missing"
2021.acl-long.253,2020.emnlp-main.439,1,0.845795,"Missing"
2021.acl-long.315,P17-1171,0,0.0259057,"e is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b) proposed a closed-domain hybrid QA dataset where each table is linked to on average 44 passages. Different from ours, their purpose is to study multi-hop reasoning over both forms of information, and each question is still given the associated table. 3 Related Work Open Domain Question Answering ODQA has been extensively studied recently including extractive models (Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2019; Min et al., 2019; Yang et al., 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Method In this section, we describe our method for hybrid open-domain question answering. It mainly consists of three components: (1) a retrieval system; (2) a joint reranker and (3) a dual Seq2Seq model that uses fusion-in-decoder (Izacard and Grave, 2020) to generate direct answer or SQL query. 4079 Figure 1: The pipeline of our proposed hybrid model. The candidates are retrieved from knowledge source such as Wikipedia inclu"
2021.acl-long.315,2020.findings-emnlp.91,0,0.140003,"Missing"
2021.acl-long.315,P18-1078,0,0.0211319,"ng retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b) proposed a closed-domain hybrid QA dataset where each table is linked to on average 44 passages. Different from ours, their purpose is to study multi-hop reasoning over both forms of information, and each question is still given the associated table. 3 Related Work Open Domain Question Answering ODQA has been extensively studied recently including extractive models (Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2019; Min et al., 2019; Yang et al., 2019) that predict spans from evidence passages, and generative models (Raffel et al., 2020; Method In this section, we describe our method for hybrid open-domain question answering. It mainly consists of three components: (1) a retrieval system; (2) a joint reranker and (3) a dual Seq2Seq model that uses fusion-in-decoder (Izacard and Grave, 2020) to generate direct answer or SQL query. 4079 Figure 1: The pipeline of our proposed hybrid model. The candidates are retrieved from knowledge source such as Wikipedia including both paragraphs and"
2021.acl-long.315,N19-1423,0,0.0365709,". 3.2 Joint Reranking The purpose of our reranking model is to produce a score si of how relevant a candidate (either an unstructured passage or table) is to a question. Specifically, the reranker input is the concatenation of question, a retrieved candidate-content, and its corresponding title if available2 , separated by special tokens shown in Figure 1. The candidate content can be either the unstructured 2 Wikipedia passages have page titles, and tables have table titles. text or flattened table. We use BERTbase model in this paper. Following Nogueira and Cho (2019), we finetune the BERT (Devlin et al., 2019) model using the following loss: X X L= log(si ) log(1 si ). (1) i2Ipos i2Ineg The Ipos is sampled from all relevant BM25 candidates, and the set Ineg is sampled from all non-relevant BM25 candidates. Different from Nogueira and Cho (2019), during training, for each question, we sample 64 candidates including one positive candidate and 63 negative candidates, that is, |Ipos |= 1 and |Ineg |= 63. If none of the 200 candidates is relevant, we skip the question. During inference, we use the hybrid reranker to assign a score to each of the 200 candidates, and choose the top 50 candidates as the in"
2021.acl-long.315,P19-1444,0,0.110496,"reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and generate either direct an"
2021.acl-long.315,2020.acl-main.398,0,0.0322855,"which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover,"
2021.acl-long.315,2020.emnlp-main.550,0,0.0848972,"Missing"
2021.acl-long.315,Q19-1026,0,0.157349,"s and the 11th International Joint Conference on Natural Language Processing, pages 4078–4088 August 1–6, 2021. ©2021 Association for Computational Linguistics the question, and finally we use a fusion-in-decoder model (Izacard and Grave, 2020) for our readerparser, which takes all the reranked candidates in addition to the question to generate direct answers or SQL queries. To evaluate the effectiveness of our D U R E PA, we construct a hybrid dataset that combines SQuAD (Rajpurkar et al., 2016) and WikiSQL (Zhong et al., 2017) questions. We also conduct experiments on NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and OTT-QA (Chen et al., 2020a) to evaluate DuRePa performance. As textual and tabular open-domain knowledge, we used textual and tabular data from Wikipedia via Wikidumps (from Dec. 21, 2016) and Wikitables (Bhagavatula et al., 2015). We study the model performance on different kinds of questions, where some of them only need one supporting evidence type while others need both textual and tabular evidence. On all question types, D U R E PA performs significantly better than baseline models that were trained on a single evidence type. We also demonstrate that D U R E PA can generate humaninte"
2021.acl-long.315,W16-0105,0,0.0296435,"d SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and"
2021.acl-long.315,D19-1284,0,0.055593,"rectly generate the answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQ"
2021.acl-long.315,2020.emnlp-main.466,0,0.0494121,"QA datasets, the hybrid methods consistently outperforms the baseline models that only take homogeneous input by a large margin. Specifically we achieve state-of-theart performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning. 1 Introduction Open-domain question answering (ODQA) is a task to answer factoid questions without a prespecified domain. Recently, generative models (Roberts et al., 2020; Lewis et al., 2020; Min et al., 2020; Izacard and Grave, 2020) have achieved the state-of-the-art performance on many ODQA tasks. These approaches all share the common pipeline where the first stage is retrieving evidence from the free-form text in Wikipedia. However, a large amount of world’s knowledge is not stored as plain text but in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured"
2021.acl-long.315,D16-1264,0,0.309104,"AlexanderYogurt/Hybrid-Open-QA 4078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4078–4088 August 1–6, 2021. ©2021 Association for Computational Linguistics the question, and finally we use a fusion-in-decoder model (Izacard and Grave, 2020) for our readerparser, which takes all the reranked candidates in addition to the question to generate direct answers or SQL queries. To evaluate the effectiveness of our D U R E PA, we construct a hybrid dataset that combines SQuAD (Rajpurkar et al., 2016) and WikiSQL (Zhong et al., 2017) questions. We also conduct experiments on NaturalQuestions (NQ) (Kwiatkowski et al., 2019) and OTT-QA (Chen et al., 2020a) to evaluate DuRePa performance. As textual and tabular open-domain knowledge, we used textual and tabular data from Wikipedia via Wikidumps (from Dec. 21, 2016) and Wikitables (Bhagavatula et al., 2015). We study the model performance on different kinds of questions, where some of them only need one supporting evidence type while others need both textual and tabular evidence. On all question types, D U R E PA performs significantly better"
2021.acl-long.315,2020.emnlp-main.437,0,0.0258695,"Missing"
2021.acl-long.315,2020.acl-main.677,0,0.0265935,"e answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In additio"
2021.acl-long.315,D19-1599,1,0.892273,"Missing"
2021.acl-long.315,N19-4013,0,0.0351031,"Missing"
2021.acl-long.315,2020.acl-main.745,0,0.0660488,"29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g. their fusion-retriever is pretrained using hyperlinks between tables and paragraphs), whereas we don’t use any link information between tables and passages. Moreover, Chen et al. (2020b)"
2021.acl-long.315,N18-2093,0,0.144196,"at require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and genera"
2021.acl-long.315,D18-1193,0,0.122604,"at require complex reasoning, as well as offering full explainability. In practice, an ideal ODQA model should be able to retrieve evidence from both unstructured textual and structured tabular information sources, as some questions are better answered by tabular evidence from databases. For example, the current state-of-the-art ODQA models struggle on questions that involve aggregation operations such as counting or averaging. One line of research on accessing databases, although not open domain, is translating natural language questions into SQL queries (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018c; Guo et al., 2019; Wang et al., 2018a, 2020; Yu et al., 2018a; Guo and Gao, 2019; Choi et al., 2020). These methods all rely on knowing the associated table for each question in advance, and hence are not trivially applicable to the open-domain setting, where the relevant evidence might come from millions of tables. In this paper, we provide a solution to the aforementioned problem by empowering the current generative ODQA models with the Text2SQL ability. More specifically, we propose a dual readerparser (D U R E PA) framework that can take both textual and tabular data as input, and genera"
2021.acl-long.315,D19-1204,0,0.0281337,"ns that require complex reasoning in the ODQA setting. 2 Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020) that directly generate the answers. Wang et al. (2018b,c); Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA"
2021.acl-long.315,D18-1425,0,0.0537764,"Missing"
2021.acl-long.315,D19-1537,0,0.0277089,"Missing"
2021.acl-long.315,2020.emnlp-main.558,0,0.027544,"sages to get higher top-n recall. Table Parsing Text2SQL is a task to translate natural questions to executable SQL queries. Brad et al. (2017) proposed SENLIDB dataset which only contains 29 tables and lacks annotation in their training set. Recently, with datasets like WikiSQL (Zhong et al., 2017), Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; Wang et al., 2020; Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020). Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020). However, they are all closed-domain and each question is given the associated table. Hybrid QA Chen et al. (2020a) also proposed an open-domain QA problem with textual and tabular evidence. Unlike our problem, they generate an answer directly from the tabular evidence instead of generating an SQL query. In addition, they assume some contextual information about table is available during retrieval stage (e.g"
2021.acl-long.536,N19-4009,0,0.0292469,"e - each summary sentence usually corresponds to an existing sentence in the input document. Evaluation metrics: We use the ROUGE (Lin, 2004) to measure general summarizaiton quality. For factual consistency, we use the QAGS protocol (see Appendix for more details) as well as the FactCC model (Kry´sci´nski et al., 2019) downloaded directly from the official website.4 In contrast to QAGS, FactCC is a BERT-based classification model that makes a binary prediction if the given claim sentence is factually consistent or not with the given input document. Implementation details: We use the Fairseq (Ott et al., 2019) implementation of BART-large (Lewis et al., 2019) for the summarization model as it is shown to achieve the state-of-the-art ROUGE scores for this task. We fine-tune the BART-large model with the standard learning rate of 3 × 10−5 4 https://github.com/salesforce/factCC Figure 3: Correlation between Q UALS and QAGS on XSUM (left) and CNNDM (right). The average QAGS tend to increase with the increase in Q UALS. on XSUM and CNNDM respectively to establish the MLE baselines. We then initialize C ON S EQ with the MLE baseline models. In C ON S EQ we use a learning rate of 3 × 10−6 . For evaluation"
2021.acl-long.536,2020.emnlp-main.439,1,0.828378,"Missing"
2021.acl-long.536,W17-2623,0,0.0355424,"Missing"
2021.acl-long.536,2020.acl-main.450,0,0.134957,"rnational Joint Conference on Natural Language Processing, pages 6881–6894 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 2: QAGen model: for an input text (p), it generates a question (q) followed by an answer (a). Figure 1: Comparison between QAGS (top) and Q UALS (bottom) protocols. Q UALS uses only one QAGen model instead of the AE, QG and QA models used in QAGS. factualness. Our main contributions lie in both areas. First, we propose an efficient automatic evaluation metric for factual consistency that is a simplification of the recently published QAGS protocol (Wang et al., 2020). Evaluating QAGS is computationally expensive and ill-suited for being part of the model training process. Our proposed protocol achieves a 55x speedup while correlating closely with QAGS1 . Second, we propose a new contrastive learning method that uses factualness as a training objective. We demonstrate through experiments that our method improves the factual consistency of summarization models measured by both automatic metrics such as QAGS as well as human evaluation. 2 An Efficient Metric for Factual Consistency In order to improve factual consistency of summarization models, we must have"
2021.acl-long.536,P18-2124,0,0.113053,"Missing"
2021.acl-long.536,D16-1264,0,0.104627,"Missing"
2021.eacl-main.235,N19-1423,0,0.0139778,"opose a set of simple metrics to quantify factual consistency at the entitylevel. We analyze the factual quality of summaries produced by the state-of-the-art BART model (Lewis et al., 2019) on three news datasets. We then propose several techniques including data filtering, multi-task learning and joint sequence generation to improve performance on these metrics. We leave the relation level consistency to future work. 2 Related work Large transformer-based neural architectures combined with pre-training have set new records across many natural language processing tasks (Vaswani et al., 2017; Devlin et al., 2019; 2727 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2727–2733 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Radford et al., 2019). In particular, the BART model (Lewis et al., 2019) has shown superior performance in many text generation tasks including abstractive summarization. In contrast to encoder-only pre-training such as in BERT (Devlin et al., 2019) or decoder-only pre-training such as in GPT-2 (Radford et al., 2019), BART is an encoder-decoder transformer-based neural translation model jointly"
2021.eacl-main.235,W18-2706,0,0.0232806,"uthors in (Kry´sci´nski et al., 2019) proposed to train a neural network model to classify if a summary is factually consistent with a given source document, similar to a natural language inference task. In the dialogue generation setting, authors in (Li et al., 2019) proposed using unlikelihood to surpress logically inconsistent responses. Our work is complementary to such existing approaches as we focus on simple entity-level metrics to quantify and improve factual consistency. Our goal of improving entity-level metrics of summaries is also related to controllable abstractive summarization (Fan et al., 2018), where a list of named-entities that a user wants to see in the summary can be passed as input to influence the generated summary. In contrast, our goal is to predict which entities are summary-worthy while generating the summary that contains them. In this view we are trying to solve a more challenging problem. 3 Entity-level factual consistency metrics We propose three new metrics that rely on off-theshelf tools to perform Named-Entity Recognition (NER). 1 We use N (t) and N (h) to denote the number of named-entities in the target (gold summary) and hypothesis (generated summary), respectiv"
2021.eacl-main.235,W19-8665,0,0.0630315,"Missing"
2021.eacl-main.235,N18-1065,0,0.0448655,"Missing"
2021.eacl-main.235,D19-1051,0,0.0373461,"Missing"
2021.eacl-main.235,2020.acl-main.703,0,0.032094,"Missing"
2021.eacl-main.235,K16-1028,1,0.883013,"Missing"
2021.eacl-main.235,D18-1206,0,0.045514,"Missing"
2021.eacl-main.235,N19-4009,0,0.0147707,"rthy named-entities, followed by a special token, and then the summary. We call this approach JAENS (Join sAlient ENtity and Summary generation). Similar to the multitask learning approach discussed earlier, JAENS encourages the model to jointly learn to identify the summary-worthy named-entities while learning to generate summaries. Since the decoder generates the salient named-entities first, the summaries that JAENS generate can further attend to these salient named-entities through decoder self-attention. 6 Experiment results We use the pre-trained BART-large model in the Fairseq library (Ott et al., 2019) to fine-tune on the 3 summarization datasets.3 The appendix contains additional details of experimental setup. In Table 3, we show the effect of the entitybased data filtering. For each dataset, we train two separate models: using the training data before and after entity-based data filtering as shown in Table 2. We evaluate both models on the “clean” test set after entity-based data 3 Our code is available at https://github.com/ amazon-research/fact-check-summarization 2729 train Newsroom val test train CNNDM val test train XSUM val test original 922,500 (1.58) 100,968 (1.60) 100,933 (1.59)"
2021.eacl-main.235,P19-1363,0,0.0279786,"the BART model (Lewis et al., 2019) has shown superior performance in many text generation tasks including abstractive summarization. In contrast to encoder-only pre-training such as in BERT (Devlin et al., 2019) or decoder-only pre-training such as in GPT-2 (Radford et al., 2019), BART is an encoder-decoder transformer-based neural translation model jointly pre-trained to reconstruct corrupted input sequences of text. Several authors have pointed out the problem of factual inconsistency in abstractive summarization models (Kryscinski et al., 2019; Kry´sci´nski et al., 2019; Cao et al., 2018; Welleck et al., 2019). The authors in (Kry´sci´nski et al., 2019) proposed to train a neural network model to classify if a summary is factually consistent with a given source document, similar to a natural language inference task. In the dialogue generation setting, authors in (Li et al., 2019) proposed using unlikelihood to surpress logically inconsistent responses. Our work is complementary to such existing approaches as we focus on simple entity-level metrics to quantify and improve factual consistency. Our goal of improving entity-level metrics of summaries is also related to controllable abstractive summariz"
2021.eacl-main.235,2020.emnlp-demos.6,0,0.0351401,"Missing"
2021.eacl-main.26,C16-1236,0,0.015436,"ge-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013;"
2021.eacl-main.26,D13-1160,0,0.365824,"e availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Be"
2021.eacl-main.26,D14-1067,0,0.447343,"ultitask learning, the unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment cente"
2021.eacl-main.26,P13-1042,0,0.0198489,"objective for the selected mini-batch. 6 Experiments We evaluate the effectiveness of our model on standard benchmarks in this section. We first conduct experiments on each sub-task with a separate BERT model in Section 6.2, 6.3 and 6.4, then evaluate the influence of sharing a BERT encoder for all three models in Section 6.5. Finally, we benchmark our method on full Freebase in Section 6.6. 6.1 Datasets and Basic Settings We evaluate our proposed model on two large-scale benchmarks: SimpleQuestions and FreebaseQA. Other existing datasets, such as WebQuestions (Berant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of questions which is even less than the number of relation types in Freebase. SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largest 351 KBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase. We follow the official train/dev/test split. To fairly compare with previous work, we"
2021.eacl-main.26,P17-1171,0,0.0234016,"vious methods usually narrow down the search space based on some heuristic rules. For example, Yih et al. (2015) and Wu et al. (2019) used keyword search to collect all nodes that have one alias exactly matching the topic entity, and Yin et al. (2016) collected all nodes that have at least one word overlapping with the topic entity. Once a smaller set of candidates is selected, complicated neural networks can be utilized to compute the similarity between a candidate node and the topic entity in the question context. Inspired from the recent success of question answering over free-text corpus (Chen et al., 2017; Wang et al., 2018, 2019), we propose a retrieveand-rerank method to solve the entity linking task in two steps. In the first retrieval step, we create an inverted index for all entity nodes, where each node is represented with all tokens from its aliases and description. Then, we use the topic entity t as a query to retrieve top-K candidate nodes from the index with the TF-IDF algorithm2 . The similar method is also used by Vakulenko et al. (2019) and Nedelchev et al. (2020). This information retrieval (IR) method is better than previous work in the following ways. First, our method can find"
2021.eacl-main.26,N19-1299,0,0.0277876,"Missing"
2021.eacl-main.26,N19-1031,0,0.0622289,"competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to deriv"
2021.eacl-main.26,P16-1076,0,0.123955,"ains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g."
2021.eacl-main.26,N19-1423,0,0.189986,"cy (TF-IDF) is a ranking function usually used together with an inverted index to estimate the relevance of documents to a given search query (Sch¨utze et al., 2008). P r(a|Q, K) = P r(t, e, r|Q, K) = Pt (t|Q, K)Pl (e|t, Q, K) Pr (r|e, t, Q, K) (2) 4 where Pt (t|Q, K) is the model for topic entity detection, Pl (e|t, Q, K) models the entity linking process, and Pr (r|e, t, Q, K) is the component for relation detection stage. We will discuss how to parameterize these components in Section 4. 3 Background We briefly introduce some background required by the following sections. BERT: BERT model (Devlin et al., 2019) follows the multi-head self-attention architecture (Vaswani et al., 2017), and is pre-trained with a masked language modeling objective on a largescale text corpus. It has achieved state-of-the-art performance on a bunch of textual tasks. Specifically, for semantic matching tasks, BERT simply concatenates two textual sequences together, and encodes the new sequence with multiple selfattention layers. Then, the output vector of the first token is fed into a linear layer to compute the similarity score between the two input textual sequences. Freebase: We take Freebase (Bollacker et al., 2008)"
2021.eacl-main.26,P15-1026,0,0.340705,"e unified model obtains further improvements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and"
2021.eacl-main.26,C18-1277,0,0.026732,"Missing"
2021.eacl-main.26,P17-1021,0,0.0175203,"vements with only 1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillme"
2021.eacl-main.26,D16-1166,0,0.0199712,", we describe how to parameterize Pt , Pl and Pr in Equation (2). 4.1 Topic Entity Detection Model Pt The goal of a topic entity detection model Pt (t|Q, K) is to identify a topic entity t that the question Q is asking about, where t is usually a substring of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advantage of this method is that no machine learning models need to be involved. However, the drawbacks include: (1) topic entities need to have the exact same surface strings as they occur in KB, and (2) memory-efficient data structures need to be designed to load the massive gazetteer into memory (Yao, 2015). Other approaches leverage a sequence labeling model to tag consecutive tokens in the question Q as topic entities (Dai et al., 2016; Bordes et al., 2015; Mohammed et al., 2018; Wu et al., 2019). This approach is able to predict more precise topic entities, thus p"
2021.eacl-main.26,N19-1028,0,0.107931,"nd tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic entity is linked to an entity node (m.04wxy8) from a list of candidate nodes, and finally the relation book.written work.author is selected as the relation-chain leading to the final answer. Previous methods usually worked on a subset of KB in order t"
2021.eacl-main.26,P17-1147,0,0.0423839,"ritten by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase. We follow the official train/dev/test split. To fairly compare with previous work, we leverage the released FB2M subset of Freebase as the back-end KB for this dataset. FB2M includes 2M entities and 5k relation types between these entities. FreebaseQA: FreebaseQA dataset (Jiang et al., 2019) is a large-scale dataset with 28K unique opendomain factoid questions which are collected from triviaQA dataset (Joshi et al., 2017) and other trivia websites. Each question can be answered by a 1hop or 2-hop relation-chain from Freebase. All questions have been matched to subject-predicateobject triples in Freebase, and verified by human annotators. Comparing with other KBQA datasets, FreebaseQA provides more linguistically sophisticated questions, because all questions are created independently from Freebase. FreebaseQA also released a new subset of Freebase, which includes 16M unique entities, and 182M triples. We follow the official train/dev/test split, and take the Freebase subset as the back-end KB for this dataset."
2021.eacl-main.26,D18-1242,0,0.0181055,"ificant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 201"
2021.eacl-main.26,N18-2047,0,0.196746,"1/3 of the original parameters. Our final model achieves competitive results on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon"
2021.eacl-main.26,D18-1051,0,0.0117971,"on the SimpleQuestions dataset and superior performance on the FreebaseQA dataset. 1 Introduction Answering natural language questions by searching over large-scale knowledge bases (KBQA) is highly demanded by real-life applications, such as Google Assistant, Siri, and Alexa. Owing to the availability of large-scale KBs, significant advancements have been made over the years. One main research direction views KBQA as a semantic matching task (Bordes et al., 2014; Dong et al., 2015; Dai et al., 2016; Hao et al., 2017; Mohammed et al., 2018; Yu et al., 2018; Wu et al., 2019; Chen et al., 2019a; Petrochuk and Zettlemoyer, 2018), and finds a relation-chain within KBs that is most similar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-te"
2021.eacl-main.26,D16-1264,0,0.0461778,"llows the same architecture for the named en4 https://www.elastic.co/products/elasticsearch Models SimpleQ. EM F1 FreebaseQA EM F1 BIO Start/End Pt 94.9 96.4 97.3 97.8 65.1 74.3 75.2 81.5 Multi-task Pt 96.0 97.7 70.6 79.3 Table 1: Results for topic entity detection. tity recognition (NER) task in Devlin et al. (2019), where we use BIO schema to annotate each question token. Since the sequence labeling method may predict multiple spans to be topic entities, we choose the span with the maximum average token score as the final prediction. We employ the metrics exact match (EM) and F1 proposed in Rajpurkar et al. (2016) to evaluate the identified topic entities. Experimental results are shown in Table 1. We can see that our Start/End prediction model works better than the BIO sequence labeling baseline. Specifically, in FreebaseQA dataset, since the questions are longer and more complicated, our Start/End model outperforms the BIO sequence labeling model by a large margin. 6.3 Entity Linking Experiments We retrieve a list of candidate nodes for each question as follows. For questions in the training sets, we use the ground-truth topic entity as the query to retrieve top-100 candidate nodes. For questions in"
2021.eacl-main.26,D19-1242,0,0.015577,"e the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic entity is linked to an entity node (m.04wxy"
2021.eacl-main.26,D18-1455,0,0.0195831,"stion in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic enti"
2021.eacl-main.26,D19-1599,1,0.867436,"Missing"
2021.eacl-main.26,P19-1417,0,0.0159766,"semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, then the topic entity is linked to an e"
2021.eacl-main.26,P16-1220,0,0.0137009,"imilar to the question in a common semantic space, where the relation-chain can be 1-hop, 2-hop or multi-hop (Chen et al., 2019b). Another research direction formulates KBQA as a semantic parsing task (Berant et al., 2013; Bao et al., 2016; Luo et al., 2018), and tackles questions that involve complex reasoning, such as ordinal (e.g. What is the second largest fulfillment center of Amazon?), and aggregation (e.g. How many fulfillment centers does Amazon have?). Most recently, some studies proposed to derive answers from both KBs and free-text corpus to deal with the low-coverage issue of KBs (Xu et al., 2016; Sun et al., 2018; Xiong et al., 2019; Sun et al., 2019). In this paper, we follow the first research direction since the relationchain type of questions counts the vast majority of real-life questions (Berant et al., 2013; Bordes et al., 2015; Jiang et al., 2019). Previous semantic matching methods for KBQA usually decompose the task into sequential subtasks consisting of topic entity detection, entity linking, and relation detection. For example in Figure 1, given the question “Who wrote the book Beau Geste?”, a KBQA system first identifies the topic entity “Beau Geste” from the question, t"
2021.eacl-main.26,N15-3014,0,0.0236683,"his section, we describe how to parameterize Pt , Pl and Pr in Equation (2). 4.1 Topic Entity Detection Model Pt The goal of a topic entity detection model Pt (t|Q, K) is to identify a topic entity t that the question Q is asking about, where t is usually a substring of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advantage of this method is that no machine learning models need to be involved. However, the drawbacks include: (1) topic entities need to have the exact same surface strings as they occur in KB, and (2) memory-efficient data structures need to be designed to load the massive gazetteer into memory (Yao, 2015). Other approaches leverage a sequence labeling model to tag consecutive tokens in the question Q as topic entities (Dai et al., 2016; Bordes et al., 2015; Mohammed et al., 2018; Wu et al., 2019). This approach is able to predict more precise to"
2021.eacl-main.26,P15-1128,0,0.515219,"king for KBQA In this section, we describe how to parameterize Pt , Pl and Pr in Equation (2). 4.1 Topic Entity Detection Model Pt The goal of a topic entity detection model Pt (t|Q, K) is to identify a topic entity t that the question Q is asking about, where t is usually a substring of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advantage of this method is that no machine learning models need to be involved. However, the drawbacks include: (1) topic entities need to have the exact same surface strings as they occur in KB, and (2) memory-efficient data structures need to be designed to load the massive gazetteer into memory (Yao, 2015). Other approaches leverage a sequence labeling model to tag consecutive tokens in the question Q as topic entities (Dai et al., 2016; Bordes et al., 2015; Mohammed et al., 2018; Wu et al., 2019). This approach is able to predict more"
2021.eacl-main.26,P16-2033,0,0.0140787,"batch. 6 Experiments We evaluate the effectiveness of our model on standard benchmarks in this section. We first conduct experiments on each sub-task with a separate BERT model in Section 6.2, 6.3 and 6.4, then evaluate the influence of sharing a BERT encoder for all three models in Section 6.5. Finally, we benchmark our method on full Freebase in Section 6.6. 6.1 Datasets and Basic Settings We evaluate our proposed model on two large-scale benchmarks: SimpleQuestions and FreebaseQA. Other existing datasets, such as WebQuestions (Berant et al., 2013), Free917 (Cai and Yates, 2013) and WebQSP (Yih et al., 2016), are not considered, because they only contain few thousands of questions which is even less than the number of relation types in Freebase. SimpleQuestions: The SimpleQuestions dataset (Bordes et al., 2015) is so far the largest 351 KBQA dataset. It consists of 108,442 English questions written by human annotators, and all questions can be answered by 1-hop relation chains in Freebase. Each question is annotated with a gold-standard subject-relation-object triple from Freebase. We follow the official train/dev/test split. To fairly compare with previous work, we leverage the released FB2M sub"
2021.eacl-main.26,C16-1164,1,0.879983,"Missing"
2021.eacl-main.26,P17-1053,1,0.921241,"parameterize Pt , Pl and Pr in Equation (2). 4.1 Topic Entity Detection Model Pt The goal of a topic entity detection model Pt (t|Q, K) is to identify a topic entity t that the question Q is asking about, where t is usually a substring of Q. Previous approaches for this task can be categorized into two types: (1) rule-based and (2) sequence labeling. The rule-based approaches take all entity names and their alias from a KB as a gazetteer, and n-grams of the question that exactly match with an entry in the gazetteer are taken as topic entities (Yih et al., 2015; Yao, 2015; He and Golub, 2016; Yu et al., 2017). The advantage of this method is that no machine learning models need to be involved. However, the drawbacks include: (1) topic entities need to have the exact same surface strings as they occur in KB, and (2) memory-efficient data structures need to be designed to load the massive gazetteer into memory (Yao, 2015). Other approaches leverage a sequence labeling model to tag consecutive tokens in the question Q as topic entities (Dai et al., 2016; Bordes et al., 2015; Mohammed et al., 2018; Wu et al., 2019). This approach is able to predict more precise topic entities, thus prunes some unimpor"
2021.emnlp-main.467,C02-1150,0,0.244717,"oup as the anchor. As a consequence, a solely distance-based sampling approach can induce certain false negatives and hurt the performance. To tackle this issue, leveraging proper structure assumption or domain-specific knowledge could be potential directions, which we leave as future work. 4.4 Transfer Learning In order to provide a fair and comprehensive comparison with the existing work, we also evaluate PairSupCon on the following seven transferring tasks: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST (Socher et al., 2013), TREC (Li and Roth, 2002), and MRPC (Dolan et al., 2004). We follow the widely used evaluation protocol, where a logistic regression classifier is Hard Negative Sampling Helps In Table 5, trained on top of the frozen representations, and the testing accuracy is used as a measure of the we compare both PairSupCon and InstanceDisc representation quality. We adopt the default configagainst their counterparts where the negatives in the instance discrimination loss are uniformly sam- urations of the SentEval (Conneau and Kiela, 2018) toolkit and report the evaluation results in Table 7 pled from data. As it shows, the hard"
2021.emnlp-main.467,P04-1035,0,0.0213378,"es within the local region of an anchor are also likely from the same semantic group as the anchor. As a consequence, a solely distance-based sampling approach can induce certain false negatives and hurt the performance. To tackle this issue, leveraging proper structure assumption or domain-specific knowledge could be potential directions, which we leave as future work. 4.4 Transfer Learning In order to provide a fair and comprehensive comparison with the existing work, we also evaluate PairSupCon on the following seven transferring tasks: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST (Socher et al., 2013), TREC (Li and Roth, 2002), and MRPC (Dolan et al., 2004). We follow the widely used evaluation protocol, where a logistic regression classifier is Hard Negative Sampling Helps In Table 5, trained on top of the frozen representations, and the testing accuracy is used as a measure of the we compare both PairSupCon and InstanceDisc representation quality. We adopt the default configagainst their counterparts where the negatives in the instance discrimination loss are uniformly sam- urations of the SentEval (Conneau and Kiela, 2018) toolkit an"
2021.emnlp-main.467,P05-1015,0,0.39236,"lenging problem, especially considering that samples within the local region of an anchor are also likely from the same semantic group as the anchor. As a consequence, a solely distance-based sampling approach can induce certain false negatives and hurt the performance. To tackle this issue, leveraging proper structure assumption or domain-specific knowledge could be potential directions, which we leave as future work. 4.4 Transfer Learning In order to provide a fair and comprehensive comparison with the existing work, we also evaluate PairSupCon on the following seven transferring tasks: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST (Socher et al., 2013), TREC (Li and Roth, 2002), and MRPC (Dolan et al., 2004). We follow the widely used evaluation protocol, where a logistic regression classifier is Hard Negative Sampling Helps In Table 5, trained on top of the frozen representations, and the testing accuracy is used as a measure of the we compare both PairSupCon and InstanceDisc representation quality. We adopt the default configagainst their counterparts where the negatives in the instance discrimination loss are uniformly sam- urations of"
2021.emnlp-main.467,D19-1410,0,0.0579585,") pairs of NLI to optimize an instance discrimination objective which tries to separate each positive pair apart from  M all other sentences. Let D = (xj , xj 0 ), yj j=1 denote a randomly sampled minibatch, with yi = ±1 indicating an entailment or contradiction pair. Then for a premise sentence xi within a positive pair (xi , xi0 ), we aim to separate its hypothesis sentence x0i from all other 2M -2 sentences within the same batch D. To be more specific, let I = {i, i0 }M i=1 denote the corresponded indices of the sentence pairs in D, we then minimize the following for xi , Following SBERT (Reimers and Gurevych, 2019a), we adopt the SNLI (Bowman et al., 2015) and exp(s(zi , zi0 )/τ ) . (1) `iID = − log P MNLI (Williams et al., 2017) as our training data, j∈Ii exp(s(zi , zj )/τ ) and refer the combined data as NLI for convenience. The NLI data consists of labeled sentence pairs and In the above equation, zj = h(ψ(xj )) denotes each can be presented in the form: (premise, hy- the output of the instance discrimination head pothesis, label). The premise sentences are selected in Figure 2, τ denotes the temperature paramefrom existing text sources and each premise sen- ter, and s(·) is chosen as the cosine si"
2021.emnlp-main.467,D13-1170,0,0.0117145,"ly from the same semantic group as the anchor. As a consequence, a solely distance-based sampling approach can induce certain false negatives and hurt the performance. To tackle this issue, leveraging proper structure assumption or domain-specific knowledge could be potential directions, which we leave as future work. 4.4 Transfer Learning In order to provide a fair and comprehensive comparison with the existing work, we also evaluate PairSupCon on the following seven transferring tasks: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST (Socher et al., 2013), TREC (Li and Roth, 2002), and MRPC (Dolan et al., 2004). We follow the widely used evaluation protocol, where a logistic regression classifier is Hard Negative Sampling Helps In Table 5, trained on top of the frozen representations, and the testing accuracy is used as a measure of the we compare both PairSupCon and InstanceDisc representation quality. We adopt the default configagainst their counterparts where the negatives in the instance discrimination loss are uniformly sam- urations of the SentEval (Conneau and Kiela, 2018) toolkit and report the evaluation results in Table 7 pled from d"
2021.emnlp-main.467,W18-5446,0,0.0470204,"Missing"
2021.emnlp-main.467,2021.naacl-main.427,1,0.820338,"Missing"
2021.emnlp-main.561,P19-1620,0,0.028017,"t necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors"
2021.emnlp-main.561,2020.acl-main.485,0,0.0226352,"Missing"
2021.emnlp-main.561,P18-1078,1,0.843692,"tor is less biased and less affected by conservative changes (Ben-David et al., 2010) to the data distribution. While the end to end QA performance of our model is comparable (↓ 0.3 F1) to Fang et al. (2020) on the original dev-set, on the adversarial set our method is better than Fang et al. (2020) (↑ 1.2 F1). Table 3 shows that the decoder of generative passage selector was able to generate multi-hop style questions from a pair of contexts. 3.2 Context pairs vs. Sentences Some context selection models for HotpotQA use a multi-label classifier that chooses top-k sentences (Fang et al., 2020; Clark and Gardner, 2018) which result in limited inter-document interaction than context pairs. To compare these two input types, we construct a multi-label sentence classifier p(s|q, C) that selects relevant sentences. This classifier projects a concatenated sentence and question representation, followed by a sigmoid, to predict if the sentence should be selected. This model has a 3.1 Adversarial Evaluation better performance over the context-pair selector but is more biased (Table 4). We use an existing adversarial set (Jiang and Bansal, 2019) for HotpotQA to test the robustness We performed similar experiments wit"
2021.emnlp-main.561,2020.acl-main.497,1,0.781523,"ocated in Richmond, Virginia. Dartmouth was founded in 1938 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but r"
2021.emnlp-main.561,D17-1090,0,0.0186536,"especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such"
2021.emnlp-main.561,2020.emnlp-main.580,0,0.0179387,"etting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is"
2021.emnlp-main.561,2020.emnlp-main.710,0,0.0268086,"eneration network gets contextual representations for context-pair candidates from the encoder and uses them to generate the question, via the decoder. The objective function increases the likelihood of the question for gold context pairs and the unlikelihood (Welleck et al., 2020) for a set of negative context pairs (Eq. 5). The negative context pairs are randomly sampled from all possible non-oracle context pairs. L(θ) = |question| X log p(qt |q<t , cgold ) Model Original Adversarial Acc F1 Acc F1 Standard Selector Generative Selector 95.3 97.5 79.5 81.9 91.4 96.3 76.0 80.1 Tu et al. (2020) Fang et al. (2020) 94.5 - 80.2 82.2 - 61.1 78.9 Table 1: HotpotQA: Passage selection accuracy and end-to-end QA F1 on the original and adversarial set (Jiang and Bansal, 2019) of the HotpotQA dataset. The results of Tu et al. (2020) and Fang et al. (2020) are as reported by Perez et al. (2020). t=1 + X |question| X n∈|neg.pairs| t=1 Model log (1 − p(qt |q<t , cn )) Standard Selector Generative Selector Accuracy EM/F1 96.8 97.2 72.8/79.9 73.5/80.2 (5) 3 Experiments and Results We experiment with two popular multi-hop datasets: HotpotQA (Yang et al., 2018) and WikiHop (Welbl et al., 2018). We use a pre-trained T5"
2021.emnlp-main.561,D19-1107,0,0.0310612,"Missing"
2021.emnlp-main.561,N18-2017,0,0.0550553,"Missing"
2021.emnlp-main.561,D17-1215,0,0.0280852,"n (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the selected set of passages. These dataset are Consider an example from HotpotQA in Figoften collected via crowdsourcing, which"
2021.emnlp-main.561,P19-1262,0,0.248753,"-12 VCU Rams men’s basketball team, led by third year head coach Shaka Smart, represented the university which was founded in what year? Gold Answer: 1838 Passage 1: The 2011-12 VCU Rams men’s basketball team represented Virginia Commonwealth University during the 2011-12 NCAA Division I men’s basketball season... Passage 2: Virginia Commonwealth University (VCU) is a public research university located in Richmond, Virginia. VCU was founded in 1838 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... Prediction: 1838 Adversarial context from Jiang and Bansal (2019): Dartmouth University is a public research university located in Richmond, Virginia. Dartmouth was founded in 1938 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases,"
2021.emnlp-main.561,P17-1147,0,0.0243601,"entence selection helped improve performance of the discriminative passage selector, we add an auxiliary loss term to our generative passage selector that also predicts the relevant sentences in the context pair when generating the question (p(q, s|ci j , Ψ)), in a multi-task manner. We see slight performance improvements by using relevant sentences as an additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body o"
2021.emnlp-main.561,2020.emnlp-main.550,0,0.0202736,"n additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent"
2021.emnlp-main.561,W13-2114,0,0.0352709,"ried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors can be attributed to existing bias in Hot- ing substantially better"
2021.emnlp-main.561,2021.ccl-1.108,0,0.0352242,"Missing"
2021.emnlp-main.561,D19-1284,0,0.0962305,"a in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the s"
2021.emnlp-main.561,P19-1416,1,0.927103,"a in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the s"
2021.emnlp-main.561,2020.emnlp-main.134,1,0.718829,", 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context select"
2021.emnlp-main.561,2020.emnlp-main.713,0,0.0133695,", 2020) for a set of negative context pairs (Eq. 5). The negative context pairs are randomly sampled from all possible non-oracle context pairs. L(θ) = |question| X log p(qt |q<t , cgold ) Model Original Adversarial Acc F1 Acc F1 Standard Selector Generative Selector 95.3 97.5 79.5 81.9 91.4 96.3 76.0 80.1 Tu et al. (2020) Fang et al. (2020) 94.5 - 80.2 82.2 - 61.1 78.9 Table 1: HotpotQA: Passage selection accuracy and end-to-end QA F1 on the original and adversarial set (Jiang and Bansal, 2019) of the HotpotQA dataset. The results of Tu et al. (2020) and Fang et al. (2020) are as reported by Perez et al. (2020). t=1 + X |question| X n∈|neg.pairs| t=1 Model log (1 − p(qt |q<t , cn )) Standard Selector Generative Selector Accuracy EM/F1 96.8 97.2 72.8/79.9 73.5/80.2 (5) 3 Experiments and Results We experiment with two popular multi-hop datasets: HotpotQA (Yang et al., 2018) and WikiHop (Welbl et al., 2018). We use a pre-trained T5 (Raffel et al., 2019) encoder-decoder model for obtaining contextual representations, which are further trained to estimate all individual probability distributions. The answering model is a fine-tuned T5-large model which has an oracle EM/F1, p(a |q, cgold ), of 74.5/83.5 a"
2021.emnlp-main.561,D18-1052,0,0.0229146,"ant sentences as an additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5"
2021.emnlp-main.561,P16-1056,0,0.0294284,"multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors can be attributed to e"
2021.emnlp-main.561,Q18-1021,0,0.145131,"showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the selected set of passages. These dataset are Consider an example from Ho"
2021.emnlp-main.561,D18-1259,0,0.125503,"ent of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way"
2021.findings-emnlp.327,2020.emnlp-main.20,0,0.0413657,"5 0.4 10 0.3 10 0.3 15 0.2 15 0.2 0 20 0.1 25 0.5 20 0.1 25 0 5 10 15 20 25 (a) fully-connected graph 0 5 10 15 20 25 0.0 (b) section graph Figure 7: An example of the graph attention pattern on graph-roberta models. The axes are graph node index. Node index 0 is the document node and the rest are passage nodes. Contrastive Learning Contrastive learning used as a self-supervised pretraining method has been widely used in NLP models (Rethmeier and Augenstein, 2021). Token or sentence-level contrastive learning tasks have been shown to be very useful in learning better contextual presentations (Clark et al., 2020; Giorgi et al., 2020; Meng et al., 2021). There also have been works that propose data augmentations for contrastive learning. Fang et al. (2020) proposed to use back-translation to construct positive sentence pairs in their contrastive learning framework. Wu et al. (2020); Qu et al. (2020) proposed multiple sentence-level augmentations strategies to do sentence contrastive learning. Most of these work still focus on either local token-level tasks or short sentence-level tasks. In our work, we directly work on document-level contrastive learning task. More recently Luo et al. (2021) proposed"
2021.findings-emnlp.327,P19-1285,0,0.0590152,"Missing"
2021.findings-emnlp.327,N19-1423,0,0.0736125,"Missing"
2021.findings-emnlp.327,2020.emnlp-main.550,0,0.0393059,"Missing"
2021.findings-emnlp.327,S19-2145,0,0.0274588,"Missing"
2021.findings-emnlp.327,P19-1612,0,0.0513504,"Missing"
2021.findings-emnlp.327,2021.ccl-1.108,0,0.0612724,"Missing"
2021.findings-emnlp.327,P11-1015,0,0.275739,"Missing"
2021.findings-emnlp.327,H89-1033,0,0.719438,"Missing"
2021.findings-emnlp.327,D19-1410,0,0.0421102,"Missing"
2021.naacl-main.427,W19-4322,0,0.268196,"rastive loss. We assess the performance of SCCL on short text clustering, which has become increasingly important due to the popularity of social media such as Twitter and Instagram. It benefits many real-world applications, including topic discovery (Kim et al., 2013), recommendation (Bouras and Tsogkas, 2017), and visualization (Sebrechts et al., 1999). However, the weak signal caused by noise and sparsity poses a significant challenge for clustering short texts. Although some improvement has been achieved by leveraging shallow neural networks to enrich the representations (Xu et al., 2017; Hadifar et al., 2019), there is still large room for improvement. We address this challenge with our SCCL model. Our main contributions are the following: • We propose a novel end-to-end framework for unsupervised clustering, which advances the state-of-the-art results on various short text clustering datasets by a large margin. Furthermore, our model is much simpler than the existing deep neural network based short text clustering approaches that often require multistage independent training. Early work focuses on solving different artificially designed pretext tasks, such as predicting masked tokens (Devlin et a"
2021.naacl-main.427,N19-1423,0,0.102265,"al., 2019), there is still large room for improvement. We address this challenge with our SCCL model. Our main contributions are the following: • We propose a novel end-to-end framework for unsupervised clustering, which advances the state-of-the-art results on various short text clustering datasets by a large margin. Furthermore, our model is much simpler than the existing deep neural network based short text clustering approaches that often require multistage independent training. Early work focuses on solving different artificially designed pretext tasks, such as predicting masked tokens (Devlin et al., 2019), generating future tokens (Radford et al., 2018), or denoising corrupted tokens (Lewis et al., 2019) for textual data, and predicting colorization (Zhang et al., 2016), rotation (Gidaris et al., 2018), or relative patch position (Doersch et al., 2015) for image data. Nevertheless, the resulting representations are tailored to the specific pretext tasks with limited generalization. Many recent successes are largely driven by instance-wise contrastive learning. Inspired by the pioneering work of Becker and Hinton (1992); Bromley et al. (1994), Instance-CL treats each data instance and its augme"
2021.naacl-main.427,N18-2072,0,0.222978,"˜i2 apart from all negative instances in B a by minimizing the following Here 1j6=i1 is an indicator function and τ denotes the temperature parameter which we set as 0.5. Following Chen et al. (2020a), we choose sim(·) as the dot product between a pair of normalized outputs, i.e., sim(˜ zi , z˜j ) = z˜iT z˜j /k˜ zi k2 k˜ zj k 2 . The Instance-CL loss is then averaged over all instances in B a , LInstance-CL = 2M X `Ii /2M . (2) i=1 To explore the above contrastive loss in the text domain, we explore three different augmentation strategies in Section 4.3.1, where we find contextual augmenter (Kobayashi, 2018; Ma, 2019) consistently performs better than the other two. 3.2 Clustering We simultaneously encode the semantic categorical structure into the representations via unsupervised clustering. Unlike Instance-CL, clustering focuses on the high-level semantic concepts and tries to bring together instances from the same semantic category together. Suppose our data consists of K semantic categories, and each category is characterized by its centroid in the representation space, denoted as µk , k ∈ {1, . . . , K}. Let ej = ψ(xj ) denote the representation of instance xj in the original set B. Followi"
2021.naacl-main.427,2020.acl-main.703,0,0.0388608,"Missing"
2021.naacl-main.427,H89-1033,0,0.527203,"Missing"
2021.naacl-main.427,2020.emnlp-demos.16,0,0.0437775,"Missing"
2021.naacl-main.427,N18-1202,0,0.0303245,"Hadifar et al., 2019), where word embedcluster distance and intra-cluster distance. dings (Mikolov et al., 2013b; Arora et al., 2017) • We explore various text augmentation techare adopted to further enhance the performance. niques for SCCL, showing that, unlike the However, the above approaches divide the learnimage domain (Chen et al., 2020a), using coming process into multiple stages, each requiring position of augmentations is not always beneindependent optimization. On the other hand, deficial in the text domain. spite the tremendous successes achieved by contextualized word embeddings (Peters et al., 2018; 2 Related Work Devlin et al., 2019; Radford et al., 2018; Reimers Self-supervised learning Self-supervised learn- and Gurevych, 2019b), they have been left largely ing has recently become prominent in providing ef- unexplored for short text clustering. In this work, fective representations for many downstream tasks. we leverage the pretrained transformer as the back5420 Figure 2: Training framework SCCL. During training, we jointly optimize a clustering loss over the original data instances and an instance-wise contrastive loss over the associated augmented pairs. bone, which is optimized in"
2021.naacl-main.427,D19-1410,0,0.305385,"hed clustering methods such as K-means (MacQueen et al., 1967; Lloyd, 1982) and Gaussian Mixture Models (Celeux and Govaert, 1995) rely on distance measured in the data space, which tends to be ineffective for highdimensional data. On the other hand, deep neural networks are gaining momentum as an effective way to map data to a low dimensional and hopefully better separable representation space. Many recent research efforts focus on integrating clustering with deep representation learning Figure 1: TSNE visualization of the embedding space learned on SearchSnippets using Sentence Transformer (Reimers and Gurevych, 2019a) as backbone. Each color indicates a ground truth semantic category. by optimizing a clustering objective defined in the representation space (Xie et al., 2016; Jiang et al., 2016; Zhang et al., 2017a; Shaham et al., 2018). Despite promising improvements, the clustering performance is still inadequate, especially in the presence of complex data with a large number of clusters. As illustrated in Figure 1, one possible reason is that, even with a deep neural network, data still has significant overlap across categories before clustering starts. Consequently, the clusters learned by optimizing"
2021.naacl-main.427,P19-1103,0,0.0280766,"ing space. In contrast, we leverage the strengths of both Clustering and Instance-CL to compliment each other. Consequently, Figure 4 shows SCCL leads to better separated clusters with each cluster being less dispersed. 4.3 4.3.1 Figure 4: Cluster-level evaluation on SearchSnippets. Each plot is summarized over five random runs. Data Augmentation Exploration of Data Augmentations To study the impact of data augmentation, we explore three different unsupervised text augmentations: (1) WordNet Augmenter5 transforms an input text by replacing its words with WordNet synonyms (Morris et al., 2020; Ren et al., 2019). (2) Contextual Augmenter6 leverages the pretrained transformers to find top-n suitable words of the input text for insertion or substitution (Kobayashi, 2018; Ma, 2019). We augment the data via word substitution, and we choose Bertbase and Roberta to generate the augmented pairs. (3) Paraphrase via back translation7 generates paraphrases of the input text by first translating it to another language (French) and then back to English. When translating back to English, we used the mixture of experts model (Shen et al., 2019) to generate ten candidate paraphrases per input to increase diversity."
C10-1056,P05-1033,0,0.076389,"ned with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to discriminatively rescore the baseline MT system’s phrase library: boost good phrase translations while prune bad ones. This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin. 1 Introduction Statistical Machine Translation (SMT) systems, including phrase-based (Och and Ney 2002; Koehn et. al. 2003), syntax-based (Yamada and Knight 2001; Galley et. al. 2004) or hybrid systems (Chiang 2005; Zollmann and Venugopal 2006), are typically built with bilingual phrase pairs, which are extracted from parallel sentences with word alignment. Due to the noises in the bilingual sentence pairs and errors from automatic word alignment, the extracted phrase pairs may contain errors, such as • dropping content words (the $num countries ,||个:&lt;null>), • length mismatch (along the lines of the ||的:of) • content irrelevance (the next $num years, || 水平:level 方面:aspect 所:&lt;null>) These incorrect phrase pairs compete with correct phrase pairs during the decoding process, and are often selected when th"
C10-1056,P08-1010,0,0.113927,"hes have been proposed over the past decade for the purpose of improving the phrase pair quality for SMT. For example, a term weight based model was presented in (Zhao, et al., 2004) to rescore phrase translation pairs. It models the translation probability with similarities between the query (source phrase) and document (target phrase). Significant improvement was obtained in the translation performance. In (Johnson, et al., 2007; Yang and Zheng, 2009), a statistical significance test was used to heavily prune the phrase table and thus achieved higher precision and better MT performance. In (Deng, et al., 2008), a generic phrase training algorithm was proposed with the focus on phrase extraction. Multiple feature functions are utilized based on information metrics or word alignment. The feature parameters are optimized to directly maximize the end-to-end system performance. Significant improvement was reported for a small MT task. But when the phrase table is large, such as in a large-scale SMT system, the computational cost of tuning with this approach will be high due to many iterations of phrase extraction and re-decoding. In this paper we attempt to improve the quality of the phrase table using"
C10-1056,N04-1035,0,0.124544,"Missing"
C10-1056,N03-1017,0,0.0944154,"Missing"
C10-1056,J03-1002,0,0.00759879,"Missing"
C10-1056,P02-1040,0,0.0786456,"Missing"
C10-1056,W06-3602,0,0.0457746,"Missing"
C10-1056,P01-1067,0,0.0275997,"nd a target phrase pair are introduced. These features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to discriminatively rescore the baseline MT system’s phrase library: boost good phrase translations while prune bad ones. This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin. 1 Introduction Statistical Machine Translation (SMT) systems, including phrase-based (Och and Ney 2002; Koehn et. al. 2003), syntax-based (Yamada and Knight 2001; Galley et. al. 2004) or hybrid systems (Chiang 2005; Zollmann and Venugopal 2006), are typically built with bilingual phrase pairs, which are extracted from parallel sentences with word alignment. Due to the noises in the bilingual sentence pairs and errors from automatic word alignment, the extracted phrase pairs may contain errors, such as • dropping content words (the $num countries ,||个:&lt;null>), • length mismatch (along the lines of the ||的:of) • content irrelevance (the next $num years, || 水平:level 方面:aspect 所:&lt;null>) These incorrect phrase pairs compete with correct phrase pairs during"
C10-1056,P09-2060,0,0.317812,"e pair will be selected for the final translation). As a result, the translation quality is degraded when these incorrect phrase pairs are selected. Various approaches have been proposed over the past decade for the purpose of improving the phrase pair quality for SMT. For example, a term weight based model was presented in (Zhao, et al., 2004) to rescore phrase translation pairs. It models the translation probability with similarities between the query (source phrase) and document (target phrase). Significant improvement was obtained in the translation performance. In (Johnson, et al., 2007; Yang and Zheng, 2009), a statistical significance test was used to heavily prune the phrase table and thus achieved higher precision and better MT performance. In (Deng, et al., 2008), a generic phrase training algorithm was proposed with the focus on phrase extraction. Multiple feature functions are utilized based on information metrics or word alignment. The feature parameters are optimized to directly maximize the end-to-end system performance. Significant improvement was reported for a small MT task. But when the phrase table is large, such as in a large-scale SMT system, the computational cost of tuning with"
C10-1056,2004.tmi-1.9,0,0.0808571,"Missing"
C10-1056,W04-3227,0,0.250511,"contain systematic alignment errors) or certain model costs are low (for example, when some source content words are translated into target function words in an incorrect phrase pair, the language model cost of the incorrect pair may be small, making it more likely that the pair will be selected for the final translation). As a result, the translation quality is degraded when these incorrect phrase pairs are selected. Various approaches have been proposed over the past decade for the purpose of improving the phrase pair quality for SMT. For example, a term weight based model was presented in (Zhao, et al., 2004) to rescore phrase translation pairs. It models the translation probability with similarities between the query (source phrase) and document (target phrase). Significant improvement was obtained in the translation performance. In (Johnson, et al., 2007; Yang and Zheng, 2009), a statistical significance test was used to heavily prune the phrase table and thus achieved higher precision and better MT performance. In (Deng, et al., 2008), a generic phrase training algorithm was proposed with the focus on phrase extraction. Multiple feature functions are utilized based on information metrics or wor"
C10-1056,W06-3119,0,0.0289699,"ar regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to discriminatively rescore the baseline MT system’s phrase library: boost good phrase translations while prune bad ones. This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin. 1 Introduction Statistical Machine Translation (SMT) systems, including phrase-based (Och and Ney 2002; Koehn et. al. 2003), syntax-based (Yamada and Knight 2001; Galley et. al. 2004) or hybrid systems (Chiang 2005; Zollmann and Venugopal 2006), are typically built with bilingual phrase pairs, which are extracted from parallel sentences with word alignment. Due to the noises in the bilingual sentence pairs and errors from automatic word alignment, the extracted phrase pairs may contain errors, such as • dropping content words (the $num countries ,||个:&lt;null>), • length mismatch (along the lines of the ||的:of) • content irrelevance (the next $num years, || 水平:level 方面:aspect 所:&lt;null>) These incorrect phrase pairs compete with correct phrase pairs during the decoding process, and are often selected when their counts are high (if they c"
C10-1056,D07-1103,0,\N,Missing
C10-1056,J93-2003,0,\N,Missing
C10-1056,P03-1021,0,\N,Missing
C16-1164,D13-1160,0,0.766093,"entities as subject are then the fact search space for this question. CharCNN and word-CNN decompose each question-fact match into an entity-mention surface-form match and a predicate-pattern semantic match. Our approach has a simple architecture, but it outperforms the state-of-the-art, a system that has a much more complicated structure. 2 Related Work As mentioned in Section 1, factoid QA against Freebase can be categorized into single-relation QA and multi-relation QA. Much work has been done on multi-relation QA in the past decade, especially after the release of benchmark WebQuestions (Berant et al., 2013). Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing th"
C16-1164,D14-1067,0,0.48883,"work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. 1 (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task. 1 Introduction Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in"
C16-1164,P16-1076,0,0.692821,"m by an embedding-based QA system developed under the framework of Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015). The setting of the SimpleQA corresponds to the elementary operation of performing a single lookup in the memory. They investigate the performance of training on the combination of SimpleQuestions, WebQuestions and Reverb training sets. Golub and He (2016) propose a character-level attention-based encoder-decoder framework to encode the question and subsequently decode into (subject, predicate) tuple. Our model in this work is much simpler than these prior systems. Dai et al. (2016) combine a unified conditional probabilistic framework with deep recurrent neural networks and neural embeddings to get state-of-the-art performance. Treating SimpleQA as fact selection is inspired by work on answer selection (e.g., Yu et al. (2014), Yin et al. (2016b), Santos et al. (2016)) that looks for the correct answer(s) from some candidates for a given question. The answer candidates in those tasks are raw text, not structured information as facts in Freebase are. We are also inspired by work that generates natural language questions given knowledge graph facts (Seyler et al., 2015; Se"
C16-1164,P15-1026,0,0.370013,"form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word"
C16-1164,D11-1142,0,0.0153623,"h the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word is first preprocessed into a count vector of character-trigram vocabulary, then forwarded into the CNN as input. We treat entities and mentions as character sequences. Our char-CNN for entity-mention match is more end-to-end without data preprocessing. (ii)"
C16-1164,P13-1158,0,0.0602297,"eebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in reality not simple at all and far from solved. In SimpleQA, a question, such as “what’s the hometown of Obama?”, asks a single and direct topic of an entity. In this example, the entity is “Obama” and the topic is hometown. So our task is reduced to finding one fact (subject, predicate, object) in Freebase that answers the question, which roughly means the subject and predicate are the best matches for the topical entity “Obama” and for the topic description “what’s the hometown of”, respectively. Thus, we aim to des"
C16-1164,D16-1166,0,0.655535,"system, given a question, is asked to choose the best answer from a list of candidates. In this work, we formulate the SimpleQA task as a fact selection problem and the key issue lies in the system design for how to match a fact candidate to the question. The first obstacle is that Freebase has an overwhelming number of facts. A common and effective way is to first conduct entity linking of a question over Freebase, so that only a small subset of facts remain as candidates. Prior work achieves entity linking by searching word n-grams of a question among all entity names (Bordes et al., 2015; Golub and He, 2016). Then, facts whose subject entities match those n-grams are kept. Our first contribution in this work is to present a simple while effective entity linker ∗ 1 This work was conducted during the first author’s internship at IBM Watson Group. We release our entity linking results at: https://github.com/Gorov/SimpleQuestions-EntityLinking This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1746 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers"
C16-1164,P14-1090,0,0.301761,"Missing"
C16-1164,W14-2416,0,0.0221327,"Missing"
C16-1164,N15-3014,0,0.0599456,"c parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashin"
C16-1164,P14-2105,0,0.095584,"e based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a"
C16-1164,P15-1128,0,0.592067,"y linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. 1 (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task. 1 Introduction Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in reality not simple at all and far from solved. In SimpleQA, a que"
C16-1164,W16-0103,1,0.856224,"Missing"
C16-1164,Q16-1019,1,0.0271245,"Missing"
C16-1164,N03-1033,0,\N,Missing
C16-1164,Q14-1002,1,\N,Missing
D11-1082,H05-1024,0,0.0195863,"al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generator perturbs an existing alignment A in order to create a set of candidate alig"
D11-1082,D10-1097,0,0.0144158,"s training data. Word alignments appear as hidden variables in IBM Models 15 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of finding the optimal alignment. Many alignment models are factorized in order to use dynamic programming and beam search for efficient marginalization and search. Such a factorization encourages - but does not require - a sequential (often left-to-right) decoding order. If left-to-right decoding is adopted (and exact dynamic programming is intractable) important right context may exist beyond the search window. For example, the linkage of an English determiner may be considered before the linkage of a distant head noun. An a"
D11-1082,P06-1009,0,0.0862178,"target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1 ...fm , and a target sentence E = e1 ...el , the alignment matrix A = {σij } is an l × m matrix of binary variables. If σij = 1, then ei is said to be linked to fj . If ei is unlinked then σij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are po"
D11-1082,J93-2003,0,0.037353,"e an alignment matrix model as a correction algorithm to an underlying sequencebased aligner. Then a greedy decoding algorithm enables the full expressive power of the alignment matrix formulation. Improved alignment performance is shown for all nine language pairs tested. The improved alignments also improved translation quality from Chinese to English and English to Italian. 1 Introduction Word-level alignments of parallel text are crucial for enabling machine learning algorithms to fully utilize parallel corpora as training data. Word alignments appear as hidden variables in IBM Models 15 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of"
D11-1082,P09-2058,0,0.0149371,"ction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These mod"
D11-1082,W08-0306,0,0.018839,"we restrict ourselves to a very simple move generator that changes the linkage of exactly one source word at a time, or exactly one target word at a time. Many of our corrections are similar to those of (Setiawan et al., 2010), although our motivation is perhaps closer to (Brown et al., 1993), who used similar perturbations to approximate intractable sums that arise when estimating the parameters of the generative models Models 3-5, and approach refined in (Och and Ney, 2003). We note that our corrections are designed to improve even a highquality starting alignment; in contrast the model of (Fossum et al., 2008) considers deletion of links from an initial alignment (union of aligners) that is likely to overproduce links. From the point of view of the alignment matrix, we consider changes to one row or one column (generically, one slice) of the alignment matrix. At each step t, the move set Mt (At ) is formed by choosing a slice of the current alignment matrix At , and generating all possible alignments from a few families of moves. Then the move generator picks another slice and repeats. The m + l slices are cycled in a fixed order: the first m slices correspond to source words (ordered according to"
D11-1082,J07-3002,0,0.0456387,"ks are more numerous than 2−1 links, in both language pairs. This is consequence of the choice of directionality and tokenization style. 6.5 Translation Impact We tested the impact of improved alignments on the performance of a phrase-based translation system (Ittycheriah and Roukos, 2007) for three language pairs. Our alignment did not improve the performance of a mature Arabic to English translation system, but two notable successes were obtained: Chinese to English, and English to Italian. It is well known that improved alignment performance does not always improve translation performance (Fraser and Marcu, 2007). A mature machine translation system may incorporate alignments obtained from multiple aligners, or from both directions of an asymmetric aligner. Furthermore, with large amounts of training data (the Gale Phase 4 Arabic English corpus consisting of 8 × 106 sentences,) a machine translation system is subject to a saturation effect: correcting an alignment may not yield a significant improvement because the the phrases learned from the correct alignment have already been acquired in other contexts. For the Chinese to English translation system (table 6) the training corpus consisted of 11 × 10"
D11-1082,H05-1012,1,0.965578,"as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1 ...fm , and a target sentence E = e1 ...el , the alignment matrix A = {σij } is an l × m matrix of binary variables. If σij = 1, then ei is said to be linked to fj . If ei is unlinked then σij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary"
D11-1082,P07-2045,0,0.0075034,"get languages results in a different aligner. This parameterization does not allow a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically traine"
D11-1082,N04-4015,0,0.0181409,"nglish and a 1.26 BLEU improvement on English to Italian translation. 2 Alignment sequence models Sequence models are the traditional workhorse for word alignment, appearing, for instance, in IBM Models 1-5. This type of alignment model is not symmetric; interchanging source and target languages results in a different aligner. This parameterization does not allow a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of"
D11-1082,N06-1014,0,0.0354576,"a target word to be linked to more than one source word, so some phrasal alignments are simply not considered. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Nevertheless, aligners that use this parameterization internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignm"
D11-1082,P05-1057,0,0.137591,"j = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are potentially allowed in alignment matrix models. For typical l and m, 2lm  (m + 1)l , the number of alignments described by a comparable sequence model. This parameterization is symmetric if source and target are interchanged, then the alignment matrix is transposed. A straightforward approach to the alignment matrix is to build a log linear model (Liu et al., 2005) for the probability of the alignment A. (We continue to refer to “source” and “target” words only for consistency of notation - alignment models such as this are indifferent to the actual direction of translation.) The log linear model for the alignment (Liu et al., 2005) is P exp ( i λi φi (A, E, F )) (1) p(A|E, F ) = Z(E, F ) where the partition function (normalization) is given by ! X X Z(E, F ) = exp λi φi (A, E, F ) . (2) A i Here the φi (A, E, F ) are feature functions. The model is parameterized by a set of weights λi , one for each feature function. Feature functions are often binary,"
D11-1082,P06-1065,0,0.0192965,"a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generator perturbs an existing"
D11-1082,H05-1011,0,0.0242944,"e restricted to a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and a move selector. The move generato"
D11-1082,W08-0303,0,0.113468,"typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). 3 Joint Models An alternate parameterization of alignment is the alignment matrix (Niehues and Vogel, 2008). For a source sentence F consisting of words f1 ...fm , and a target sentence E = e1 ...el , the alignment matrix A = {σij } is an l × m matrix of binary variables. If σij = 1, then ei is said to be linked to fj . If ei is unlinked then σij = 0 for all j. There is no constraint limiting the number of source tokens to which a target word is linked either; thus the binary matrix allows some alignments that cannot be modeled by the sequence parameterization. All 2lm binary matrices are potentially allowed in alignment matrix models. For typical l and m, 2lm  (m + 1)l , the number of alignments"
D11-1082,J03-1002,0,0.163895,"or alignment correction target word is sufficient. 4.1 Move generation Many different types of alignment perturbations are possible. Here we restrict ourselves to a very simple move generator that changes the linkage of exactly one source word at a time, or exactly one target word at a time. Many of our corrections are similar to those of (Setiawan et al., 2010), although our motivation is perhaps closer to (Brown et al., 1993), who used similar perturbations to approximate intractable sums that arise when estimating the parameters of the generative models Models 3-5, and approach refined in (Och and Ney, 2003). We note that our corrections are designed to improve even a highquality starting alignment; in contrast the model of (Fossum et al., 2008) considers deletion of links from an initial alignment (union of aligners) that is likely to overproduce links. From the point of view of the alignment matrix, we consider changes to one row or one column (generically, one slice) of the alignment matrix. At each step t, the move set Mt (At ) is formed by choosing a slice of the current alignment matrix At , and generating all possible alignments from a few families of moves. Then the move generator picks a"
D11-1082,D10-1052,0,0.0974221,"Missing"
D11-1082,2006.amta-papers.25,0,0.0203284,"ng an alignment may not yield a significant improvement because the the phrases learned from the correct alignment have already been acquired in other contexts. For the Chinese to English translation system (table 6) the training corpus consisted of 11 × 106 sentence pairs, subsampled to 106 . The test set was NIST MT08 Newswire, consisting of 691 sentences and 4 reference translations. Corpus-level performance (columns 2 and 3) improved when measured by BLEU, but not by TER. Performance on the most difficult sentences (near the 90th percentile, columns 4 and 5) improved on both BLEU and TER (Snover et al., 2006), and the improvement in BLEU was larger for the more difficult sentences than it was overall. Translation performance further improved, by a smaller amount, using both ME-seq and corr(ME-seq) alignments during the training. The improved alignments impacted the translation performance of the English to Italian translation system (table 7) even more strongly. Here the training corpus consisted of 9.4×106 sentence pairs, subsampled to 387000 pairs. The test set consisted of 7899 sentences. Overall performance improved as measured by both TER and BLEU (1.26 points.) 7 Conclusions A log linear mod"
D11-1082,H05-1010,0,0.0264195,"example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al., 2005). This approximation was found to be inconsistent for small n unless the merged results of several aligners were used. Alternately, loopy belief propagation techniques were used in (Niehues and Vogel, 2008). Loopy belief propagation is not guaranteed to converge, and feature design is influenced by consideration of the loops created by the features. Outside of the maximum entropy framework, similar models have been trained using maximum weighted bipartite graph matching (Taskar et al., 2005), averaged perceptron (Moore, 2005), (Moore et al., 2006), and transformation-based learning (Ayan et al., 2005). 4 Alignment Correction Model In this section we describe a novel approach to word alignment, in which we train a log linear (maximum entropy) model of alignment by viewing it as correction model that fixes the errors of an existing aligner. We assume a priori that the aligner will start from an existing alignment of reasonable quality, and will attempt to apply a series of small changes to that alignment in order to correct it. The aligner naturally consists of a move generator and"
D11-1082,C96-2141,0,0.400464,"internally often incorporate various heuristics in order to augment their output with the disallowed alignments - for example, swapping source and target languages to obtain a second alignment (Koehn et al., 2007) with different limitations. Training both directions jointly (Liang et al., 2006) and using posterior probabilities during alignment prediction even allows the model to see limited right context. Another alignment combination strategy (Deng and Zhou, 2009) directly optimizes the size of the phrase table of a target MT system. Generative models (such as Models 1-5, and the HMM model (Vogel et al., 1996)) motivate a narrative where alignments are selected left-to-right and target words are then generated conditioned upon the alignment and the source words. Generative models are typically trained unsupervised, from parallel corpora without manually annotated word-level alignments. Discriminative models of alignment incorporate source and target words, as well as more linguisti890 cally motivated features into the prediction of alignment. These models are trained from annotated word alignments. Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional rando"
D11-1082,H01-1035,0,0.0575331,"tilize parallel corpora as training data. Word alignments appear as hidden variables in IBM Models 15 (Brown et al., 1993) in order to bridge a gap between the sentence-level granularity that is explicit in the training data, and the implicit word-level correspondence that is needed to statistically model lexical ambiguity and word order rearrangements that are inherent in the translation process. Other notable applications of word alignments include crosslanguage projection of linguistic analyzers (such as POS taggers and named entity detectors,) a subject which continues to be of interest. (Yarowsky et al., 2001), (Benajiba and Zitouni, 2010) The structure of the alignment model is tightly linked to the task of finding the optimal alignment. Many alignment models are factorized in order to use dynamic programming and beam search for efficient marginalization and search. Such a factorization encourages - but does not require - a sequential (often left-to-right) decoding order. If left-to-right decoding is adopted (and exact dynamic programming is intractable) important right context may exist beyond the search window. For example, the linkage of an English determiner may be considered before the linkag"
D11-1082,N07-1008,1,\N,Missing
D13-1048,N04-4026,0,\N,Missing
D13-1048,W09-2307,0,\N,Missing
D13-1048,N09-1053,0,\N,Missing
D13-1048,D08-1089,0,\N,Missing
D13-1048,W05-0823,0,\N,Missing
D13-1048,P11-2080,0,\N,Missing
D13-1048,W12-3125,0,\N,Missing
D13-1048,P07-2045,0,\N,Missing
D13-1048,P12-1095,0,\N,Missing
D13-1048,P05-1033,0,\N,Missing
D13-1048,N13-1001,0,\N,Missing
D13-1048,W06-3108,0,\N,Missing
D13-1048,J97-3002,0,\N,Missing
D13-1048,P09-1037,1,\N,Missing
D13-1048,P11-1086,0,\N,Missing
D13-1048,C10-2033,0,\N,Missing
D13-1048,P07-1090,1,\N,Missing
D13-1048,P08-1066,0,\N,Missing
D13-1048,P13-1124,1,\N,Missing
D13-1048,N13-1002,0,\N,Missing
D13-1048,D11-1125,0,\N,Missing
D16-1223,H94-1010,0,0.121505,"tead, we directly fed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and evaluation data of"
D16-1223,K16-1028,1,0.595295,"Missing"
D16-1223,H90-1020,0,0.139898,"smaller. Instead, we directly fed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and"
D16-1223,P06-2113,0,0.0276978,"ed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and evaluation data of 893 sentences. The"
D16-1223,P15-1109,0,0.0100616,"ummarization (Nallapati et al., 2016) and so on. The difference is that the proposed encoder-labeler LSTM accepts the same input sequence twice while the usual encoder-decoder LSTM accepts the input sequence once in the encoder. Note that the LSTMs for encoding and labeling are different in the encoder-labeler LSTM, but the same word embedding matrix is used both for the encoder and labeler since the same input sequence is fed twice. 2.4 Related Work on Considering Sentence-level Information Bi-directional RNN/LSTM have been proposed to capture sentence-level information (Mesnil et al., 2015; Zhou and Xu, 2015; Vu et al., 2016). While the bi-directional RNN/LSTM model the preceding and succeeding contexts at a specific word and O O O O O O B-ToCity I need a ticket to Seattle O &lt;B&gt; O I (a) Labeler LSTM(W). O O need O O O a B-ToCity O ticket O to Seattle to ticket a need I O O &lt;B&gt; O Encoder (backward) LSTM Seattle (b) Labeler LSTM(W+L). O I need O O O B-ToCity to ticket a need I Encoder LSTM (backward) a ticket to Seattle Labeler LSTM(W) to ticket a need I O &lt;B&gt; B-ToCity O O O O O O I Encoder LSTM (backward) (d) Encoder-labeler LSTM(W). O (c) Encoder-decoder LSTM. Seattle Seattle O Decoder LSTM O O O"
D18-1096,E14-1056,0,\N,Missing
D18-1096,D14-1162,0,\N,Missing
D18-1096,W13-0102,0,\N,Missing
D18-1096,P16-2068,0,\N,Missing
D19-1599,P17-1171,0,0.0590093,"ntation of the first token [CLS]. We also apply sof tmax over all passage scores corresponding to the same question, and train to maximize the log-likelihood of passages containing the correct answers. Denote the passage score as P r(Pi |Q, P ), then the score of an answer span from passage Pi will be P r(Pi |Q, P )Ps (as |Q, P )Pe (ae |Q, P ). 3 Experiments Datasets: We experiment on four open-domain QA datasets. (1) OpenSQuAD: question-answer pairs are from SQuAD 1.1 (Rajpurkar et al., 2016), but a QA model will find answers from the entire Wikipedia rather than the given context. Following Chen et al. (2017), we use the 2016-1221 English Wikipedia dump. 5,000 QA pairs are randomly selected from the original training set as our validation set, and the remaining QA pairs are taken as our new training set. The original development set is used as our test set. (2) TriviaQA: TriviaQA unfiltered version (Joshi et al., 2017) are used. Following Pang et al. (2019), we randomly hold out 5,000 QA pairs from the original training set as our validation set, and take the remaining pairs as our new training set. The original development set is used as our test set. (3) Quasar-T (Dhingra et al., 2017) and (4) S"
D19-1599,P18-1078,0,0.255981,"training, passages corresponding to the same question are taken as independent training instances. During inference, the BERT-RC model is applied to each passage individually to predict an answer span, and then the highest scoring span is selected as the final answer. Although this method achieves significant improvements on several datasets, there are still several unaddressed issues. First, viewing passages of the same question as independent training instances may result in incomparable answer scores across passages. Thus, globally normalizing scores over all passages of the same question (Clark and Gardner, 2018) may be helpful. Second, previous work defines passages as articles, paragraphs, or sentences. However, the question of proper granularity of passages is still underexplored. Third, passage ranker for selecting high-quality passages has been shown to be very useful in previous open-domain QA systems (Wang et al., 2018a; Lin et al., 2018; Pang et al., 2019). However, we do not know whether it is still required for BERT. Fourth, most effective QA and RC models highly rely on explicit inter-sentence matching between questions and passages (Wang and Jiang, 2017; Wang et al., 2016; Seo et al., 2017"
D19-1599,P17-1147,0,0.0600789,")Ps (as |Q, P )Pe (ae |Q, P ). 3 Experiments Datasets: We experiment on four open-domain QA datasets. (1) OpenSQuAD: question-answer pairs are from SQuAD 1.1 (Rajpurkar et al., 2016), but a QA model will find answers from the entire Wikipedia rather than the given context. Following Chen et al. (2017), we use the 2016-1221 English Wikipedia dump. 5,000 QA pairs are randomly selected from the original training set as our validation set, and the remaining QA pairs are taken as our new training set. The original development set is used as our test set. (2) TriviaQA: TriviaQA unfiltered version (Joshi et al., 2017) are used. Following Pang et al. (2019), we randomly hold out 5,000 QA pairs from the original training set as our validation set, and take the remaining pairs as our new training set. The original development set is used as our test set. (3) Quasar-T (Dhingra et al., 2017) and (4) SearchQA (Dunn et al., 2017) are leveraged with the official split. Basic Settings: If not specified, the pre-trained BERT-base model with default hyper-parameters is leveraged. ElasticSearch with BM25 algorithm is employed as our retriever for OpenSQuAD. Passages for other datasets are from the corresponding releas"
D19-1599,P18-1161,0,0.287338,"Missing"
D19-1599,P17-1018,0,0.034762,"1 28.7 - 37.5 36.6 - BERT (Large) (Nogueira et al., 2018) BERT serini (Yang et al., 2019) BERT-RC (Ours) 49.7 56.8 63.7 69.1 68.7 61.0 66.9 38.6 45.4 46.1 52.5 Multi-Passage BERT (Base) Multi-Passage BERT (Large) 51.3 51.1 59.0 59.1 65.2 65.1 70.6 70.7 62.0 63.7 67.5 69.2 51.2 53.0 59.0 60.9 Table 2: Comparison with state-of-the-art models, where the first group are models without using BERT, the second group are BERT-based models, and the last group are our multi-passage BERT models. sages with questions, aka inter-sentence matching (Wang and Jiang, 2017; Wang et al., 2016; Seo et al., 2017; Wang et al., 2017; Song et al., 2017). However, BERT model simply concatenates a passage with a question, and differentiates them by separating them with a delimiter token [SEP], and assigning different segment ids for them. Here, we aim to check whether explicit inter-sentence matching still matters for BERT. We employ a shared BERT model to encode a passage and a question individually, and a weighted sum of all BERT layers is used as the final tokenlevel representation for the question or passage, where weights for all BERT layers are trainable parameters. Then the passage and question representations are in"
D19-1599,N19-4013,0,0.23695,"Missing"
D19-1599,D16-1264,0,0.139503,"lti-passage BERT except that at the output layer it only predicts a single score for each passage based on the vector representation of the first token [CLS]. We also apply sof tmax over all passage scores corresponding to the same question, and train to maximize the log-likelihood of passages containing the correct answers. Denote the passage score as P r(Pi |Q, P ), then the score of an answer span from passage Pi will be P r(Pi |Q, P )Ps (as |Q, P )Pe (ae |Q, P ). 3 Experiments Datasets: We experiment on four open-domain QA datasets. (1) OpenSQuAD: question-answer pairs are from SQuAD 1.1 (Rajpurkar et al., 2016), but a QA model will find answers from the entire Wikipedia rather than the given context. Following Chen et al. (2017), we use the 2016-1221 English Wikipedia dump. 5,000 QA pairs are randomly selected from the original training set as our validation set, and the remaining QA pairs are taken as our new training set. The original development set is used as our test set. (2) TriviaQA: TriviaQA unfiltered version (Joshi et al., 2017) are used. Following Pang et al. (2019), we randomly hold out 5,000 QA pairs from the original training set as our validation set, and take the remaining pairs as o"
D19-6109,P13-2119,0,0.0614421,"Missing"
D19-6109,P18-1031,0,0.0347485,"a base network is trained with the source data, and then the first n layers of the 76 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 76–83 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 to the complex attention mechanisms and large parameter size, it is hard to train BERT for domain adaptation using the domain-adversarial approach. Our initial experiments demonstrated the unsteadiness of this approach when applied to BERT. Unsupervised language model (LM) finetuning method (Howard and Ruder, 2018) consisting of general-domain LM pre-training and target task LM fine-tuning is effective using a AWDLSTM language model on many text classification tasks such as sentimental analysis, question classification and topic classification. However, due to the unique objective of BERT language model pre-training (masked LM and next sentence prediction) which requires multi-sentences natural language paragraphs, unsupervised fine-tuning of BERT LM does not apply to many sentence-pair classification datasets. In this work, we propose a novel domain adaptation framework, in which the idea of domainadve"
D19-6109,P07-1034,0,0.166658,"n classifier is trained via a gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation. As the training progresses, the approach promotes the emergence of a representation that is discriminative for the main learning task and indiscriminate with respect to the shift between the domains. However, such type of models are usually hard to train since the optimization problem involves a minimization with respect to some parameters, as well as a maximization with respect to the others. Very early approaches in NLP utilized instance re-weighting (Jiang and Zhai, 2007) and target data co-training (Chen et al., 2011) to achieve domain adaptation. Recently, Denoising Autoencoders (Glorot et al., 2011), domain discrepancy regularization and domain adversarial training (Shah et al., 2019; Shen et al., 2017) have been employed to learn a domain invariant representation for neural network models. Many domain adaptation studies have focused on tasks such as sentiment analysis (Glorot et al., 2011; Shen et al., 2017) , Part-Of-Speech (POS) tagging (Ruder et al., 2017a) and paraphrase detection (Shah et al., 2019), and tested on neural network models such as multila"
D19-6109,D11-1033,0,0.202275,"Missing"
D19-6109,D16-1264,0,0.038806,"ents, in order to determine the optimal number of data points selected from the source domain, we set aside a small target domain dataset for validation. Starting from only a hundred examples, we double the training data size every time we observe a significant change in transfer performance evaluated on the validation set. 4.1 matched (in-domain) section. Similar as in SNLI, we convert the three-label classification task into a binary classification task. QNLI The Question-answering Natural Language Inference (QNLI) is a dataset converted from the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). Although its name contains “natural language inference”, the text domain and task type of QNLI are fundamentally different from those of SNLI and MNLI. The original SQuAD dataset consists of questionparagraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). GLUE converts the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context and filtering out pairs with low lexical overlap between the question and the context sentence"
D19-6109,D15-1075,0,0.0265892,"much smaller variance in evaluation metrics compared with smaller datasets. We used the pre-processed datasets from GLUE natural language understanding benchmark (Wang et al., 2018). A summary of the dataset statistics and the details of the experiment setup are presented in Table 1. Task Category Natural Language Inference Natural Language Inference Answer Sentence Selection Paraphrase Detection Dataset SNLI MNLI QNLI Quora Train Size 510,711 392,702 108,436 363,847 Dev Size 9,831 9,815 5,732 40,430 Table 1: Summary of the datasets SNLI The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a collection of 570k human-written English sentence pairs supporting the task of natural language inference. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). In order to make the label set the same across all the datasets, we convert the original three-label classification task into a binary classification task with “entailment” as the positive label, and “contradiction” and “neutral” as negative. MNLI The Multi-Genre Natural Language Inferenc"
D19-6109,D17-1038,0,0.0887119,"Missing"
D19-6109,P16-1013,0,0.067069,"Missing"
D19-6109,W18-5446,0,0.0330477,"mples are pairs of “related questions” which, although pertaining to similar topics, are not truly semantically equivalent. Due to community nature, the ground-truth labels contain some amount of noise. Datasets We tested our framework on four large public datasets across three task categories: natural language inference (SNLI and MNLI), answer sentence selection (QNLI) and paraphrase detection (Quora). Large datasets usually have a much smaller variance in evaluation metrics compared with smaller datasets. We used the pre-processed datasets from GLUE natural language understanding benchmark (Wang et al., 2018). A summary of the dataset statistics and the details of the experiment setup are presented in Table 1. Task Category Natural Language Inference Natural Language Inference Answer Sentence Selection Paraphrase Detection Dataset SNLI MNLI QNLI Quora Train Size 510,711 392,702 108,436 363,847 Dev Size 9,831 9,815 5,732 40,430 Table 1: Summary of the datasets SNLI The Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015) is a collection of 570k human-written English sentence pairs supporting the task of natural language inference. Given a premise sentence and a hypothesis sentenc"
K16-1028,P16-1014,1,0.434022,"atures. Pointer networks (Vinyals et al., 2015) have also been used earlier for the problem of rare words in the context of machine translation (Luong et al., 2015), but the novel addition of switch in our model allows it to strike a balance between when to be faithful to the original source (e.g., for named entities and OOV) and when it is allowed to be creative. We believe such a process arguably mimics how human produces summaries. For a more detailed treatment of this model, and experiments on multiple tasks, please refer to the parallel work published by some of the authors of this work (Gulcehre et al., 2016). Hierarchical attention model (Sec. 2.4): Previously proposed hierarchical encoder-decoder models use attention only at sentence-level (Li et al., 2015). The novelty of our approach lies in joint modeling of attention at both sentence and word levels, where the word-level attention is further influenced by sentence-level attention, thus capturing the notion of important sentences and important words within those sentences. Concatenation of positional embeddings with the hidden state at sentence-level is also new. 4 (2015). We used the scripts made available by the authors of this work4 to pre"
K16-1028,H90-1087,0,0.206907,"Missing"
K16-1028,P00-1041,0,0.534682,"UC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004), used a combination of linguistically motivated compression techniques, and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword an"
K16-1028,D15-1229,0,0.137711,"considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets. In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese dataset using an encoder-decoder RNN, but do not report experiments on English corpora. In another very recent work, Cheng and Lapata (2016) used RNN based encoder-decoder for extractive summarization of documents. Our work starts with the same framework as (Hu et al., 2015), but we go beyond the stan, HiddenState Wordlayer HiddenState Sentencelayer OutputLayer where Pwa (j) is the word-level attention weight at j th position of the source document, and s(j) is the ID of the sentence a"
K16-1028,P16-1046,0,0.384532,"network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets. In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese dataset using an encoder-decoder RNN, but do not report experiments on English corpora. In another very recent work, Cheng and Lapata (2016) used RNN based encoder-decoder for extractive summarization of documents. Our work starts with the same framework as (Hu et al., 2015), but we go beyond the stan, HiddenState Wordlayer HiddenState Sentencelayer OutputLayer where Pwa (j) is the word-level attention weight at j th position of the source document, and s(j) is the ID of the sentence at j th word position, Psa (l) is the sentence-level attention weight for the lth sentence in the source, Nd is the number of words in the source document, and P a (j) is the re-scaled attention at the j th word position. The re-scaled attention is th"
K16-1028,D16-1053,0,0.0194114,"Missing"
K16-1028,N16-1012,0,0.712569,"on rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets. In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese dataset using an encoder-decoder RNN, but do not report experiments on English corpora. In another very recent work, Cheng and Lapata (2016) used RNN based encoder-decoder for extractive summarization of documents. Our work starts with the same framework as (Hu et a"
K16-1028,P15-1107,0,0.0605365,"Missing"
K16-1028,W08-1404,0,0.0472543,"g token from the source document can be displayed in the summary even when it is not part of the training vocabulary either on the source side or the target side. RNNs on the source side, one at the word level and the other at the sentence level. The attention mechanism operates at both levels simultaneously. The word-level attention is further re-weighted by the corresponding sentence-level attention and renormalized as shown below: P a (j) = Pwa (j)Psa (s(j)) PNd a a k=1 Pw (k)Ps (s(k)) al., 2002; Erkan and Radev, 2004; Wong et al., 2008a; Filippova and Altun, 2013; Colmenares et al., 2015; Litvak and Last, 2008; K. Riedhammer and Hakkani-Tur, 2010; Ricardo Ribeiro, 2013). Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004),"
K16-1028,C08-1018,0,0.00948802,"s stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004), used a combination of linguistically motivated compression techniques, and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a simi"
K16-1028,P15-1002,0,0.0271305,"., 2008b), but they are novel in the context of deep learning approaches for abstractive summarization, to the best of our knowledge. Switching generator-pointer model (Sec. 2.3): This model combines extractive and abstractive approaches to summarization in a single end-toend framework. Rush et al. (2015) also used a combination of extractive and abstractive approaches, but their extractive model is a separate log-linear classifier with handcrafted features. Pointer networks (Vinyals et al., 2015) have also been used earlier for the problem of rare words in the context of machine translation (Luong et al., 2015), but the novel addition of switch in our model allows it to strike a balance between when to be faithful to the original source (e.g., for named entities and OOV) and when it is allowed to be creative. We believe such a process arguably mimics how human produces summaries. For a more detailed treatment of this model, and experiments on multiple tasks, please refer to the parallel work published by some of the authors of this work (Gulcehre et al., 2016). Hierarchical attention model (Sec. 2.4): Previously proposed hierarchical encoder-decoder models use attention only at sentence-level (Li et"
K16-1028,N15-1014,0,0.012696,"s known, the corresponding token from the source document can be displayed in the summary even when it is not part of the training vocabulary either on the source side or the target side. RNNs on the source side, one at the word level and the other at the sentence level. The attention mechanism operates at both levels simultaneously. The word-level attention is further re-weighted by the corresponding sentence-level attention and renormalized as shown below: P a (j) = Pwa (j)Psa (s(j)) PNd a a k=1 Pw (k)Ps (s(k)) al., 2002; Erkan and Radev, 2004; Wong et al., 2008a; Filippova and Altun, 2013; Colmenares et al., 2015; Litvak and Last, 2008; K. Riedhammer and Hakkani-Tur, 2010; Ricardo Ribeiro, 2013). Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC-2004 task, called TOPIAR"
K16-1028,D13-1155,0,0.0863677,"he RNN. Once the position is known, the corresponding token from the source document can be displayed in the summary even when it is not part of the training vocabulary either on the source side or the target side. RNNs on the source side, one at the word level and the other at the sentence level. The attention mechanism operates at both levels simultaneously. The word-level attention is further re-weighted by the corresponding sentence-level attention and renormalized as shown below: P a (j) = Pwa (j)Psa (s(j)) PNd a a k=1 Pw (k)Ps (s(k)) al., 2002; Erkan and Radev, 2004; Wong et al., 2008a; Filippova and Altun, 2013; Colmenares et al., 2015; Litvak and Last, 2008; K. Riedhammer and Hakkani-Tur, 2010; Ricardo Ribeiro, 2013). Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC"
K16-1028,D15-1044,0,0.780207,"ed from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets. In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese d"
K16-1028,C08-1124,0,0.733791,"’ token encoded by the RNN. Once the position is known, the corresponding token from the source document can be displayed in the summary even when it is not part of the training vocabulary either on the source side or the target side. RNNs on the source side, one at the word level and the other at the sentence level. The attention mechanism operates at both levels simultaneously. The word-level attention is further re-weighted by the corresponding sentence-level attention and renormalized as shown below: P a (j) = Pwa (j)Psa (s(j)) PNd a a k=1 Pw (k)Ps (s(k)) al., 2002; Erkan and Radev, 2004; Wong et al., 2008a; Filippova and Altun, 2013; Colmenares et al., 2015; Litvak and Last, 2008; K. Riedhammer and Hakkani-Tur, 2010; Ricardo Ribeiro, 2013). Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best"
K16-1028,D10-1050,0,0.0118157,"er story generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004), used a combination of linguistically motivated compression techniques, and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder"
N07-1029,P05-1033,0,0.137728,"through this network. The alignment of speech recognition outputs is fairly straightforward due to the strict constraint in word order. However, machine translation outputs do not have this constraint as the word order may be different between the source and target languages. MT systems employ various re-ordering (distortion) models to take this into account. In recent years, machine translation systems based on new paradigms have emerged. These systems employ more than just the surface-level information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galley et al., 2006) systems have recently improved in both accuracy and scalability. Three MT system combination methods are presented in this paper. They operate on the sentence, phrase and word level. The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists. This method does not generate new hypotheses – unlike the phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sen"
N07-1029,A94-1016,0,0.891074,". The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists. This method does not generate new hypotheses – unlike the phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation s"
N07-1029,P05-3026,0,0.607609,"ion Systems  Antti-Veikko I. Rosti and Necip Fazil Ayan and Bing Xiang and Spyros Matsoukas and Richard Schwartz and Bonnie J. Dorr BBN Technologies, 10 Moulton Street, Cambridge, MA 02138   arosti,bxiang,smatsouk,schwartz @bbn.com   Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742  nfa,bonnie @umiacs.umd.edu  Abstract Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting in"
N07-1029,koen-2004-pharaoh,0,0.0363196,"imilarity score weights. The parameters  through L interpolate between the sum, average and maximum of the similarity scores. These interpolation weights and the system weights  are constrained to sum to one. The number of tunable combination weights, in ad  + dition to normal decoder weights, is  where  is the number of systems and is the + number of similarity levels; i.e.,  free system similarity score weights and two free inweights, terpolation weights.       4.2 Phrase-Based Decoding The phrasal decoder used in the phrase-level combination is based on standard beam search (Koehn, 2004). The decoder features are: a trigram language model score, number of target phrases, number of target words, phrase distortion, phrase distortion computed over the original translations and phrase translation confidences estimated in Section 4.1. The total score for a hypothesis is computed as a log-linear combination of these features. The feature weights and combination weights (system and similarity) may be tuned using Powell’s method on -best lists as described in Section 2. The phrase-level combination tuning can be summarized as follows:  1. Estimate sentence posteriors given the total"
N07-1029,E06-1005,0,0.801984,"I. Rosti and Necip Fazil Ayan and Bing Xiang and Spyros Matsoukas and Richard Schwartz and Bonnie J. Dorr BBN Technologies, 10 Moulton Street, Cambridge, MA 02138   arosti,bxiang,smatsouk,schwartz @bbn.com   Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742  nfa,bonnie @umiacs.umd.edu  Abstract Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006). Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from -best li"
N07-1029,J03-1002,0,0.00618995,"mber of features; i.e., $  system score scaling factors (  ), three free interpolation weights (Equation 4) for the scaling factor estimation,   GLM weights (  ), three free interpolation weights (Equation 4) for the hypothesis confidence estimation and two free LM re-scoring weights (Equation 6). All parameters may be tuned using Powell’s method on -best lists as described in Section 2. The tuning of the sentence-level combination method may be summarized as follows:     individual systems. If the alignments are not available, they can be automatically generated; e.g., using GIZA++ (Och and Ney, 2003). The phrase translation table is generated for each source sentence using confidence scores derived from sentence posteriors with system-specific total score scaling factors and similarity scores based on the agreement among the phrases from all systems. 4.1 Phrase Confidence Estimation   Each phrase has an initial confidence based on the sentence posterior  estimated from an -best list in the same fashion as in Section 3.2. The confidence of the phrase table entry is increased if several systems agree on the target words. The agreement is measured by four levels of similarity:  1. Same"
N07-1029,P03-1021,0,0.0903169,"een a system output and a targeted reference which preserves the meaning and fluency of the sentence (Snover et al., 2006). The targeted reference is generated by human posteditors who make edits to a reference translation so as to minimize the TER between the reference and 229  the vectors is gradually moved toward a larger positive change in the evaluation metric. To improve the chances of finding a global optimum, the algorithm is repeated with varying initial values. The modified Powell’s method has been previously used in optimizing the weights of a standard feature-based MT decoder in (Och, 2003) where a more efficient algorithm for log-linear models was proposed. However, this is specific to log-linear models and cannot be easily extended for more complicated functions. scaling factors were estimated from the tuning data , + to limit the feature values between   . The same scaling factors have to be applied to the features obtained from the test data. The total confidence score of hypothesis  is obtained from the system confidences  as + @   8      L      BED       &quot;! + @    BED  (4) where  is the number of systems generating the hypo"
N07-1029,P02-1040,0,0.112333,"Missing"
N07-1029,2006.amta-papers.25,1,0.904798,"phrase and word-level methods. The phrase-level combination  228 Proceedings of NAACL HLT 2007, pages 228–235, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics is based on extracting sentence-specific phrase translation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on consensus network decoding. Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell’s method (Brent, 1973) on -best lists. Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination. Experiments on combining six machine translation system outputs were performed. Three systems were phrasal, two hierarchical and one syntaxbased. The systems were evaluated on NIST MT05 and the newsgroup"
N07-1029,P06-1121,0,\N,Missing
N16-1063,N15-1011,0,0.011196,"Missing"
N16-1063,D14-1181,0,0.00999026,"e a novel NN initialization method that treats some of the neurons in the final hidden layer as dedicated neurons for each pattern of label co-occurrence. These dedicated neurons are initialized to connect to the corresponding cooccurring labels with stronger weights than to others. While initialization of an NN is an important research topic (Glorot and Bengio, 2010; Sutskever et al., 2013; Le et al., 2015), to the best of our knowledge, there has been no attempt to leverage label cooccurrence for NN initialization. 2 Related Work Along with the recent success in NNs (Collobert et al., 2011; Kim, 2014), NN-based multi-label classification has been proposed. An NN for NLQ classification needs to accept queries with variable length and output their labels. Figure 1 shows a typical NN architecture (Collobert et al., 2011). This NN first transforms words in the input query into word embeddings (Mikolov et al., 2013), then applies Convolutional Neural Network (CNN) and Max-pooling over time to extract fixed-length feature vectors, and feed them into the output layer to predict the label for the query (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014). To take care of multi-la"
N16-1063,P14-2105,0,0.0197249,"in NNs (Collobert et al., 2011; Kim, 2014), NN-based multi-label classification has been proposed. An NN for NLQ classification needs to accept queries with variable length and output their labels. Figure 1 shows a typical NN architecture (Collobert et al., 2011). This NN first transforms words in the input query into word embeddings (Mikolov et al., 2013), then applies Convolutional Neural Network (CNN) and Max-pooling over time to extract fixed-length feature vectors, and feed them into the output layer to predict the label for the query (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014). To take care of multi-labels, label co-occurrence has been incorporated into loss functions such as pairwise ranking loss (Zhang and Zhou, 2006). More recently, Nam et 521 Proceedings of NAACL-HLT 2016, pages 521–526, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics al. (2014) reported that binary cross entropy can outperform the pairwise ranking loss by leveraging rectified linear units (ReLUs) for nonlinearity (Nair and Hinton, 2010), AdaGrad for optimization (Duchi et al., 2011), and dropout for generalization (Srivastava et al., 2014). Considering"
P10-2005,P05-1066,0,0.092531,"Missing"
P10-2005,P09-2058,1,0.838923,"nt combination in the past has focused on how to combine the alignments from two different directions, sourceto-target and target-to-source. Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (Koehn et al., 2003), to achieve a better balance compared to using either intersection (high precision) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance than the baseline. More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model,"
P10-2005,P04-3014,0,0.0534274,"Missing"
P10-2005,D07-1006,0,0.0160649,"guistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. 22 Proceedings of the ACL 2010 Conference Short Papers, pages 22–26, c Uppsala, Sweden, 11-16 July 20"
P10-2005,D09-1024,0,0.0183355,"ology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. 22 Proceedings of the ACL 2010 Conference Short Papers, pages 22–26, c Uppsala, Sweden, 11-16 July 2010. 2010 Associati"
P10-2005,P09-1105,0,0.250249,"on) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance than the baseline. More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model). In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. There is no need for a pre-determined threshold as used in (Huang, 2009). Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners. Our strategy is to diversify and th"
P10-2005,H05-1012,0,0.020907,"erent motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. 22 Proceedings of the ACL 2010 Conference Short Papers, pages 22–26, c Uppsal"
P10-2005,N03-1017,0,0.116303,"Missing"
P10-2005,P00-1056,0,0.212809,"ght neighboring word is aligned to tk so far; • tk is not aligned and its left or right neighboring word is aligned to sj so far. 3. Repeat scanning all candidate links until no more links can be added. In this way, those alignment links with higher confidence scores have higher priority to be included in the combined alignment. 4 Experiments 4.1 Baseline Our training data contains around 70K EnglishPashto sentence pairs released under the DARPA TRANSTAC project, with about 900K words on the English side. The baseline is a phrase-based MT system similar to (Koehn et al., 2003). We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then 24 gdf. We also notice that its higher F-score is mainly due to the higher precision, which should result from the consideration of confidence scores. Alignment Baseline V S P X B+V B+V B+V B+S B+P B+X B+V+P B+V+S+P B+V+S+P Comb c0 c1 c2 c2 c2 c2 c2 gdf c2 P 0.6923 0.6934 0.7376 0.7665 0.7615 0.7639 0.7645 0.7895 0.7942 0.8006 0.7827 0.7912 0.7238 0.7906 R 0.6414 0.6388 0.6495 0.6643 0.6641 0.6312 0.6373 0.6505 0.6553 0.6612 0.6670 0.6755 0.7042 0.6852 F 0.6659 0.6650 0.6907 0.7118 0.7095 0.6913 0.6951 0.7133 0.7181 0.7242 0.7202"
P10-2005,P03-1021,0,0.0199596,"nce of aijk as Stemming Pashto is one of the morphologically rich languages. In addition to the linguistic knowledge applied in the syntactic reordering described above, we also utilize morphological analysis by applying stemming on both the English and Pashto sides. For English, we use Porter stemming (Porter, c(aijk |S, T ) = 23 q qs2t (aijk |S, T )qt2s (aijk |T, S), (1) where the source-to-target link posterior probability pi (tk |sj ) qs2t (aijk |S, T ) = PK , k ′ =1 pi (tk ′ |sj ) apply grow-diagonal-final (gdf). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). (2) and the target-to-source link posterior probabil"
P10-2005,P02-1040,0,0.078807,"e of the morphologically rich languages. In addition to the linguistic knowledge applied in the syntactic reordering described above, we also utilize morphological analysis by applying stemming on both the English and Pashto sides. For English, we use Porter stemming (Porter, c(aijk |S, T ) = 23 q qs2t (aijk |S, T )qt2s (aijk |T, S), (1) where the source-to-target link posterior probability pi (tk |sj ) qs2t (aijk |S, T ) = PK , k ′ =1 pi (tk ′ |sj ) apply grow-diagonal-final (gdf). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). (2) and the target-to-source link posterior probability qt2s (aijk |T, S) is defined similarly. pi ("
P10-2005,W97-0301,0,0.009103,"eights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). (2) and the target-to-source link posterior probability qt2s (aijk |T, S) is defined similarly. pi (tk |sj ) is the lexical translation probability between source word sj and target word tk in the i-th set of alignments. Our alignment combination algorithm is as follows. 1. Each candidate link ajk gets soft votes from N sets of alignments via weighted confidence scores: v(ajk |S, T ) = N X wi ∗ c(aijk |S, T ), 4.2 Improvement in Word Alignment In Table 1 we show the precision, recall and Fscore of each set of word alignments for the 150sentence set. Using partial word provides the highest F-"
P10-2005,D07-1077,0,0.0237779,"nglish-to-Pashto MT task as an example and create three sets of additional alignments on top of the baseline alignment. 2.1 P: they hQvy E’: they are stAsO your VP NP VBP NNS your employees and you kArvAl employees dy Av are and know NP ADVP PRP RB them tAsO hQvy smh you them well well pOZnB know Figure 1: Alignment before/after VP-based reordering. Syntactic Reordering Pashto is a subject-object-verb (SOV) language, which puts verbs after objects. People have proposed different syntactic rules to pre-reorder SOV languages, either based on a constituent parse tree (Dr´abek and Yarowsky, 2004; Wang et al., 2007) or dependency parse tree (Xu et al., 2009). In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse. The VP-based reordering rule we apply in the work is: 1980), a widely applied algorithm to remove the common morphological and inflexional endings from words in English. For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et al., 2006). We start from a fixed set of affixes with 8 prefixes and 21 suffixes. The prefixes and suffixes are stripped off from the Pashto"
P10-2005,N09-1028,0,0.0265802,"eate three sets of additional alignments on top of the baseline alignment. 2.1 P: they hQvy E’: they are stAsO your VP NP VBP NNS your employees and you kArvAl employees dy Av are and know NP ADVP PRP RB them tAsO hQvy smh you them well well pOZnB know Figure 1: Alignment before/after VP-based reordering. Syntactic Reordering Pashto is a subject-object-verb (SOV) language, which puts verbs after objects. People have proposed different syntactic rules to pre-reorder SOV languages, either based on a constituent parse tree (Dr´abek and Yarowsky, 2004; Wang et al., 2007) or dependency parse tree (Xu et al., 2009). In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse. The VP-based reordering rule we apply in the work is: 1980), a widely applied algorithm to remove the common morphological and inflexional endings from words in English. For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et al., 2006). We start from a fixed set of affixes with 8 prefixes and 21 suffixes. The prefixes and suffixes are stripped off from the Pashto words under the two constraints:(1) Longest"
P10-2005,J93-2003,0,\N,Missing
P10-2005,N06-1013,0,\N,Missing
P11-2074,P08-1024,0,0.0616792,"to millions). For example, the system described in (Koehn 424 et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set. Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to apply the same approach to millions of featur"
P11-2074,N09-1025,0,0.0937869,"res include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set. Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to apply the same approach to millions of features due to the data sparseness issue, since the development set is usually small. In (Ittycheriah and Roukos, 2007), a maximum entropy (ME) model is proposed, which utilizes millions of features. All"
P11-2074,D10-1044,0,0.022976,"g features from ME alignment only (note that phrases are from all the alignments). With the mixture model, 427 we can achieve another 0.5 gain compared to the baseline, especially with less number of features. This presents a new way of doing alignment combination in the feature space instead of in the usual phrase space. System Newswire Weblog UN Baseline Mixture Features 8898K 1990K 4700K 18159K 15588K BLEU 38.82 38.20 38.21 39.36 39.81 Table 3: MT results with different training sub-corpora, baseline, or mixture model. 4.4 Domain Adaptation Another popular task in SMT is domain adaptation (Foster et al., 2010). It tries to take advantage of any out-of-domain training data by combining them with the in-domain data in an appropriate way. In our sub-sampled training corpus, there exist three subsets: newswire (1M sentences), weblog (200K), and UN data (300K). We train three mixture components, each on one of the training subsets. All results are compared in Table 3. The baseline that was trained on all the data achieved 0.5 gain compared to using the newswire training data alone (understandably it is the best component given the newswire test data). Note that since the baseline is trained on subsample"
P11-2074,H05-1012,1,0.836727,"tion features based on morphological analysis that examine source morphemes, target word and jump; • F6: Part-of-speech (POS) features that examine the source and target POS tags and their neighbors, along with target word and jump; • F7: Source parse tree features that collect the information from the parse labels of the source words and their siblings in the parse trees, along with target word and jump; with about 10M sentence pairs and 300M words in total (counted at the English side). For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al., 1996). Our tuning and test sets are extracted from the GALE DEV10 Newswire set, with no overlap between tuning and test. There are 1063 sentences (168 documents) in the tuning set, and 1089 sentences (168 documents) in the test set. Both sets have one reference translation for each sentence. Instead of using all the training data, we sample the training corpus based on the tuning/test set to train the systems more efficiently. In the end, about 1.5M sentence pairs are selected for the sampled training. A 5-gram language m"
P11-2074,N07-1008,1,0.946257,"ations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set. Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to apply the same approach to millions of features due to the data sparseness issue, since the development set is usually small. In (Ittycheriah and Roukos, 2007), a maximum entropy (ME) model is proposed, which utilizes millions of features. All the feature weights are trained with a maximum-likelihood (ML) approach on the full training corpus. It achieves significantly better performance than a normal phrase-based system. However, the estimation of feature weights has no direct connection with the final translation perforProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 424–428, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics mance. In this paper, we propose"
P11-2074,N03-1017,0,0.0198989,"ize the translation performance. This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT. It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task. 1 Introduction Significant progress has been made in statistical machine translation (SMT) in recent years. Among all the proposed approaches, the phrasebased method (Koehn et al., 2003) has become the widely adopted one in SMT due to its capability of capturing local context information from adjacent words. There exists significant amount of work focused on the improvement of translation performance with better features. The feature set could be either small (at the order of 10), or large (up to millions). For example, the system described in (Koehn 424 et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases,"
P11-2074,P06-1096,0,0.286558,"Missing"
P11-2074,P00-1056,0,0.057773,"t examine source morphemes, target word and jump; • F6: Part-of-speech (POS) features that examine the source and target POS tags and their neighbors, along with target word and jump; • F7: Source parse tree features that collect the information from the parse labels of the source words and their siblings in the parse trees, along with target word and jump; with about 10M sentence pairs and 300M words in total (counted at the English side). For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al., 1996). Our tuning and test sets are extracted from the GALE DEV10 Newswire set, with no overlap between tuning and test. There are 1063 sentences (168 documents) in the tuning set, and 1089 sentences (168 documents) in the test set. Both sets have one reference translation for each sentence. Instead of using all the training data, we sample the training corpus based on the tuning/test set to train the systems more efficiently. In the end, about 1.5M sentence pairs are selected for the sampled training. A 5-gram language model is trained from the English Gigaw"
P11-2074,P02-1038,0,0.384707,"ine translation (SMT) in recent years. Among all the proposed approaches, the phrasebased method (Koehn et al., 2003) has become the widely adopted one in SMT due to its capability of capturing local context information from adjacent words. There exists significant amount of work focused on the improvement of translation performance with better features. The feature set could be either small (at the order of 10), or large (up to millions). For example, the system described in (Koehn 424 et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expe"
P11-2074,P03-1021,0,0.244417,"s. There exists significant amount of work focused on the improvement of translation performance with better features. The feature set could be either small (at the order of 10), or large (up to millions). For example, the system described in (Koehn 424 et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of transla"
P11-2074,P02-1040,0,0.0810807,"log Zk (s) Z(s) k X X + wk λki φki (t, j, s). (4) 2 Maximum-Entropy Model for MT In this section we give a brief review of a special maximum-entropy (ME) model as introduced in (Ittycheriah and Roukos, 2007). The model has the following form, X p0 (t, j|s) exp λi φi (t, j, s), (1) p(t, j|s) = Z(s) p0 (t, j|s) Y pk (t, j|s)wk . Z(s) = log k i The individual feature weights λki for the i-th feature in cluster k are estimated in the maximumentropy framework as in the baseline model. However, the mixture weights wk can be optimized directly towards the translation evaluation metric, such as BLEU (Papineni et al., 2002), along with other usual costs (e.g. language model scores) on a development set. Note that the number of mixture components is relatively small (less than 10) compared to millions of features in baseline. Hence the optimization can be conducted easily to generate reliable mixture weights for decoding with MERT (Och, 2003) or other optimization algorithms, such as the Simplex Armijo Downhill algorithm proposed in (Zhao and Chen, 2009). 3.2 Partition of Feature Space Given the proposed mixture model, how to split the feature space into multiple regions becomes crucial. In order to surpass the b"
P11-2074,P06-1091,0,0.438755,"ther small (at the order of 10), or large (up to millions). For example, the system described in (Koehn 424 et al., 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The features include phrase translation probabilities, lexical probabilities, number of phrases, and language model scores, etc. The feature weights are usually optimized with minimum error rate training (MERT) as in (Och, 2003). Besides the MERT-based feature weight optimization, there exist other alternative discriminative training methods for MT, such as in (Tillmann and Zhang, 2006; Liang et al., 2006; Blunsom et al., 2008). However, scalability is a challenge for these approaches, where all possible translations of each training example need to be searched, which is computationally expensive. In (Chiang et al., 2009), there are 11K syntactic features proposed for a hierarchical phrase-based system. The feature weights are trained with the Margin Infused Relaxed Algorithm (MIRA) efficiently on a forest of translations from a development set. Even though significant improvement has been obtained compared to the baseline that has small number of features, it is hard to ap"
P11-2074,C96-2141,0,0.248378,"and jump; • F6: Part-of-speech (POS) features that examine the source and target POS tags and their neighbors, along with target word and jump; • F7: Source parse tree features that collect the information from the parse labels of the source words and their siblings in the parse trees, along with target word and jump; with about 10M sentence pairs and 300M words in total (counted at the English side). For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al., 1996). Our tuning and test sets are extracted from the GALE DEV10 Newswire set, with no overlap between tuning and test. There are 1063 sentences (168 documents) in the tuning set, and 1089 sentences (168 documents) in the test set. Both sets have one reference translation for each sentence. Instead of using all the training data, we sample the training corpus based on the tuning/test set to train the systems more efficiently. In the end, about 1.5M sentence pairs are selected for the sampled training. A 5-gram language model is trained from the English Gigaword corpus and the English portion of th"
P11-2074,2004.tmi-1.9,0,0.0479759,"subsets. All results are compared in Table 3. The baseline that was trained on all the data achieved 0.5 gain compared to using the newswire training data alone (understandably it is the best component given the newswire test data). Note that since the baseline is trained on subsampled training data, there is already certain domain adaptation effect involved. On top of that, the mixture model results in another 0.45 gain in BLEU. All the improvements in the mixture models above against the baseline are statistically significant with p-value &lt; 0.0001 by using the confidence tool described in (Zhang and Vogel, 2004). 5 Conclusion In this paper we presented a novel discriminative mixture model for bridging the gap between the maximum-likelihood training and the discriminative training in SMT. We partition the feature space into multiple regions. The features in each region are tied together to share the same mixture weights that are optimized towards the maximum BLEU scores. It was shown that the same model structure can be effectively applied to feature combination, alignment combination and domain adaptation. We also point out that it is straightforward to combine any of these three. For example, we can"
P11-2074,N09-2006,0,0.511327,"mentropy framework as in the baseline model. However, the mixture weights wk can be optimized directly towards the translation evaluation metric, such as BLEU (Papineni et al., 2002), along with other usual costs (e.g. language model scores) on a development set. Note that the number of mixture components is relatively small (less than 10) compared to millions of features in baseline. Hence the optimization can be conducted easily to generate reliable mixture weights for decoding with MERT (Och, 2003) or other optimization algorithms, such as the Simplex Armijo Downhill algorithm proposed in (Zhao and Chen, 2009). 3.2 Partition of Feature Space Given the proposed mixture model, how to split the feature space into multiple regions becomes crucial. In order to surpass the baseline model, where all features can be viewed as existing in a single mixture component, the separated mixture components should be complementary to each other. In this work, we explore three different ways of partitions, based on either feature types, word alignment types, or the domain of training data. In the feature-type-based partition, we split the ME features into 8 categories: • F1: Lexical features that examine source word,"
P11-2074,P08-1000,0,\N,Missing
P13-1081,P03-1055,0,0.106741,"ased system trained on only 60K sentences, while we conduct experiments on more advanced Hiero and tree-to-string systems, trained on 2M sentences in a much larger corpus. We directly take advantage of the augmented parse trees in the tree-to-string grammar, which could have larger impact on the MT system performance. Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang"
P13-1081,N06-1024,0,0.0252752,"nly 60K sentences, while we conduct experiments on more advanced Hiero and tree-to-string systems, trained on 2M sentences in a much larger corpus. We directly take advantage of the augmented parse trees in the tree-to-string grammar, which could have larger impact on the MT system performance. Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where"
P13-1081,D07-1027,0,0.0294301,"Missing"
P13-1081,D11-1125,0,0.0346264,"Missing"
P13-1081,P02-1018,0,0.0593778,"y use a phase-based system trained on only 60K sentences, while we conduct experiments on more advanced Hiero and tree-to-string systems, trained on 2M sentences in a much larger corpus. We directly take advantage of the augmented parse trees in the tree-to-string grammar, which could have larger impact on the MT system performance. Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a"
P13-1081,N03-1017,0,0.0472565,"Missing"
P13-1081,P11-2037,0,0.434892,"odifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where each word in the sentence receives a tag indicating whether there is an EC before it. A maximum entropy model is utilized to predict the tags, but different types of ECs are not distinguished. In (Cai et al., 2011), a language-independent method was proposed to integrate the recovery of empty elements into syntactic parsing. As shown in the previous section, our model outperforms the model in (Yang and Xue, 2010) and (Cai et al., 2011) significantly using the same training and test data. (Luo and Zhao, 2011) also tries to predict the existence of an EC 6 Conclusions and Future Work In this paper, we presented a novel structured approach to EC prediction, which utilizes a maximum entropy model with various syntactic features and shows significantly higher accuracy than the state-of-the-art approaches. We"
P13-1081,P05-1033,0,0.300708,"nted Data With the pre-processed MT training corpus, an unsupervised word aligner, such as GIZA++, can be used to generate automatic word alignment, as the first step of a system training pipeline. The effect of inserting ECs is two-fold: first, it can impact the automatic word alignment since now it allows the target-side words, especially the function words, to align to the inserted ECs and fix some errors in the original word alignment; second, new phrases and rules can be extracted from the preprocessed training data. For example, for a hierarchical MT system, some phrase pairs and Hiero (Chiang, 2005) rules can be extracted with recovered *pro* and *PRO* at the Chinese side. In this work we also take advantages of the augmented Chinese parse trees (with ECs projected to the surface) and extract tree-to-string grammar (Liu et al., 2006) for a tree-to-string MT system. Due to the recovered ECs in the source parse trees, the tree-to-string grammar extracted from such trees can be more discriminative, with an increased capability of distinguishing different context. An example of an augmented Chinese parse tree aligned to an English string is shown in Figure 3, in which the incorrect alignment"
P13-1081,D10-1062,0,0.452167,"acted Hiero rules and tree-to-string rules are also listed, which we would not have been able to extract from the original incorrect word alignment when the *pro* was missing. Table 2: List of features. 3 Integrating Empty Categories in Machine Translation In this section, we explore multiple approaches of utilizing recovered ECs in machine translation. 3.1 Explicit Recovery of ECs in MT We conducted some initial error analysis on our MT system output and found that most of the errors that are related to ECs are due to the missing *pro* and *PRO*. This is also consistent with the findings in (Chung and Gildea, 2010). One of the other frequent ECs, *OP*, appears in the Chinese relative clauses, which usually have a Chinese word “De” aligned to the target side “that” or “which”. And the trace, *T*, exists in both Chinese and English sides. For MT we want to focus on the places where there exist mismatches between the source and target languages. A straightforward way of utilizing the recovered *pro* and *PRO* is to pre-process the MT training and test data by inserting ECs into the original source text (i.e. Chinese in this case). As mentioned in the previous section, the output of our EC predictor is a ne"
P13-1081,P05-1066,0,0.0744819,"Missing"
P13-1081,P06-1077,0,0.013018,"ld: first, it can impact the automatic word alignment since now it allows the target-side words, especially the function words, to align to the inserted ECs and fix some errors in the original word alignment; second, new phrases and rules can be extracted from the preprocessed training data. For example, for a hierarchical MT system, some phrase pairs and Hiero (Chiang, 2005) rules can be extracted with recovered *pro* and *PRO* at the Chinese side. In this work we also take advantages of the augmented Chinese parse trees (with ECs projected to the surface) and extract tree-to-string grammar (Liu et al., 2006) for a tree-to-string MT system. Due to the recovered ECs in the source parse trees, the tree-to-string grammar extracted from such trees can be more discriminative, with an increased capability of distinguishing different context. An example of an augmented Chinese parse tree aligned to an English string is shown in Figure 3, in which the incorrect alignment in Figure 1 is fixed. A few examples of the extracted Hiero rules and tree-to-string rules are also listed, which we would not have been able to extract from the original incorrect word alignment when the *pro* was missing. Table 2: List"
P13-1081,P11-1123,1,0.865531,"ith their intended usages. Readers are referred to the documentation (Xue et al., 2005) of CTB for detailed discussions about the characterization of empty categories. EC *T* * *PRO* *pro* *OP* *RNR* Meaning trace of A’-movement trace of A-movement big PRO in control structures pro-drop operator in relative clauses for right node raising Table 1: List of empty categories in the CTB. In this section, we tackle the problem of recovering Chinese ECs. The problem has been studied before in the literature. For instance, Yang and Xue (2010) attempted to predict the existence of an EC before a word; Luo and Zhao (2011) predicted ECs on parse trees, but the position information of some ECs is partially lost in their representation. Furthermore, Luo and Zhao (2011) conducted experiments on gold parse trees only. In 823 Figure 2: Example of tree transformation on training data to encode an empty category and its position information. tactic tree, as opposed to simply attaching it to a neighboring word, as was done in (Yang and Xue, 2010). We believe this is one of the reasons why our model has better accuracy than that of (Yang and Xue, 2010) (cf. Table 7). In summary, a projected tag consists of an EC type (s"
P13-1081,J93-2004,0,0.0430812,"the predicted ECs into a Chinese-to-English machine translation task through multiple approaches, including the extraction of EC-specific sparse features. We show that the recovered empty categories not only improve the word alignment quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system. Figure 1: Example of incorrect word alignment due to missing pronouns on the Chinese side. In order to account for certain language phenomena such as pro-drop and wh-movement, a set of special tokens, called empty categories (EC), are used in Penn Treebanks (Marcus et al., 1993; Bies and Maamouri, 2003; Xue et al., 2005). Since empty categories do not exist in the surface form of a language, they are often deemed elusive and recovering ECs is even figuratively called “chasing the ghost” (Yang and Xue, 2010). In this work we demonstrate that, with the availability of large-scale EC annotations, it is feasible to predict and recover ECs with high accuracy. More importantly, with various approaches of modeling the recovered ECs in SMT, we are able to achieve significant improvements1 . The contributions of this paper include the following: 1 Introduction One of the key"
P13-1081,P00-1056,0,0.0410984,"Missing"
P13-1081,P02-1040,0,0.088566,"Missing"
P13-1081,W97-0301,0,0.0615855,"Missing"
P13-1081,C10-2158,0,0.350208,"quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system. Figure 1: Example of incorrect word alignment due to missing pronouns on the Chinese side. In order to account for certain language phenomena such as pro-drop and wh-movement, a set of special tokens, called empty categories (EC), are used in Penn Treebanks (Marcus et al., 1993; Bies and Maamouri, 2003; Xue et al., 2005). Since empty categories do not exist in the surface form of a language, they are often deemed elusive and recovering ECs is even figuratively called “chasing the ghost” (Yang and Xue, 2010). In this work we demonstrate that, with the availability of large-scale EC annotations, it is feasible to predict and recover ECs with high accuracy. More importantly, with various approaches of modeling the recovered ECs in SMT, we are able to achieve significant improvements1 . The contributions of this paper include the following: 1 Introduction One of the key challenges in statistical machine translation (SMT) is to effectively model inherent differences between the source and the target language. Take the Chinese-English SMT as an example: it is non-trivial to produce correct pronouns on"
P13-1081,D08-1060,0,0.0502522,"Missing"
P13-1081,D07-1057,0,0.0851818,"nce. Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where each word in the sentence receives a tag indicating whether there is an EC before it. A maximum entropy model is utilized to predict the tags, but different types of ECs are not distinguished. In (Cai et al., 2011), a language-independent method was proposed to integrate the recovery of"
P13-1124,D10-1014,1,\N,Missing
P13-1124,N10-1016,0,\N,Missing
P13-1124,N04-4026,0,\N,Missing
P13-1124,W09-2307,0,\N,Missing
P13-1124,D09-1105,0,\N,Missing
P13-1124,N09-1053,0,\N,Missing
P13-1124,D08-1089,0,\N,Missing
P13-1124,W09-0435,0,\N,Missing
P13-1124,P11-2080,0,\N,Missing
P13-1124,D08-1024,0,\N,Missing
P13-1124,N09-1027,0,\N,Missing
P13-1124,D09-1008,1,\N,Missing
P13-1124,P07-2045,0,\N,Missing
P13-1124,P12-1095,0,\N,Missing
P13-1124,C10-1043,0,\N,Missing
P13-1124,P08-1114,0,\N,Missing
P13-1124,W06-1609,0,\N,Missing
P13-1124,P05-1033,0,\N,Missing
P13-1124,D11-1045,0,\N,Missing
P13-1124,W06-3108,0,\N,Missing
P13-1124,P09-1037,1,\N,Missing
P13-1124,P07-1090,1,\N,Missing
P13-1124,P08-1066,1,\N,Missing
P13-1124,P06-1090,0,\N,Missing
P13-1124,D11-1125,0,\N,Missing
P14-2071,P13-2005,0,0.0912633,"guish the two cases above, they still may not be enough to get the sentiment on the whole message correct. Also, the local features often suffer from the sparsity problem. This motivates us to explore topic information explicitly in the task of sentiment analysis on Twitter data. There exists some work on applying topic information in sentiment analysis, such as (Mei et al., 2007), (Branavan et al., 2008), (Jo and Oh, 2011) and (He et al., 2012). All these work are significantly different from what we propose in this work. Also they are conducted in a domain other than Twitter. Most recently, Si et al. (2013) propose a continuous Dirichlet Process Mixture model for Twitter sentiment, for the purpose of stock prediction. Unfortunately there is no evaluation on the accuracy of sentiment classification alone in that work. Furthermore, no standard training or test corpus is used, which makes comparison with other approaches difficult. Our work is organized in the following way: Introduction Social media, such as Twitter and Facebook, has attracted significant attention in recent years. The vast amount of data available online provides a unique opportunity to the people working on natural language proc"
P14-2071,P08-1031,0,0.0243091,"is used as a negative word (as in the first tweet), but it bears no sentiment in the second tweet when people are talking about a football game. Even though some local contextual features could be helpful to distinguish the two cases above, they still may not be enough to get the sentiment on the whole message correct. Also, the local features often suffer from the sparsity problem. This motivates us to explore topic information explicitly in the task of sentiment analysis on Twitter data. There exists some work on applying topic information in sentiment analysis, such as (Mei et al., 2007), (Branavan et al., 2008), (Jo and Oh, 2011) and (He et al., 2012). All these work are significantly different from what we propose in this work. Also they are conducted in a domain other than Twitter. Most recently, Si et al. (2013) propose a continuous Dirichlet Process Mixture model for Twitter sentiment, for the purpose of stock prediction. Unfortunately there is no evaluation on the accuracy of sentiment classification alone in that work. Furthermore, no standard training or test corpus is used, which makes comparison with other approaches difficult. Our work is organized in the following way: Introduction Social"
P14-2071,H05-1044,0,0.0220168,"model for Twitter sentiment. The model is integrated in the framework of semi-supervised training that takes advantage of large amount of un-annotated Twitter data. Such a mixture model results in further improvement on the sentiment classification accuracy. • Manual lexicons: it has been shown in other work (Nakov et al., 2013) that lexicons with positive and negative words are important to sentiment classification. In this work, we adopt the lexicon from Bing Liu (Hu and Liu, 2004) which includes about 2000 positive words and 4700 negative words. We also experimented with the popular MPQA (Wilson et al., 2005) lexicon but found no extra improvement on accuracies. A short list of Twitter-specific positive/negative words are also added to enhance the lexicons. We generate two features based on the lexicons: total number of positive words or negative words found in each tweet. • We propose a smoothing technique through interpolation between universal model and topic-based mixture model. • We also compare different approaches for topic modeling, such as cross-domain topic identification by utilizing data from newswire domain. 2 Universal Sentiment Classifier • Emoticons: it is known that people use emo"
P14-2071,P11-2008,0,0.0213691,"Missing"
P14-2071,S13-2053,0,0.0629441,"ve been utilized for * This work was done when the author was with Thomson Reuters. 434 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 434–439, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics It is shown in Section 5 that such customized tokenization is helpful. Here are the features that we use for classification: • We first propose a universal sentiment model that utilizes various features and resources. The universal model outperforms the top system submitted to the SemEval-2013 task (Mohammad et al., 2013), which was trained and tested on the same data. The universal model serves as a strong baseline and also provides an option for smoothing later. • Word N-grams: if certain N-gram (unigram, bigram, trigram or 4-gram) appears in the tweet, the corresponding feature is set to 1, otherwise 0. These features are collected from training data, with a count cutoff to avoid overtraining. • We introduce a topic-based mixture model for Twitter sentiment. The model is integrated in the framework of semi-supervised training that takes advantage of large amount of un-annotated Twitter data. Such a mixture"
P14-2071,S13-2052,0,0.523499,"g (NLP) and related fields. Sentiment analysis is one of the areas that has large potential in real-world applications. For example, monitoring the trend of sentiment for a specific company or product mentioned in social media can be useful in stock prediction and product marketing. In this paper, we focus on sentiment analysis of Twitter data (tweets). It is one of the challenging tasks in NLP given the length limit on each tweet (up to 140 characters) and also the informal conversation. Many approaches have been proposed previously to improve sentiment analysis on Twitter data. For example, Nakov et al. (2013) provide an overview on the systems submitted to one of the SemEval-2013 tasks, Sentiment Analysis in Twitter. A variety of features have been utilized for * This work was done when the author was with Thomson Reuters. 434 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 434–439, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics It is shown in Section 5 that such customized tokenization is helpful. Here are the features that we use for classification: • We first propose a universal sentiment"
P15-1061,D09-1149,0,0.164217,"tation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. Introduction Relation classification is an important Natural Language Processing (NLP) task which is normally used as an intermediate step in many complex NLP applications such as question-answering and automatic knowledge base construction. Since the last decade there has been increasing interest in applying machine learning approaches to this task (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010). One reason is the availability of benchmark datasets such as the SemEval-2010 626 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 626–634, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics The remainder of the paper is structured as follows. Section 2 details the proposed neural network. In Section 3, we present details about the setup of experimental evaluation, and then describe the results in Section 4. In Sect"
P15-1061,S10-1057,0,0.870625,"icial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. Introduction Relation classification is an important Natural Language Processing (NLP) task which is normally used as an intermediate step in many complex NLP applications such as question-answering and automatic knowledge base construction. Since the last decade there has been increasing interest in applying machine learning approaches to this task (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010). One reason is the availability of benchmark datasets such as the SemEval-2010 626 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 626–634, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics The remainder of the paper is structured as follows. Section 2 details the proposed neural network. In Section 3, we present details about the setup of experimental evaluation, and then describe the results in Section 4. In Section 5, we discuss previous"
P15-1061,D12-1110,0,0.777357,"ur experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. 1 The [introduction]e1 in the [book]e2 is a summary of what is in the text. Some recent work on relation classification has focused on the use of deep neural networks with the aim of reducing the number of handcrafted features (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014). However, in order to achieve state-ofthe-art results these approaches still use some features derived from lexical resources such as WordNet or NLP tools such as dependency parsers and named entity recognizers (NER). In this work, we propose a new convolutional neural network (CNN), which we name Classification by Ranking CNN (CR-CNN), to tackle the relation classification task. The proposed network learns a distributed vector representation for each relation class. Given an input text segment, the network uses a convolutional layer to produce a distribut"
P15-1061,C14-1008,1,0.41496,"ameters to be chosen by the user. It is important to note that dc corresponds to the size of the sentence representation. Sentence Representation 2.4 The next step in the NN consists in creating the distributed vector representation rx for the input sentence x. The main challenges in this step are the sentence size variability and the fact that important information can appear at any position in the sentence. In recent work, convolutional approaches have been used to tackle these issues when creating representations for text segments of different sizes (Zeng et al., 2014; Hu et al., 2014; dos Santos and Gatti, 2014) and characterlevel representations of words of different sizes (dos Santos and Zadrozny, 2014). Here, we use a convolutional layer to compute distributed vector representations of the sentence. The convolutional layer first produces local features around each word in the sentence. Then, it combines these local features using a max operation to create a fixed-sized vector for the input sentence. Given a sentence x, the convolutional layer applies a matrix-vector operation to each window of size k of successive windows in embx = {rw1 , rw2 , ..., rwN }. Let us define the vector zn ∈ w Rd k as t"
P15-1061,N03-1033,0,0.011564,"used in our experiments are initialized by means of unsupervised pre-training. We perform pre-training using the skip-gram NN architecture (Mikolov et al., 2013) available in the word2vec tool. We use the December 2013 snapshot of the English Wikipedia corpus to train word embeddings with word2vec. We preprocess the Wikipedia text using the steps described in (dos Santos and Gatti, 2014): (1) removal of paragraphs that are not in English; (2) substitution of non-western characters for a special character; (3) tokenization of the text using the tokenizer available with the Stanford POS Tagger (Toutanova et al., 2003); (4) removal of sentences that are less than 20 characters long (including white spaces) or have less than 5 tokens. (5) lowercase all words and substitute each numerical digit by a 0. The resulting clean corpus contains about 1.75 billion tokens. 3.3 Neural Network Hyper-parameter We use 4-fold cross-validation to tune the neural network hyperparameters. Learning rates in the range of 0.03 and 0.01 give relatively similar results. Best results are achieved using between 10 and 15 training epochs, depending on the CR-CNN configuration. In Table 1, we show the selected hyperparameter values. A"
P15-1061,D14-1002,0,0.0174498,"core sθ (x)c− decreases. Training CR-CNN by minimizing the loss function in Equation 1 has the effect of training to give scores greater than m+ for the correct class and (negative) scores smaller than −m− for incorrect classes. In our experiments we set γ to 2, m+ to 2.5 and m− to 0.5. We use L2 regularization by adding the term βkθk2 to Equation 1. In our experiments we set β to 0.001. We use stochastic gradient descent (SGD) to minimize the loss function with respect to θ. Like some other ranking approaches that only update two classes/examples at every training round (Weston et al., 2011; Gao et al., 2014), we can efficiently train the network for tasks which have a very large number of classes. This is an advantage over softmax classifiers. On the other hand, sampling informative negative classes/examples can have a significant impact in the effectiveness of the learned model. In the case of our loss function, more informative negative classes are the ones with a score larger than −m− . The number of classes in the relation classification dataset that we use in our experiments is small. Therefore, in our experiments, given a sentence x with class label y + , the incorrect class c− that we choo"
P15-1061,D14-1194,0,0.0160439,"Missing"
P15-1061,S10-1006,0,0.433515,"Missing"
P15-1061,C14-1220,0,0.131552,"ts show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. 1 The [introduction]e1 in the [book]e2 is a summary of what is in the text. Some recent work on relation classification has focused on the use of deep neural networks with the aim of reducing the number of handcrafted features (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014). However, in order to achieve state-ofthe-art results these approaches still use some features derived from lexical resources such as WordNet or NLP tools such as dependency parsers and named entity recognizers (NER). In this work, we propose a new convolutional neural network (CNN), which we name Classification by Ranking CNN (CR-CNN), to tackle the relation classification task. The proposed network learns a distributed vector representation for each relation class. Given an input text segment, the network uses a convolutional layer to produce a distributed vector represent"
P15-1061,P14-1062,0,0.00694751,"reat it as a multiclass classification problem and apply a variety of machine learning techniques to the task in order to achieve a high accuracy. Recently, deep learning (Bengio, 2009) has become an attractive area for multiple applications, including computer vision, speech recognition and natural language processing. Among the different deep learning strategies, convolutional neural networks have been successfully applied to different NLP task such as part-of-speech tagging (dos Santos and Zadrozny, 2014), sentiment analysis (Kim, 2014; dos Santos and Gatti, 2014), question classification (Kalchbrenner et al., 2014), semantic role labeling (Collobert et al., 2011), hashtag prediction (Weston et al., 2014), sentence completion and response matching (Hu et al., 2014). Some recent work on deep learning for relation classification include Socher et al. (2012), Zeng et al. (2014) and Yu et al. (2014). In (Socher et al., 2012), the authors tackle relation classification using a recursive neural network (RNN) that assigns a matrix-vector representation to every node in a parse tree. The representation for the complete sentence is computed bottom-up by recursively combining the words according to the syntactic s"
P15-1061,D14-1181,0,\N,Missing
P15-2029,E06-1011,0,0.0423355,"Missing"
P15-2029,C04-1121,0,0.176218,"Missing"
P15-2029,D15-1279,0,0.0480131,"but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grandparent, great-grand-parent, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work, Mou et al. (2015, unpublished) reported related efforts; see Sec. 3.3. In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To combine deep learning with linguistic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achi"
P15-2029,P05-1015,0,0.29457,"Missing"
P15-2029,D14-1070,0,0.0374971,"Missing"
P15-2029,P06-1063,0,0.0715932,"Missing"
P15-2029,P14-1062,0,0.550006,", thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully a"
P15-2029,D14-1181,0,0.224626,"ements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds? So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grandparent, great-grand-parent, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work,"
P15-2029,D11-1014,0,0.694798,"being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully affect the sentiment, subjectivity, or other categorization of the sentence. 2 Dependency-based Convolution The original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi ∈ Rd represents the d dimensional word representation for the i-th word in ∗ This work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions. 174 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages"
P15-2029,P10-1001,0,0.014268,"Missing"
P15-2029,W04-3239,0,0.100597,"Missing"
P15-2029,D13-1170,0,0.0564206,"Missing"
P15-2029,P14-2105,0,0.0131604,"stic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the lat"
P15-2029,C02-1150,0,0.030806,"Missing"
P15-2029,P14-5010,0,0.00964441,"Missing"
P16-1044,P15-2114,1,0.424703,"task. The approaches normally pursue the solution on the following directions. First, a joint feature vector is constructed based on both the question and the answer, and then the task can be converted into a classification or ranking problem (Wang and Nyberg, 2015; Hu et al., 2014). Second, recently proposed models for text generation can intrinsically be used for answer selection and generation (Bahdanau et al., 2015; Vinyals and Le, 2015). Finally, the question and answer representations can be learned and then matched by certain similarity metrics (Feng et al., 2015; Yu et al., 2014; dos Santos et al., 2015; Qiu and Huang, 2015). Fundamentally, our proposed models belong to the last category. 3.1 LSTM for Answer Selection Our LSTM implementation is similar to the one in (Graves et al., 2013) with minor modifications. Given an input sequence X = {x(1), x(2), · · · , x(n)}, where x(t) is an Edimension word vector in this paper, the hidden vector h(t) (with size H) at the time step t is updated as follows. it = σ(Wi x(t) + Ui h(t − 1) + bi ) (1) ft = σ(Wf x(t) + Uf h(t − 1) + bf ) (2) ot = σ(Wo x(t) + Uo h(t − 1) + bo ) (3) C˜t = tanh(Wc x(t) + Uc h(t − 1) + bc )(4) Ct = it ∗ C˜t + ft ∗ Ct−1 (5) Me"
P16-1044,N10-1145,0,0.0273977,"use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the two input texts, while dos Santos et al. (2016) resort to attentive parameter matrices. cal matching between the question/answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees (Heilman and Smith, 2010; Yao et al., 2013). Discriminative tree-edit feature extraction and engineering over parsing trees were automated in (Severyn and Moschitti, 2013). Such methods might suffer from the availability of additional resources, the effort of feature engineering and the systematic complexity introduced by the linguistic tools, such as parse trees and dependency trees. 3 Approaches In this section, we first present our basic discriminative framework for answer selection based on long short-term memory (LSTM), which we call QA-LSTM. Next, we detail the proposed hybrid and attentive neural networks that"
P16-1044,C10-1131,0,0.0659681,"ortant local information. How to combine the merits from both has not been sufficiently explored. Secondly, previous approaches are usually based on independently generated question and answer embeddings; the quality of such representations, however, usually degrades as the answer sequences grow longer. 2 Related work Previous work on answer selection normally used feature engineering, linguistic tools, or external resources. For example, semantic features were constructed based on WordNet in (Yih et al., 2013). This model pairs semantically related words based on word semantic relations. In (Wang and Manning, 2010; Wang et al., 2007), the answer selection problem was transformed to a syntacti1 git clone https://github.com/shuzi/insuranceQA.git (We use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the two input texts, while dos Santos et al. (2016) resort to attentive parameter matrices. cal matching betwee"
P16-1044,P15-2116,0,0.168548,"Missing"
P16-1044,P14-1062,0,0.00708652,"hitecture, we replace the simple pooling layers (average/maxpooling) by a convolutional layer, which allows to capture richer local information by applying a convolution over sequences of LSTM output vectors. The number of output vectors k (context window size) considered by the convolution is a hyper-parameter of the model. The convolution structure adopted in this work is as follows: Z ∈ Rk|h|×L is a matrix where the m-th column is the concatenation of k hidden vectors generated from biLSTM centralized in the m-th word of the sequence, L is the length of the sequence after wide convolution (Kalchbrenner et al., 2014). The output of the convolution with c filters is, Convolutional LSTMs The pooling strategies used in QA-LSTM suffer from the incapability of filtering important local information, especially when dealing with long answer sequences. Also, it is well known that LSTM models successfully keep the useful information from longrange dependency. But the strength has a tradeoff effect of ignoring the local n-gram coherence. This can be partially alleviated with bidirectional architectures. Meanwhile, the convolutional structures have been widely used in the question answering tasks, C = tanh(Wcp Z) (8"
P16-1044,D07-1003,0,0.387562,"tructures to distinguish better between useful and irrelevant pieces presented in questions and answers. Next, by breaking the independence assumption of the question and answer embedding, we introduce an effective attention mechanism to generate answer representations according to the question, such that the embeddings do not overlook informative parts of the answers. We report experimental results for two answer selection datasets: (1) InsuranceQA (Feng et al., 2015) 1 , a recently released large-scale nonfactoid QA dataset from the insurance domain, and (2) TREC-QA 2 , which was created by Wang et al. (2007) based on Text REtrieval Conference (TREC) QA track data. The contribution of this paper is hence threefold: 1) We propose hybrid neural networks, which learn better representations for both questions and answers by combining merits of both RNN and CNN. 2) We prove the effectiveness of attention on the answer selection task, which has not been sufficiently explored in prior work. 3) We achieve the state-of-the-art results on both TRECQA and InsuranceQA datasets. The rest of the paper is organized as follows: Section 2 describes the related work for answer selection; Section 3 provides the deta"
P16-1044,D14-1194,0,0.0139045,"e retrieve the word embeddings (WEs) of both q and a. Then, we separately apply a biLSTM over the two sequences of WEs. Next, 466 we generate a fixed-sized distributed vector representations using one of the following three approaches: (1) the concatenation of the last vectors on both directions of the biLSTM; (2) average pooling over all the output vectors of the biLSTM; (3) max pooling over all the output vectors. Finally, we use cosine similarity sim(q, a) to score the input (q, a) pair. It is important to note that the same biLSTM is applied to both q and a. Similar to (Feng et al., 2015; Weston et al., 2014; Hu et al., 2014), we define the training objective as a hinge loss. such as (Yu et al., 2014; Feng et al., 2015; Hu et al., 2014). Classical convolutional layers usually emphasize the local lexical connections of the n-gram. However, the local pieces are associated with each other only at the pooling step. No longrange dependencies are taken into account during the formulation of convolution vectors. Fundamentally, recurrent and convolutional neural networks have their own pros and cons, due to their different topologies. How to keep both merits motivates our studies of the following two hyb"
P16-1044,N13-1106,0,0.0316526,"representations, however, usually degrades as the answer sequences grow longer. 2 Related work Previous work on answer selection normally used feature engineering, linguistic tools, or external resources. For example, semantic features were constructed based on WordNet in (Yih et al., 2013). This model pairs semantically related words based on word semantic relations. In (Wang and Manning, 2010; Wang et al., 2007), the answer selection problem was transformed to a syntacti1 git clone https://github.com/shuzi/insuranceQA.git (We use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the two input texts, while dos Santos et al. (2016) resort to attentive parameter matrices. cal matching between the question/answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees (Heilman and Smith, 2010; Yao et al., 2013). Discriminative tree-edit feature extract"
P16-1044,P13-1171,0,0.021363,"l interaction within n-gram, while RNN is designed to capture long range information and forget unimportant local information. How to combine the merits from both has not been sufficiently explored. Secondly, previous approaches are usually based on independently generated question and answer embeddings; the quality of such representations, however, usually degrades as the answer sequences grow longer. 2 Related work Previous work on answer selection normally used feature engineering, linguistic tools, or external resources. For example, semantic features were constructed based on WordNet in (Yih et al., 2013). This model pairs semantically related words based on word semantic relations. In (Wang and Manning, 2010; Wang et al., 2007), the answer selection problem was transformed to a syntacti1 git clone https://github.com/shuzi/insuranceQA.git (We use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the t"
P16-1044,D15-1044,0,0.0149604,"(1), x(2), · · · , x(n)}, where x(t) is an Edimension word vector in this paper, the hidden vector h(t) (with size H) at the time step t is updated as follows. it = σ(Wi x(t) + Ui h(t − 1) + bi ) (1) ft = σ(Wf x(t) + Uf h(t − 1) + bf ) (2) ot = σ(Wo x(t) + Uo h(t − 1) + bo ) (3) C˜t = tanh(Wc x(t) + Uc h(t − 1) + bc )(4) Ct = it ∗ C˜t + ft ∗ Ct−1 (5) Meanwhile, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), machine reading comprehension (Hermann et al., 2015), text summarization (Rush et al., 2015) and text entailment (Rockt¨aschel et al., 2016). Such models learn to focus their attention to specific parts of their input and most of them are based on a one-way attention, in which the attention is basically performed merely over one type of input based on another (e.g. over target languages based on the source languages for machine translation, or over documents according to queries for reading comprehension). Most recently, several two-way attention mechanisms are proposed, where the information from the two input items can influence the computation of each others representations. Rockt"
P16-1044,D13-1044,0,0.127246,"a-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the two input texts, while dos Santos et al. (2016) resort to attentive parameter matrices. cal matching between the question/answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees (Heilman and Smith, 2010; Yao et al., 2013). Discriminative tree-edit feature extraction and engineering over parsing trees were automated in (Severyn and Moschitti, 2013). Such methods might suffer from the availability of additional resources, the effort of feature engineering and the systematic complexity introduced by the linguistic tools, such as parse trees and dependency trees. 3 Approaches In this section, we first present our basic discriminative framework for answer selection based on long short-term memory (LSTM), which we call QA-LSTM. Next, we detail the proposed hybrid and attentive neural networks that are built on top of the QA-LSTM framework. Some recent work has used deep learning methods for the passage-level answer selection task. The approa"
P17-1053,C16-1236,0,0.0667671,"Missing"
P17-1053,D13-1160,0,0.384459,"ames via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field su"
P17-1053,P14-2012,0,0.113812,"extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of re"
P17-1053,D16-1244,0,0.0119806,"Missing"
P17-1053,S10-1057,0,0.0312402,"m to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2"
P17-1053,P16-1076,0,0.0424625,"embeddings). For relation detection in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model.3 Relation Detection in KBQA Systems Relation detection for KBQA also starts with featurerich approaches (Yao and Van Durme, 2014; Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Xu et al., 2016; Dai et al., 2016) and attention models (Yin et al., 2016; Golub and He, 2016). Many of the above relation detection research could naturally support large relation vocabulary and open relation sets (especially for QA with OpenIE KB like ParaLex (Fader et al., 2013)), in order to fit the goal of open-domain question answering. Different KBQA data sets have different levels of requirement about the above open-domain capacity. For example, most of the gold test relations in WebQuestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general RE re"
P17-1053,D16-1054,0,0.0546844,"Missing"
P17-1053,P15-1061,1,0.455256,"rks on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wiki"
P17-1053,P11-1053,0,0.0156809,"art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et a"
P17-1053,P13-1158,0,0.0412673,"ation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 571–581 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1053 Question: what episode was mike kelley the writer of Entity Linking Question: what tv show did grant show play on in 2008 Entity Link"
P17-1053,D16-1166,0,0.291728,"tion is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model.3 Relation Detection in KBQA Systems Relation detection for KBQA also starts with featurerich approaches (Yao and Van Durme, 2014; Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Xu et al., 2016; Dai et al., 2016) and attention models (Yin et al., 2016; Golub and He, 2016). Many of the above relation detection research could naturally support large relation vocabulary and open relation sets (especially for QA with OpenIE KB like ParaLex (Fader et al., 2013)), in order to fit the goal of open-domain question answering. Different KBQA data sets have different levels of requirement about the above open-domain capacity. For example, most of the gold test relations in WebQuestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general RE research. While for data sets like SimpleQuestions and ParaLex"
P17-1053,N16-1065,0,0.032177,"Missing"
P17-1053,D15-1205,1,0.564694,"Missing"
P17-1053,P16-1123,0,0.048317,"Missing"
P17-1053,C16-1164,1,0.738237,"Missing"
P17-1053,N16-1170,0,0.0445311,"Missing"
P17-1053,N16-1117,1,0.8342,"ng capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wikipedia relations. All are much fewer than thousands of relations in KBQA. As a result, few work in this field focuses on dealing with large number of relations or unseen relations. Yu et al. (2016) proposed to use relation embeddings in a low-rank tensor method. However their relation embeddings are still trained in supervised way and the number of relations is not large in the experiments. search assumes that the two argument entities are both available. Thus it usually benefits from features (Nguyen and Grishman, 2014; Gormley et al., 2015) or attention mechanisms (Wang et al., 2016) based on the entity information (e.g. entity types or entity embeddings). For relation detection in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the"
P17-1053,C14-1220,0,0.029909,"n this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it c"
P17-1053,P16-1220,0,0.44264,"entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55"
P17-1053,P15-1049,0,0.0158297,"sts of a Freebase subset with 2M entities (FB2M) (Bordes et al., 2015), in order to compare with previous research. Yin et al. (2016) also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results6 . Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following Yih et al. (2016), we use S-MART (Yang and Chang, 2015) entity-linking outputs.7 In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set.8 For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples. We tune the following hyper-parameters on development sets: (1) the size of hidden states for LSTMs ({50, 100, 200, 400})9 ; (2) learning ra"
P17-1053,P05-1053,0,0.147839,"r simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Ta"
P17-1053,P16-2034,0,0.0088906,"t paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wikipedia relations. All are much fewer than thousands of rela"
P17-1053,W14-2416,0,0.0230301,"Missing"
P17-1053,P14-1090,0,0.128408,"Missing"
P17-1053,P15-1128,0,0.342367,"m that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Pro"
P17-1053,P14-2105,0,0.0490471,"nchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 571–581 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1053 Question: what episode was mike kelley the writer of Entity Linking Question: what tv show did grant show play on in 2008 Entity Linking Relation Detec"
P17-1053,P16-2033,0,0.0532749,"ion KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) (Bordes et al., 2015), in order to compare with previous research. Yin et al. (2016) also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results6 . Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following Yih et al. (2016), we use S-MART (Yang and Chang, 2015) entity-linking outputs.7 In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set.8 For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples. We tune the following hyper-parameters on development sets: (1) the size of hidden states for LSTMs ({"
P17-2053,1993.eamt-1.1,0,0.647058,"Missing"
P17-2053,P15-2029,1,0.851551,"ictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect of question classification is the well prepa"
P17-2053,P14-1062,0,0.101124,"n et al., 2013). To apply dictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect of question classification"
P17-2053,D14-1181,0,0.151024,"(SGL) (Simon et al., 2013). To apply dictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect"
P18-2017,P15-1136,0,0.125868,"three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of ha"
P18-2017,D16-1245,0,0.116186,"chieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achie"
P18-2017,P16-1061,0,0.124566,"chieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achie"
P18-2017,H05-1013,0,0.138808,"Missing"
P18-2017,N07-1030,0,0.170921,"are considered as mentions. We show the mention detection accuracy breakdown by span widths in Figure 2. Our model indeed performs better thanks to the mention detection loss. The advantage is even clearer for longer spans which consist of 5 or more words. In addition, it is important to note that our model can detect mentions that do not exist in the training data. While Moosavi and Strube (2017) observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly de105 the Entity Detection and Tracking task simultaneously. Denis and Baldridge (2007a) propose to use integer linear programming framework to model anaphoricity and coreference as a joint task. tect 1048 mentions which are not detected by Lee et al. (2017), consisting of 386 mentions existing in training data and 662 mentions not existing in training data. From those 662 mentions, some examples are (1) a suicide murder (2) Hong Kong Island (3) a US Airforce jet carrying robotic undersea vehicles (4) the investigation into who was behind the apparent suicide attack. This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memori"
P18-2017,P10-1142,0,0.0608018,"S Airforce jet carrying robotic undersea vehicles (4) the investigation into who was behind the apparent suicide attack. This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is core"
P18-2017,P02-1014,0,0.0458906,"in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj"
P18-2017,D13-1203,0,0.0437191,"Missing"
P18-2017,Q14-1037,0,0.0418307,"Missing"
P18-2017,D14-1162,0,0.0817454,"which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layers with 150 units and ReLU (Nair and Hinton, 2010) activations. We include (speaker ID, document FFNNanaphora and FFNNantecedent reduce span representation dimensions and only keep information relevant to coreference decisions. Compared with the traditional FFNN approach in Lee et al. (2017), biaffine attention directly models both the compatibility of si and sj by ˆs|j Ub"
P18-2017,W12-4501,0,0.2554,"ine attention model to produce scores c(i, j): where yˆi = sigmoid(m(i)), yi = 1 if and only if si is in one of the gold mention clusters. Our final loss combines mention detection and clustering: Lloss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe"
P18-2017,D09-1120,0,0.0634763,"grouping mentions in a text such that all mentions in a cluster refer to the same entity. An example is given below (Bj¨orkelund and Kuhn, 2014) where mentions for two entities are labeled in two clusters: [Drug Emporium Inc.]a1 said [Gary Wilber]b1 was named CEO of [this drugstore chain]a2 . [He]b2 succeeds his father, Philip T. Wilber, who founded [the company]a3 and remains chairman. Robert E. Lyons III, who headed the [company]a4 ’s Philadelphia region, was appointed president and chief operating officer, succeeding [Gary Wilber]b3 . Many traditional coreference systems, either rulebased (Haghighi and Klein, 2009; Lee et al., 2011) ∗ Dragomir R. Radev Yale University dragomir.radev@yale.edu Work done during the internship at IBM Watson. 102 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 102–107 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Model architecture. We consider all text spans up to 10-word length as possible mentions. For brevity, we only show three candidate antecedent spans (“Drug Emporium Inc.”, “Gary Wilber”, “was named CEO”) for the current span “this drugstore chain”. 3"
P18-2017,J01-4004,0,0.634055,"ing to new mentions in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Ferna"
P18-2017,P10-1040,0,0.0385321,"ns 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layers with 150 units and ReLU (Nair and Hinton, 2010) activations. We include (speaker ID, document FFNNanaphora and FFNNantecedent reduce span representation dimensions and only keep information relevant to coreference decisions. Compared with the traditional FFNN approach in Lee et al. (2017), biaffine attention directly models both the compatibility of si and sj by ˆs|j Ubiˆsi and the prior | ˆsi . likelihood of si having an antecedent"
P18-2017,N06-2015,0,0.0404729,"moid(m(i)), yi = 1 if and only if si is in one of the gold mention clusters. Our final loss combines mention detection and clustering: Lloss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embed"
P18-2017,M95-1005,0,0.59745,"loss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs ha"
P18-2017,W04-3250,0,0.110477,"Missing"
P18-2017,N16-1114,0,0.121226,"-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of hand-crafted features. F"
P18-2017,W11-1902,0,0.0332067,"t such that all mentions in a cluster refer to the same entity. An example is given below (Bj¨orkelund and Kuhn, 2014) where mentions for two entities are labeled in two clusters: [Drug Emporium Inc.]a1 said [Gary Wilber]b1 was named CEO of [this drugstore chain]a2 . [He]b2 succeeds his father, Philip T. Wilber, who founded [the company]a3 and remains chairman. Robert E. Lyons III, who headed the [company]a4 ’s Philadelphia region, was appointed president and chief operating officer, succeeding [Gary Wilber]b3 . Many traditional coreference systems, either rulebased (Haghighi and Klein, 2009; Lee et al., 2011) ∗ Dragomir R. Radev Yale University dragomir.radev@yale.edu Work done during the internship at IBM Watson. 102 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 102–107 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Model architecture. We consider all text spans up to 10-word length as possible mentions. For brevity, we only show three candidate antecedent spans (“Drug Emporium Inc.”, “Gary Wilber”, “was named CEO”) for the current span “this drugstore chain”. 3 tion model instead"
P18-2017,P15-1137,0,0.652723,"detection systems. In addition, thanks to the representation power of pre-trained word embeddings and deep neural networks, the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width). The core of the end-to-end neural coreference resolver is the scoring function to compute the mention scores for all possible spans and the antecedent scores for a pair of spans. Furthermore, one major challenge of coreference resolution is that most mentions in the document are singleton or non-anaphoric, i.e., not coreferent with any previous mention (Wiseman et al., 2015). Since the data set only have annotations for mention clusters, the end-to-end coreference resolution system needs to detect mentions, detect anaphoricity, and perform coreference linking. Therefore, research questions still remain on good designs of the scoring architecture and the learning strategy for both mention detection and antecedent scoring given only the gold cluster labels. To this end, we propose to use a biaffine attenCoreference resolution aims to identify in a text all mentions that refer to the same real-world entity. The state-of-the-art endto-end neural coreference model con"
P18-2017,D17-1018,0,0.2209,"tson and Roth, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014), usually solve the problem in two separate stages: (1) a mention detector to propose entity mentions from the text, and (2) a coreference resolver to cluster proposed mentions. At both stages, they rely heavily on complicated, fine-grained, conjoined features via heuristics. This pipeline approach can cause cascading errors, and in addition, since both stages rely on a syntactic parser and complicated handcraft features, it is difficult to generalize to new data sets and languages. Very recently, Lee et al. (2017) proposed the first state-of-the-art end-to-end neural coreference resolution system. They consider all text spans as potential mentions and therefore eliminate the need of carefully hand-engineered mention detection systems. In addition, thanks to the representation power of pre-trained word embeddings and deep neural networks, the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width). The core of the end-to-end neural coreference resolver is the scoring function to compute the mention scores for all possible spans and the antecedent"
P18-2017,H05-1004,0,0.153336,"where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layer"
P18-2017,Q15-1029,0,0.0391508,". (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of hand-crafted features. For example, Wiseman et al. (2015) propose the first neural coreference resolution system by training a deep feed-forward neural network for mention ranking. How"
P18-2017,P17-2003,0,0.0117705,"ts have contributions and when they work together the total gain is even higher. Mention Detection Subtask To further understand our model, we perform a mention detection subtask where spans with mention scores higher than 0 are considered as mentions. We show the mention detection accuracy breakdown by span widths in Figure 2. Our model indeed performs better thanks to the mention detection loss. The advantage is even clearer for longer spans which consist of 5 or more words. In addition, it is important to note that our model can detect mentions that do not exist in the training data. While Moosavi and Strube (2017) observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly de105 the Entity Detection and Tracking task simultaneously. Denis and Baldridge (2007a) propose to use integer linear programming framework to model anaphoricity and coreference as a joint task. tect 1048 mentions which are not detected by Lee et al. (2017), consisting of 386 mentions existing in training data and 662 mentions not existing in training data. From those 662 mentions, some examples are (1) a suicide murder (2) Hong Kong Island (3) a US Airfor"
P18-2017,D08-1031,0,\N,Missing
P19-1640,W13-0102,0,0.162549,"Dirichlet prior, our model represents a much more faithful generalization of LDA to neural network based topic models. Second, our model matches the aggregated posterior to the prior. As a result, the latent codes of different examples get to stay away from each other, promoting a better reconstruction (Tolstikhin et al., 2017). We are thus able to avoid the problem of posterior collapse. To evaluate the quality of the topics from WLDA and other models, we measure the coherence of the representative words of the topics using the widely accepted Normalized Pointwise Mutual Information (NPMI) (Aletras and Stevenson, 2013) score, which is shown to closely match human judgments (Lau et al., 2014). While NPMI captures topic coherence, it is also important that the discovered topics are diverse (not repetitive). Yet such a measure has been missing in the topic model literature.1 We therefore propose a simple Topic Uniqueness (TU) measure for this purpose. Given a set of representative words from all the topics, the TU score is inversely proportional to the number of times each word is repeated in the set. High TU score means the representative words are rarely repeated and the topics are unique to each other. Usin"
P19-1640,K16-1002,0,0.0644204,"aplace approximation of the Dirichlet distribution in the softmax basis as prior. Second, the KL divergence term in the VAE objective forces posterior distributions for all examples to match 6345 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6345–6381 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the prior, essentially making the encoder output independent of the input. This leads to the problem commonly known as posterior collapse (He et al., 2019). Although various heuristics such as KL-annealing (Bowman et al., 2016) have been proposed to address this problem, they are shown to be ineffective in more complex datasets (Kim et al., 2018). In this work we leverage the expressive power and efficiency of neural networks and propose a novel neural topic model to address the above difficulties. Our neural topic model belongs to a broader family of Wasserstein autoencoders (WAE) (Tolstikhin et al., 2017). We name our neural topic model W-LDA to emphasize the connection with WAE. Compared to the VAE-based topic models, our model has a few advantages. First, we encourage the latent document-topic vectors to follow"
P19-1640,D18-1096,1,0.863927,"cently, deep neural networks have been successfully used for such probabilistic models with the emergence of variational autoencoders ∗ This work was done when the author was with Amazon. (VAE) (Kingma and Welling, 2013). The key advantage of such neural network based models is that inference can be carried out easily via a forward pass of the recognition network, without the need for expensive iterative inference scheme per example as in VB and collapsed Gibbs sampling. Topic models that fall in this framework include NVDM (Miao et al., 2016), ProdLDA (Srivastava and Sutton, 2017) and NTM-R (Ding et al., 2018). At a high level, these models consist of an encoder network that maps the Bag-of-Words (BoW) input to a latent document-topic vector and a decoder network that maps the document-topic vector to a discrete distribution over the words in the vocabulary. They are autoencoders in the sense that the output of the decoder aims to reconstruct the word distribution of the input BoW representation. Besides the reconstruction loss, VAE-based methods also minimize a KL-divergence term between the prior and posterior of the latent vector distributions. Despite their popularity, these VAEbased topic mode"
Q16-1019,D12-1050,0,0.00810263,"tworks to model sentence pairs for AS, PI and TE. For AS, Yu et al. (2014) present a bigram CNN to model question and answer candidates. Yang et al. (2015) extend this method and get state-of-the-art performance on the WikiQA dataset (Section 5.1). Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2016) explore bidirectional LSTMs on the same dataset. Our approach is different because we do not model the sentences by two independent neural networks in parallel, but instead as an interdependent sentence pair, using attention. For PI, Blacoe and Lapata (2012) form sentence representations by summing up word embeddings. Socher et al. (2011) use recursive autoencoders (RAEs) to model representations of local phrases in sentences, then pool similarity values of phrases from the two sentences as features for binary classification. Yin and Sch¨utze (2015a) similarly replace an RAE with a CNN. In all three papers, the representation of one sentence is not influenced by the other – in contrast to our attention-based model. For TE, Bowman et al. (2015b) use recursive neural networks to encode entailment on SICK (Marelli et al., 2014b). Rockt¨aschel et al."
Q16-1019,D15-1075,0,0.242137,"when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture tha"
Q16-1019,W15-4002,0,0.0821956,"when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture tha"
Q16-1019,N10-1066,0,0.0141601,"is on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such approaches often require more computational resources. In addition, employing syntactic or semantic parsers – which produce errors on many sentences – to find the best match between the structured representations of two sentences is not trivial. DL on Sentence Pair Modeling. To address some of the challenges of non-DL work, mu"
Q16-1019,C04-1051,0,0.18671,"Missing"
Q16-1019,D15-1181,0,0.40273,"Missing"
Q16-1019,N10-1145,0,0.0633371,"attracted lots of attention in the past decades. Many tasks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phra"
Q16-1019,D13-1090,0,0.0125067,"Missing"
Q16-1019,S14-2131,0,0.0262344,"Missing"
Q16-1019,P14-1062,0,0.0950702,"nize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture that computes different representations of si for different s1−i (i ∈ {0, 1}). 259 Transactions of the Association for Computational Linguistics, vol. 4, pp. 259–272, 2016. Action Editor: Brian Roark. Submission batch: 12/2015; Revision batch: 3/2016; Published 6/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Convolutional Neural Networks (CNNs) (LeCun et al., 1998) are widely used to model sentences (Kalchbrenner et al., 2014; Kim, 2014) and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying atte"
Q16-1019,D14-1181,0,0.0350541,"and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture that computes different representations of si for different s1−i (i ∈ {0, 1}). 259 Transactions of the Association for Computational Linguistics, vol. 4, pp. 259–272, 2016. Action Editor: Brian Roark. Submission batch: 12/2015; Revision batch: 3/2016; Published 6/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Convolutional Neural Networks (CNNs) (LeCun et al., 1998) are widely used to model sentences (Kalchbrenner et al., 2014; Kim, 2014) and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying attention; e.g.,"
Q16-1019,S14-2055,0,0.00838841,"ctic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such approaches often require more computational resources. In addition, employing syntactic or semantic parsers – which produce errors on many sentences – to find the best match between the structured representations of two sentences is not trivial. DL on Sentence Pair Modeling. To address some of the challenges of non-DL work, much recent work uses neural ne"
Q16-1019,P15-1107,0,0.199045,"Missing"
Q16-1019,W04-1013,0,0.0122377,"Missing"
Q16-1019,D15-1166,0,0.137021,"only processing the selected regions at high resolution. Gregor et al. (2015) combine a spatial attention mechanism with RNNs for image generation. Ba et al. (2015) investigate attention-based RNNs for recognizing multiple objects in images. Chorowski et al. (2014) and Chorowski et al. (2015) use attention in RNNs for speech recognition. Attention-Based DL in NLP. Attention-based DL systems have been applied to NLP after their success in computer vision and speech recognition. They mainly rely on RNNs and end-to-end encoderdecoders for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). Our work takes the lead in exploring attention mechanisms in CNNs for NLP tasks. 3 BCNN: Basic Bi-CNN We now introduce our basic (non-attention) CNN that is based on the Siamese architecture (Bromley et al., 1993), i.e., it consists of two weightsharing CNNs, each processing one of the two sentences, and a final layer that solves the sentence pair task. See Figure 2. We refer to this architecture as the BCNN. The next section will then introduce the ABCNN, an attention architecture that extends the BCNN. Table 1 gives our notationa"
Q16-1019,N12-1019,0,0.00988376,"al influence of the two sentences in the context of the task. It also contradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors"
Q16-1019,S14-2001,0,0.275524,"tradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need f"
Q16-1019,marelli-etal-2014-sick,0,0.153021,"tradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need f"
Q16-1019,D15-1044,0,0.195677,"r et al. (2015) combine a spatial attention mechanism with RNNs for image generation. Ba et al. (2015) investigate attention-based RNNs for recognizing multiple objects in images. Chorowski et al. (2014) and Chorowski et al. (2015) use attention in RNNs for speech recognition. Attention-Based DL in NLP. Attention-based DL systems have been applied to NLP after their success in computer vision and speech recognition. They mainly rely on RNNs and end-to-end encoderdecoders for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). Our work takes the lead in exploring attention mechanisms in CNNs for NLP tasks. 3 BCNN: Basic Bi-CNN We now introduce our basic (non-attention) CNN that is based on the Siamese architecture (Bromley et al., 1993), i.e., it consists of two weightsharing CNNs, each processing one of the two sentences, and a final layer that solves the sentence pair task. See Figure 2. We refer to this architecture as the BCNN. The next section will then introduce the ABCNN, an attention architecture that extends the BCNN. Table 1 gives our notational conventions. In our implementation and also in the mathemat"
Q16-1019,D07-1002,0,0.00848433,"atures are used. 2 Related Work Non-DL on Sentence Pair Modeling. Sentence pair modeling has attracted lots of attention in the past decades. Many tasks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, s"
Q16-1019,D07-1003,0,0.344451,"sks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such ap"
Q16-1019,D15-1237,0,0.717817,"Missing"
Q16-1019,D13-1056,0,0.00563054,"Missing"
Q16-1019,P13-2029,0,0.00630854,"Missing"
Q16-1019,P13-1171,0,0.0672204,"and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying attention; e.g., Yih et al. (2013) employ word alignment to match related parts of the two sentences. In contrast, our attention scheme based on CNNs models relatedness between two parts fully automatically. Moreover, attention at multiple levels of granularity, not only at word level, is achieved as we stack multiple convolution layers that increase abstraction. Prior work on attention in deep learning (DL) mostly addresses long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997). LSTMs achieve attention usually in a word-to-word scheme, and word representations mostly encode the whole context within the sen"
Q16-1019,N15-1091,1,0.587059,"Missing"
Q16-1019,P15-1007,1,0.520976,"Missing"
Q16-1019,S14-2044,0,0.0779176,"Missing"
W08-0403,J07-2003,0,0.270841,"dels that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based tr"
W08-0403,P05-1066,0,0.0614724,"Missing"
W08-0403,N04-1035,0,0.201986,"improved models based on tree kernel methods. Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models, while both of them achieved significant improvements over a state-of-theart phrase-based SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translat"
W08-0403,P06-1121,0,0.210961,"oding. Recent work by (Zhang et al., 2006) shows a practically efficient approach that binarizes linguistically SCFG rules when possible. 19 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19–27, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics language model, which is a key element to ensure translation output quality. On the other hand, available linguistic theory and annotations could provide invaluable benefits in grammar induction and scoring, as shown by recent progress on such models (Galley et al., 2006). In contrast, formally syntax-based grammars often lack explicit linguistic constraints. In this paper, we propose a scheme to enrich formally syntax-based models with linguistically syntactic knowledge. In other words, we maintain our grammar to be based on formal syntax on surface, but incorporate linguistic knowledge into our models to leverage syntax theory and annotations. Our goal is two-fold. First, how to score SCFG rules whose general abstraction forms are unseen in the training data is an important question to answer. In hierarchical models, Chiang (Chiang, 2007) utilizes heuristics"
W08-0403,P03-1011,0,0.0217745,"tion. The remainder of the paper is organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 summarized our contributions with discussions and future work. 2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature (Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Galley et al., 2004; Satta and Peserico, 2005). Recent work by (Chiang, 2007; Galley et al., 2006) shows promising improvements compared to phrase-based models for large-scale tasks. However, few previous work directly applied linguistically syntactic information into a formally syntaxbased models, which is explored in this paper. Kernel methods leverage the fact that the only operation in a procedure is the evaluation of inner dot products between pairs of observations, where the inner product is thus replaced with a Mercer kernel that provides an efficient way to carry out computation when"
W08-0403,N03-1017,0,0.0318403,"n source and target sides are identical). By “optimal”, it indicates that the derivation D maximizes following log-linear models over all possible derivations: P (D) ∝ PLM (e)λLM × Q Q λi i X→<γ,α>∈D φi (X →< γ, α >) , (4) where the set of φi (X →< γ, α >) are features defined over given production rule, and P LM (e) is the language model score on hypothesized output, the λi is the feature weight. Our baseline model follows Chiang’s hierarchical model (Chiang, 2007) in conjunction with additional features: • conditional probabilities in both directions: P (γ|α) and P (α|γ); • lexical weights (Koehn et al., 2003) in both directions: Pw (γ|α) and Pw (α|γ); • word counts |e|; • rule counts |D|; • target n-gram language model PLM (e); • glue rule penalty to learn preference of nonterminal rewriting over serial combination through Eq. 3; Moreover, we propose an additional feature, namely the abstraction penalty, to account for the accumulated number of nonterminals applied in D: • abstraction penalty exp(−Na ), where Na = P X→<γ,α>∈D n(γ) where n(γ) is the number of nonterminals in γ. This feature aims to learn the preference among phrasal rules, and abstract rules with one or two nonterminals. This makes"
W08-0403,P06-1077,0,0.0443672,"re derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based translation models using SCFG can be further categorized into two classes, based on their dependency on annotated corWhil"
W08-0403,E06-1015,0,0.201305,"y, we propose a linguisticallymotivated method to train prior derivation models for formally syntax-based translation. In this framework, prior derivation models can be viewed as a smoothing of rule translation models, addressing the weakness of the baseline model estimation that relies on relative counts obtained from heuristics. First, we apply automatic parsers to obtain syntax annotations on the English side of the parallel corpus. Next, we extract tree fragments associated with phrase pairs, and measure similarity between such tree fragments using kernel methods (Collins and Duffy, 2002; Moschitti, 2006). Finally, we score 20 and rank rules based on their minimal cluster similarity of their nonterminals, which is used to compute the prior distribution of hypothesis derivations during decoding for improved translation. The remainder of the paper is organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 s"
W08-0403,P00-1056,0,0.208906,"rminal symbol X in the grammar, X → hγ, α, ∼i, (1) where ∼ is the one-to-one correspondence between X’s in γ and α, which is indicated by underscripted co-indices on both sides. For example, some English-to-Chinese production rules can be represented as follows: X → hX1 enjoy readingX2 , (2) X1 xihuan(enjoy) yuedu(reading)X2 i X → hX1 enjoy readingX2 , X1 xihuan(enjoy)X2 yuedu(reading)i The set of rules, denoted as R, are automatically extracted from sentence-aligned parallel corpus (Chiang, 2007). First, bidirectional word-level alignment is carried out on the parallel corpus running GIZA++ (Och and Ney, 2000). Based on the resulting Viterbi alignments Ae2f and Af 2e , the union, AU = Ae2f ∪ Af 2e , is taken as the symmetrized word-level alignment. Next, bilingual phrase pairs consistent with word alignments are extracted from AU (Och and Ney, 2004). Specifically, any pair of consecutive sequences of words below a maximum length M is considered to be a phrase pair if its component words are aligned only within the phrase pair and not to any words outside. The resulting bilingual phrase pair inventory is denoted as BP. Each phrase pair PP ∈ BP is represented as a production rule X → hfij , elk i, wh"
W08-0403,J04-4002,0,0.55546,"ctures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based translation models using SCFG can be further categorized into two classes, based on their dependency on annotated corWhile these two often resemble in appearance, from practical viewpoints, there are some distinctions in training and decoding procedures differentiating formally syntax-based models from linguistically syntax-based models. First, the former has no dependency o"
W08-0403,P03-1021,0,0.0136676,"from transcription and human translation of conversations. The vocabulary size is 37K for English and 44K for Chinese after segmentation. Our evaluation data is a held out data set of 2755 sentences pairs. We extracted every one out of two sentence pairs into the dev-set, and left the remainder as the test-set. We thereby obtained a dev-set of 1378 sentence pairs, and a test-set with 1377 sentence pairs. In both cases, there are about 15K running words on English side. All Chinese sentences in training, dev and test sets are all automatically segmented into words. Minimum-error-rate training (Och, 2003) are conducted on dev-set to optimize feature weights maximizing the BLEU score up to 4grams, and the obtained feature weights are blindly applied on the test-set. To compare performances excluding tokenization effects, all BLEU scores are optimized (on dev-set) and reported (on test-set) at Chinese character-level. From training data, we extracted an initial phrase pair set with 3.7M entries for phrases up to 8 words 25 on Chinese side. We trained a 4-gram language model for Chinese at word level, which is shared by all translation systems reported in this paper, using the Chinese side of the"
W08-0403,H05-1101,0,0.0123004,"organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 summarized our contributions with discussions and future work. 2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature (Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Galley et al., 2004; Satta and Peserico, 2005). Recent work by (Chiang, 2007; Galley et al., 2006) shows promising improvements compared to phrase-based models for large-scale tasks. However, few previous work directly applied linguistically syntactic information into a formally syntaxbased models, which is explored in this paper. Kernel methods leverage the fact that the only operation in a procedure is the evaluation of inner dot products between pairs of observations, where the inner product is thus replaced with a Mercer kernel that provides an efficient way to carry out computation when original feature dimension is large or even inf"
W08-0403,J97-3002,0,0.344192,"ed SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by f"
W08-0403,P01-1067,0,0.321147,"ing algorithm to achieve such improved models based on tree kernel methods. Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models, while both of them achieved significant improvements over a state-of-theart phrase-based SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorpo"
W08-0403,N06-1033,0,0.0196192,"s in training and decoding procedures differentiating formally syntax-based models from linguistically syntax-based models. First, the former has no dependency on available linguistic theory and annotations for targeting language pairs, and thus the training and rule extraction are more efficient. Secondly, the decoding complexity of the former is lower 1 , especially when integrating a n-gram based 1 The complexity is dominated by synchronous parsing and boundary words keeping. Thus binary SCFG employed in formally syntax-based systems help to maintain efficient CKY decoding. Recent work by (Zhang et al., 2006) shows a practically efficient approach that binarizes linguistically SCFG rules when possible. 19 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19–27, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics language model, which is a key element to ensure translation output quality. On the other hand, available linguistic theory and annotations could provide invaluable benefits in grammar induction and scoring, as shown by recent progress on such models (Galley et al., 2006). In contrast, formall"
W11-1007,P06-1067,0,0.0461222,"Missing"
W11-1007,W10-1749,0,0.0474896,"t only once if the phrase boundary is correct. We propose the following pair-wise distortion metric. From an MT output, we first extract the source visit sequence: Hyp:{h1, h2, . . . hn } where hi are the visit order of the source sentence. From the reference, we extract the true visit sequence: Ref:{r1, r2, . . . rn } The Pair-wise Distortion metric PDscore can be computed as follows: n X − → I(hi = rj ∧ hi−1 = rj−1 ) P Dscore( H ) = n i=1 (8) It measures how often the translation output gets the pair-wise source visit order correct. We notice that an MT metric named LRscore was proposed in (Birch and Osborne, 2010). It computes the distance between two word order sequences, which is different from the metric we proposed here. 7 Experiments 7.1 Data and Baseline We conduct a set of experiments on a Chinese-toEnglish MT task. The training data includes the UN parallel corpus and LDC-released parallel corpora, with about 11M sentence pairs, 320M words in total (counted at the English side). To evaluate the smoothed distortion priors and different features, we use an internal data set as the development set and the NIST MT08 evaluation set as the test set, which includes 76 documents (691 sentences) in news"
W11-1007,J93-2003,0,0.0284201,"that both smoothed priors and syntax-based features help to significantly improve the reordering and hence the translation performance on a large-scale Chinese-to-English machine translation task. 1 Introduction Over the past decade, statistical machine translation (SMT) has evolved into an attractive area in natural language processing. SMT takes a source sequence, S = [s1 s2 . . . sK ] from the source language, and generates a target sequence, T ∗ = [t1 t2 . . . tL ], by finding the most likely translation given by: T ∗ = arg max p(T |S) T (1) In most of the existing approaches, following (Brown et al., 1993), Eq. (1) is factored using the source-channel model into T ∗ = arg max p(S|T )pλ (T ), T (2) where the two models, the translation model, p(S|T ), and the language model (LM), p(T ), are es61 In recent years, among all the proposed approaches, the phrase-based method has become the widely adopted one in SMT due to its capability of capturing local context information from adjacent words. Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al., 2003), among all the features utilized in a maximum-"
W11-1007,W09-2307,0,0.03489,"and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al., 2009). More recently, in (Ge, 2010) a proba62 bilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency. The reordering model is used to generate a reordering lattice which encodes many reordering and their costs (negative log probability). Another recent work is (Green et al., 2010), which estimates future linear distortion cost and presents a discriminative distortion model that predicts word movement during translation based on multiple feature"
W11-1007,P05-1066,0,0.157611,"Missing"
W11-1007,D08-1089,0,0.0531702,"nt, some phrase-level reordering models indicate how to move phrases, also called orientations. Orientations typically apply to the adjacent phrases. Two adjacent phrases can be either placed monotonically (sometimes called straight) or swapped (non-monotonically or inverted). In (Och and Ney, 2004; Tillmann, 2004; Kumar and Byrne, 2005; AlOnaizan and Papineni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al., 2009). More recently, in (Ge, 2010) a proba62 bilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no"
W11-1007,N10-1127,1,0.770684,"presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al., 2009). More recently, in (Ge, 2010) a proba62 bilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency. The reordering model is used to generate a reordering lattice which encodes many reordering and their costs (negative log probability). Another recent work is (Green et al., 2010), which estimates future linear distortion cost and presents a discriminative distortion model that predicts word movement during translation based on multiple features. This work differentiates it"
W11-1007,N10-1129,0,0.0184982,"data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al., 2009). More recently, in (Ge, 2010) a proba62 bilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency. The reordering model is used to generate a reordering lattice which encodes many reordering and their costs (negative log probability). Another recent work is (Green et al., 2010), which estimates future linear distortion cost and presents a discriminative distortion model that predicts word movement during translation based on multiple features. This work differentiates itself from all the previous work on the phrase reordering as the following. Firstly, we propose a smoothed distortion prior probability in the maximum-entropy-based MT framework. It not only takes into account the distortion in the prior, but also alleviates the data sparseness problem. Secondly, we propose multiple syntactic features based on the source-side parse tree to capture the reordering pheno"
W11-1007,N07-1008,1,0.936071,"en two different languages. The correct reordering patterns will be automatically favored during the decoding, due to the higher weights obtained through the maximum entropy training on the parallel data. Finally, we also introduce a new metric to quantify the effect on the distortions in different systems. The experiments on a Chinese-English MT task show that these proposed approaches additively improve both the distortion and translation performance significantly. 3 Maximum-Entropy Model for MT In this section we give a brief review of a special maximum-entropy (ME) model as introduced in (Ittycheriah and Roukos, 2007). The model has the following form, p(t, j|s) = X p0 (t, j|s) exp λi φi (t, j, s), Z (3) i where s is a source phrase, and t is a target phrase. j is the jump distance from the previously translated source word to the current source word. During training j can vary widely due to automatic word alignment in the parallel corpus. To limit the sparseness created by long jumps, j is capped to a window of source words (-5 to 5 words) around the last translated source word. Jumps outside the window are treated as being to the edge of the window. In Eq. (3), p0 is a prior distribution, Z is a normaliz"
W11-1007,N03-1017,0,0.307037,"In most of the existing approaches, following (Brown et al., 1993), Eq. (1) is factored using the source-channel model into T ∗ = arg max p(S|T )pλ (T ), T (2) where the two models, the translation model, p(S|T ), and the language model (LM), p(T ), are es61 In recent years, among all the proposed approaches, the phrase-based method has become the widely adopted one in SMT due to its capability of capturing local context information from adjacent words. Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al., 2003), among all the features utilized in a maximum-entropy (loglinear) model (Och and Ney, 2002). The distortion cost utilized during the decoding is usually a penalty linearly proportional to the number of words in the source sentence that are skipped in a translation path. In this paper, we propose several novel approaches to improve reordering in the phrase-based translation with a maximum-entropy model. In Section 2, we review the previous work that focused on the distortion and phrase reordering in SMT. In Section 3, we briefly review the baseline of this work. In Section 4, we introduce a sm"
W11-1007,H05-1021,0,0.214705,". It has trouble to produce the right translation order if the training data does not contain the specific phrase pairs. For example, phrases do not capture the phenomenon that Arabic adjectives and nouns need to be reordered. Instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations. Orientations typically apply to the adjacent phrases. Two adjacent phrases can be either placed monotonically (sometimes called straight) or swapped (non-monotonically or inverted). In (Och and Ney, 2004; Tillmann, 2004; Kumar and Byrne, 2005; AlOnaizan and Papineni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; C"
W11-1007,P07-1091,0,0.0203373,"al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al., 2009). More recently, in (Ge, 2010) a proba62 bilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency. The reordering model is used to generate a reordering lattice which encodes many reordering and their costs (negative log probability). Another recent work is (Green et al., 2010), which estimates future linear distortion cost and presents a discriminative distortion model that predicts word movement during translation base"
W11-1007,P09-2061,0,0.0191942,"the specific phrase pairs. For example, phrases do not capture the phenomenon that Arabic adjectives and nouns need to be reordered. Instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations. Orientations typically apply to the adjacent phrases. Two adjacent phrases can be either placed monotonically (sometimes called straight) or swapped (non-monotonically or inverted). In (Och and Ney, 2004; Tillmann, 2004; Kumar and Byrne, 2005; AlOnaizan and Papineni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al., 2009). More recen"
W11-1007,P02-1038,0,0.151144,"the source-channel model into T ∗ = arg max p(S|T )pλ (T ), T (2) where the two models, the translation model, p(S|T ), and the language model (LM), p(T ), are es61 In recent years, among all the proposed approaches, the phrase-based method has become the widely adopted one in SMT due to its capability of capturing local context information from adjacent words. Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al., 2003), among all the features utilized in a maximum-entropy (loglinear) model (Och and Ney, 2002). The distortion cost utilized during the decoding is usually a penalty linearly proportional to the number of words in the source sentence that are skipped in a translation path. In this paper, we propose several novel approaches to improve reordering in the phrase-based translation with a maximum-entropy model. In Section 2, we review the previous work that focused on the distortion and phrase reordering in SMT. In Section 3, we briefly review the baseline of this work. In Section 4, we introduce a smoothed prior probability by taking into account the distortions in the priors. In Section 5,"
W11-1007,J04-4002,0,0.111589,"tances and not reordering phenomena. It has trouble to produce the right translation order if the training data does not contain the specific phrase pairs. For example, phrases do not capture the phenomenon that Arabic adjectives and nouns need to be reordered. Instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations. Orientations typically apply to the adjacent phrases. Two adjacent phrases can be either placed monotonically (sometimes called straight) or swapped (non-monotonically or inverted). In (Och and Ney, 2004; Tillmann, 2004; Kumar and Byrne, 2005; AlOnaizan and Papineni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reorderi"
W11-1007,P02-1040,0,0.0805578,"Missing"
W11-1007,W97-0301,0,0.0383798,"ciently. The most recent and good-quality corpora are sampled first. For the given test set, we obtain the first 20 instances of n-grams (length from 1 to 15) from the test that occur in the training universe and the resulting sentences then form the training sample. In the end, 1M sentence pairs are selected for the sampled training for each genre of the MT08 test set. A 5-gram language model is trained from the English Gigaword corpus and the English portion of the parallel corpus used in the translation model training. The Chinese parse trees are produced by a maximum entropy based parser (Ratnaparkhi, 1997). The baseline decoder is a phrase-based decoder that employs both normal phrases and also non-contiguous phrases. The value of maximum skip is set to 9 in all the experiments. The smoothing parameter α for distortion prior is set to 0.9 empiri66 cally based on the results on the development set. 7.2 Distortion Evaluation We evaluate the MT distortion using the metric in Eq. (8) on two hand-aligned test sets. Test-278 includes 278 held-out sentences. Test-52 contains the first 52 sentences from the MT08 Newswire set, with the Chinese input sentences manually aligned to the first set of referen"
W11-1007,2006.amta-papers.25,0,0.105308,"Missing"
W11-1007,N04-4026,0,0.105817,"dering phenomena. It has trouble to produce the right translation order if the training data does not contain the specific phrase pairs. For example, phrases do not capture the phenomenon that Arabic adjectives and nouns need to be reordered. Instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations. Orientations typically apply to the adjacent phrases. Two adjacent phrases can be either placed monotonically (sometimes called straight) or swapped (non-monotonically or inverted). In (Och and Ney, 2004; Tillmann, 2004; Kumar and Byrne, 2005; AlOnaizan and Papineni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in ("
W11-1007,D07-1077,0,0.0229952,"eni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al., 2009). More recently, in (Ge, 2010) a proba62 bilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency. The reordering model is used to generate a reordering lattice which encodes many reordering and their costs (negative log probability). Another recent work is (Green et al., 2010), which estimates future linear distortion cost and presents a discriminative distortion model that predicts word movement during"
W11-1007,C04-1073,0,0.0552021,"; Kumar and Byrne, 2005; AlOnaizan and Papineni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al., 2009). More recently, in (Ge, 2010) a proba62 bilistic reordering model is presented to model directly the source translation sequence and explicitly assign probabilities to the reordering of the source input with no restrictions on gap, length or adjacency. The reordering model is used to generate a reordering lattice which encodes many reordering and their costs (negative log probability). Another recent work is (Green et al., 2010), which estimates future linear distortion cost and presents a discriminative distortion"
W11-1007,P06-1066,0,0.0833026,"er if the training data does not contain the specific phrase pairs. For example, phrases do not capture the phenomenon that Arabic adjectives and nouns need to be reordered. Instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations. Orientations typically apply to the adjacent phrases. Two adjacent phrases can be either placed monotonically (sometimes called straight) or swapped (non-monotonically or inverted). In (Och and Ney, 2004; Tillmann, 2004; Kumar and Byrne, 2005; AlOnaizan and Papineni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al.,"
W11-1007,W06-3108,0,0.090919,"ata does not contain the specific phrase pairs. For example, phrases do not capture the phenomenon that Arabic adjectives and nouns need to be reordered. Instead of directly modeling the distance of word movement, some phrase-level reordering models indicate how to move phrases, also called orientations. Orientations typically apply to the adjacent phrases. Two adjacent phrases can be either placed monotonically (sometimes called straight) or swapped (non-monotonically or inverted). In (Och and Ney, 2004; Tillmann, 2004; Kumar and Byrne, 2005; AlOnaizan and Papineni, 2006; Xiong et al., 2006; Zens and Ney, 2006; Ni et al., 2009), people presented models that use lexical features from the phrases to predict their orientations. These models are very powerful in predicting local phrase placements. In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. Because of the heavy use of lexical features, these models tend to suffer from data sparseness problems. Syntax information has been used for reordering, such as in (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2007; Chang et al.,"
W11-1007,2004.tmi-1.9,0,0.0460649,"by 0.3 to 0.6. The parse sibling feature alone provides the largest individual contribution. When adding both types of new features, the improvement is around 0.6 to 0.8 on two genres. Finally, applying all three results in the best performance (the last row). On the Newswire set, the final system is more than 3 points better than the PBT baseline and 1 point better than the ME baseline. On the Weblog set, it is more than 3 points better than PBT and 0.8 better than the ME baseline. All the MT results above are statistically significant 67 with p-value < 0.0001 by using the tool described in (Zhang and Vogel, 2004). 7.4 Analysis To better understand the distortion and translation results, we take a closer look at the parse-based features. In Table 5, we list the most frequent parse sibling features that are related to the Chinese phrases with “PP VV” structures. It is known that in Chinese usually the preposition phrases (“PP”) are written/spoken before the verbs (“VV”), with a different order from English. Table 5 shows how such reordering phenomenon is captured by the parse sibling features. Recall that when αi is greater than 1, the system prefers the reordering with that feature fired. When αi is sm"
