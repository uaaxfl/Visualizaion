2021.inlg-1.12,Explaining Decision-Tree Predictions by Addressing Potential Conflicts between Predictions and Plausible Expectations,2021,-1,-1,3,0,5929,sameen maruf,Proceedings of the 14th International Conference on Natural Language Generation,0,"We offer an approach to explain Decision Tree (DT) predictions by addressing potential conflicts between aspects of these predictions and plausible expectations licensed by background information. We define four types of conflicts, operationalize their identification, and specify explanatory schemas that address them. Our human evaluation focused on the effect of explanations on users{'} understanding of a DT{'}s reasoning and their willingness to act on its predictions. The results show that (1) explanations that address potential conflicts are considered at least as good as baseline explanations that just follow a DT path; and (2) the conflict-based explanations are deemed especially valuable when users{'} expectations disagree with the DT{'}s predictions."
2021.inlg-1.23,Generation Challenges: Results of the Accuracy Evaluation Shared Task,2021,-1,-1,2,1,5939,craig thomson,Proceedings of the 14th International Conference on Natural Language Generation,0,"The Shared Task on Evaluating Accuracy focused on techniques (both manual and automatic) for evaluating the factual accuracy of texts produced by neural NLG systems, in a sports-reporting domain. Four teams submitted evaluation techniques for this task, using very different approaches and techniques. The best-performing submissions did encouragingly well at this difficult task. However, all automatic submissions struggled to detect factual errors which are semantically or pragmatically complex (for example, based on incorrect computation or inference)."
2021.inlg-1.24,The {R}epro{G}en Shared Task on Reproducibility of Human Evaluations in {NLG}: Overview and Results,2021,-1,-1,4,0.857143,5962,anya belz,Proceedings of the 14th International Conference on Natural Language Generation,0,"The NLP field has recently seen a substantial increase in work related to reproducibility of results, and more generally in recognition of the importance of having shared definitions and practices relating to evaluation. Much of the work on reproducibility has so far focused on metric scores, with reproducibility of human evaluation results receiving far less attention. As part of a research programme designed to develop theory and practice of reproducibility assessment in NLP, we organised the first shared task on reproducibility of human evaluations, ReproGen 2021. This paper describes the shared task in detail, summarises results from each of the reproduction studies submitted, and provides further comparative analysis of the results. Out of nine initial team registrations, we received submissions from four teams. Meta-analysis of the four reproduction studies revealed varying degrees of reproducibility, and allowed very tentative first conclusions about what types of evaluation tend to have better reproducibility."
2021.humeval-1.6,Towards Objectively Evaluating the Quality of Generated Medical Summaries,2021,-1,-1,4,0,6009,francesco moramarco,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),0,"We propose a method for evaluating the quality of generated text by asking evaluators to count facts, and computing precision, recall, f-score, and accuracy from the raw counts. We believe this approach leads to a more objective and easier to reproduce evaluation. We apply this to the task of medical report summarisation, where measuring objective quality and accuracy is of paramount importance."
2021.humeval-1.7,A Preliminary Study on Evaluating Consultation Notes With Post-Editing,2021,-1,-1,4,0,6009,francesco moramarco,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),0,"Automatic summarisation has the potential to aid physicians in streamlining clerical tasks such as note taking. But it is notoriously difficult to evaluate these systems and demonstrate that they are safe to be used in a clinical setting. To circumvent this issue, we propose a semi-automatic approach whereby physicians post-edit generated notes before submitting them. We conduct a preliminary study on the time saving of automatically generated consultation notes with post-editing. Our evaluators are asked to listen to mock consultations and to post-edit three generated notes. We time this and find that it is faster than writing the note from scratch. We present insights and lessons learnt from this experiment."
2021.eacl-main.29,A Systematic Review of Reproducibility Research in Natural Language Processing,2021,-1,-1,4,0.857143,5962,anya belz,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on reproducibility in NLP,"
2020.nl4xai-1.7,Explaining {B}ayesian Networks in Natural Language: State of the Art and Challenges,2020,-1,-1,3,0,16443,conor hennessy,2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence,0,"In order to increase trust in the usage of Bayesian Networks and to cement their role as a model which can aid in critical decision making, the challenge of explainability must be faced. Previous attempts at explaining Bayesian Networks have largely focused on graphical or visual aids. In this paper we aim to highlight the importance of a natural language approach to explanation and to discuss some of the previous and state of the art attempts of the textual explanation of Bayesian Networks. We outline several challenges that remain to be addressed in the generation and validation of natural language explanations of Bayesian Networks. This can serve as a reference for future work on natural language explanations of Bayesian Networks."
2020.intellang-1.4,{S}port{S}ett:Basketball - A robust and maintainable data-set for Natural Language Generation,2020,-1,-1,2,1,5939,craig thomson,Proceedings of the Workshop on Intelligent Information Processing and Natural Language Generation,0,None
2020.intellang-1.5,Iterative Neural Scoring of Validated Insight Candidates,2020,-1,-1,3,0,18953,allmin susaiyah,Proceedings of the Workshop on Intelligent Information Processing and Natural Language Generation,0,None
2020.intellang-1.7,How are you? Introducing stress-based text tailoring,2020,-1,-1,2,0,18961,simone balloccu,Proceedings of the Workshop on Intelligent Information Processing and Natural Language Generation,0,None
2020.inlg-1.2,{A}rabic {NLG} Language Functions,2020,-1,-1,2,0,18991,wael abed,Proceedings of the 13th International Conference on Natural Language Generation,0,"The Arabic language has very limited supports from NLG researchers. In this paper, we explain the challenges of the core grammar, provide a lexical resource, and implement the first language functions for the Arabic language. We did a human evaluation to evaluate our functions in generating sentences from the NADA Corpus."
2020.inlg-1.22,A Gold Standard Methodology for Evaluating Accuracy in Data-To-Text Systems,2020,-1,-1,2,1,5939,craig thomson,Proceedings of the 13th International Conference on Natural Language Generation,0,"Most Natural Language Generation systems need to produce accurate texts. We propose a methodology for high-quality human evaluation of the accuracy of generated texts, which is intended to serve as a gold-standard for accuracy evaluations of data-to-text systems. We use our methodology to evaluate the accuracy of computer generated basketball summaries. We then show how our gold standard evaluation can be used to validate automated metrics."
2020.inlg-1.28,Shared Task on Evaluating Accuracy,2020,-1,-1,1,1,5931,ehud reiter,Proceedings of the 13th International Conference on Natural Language Generation,0,"We propose a shared task on methodologies and algorithms for evaluating the accuracy of generated texts, specifically summaries of basketball games produced from basketball box score and other game data. We welcome submissions based on protocols for human evaluation, automatic metrics, as well as combinations of human evaluations and metrics."
2020.inlg-1.29,{R}epro{G}en: Proposal for a Shared Task on Reproducibility of Human Evaluations in {NLG},2020,-1,-1,4,0.857143,5962,anya belz,Proceedings of the 13th International Conference on Natural Language Generation,0,"Across NLP, a growing body of work is looking at the issue of reproducibility. However, replicability of human evaluation experiments and reproducibility of their results is currently under-addressed, and this is of particular concern for NLG where human evaluations are the norm. This paper outlines our ideas for a shared task on reproducibility of human evaluations in NLG which aims (i) to shed light on the extent to which past NLG evaluations are replicable and reproducible, and (ii) to draw conclusions regarding how evaluations can be designed and reported to increase replicability and reproducibility. If the task is run over several years, we hope to be able to document an overall increase in levels of replicability and reproducibility over time."
W19-8402,Natural Language Generation Challenges for Explainable {AI},2019,15,0,1,1,5931,ehud reiter,Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019),0,"Good quality explanations of artificial intelligence (XAI) reasoning must be written (and evaluated) for an explanatory purpose, targeted towards their readers, have a good narrative and causal structure, and highlight where uncertainty and data quality affect the AI output. I discuss these challenges from a Natural Language Generation (NLG) perspective, and highlight four specific NLG for XAI research challenges."
W18-6544,Comprehension Driven Document Planning in Natural Language Generation Systems,2018,0,0,2,1,5939,craig thomson,Proceedings of the 11th International Conference on Natural Language Generation,0,This paper proposes an approach to NLG system design which focuses on generating output text which can be more easily processed by the reader. Ways in which cognitive theory might be combined with existing NLG techniques are discussed and two simple experiments in content ordering are presented.
W18-6548,Generating Summaries of Sets of Consumer Products: Learning from Experiments,2018,0,0,2,0,27686,kittipitch kuptavanich,Proceedings of the 11th International Conference on Natural Language Generation,0,"We explored the task of creating a textual summary describing a large set of objects characterised by a small number of features using an e-commerce dataset. When a set of consumer products is large and varied, it can be difficult for a consumer to understand how the products in the set differ; consequently, it can be challenging to choose the most suitable product from the set. To assist consumers, we generated high-level summaries of product sets. Two generation algorithms are presented, discussed, and evaluated with human users. Our evaluation results suggest a positive contribution to consumers{'} understanding of the domain."
W18-6551,Meteorologists and Students: A resource for language grounding of geographical descriptors,2018,8,0,2,0,27652,alejandro ramossoto,Proceedings of the 11th International Conference on Natural Language Generation,0,"We present a data resource which can be useful for research purposes on language grounding tasks in the context of geographical referring expression generation. The resource is composed of two data sets that encompass 25 different geographical descriptors and a set of associated graphical representations, drawn as polygons on a map by two groups of human subjects: teenage students and expert meteorologists."
J18-3002,A Structured Review of the Validity of {BLEU},2018,10,25,1,1,5931,ehud reiter,Computational Linguistics,0,"The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique{---}in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing."
W17-3519,A Commercial Perspective on Reference,2017,0,0,1,1,5931,ehud reiter,Proceedings of the 10th International Conference on Natural Language Generation,0,"I briefly describe some of the commercial work which XXX is doing in referring expression algorithms, and highlight differences between what is commercially important (at least to XXX) and the NLG research literature. In particular, XXX is less interested in generic reference algorithms than in high-quality algorithms for specific types of references, such as components of machines, named entities, and dates."
W17-3535,Textually Summarising Incomplete Data,2017,0,0,2,0,5936,stephanie inglis,Proceedings of the 10th International Conference on Natural Language Generation,0,"Many data-to-text NLG systems work with data sets which are incomplete, ie some of the data is missing. We have worked with data journalists to understand how they describe incomplete data, and are building NLG algorithms based on these insights. A pilot evaluation showed mixed results, and highlighted several areas where we need to improve our system."
W16-6643,Absolute and Relative Properties in Geographic Referring Expressions,2016,22,0,3,1,33341,rodrigo oliveira,Proceedings of the 9th International Natural Language Generation conference,0,None
W15-4723,Designing an Algorithm for Generating Named Spatial References,2015,14,7,3,1,33341,rodrigo oliveira,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"We describe an initial version of an algorithm for generating named references to locations of geographic scale. We base the algorithm design on evidence from corpora and experiments, which show that named entity usage is extremely frequent, even in less obvious scenes, and that names are normally used as the first focus on a global region. The current algorithm normally selects the Frames of Reference that humans also select, but it needs improvement to mix frames via a mereological mechanism."
W15-4726,Creating Textual Driver Feedback from Telemetric Data,2015,28,3,2,0,2852,daniel braun,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"Usage based car insurances, which use sensors to track driver behaviour, are enjoying growing popularity. Although the data collected by these insurances could provide detailed feedback about the driving style, this information is usually kept away from the driver and is used only to calculate insurance premiums. In this paper, we explored the possibility of providing drivers with textual feedback based on telemetric data in order to improve individual driving, but also general road safety. We report that textual feedback generated through NLG was preferred to non-textual summaries currently popular in the field and specifically was better at giving users a concrete idea of how to adapt their driving."
W14-4419,Generating Annotated Graphs using the {NLG} Pipeline Architecture,2014,14,4,3,1,5937,saad mahamood,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,The Arria NLG Engine has been extended to generate annotated graphs: data graphs that contain computer-generated textual annotations to explain phenomena in those graphs. These graphs are generated alongside text-only data summaries.
W13-2119,{MIME} - {NLG} in Pre-Hospital Care,2013,13,6,4,0,40974,anne schneider,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"The cross-disciplinary MIME project aims to develop a mobile medical monitoring system that improves handover transactions in rural pre-hospital scenarios between the first person on scene and ambulance clinicians. NLG is used to produce a textual handover report at any time, summarising data from novel medical sensors, as well as observations and actions recorded by the carer. We describe the MIME project with a focus on the NLG algorithm and an initial evaluation of the generated reports."
W13-2128,{MIME}- {NLG} Support for Complex and Unstable Pre-hospital Emergencies,2013,5,3,4,0,40974,anne schneider,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"We present the first prototype of a handover report generator developed for the MIME (Managing Information in Medical Emergencies) project. NLG applications in the medical domain have been varied but most are deployed in clinical situations. We develop a mobile device for prehospital care which receives streamed sensor data and user input, and converts these into a handover report for paramedics."
N13-1137,Generating Expressions that Refer to Visible Objects,2013,52,44,3,0.555556,8841,margaret mitchell,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,Funding for this research has been provided by SICSA and ORSAS. We thank the anonymous reviewers for useful comments on this paper.
W12-1516,Working with Clinicians to Improve a Patient-Information {NLG} System,2012,9,2,2,1,5937,saad mahamood,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"NLG developers must work closely with domain experts in order to build good NLG systems, but relatively little has been published about this process. In this paper, we describe how NLG developers worked with clinicians (nurses) to improve an NLG system which generates information for parents of babies in a neonatal intensive care unit, using a structured revision process. We believe that such a process can significantly enhance the quality of many NLG systems, in medicine and elsewhere."
W11-2803,Generating Affective Natural Language for Parents of Neonatal Infants,2011,30,25,2,1,5937,saad mahamood,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"This paper presents several affective NLG strategies for generating medical texts for parents of pre-term neonates. Initially, these were meant to be personalised according to a model of the recipient's level of stress. However, our evaluation showed that all recipients preferred texts generated with the affective strategies, regardless of predicted stress level."
W11-2804,What is in a text and what does it do: Qualitative Evaluations of an {NLG} system {--} the {BT}-Nurse {--} using content analysis and discourse analysis,2011,15,4,2,0,44143,rahul sambaraju,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"Evaluations of NLG systems generally are quantiative, that is, based on corpus comparison statistics and/or results of experiments with people. Outcomes of such evaluations are important in demonstrating whether or not an NLG system is successful, but leave gaps in understanding why this is the case. Alternatively, qualitative evaluations carried out by experts provide knowledge on where a system needs to be improved. In this paper we describe two such evaluations carried out for the BT-Nurse system, using two different methodologies (content analysis and discourse analysis). The outcomes of such evaluations are discussed in comparison to what was learnt from a quantitiave evaluation of BT-Nurse. Implications for the role of similar evaluations in NLG are also discussed."
W11-2808,Two Approaches for Generating Size Modifiers,2011,18,10,3,0.952381,8841,margaret mitchell,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"This paper offers a solution to a small problem within a much larger problem. We focus on modelling how people use size in reference, words like big and tall, which is one piece within the much larger problem of how people refer to visible objects. Examining size in isolation allows us to begin untangling a few of the complex and interacting features that affect reference, and we isolate a set of features that may be used in a hand-coded algorithm or a machine learning approach to generate one of six basic size types. The hand-coded algorithm generates a modifier type with a high correspondence to those observed in human data, and achieves 81.3% accuracy in an entirely new domain. This trails oracle accuracy for this task by just 8%. Features used by the hand-coded algorithm are added to a larger set of features in the machine learning approach, and we do not find a statistically significant difference between the precision and recall of the two systems. The input and output of these systems are a novel characterization of the factors that affect referring expression generation, and the methods described here may serve as one building block in future work connecting vision to language."
W11-2704,Task-Based Evaluation of {NLG} Systems: Control vs Real-World Context,2011,9,7,1,1,5931,ehud reiter,Proceedings of the {UCNLG}+{E}val: Language Generation and Evaluation Workshop,0,"Currently there is little agreement about, or even discussion of, methodologies for task-based evaluation of NLG systems. I discuss one specific issue in this area, namely the importance of control vs the importance of ecological validity (real-world context), and suggest that perhaps we need to put more emphasis on ecological validity in NLG evaluations."
W10-4210,Natural Reference to Objects in a Visual Domain,2010,32,37,3,0.952381,8841,margaret mitchell,Proceedings of the 6th International Natural Language Generation Conference,0,"This paper discusses the basic structures necessary for the generation of reference to objects in a visual scene. We construct a study designed to elicit naturalistic referring expressions to relatively complex objects, and find aspects of reference that have not been accounted for in work on Referring Expression Generation (REG). This includes reference to object parts, size comparisons without crisp measurements, and the use of analogies. By drawing on research in cognitive science, neurophysiology, and psycholinguistics, we begin developing the input structure and background knowledge necessary for an algorithm capable of generating the kinds of reference we observe."
W10-1301,Using {NLG} and Sensors to Support Personal Narrative for Children with Complex Communication Needs,2010,28,20,3,0,45438,rolf black,Proceedings of the {NAACL} {HLT} 2010 Workshop on Speech and Language Processing for Assistive Technologies,0,"We are building a tool that helps children with Complex Communication Needs (CCN) to create stories about their day at school. The tool uses Natural Language Generation (NLG) technology to create a draft story based on sensor data of the child's activities, which the child can edit. This work is still in its early stages, but we believe it has great potential to support interactive personal narrative which is not well supported by current Augmentative and Alternative Communication (AAC) tools."
W10-1302,Automatic generation of conversational utterances and narrative for Augmentative and Alternative Communication: a prototype system,2010,26,9,3,0,45439,martin dempster,Proceedings of the {NAACL} {HLT} 2010 Workshop on Speech and Language Processing for Assistive Technologies,0,"We detail the design, development and evaluation of Augmentative and Alternative Communication (AAC) software which encourages rapid conversational interaction. The system uses Natural Language Generation (NLG) technology to automatically generate conversational utterances from a domain knowledge base modelled from content suggested by a small AAC user group. Findings from this work are presented along with a discussion about how NLG might be successfully applied to conversational AAC systems in the future."
W09-0601,Using {NLG} to Help Language-Impaired Users Tell Stories and Participate in Social Dialogues,2009,23,23,1,1,5931,ehud reiter,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"Augmentative and Alternative Communication (AAC) systems are communication aids for people who cannot speak because of motor or cognitive impairments. We are developing AAC systems where users select information they wish to communicate, and this is expressed using an NLG system. We believe this model will work well in contexts where AAC users wish to go beyond simply making requests or answering questions, and have more complex communicative goals such as story-telling and social interaction."
W09-0607,Generating Approximate Geographic Descriptions,2009,35,37,3,1,38400,ross turner,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"Georeferenced data sets are often large and complex. Natural Language Generation (NLG) systems are beginning to emerge that generate texts from such data. One of the challenges these systems face is the generation of geographic descriptions referring to the location of events or patterns in the data. Based on our studies in the domain of meteorology we present a two staged approach to generating geographic descriptions. The first stage involves using domain knowledge based on the task context to select a frame of reference, and the second involves using constraints imposed by the end user to select values within a frame of reference. Because geographic concepts are inherently vague our approach does not guarantee a distinguishing description. Our evaluation studies show that NLG systems, because they can analyse input data exhaustively, can produce more fine-grained geographic descriptions that are more useful to end users than those generated by human experts."
W09-0613,{S}imple{NLG}: A Realisation Engine for Practical Applications,2009,12,205,2,0.30303,6764,albert gatt,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"This paper describes SimpleNLG, a realisation engine for English which aims to provide simple and robust interfaces to generate syntactic structures and linearise them. The library is also flexible in allowing the use of mixed (canned and non-canned) representations."
J09-4008,An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems,2009,57,76,1,1,5931,ehud reiter,Computational Linguistics,0,"There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies."
2009.jeptalnrecital-long.13,Le projet {B}aby{T}alk : g{\\'e}n{\\'e}ration de texte {\\`a} partir de donn{\\'e}es h{\\'e}t{\\'e}rog{\\`e}nes pour la prise de d{\\'e}cision en unit{\\'e} n{\\'e}onatale,2009,-1,-1,4,0,14071,franccois portet,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Notre soci{\'e}t{\'e} g{\'e}n{\`e}re une masse d{'}information toujours croissante, que ce soit en m{\'e}decine, en m{\'e}t{\'e}orologie, etc. La m{\'e}thode la plus employ{\'e}e pour analyser ces donn{\'e}es est de les r{\'e}sumer sous forme graphique. Cependant, il a {\'e}t{\'e} d{\'e}montr{\'e} qu{'}un r{\'e}sum{\'e} textuel est aussi un mode de pr{\'e}sentation efficace. L{'}objectif du prototype BT-45, d{\'e}velopp{\'e} dans le cadre du projet Babytalk, est de g{\'e}n{\'e}rer des r{\'e}sum{\'e}s de 45 minutes de signaux physiologiques continus et d{'}{\'e}v{\'e}nements temporels discrets en unit{\'e} n{\'e}onatale de soins intensifs (NICU). L{'}article pr{\'e}sente l{'}aspect g{\'e}n{\'e}ration de texte de ce prototype. Une exp{\'e}rimentation clinique a montr{\'e} que les r{\'e}sum{\'e}s humains am{\'e}liorent la prise de d{\'e}cision par rapport {\`a} l{'}approche graphique, tandis que les textes de BT-45 donnent des r{\'e}sultats similaires {\`a} l{'}approche graphique. Une analyse a identifi{\'e} certaines des limitations de BT-45 mais en d{\'e}pit de cellesci, notre travail montre qu{'}il est possible de produire automatiquement des r{\'e}sum{\'e}s textuels efficaces de donn{\'e}es complexes."
W08-1104,Using Spatial Reference Frames to Generate Grounded Textual Summaries of Georeferenced Data,2008,24,26,3,1,38400,ross turner,Proceedings of the Fifth International Natural Language Generation Conference,0,Summarising georeferenced (can be identified according to it's location) data in natural language is challenging because it requires linking events describing its non-geographic attributes to their underlying geography. This mapping is not straightforward as often the only explicit geographic information such data contains is latitude and longitude. In this paper we present an approach to generating textual summaries of georeferenced data based on spatial reference frames. This approach has been implemented in a data-to-text system we have deployed in the weather forecasting domain.
W08-1119,The Importance of Narrative and Other Lessons from an Evaluation of an {NLG} System that Summarises Clinical Data,2008,19,34,1,1,5931,ehud reiter,Proceedings of the Fifth International Natural Language Generation Conference,0,"The BABYTALK BT-45 system generates textual summaries of clinical data about babies in a neonatal intensive care unit. A recent task-based evaluation of the system suggested that these summaries are useful, but not as effective as they could be. In this paper we present a qualitative analysis of problems that the evaluation highlighted in BT-45 texts. Many of these problems are due to the fact that BT-45 does not generate good narrative texts; this is a topic which has not previously received much attention from the NLG research community, but seems to be quite important for creating good data-to-text systems."
W07-2315,An Architecture for Data-to-Text Systems,2007,20,131,1,1,5931,ehud reiter,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"I present an architecture for data-to-text systems, that is NLG systems which produce texts from non-linguistic input data; this essentially extends the architecture of Reiter and Dale (2000) to systems whose input is raw data instead of AI knowledge bases. This architecture is being used in the BabyTalk project, and is based on experiences in several projects at Aberdeen; it also seems to be compatible with many data-to-text systems developed elsewhere. It consists of four stages which are organised in a pipeline: Signal Analysis, Data Interpretation, Document Planning, and Microplanning and Realisation."
W07-2325,A Comparison of Hedged and Non-hedged {NLG} Texts,2007,9,1,2,1,5937,saad mahamood,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"We assess the use of hedge phrases in affective NLG texts. A simple experiment suggests non-native speakers prefer texts that contain hedge phrases, but native speakers prefer texts that do not contain hedge phrases."
J07-2013,Last Words: The Shrinking Horizons of Computational Linguistics,2007,4,14,1,1,5931,ehud reiter,Computational Linguistics,0,"Understanding language is one of the great challenges of science, and languagerelated technology is one of the great opportunities of Information Technology. Consequently, many different kinds of researchers work on language issues. Within the computer science community, language is studied by the xe2x80x9cACL community,xe2x80x9d by which I mean researchers who regularly publish in Association for Computational Linguistics (ACL) venues, such as the journal Computational Linguistics and ACL conferences. But language-related research is also carried out by researchers in other areas of computer science, including knowledge representation, cognitive modeling, vision and robotics, and humanxe2x80x93computer interaction communities. Additionally, there are even more people outside computer science who study language, including linguists, psycholinguists, philosophers, and sociolinguists. This is fine; understanding language and developing language technology are huge problems, and it is very useful to have many research communities from diverse backgrounds working on language. This will be especially true if the different research communities are aware of each other, so they can share insights, observations, problems, and so forth. Unfortunately, my impression is that the ACL community is much less interested in research with other language-related research communities than it used to be. This impression is mostly based on discussions I have had with researchers who are on the border between ACL and another language-research community. Several such people have told me that whereas ten years ago they occasionally submitted papers to ACL venues and attended ACL conferences, now they do not bother, because they believe that the ACL community has no interest in their research. In attempt to quantify this insight, I have analyzed citations from papers published in Computational Linguistics in 1995 and in 2005. Specifically, I extracted all citations from Computational Linguistics (CL) articles (excluding book reviews) in these years to journal papers. I then classified the cited journal papers into one of the categories shown in Table 1; whenever possible this classification was based on the subject category assigned by ISI Journal Citation Reports (JCR) to the cited journal. For example, a citation of a paper in Cognitive Science would count as a psychology citation, since ISI JCR classifies Cognitive Science as xe2x80x9cPsychology, Experimental.xe2x80x9d I counted citations myself, rather than relying on ISI JCRxe2x80x99s count, as there were some mistakes in JCRxe2x80x99s counting. I also created my own xe2x80x9cother NLP and speechxe2x80x9d classification (that is, references to speech and NLP"
2007.mtsummit-ucnlg.13,The attribute selection for generation of referring expressions challenge. [Introduction to Shared Task Evaluation Challenge.],2007,-1,-1,3,0.3986,26421,anja belz,Proceedings of the Workshop on Using corpora for natural language generation,0,None
W06-1422,{GENEVAL}: A Proposal for Shared-task Evaluation in {NLG},2006,10,7,1,1,5931,ehud reiter,Proceedings of the Fourth International Natural Language Generation Conference,0,"We propose to organise a series of sharedtask NLG events, where participants are asked to build systems with similar input/output functionalities, and these systems are evaluated with a range of different evaluation techniques. The main purpose of these events is to allow us to compare different evaluation techniques, by correlating the results of different evaluations on the systems entered in the events."
E06-2020,Generating Spatio-Temporal Descriptions in Pollen Forecasts,2006,7,19,3,1,38400,ross turner,Demonstrations,0,We describe our initial investigations into generating textual summaries of spatiotemporal data with the help of a prototype Natural Language Generation (NLG) system that produces pollen forecasts for Scotland.
E06-1040,Comparing Automatic and Human Evaluation of {NLG} Systems,2006,21,97,2,0.3986,26421,anja belz,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We consider the evaluation problem in Natural Language Generation (NLG) and present results for evaluating several NLG systems with similar functionality, including a knowledge-based generator and several statistical systems. We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, including NI ST, B LEU, and ROUGE. We find that NI ST scores correlate best (>0.8) with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic evaluation of NLG systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available. However, in general it is probably best for automatic evaluations to be supported by human based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain."
W05-1615,Evaluation of an {NLG} System using Post-Edit Data: Lessons Learnt,2005,12,12,2,1,18952,somayajulu sripada,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,None
W05-1616,Generating Readable Texts for Readers with Low Basic Skills,2005,28,37,2,1,41079,sandra williams,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,"Most NLG systems generate texts for readers with good reading ability, but SkillSum adapts its output for readers with poor literacy. Evaluation with lowskilled readers confirms that SkillSum's knowledge-based microplanning choices enhance readability. We also discuss future readability improvements."
W03-2312,Acquiring and Using Limited User Models in {NLG},2003,13,13,1,1,5931,ehud reiter,Proceedings of the 9th {E}uropean Workshop on Natural Language Generation ({ENLG}-2003) at {EACL} 2003,0,"It is a truism of NLG that good knowledge of the reader can improve the quality of generated texts, and many NLG systems have been developed that exploit detailed user models when generating texts. Unfortunately, it is very difficult in practice to obtain detailed information about users. In this paper we describe our experiences in acquiring and using limited user models for NLG in four different systems, each of which took a different approach to this issue. One general conclusion is that it is useful if imperfect user models are understandable to users or domain experts, and indeed perhaps can be directly edited by them; this agrees with recent thinking about user models in other applications such as intelligent tutoring systems (Kay, 2001)."
W03-2317,Experiments with discourse-level choices and readability,2003,-1,-1,2,1,41079,sandra williams,Proceedings of the 9th {E}uropean Workshop on Natural Language Generation ({ENLG}-2003) at {EACL} 2003,0,None
W03-0611,Learning the Meaning and Usage of Time Phrases from a Parallel Text-Data Corpus,2003,22,15,1,1,5931,ehud reiter,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Learning Word Meaning from Non-Linguistic Data,0,"We present an empirical corpus study of the meaning and usage of time phrases in weather forecasts; this is based on a novel corpus analysis technique where we align phrases from the forecast text with data extracted from a numerical weather simulation. Previous papers have summarised this analysis and discussed the substantial variations we discovered among individual writers, which was perhaps our most surprising finding. In this paper we describe our analysis procedure and results in considerably more detail, and also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words."
E03-1021,Summarizing Neonatal Time Series Data,2003,7,18,2,1,18952,somayajulu sripada,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,We describe our investigations in generating textual summaries of physiological time series data to aid medical personnel in monitoring babies in neonatal intensive care units. Our studies suggest that summarization is a communicative task that requires data analysis techniques for determining the content of the summary. We describe a prototype system that summarizes physiological time series.
W02-2113,Should Corpora Texts Be Gold Standards for {NLG}?,2002,0,52,1,1,5931,ehud reiter,Proceedings of the International Natural Language Generation Conference,0,None
J02-4007,Squibs and Discussions: Human Variation and Lexical Choice,2002,19,44,1,1,5931,ehud reiter,Computational Linguistics,0,"Much natural language processing research implicitly assumes that word meanings are fixed in a language community, but in fact there is good evidence that different people probably associate slightly different meanings with words. We summarize some evidence for this claim from the literature and from an ongoing research project, and discuss its implications for natural language generation, especially for lexical choice, that is, choosing appropriate words for a generated text."
W01-0802,A Two-Staged Model For Content Determination,2001,10,25,2,0,53823,somayajula sripada,Proceedings of the {ACL} 2001 Eighth {E}uropean Workshop on Natural Language Generation ({EWNLG}),0,"In this paper we describe a two-stage model for content determination in systems that summarise time series data. The first stage involves building a qualitative overview of the data set, and the second involves using this overview, together with the actual data, to produce summaries of the time-series data. This model is based on our observations of how human experts summarise time-series data."
P01-1057,Using a Randomised Controlled Clinical Trial to Evaluate an {NLG} System,2001,12,17,1,1,5931,ehud reiter,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"The STOP system, which generates personalised smoking-cessation letters, was evaluated by a randomised controlled clinical trial. We believe this is the largest and perhaps most rigorous task effectiveness evaluation ever performed on an NLG system. The detailed results of the clinical trial have been presented elsewhere, in the medical literature. In this paper we discuss the clinical trial itself: its structure and cost, what we did and did not learn from it (especially considering that the trial showed that STOP was not effective), and how it compares to other NLG evaluation techniques."
W00-1429,Knowledge Acquisition for Natural Language Generation,2000,11,28,1,1,5931,ehud reiter,{INLG}{'}2000 Proceedings of the First International Conference on Natural Language Generation,0,"We describe the knowledge acquisition (KA) techniques used to build the STOP system, especially sorting and think-aloud protocols. That is, we describe the ways in which we interacted with domain experts to determine appropriate user categories, schemas, detailed content rules, and so forth for STOP. Informal evaluations of these techniques suggest that they had some benefit, but perhaps were most successful as a source of insight and hypotheses, and should ideally have been supplemented by other techniques when deciding on the specific rules and knowledge incorporated into STOP."
J00-2005,Pipelines and size constraints,2000,7,26,1,1,5931,ehud reiter,Computational Linguistics,0,"Some types of documents need to meet size constraints, such as fitting into a limited number of pages. This can be a difficult constraint to enforce in a pipelined natural language generation (NLG) system, because size is mostly determined by content decisions, which usually are made at the beginning of the pipeline, but size cannot be accurately measured until the document has been completely processed by the NLG system. I present experimental data on the performance of single-solution pipeline, multiple-solution pipeline, and revision-based variants of the STOP system (which produces personalized smoking-cessation leaflets) in meeting a size constraint. This shows that a multiple-solution pipeline does much better than a single-solution pipeline, and that a revision-based system does best of all."
W97-0905,Tailored Patient Information: Some Issues and Questions,1997,9,12,1,1,5931,ehud reiter,From Research to Commercial Applications: Making {NLP} Work in Practice,0,"Tailored patient information (TPI) systems are computer programs which produce personalised heath-information material for patients. TPI systems are of growing interest to the natural-language generation (NLG) community; many TPI systems have also been developed in the medical community, usually with mail-merge technology. No matter what technology is used, experience shows that it is not easy to field a TPI system, even if it is shown to be effective in clinical trials. In this paper we discuss some of the difficulties in fielding TPI systems. This is based on our experiences with 2 TPI systems, one for generating asthma-information booklets and one for generating smoking-cessation letters."
A97-1037,Customizable Descriptions of Object-Oriented Models,1997,10,23,3,1,51615,benoit lavoie,Fifth Conference on Applied Natural Language Processing,0,"With the emergence of object-oriented technology and user-centered software engineering paradigms, the requirements analysis phase has changed in two important ways: it has become an iterative activity, and it has become more closely linked to the design phase of software engineering (Davis, 1993). A requirements analyst builds a formal object-oriented (OO) domain model. A user (domain expert) validates the domain model. The domain model undergoes subsequent evolution (modi cation or adjustment) by a (perhaps di erent) analyst. Finally, the domain model is passed to the designer (system analyst), who re nes the model into a OO design model used as the basis for implementation. Thus, we can see that the OO models form the basis of many important ows of information in OO software engineering methodologies. How can this information best be communicated? It is widely believed that graphical representations are easy to learn and use, both for modeling and for communication among the engineers and domain experts who together develop the OO domain model. This belief is re ected by the large number of graphical OO modeling tools currently in research labs and on the market. However, this belief is not accurate, as some recent empirical studies show. For example, Kim (1990) simulated a modeling task with experienced analysts and a validation task with sophisticated users not familiar with the particular graphical language. Both user groups showed semantic error rates between 25% and 70% for the separately scored areas of entities, attributes, and relations. Relations were particularly troublesome to both analysts and users. Petre (1995) compares diagrams with textual representations of nested conditional structures (which can be compared to OO modeling in the complexity of the paths through the system). She nds that the intrinsic di culty of the graphics mode was the strongest e ect observed (p.35). We therefore conclude that graphics, in order to assure maximum communicative e ciency, needs to be complemented by an alternate view of the data. We claim that the alternate view should be provided by an explanation tool that represents the data in the form of a uent English text. This paper presents such a tool, the ModelExplainer, or ModEx for short, and focuses on the customizability of the system. Automatically generating natural-language descriptions of software models and speci cations is not a new idea. The rst such system was Swartout's GIST Paraphraser (Swartout, 1982). More recent projects include the paraphraser in ARIES (Johnson et al., 1992); the GEMA dataow diagram describer (Scott and de Souza, 1989); and Gulla's paraphraser for the PPP system (Gulla, 1993). ModEx certainly belongs in the tradition of these speci cation paraphrasers, but the combination of features that we will describe in the next section (and in particular the customizability) is, to our knowledge, unique."
W96-0503,The {M}odel{E}xplainer,1996,-1,-1,3,1,51615,benoit lavoie,Eighth International Natural Language Generation Workshop (Posters and Demonstrations),0,None
W94-0319,"Has a Consensus {NL} Generation Architecture Appeared, and is it Psycholinguistically Plausible?",1994,24,138,1,1,5931,ehud reiter,Proceedings of the Seventh International Workshop on Natural Language Generation,0,"I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other. I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems."
P92-1034,Using Classification to Generate Text,1992,24,21,1,1,5931,ehud reiter,30th Annual Meeting of the Association for Computational Linguistics,1,"The IDAS natural-language generation system uses a KL-ONE type classifier to perform content determination, surface realisation, and part of text planning. Generation-by-classification allows IDAS to use a single representation and reasoning component for both domain and linguistic knowledge, which is difficult for systems based on unification or systemic generation techniques."
C92-1038,A Fast Algorithm for the Generation of Referring Expressions,1992,22,91,1,1,5931,ehud reiter,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We simplify previous work in the development of algorithms for the generation of referring expressions while at the same time taking account of psycholinguistic findings and transcript data. The result is a straightforward algorithm that is computationally tractable, sensitive to the preferences of human users, and reasonably domain-independent. We provide a specification of the resources a host system must provide in order to make use of the algorithm, and describe an implementation used in the IDAS system."
A92-1009,Automatic Generation of On-Line Documentation in the {IDAS} Project,1992,15,52,1,1,5931,ehud reiter,Third Conference on Applied Natural Language Processing,0,"The Intelligent Documentation Advisory System generates on-line documentation and help messages from a domain knowledge base, using natural-language (NL) generation techniques. This paper gives an overview of IDAS, with particular emphasis on: (1) its architecture and the types of questions it is capable of answering; (2) its KR and NL generation systems, and lessons we have learned in designing them; and (3) its hypertext-like user interface, and the benefits such an interface brings."
W90-0104,A New Model for Lexical Choice for Open-Class Words,1990,12,16,1,1,5931,ehud reiter,Proceedings of the Fifth International Workshop on Natural Language Generation,0,None
P90-1013,The Computational Complexity of Avoiding Conversational Implicatures,1990,15,51,1,1,5931,ehud reiter,28th Annual Meeting of the Association for Computational Linguistics,1,"Referring expressions and other object descriptions should be maximal under the Local Brevity, No Unnecessary Components, and Lexical Preference preference rules; otherwise, they may lead hearers to infer unwanted conversational implicatures. These preference rules can be incorporated into a polynomial time generation algorithm, while some alternative formalizations of conversational implicature make the generation task NP-Hard."
