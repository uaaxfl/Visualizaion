1985.tmi-1.4,J82-3004,0,\N,Missing
1985.tmi-1.4,J83-3001,1,\N,Missing
1985.tmi-1.4,A83-1027,0,\N,Missing
1985.tmi-1.4,P84-1102,1,\N,Missing
1985.tmi-1.4,1985.tmi-1.19,1,\N,Missing
1987.mtsummit-1.32,C86-1138,1,0.881052,"Missing"
1987.mtsummit-1.32,P84-1086,0,0.0467623,"Missing"
1987.mtsummit-1.32,C86-1148,0,0.0414563,"Missing"
1987.mtsummit-1.32,P85-1017,0,0.0278978,"mantic roles to noun phrase constituents. Moreover, functional structures integrate far more coherently into frame based semantic structures. (*SYMPTOM (is-a (value *NOMINAL)) (:severity (sem *SEVERITY)) (:location (sem *BODY-PART)) (:pain-spec (sem *PAIN-TYPE)) (:name (sem *SYMPTOM-NAME)) ) (*PAIN (is-a (value *SYMPTOM)) ) Fig. 3-3 Fragment of domain semantics specification Two well-functional grammar formalisms are Functional Unification Grammar (UG) (Kay 1984) and Lexical Functional Grammar (LFG) (Bresnan 1982). -76- Figure 3-5 is a fragment of LFG written in a notation similar to PATR-II (Pereira 1985, Shieber 1985). The last rule is generated automatically from the dictionary and general morphological rules. 1985), the functional grammar formalisms are considered far less efficient than formalisms like ATNs (Woods 1970) or (especially) context-free phrase structure grammars. Moreover, pure functional grammars do not provide the semantic information that is required to eliminate nonsensical parses and to construct the output meaning representation. We address both problems by precompiling a syntactic LFG together with a separate domain semantics specification into an augmented context-free"
1987.mtsummit-1.32,C86-1149,1,0.86677,"Missing"
1992.tmi-1.20,J90-2002,0,0.126728,"Missing"
1992.tmi-1.20,1991.mtsummit-papers.18,0,0.0331948,"Missing"
1992.tmi-1.20,1991.mtsummit-papers.9,1,0.856286,"Missing"
1992.tmi-1.20,P91-1024,0,0.0387602,"Missing"
1992.tmi-1.20,1983.tc-1.13,0,0.0601175,"Missing"
1993.tmi-1.28,1992.tmi-1.20,1,0.857485,"Missing"
1993.tmi-1.28,C92-2081,0,0.0640592,"Missing"
1993.tmi-1.28,1992.tmi-1.23,0,0.0390608,"Missing"
1993.tmi-1.28,C92-2099,0,0.0610472,"Missing"
1993.tmi-1.28,1991.mtsummit-papers.9,1,0.892007,"Missing"
1993.tmi-1.28,C92-3168,0,0.0613112,"Missing"
1993.tmi-1.28,C92-2085,0,0.0625901,"Missing"
1993.tmi-1.28,C92-2088,0,0.06063,"Missing"
2001.mtsummit-road.7,J90-2002,0,0.143628,"Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are using the EBMT engine that was developed for the Diplomat and Tongues systems (Brown, 1996)1. In addition, we plan to develop statistical techniques for robust MT with sparse data using expone"
2001.mtsummit-road.7,C96-1030,1,0.787634,"rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are using the EBMT engine that was developed for the Diplomat and Tongues systems (Brown, 1996)1. In addition, we plan to develop statistical techniques for robust MT with sparse data using exponential models a"
2001.mtsummit-road.7,jones-havrilla-1998-twisted,0,0.0712645,"ng indigenous people to give up their languages. MT additionally facilitates the design of educational programs in endangered languages, which can be a tool for their documentation and preservation. Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are u"
2001.mtsummit-road.7,P98-2160,0,0.151102,"the internet without requiring indigenous people to give up their languages. MT additionally facilitates the design of educational programs in endangered languages, which can be a tool for their documentation and preservation. Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. Fo"
2001.mtsummit-road.7,sheremetyeva-nirenburg-2000-towards,0,0.0663845,"systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages. Keywords Low-density languages, minor languages, feature detection, version space learning 1. Introduction and Motivation Recently, efforts in machine translation have spread in two directions. Long-term, high-cost development cycles have given way to research on how to build MT systems for new languages quickly and cheaply (Somers, 1997; Nirenburg, 1998; Nirenburg & Raskin 1998; Sherematyeva & Nirenburg, 2000, Jones & Havrilla 1998). Rapid deployment of MT can be useful in situations such as crises in which time and money are short, but more importantly, lowering the cost of MT has, in turn, opened the option of building MT systems for languages that do not have enough speakers to financially support a costly development process. (See Frederking, to appear; Frederking, Rudnicky, Hogan 1997; and the SALTMIL discussion group, http://193.2.100.60/SALTMIL). There is now increasing awareness of the importance of MT for low-density languages as a way of providing access to government, education, healthc"
2001.mtsummit-road.7,1997.tc-1.13,0,0.046319,"gorithm. Our vision for MT in the future is one in which systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages. Keywords Low-density languages, minor languages, feature detection, version space learning 1. Introduction and Motivation Recently, efforts in machine translation have spread in two directions. Long-term, high-cost development cycles have given way to research on how to build MT systems for new languages quickly and cheaply (Somers, 1997; Nirenburg, 1998; Nirenburg & Raskin 1998; Sherematyeva & Nirenburg, 2000, Jones & Havrilla 1998). Rapid deployment of MT can be useful in situations such as crises in which time and money are short, but more importantly, lowering the cost of MT has, in turn, opened the option of building MT systems for languages that do not have enough speakers to financially support a costly development process. (See Frederking, to appear; Frederking, Rudnicky, Hogan 1997; and the SALTMIL discussion group, http://193.2.100.60/SALTMIL). There is now increasing awareness of the importance of MT for low-densit"
2001.mtsummit-road.7,C98-2155,0,\N,Missing
2003.mtsummit-papers.4,C96-1030,1,\N,Missing
2003.mtsummit-papers.4,J93-2003,0,\N,Missing
2003.mtsummit-papers.4,J90-2002,0,\N,Missing
2003.mtsummit-papers.4,P02-1040,0,\N,Missing
2003.mtsummit-papers.4,P02-1039,0,\N,Missing
2003.mtsummit-papers.4,P91-1024,0,\N,Missing
2003.mtsummit-papers.4,2001.mtsummit-ebmt.1,1,\N,Missing
2004.eamt-1.14,J90-2002,0,0.23859,"ransfer rules, which encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpu"
2004.eamt-1.14,J93-2003,0,0.0115785,"encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods requi"
2004.eamt-1.14,1997.tmi-1.13,0,0.0783073,"stem to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel text for the two languages – on the order of magnitude of a million words or more. Such resources are curre"
2004.eamt-1.14,P02-1040,0,0.0790722,"Missing"
2004.eamt-1.14,2001.mtsummit-road.7,1,0.788397,"al of learning compositional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures"
2004.eamt-1.14,2002.tmi-papers.17,1,0.666028,"tional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures the compositional makeup o"
2004.eamt-1.14,2003.mtsummit-papers.53,1,0.816612,"ich four English reference translations are available. The following systems were evaluated in the experiment: 1. Three versions of the Hindi-to-English XFER system: 1a. XFER with No Grammar: the XFER system with no syntactic transfer rules (i.e. only lexical phrase-to-phrase matches and word-toword lexical transfer rules, with and without morphology). 1b. XFER with Learned Grammar: The XFER system with automatically learned syntactic transfer rules. 1c. XFER with Manual Grammar: The XFER system with the manually developed syntactic transfer rules. 2. SMT: The CMU Statistical MT (SMT) system (Vogel et al, 2003), trained on the limited-data parallel text resources. 3. EBMT: The CMU Example-based MT (EBMT) system (Brown, 1997), trained on the limiteddata parallel text resources. 4. MEMT: A “multi-engine” version that combines the lattices produced by the SMT system, and the XFER system with manual grammar. The decoder then selects an output from the joint lattice. Performance of the systems was measured using the NIST scoring metric (Doddington, 2002), as well as the BLEU score (Papineni et al, 2002). In order to validate the statistical significance of the differences in NIST and BLEU scores, we appl"
2004.eamt-1.14,P01-1067,0,0.0309867,"he source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel tex"
2004.eamt-1.14,C90-3044,0,\N,Missing
2004.eamt-1.14,P02-1038,0,\N,Missing
2004.eamt-1.5,1999.tmi-1.3,1,0.93927,"eds are presently limited to English and Spanish, one goal of the project is to provide a model that is extensible for use by different agencies in other countries and other domains. Lacking sufficient resources to develop a knowledge- or transfer-based machine translation (MT) system, and requiring an approach that can be quickly adapted to other domains and languages, we chose to use CMU’s Pangloss-Lite (Panlite) MultiEngine Machine Translation (MEMT) system (Frederking & Brown, 1996) and rely primarily on the Example-Based MT engine, which has been undergoing continuous enhancements (e.g., Brown, 1999; Brown, 2000; Brown et al., 2003). Panlite was the underlying translation system in the rapiddeployment speech-to-speech DIPLOMAT project (www.lti.cs.cmu.edu/Research/Diplomat/; Frederking et al., 1997); it therefore seemed to be an ideal choice. Nonetheless, the use of Panlite in the context of this project has been challenging for a variety of reasons. In the first place, the lack of domain-specific parallel data has made it necessary to exercise a great deal of creativity in building the linguistic data resources. Secondly, after the data has been obtained, there are still challenges to be"
2004.eamt-1.5,2003.mtsummit-papers.4,1,0.923673,"to English and Spanish, one goal of the project is to provide a model that is extensible for use by different agencies in other countries and other domains. Lacking sufficient resources to develop a knowledge- or transfer-based machine translation (MT) system, and requiring an approach that can be quickly adapted to other domains and languages, we chose to use CMU’s Pangloss-Lite (Panlite) MultiEngine Machine Translation (MEMT) system (Frederking & Brown, 1996) and rely primarily on the Example-Based MT engine, which has been undergoing continuous enhancements (e.g., Brown, 1999; Brown, 2000; Brown et al., 2003). Panlite was the underlying translation system in the rapiddeployment speech-to-speech DIPLOMAT project (www.lti.cs.cmu.edu/Research/Diplomat/; Frederking et al., 1997); it therefore seemed to be an ideal choice. Nonetheless, the use of Panlite in the context of this project has been challenging for a variety of reasons. In the first place, the lack of domain-specific parallel data has made it necessary to exercise a great deal of creativity in building the linguistic data resources. Secondly, after the data has been obtained, there are still challenges to be met in using and managing the dat"
2004.eamt-1.5,C00-1019,1,0.847125,"ntly limited to English and Spanish, one goal of the project is to provide a model that is extensible for use by different agencies in other countries and other domains. Lacking sufficient resources to develop a knowledge- or transfer-based machine translation (MT) system, and requiring an approach that can be quickly adapted to other domains and languages, we chose to use CMU’s Pangloss-Lite (Panlite) MultiEngine Machine Translation (MEMT) system (Frederking & Brown, 1996) and rely primarily on the Example-Based MT engine, which has been undergoing continuous enhancements (e.g., Brown, 1999; Brown, 2000; Brown et al., 2003). Panlite was the underlying translation system in the rapiddeployment speech-to-speech DIPLOMAT project (www.lti.cs.cmu.edu/Research/Diplomat/; Frederking et al., 1997); it therefore seemed to be an ideal choice. Nonetheless, the use of Panlite in the context of this project has been challenging for a variety of reasons. In the first place, the lack of domain-specific parallel data has made it necessary to exercise a great deal of creativity in building the linguistic data resources. Secondly, after the data has been obtained, there are still challenges to be met in using"
2004.eamt-1.5,1996.amta-1.35,1,0.903197,"to borders with Guatemala and Mexico. The Dominican Republic is primarily and officially Spanish speaking.2,3 While our translation needs are presently limited to English and Spanish, one goal of the project is to provide a model that is extensible for use by different agencies in other countries and other domains. Lacking sufficient resources to develop a knowledge- or transfer-based machine translation (MT) system, and requiring an approach that can be quickly adapted to other domains and languages, we chose to use CMU’s Pangloss-Lite (Panlite) MultiEngine Machine Translation (MEMT) system (Frederking & Brown, 1996) and rely primarily on the Example-Based MT engine, which has been undergoing continuous enhancements (e.g., Brown, 1999; Brown, 2000; Brown et al., 2003). Panlite was the underlying translation system in the rapiddeployment speech-to-speech DIPLOMAT project (www.lti.cs.cmu.edu/Research/Diplomat/; Frederking et al., 1997); it therefore seemed to be an ideal choice. Nonetheless, the use of Panlite in the context of this project has been challenging for a variety of reasons. In the first place, the lack of domain-specific parallel data has made it necessary to exercise a great deal of creativity"
2004.eamt-1.5,H94-1005,0,0.0416755,"l soon run in the Dominican Republic too. The presence of the prototype system at universities will allow our colleagues in the client countries to collect data from actual future users prior to fielding the system at actual ports of entry. It will also give us an opportunity to collect more authentic data. formal documents and some newswire. They diverge widely in content and style from text in our domain and we have found that they provide little translation help. A baseline version of the system, trained on a hand-refined version of the EnglishSpanish United Nations (U.N.) parallel corpus (Graff & Finch, 1994) with some additional parallel text obtained from the Pan American Health Organization (PAHO) and a statistically-derived dictionary based on that corpus, produces translations that are virtually useless in the context of our project, and is particularly ill-suited to the translation of dialogues. If suitable monolingual data were available, it would be possible to create a parallel corpus through manual or semi-automatic translation, but very little of that data is to be found either. In the Dominican Republic, computerized access to a database of traveler information is available at major po"
2004.eamt-1.5,cavalli-sforza-etal-2004-developing,1,\N,Missing
2005.eamt-1.13,P93-1035,0,0.119387,"Missing"
2005.eamt-1.13,2003.mtsummit-papers.8,0,0.0325102,"Missing"
2005.eamt-1.13,font-llitjos-carbonell-2004-translation,1,0.638046,"Missing"
2005.eamt-1.13,P03-1057,0,0.0726092,"Missing"
2005.eamt-1.13,C94-1023,0,0.0651462,"Missing"
2005.eamt-1.13,W01-1406,0,0.101271,"Missing"
2005.eamt-1.13,1995.tmi-1.27,0,0.155915,"Missing"
2005.eamt-1.13,1995.tmi-1.23,0,0.157955,"Missing"
2005.eamt-1.13,2001.mtsummit-ebmt.4,0,\N,Missing
2005.eamt-1.21,1997.tmi-1.13,1,0.813788,"Missing"
2005.eamt-1.21,2003.mtsummit-papers.4,1,0.883484,"Missing"
2005.eamt-1.21,C00-1019,1,0.93939,"Missing"
2005.eamt-1.21,1994.amta-1.10,1,0.69518,"Missing"
2005.eamt-1.21,P02-1040,0,0.0771603,"Missing"
2006.amta-papers.3,abir-etal-2002-fluent,1,0.765389,"form bilingual dictionary for each language pair. CBMT exhibits two advantages over traditional MT approaches: (1) higher accuracy due to the model’s ability to decode long n-grams (as evidenced by the performance of CBMT’s stillincomplete prototype which achieved a higher BLEU score in initial testing than any other MT system to date), and (2) the ability to extend fairly rapidly to new language pairs, including those that lack sufficient parallel text. This paper describes the fundamental new techniques underlying CBMT, and presents results from Spanish-toEnglish MT. In our AMTA-2002 paper (Abir et al., 2002), we introduced some of the component concepts on a general level, but details were not provided. This paper addresses several of those concepts as well as others, and provides methodological details. Furthermore, our 2002 paper stated that we were experimenting with a corpus-based Abstract Context-Based Machine Translation™ (CBMT) is a new paradigm for corpusbased translation that requires no parallel text. Instead, CBMT relies on a lightweight translation model utilizing a fullform bilingual dictionary and a sophisticated decoder using long-range context via long n-grams and cascaded overlap"
2006.amta-papers.3,P01-1008,0,0.0410266,"tain the desired word or phrase anywhere in the massive monolingual indexed corpus. Anywhere from 1,000 to 100,000 paired contexts, which may be of variable length, are typically generated. 3.4 Process 3: Word and Phrasal Synonym Generation Step 2: Tabulate, sort, and unify paired contexts (e.g., a long paired context that occurs multiple times ranks above one without repeat occurrences). The CBMT system has a method for identifying synonyms or near-synonyms on a word or phrasal level using a monolingual corpus. This approach differs from others in that it does not require parallel resources (Barzilay and McKeown, 2001; Lin et al., 2003; Callison-Burch et al., 2006) nor does it use pre-determined sets of manually coded patterns (Lin et al., 2003). In addition, CBMT’s methods work on the word as well as phrasal level. If the ngrams in the above described lattice fail to overlap fully (or at all) we could flood deeper by retrieving more than the top m flooding candidates from the target language corpus, but that would compromise computational tractability and degrade the quality of the results as we get fewer and fewer complete matches. As an alternative, the CBMT system’s word and phrasal synonym (and near-s"
2006.amta-papers.3,J90-2002,0,0.0901176,"nt (Uchida and Zhu, 1993; Carbonell et al., 1994; Mitamura et al., 1994). Both methods rely on extensive human knowledge engineering in all phases. CBMT is radically different. 5 Related Work Traditional Rule-Based MT systems are comprised of an analysis phase (typically a string-to-tree parser), a transfer phase (typically a rule-based tree-to-tree transformer), and a synthesis phase (typically a tree-to-linear-string generator). Hut26 Corpus-based systems, such as ExampleBased MT (Nagao, 1984; Sumita and Iida, 1991; Brown et al., 2003; Kim et al., 2005; Doi et al., 2005) and Statistical MT (Brown et al., 1990; Yamada and Knight, 2002; Och, 2005) are comprised of a translation model and a target language model. The two components are readily evident, for instance, in the original IBM Candide system (Brown et al., 1990), where the central equation is: Example 1 Input: Un coche bomba estalla junto a una comisaría de policía en Bagdad  CBMT: a car bomb explodes next to a police station in baghdad  SYSTRAN: A car pump explodes next to a police station of police in Bagdad Example 2 Input: Hamas anunció este jueves el fin de su cese del fuego con Israel Topt = Arg max[P(S |Ti ) P(Ti )]  CBMT: hamas an"
2006.amta-papers.3,1994.amta-1.36,1,0.49874,"mprovements from 0.5953 to 0.6267 were due to bug fixes/algorithmic improvements. (7) Non-Blind test improvements are largely due to algorithmic development. (8) We scored each human reference set against the other 3 references plus an additional human set so that each was scored against 4 references total. The human scores ranged from 0.7172 to 0.7941. chins (1986) and Nagao (1989) give excellent overviews. In contrast, interlingua-based systems perform multi-phase analysis and synthesis, but reduce or eliminate the need for a transfer component (Uchida and Zhu, 1993; Carbonell et al., 1994; Mitamura et al., 1994). Both methods rely on extensive human knowledge engineering in all phases. CBMT is radically different. 5 Related Work Traditional Rule-Based MT systems are comprised of an analysis phase (typically a string-to-tree parser), a transfer phase (typically a rule-based tree-to-tree transformer), and a synthesis phase (typically a tree-to-linear-string generator). Hut26 Corpus-based systems, such as ExampleBased MT (Nagao, 1984; Sumita and Iida, 1991; Brown et al., 2003; Kim et al., 2005; Doi et al., 2005) and Statistical MT (Brown et al., 1990; Yamada and Knight, 2002; Och, 2005) are comprised of"
2006.amta-papers.3,2003.mtsummit-papers.4,1,0.913286,"lysis and synthesis, but reduce or eliminate the need for a transfer component (Uchida and Zhu, 1993; Carbonell et al., 1994; Mitamura et al., 1994). Both methods rely on extensive human knowledge engineering in all phases. CBMT is radically different. 5 Related Work Traditional Rule-Based MT systems are comprised of an analysis phase (typically a string-to-tree parser), a transfer phase (typically a rule-based tree-to-tree transformer), and a synthesis phase (typically a tree-to-linear-string generator). Hut26 Corpus-based systems, such as ExampleBased MT (Nagao, 1984; Sumita and Iida, 1991; Brown et al., 2003; Kim et al., 2005; Doi et al., 2005) and Statistical MT (Brown et al., 1990; Yamada and Knight, 2002; Och, 2005) are comprised of a translation model and a target language model. The two components are readily evident, for instance, in the original IBM Candide system (Brown et al., 1990), where the central equation is: Example 1 Input: Un coche bomba estalla junto a una comisaría de policía en Bagdad  CBMT: a car bomb explodes next to a police station in baghdad  SYSTRAN: A car pump explodes next to a police station of police in Bagdad Example 2 Input: Hamas anunció este jueves el fin de su"
2006.amta-papers.3,N06-1003,0,0.0277785,"e massive monolingual indexed corpus. Anywhere from 1,000 to 100,000 paired contexts, which may be of variable length, are typically generated. 3.4 Process 3: Word and Phrasal Synonym Generation Step 2: Tabulate, sort, and unify paired contexts (e.g., a long paired context that occurs multiple times ranks above one without repeat occurrences). The CBMT system has a method for identifying synonyms or near-synonyms on a word or phrasal level using a monolingual corpus. This approach differs from others in that it does not require parallel resources (Barzilay and McKeown, 2001; Lin et al., 2003; Callison-Burch et al., 2006) nor does it use pre-determined sets of manually coded patterns (Lin et al., 2003). In addition, CBMT’s methods work on the word as well as phrasal level. If the ngrams in the above described lattice fail to overlap fully (or at all) we could flood deeper by retrieving more than the top m flooding candidates from the target language corpus, but that would compromise computational tractability and degrade the quality of the results as we get fewer and fewer complete matches. As an alternative, the CBMT system’s word and phrasal synonym (and near-synonym) generation method can identify synonyms"
2006.amta-papers.3,2005.mtsummit-tutorials.1,0,0.0210023,"994; Mitamura et al., 1994). Both methods rely on extensive human knowledge engineering in all phases. CBMT is radically different. 5 Related Work Traditional Rule-Based MT systems are comprised of an analysis phase (typically a string-to-tree parser), a transfer phase (typically a rule-based tree-to-tree transformer), and a synthesis phase (typically a tree-to-linear-string generator). Hut26 Corpus-based systems, such as ExampleBased MT (Nagao, 1984; Sumita and Iida, 1991; Brown et al., 2003; Kim et al., 2005; Doi et al., 2005) and Statistical MT (Brown et al., 1990; Yamada and Knight, 2002; Och, 2005) are comprised of a translation model and a target language model. The two components are readily evident, for instance, in the original IBM Candide system (Brown et al., 1990), where the central equation is: Example 1 Input: Un coche bomba estalla junto a una comisaría de policía en Bagdad  CBMT: a car bomb explodes next to a police station in baghdad  SYSTRAN: A car pump explodes next to a police station of police in Bagdad Example 2 Input: Hamas anunció este jueves el fin de su cese del fuego con Israel Topt = Arg max[P(S |Ti ) P(Ti )]  CBMT: hamas announced thursday the end of the cease"
2006.amta-papers.3,2005.mtsummit-ebmt.6,0,0.26618,"el (the second probability). The decoder is the process that estimates Topt. The translation model is typically trained from a large (e.g., 100MB) sentence-aligned parallel corpus of professionally translated text. The target language model is typically trained from a much larger monolingual corpus. CBMT is also a corpus-based approach, closest to EBMT, but radically different in terms of requiring no parallel text whatsoever. In a sense, it is reminiscent of the old “Shake and Bake” idea (Whitelock, 1992), the newer EXERGE method in generation-heavy MT (Habash, 2003) and also the METIS work (Dirix et al., 2005) at a very abstract level of letting the target language establish lexical order. It bears some commonality to the work of Brown et al. (Brown et al., 2003) in that it permits combination of lattice entries with overlap (although the relevant CBMT components are covered in patent applications filed in 2001). However, it differs greatly from all previous systems in the maximal overlap principle for decoding and confirmation, in using consistently long ngrams, in near-synonym phrase substitutability, and in requiring no parallel text whatsoever to translate.  SYSTRAN: Hamas announced east Thurs"
2006.amta-papers.3,P91-1024,0,0.0218862,"perform multi-phase analysis and synthesis, but reduce or eliminate the need for a transfer component (Uchida and Zhu, 1993; Carbonell et al., 1994; Mitamura et al., 1994). Both methods rely on extensive human knowledge engineering in all phases. CBMT is radically different. 5 Related Work Traditional Rule-Based MT systems are comprised of an analysis phase (typically a string-to-tree parser), a transfer phase (typically a rule-based tree-to-tree transformer), and a synthesis phase (typically a tree-to-linear-string generator). Hut26 Corpus-based systems, such as ExampleBased MT (Nagao, 1984; Sumita and Iida, 1991; Brown et al., 2003; Kim et al., 2005; Doi et al., 2005) and Statistical MT (Brown et al., 1990; Yamada and Knight, 2002; Och, 2005) are comprised of a translation model and a target language model. The two components are readily evident, for instance, in the original IBM Candide system (Brown et al., 1990), where the central equation is: Example 1 Input: Un coche bomba estalla junto a una comisaría de policía en Bagdad  CBMT: a car bomb explodes next to a police station in baghdad  SYSTRAN: A car pump explodes next to a police station of police in Bagdad Example 2 Input: Hamas anunció este"
2006.amta-papers.3,1993.mtsummit-1.14,0,0.0557415,"ng was also a factor. The earlier Blind test improvements from 0.5953 to 0.6267 were due to bug fixes/algorithmic improvements. (7) Non-Blind test improvements are largely due to algorithmic development. (8) We scored each human reference set against the other 3 references plus an additional human set so that each was scored against 4 references total. The human scores ranged from 0.7172 to 0.7941. chins (1986) and Nagao (1989) give excellent overviews. In contrast, interlingua-based systems perform multi-phase analysis and synthesis, but reduce or eliminate the need for a transfer component (Uchida and Zhu, 1993; Carbonell et al., 1994; Mitamura et al., 1994). Both methods rely on extensive human knowledge engineering in all phases. CBMT is radically different. 5 Related Work Traditional Rule-Based MT systems are comprised of an analysis phase (typically a string-to-tree parser), a transfer phase (typically a rule-based tree-to-tree transformer), and a synthesis phase (typically a tree-to-linear-string generator). Hut26 Corpus-based systems, such as ExampleBased MT (Nagao, 1984; Sumita and Iida, 1991; Brown et al., 2003; Kim et al., 2005; Doi et al., 2005) and Statistical MT (Brown et al., 1990; Yama"
2006.amta-papers.3,C92-2117,0,0.0585535,"e that maximizes the product of the translation model (the first probability) and the target language model (the second probability). The decoder is the process that estimates Topt. The translation model is typically trained from a large (e.g., 100MB) sentence-aligned parallel corpus of professionally translated text. The target language model is typically trained from a much larger monolingual corpus. CBMT is also a corpus-based approach, closest to EBMT, but radically different in terms of requiring no parallel text whatsoever. In a sense, it is reminiscent of the old “Shake and Bake” idea (Whitelock, 1992), the newer EXERGE method in generation-heavy MT (Habash, 2003) and also the METIS work (Dirix et al., 2005) at a very abstract level of letting the target language establish lexical order. It bears some commonality to the work of Brown et al. (Brown et al., 2003) in that it permits combination of lattice entries with overlap (although the relevant CBMT components are covered in patent applications filed in 2001). However, it differs greatly from all previous systems in the maximal overlap principle for decoding and confirmation, in using consistently long ngrams, in near-synonym phrase substi"
2006.amta-papers.3,2003.mtsummit-papers.20,0,0.439516,"robability) and the target language model (the second probability). The decoder is the process that estimates Topt. The translation model is typically trained from a large (e.g., 100MB) sentence-aligned parallel corpus of professionally translated text. The target language model is typically trained from a much larger monolingual corpus. CBMT is also a corpus-based approach, closest to EBMT, but radically different in terms of requiring no parallel text whatsoever. In a sense, it is reminiscent of the old “Shake and Bake” idea (Whitelock, 1992), the newer EXERGE method in generation-heavy MT (Habash, 2003) and also the METIS work (Dirix et al., 2005) at a very abstract level of letting the target language establish lexical order. It bears some commonality to the work of Brown et al. (Brown et al., 2003) in that it permits combination of lattice entries with overlap (although the relevant CBMT components are covered in patent applications filed in 2001). However, it differs greatly from all previous systems in the maximal overlap principle for decoding and confirmation, in using consistently long ngrams, in near-synonym phrase substitutability, and in requiring no parallel text whatsoever to tra"
2006.amta-papers.3,P02-1039,0,0.0169273,"1993; Carbonell et al., 1994; Mitamura et al., 1994). Both methods rely on extensive human knowledge engineering in all phases. CBMT is radically different. 5 Related Work Traditional Rule-Based MT systems are comprised of an analysis phase (typically a string-to-tree parser), a transfer phase (typically a rule-based tree-to-tree transformer), and a synthesis phase (typically a tree-to-linear-string generator). Hut26 Corpus-based systems, such as ExampleBased MT (Nagao, 1984; Sumita and Iida, 1991; Brown et al., 2003; Kim et al., 2005; Doi et al., 2005) and Statistical MT (Brown et al., 1990; Yamada and Knight, 2002; Och, 2005) are comprised of a translation model and a target language model. The two components are readily evident, for instance, in the original IBM Candide system (Brown et al., 1990), where the central equation is: Example 1 Input: Un coche bomba estalla junto a una comisaría de policía en Bagdad  CBMT: a car bomb explodes next to a police station in baghdad  SYSTRAN: A car pump explodes next to a police station of police in Bagdad Example 2 Input: Hamas anunció este jueves el fin de su cese del fuego con Israel Topt = Arg max[P(S |Ti ) P(Ti )]  CBMT: hamas announced thursday the end"
2006.amta-papers.3,C86-1155,0,0.471386,"Missing"
2006.amta-papers.3,2005.eamt-1.21,1,0.837647,"but reduce or eliminate the need for a transfer component (Uchida and Zhu, 1993; Carbonell et al., 1994; Mitamura et al., 1994). Both methods rely on extensive human knowledge engineering in all phases. CBMT is radically different. 5 Related Work Traditional Rule-Based MT systems are comprised of an analysis phase (typically a string-to-tree parser), a transfer phase (typically a rule-based tree-to-tree transformer), and a synthesis phase (typically a tree-to-linear-string generator). Hut26 Corpus-based systems, such as ExampleBased MT (Nagao, 1984; Sumita and Iida, 1991; Brown et al., 2003; Kim et al., 2005; Doi et al., 2005) and Statistical MT (Brown et al., 1990; Yamada and Knight, 2002; Och, 2005) are comprised of a translation model and a target language model. The two components are readily evident, for instance, in the original IBM Candide system (Brown et al., 1990), where the central equation is: Example 1 Input: Un coche bomba estalla junto a una comisaría de policía en Bagdad  CBMT: a car bomb explodes next to a police station in baghdad  SYSTRAN: A car pump explodes next to a police station of police in Bagdad Example 2 Input: Hamas anunció este jueves el fin de su cese del fuego co"
2007.mtsummit-papers.25,2005.eamt-1.13,1,0.863818,"Missing"
2007.mtsummit-papers.25,font-llitjos-carbonell-2004-translation,1,0.890671,"Missing"
2007.mtsummit-papers.25,P03-1057,0,0.0184025,"repancies between intermediate representations of the source language and the target language side, namely an original MT system (Japanese to English) and a reverse MT system (English to Japanese) which was applied to the post-edited English translation. The grammar rules of our TBMT system integrate information from the three components of a typical transfer system: syntactic analysis (parsing), transfer and generation. Thus, in comparison with the PECOF system, blame assignment becomes highly simplified, since it is more directly inferable from corrections. Menezes and Richardson (2001) and Imamura et al. (2003) have proposed the use of reference translations to “clean” incorrect or redundant rules after automatic acquisition. The method of Imamura and colleagues consists of selecting or removing translation rules to increase the BLEU score of an evaluation corpus. In contrast to filtering out incorrect or redundant rules, we propose to actually refine the translation rules themselves, by editing valid but inaccurate rules that might be lacking a constraint, for example. 3 Error Correction Extraction The first step of the rule refinement process is the extraction of error correction information. Our"
2007.mtsummit-papers.25,2005.mtsummit-papers.11,0,0.00561019,"etween oracle scores, which means that the decoder can not fully leverage the improvements made in the grammar. This is also to be expected, since the decoder fails to select the best translation in most cases. 3 Translation rules in our MT system include parsing, transfer and generation information, which might otherwise be expressed with three different rules in other TBMT systems. 4 According to the standard paired two-tailed t-Test. Even though the language model (LM) for the BTEC data is rather small, using a larger LM with additional out-of-domain data from the Europarl training corpus (Koehn, 2005) did not improve these results. 5.5 Error Analysis After manual inspection, most of the differences between the Baseline and Refined systems were due to three of the 14 CIs processed by the Automatic Rule Refiner, namely 4, 5 and 7 in Figure 10, all of which yielded a BIFURCATE operation. In 56 cases, the additional generation capabilities of the refined system successfully produced a better translation than the baseline system; 37 of these improvements were ranked as 1-best by the decoder. Table 5 shows examples of the three most common types of fixes yielded by automatic refinements. 5.6 MER"
2007.mtsummit-papers.25,lavie-etal-2004-significance,1,0.818158,"Missing"
2007.mtsummit-papers.25,W01-1406,0,0.028629,"MT systems in order to detect discrepancies between intermediate representations of the source language and the target language side, namely an original MT system (Japanese to English) and a reverse MT system (English to Japanese) which was applied to the post-edited English translation. The grammar rules of our TBMT system integrate information from the three components of a typical transfer system: syntactic analysis (parsing), transfer and generation. Thus, in comparison with the PECOF system, blame assignment becomes highly simplified, since it is more directly inferable from corrections. Menezes and Richardson (2001) and Imamura et al. (2003) have proposed the use of reference translations to “clean” incorrect or redundant rules after automatic acquisition. The method of Imamura and colleagues consists of selecting or removing translation rules to increase the BLEU score of an evaluation corpus. In contrast to filtering out incorrect or redundant rules, we propose to actually refine the translation rules themselves, by editing valid but inaccurate rules that might be lacking a constraint, for example. 3 Error Correction Extraction The first step of the rule refinement process is the extraction of error co"
2007.mtsummit-papers.25,C88-2101,0,0.398343,"Missing"
2007.mtsummit-papers.25,P02-1040,0,0.0720235,"Missing"
2007.mtsummit-papers.25,takezawa-etal-2002-toward,0,0.0400664,"Missing"
2007.mtsummit-papers.25,2005.mtsummit-papers.33,0,0.0594552,"Missing"
2007.mtsummit-papers.32,J84-3009,0,0.701915,"Missing"
2009.mtsummit-posters.2,W05-0909,1,0.175736,"nce we are interested in studying the affect of the lexical coverage licensed by these different extraction scenarios, in all our experiments we run our decoder in a monotonic mode without any hierarchical models. We performed translation experiments using the experimental setup defined above and use StatResults The results are shown in Table 3. The overall problem of low coverage from using trees on both sides can be seen in the final translation quality too. Using syntax on one side produces syntax tables of larger coverage which also reflects in MT quality as judged both by BLEU and METEOR(Banerjee and Lavie, 2005) metrics. Our non-isomorphic tree restructuring technique that benefits from syntactic boundaries from both trees shows significant 2 improvement over the two other approaches. Symmetric rule induction that is a union of rules extracted by restructuring both trees has similar benefits from restructuring and also produces the largest possible syntactic phrase table. Although these results are slightly worse when compared to standard PB-SMT baseline (30.18 BLEU), it is to be noted that we are working with only syntactic phrase tables which are clean resources and are still relatively smaller in"
2009.mtsummit-posters.2,P05-1033,0,0.117643,"in (Lavie et al., 2008). In this paper, we are interested in the constraints introduced by the parse trees and study them in detail. Syntax trees from both sides introduce constraints on the possible segmentations. One variation to the above tree-tree model is to extract rules using target tree only. This can be seen as an instance of the framework, where CS , LS , PS = N U LL . A dependency tree can be seen as constituting a hidden unlabeled constituent tree. In such a case, the rule extraction can be used in a configuration where LT , LS = ‘X 0 , simulating a ’hiero’ style rule extraction (Chiang, 2005). As seen previously, variations with the choice of label sets LS , LT are also possible. For a language where only base-np chunkers and part-of-speech taggers are available, the framework can be instantiated as CT = {(i, j)} as the set of span boundaries from the chunker and LT = Lpos ∪ {0 X 0 } where Lpos come from the part-of-speech tagger and can be assigned to the preterminals, and ’X’ label to the non-terminals. We experiment with two configurations, using tree on one side vs. both sides. 3 Restructuring Parse Trees The parsers that generate syntactic analysis are built under varying ass"
2009.mtsummit-posters.2,P07-2045,0,0.0107491,"Missing"
2009.mtsummit-posters.2,N04-1035,0,0.128285,"ctic information that may come from either sides of the language pair. Our rule learning framework is general and works with syntax on both sides or any other annotation that is available, such as dependency structures, part-ofspeech tags etc. We achieve this by factorizing syntax into three essential components - structure, constituency and labeling. 2.1 Problem and Inputs A parallel corpus D is defined as a set of sentence pairs (FI1 , EJ1 , A), where A ⊆ {(i, j) : i = 1..I 0 , j = 1..J 0 } is the word alignment relation over each pair. We subscribe to the theory of alignments discussed in (Galley et al., 2004), that explains the concept of ’consistency of word-alignment’ for minimal rule extraction from a tree. Consistent alignment requires all the words in a particular segment of the source side to align with a particular contiguous segment of the target sentence, as decided by the word-level alignment. Formally, a span (is , js ) projects to a target span(it , jt ) if and only if ∀ks ∈ (is , js ), A(ks ) ∈ (it , jt ). We introduce syntax into this induction process by defining every node in the source tree as a tuple of three entities T ree(FI1 ) = {nsi : hcsi , lsi , psi i}, where csi ∈ CS is a"
2009.mtsummit-posters.2,W09-2301,1,0.907789,"B-SMT) techniques for extracting phrases although not syntactically motivated, enjoy a very high coverage. In order to bridge the gap some successful approaches to syntax in MT resort to binarization of trees(Wang et al., 2007) that systematically alter the structure of the source side parse tree to increase the space of segmentation allowed. This improves the recall of the syntactic translation models in particular the flat rules corresponding to syntactic phrasal entries. Another promising approach for bridging the coverage gap is combination of non-syntactic phrases with syntactic phrases (Hanneman and Lavie, 2009). Such techniques have shown that starting with large syntactic phrase tables and preferring syntactic phrases when overlapping with non-syntactic ones is beneficial for a syntactic MT system. They show improvements in decoding speeds and also improvement in translation quality that results from the precision of these syntax motivated phrases. The syntactic tables we produce in our work are precise and much high in coverage and can directly support these approaches. (Hanneman and Lavie, 2009) also show that a small set of manual synchronous grammar rules already benefit from clean syntactic ta"
2009.mtsummit-posters.2,N06-1031,0,0.0679123,"rmat below, where cs ∈ LS and ct ∈ LT represent syntactic categories and ws and wt are the word or phrase strings for the source- and targetsides correspondingly. The phrasal entries are collected from the entire corpus and scored by conditioning on the source side together with the label for assigning probabilities and form a syntactic phrase table. cs :: ct → [ws ] :: [wt ] Our alignment algorithm does not depend on the label sets LS or LT . Given that the original syntactic labels associated with the two parse trees are designed independent from each other, they may be sub-optimal for MT. (Huang and Knight, 2006) achieved improved translation quality by relabeling trees from which translation models were extracted. Decoupling the choice of labels from the alignment algorithm and delaying the assignment of labels until the output phase, enables our framework to experiment with various labeling strategies. In all our experiments, we retain the labels from both the sets. Exploring other possibilities for labeling, although interesting, is beyond the scope of this paper. 2.2.2 Hierarchical Rule Extraction: Synchronous Grammar Given two synchronous trees and their node alignment AN , we developed a tree tr"
2009.mtsummit-posters.2,W08-0411,1,0.946656,"ill in for the lack of lexical coverage in Syntax based Machine Translation approaches. 1 Introduction Recent approaches to Syntax based Machine Translation (MT) incorporate linguistic syntax for one side of the language pair, and obtain phrase tables and hierarchical translation rules. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments, which are usually extracted using syntactically uninformed generative models, are not optimal for the syntactic phrase extraction problem. Other approaches (Tinsley et al., 2007),(Lavie et al., 2008) have been proposed for using syntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce. Phrase based statistical MT (PB-SMT) techniques for extracting phrases although not syntactically motivated, enjoy a very high coverage. In order to bridge the gap some successful approaches to syntax in MT"
2009.mtsummit-posters.2,W06-1606,0,0.015637,"ned in the original parse trees. We also show that combining rules extracted by restructuring syntactic trees on both sides produces significantly better translation models. The improved precision and coverage of our syntax tables particularly fill in for the lack of lexical coverage in Syntax based Machine Translation approaches. 1 Introduction Recent approaches to Syntax based Machine Translation (MT) incorporate linguistic syntax for one side of the language pair, and obtain phrase tables and hierarchical translation rules. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments, which are usually extracted using syntactically uninformed generative models, are not optimal for the syntactic phrase extraction problem. Other approaches (Tinsley et al., 2007),(Lavie et al., 2008) have been proposed for using syntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of constituent segmentations t"
2009.mtsummit-posters.2,J03-1002,0,0.00719302,"Experimental Setup We build a French to English translation system using our decoding framework. We do not exploit the hierarchical nature of the decoder, as the translation models with which we would like to experiment are flat syntactic phrases. The parallel data we used to build our translation models is the Europarl data consisting of 1.3M translation sentence pairs. The English side of the corpus is parsed using the Stanford parser(Klein and Manning, 2002). The French side of the corpus was parsed by the Berkeley Parser. Word alignments for the parallel corpus were obtained using GIZA++ (Och and Ney, 2003) followed by a symmertrization technique called ‘sym2’ from the Thot toolkit (Ortiz-Mart´ınez et al., 2005). We used this technique as it was shown to provide good node alignment results across trees in both (Lavie et al., 2008) and (Tinsley et al., 2007). We then perform the extraction of the phrase pairs under all modes of rule learning - tree on one side, trees on both sides, restructuring tree and symmetric rule induction. Since we are interested in studying the affect of the lexical coverage licensed by these different extraction scenarios, in all our experiments we run our decoder in a m"
2009.mtsummit-posters.2,P03-1021,0,0.00656821,"ne side we lose grammar rules related to composing the verb ’am’ with the pronoun ’I’, but symmetrization fills the gap. The new generalized rule V N :: V N [P RP 1 AU X 2 ] → [CL1 V2 ] can now combine other verbs like ‘see’,‘think’ to form constituents ‘I see’, ‘I think’ etc, which were earlier not possible. 1 http://code.google.com/berkeleyparser XFER (Lavie, 2008) as the decoding framework. We built a suffix array language model (SALM) (Zhang and Vogel, 2006) over 430 million words including the English side of the parallel corpus. The weights on the features are tuned using standard MERT (Och, 2003) techniques over a 600-sentence dev set. The test set used was released by the WMT shared task 2007 and consists of 2000 sentences. When run without hierarchical syntax, this decoder is very similar to Moses decoder (et al, 2007). 5.2 Figure 3: Symmetric Rule Induction: Joint restructuring 5 5.1 Evaluation Experimental Setup We build a French to English translation system using our decoding framework. We do not exploit the hierarchical nature of the decoder, as the translation models with which we would like to experiment are flat syntactic phrases. The parallel data we used to build our trans"
2009.mtsummit-posters.2,2005.mtsummit-papers.19,0,0.0941244,"Missing"
2009.mtsummit-posters.2,2007.mtsummit-papers.62,0,0.0346035,"x tables particularly fill in for the lack of lexical coverage in Syntax based Machine Translation approaches. 1 Introduction Recent approaches to Syntax based Machine Translation (MT) incorporate linguistic syntax for one side of the language pair, and obtain phrase tables and hierarchical translation rules. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments, which are usually extracted using syntactically uninformed generative models, are not optimal for the syntactic phrase extraction problem. Other approaches (Tinsley et al., 2007),(Lavie et al., 2008) have been proposed for using syntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce. Phrase based statistical MT (PB-SMT) techniques for extracting phrases although not syntactically motivated, enjoy a very high coverage. In order to bridge the gap some successful approa"
2009.mtsummit-posters.2,D07-1078,0,0.0183577,"ntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce. Phrase based statistical MT (PB-SMT) techniques for extracting phrases although not syntactically motivated, enjoy a very high coverage. In order to bridge the gap some successful approaches to syntax in MT resort to binarization of trees(Wang et al., 2007) that systematically alter the structure of the source side parse tree to increase the space of segmentation allowed. This improves the recall of the syntactic translation models in particular the flat rules corresponding to syntactic phrasal entries. Another promising approach for bridging the coverage gap is combination of non-syntactic phrases with syntactic phrases (Hanneman and Lavie, 2009). Such techniques have shown that starting with large syntactic phrase tables and preferring syntactic phrases when overlapping with non-syntactic ones is beneficial for a syntactic MT system. They show"
2009.mtsummit-posters.2,P01-1067,0,0.0602187,"onstituents that were aligned in the original parse trees. We also show that combining rules extracted by restructuring syntactic trees on both sides produces significantly better translation models. The improved precision and coverage of our syntax tables particularly fill in for the lack of lexical coverage in Syntax based Machine Translation approaches. 1 Introduction Recent approaches to Syntax based Machine Translation (MT) incorporate linguistic syntax for one side of the language pair, and obtain phrase tables and hierarchical translation rules. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments, which are usually extracted using syntactically uninformed generative models, are not optimal for the syntactic phrase extraction problem. Other approaches (Tinsley et al., 2007),(Lavie et al., 2008) have been proposed for using syntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of consti"
2010.eamt-1.27,J93-2003,0,0.0213917,"nglish translation tasks. 1 Introduction In general, corpus-based machine translation systems prefer longer units because they naturally convey local context and local reordering. This was achieved by phrases in Phrase-Based Statistical Machine Translation (Koehn et al., 2007; Vogel et al., 2003) and surface form matches in lexical Example-Based Machine Translation (Brown, 2005; Veale and Way, 1997). These systems use phrasal alignment to find translations of matched n-grams for an input sentence. However, because the alignment algorithms used in these systems purely depend on word alignment (Brown et al., 1993) they cannot address structural translations, other than hoping for structural parallelism between source and target. For example, these algorithms cannot reliably find ’an/the office’ as a translation of ’sa-moo-sil’ in c 2010 European Association for Machine Translation. Korean to English translation because Korean does not have articles. For this reason, we investigated the chunk as our translation unit. The chunk was pioneered by Abney (1991). It is a continuous and non-recursive syntactic segment around a head, comparable to a morphologically complex word in synthetic languages, and is no"
2010.eamt-1.27,2005.mtsummit-ebmt.2,1,0.884299,"nsertion or deletion words between two distant languages. In this work, we used syntactic chunks as translation units to alleviate this problem, improve alignments and show improvement in BLEU for Korean to English and Chinese to English translation tasks. 1 Introduction In general, corpus-based machine translation systems prefer longer units because they naturally convey local context and local reordering. This was achieved by phrases in Phrase-Based Statistical Machine Translation (Koehn et al., 2007; Vogel et al., 2003) and surface form matches in lexical Example-Based Machine Translation (Brown, 2005; Veale and Way, 1997). These systems use phrasal alignment to find translations of matched n-grams for an input sentence. However, because the alignment algorithms used in these systems purely depend on word alignment (Brown et al., 1993) they cannot address structural translations, other than hoping for structural parallelism between source and target. For example, these algorithms cannot reliably find ’an/the office’ as a translation of ’sa-moo-sil’ in c 2010 European Association for Machine Translation. Korean to English translation because Korean does not have articles. For this reason, w"
2010.eamt-1.27,Y04-1013,0,0.0729874,"section 2, our approaches for chunk alignment and translation in section 3. In section 4, we describe our experiments, and we discuss our conclusions in section 5. Koehn and Knight (2002) decomposed the translation model into sentence level reordering (SLR), chunk mapping (CM) and word translations (W): p(f |e) = p(SLR|e) × 2 Related Work Some researchers have studied exploiting chunks in translation. Le et al. (2000) used chunk alignment to get better word alignment. Given a human dictionary and chunked English sentences, they got corresponding Chinese sentences chunked via chunk projection. Hwang et al. (2004) used chunk alignment to extract Korean dependency parse trees given Japanese dependency parse trees and a human dictionary. They first aligned words by consulting a Japanese-Korean dictionary to find chunk boundaries and alignment and then they aligned the remaining words. They finally extracted bilingual knowledge from the aligned chunk pairs. Zhou et al. (2004) extract chunk pairs automatically to use in an SMT system. Their chunk detection is based on the assumption that the most frequently co-occurring word sequence may be a potential chunk. After aligning chunks using their cooccurrence"
2010.eamt-1.27,2005.eamt-1.21,1,0.828115,"face form matching over the source half of the training set. Then it finds their translations using a phrasal aligner, and combines them to form hypothesis translations. The aligner provides several feature values to a standard beam decoder. 2 Next, the EBMT system collects some 1 The Korean sentence is missing a subject. And there is an error on the Korean sentence chunking. [order to] and [related/about] should be merged into one chunk. However this error was overcome by consistent chunk translation sequence pair extraction. 2 The aligner’s feature scores include uni-directional SPA scores (Kim et al., 2005), number of untranslated words on both source and target sides and so on. Figure 1: Chunk Translation Sequence Pair Extraction Lang. Kr-En Cn-En Data Dev Unseen Dev Unseen Moses 0.2222 0.2289 0.2610 0.2533 EBMT 0.2382 0.2502 0.2538 0.2295 Table 1: Baseline System vs. Moses more feature values outside the aligner to be used in decoding. Finally, the translation with the highest score is chosen as the best hypothesis translation. The score is calculated as a combination of feature values with their weights tuned in a separate tuning process in a log linear model. The performance of this EBMT sys"
2010.eamt-1.27,W00-1314,0,0.0680159,"or the Romance languages. In this paper, we show that we obtain significant improvements using chunks for translation in both Korean to English and Chinese to English. We discuss related work in section 2, our approaches for chunk alignment and translation in section 3. In section 4, we describe our experiments, and we discuss our conclusions in section 5. Koehn and Knight (2002) decomposed the translation model into sentence level reordering (SLR), chunk mapping (CM) and word translations (W): p(f |e) = p(SLR|e) × 2 Related Work Some researchers have studied exploiting chunks in translation. Le et al. (2000) used chunk alignment to get better word alignment. Given a human dictionary and chunked English sentences, they got corresponding Chinese sentences chunked via chunk projection. Hwang et al. (2004) used chunk alignment to extract Korean dependency parse trees given Japanese dependency parse trees and a human dictionary. They first aligned words by consulting a Japanese-Korean dictionary to find chunk boundaries and alignment and then they aligned the remaining words. They finally extracted bilingual knowledge from the aligned chunk pairs. Zhou et al. (2004) extract chunk pairs automatically t"
2010.eamt-1.27,2007.tmi-papers.14,0,0.0168459,"ependency parse trees and a human dictionary. They first aligned words by consulting a Japanese-Korean dictionary to find chunk boundaries and alignment and then they aligned the remaining words. They finally extracted bilingual knowledge from the aligned chunk pairs. Zhou et al. (2004) extract chunk pairs automatically to use in an SMT system. Their chunk detection is based on the assumption that the most frequently co-occurring word sequence may be a potential chunk. After aligning chunks using their cooccurrence similarity, they extracted chunk pairs and reported a significant improvement. Ma et al. (2007) studied an adaptable monolingual chunking approach. They learned word alignment on a parallel corpus and used this alignment information to find chunk boundaries in both languages. Wu (1997) studied the inversion transduction grammar (ITG) formalism for bilingual parsing for a parallel corpus. In this parse tree pair, the method naturally provides bilingual bracketing and alignment to extract aligned chunk pairs. However, it remains difficult to write a broad bilingual ITG grammar to deal with long sentences. Watanabe et al. (2003) built a chunk-based statistical translation system. They deco"
2010.eamt-1.27,2003.mtsummit-papers.51,0,0.0409428,"mostly likely to have benefits by Dev 82.58 94.53 74.11 86.23 69.80 79.08 Table 4: Training Set Coverage for Korean Table 3: Test sets for Korean-English translation evaluation metric. type token type token type token translating chunks which are longer than one word by properly dealing with word deletion/insertion as explained. For Chinese-English translation, we used 340,000 sentence pairs for training and 230 sentences with 4 references for parameter tuning. To measure performance on an unseen data set, we used 919 sentences with 4 references from NIST Machine Translation Evaluation 2003 (NIST, 2003). The Chinese training data was drawn from the FBIS Chinese-English parallel text by (NIST, 2003). We used sentence pairs with 70 or fewer words in the source side. On average, the Chinese sentences are 26.8 words long and 18.1 chunks long and chunks are composed of 1.5 words. And the English sentences are 33.9 words long and 18 chunks long and chunks are 1.8 words long. Table 5 shows Chinese to English training data. Both Korean-English and Chinese-English language pairs shows chunk level correspondence is higher than word level correspondence. Table 6 shows Chinese-English test sets for tran"
2010.eamt-1.27,J03-1002,0,0.0260674,"vel template first and then use chunk mapping and word translation to generate a target translation. Whereas the method is good, each step could be prone to error, and errors could compound. Our system relies on chunks as the basic unit when it can find evidence of good chunk level translations, but otherwise it falls back to a word/phrase-based model. 3 ciently large corpus for chunk alignment. But in reality, it is hard to build such a large corpus for many languages. Instead, we investigated a new method that induces chunk alignment from word alignment. For chunk alignment, we used GIZA++ (Och and Ney, 2003) that works on a chunk annotated corpus. We created a chunk unit which will be used as a basic unit in GIZA++ by concatenating all the words in the chunk placing a special delimiter character between adjacent words. The GIZA++ was modified to use word alignment information in chunk alignment in this way: • Let f and e be chunks and f be f1n = f1 f2 ...fn and e be em 1 = e1 e2 ...em . • T (f |e) in IBM models is C(f , e) k C(fk , e) (2) C ′ (f , e) ′ k C (fk , e) (3) T (f |e) = P • We redefine it as, T (f |e) = P Chunk-Based System where 3.1 Chunk Detection C ′ (f , e) = C(f , e) × F (f , e) In"
2010.eamt-1.27,P02-1040,0,0.0791975,"Missing"
2010.eamt-1.27,C02-1092,0,0.0415757,"Missing"
2010.eamt-1.27,2006.iwslt-evaluation.4,0,0.0147995,"ge parallel corpus in which we can find reasonable statistical evidence to align chunks. However, our approach allows a relatively smaller corpus by boosting chunk alignment with word alignment information, making it practical for low and medium resource conditions. Last, in decoding, our method combines target chunks as well as target fragments which are not chunks. Unlike the lexical EBMT system by Brown (2005), this chunk-based system is a hybrid system that combines a typical string-based EBMT system and a chunk-based EBMT system. It is close to the EBMT system by Veale and Way (1997) and Stroppa and Way (2006) in that it uses constituent-like units but different in that the work is fully extended to include chunks at all levels. Our work also differs from the ChunkMT work (Koehn and Knight, 2002), in which the translation was decomposed into sentence label chunk reordering, chunk mapping and word translation. When an input is given, they build a sentence level template first and then use chunk mapping and word translation to generate a target translation. Whereas the method is good, each step could be prone to error, and errors could compound. Our system relies on chunks as the basic unit when it c"
2010.eamt-1.27,2003.mtsummit-papers.53,0,0.0240114,"training corpus. However, they still cannot properly deal with systematic translation for insertion or deletion words between two distant languages. In this work, we used syntactic chunks as translation units to alleviate this problem, improve alignments and show improvement in BLEU for Korean to English and Chinese to English translation tasks. 1 Introduction In general, corpus-based machine translation systems prefer longer units because they naturally convey local context and local reordering. This was achieved by phrases in Phrase-Based Statistical Machine Translation (Koehn et al., 2007; Vogel et al., 2003) and surface form matches in lexical Example-Based Machine Translation (Brown, 2005; Veale and Way, 1997). These systems use phrasal alignment to find translations of matched n-grams for an input sentence. However, because the alignment algorithms used in these systems purely depend on word alignment (Brown et al., 1993) they cannot address structural translations, other than hoping for structural parallelism between source and target. For example, these algorithms cannot reliably find ’an/the office’ as a translation of ’sa-moo-sil’ in c 2010 European Association for Machine Translation. Kore"
2010.eamt-1.27,P03-1039,0,0.668756,"they extracted chunk pairs and reported a significant improvement. Ma et al. (2007) studied an adaptable monolingual chunking approach. They learned word alignment on a parallel corpus and used this alignment information to find chunk boundaries in both languages. Wu (1997) studied the inversion transduction grammar (ITG) formalism for bilingual parsing for a parallel corpus. In this parse tree pair, the method naturally provides bilingual bracketing and alignment to extract aligned chunk pairs. However, it remains difficult to write a broad bilingual ITG grammar to deal with long sentences. Watanabe et al. (2003) built a chunk-based statistical translation system. They decomposed the P translation model P (J|E) = P (J, A|E) P P A P (J, J , E|E) to P (J|E) = J E where J and E are the chunked sentences for J and E respectively. Then they decomposed P (J, J , E|E) further to P P P (J, J , E|E) = A A P (J, J , A, A, E|E) where A is chunk alignment and A is word alignment for each chunk translation. Y p(CMi |e, SLR) i × Y (Wij |CMi , SLR, e) (1) j Sentence level chunk reordering defines how source and target chunks are connected and chunk mapping defines an alignment of source to target POSs. Finally word"
2010.eamt-1.27,J97-3002,0,0.641506,"ey finally extracted bilingual knowledge from the aligned chunk pairs. Zhou et al. (2004) extract chunk pairs automatically to use in an SMT system. Their chunk detection is based on the assumption that the most frequently co-occurring word sequence may be a potential chunk. After aligning chunks using their cooccurrence similarity, they extracted chunk pairs and reported a significant improvement. Ma et al. (2007) studied an adaptable monolingual chunking approach. They learned word alignment on a parallel corpus and used this alignment information to find chunk boundaries in both languages. Wu (1997) studied the inversion transduction grammar (ITG) formalism for bilingual parsing for a parallel corpus. In this parse tree pair, the method naturally provides bilingual bracketing and alignment to extract aligned chunk pairs. However, it remains difficult to write a broad bilingual ITG grammar to deal with long sentences. Watanabe et al. (2003) built a chunk-based statistical translation system. They decomposed the P translation model P (J|E) = P (J, A|E) P P A P (J, J , E|E) to P (J|E) = J E where J and E are the chunked sentences for J and E respectively. Then they decomposed P (J, J , E|E)"
2010.eamt-1.27,koen-2004-pharaoh,0,\N,Missing
2010.eamt-1.27,P07-2045,0,\N,Missing
2010.eamt-1.40,2003.mtsummit-papers.4,1,0.745755,"Missing"
2010.eamt-1.40,C00-1019,1,0.620774,"es is observed, even above the empirically found N . 1 Introduction An EBMT system uses a parallel corpus to translate new input source sentences. In the Translation Model (TM), the input sentence to be translated is matched against the source sentences. When a match is found, the corresponding translation in the target language is obtained through subsentential alignment. In our EBMT system, the final target translation is obtained from these partial target translations with a beam-search decoder using a target Language Model (LM). EBMT systems require large amounts of data to function well (Brown, 2000). c 2010 European Association for Machine Translation. Generalization using equivalence classes (Veale and Way, 1997; Brown, 2000) reduces the amount of pre-translated text required and improves translation quality. Translation templates (or short reusable sequences) are generalizations of source and target sentences where sequences of one or more words are replaced by variables. Various methods have been proposed to create such templates in EBMT and differ in the way the templates are created. Some systems required a full template to match the input source sentence for target sentence generat"
2010.eamt-1.40,N06-2011,1,0.896727,"Missing"
2010.eamt-1.40,koen-2004-pharaoh,0,0.0771191,"Missing"
2011.mtsummit-papers.12,ambati-etal-2010-active,1,0.722407,"re-training and re-tuning an SMT system after translating every single sentence is computationally inefﬁcient and may not have a signiﬁcant effect on the underlying models. We, therefore continue to select a batch of N sentences before retraining the system on newly created labeled set Lk=1 . Our framework for active learning in SMT is discussed in Algorithm 1. 3.2 P hrases(s) d(s) = u(s) = 124 P (x|U ) ∗ e−λcount(x|L) |P hrases(S)| P hrases(s) Sentence Selection Our sentence selection strategy to be independent of the underlying SMT system or the models and has been shown to perform well (Ambati et al., 2010). For the sake of comprehensiveness we discuss the approach here as well. We use only monolingual data U and bilingual corpus L to select sentences. This makes our approach applicable to any corpusbased MT paradigm and system, even though we test with SMT. The basic units of an SMT system are x α |P hrases(s)| x α = Score(s) = 4 1 x∈ / P hrases(L) 0 (1 + β 2 )d(s) ∗ u(s) β 2 d(s) + u(s) DUAL Strategy Let us consider the DWDS approach in more detail. It has two components for scoring a sentence Figure 1: Density vs Diversity performance curves S, a density component d(s) and a diversity compone"
2011.mtsummit-papers.12,2005.mtsummit-papers.30,1,0.845432,"requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009) use a pool-based strategy that maximizes a measure of expected future improvement, to sample instances from a large parallel corpus. Their goal is to select the most informative sentence pairs to build an MT system, and hence they assume the existence of target-side translations along with the source-side sentences. We however are interested in selecting most informative sentences to reduce the effort and cost involved in translation. (Eck et al., 2005) use a weighting scheme to select more informative sentences, wherein the importance is estimated using unseen n-grams in previously selected sentences. Although our selection strategy has a density based motivation similar to theirs, we augment this by adding a diminishing effect to discourage the domination of density and favor unseen n-grams. Our approach, therefore, naturally works well in pool-based active learning strategy when compared to (Eck et al., 2005). In case of instancebased active learning, both approaches work comparably, with our approach working slightly better. Ensemble app"
2011.mtsummit-papers.12,W09-4633,1,0.857303,"004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009) use a pool-based strategy that maximizes a measure of expected future improvement, to sample instances from a large parallel corpus. Their goal is to select the most informative sentence pairs to build an MT system, and hence they assume the existence of target-side translations along with the source-side sentences. We however are interested in selecting most informative sentences to reduce the effort and cost involved in translation. (Eck et al., 2005) use a weighting scheme to select more informative sentences, wherein the importance is estimated using unseen n-grams in previously selected"
2011.mtsummit-papers.12,N09-1047,0,0.23323,"on 2. 123 2 Related Work Active learning has been applied to various ﬁelds of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009) use a pool-based strategy that maximizes a measure of expected future improvement, to sample instances from a large parallel corpus. Their goal is to select the most informative sentence pairs to build an MT system, and hence they assume the existence of target-side translations along with the source-side sentences. We however are interested in selecting most informative sentences to reduce the effort and cost involved"
2011.mtsummit-papers.12,J04-3001,0,0.0205627,"ction 3 we present our framework for active learning in SMT and discuss our sentence selection algorithm. Section 4 describes DUAL, a multi-strategy approach that focuses on switching between two strategies. Section 5 discusses GraDUAL, another hybrid approach that addresses some of the issues with DUAL. Section 6 presents experiments and results on Spanish-English language pair. We conclude with discussion of related work in Section 2. 123 2 Related Work Active learning has been applied to various ﬁelds of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gan"
2011.mtsummit-papers.12,P07-2045,0,0.00472538,"ting with other functions for f (β). β = Abs(Δ(DW DS) − Δ(DIV )) α = ⎧ ⎪ ⎨ 1 β>δ 0 β<δ ⎪ ⎩ f (β) Score(s) = αDW DS(s) + (1 − α)DIV (s) Figure 3: Spanish-English results:Single-Strategy 6 6.1 Experiments Setup We perform our experiments on the Spanish-English language pair in order to simulate a resource-poor language pair. We have parallel corpora and evaluation data sets for the Spanish-English language pair allowing us to run multiple experiments efﬁciently. We use BTEC parallel corpus (Takezawa et al., 2002) from the IWSLT tasks with 127K sentence pairs. We use the standard Moses pipeline (Koehn et al., 2007) for extraction, training and tuning our system. We built an SRILM language model using English data consisting of 1.6M words. While experimenting with data sets of varying size, we do not vary the language model. The weights of the different translation features are tuned using standard MERT (Och, 2003). Our development set consists of 506 sentences and test set consists of 343 sentences. We report results on the test set. 6.2 Results: AL for MT We ﬁrst test the performance of our active learning sentence selection strategy. We start with an initial system trained on 1000 sentence pairs. We t"
2011.mtsummit-papers.12,P03-1021,0,0.0278712,"uage pair. We have parallel corpora and evaluation data sets for the Spanish-English language pair allowing us to run multiple experiments efﬁciently. We use BTEC parallel corpus (Takezawa et al., 2002) from the IWSLT tasks with 127K sentence pairs. We use the standard Moses pipeline (Koehn et al., 2007) for extraction, training and tuning our system. We built an SRILM language model using English data consisting of 1.6M words. While experimenting with data sets of varying size, we do not vary the language model. The weights of the different translation features are tuned using standard MERT (Och, 2003). Our development set consists of 506 sentences and test set consists of 343 sentences. We report results on the test set. 6.2 Results: AL for MT We ﬁrst test the performance of our active learning sentence selection strategy. We start with an initial system trained on 1000 sentence pairs. We then train the system iteratively on datasets of increasing size. In each iteration, we ﬁrst selectively sample 1000 Spanish sentences from source side of the entire corpus. We simulate human translation in our experiment, as we already have access to the translations from the BTEC corpus. We then retrain"
2011.mtsummit-papers.12,P04-1075,0,0.0204503,"or active learning in SMT and discuss our sentence selection algorithm. Section 4 describes DUAL, a multi-strategy approach that focuses on switching between two strategies. Section 5 discusses GraDUAL, another hybrid approach that addresses some of the issues with DUAL. Section 6 presents experiments and results on Spanish-English language pair. We conclude with discussion of related work in Section 2. 123 2 Related Work Active learning has been applied to various ﬁelds of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009) use a pool-based s"
2011.mtsummit-papers.12,N03-1031,0,0.0377548,"present our framework for active learning in SMT and discuss our sentence selection algorithm. Section 4 describes DUAL, a multi-strategy approach that focuses on switching between two strategies. Section 5 discusses GraDUAL, another hybrid approach that addresses some of the issues with DUAL. Section 6 presents experiments and results on Spanish-English language pair. We conclude with discussion of related work in Section 2. 123 2 Related Work Active learning has been applied to various ﬁelds of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009"
2011.mtsummit-papers.12,takezawa-etal-2002-toward,0,0.0636683,"Missing"
2020.acl-main.722,W19-2804,0,0.0789312,"Missing"
2020.acl-main.722,I17-2016,0,0.0180834,"res to train strong NER models diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have been applied to low-resource settings (Cotterell and Duh, 2017; Huang et al., 2019), directly integrating gazetteer features into these models is difficult because gazetteers in these languages are either limited in coverage or completely absent. Expanding them is time-consuming and expensive, due to the lack of available annotators for low-resource languages (Strassel and Tracey, 2016). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to"
2020.acl-main.722,N19-1383,0,0.01658,"odels diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have been applied to low-resource settings (Cotterell and Duh, 2017; Huang et al., 2019), directly integrating gazetteer features into these models is difficult because gazetteers in these languages are either limited in coverage or completely absent. Expanding them is time-consuming and expensive, due to the lack of available annotators for low-resource languages (Strassel and Tracey, 2016). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to extract information"
2020.acl-main.722,N16-1030,0,0.0741771,"es ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.1 1 Introduction Before the widespread adoption of neural networks for natural language processing tasks, named entity recognition (NER) systems used linguistic features based on lexical and syntactic knowledge to improve performance (Ratinov and Roth, 2009). With the introduction of the neural LSTM-CRF model (Huang et al., 2015; Lample et al., 2016), the need to develop hand-crafted features to train strong NER models diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have"
2020.acl-main.722,W09-1119,0,0.845134,"hese languages. To address this problem, we propose a method of “soft gazetteers” that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.1 1 Introduction Before the widespread adoption of neural networks for natural language processing tasks, named entity recognition (NER) systems used linguistic features based on lexical and syntactic knowledge to improve performance (Ratinov and Roth, 2009). With the introduction of the neural LSTM-CRF model (Huang et al., 2015; Lample et al., 2016), the need to develop hand-crafted features to train strong NER models diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and dat"
2020.acl-main.722,L16-1521,0,0.041183,"r are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have been applied to low-resource settings (Cotterell and Duh, 2017; Huang et al., 2019), directly integrating gazetteer features into these models is difficult because gazetteers in these languages are either limited in coverage or completely absent. Expanding them is time-consuming and expensive, due to the lack of available annotators for low-resource languages (Strassel and Tracey, 2016). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to extract information from these resources and integrate it into the commonlyused CNN-LSTM-CRF NER model (Ma and Hovy, 2016) using a carefully designed feature set. We use entity linking methods designed for low-resource languages, which require far fewer resources than traditional gazetteer features (Upadhyay et al., 2018; Zh"
2020.acl-main.722,D18-1270,0,0.0683232,"Missing"
2020.acl-main.722,D16-1157,0,0.0175096,"the size of the CoNLL-2003 English dataset. These sentences are also annotated with entity links to a knowledge base of 11 million entries, which we use only to aid our analysis. Of particular interest are “NIL” entity mentions that do not have a corresponding entry in the knowledge base (Blissett and Ji, 2019). The fraction of mentions that are NIL is shown in Table 1. bilingual Wikipedia links are used to retrieve the appropriate English KB candidates. • Pivot-based-entity-linking (Zhou et al., 2020): This method encodes entity mentions on the character level using n-gram neural embeddings (Wieting et al., 2016) and computes their similarity with KB entries. We experiment with two variants and follow Zhou et al. (2020) for hyperparameter selection: 1) P BEL S UPERVISED: trained on the small number of bilingual Wikipedia links available in the target low-resource language. 2) P BEL Z ERO: trained on some high-resource language (“the pivot”) and transferred to the target language in a zero-shot manner. The transfer languages we use are Swahili for Kinyarwanda, Indonesian for Oromo, Hindi for Sinhala, and Amharic for Tigrinya. Gazetteer Data We also compare our method with binary gazetteer features, usi"
2020.acl-main.722,P16-1101,0,0.706125,"etteers in these languages are either limited in coverage or completely absent. Expanding them is time-consuming and expensive, due to the lack of available annotators for low-resource languages (Strassel and Tracey, 2016). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to extract information from these resources and integrate it into the commonlyused CNN-LSTM-CRF NER model (Ma and Hovy, 2016) using a carefully designed feature set. We use entity linking methods designed for low-resource languages, which require far fewer resources than traditional gazetteer features (Upadhyay et al., 2018; Zhou et al., 2020). Our experiments demonstrate the effectiveness of our proposed soft gazetteer features, with an average improvement of 4 F1 points over the baseline, across four low-resource languages: Kinyarwanda, Oromo, Sinhala, and Tigrinya. 2 Background Named Entity Recognition NER identifies named entity spans in an input sentence, and classifies them into predefined types (e.g., locatio"
2020.acl-main.722,D18-1310,0,0.21267,"recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.1 1 Introduction Before the widespread adoption of neural networks for natural language processing tasks, named entity recognition (NER) systems used linguistic features based on lexical and syntactic knowledge to improve performance (Ratinov and Roth, 2009). With the introduction of the neural LSTM-CRF model (Huang et al., 2015; Lample et al., 2016), the need to develop hand-crafted features to train strong NER models diminished. However, Wu et al. (2018) have recently demonstrated that integrating linguistic features based on part-of-speech tags, word shapes, and manually created lists of entities called gazetteers into neural models leads to better NER on English data. Of particular interest to this paper are the gazetteer-based features – binary-valued features determined by whether or not an entity is present in the gazetteer. 1 Code and data are available at https://github. com/neulab/soft-gazetteers. Although neural NER models have been applied to low-resource settings (Cotterell and Duh, 2017; Huang et al., 2019), directly integrating g"
2020.acl-main.722,2020.tacl-1.8,1,0.836325,"6). As an alternative, we introduce “soft gazetteers”, a method to create continuous-valued gazetteer features based on readily available data from highresource languages and large English knowledge bases (e.g., Wikipedia). More specifically, we use entity linking methods to extract information from these resources and integrate it into the commonlyused CNN-LSTM-CRF NER model (Ma and Hovy, 2016) using a carefully designed feature set. We use entity linking methods designed for low-resource languages, which require far fewer resources than traditional gazetteer features (Upadhyay et al., 2018; Zhou et al., 2020). Our experiments demonstrate the effectiveness of our proposed soft gazetteer features, with an average improvement of 4 F1 points over the baseline, across four low-resource languages: Kinyarwanda, Oromo, Sinhala, and Tigrinya. 2 Background Named Entity Recognition NER identifies named entity spans in an input sentence, and classifies them into predefined types (e.g., location, person, organization). A commonly used method for doing so is the BIO tagging scheme, representing the Beginning, the Inside and the Outside of a text segment (Ratinov and Roth, 2009). The first word of a named entity"
2020.acl-main.722,W03-0419,0,\N,Missing
2020.emnlp-main.39,D16-1264,0,0.0300389,"rom (Zhang et al., 2015) spanning four text classification tasks: (1) news classification (AGNews), (2) sentiment analysis (Yelp, Amazon), (3) Wikipedia article classification (DBPedia) and (4) questions and answers categorization (Yahoo). To compare our framework with (d’Autume et al., 2019), we follow the same data processing procedure as described by them to produce balanced datasets. In total, we have 33 classes, 575, 000 training examples and 38, 000 test examples from all datasets. Question Answering Following (d’Autume et al., 2019), we use three question answering datasets: SQuAD v1.1(Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and QuAC (Choi et al., 2018). TriviaQA has two sections, Web and Wikipedia, which we consider as separate datasets. We process the datasets to follow the same setup as (d’Autume et al., 2019). Our processed datasets includes 60, 000-90, 000 training and 7, 000-10, 000 validation examples per task. 4.2 Experimental Setup We consider the prominent baselines corresponding to each one of the three principles as introduced in Section 2. We first consider a standard encoder-decoder model (Enc-Dec) which does not utilize any lifelong learning regularization. In the spi"
2020.tacl-1.8,P18-1073,0,0.040963,"Missing"
2020.tacl-1.8,W19-2804,0,0.221558,"Missing"
2020.tacl-1.8,E06-1002,0,0.0799483,"Missing"
2020.tacl-1.8,D18-1024,0,0.0613179,"Missing"
2020.tacl-1.8,D11-1072,0,0.317046,"Missing"
2020.tacl-1.8,D17-1078,0,0.032933,"Missing"
2020.tacl-1.8,D07-1074,0,0.307549,"Missing"
2020.tacl-1.8,P04-1021,0,0.305166,"Missing"
2020.tacl-1.8,C10-1032,0,0.17316,"Missing"
2020.tacl-1.8,D17-1277,0,0.0432194,"erformance of candidate generation is measured by the gold candidate recall, which is the proportion of retrieved candidate lists that contains the correct entity. It is critical that this number is high, as any time the correct entity is excluded, the disambiguation model will be unable to recover it. Formally, if we denote the correct entity of each mention m as eˆ, the gold candidate recall r is defined as: r= PN ei i=1 δ (ˆ ∈ ei ) N where δ(·) is the indicator function, which is 1 if true else 0, and N is the total number of mentions among all documents. We follow Yamada et al. (2017) and Ganea and Hofmann (2017) to ignore 111 mentions whose linked entity does not exist in the KB in this work.3 We use ‘‘EN’’ to denote the target language English, ‘‘HRL’’ to denote any high-resource language and ‘‘LRL’’ to denote any low-resource language. For example, KHRL is a KB in an HRL (e.g., Spanish Wikipedia), eHRL is an entity in KHRL . Because our focus is on low-resource XEL, the source language is always an LRL. We also refer to the HRL as the ‘‘pivoting’’ language below. 2.2 Baseline Candidate Generation Models In this section, we introduce two existing categories of techniques for candidate generation. Di"
2020.tacl-1.8,P16-1059,0,0.0488408,"Missing"
2020.tacl-1.8,P08-1088,0,0.0263698,"Missing"
2020.tacl-1.8,I11-1029,0,0.243793,"Missing"
2020.tacl-1.8,D19-1680,0,0.0329934,"Missing"
2020.tacl-1.8,L18-1429,0,0.0619928,"pout is set to 0.5.9 For each training language, we set aside a small subset of training data (mHRL − eEN ) as our development set. For all models, we stop training if top-30 gold candidate recall on the development set does not increase for 50 epochs, and the maximum number of training epochs is set to 200. We select the HRL that has the highest character n-gram overlap with the source LRL, a decision we discuss more in §5.4. Rijhwani et al. (2019) used phoneme-based representations to help deal with the fact that different languages use different scripts, and we do so as well using Epitran (Mortensen et al., 2018) to convert strings to international phonetic alphabet (IPA) symbols. The selection of the HRL and the representation of each LRL is shown in Table 2. Epitran has relatively wide and growing coverage (55 languages at the time of this writing). Our method could also potentially be used with other tools such as the Romanizer uroman,10 which is a less accurate phonetic representation than Epitran but covers most languages in the world. However, testing different romanizers is somewhat orthogonal to the main claims of this paper, and thus we have not explicitly performed experiments on this. Our H"
2020.tacl-1.8,P17-1178,0,0.0739865,"Missing"
2020.tacl-1.8,L16-1521,0,0.199314,"Missing"
2020.tacl-1.8,N18-1167,0,0.0495691,"Missing"
2020.tacl-1.8,N12-1052,0,0.103568,"Missing"
2020.tacl-1.8,P19-1015,0,0.0611527,"Missing"
2020.tacl-1.8,N16-1072,0,0.285922,"Missing"
2020.tacl-1.8,D18-1270,0,0.689637,"n batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. et al., 2018). String similarity (e.g., edit distance) cannot easily extend to XEL because surface forms of entities often differ significantly across the source and target language, particularly when the languages are in different scripts. Wikipedia link methods can be extended to XEL by using inter-language links between the two languages to redirect entities to the English KB (Spitkovsky and Chang, 2012; Sil and Florian, 2016; Sil et al., 2018; Upadhyay et al., 2018a). This method works to some extent, but often under-performs on lowresource languages due to the lack of source language Wikipedia resources. Although scarce, there are some methods that propose to improve entity candidate generation by training translation models with low resourcelanguage (LRL)-English entity gazetteers (Pan et al., 2017), or learning neural string matching models based on an entity gazetteer in a related high-resource language (HRL) which is then applied to the LRL (Rijhwani et al., 2019) (more in §2). However, even with these relatively sophisticated methods, top-30 candi"
2020.tacl-1.8,P16-1213,0,0.384956,"03 Action Editor: Radu Florian. Submission batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. et al., 2018). String similarity (e.g., edit distance) cannot easily extend to XEL because surface forms of entities often differ significantly across the source and target language, particularly when the languages are in different scripts. Wikipedia link methods can be extended to XEL by using inter-language links between the two languages to redirect entities to the English KB (Spitkovsky and Chang, 2012; Sil and Florian, 2016; Sil et al., 2018; Upadhyay et al., 2018a). This method works to some extent, but often under-performs on lowresource languages due to the lack of source language Wikipedia resources. Although scarce, there are some methods that propose to improve entity candidate generation by training translation models with low resourcelanguage (LRL)-English entity gazetteers (Pan et al., 2017), or learning neural string matching models based on an entity gazetteer in a related high-resource language (HRL) which is then applied to the LRL (Rijhwani et al., 2019) (more in §2). However, even with these relat"
2020.tacl-1.8,D18-1046,0,0.540546,"n batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. et al., 2018). String similarity (e.g., edit distance) cannot easily extend to XEL because surface forms of entities often differ significantly across the source and target language, particularly when the languages are in different scripts. Wikipedia link methods can be extended to XEL by using inter-language links between the two languages to redirect entities to the English KB (Spitkovsky and Chang, 2012; Sil and Florian, 2016; Sil et al., 2018; Upadhyay et al., 2018a). This method works to some extent, but often under-performs on lowresource languages due to the lack of source language Wikipedia resources. Although scarce, there are some methods that propose to improve entity candidate generation by training translation models with low resourcelanguage (LRL)-English entity gazetteers (Pan et al., 2017), or learning neural string matching models based on an entity gazetteer in a related high-resource language (HRL) which is then applied to the LRL (Rijhwani et al., 2019) (more in §2). However, even with these relatively sophisticated methods, top-30 candi"
2020.tacl-1.8,spitkovsky-chang-2012-cross,0,0.0848577,"Missing"
2020.tacl-1.8,W16-1403,0,0.0635414,"Missing"
2020.tacl-1.8,N16-1156,0,0.059149,"Missing"
2020.tacl-1.8,D16-1157,1,0.8678,"arce, so we did not attempt to expand entity names on the HRL side. 4.3 More Explicit String Encoding As mentioned previously, while BI-LSTMS have proven powerful in modeling sequential data in the literature, we argue that they are not an ideal string encoder for this setting. This is because our training data contain a nontrival number of pairs that contains less predictable word mappings (e.g., translations). With such large freedom in the face of insufficient and noisy training data, this encoder seemingly overfits, resulting in poor generalization. Previous researchers (Dai and Le, 2015; Wieting et al., 2016a) have noticed similar problems when using LSTMs for representation learning. As an alternative, we propose the use of the CHARAGRAM model (Wieting et al., 2016) as the string encoder. This model scans the string with various window sizes and produces a bag of character n-grams. It then maps these n-grams to their corresponding embeddings through a lookup table. The final embedding of the string is the sum of all the n-gram embeddings followed by a nonlinear activation function. Figure 3 shows an illustration of the model. Formally, we denote a string as a sequence of characters x = [x1 , x2"
2020.tacl-1.8,D19-6127,1,0.694919,"fferent words. One thing that is worth mentioning is that CHARAGRAM is able to correctly recognize some mappings of nontransliterated words. For instance, ‘‘Jaamacadda’’ in so is the parallel of ‘‘University’’ in English, and the model was able to correctly align n-grams corresponding to these words. This result demonstrates one way how CHARAGRAM alleviates the TRANS error that BI-LSTM suffers from. 5.6 Improving End-to-end XEL Systems To investigate how our candidate generation model influences the end-to-end XEL system, we use its candidate lists in the disambiguation model BURN proposed by Zhou et al. (2019). BURN creates a fully connected graph for each document and performs joint inference on all mentions in the document. To the best of our knowledge, it is currently the disambiguation model that has demonstrated the strongest empirical results for XEL without any targeted LRL resources. Therefore, we believe it is the most reasonable choice in our low-resource scenario. For details, 5.5 Properties of Learned n-grams As discussed in the previous sections, the objective of CHARAGRAM is to learn n-gram mappings 119 we encourage readers to refer to the original paper.14 To make the best use of sca"
2020.tacl-1.8,Q17-1028,0,\N,Missing
2020.tacl-1.8,J98-4003,0,\N,Missing
2021.eacl-main.220,W03-0501,0,0.0856166,"ed (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.1 1 Introduction Text summarization aims at identifying important information in long source documents and expressing it in human readable summaries. Two prominent methods of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where important sentences in the source article are selected to form a summary, and abstractive (Rush 1 Code and data available at: https://github.com/ vidhishanair/structured_summarizer et al., 2015; See et al., 2017), where the model restructures and rephrases essential content into a paraphrased summary. State of the art approaches to abstractive summarization employ neural encoder-decoder methods that encode the source document as a sequence of tokens producing latent document representations and decode the summary conditioned on the representations. Recent studi"
2021.eacl-main.220,P18-1063,0,0.0192659,"he right direction . the win was their third in six in the press championship having been relegated Figure 4: Examples of induced structures and generated summaries. summary generation. We also see in example 1 that the latent structures cluster sentences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization ("
2021.eacl-main.220,D16-1053,0,0.0445439,"Missing"
2021.eacl-main.220,N18-2097,0,0.0182117,"l., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our knowledge, StructSum is the first to jointly incorporate latent and explicit document structure in a summarization framework. 7 Conclusion and Future Work In this work, we propose the framework StructSum for incorporating latent and explicit document 2582 structure in"
2021.eacl-main.220,P19-1062,0,0.0609612,"Missing"
2021.eacl-main.220,P19-1630,0,0.0153658,"t al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our knowledge, StructSum is the first to jointly incorporate latent and explicit document structure in a summarization framework. 7 Conclusion and Future Work In this work, we propose the framewo"
2021.eacl-main.220,D18-1443,0,0.0326571,"Missing"
2021.eacl-main.220,P18-1013,0,0.0160938,"ctures cluster sentences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntac"
2021.eacl-main.220,P19-1206,0,0.0163096,"ic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our knowledge, StructSum is the first to jointly incorporate latent and explicit document structure in a summarization framework. 7 Conclusion and Future Work In this work, we propose the framework StructSum for incorporating latent and explicit document 2582 structure in neural abstractive summarization. We introduce a novel explicit-at"
2021.eacl-main.220,D18-1208,0,0.0606948,"Missing"
2021.eacl-main.220,P14-2052,0,0.0320829,"rds or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et"
2021.eacl-main.220,D19-1051,0,0.0254577,"Missing"
2021.eacl-main.220,2020.acl-main.703,0,0.0908276,"Missing"
2021.eacl-main.220,D18-1441,0,0.0150896,"shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our"
2021.eacl-main.220,N15-1114,0,0.0243291,"o either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structu"
2021.eacl-main.220,D19-1387,0,0.0336493,"der. We use the Adagrad optimizer (Duchi et al., 2011) with a learning rate of 0.15 and an initial accumulator value of 0.1. We do not use dropout and use 4 The best results from Gehrmann et al. (2018) outperform DiffMask experiment, but they use inference-time hard masking which can be applied on ours. Our baselines also exclude Reinforcement Learning (RL) based systems as they are not directly comparable, but our approach can be introduced in an encoder-decoder based RL system. Since we do not incorporate any pretraining, we do not compare with recent contextual representation based models (Liu and Lapata, 2019). 5 https://github.com/atulkum/pointer_ summarizer 2578 Model Pointer-Generator (See et al., 2017) Pointer-Generator + Coverage (See et al., 2017) Graph Attention (Tan et al., 2017) Pointer-Generator + DiffMask (Gehrmann et al., 2018) ROUGE 1 36.44 39.53 38.10 38.45 ROUGE 2 15.66 17.28 13.90 16.88 ROUGE L 33.42 36.38 34.00 35.81 Pointer-Generator (Re-Implementation) Pointer-Generator + Coverage (Re-Implementation) Latent-Structure (LS) Attention Explicit-Structure (ES) Attention LS + ES Attention 35.55 39.07 39.52 39.63 39.62 15.29 16.97 16.94 16.98 17.00 32.05 35.87 36.71 36.72 36.95 Table 1:"
2021.eacl-main.220,R11-1066,0,0.0334585,"ntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) induce latent structures for aspect based summarization, Cohan et al. (2018) focus on summarization of scientific papers, Isonuma et al. (2019) reviews unsupervised summarization, Mithun and Kosseim (2011) use discourse structures to improve coherence in blog summarization and Ren et al. (2018) use sentence relations for multi-document summarization. These are complementary directions to our work. To our knowledge, StructSum is the first to jointly incorporate latent and explicit document structure in a summarization framework. 7 Conclusion and Future Work In this work, we propose the framework StructSum for incorporating latent and explicit document 2582 structure in neural abstractive summarization. We introduce a novel explicit-attention module which incorporates external linguistic structur"
2021.eacl-main.220,K16-1028,0,0.0553552,"Missing"
2021.eacl-main.220,D18-1206,0,0.122169,"Missing"
2021.eacl-main.220,W08-1404,0,0.0610077,"et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and augmenting them to decoder copy mechanism. In contrast, we model sentence dependencies as latent structures and explicit coreference structures; we do not use heuristics or salient features. Li et al. (2018) propose structural compression and coverage regularizers incorporating structural bias of target summaries while we model the structure of the source document. Frermann and Klementiev (2019) ind"
2021.eacl-main.220,D15-1044,0,0.0718388,"wn his appointment , though he felt broncos were moving in the right direction . the win was their third in six in the press championship having been relegated Figure 4: Examples of induced structures and generated summaries. summary generation. We also see in example 1 that the latent structures cluster sentences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural"
2021.eacl-main.220,P17-1099,0,0.32103,"es by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.1 1 Introduction Text summarization aims at identifying important information in long source documents and expressing it in human readable summaries. Two prominent methods of generating summaries are extractive (Dorr et al., 2003; Nallapati et al., 2017), where important sentences in the source article are selected to form a summary, and abstractive (Rush 1 Code and data available at: https://github.com/ vidhishanair/structured_summarizer et al., 2015; See et al., 2017), where the model restructures and rephrases essential content into a paraphrased summary. State of the art approaches to abstractive summarization employ neural encoder-decoder methods that encode the source document as a sequence of tokens producing latent document representations and decode the summary conditioned on the representations. Recent studies suggest that these models suffer from several key challenges. First, since standard training datasets are derived from news articles, model outputs are strongly affected by the layout bias of the articles, with models relying on the leading s"
2021.eacl-main.220,C18-1146,0,0.0169814,"tences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowdhury et al., 2020). In pre-neural era, document structure played a critical role in summarization (Leskovec et al., 2004; Litvak and Last, 2008; Liu et al., 2015; Durrett et al., 2016; Kikuchi et al., 2014). More recently Song et al. (2018) infuse source syntactic structure into the pointer-generator using wordlevel syntactic features and aug"
2021.eacl-main.220,P17-1108,0,0.0588999,"Missing"
2021.eacl-main.220,N16-1174,0,0.0611917,"Schmidhuber, 1997; Vaswani et al., 2017). The encoder produces a set of hidden representations {h}. A decoder maps the previously generated token yt−1 to a hidden state and computes a soft attention probability distribution p(at |x, y1:t−1 ) over encoder hidden states. A distribution p over the vocabulary is computed at every time step t and the network is trained using the negative log likelihood loss: losst = −log p(yt ). StructSum modifies the above architecture as follows. We aggregate the token representations from the encoder to form sentence representations as in hierarchical encoders (Yang et al., 2016). We then use implicit- and explicit-structure attention modules to augment the sentence representations with sentence dependency information, leveraging both a learned latent structure and an external structure from other NLP modules. The attended vectors are then passed to the decoder, which produces the output abstractive summary. In the rest of this section, we describe our framework architecture, shown in Figure 1, in detail. 2.1 Sentence Representations We consider an encoder which takes a sequence of words in a sentence si = {w} as input and produces contextual hidden representation for"
2021.eacl-main.220,D18-1088,0,0.0188402,"eakened side put on fine show to crown his appointment , though he felt broncos were moving in the right direction . the win was their third in six in the press championship having been relegated Figure 4: Examples of induced structures and generated summaries. summary generation. We also see in example 1 that the latent structures cluster sentences based on the main topic of the document. Sentences 1,2,3 differ from sentences 5,6,7 in the topic discussed and our model clustered the two sets separately. 6 Related Work Data-driven neural summarization falls into extractive (Cheng et al., 2016; Zhang et al., 2018) or abstractive (Rush et al., 2015; See et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018). Pointer-generator See et al. (2017) learns to either generate novel in-vocabulary words or copy from the source. It has been the foundation for much work on abstractive summarization (Gehrmann et al., 2018; Hsu et al., 2018; Song et al., 2018). Our model extends it by incorporating latent/explicit structure, but these extensions are applicable to any other encoder-decoder architecture. For example, a follow-up study has already shown benefits of our method in multi-document summarization (Chowd"
ambati-etal-2010-active,2005.mtsummit-papers.30,1,\N,Missing
ambati-etal-2010-active,D08-1027,0,\N,Missing
ambati-etal-2010-active,J04-3001,0,\N,Missing
ambati-etal-2010-active,W09-4633,1,\N,Missing
ambati-etal-2010-active,P02-1040,0,\N,Missing
ambati-etal-2010-active,N09-1047,0,\N,Missing
ambati-etal-2010-active,P07-2045,0,\N,Missing
ambati-etal-2010-active,W07-0734,0,\N,Missing
ambati-etal-2010-active,D09-1030,0,\N,Missing
ambati-etal-2010-active,N06-1003,0,\N,Missing
ambati-etal-2010-active,N03-1017,0,\N,Missing
ambati-etal-2010-active,W09-1904,0,\N,Missing
ambati-etal-2010-active,P02-1016,0,\N,Missing
ambati-etal-2010-active,takezawa-etal-2002-toward,0,\N,Missing
ambati-etal-2010-active,P03-1021,0,\N,Missing
ambati-etal-2010-active,D09-1006,0,\N,Missing
C10-2037,P01-1008,0,0.060723,"is to find those n-gram candidate translations from a large target corpus that contain as many potential word and phrase translations of the source text from the dictionary and fewer spurious content words. The overlap decoder combines the target n-gram translation candidates by finding maximal left and right overlaps with the translation candidates of the previous and following n-grams. When the overlap decoder does not find coherent sequences of overlapping target n-grams, more candidate transla321 tions are obtained by substituting words or phrases in the target n-grams by their synonyms. Barzilay and McKeown (2001) and CallisonBurch et al. (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source. The synonym generation in Carbonell et al. (2006) differs from the above in that it does not require parallel resources containing multiple translations for the same source language. In Carbonell et al. (2006), a list of paired left and right contexts that contain the desired word or phrase are extracted from the monolingual corpus. The same corpus is used to find other words and phrases that fit the paired contexts in the list. The idea is based"
C10-2037,2003.mtsummit-papers.4,1,0.889386,"Missing"
C10-2037,C00-1019,1,0.652291,"al Profiles for Word Substitution in Machine Translation Rashmi Gangadharaiah rgangadh@cs.cmu.edu Ralf D. Brown ralf@cs.cmu.edu Jaime Carbonell jgc@cs.cmu.edu Language Technologies Institute, Carnegie Mellon University Abstract corpus. When matches are found, the corresponding translations in the target language are obtained through sub-sentential alignment. In our EBMT system, the final translation is obtained by combining the partial target translations using a statistical target Language Model. EBMT systems, like other data-driven approaches, require large amounts of data to function well (Brown, 2000). Out-of-vocabulary (OOV) words present a significant challenge for Machine Translation. For low-resource languages, limited training data increases the frequency of OOV words and this degrades the quality of the translations. Past approaches have suggested using stems or synonyms for OOV words. Unlike the previous methods, we show how to handle not just the OOV words but rare words as well in an Example-based Machine Translation (EBMT) paradigm. Presence of OOV words and rare words in the input sentence prevents the system from finding longer phrasal matches and produces low quality translati"
C10-2037,N06-1003,0,0.0551967,"Missing"
C10-2037,P08-2015,0,0.100552,"s the method adopted in this paper. Section 4 describes the experimental setup. Section 5 reports the results obtained with the new framework for English-Chinese and English-Haitian translation systems. Section 6 concludes and suggests possible future work. 2 Related Work Orthographic and morpho-syntactic techniques for preprocessing training and test data have been shown to reduce OOV word rates. Popovi´c and Ney (2004) demonstrated this on rich morphological languages in an SMT system. They introduced different types of transformations to the verbs to reduce the number of unseen word forms. Habash (2008) addresses spelling, nametransliteration OOVs and morphological OOVs in an Arabic-English Machine Translation system. Phrases with the OOV replacements in the phrase table of a phrase-based SMT system were “recycled” to create new phrases in which the replacements were replaced by the OOV words. Yang and Kirchhoff (2006) proposed a backoff model for phrase-based SMT that translated word forms in the source language by hierarchical morphological phrase level abstractions. If an unknown word was found, the word was first stemmed and the phrase table entries for words sharing the same stem were m"
C10-2037,koen-2004-pharaoh,0,0.0755058,"Missing"
C10-2037,D09-1040,0,0.293466,"on. When data is limited, the number of OOV words increases, leading to the poor performance of the translation models and the language models due to the absence of longer sequences of source word matches and less reliable language model estimates. Introduction An EBMT system makes use of a parallel corpus to translate new sentences. Each input sentence is matched against the source side of a training Approaches in the past have suggested using stems or synonyms for OOV words as replacements (Yang and Kirchhoff, 2006). Similarity measures have been used to find words that are closely related (Marton et al., 2009). For morpho320 Coling 2010: Poster Volume, pages 320–328, Beijing, August 2010 logically rich languages, the OOV word is morphologically analyzed and the stem is used as its replacement (Popovi´c and Ney, 2004). This paper presents a simpler method inspired by the Context-based MT approach (Carbonell et al., 2006) to improve translation quality. The method requires a large source language monolingual corpus and does not require any other language dependent resources to obtain replacements. Approaches suggested in the past only concentrated on finding replacements for the OOV words and not the"
C10-2037,P02-1040,0,0.0924833,"gual corpus. The corresponding target phrasal translations are obtained through subsentential alignment. When an input lattice is given instead of an input sentence, the system performs the same matching process for all possible phrases obtained from the input lattice. Hence, the system also finds matches for source phrases that contain the replacements for the OOV/rareword. Only the top C ranking replacement candi同 一 箭 hawks 一箭 三 雕 一 箭 三雕 “ three birds ” We would like to select those feature weights (~λ) which would lead to the least expected loss in translation quality (Eqn 3). −log(BLEU ) (Papineni et al., 2002) is used to calculate the expected loss over a development set. As this objective function has many local minima and is piecewise constant, the surface is smoothed using the L2norm regularization. Powell’s algorithm (Powell, 1964) with grid-based line optimization is used to find the best weights. 7 different random guesses are used to initialize the algorithm. 3.7 移动 Tuning feature weights min Eλ [L(ttune )] + τ ∗ ||λ||2 基地 一 where, f~i,j is the feature vector for the j th replacement candidate of wordi , S is the number of replacements, ~λ is the weight vector indicating the importance of th"
C10-2037,popovic-ney-2004-towards,0,0.301374,"Missing"
C10-2037,W07-0705,0,0.0180861,"hrase-based SMT that translated word forms in the source language by hierarchical morphological phrase level abstractions. If an unknown word was found, the word was first stemmed and the phrase table entries for words sharing the same stem were modified by replacing the words with their stems. If a phrase entry or a single word phrase was found, the corresponding translation was used, otherwise the model backed off to the next level and applied compound splitting to the unknown word. The phrase table included phrasal entries based on full word forms as well as stemmed and split counterparts. Vilar et al. (2007) performed the translation process treating both the source and target sentences as a string of letters. Hence, there are no unknown words when carrying out the actual translation of a test corpus. The word-based system did most of the translation work and the letterbased system translated the OOV words. The method proposed in this work to handle OOV and rare words is very similar to the method adopted by Carbonell et al. (2006) to generate word and phrasal synonyms in their Contextbased MT system. Context-based MT does not require parallel text but requires a large monolingual target language"
C10-2037,E06-1006,0,0.426321,"ion systems either ignore these unknown words or leave them untranslated in the final target translation. When data is limited, the number of OOV words increases, leading to the poor performance of the translation models and the language models due to the absence of longer sequences of source word matches and less reliable language model estimates. Introduction An EBMT system makes use of a parallel corpus to translate new sentences. Each input sentence is matched against the source side of a training Approaches in the past have suggested using stems or synonyms for OOV words as replacements (Yang and Kirchhoff, 2006). Similarity measures have been used to find words that are closely related (Marton et al., 2009). For morpho320 Coling 2010: Poster Volume, pages 320–328, Beijing, August 2010 logically rich languages, the OOV word is morphologically analyzed and the stem is used as its replacement (Popovi´c and Ney, 2004). This paper presents a simpler method inspired by the Context-based MT approach (Carbonell et al., 2006) to improve translation quality. The method requires a large source language monolingual corpus and does not require any other language dependent resources to obtain replacements. Approac"
C10-2037,N03-1017,0,\N,Missing
C10-2037,I05-3027,0,\N,Missing
C12-3041,P05-1045,0,0.01552,"Missing"
C12-3041,marujo-etal-2012-supervised,1,0.823786,"conclusions and future work. 2 Technical Approach We used a two-step approach to find named event passages (NEP) in a document. First, we used a previously trained classifier to label each sentence in the document. Second, we used both a rule-based model and HMM based statistical model to find contiguous sequences of sentences covering the same event. For the classifier part, we adopted a supervised learning framework, based on Weka (Hall et al. 2009). 2.1 Features and feature extraction As features, we used key-phrases identified by an automatic key-phrase extraction algorithm described in (Marujo et al. 2012). Key phrases consist of one or more words that represent the main concepts of the document. Marujo (2012) enhanced a state-of-the-art Supervised Key Phrase Extractor based on bagging over C4.5 decision tree classifier (Breiman, 1996; Quinlan, 1994) with several types of features, such as shallow semantic, rhetorical signals, and sub-categories from Freebase. The authors also included 2 forms of document pre-processing that were called light filtering and co-reference normalization. Light filtering removes sentences from the document, which are judged peripheral to its main content. Co-referen"
C16-1114,Q16-1031,0,0.0857954,"and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event extraction is that the amount of available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to leverage existing language resources for event extraction in new languages. This problem is closely related to low-resource NLP, which"
C16-1114,P15-2061,0,0.019842,"ple languages using a combination of both language-dependent and language-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011;"
C16-1114,W09-2209,0,0.707844,"age-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and"
C16-1114,N09-2053,0,0.190127,"age-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and"
C16-1114,C12-1033,0,0.325075,"here target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event extraction is that"
C16-1114,chen-ng-2014-sinocoreferencer,0,0.135282,"training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event extraction is that the amount of avail"
C16-1114,P15-1017,0,0.137994,"ments. In most cases, the event trigger extraction step is conducted first to identify the event mentions, and then event argument extraction is performed on top of this to identify the particular entities fulfilling argument roles for these event mentions. 3 Related Work A variety of machine learning methods have been used for event extraction in the past, including pipelines of classifiers (Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2013; Li and Ji, 2014; Yang and Mitchell, 2016), and neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) – the vast majority of which focus solely on the English monolingual training scenario. A subset of the event extraction literature has considered the study of Chinese event extraction (Chen and Ji, 2009b; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, most of these works also focus solely on the monolingual case, and do not leverage any additional training data from other languages. The most related work to our approach is that of Chen and Ji (2009a). In their model, they designed a co-training approach to augment a small Chinese training corpus with additional examples fro"
C16-1114,D11-1005,0,0.0516451,"ta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event extraction is that the amount of available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to leverage existing language"
C16-1114,D14-1096,0,0.134198,"that the amount of available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to leverage existing language resources for event extraction in new languages. This problem is closely related to low-resource NLP, which has been gathering increased interest among researchers (Garrette et al., 2013; Miao et al., 2013; Duong et al., 2014; Duong et al., 2015). In this paper, we propose a novel approach for cross-lingual event extraction, which trains on multiple languages and leverages both language-dependent and language-independent features in order to boost performance. Using such a system we aim to jointly leverage available multilingual resources (annotated data and induced features) to overcome the annotation-scarcity issue in the target language of interest. Empirically we show that our approach can substantially improve the performance of monolingual systems for the task of Chinese event argument extraction. Our approa"
C16-1114,D15-1040,0,0.0235475,"available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to leverage existing language resources for event extraction in new languages. This problem is closely related to low-resource NLP, which has been gathering increased interest among researchers (Garrette et al., 2013; Miao et al., 2013; Duong et al., 2014; Duong et al., 2015). In this paper, we propose a novel approach for cross-lingual event extraction, which trains on multiple languages and leverages both language-dependent and language-independent features in order to boost performance. Using such a system we aim to jointly leverage available multilingual resources (annotated data and induced features) to overcome the annotation-scarcity issue in the target language of interest. Empirically we show that our approach can substantially improve the performance of monolingual systems for the task of Chinese event argument extraction. Our approach is novel compared"
C16-1114,P13-1057,0,0.0191637,"An important issue in event extraction is that the amount of available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to leverage existing language resources for event extraction in new languages. This problem is closely related to low-resource NLP, which has been gathering increased interest among researchers (Garrette et al., 2013; Miao et al., 2013; Duong et al., 2014; Duong et al., 2015). In this paper, we propose a novel approach for cross-lingual event extraction, which trains on multiple languages and leverages both language-dependent and language-independent features in order to boost performance. Using such a system we aim to jointly leverage available multilingual resources (annotated data and induced features) to overcome the annotation-scarcity issue in the target language of interest. Empirically we show that our approach can substantially improve the performance of monolingual systems for the task of Chines"
C16-1114,P09-2093,0,0.0258838,"g to boost performance. We propose a new event extraction approach that trains on multiple languages using a combination of both language-dependent and language-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2"
C16-1114,P08-1030,0,0.760974,"g cross-lingual training to boost performance. We propose a new event extraction approach that trains on multiple languages using a combination of both language-dependent and language-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2"
C16-1114,D14-1198,0,0.0607946,"nt extraction task is to identify all of the event arguments contained within a set of documents. In most cases, the event trigger extraction step is conducted first to identify the event mentions, and then event argument extraction is performed on top of this to identify the particular entities fulfilling argument roles for these event mentions. 3 Related Work A variety of machine learning methods have been used for event extraction in the past, including pipelines of classifiers (Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2013; Li and Ji, 2014; Yang and Mitchell, 2016), and neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) – the vast majority of which focus solely on the English monolingual training scenario. A subset of the event extraction literature has considered the study of Chinese event extraction (Chen and Ji, 2009b; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, most of these works also focus solely on the monolingual case, and do not leverage any additional training data from other languages. The most related work to our approach is that of Chen and Ji (2009a). In their model, they designed"
C16-1114,D12-1092,0,0.531557,"cus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event"
C16-1114,P13-1008,0,0.474061,"t trains on multiple languages using a combination of both language-dependent and language-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 201"
C16-1114,C10-1077,0,0.0319981,"ce. We propose a new event extraction approach that trains on multiple languages using a combination of both language-dependent and language-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing"
C16-1114,R11-1002,0,0.519336,"t extraction approach that trains on multiple languages using a combination of both language-dependent and language-independent features, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008;"
C16-1114,P14-5010,0,0.00396925,"st our approach, we conduct experiments on two separate tasks: event trigger extraction and event argument extraction. We begin by describing our experimental setup and metrics, and subsequently show empirical results on the two tasks. 5.1 Dataset We conduct experiments on the ACE 2005 dataset, the most dominating benchmark dataset for event trigger extraction and event argument extraction. The English and Chinese portions of ACE each contain several hundred documents annotated with gold standard entity and event information. We preprocess the raw text of each document using Stanford CoreNLP (Manning et al., 2014). We split the Chinese portion into 10 folds, and perform cross-validation. In the ACE collection, the number of labeled Chinese documents is approximately the same as the number of English documents, so to simulate a low resource scenario, we select just one training fold for each round of cross-validation. We use another fold for parameter tuning, and use the remaining folds in each round for testing. We use CC-CEDICT2 as our bilingual dictionary between English and Chinese. For our baseline system, we use just the Chinese data for training, and only the monolingual features. Our cross-lingu"
C16-1114,D11-1006,0,0.197366,"ronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event extraction is that the amount of available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to leverage existing language resources for event extraction in new languages. This problem is closely related to lo"
C16-1114,P15-2060,0,0.143016,"tained within a set of documents. In most cases, the event trigger extraction step is conducted first to identify the event mentions, and then event argument extraction is performed on top of this to identify the particular entities fulfilling argument roles for these event mentions. 3 Related Work A variety of machine learning methods have been used for event extraction in the past, including pipelines of classifiers (Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2013; Li and Ji, 2014; Yang and Mitchell, 2016), and neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) – the vast majority of which focus solely on the English monolingual training scenario. A subset of the event extraction literature has considered the study of Chinese event extraction (Chen and Ji, 2009b; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, most of these works also focus solely on the monolingual case, and do not leverage any additional training data from other languages. The most related work to our approach is that of Chen and Ji (2009a). In their model, they designed a co-training approach to augment a small Chinese training corpus with add"
C16-1114,petrov-etal-2012-universal,0,0.0382032,"ction system. The argument component relies on the predictions from the trigger component. Event Trigger Extraction Features Lexical features (e.g. words and lemmas within a context window) Length of the current word Language-specific POS tags within a context window Universal POS tags within a context window Word embedding vector for current word Dependent/Governor information from dependency parsing Bilingual dictionary word pairs Table 1: Features used in the Event Trigger Extraction component Multilingual training is leveraged via the use of four types of features: 1.) Universal POS Tags (Petrov et al., 2012), 2.) Universal Dependencies (McDonald et al., 2013), 3.) limited bilingual dictionaries, and 4.) aligned multilingual word embeddings. The Universal POS tags and Universal Dependencies allow us to use a single set of tags for both languages, which thereby enables the use of English training data directly in our model. The bilingual dictionary provides a limited number of translations between words, and may be used both directly in the model and for aligning word embeddings. The aligned word embeddings similarly allow us to directly use English training data, as each component in the vector is"
C16-1114,R11-1029,0,0.131792,"ures, with particular focus on the case where target domain training data is of very limited size. We show empirically that multilingual training can boost performance for the tasks of event trigger extraction and event argument extraction on the Chinese ACE 2005 dataset. 1 Introduction Traditionally, event extraction has focused on monolingual training – typically English (Grishman et al., 2005; Ji and Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An importa"
C16-1114,P08-1001,0,0.0244955,"and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event extraction is that the amount of available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to leverage existing language resources for event extraction in new languages. This problem is closely related to low-resource NLP, which has been gathering increased interest among researchers"
C16-1114,C02-1070,0,0.0726277,"e extracted argument fillers into English, and merge together argument fillers across documents. Using this cross-document information fusion, they find improved performance over monolingual systems. However, this work relies on having documents across multiple languages that describe the exact same event, which is an unrealistic case in practice. Additionally, they also rely on having high quality machine translation in order to translate the argument output of each monolingual system into English. There does exist some prior work on the broader field of cross-lingual information extraction. Riloff et al. (2002) start with English annotated source texts, create a parallel corpus via machine translation, and project the annotations via alignments. The projected annotations are then used to conduct training in the target language. Sudo et al. (2004) presents an approach for extracted patterns in a source language and translating these patterns for use on a target language. However, these works are limited to entity extraction, whereas our focus is on event extraction. Furthermore, both works rely on having high-quality machine translation output. Beyond information extraction, cross-lingual training ha"
C16-1114,N09-1010,0,0.0159886,"d Grishman, 2008; Gupta and Ji, 2009; Liao and Grishman, 2010; Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event extraction is that the amount of available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to lever"
C16-1114,C04-1127,0,0.0409817,"nts across multiple languages that describe the exact same event, which is an unrealistic case in practice. Additionally, they also rely on having high quality machine translation in order to translate the argument output of each monolingual system into English. There does exist some prior work on the broader field of cross-lingual information extraction. Riloff et al. (2002) start with English annotated source texts, create a parallel corpus via machine translation, and project the annotations via alignments. The projected annotations are then used to conduct training in the target language. Sudo et al. (2004) presents an approach for extracted patterns in a source language and translating these patterns for use on a target language. However, these works are limited to entity extraction, whereas our focus is on event extraction. Furthermore, both works rely on having high-quality machine translation output. Beyond information extraction, cross-lingual training has offered benefits for a variety of tasks. McDonald et al. (2011) use a delexicalized English parser to seed a lexicalized parser in the target language, and then iteratively improve upon this model via constraint driven learning. Duong et"
C16-1114,N16-1033,0,0.0635025,"k is to identify all of the event arguments contained within a set of documents. In most cases, the event trigger extraction step is conducted first to identify the event mentions, and then event argument extraction is performed on top of this to identify the particular entities fulfilling argument roles for these event mentions. 3 Related Work A variety of machine learning methods have been used for event extraction in the past, including pipelines of classifiers (Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2011), joint inference models (Li et al., 2013; Li and Ji, 2014; Yang and Mitchell, 2016), and neural networks (Nguyen and Grishman, 2015; Chen et al., 2015) – the vast majority of which focus solely on the English monolingual training scenario. A subset of the event extraction literature has considered the study of Chinese event extraction (Chen and Ji, 2009b; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, most of these works also focus solely on the monolingual case, and do not leverage any additional training data from other languages. The most related work to our approach is that of Chen and Ji (2009a). In their model, they designed a co-training approach to"
C16-1114,I08-3008,0,0.0381417,"Liao and Grishman, 2011; Li et al., 2013; Bronstein et al., 2015), and occasionally Chinese or other languages (Chen and Ji, 2009b; Piskorski et al., 2011; Li et al., 2012; Chen and Ng, 2012; Chen and Ng, 2014). However, apart from a few isolated studies (Chen and Ji, 2009a; Piskorski et al., 2011), to date there is very little work leveraging cross-lingual information for event extraction. Cross-lingual approaches have proven useful for many other tasks in natural language processing (NLP), including part-of-speech (POS) tagging (Snyder et al., 2009; Cohen et al., 2011), dependency parsing (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Ammar et al., 2016), and named entity recognition (Richman and Schone, 2008). An important issue in event extraction is that the amount of available training data is often insufficient or unbalanced across domains and/or languages. Event extraction training datasets typically contain merely a few hundreds of documents, owing to the complexity and high costs of human annotation. This issue is even more severe for new event types in new languages. This provides strong motivation to leverage existing language resources for event extraction in new langu"
C86-1037,J83-3001,1,0.839493,"s words, or use incorrect punctuation• Moreover, they often do not notice their errors, as task knowledge and redundancies in the language allow for fairly easy human comprehension of sentences that fail to respect all grammatical niceties. Approximately a third of all sentences in our analyzed sample of several hundred interactions were extragrammatical in a strict sense, mostly due to sloppy user input. However, initial work at automated recovery when possible, and focused interactive correction when needed, shows promise for future improvements in this important aspect of user habitability [1]. Response time - Next to grammatical coverage, perhaps the most widely recognized requirernent for habitability is real-time response. We find that whereas this is indeed an issue, the combination of new parsing techniques [8], faster hardware, and on-line parsing 2 mean that real time performance will be less of a concern for developers of task oriented natural language interfaces. Back-end response - Last but certainly not least, the manner by which the backend system responds to the user is crucial. An ideal natural language recognizer coupled to an expert system or data base that returns"
C86-1037,C86-1149,1,0.815883,"em were compartamentatized into separate subsystem,'~ because the former is domain general whereas the latter is domain specific. However, such separation entails serious performance compromises, both in speed and accuracy of the resultant analysis (e.g. the inability to resolve syntactic ambiguities without semantic criteria, and the inability to recover from ill-formed input unless both semantic and syntactic constraints are u~fified in the recognition process). Lately, a new approach is emerging, where separate syntactic and semantic knowledge sources are precompiled into a unified grammar [7], thus sharing the advantages o f separation of knowledge sources at development time and integrated robust parsing at run time. d e v e l o p m e n t w o r k b e n c h e s - In order to speed the development of a new interface, and to ensure consistency and well-formedness of new grammars and lexicons, specialized software tools are begin developed, much like the structured editors ~md prnura!nnfing environments that improve programmer prodLIctivily. Moreover, grammars are more highly structured tlutn computer programs, thus such tools have an even qreater impact in improving grammar-writer p"
C86-1037,P83-1025,1,\N,Missing
C86-1138,P81-1032,1,0.895529,"ng these islands. Island growing, attractive in theory, presents serious practical problems for ATN parsers, not the least of which is the requirement of running ATNs from right to left. This method of interpreting the networks, necessary with center-out teehr4ques, fails when tests depend on registers that have not yet been set, No modifications to network-based techniques have been totally successful. 3. Semantic Caseframe Parsing Our approach is quite different from the transition network approach and is derived from recent work at Carnegie-Mellon University by Carbonell, Hayes, and others [3, 7, 6, 2] on understanding typed, restricted domain natural language, with a particular concentration on handling ill-formed input. The technique that makes it possible to process sensible but potentially imperfect or incomplete uttere.nces is called semantic caseframe instantiation. Unlike network-based techniques, caseframe methods er~able a parser to anchor its interpretation on the most significant input parts, and to grow its islands of interpretation to the less significant segments. Since the more significant words tend to be longer and therefore more likely to be recognized reliably, the island"
C86-1138,P81-1033,1,0.888489,"ers is that tile resulting caseframe combinations form a ready. made semantic interpretation of the input. The interpretation is typically incomplete until it is filled out in the subsequent gap-filling stage. However, if the recognition of some or all of the rernaining words is so poor that the semantic interpretation is never fully completed; then the parser still has something to report. Depending on the application domain, a skeleton interpretation could be sufficient for the application, or would at least form the basis of a Iocussed t~eguest for confirmation or clarification to the user [5]. In the remainder of this section, we examine irJ more detail our current implemental.ion of,.the approach outlined above, starting first with a description of the word lattice that drives our casefl&apos;am(.&apos;-based parser for spoken input. This parser operates in the context of&quot; a complete speech understanding system Hint handles sp~aker independent continuous speech with a 200 word vocabulary in an electronic mail domain. 589 Forward message Smith CMUA 4.1. The word lattice and these would have the nesting structure: Tile input to our caseframe speech parser can be viewed as a two[ForwardAction"
C86-1138,P84-1047,1,0.87705,"ng these islands. Island growing, attractive in theory, presents serious practical problems for ATN parsers, not the least of which is the requirement of running ATNs from right to left. This method of interpreting the networks, necessary with center-out teehr4ques, fails when tests depend on registers that have not yet been set, No modifications to network-based techniques have been totally successful. 3. Semantic Caseframe Parsing Our approach is quite different from the transition network approach and is derived from recent work at Carnegie-Mellon University by Carbonell, Hayes, and others [3, 7, 6, 2] on understanding typed, restricted domain natural language, with a particular concentration on handling ill-formed input. The technique that makes it possible to process sensible but potentially imperfect or incomplete uttere.nces is called semantic caseframe instantiation. Unlike network-based techniques, caseframe methods er~able a parser to anchor its interpretation on the most significant input parts, and to grow its islands of interpretation to the less significant segments. Since the more significant words tend to be longer and therefore more likely to be recognized reliably, the island"
C86-1138,J80-2003,0,\N,Missing
C86-1138,J83-3003,0,\N,Missing
C86-1138,J83-3001,1,\N,Missing
C86-1149,T87-1035,0,0.0274473,"n proven in practice for large scale systems. Our approach is not to interpret the bi-directienal grammars directly, but rather to compile them into much more efficient (and different) parsing and generation grarnmars, The latter endeavor still requires empirical wtlidation. o h l c r e n l e r l t a l C o m p i l a t i o n .. In order to expedite the grammar development and testing cycle, we are contemplating incremental compilation for new additions or recent changes into large existing gn~mmars rapidly. Although the compilation process has proven successful in earlier parsers we have built [3,27], incremental compilation introduces new teuhnical problems. ~, User ex'tensJbility -- A longer range research topic is to provide zt structured interface whereby a user of th(~ KI]MT system couM add donlain knowledge (entities) sad dictionary envies without r(.'quiring any kno~Icdgc of the internal struciure of the system. Extendir~ 9 th,!~ lexicon i'.~,of course, milch simpler them extending the domain semantics. All such extensions would work in concert with existing domain knowledge, lexicon, and grammar. -- The recognition of ill-structured language is very important, especially for the s"
C86-1149,P81-1032,1,0.885271,"Missing"
C86-1149,J83-3001,1,0.880512,"tionary envies without r(.'quiring any kno~Icdgc of the internal struciure of the system. Extendir~ 9 th,!~ lexicon i'.~,of course, milch simpler them extending the domain semantics. All such extensions would work in concert with existing domain knowledge, lexicon, and grammar. -- The recognition of ill-structured language is very important, especially for the shorttext domains we envision for our system (telex messages, banking transactions, doctor.patient dialogs, etc.). We have built selective-relaxation methods that integrate semantic and syntactic constrains before in the MULTIPAR system [7, 12], but have not yet investigated their application or extension into the functional/entity paradigm selected here. e Robustness • S p e e c h C o m p a t i b i l i t y -- A long-term objective is to integrate speech recognition and generation with online real-time machine translation. A parallel project at CMU is integrating speaker-independent continuousospeech recognition with a case.frame semantic parser of English [15]. We expect results of that investigation, which is already moving towards the precornpilation parsers discussed here, to pave the way towards eventual translation of spoken l"
C86-1149,1997.mtsummit-plenaries.5,0,0.0504036,"are developed separately in the entity-oriented and functional grammar formalisms, and a multi-stage grammar preoompfler compiles tfiem into a single knowledge base which contains both syntactic and semantic information in a form suitable for efficient real-time parsing. Our integrated approach copes with limitations of both entity-oriented and functional grammar formalisms, retaining the advantages of each. The approach is particularly well suited for machine translation, where knowledge of multiple languages must be represented in a uniform manner. Knowledge-based machine translation (KBMT) [8] is the process of applying syntactic knowledge of the source language and semantic knowledge pertinent to the source text in order to produce a canonical language-free meaning representation, which may then be rendered in many different languages. The analysis process of producing a meaning representation is far more complex than that of using target-language knowledge to express the meaning, representation in the target language, because the former is a many-to-one mapping, whereas the latter may be coerced into a one-to-one mapping. 2 Whereas KBMT is in principle far superior to conventiona"
C86-1149,P83-1025,1,0.921355,"but more efficient. This merged grammar is further precompiled into a yet larger parsing table for added efficiency, enabling the run-time system to parse input text in a very efficient manner using the parsing algorithm recently introduced by Tomita [26, 25]. More on this issue shall be discussed in section 6. 634 Inrerenc+r ~ OUTPUT s itself in fairly non-extensible ways [23, 17, 2, 6]. Subsequent improvements have succeeded in factoring out much of the domain semantics, but leaving the syntactic interpret;.{tion as part of the recognition program rather than as an explicit external grammar [9, 14, 16]. Many of the syntactic analysis methods do not integrate well with semantic knowledge, especially knowledge that must be kept in separate data structures and integrated only by the preeompiler at the run:time language intepretation process. Similarly, many of the semantic representation formalisms do not lend themselves well to dynamic integration with syntactic constraints at parse time. The best fit we have been able to achieve comes from precompiling syntactic and semantic knowledge into a single knowledge base which is used only at run.time, as described in the subsequent sections. I ~ sp"
C86-1149,C86-1138,1,0.874514,"Missing"
C86-1149,P84-1047,0,0.140904,"general, and semantic knowledge bases, domain specific but language general. Subsequently, grammars and domain knowledge are precompiled automatically in any desired combination to produce very efficient and very thorough real-time parsers. A pilot implementation of our KBMT architecture using functional grammars and entity-oriented semantics demonstrates the feasibility of the new approach? 1. Introduction This paper introduces a new approach to knowledge-based machine translation for well-defined domains, integrating two recent advances in computational linguistics: entity-oriented parsing [16] and functional grammars [4, 19]. The entity-oriented formalism has several strengths in representing semantic knowledge for circumscribed domains, but has limitations in representing general syntactic knowledge. Functional grammar formalisms, such as lexical functional grammar (LFG) and functional unification grammar (UG), on the other hand, can represent general syntactic knowledge, but are severely limited in their ability to represent general semantic information. In our approach, the semantic and syntactic knowledge bases are developed separately in the entity-oriented and functional gram"
C86-1149,P84-1018,0,0.0659997,"dge bases, domain specific but language general. Subsequently, grammars and domain knowledge are precompiled automatically in any desired combination to produce very efficient and very thorough real-time parsers. A pilot implementation of our KBMT architecture using functional grammars and entity-oriented semantics demonstrates the feasibility of the new approach? 1. Introduction This paper introduces a new approach to knowledge-based machine translation for well-defined domains, integrating two recent advances in computational linguistics: entity-oriented parsing [16] and functional grammars [4, 19]. The entity-oriented formalism has several strengths in representing semantic knowledge for circumscribed domains, but has limitations in representing general syntactic knowledge. Functional grammar formalisms, such as lexical functional grammar (LFG) and functional unification grammar (UG), on the other hand, can represent general syntactic knowledge, but are severely limited in their ability to represent general semantic information. In our approach, the semantic and syntactic knowledge bases are developed separately in the entity-oriented and functional grammar formalisms, and a multi-stag"
C86-1149,P85-1017,0,0.0535675,"each constituent. Moreover, functional structures integrate far more coherently into case-frame based semantic structures such as entity definitions. Two well-known functional grammar formalisms are Functional Unification Grammar rUG)[19] and Lexical Function Grammar (LFG] [4]. In this paper, however, we do not distinguish between them and refer to both by the term &quot;functional grammar&quot;. Application of the functional grammar formalism to machine translation is discussed in [tg]. Attempts have being made to implement parsers using these grammars, most notably in the PATR-II project at Stanford [22, 24]. However, these efforts have not been integrated with external semantic knowledge bases, and have not been applied in the context of KBMT systems. There are two main advantages of using the functional grammar formalism in practical machine translation systems: • A system implemented strictly within the functional grammar formalism will be reversible, in the sense that if the system maps from A to B then, to the same extent, it maps from Eli to A. lhus, we do not need to write separate grammars for parsing and generation. We merely compile the same grammar into an efficient uni.directional str"
C86-1149,P85-1018,0,0.033888,"each constituent. Moreover, functional structures integrate far more coherently into case-frame based semantic structures such as entity definitions. Two well-known functional grammar formalisms are Functional Unification Grammar rUG)[19] and Lexical Function Grammar (LFG] [4]. In this paper, however, we do not distinguish between them and refer to both by the term &quot;functional grammar&quot;. Application of the functional grammar formalism to machine translation is discussed in [tg]. Attempts have being made to implement parsers using these grammars, most notably in the PATR-II project at Stanford [22, 24]. However, these efforts have not been integrated with external semantic knowledge bases, and have not been applied in the context of KBMT systems. There are two main advantages of using the functional grammar formalism in practical machine translation systems: • A system implemented strictly within the functional grammar formalism will be reversible, in the sense that if the system maps from A to B then, to the same extent, it maps from Eli to A. lhus, we do not need to write separate grammars for parsing and generation. We merely compile the same grammar into an efficient uni.directional str"
C88-1021,J81-4001,0,\N,Missing
C88-1021,P83-1025,1,\N,Missing
C94-1013,C92-3168,1,\N,Missing
C94-1013,1991.mtsummit-papers.9,1,\N,Missing
carbonell-etal-2002-automatic,2001.mtsummit-road.7,1,\N,Missing
cavalli-sforza-etal-2004-developing,C00-1019,0,\N,Missing
cavalli-sforza-etal-2004-developing,2003.mtsummit-papers.4,1,\N,Missing
D16-1153,W02-2004,0,0.0260502,"Missing"
D16-1153,W03-0423,0,0.0343727,"posed. Despite the noisy supervision provided in the target language, transferring from Turkish and Uzbek provides a +14.1 F1 improvement over a state of the art monolingual model trained on the same Uyghur annotations. It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons. 4 Prior Work NER is a well-studied sequence-labeling problem for which many different approaches have been proposed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models (Chieu and Ng, 2003), hierarchically smoothed tries (Cucerzan and Yarowsky, 1999), decision trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Mo"
D16-1153,W14-4012,0,0.0384515,"Missing"
D16-1153,W99-0612,0,0.0745531,"target language, transferring from Turkish and Uzbek provides a +14.1 F1 improvement over a state of the art monolingual model trained on the same Uyghur annotations. It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons. 4 Prior Work NER is a well-studied sequence-labeling problem for which many different approaches have been proposed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models (Chieu and Ng, 2003), hierarchically smoothed tries (Cucerzan and Yarowsky, 1999), decision trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very"
D16-1153,W15-3904,0,0.0447429,"Missing"
D16-1153,W03-0425,0,0.103213,"nsfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons. 4 Prior Work NER is a well-studied sequence-labeling problem for which many different approaches have been proposed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models (Chieu and Ng, 2003), hierarchically smoothed tries (Cucerzan and Yarowsky, 1999), decision trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016)."
D16-1153,P12-1073,0,0.0342719,"imilar architecture to Lample et al. (2016), replacing the LSTMs with Gated Recurrent Units (Cho et al., 2014). However, Yang et al. (2016) also tackle multi task and multi-lingual joint training scenarios. Most of the models cited so far are monolingual either because they use hand crafted features and language specific resources or because of deepseated assumptions. For example a change in orthography, lexicon, script, word order or addition of complex morphology makes transfer impossible. This is the central challenge that we tackle. There has been much less work catering to this scenario. Kim et al. (2012) use weak annotations from Wikipedia metadata and parallel data for multi lingual NER. Yang et al. (2016) addresses the use case of multilingual joint training, which assumes there is sufficient data available in all languages. Nothman et al. (2013) also operate under the assumption of availability of Wikipedia data. To the best of our knowledge, a scenario involving transfer of a model trained in one (or more) source language(s) to another language with little or no labeled data, different script, different morphology, different lexicon, lack of transliteration, non-mutual intelligibility etc"
D16-1153,W03-0428,0,0.0902775,"trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The follow"
D16-1153,N16-1030,1,0.138311,"or engineering features have been developed. A more challenging task is to design a model that retains competence in monolingual scenarios and can easily be transferred to a low resource language with minimum overhead in terms of data annotation requirements and feature engineering. This transfer setting introduces additional challenges such as varying character usage conventions across languages with same script, differing scripts, lack of NE transliteration, varying morphology, different lexicons and mutual non-intelligibility to name a few. We propose the following changes over prior work (Lample et al., 2016) to address the challenges of the low-resource transfer setting. We use: 1. Language universal phonological character representations instead of orthographic ones. 2. Attention over characters of a word while labeling it with an NE category. 1462 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1462–1472, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Figure 1: Attentional LSTM-CRF architecture. li denotes the encoding of a word and its left context (forward LSTM) while ri includes only right context (backward LS"
D16-1153,P09-1116,0,0.0162238,"1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features while still maintaining a monolingual focus. Collobert et al. (20"
D16-1153,N15-1142,1,0.780835,"put sequence X is: Y ∗ = arg max S(X, Y ) (3) Y ∈YX Normally, evaluating the partition function over the exponential space of all possible labelings would be intractable. However, as described in (Lafferty et al., 2001), this can be done efficiently for linear chain CRFs using the forward backward algorithm. 2.4 Word Representations The inputs to our model are in the form of type level word representations (figure 2). Motivated by the distributional hypothesis (Harris, 1954; Firth, 1957) we use word embeddings as inputs. In the monolingual scenario, we use structured skipgram word embeddings (Ling et al., 2015a). For the transfer scenario, embeddings can optionally be trained using techniques like multi CCA described in (Ammar et al., 2016). By learning a linear transformation from a shared vector space between languages, the model may acquire some transfer capability to the target language. We use character bi-LSTMs to handle the Out Of Vocabulary (OOV) problem as in (Ling et al., 2015b). However, just as a distributional hypothesis exists for words, prior work (Tsvetkov and Dyer, 2015; Tsvetkov et al., 2015) suggests phonological character representations capture inherent similarities between cha"
D16-1153,L16-1529,1,0.78301,"al representations enables model transfer from one or more source languages to a target language with no extra effort, even when the languages use different scripts. We demonstrate that while attention over characters of words has marginal utility in monolingual and high resource settings, it greatly improves the statistical efficiency of the model in 0-shot and low resource transfer settings. We do require a mapping from a language’s script to phonological feature space which is script specific and not task specific. This presents little or no overhead due to existence of tools like PanPhon (Littell et al., 2016). 2 Our Approach Figure 1 provides a high level overview of our model. We model the words of a sentence at the type level and the token level. At the type level (ignorant of sentential context), we use bidirectional character LSTMs as in figure 2 to compose characters of a word to obtain its word representation and concatenate this with a word embedding that captures distributional semantics. This can memorize entities or capture morphological and suffixal clues 1463 Figure 2: Type level word representations - l denotes a word prefix encoding (by forward char LSTM) while r denotes a word suffi"
D16-1153,P16-1101,0,0.0170303,"lorian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features while still maintaining a monolingual focus. Collobert et al. (2011), Turian et al. (2010), and Ando and Zhang (2005) use uns"
D16-1153,W03-0430,0,0.142459,"incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features w"
D16-1153,W09-1119,0,0.0627792,", semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin and Wu, 2009; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features while still maintaining a"
D16-1153,W02-2024,0,0.69861,"Missing"
D16-1153,N15-1062,1,0.920448,"ul type level phenomena during inference and improves the statistical efficiency of the model in certain scenarios. Having described our intuitions, we now provide mathematical details of our model. 2.1 Figure 3: Use of Epitran and PanPhon to enable transfer across orthographies in Uyghur (Perso-Arabic script) and Turkish (Latin script), thus making the equivalence across scripts apparent. We concatenate the feature vectors from PanPhon and 1-hot encodings of the corresponding IPA characters and use these as inputs to the character bi-LSTMs. This shift to IPA space is motivated by prior work (Tsvetkov et al., 2015; Tsvetkov and Dyer, 2015) which demonstrated the value of projecting orthographic surface forms of words into a phonological space for detecting loan words, transliteration and cognates even in language pairs that exhibit significant typological, morphological and phonological differences. Our underlying assumption is that named entities are likely to be transliterated or retain pronunciation patterns across languages. Additionally, phenomena such as vowel harmony manifest explicitly in IPA representation and can potentially be helpful for NER. Foreign named entities for example, need not obe"
D16-1153,P10-1040,0,0.0188753,"ple et al., 2016; Yang et al., 2016; Ma and Hovy, 2016). CRFs eventually became more popular because they are discriminative models that directly model the required posterior probability of a labeling sequence using parametrized functions of features. They do not model the probability of the observed sentence itself, avoid Markovian independence assumptions made by HMMs and avoid the label bias problem. Most of the work cited so far makes use of hand engineered features. The following approaches minimize the use of features while still maintaining a monolingual focus. Collobert et al. (2011), Turian et al. (2010), and Ando and Zhang (2005) use unsupervised features in conjunction with engineered features capturing capitalization, character categories and gazetteer matches. Collobert et al. (2011) use a Convolutional Neural Network (CNN) over the sequence of word embeddings. Huang et al. (2015) instead use bi-directional LSTMs over the sequence of words, along with spelling and orthographic features. The most recent work eliminates feature engineering altogether and combines CRFs with LSTMs which can model long sequences while remembering appropriate past context. Lample et al. (2016) proposed an archi"
D16-1153,C96-1071,0,0.110928,"on the same Uyghur annotations. It is worth pointing out that this transfer was achieved across 3 languages each with different scripts, morphology, phonology and lexicons. 4 Prior Work NER is a well-studied sequence-labeling problem for which many different approaches have been proposed. Early works had a monolingual focus and relied heavily on feature engineering. Approaches include maximum entropy models (Chieu and Ng, 2003), hierarchically smoothed tries (Cucerzan and Yarowsky, 1999), decision trees (Carreras et al., 2002) and models incorporating syntactic, semantic and world knowledge (Wakao et al., 1996). Each of these models brings in a bias of its own. Florian et al. (2003) successfully tried ensembling multiple 1469 classifiers and improved performance. Since NER is a sequence labeling problem, there are local dependencies both among NE labels associated with words and among the words themselves, that could aid the labeling process. To explicitly deal with these sequential characteristics, Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became very popular. (Klein et al., 2003; Florian et al., 2003; McCallum and Li, 2003; Ratinov and Roth, 2009; Chandra et al., 1981; Lin a"
D18-1034,D16-1250,0,0.390551,"e 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have differ"
D18-1034,P17-1042,0,0.167017,"tional Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not availa"
D18-1034,D16-1153,1,0.872625,"g the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our methods in detail (§2.2), and provide some additional motivati"
D18-1034,Q17-1010,0,0.462655,"is shared space further, we iteratively perform a self-learning refinement step k 2 times by: 4. Train an NER model using the translated words along with the named entity tags from the English corpus (§2.2.4). We consider each in detail. 2.2.1 Learning Monolingual Embeddings Given text in the source and target language, we first independently learn word embedding matrices X and Y in the source and target languages respectively. These embeddings can be learned on monolingual text in both languages with any of the myriad of word embedding methods (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). 2.2.2 Learning Bilingual Embeddings Next, we learn a cross-lingual projection of X and Y into a shared space. Assume we are given a dictionary {xi , yi }D i=1 , where xi and yi denote the embeddings of a word pair. Let XD = [x1 , x2 , · · · , xD ]> and YD = [y1 , y2 , · · · , yD ]> denote two embedding matrices consisting of word pairs from the dictionary. Following previous work (Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017), we optimize the following objective: min W d X 2 kW xi − yi k s.t. W W > 1. Using the aligned embeddings to generate a new dictionary that consists of"
D18-1034,Q16-1026,0,0.0512909,"methods achieve state-of-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 36"
D18-1034,P12-1073,0,0.0773916,"guage with abundant entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit"
D18-1034,N16-1030,0,0.891813,"-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Bel"
D18-1034,I17-2016,0,0.0616114,"words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across different languages by sharing parameters to allow for knowledge transfer (Ammar et al., 2016a; Yang et al., 2017; Cotterell and Duh, 2017; Lin et al., 2018). However, such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through j"
D18-1034,P18-1074,0,0.126914,"es and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across different languages by sharing parameters to allow for knowledge transfer (Ammar et al., 2016a; Yang et al., 2017; Cotterell and Duh, 2017; Lin et al., 2018). However, such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Appr"
D18-1034,P11-1061,0,0.0854468,"some named entities are not covered by the dictionary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent"
D18-1034,R11-1017,0,0.479375,"gh-resource source language with abundant entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can"
D18-1034,P16-1101,0,0.605954,"gual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit features extracted from the surface forms (e.g. through character-level neural feature extractors), which has proven essential for high accuracy in the monolingual scenario (Ma and Hovy, 2016). However, these methods are largely predicated on the availability of large-scale parallel resources, and thus, their applicability to lowresource languages is limited. In contrast, it is also possible to learn lexical mappings through bilingual word embeddings (BWE). These bilingual embeddings can be obtained by using a small dictionary to project two sets of embeddings into a consistent space (Mikolov et al., 2013a; Faruqui and Dyer, For languages with no annotated resources, unsupervised transfer of natural language processing models such as named-entity recognition (NER) from resource-ric"
D18-1034,K16-1018,0,0.0300548,"onary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose"
D18-1034,D17-1269,0,0.420462,"Missing"
D18-1034,P17-2093,0,0.159085,"rpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our methods in detail (§2.2"
D18-1034,D11-1006,0,0.0876572,"ies is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches uti"
D18-1034,E14-1049,0,0.117455,"Missing"
D18-1034,P15-1119,0,0.192844,"ng the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our p"
D18-1034,P17-1135,0,0.671727,"Missing"
D18-1034,P14-1006,0,0.028839,"m our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar representations for aligned sentences (Hermann and Blunsom, 2014; Chandar et al., 2014). Jointlytrained models combine the common monolingual training objective with a cross-lingual training objective that often comes from parallel corpus (Zou et al., 2013; Gouws et al., 2015). Recently, unsupervised approaches also have been used to align two sets of word embeddings by learning a mapping through adversarial learning or selflearning (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). References 6 Acknowledgments We thank Stephen Mayhew for sharing the data, and Zihang Dai for meaningful discussion. This research was sponsored by Defense Advanc"
D18-1034,D14-1162,0,0.0789903,"efine the alignment in this shared space further, we iteratively perform a self-learning refinement step k 2 times by: 4. Train an NER model using the translated words along with the named entity tags from the English corpus (§2.2.4). We consider each in detail. 2.2.1 Learning Monolingual Embeddings Given text in the source and target language, we first independently learn word embedding matrices X and Y in the source and target languages respectively. These embeddings can be learned on monolingual text in both languages with any of the myriad of word embedding methods (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). 2.2.2 Learning Bilingual Embeddings Next, we learn a cross-lingual projection of X and Y into a shared space. Assume we are given a dictionary {xi , yi }D i=1 , where xi and yi denote the embeddings of a word pair. Let XD = [x1 , x2 , · · · , xD ]> and YD = [y1 , y2 , · · · , yD ]> denote two embedding matrices consisting of word pairs from the dictionary. Following previous work (Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017), we optimize the following objective: min W d X 2 kW xi − yi k s.t. W W > 1. Using the aligned embeddings to generate a new di"
D18-1034,P17-1161,0,0.02521,"languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for C"
D18-1034,N18-1202,0,0.0600342,", with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics art cross-lingu"
D18-1034,N16-1156,0,0.243386,"edia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our m"
D18-1034,D17-1035,0,0.019569,"tput of attention layer is defined as: Experimental Settings We evaluate our proposed methods on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain 4 European languages, English, German, Dutch and Spanish. For all experiments, we use English as the source language and translate its training data into the target language. We train a model on the translated data, and test it on the target language. For each experiment, we run our models 5 times using different seeds and report the mean and standard deviation, as suggested by Reimers and Gurevych (2017). Word Embeddings For all languages, we use two different embedding methods, fastText (Bojanowski et al., 2017) and GloVe (Pennington et al., 2014), to perform word-embedding based translations and train the NER model, respectively. For fastText, we use the publicly available embeddings trained on Wikipedia for all languages. For GloVe, we use the publicly available embeddings pre-trained on Gigaword and Wikipedia for English. For Spanish, German and Dutch, we use Spanish Gigaword and Wikipedia, German WMT News Crawl data and Wikipedia, and Dutch H a = softmax(QK > ) (E − I)H = [ha1 , ha2 , .."
D18-1034,P15-2064,0,0.261305,"y using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across differ"
D18-1034,D08-1063,0,0.0600569,"We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space"
D18-1034,D13-1141,0,0.0502521,"ges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar representations for aligned sentences (Hermann and Blunsom, 2014; Chandar et al., 2014). Jointlytrained models combine the common monolingual training objective with a cross-lingual training objective that often comes from parallel corpus (Zou et al., 2013; Gouws et al., 2015). Recently, unsupervised approaches also have been used to align two sets of word embeddings by learning a mapping through adversarial learning or selflearning (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). References 6 Acknowledgments We thank Stephen Mayhew for sharing the data, and Zihang Dai for meaningful discussion. This research was sponsored by Defense Advanced Research Projects Agency Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA/I2O under Contract No. HR0011-15-C011"
D18-1034,Q13-1001,0,0.337503,"Missing"
D18-1034,N12-1052,0,0.317254,"Missing"
D18-1034,W02-2024,0,0.222579,"rs similar to those seen at training time, which we posit introduces a level of flexibility with respect to the word order, and thus may allow for better generalization. Let H = [h1 , h2 , · · · , hn ]> be a sequence of word-level hidden representations. We apply a single layer MLP on H to obtain the queries Q and keys K = tanh(HW + b), where W ∈ Rd×d is a parameter matrix and b ∈ Rd is a bias term, with d being the hidden state size. The output of attention layer is defined as: Experimental Settings We evaluate our proposed methods on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain 4 European languages, English, German, Dutch and Spanish. For all experiments, we use English as the source language and translate its training data into the target language. We train a model on the translated data, and test it on the target language. For each experiment, we run our models 5 times using different seeds and report the mean and standard deviation, as suggested by Reimers and Gurevych (2017). Word Embeddings For all languages, we use two different embedding methods, fastText (Bojanowski et al., 2017) and GloVe (Pennington et a"
D18-1034,W03-0419,0,0.350689,"Missing"
D18-1034,K16-1022,0,0.679344,"umber with 0 when used as input to the character level Bi-LSTM. Network Training We use SGD with momentum to train the NER model for 30 epochs, and select the best model on the target language development set. We choose the initial learning rate 4.2.1 Comparison with Dictionary-Based Translation Table 1 also presents results of a comparison between our proposed BWE translation method and the “cheap translation” baseline of (Mayhew et al., 2017). The size of the dictionaries used by both 4 https://github.com/facebookresearch/ MUSE 374 Model ∗ T¨ackstr¨om et al. (2012) ∗ Nothman et al. (2013) ∗ Tsai et al. (2016) ∗ Ni et al. (2017) ∗† Mayhew et al. (2017) ∗ Mayhew et al. (2017) (only Eng. data) Our methods: BWET (id.c.) BWET (id.c.) + self-att. BWET (adv.) BWET (adv.) + self-att. BWET BWET + self-att. ∗ BWET on data from Mayhew et al. (2017) ∗ BWET + self-att. on data from Mayhew et al. (2017) ∗ Our supervised results Spanish 59.30 61.0 60.55 65.10 65.95 51.82 Dutch 58.40 64.00 61.60 65.40 66.50 53.94 German 40.40 55.80 48.10 58.50 59.11 50.96 Extra Resources parallel corpus Wikipedia Wikipedia Wikipedia, parallel corpus, 5K dict. Wikipedia, 1M dict. 1M dict. 71.14 ± 0.60 72.37 ± 0.65 70.54 ± 0.85 71."
D18-1034,Q14-1005,0,0.030546,"t entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit features extracted from"
D18-1034,H01-1035,0,0.339379,"esource languages when some named entities are not covered by the dictionary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to trai"
D18-1034,P17-1179,0,0.166689,"ociation for Computational Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surfac"
D18-1196,S07-1018,0,0.0381527,"eir POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 2015; FitzGerald et al., 2015; Roth, 2016). Systems in both paradigms are constrained by their underlying representations. PropBank covers only verbs and certain nominal and adjectival predicates. FrameNet’s frame-evoking elements are broader, including verbs, prepositions, a"
D18-1196,W04-2412,0,0.0590212,"cantly outperforms prior construction-based work on predicting causal frames (§7). Finally, we discuss how the transition system and tagger model could be adapted to more difficult SCL tasks (§8). 1691 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1691–1701 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 2.1 Background and related work Shallow semantic parsing SCL of course inherits from SSP, which has a venerable tagging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-"
D18-1196,W05-0620,0,0.124058,"Missing"
D18-1196,C14-1078,0,0.0477612,"Missing"
D18-1196,P11-2121,0,0.0145517,"transcripts fully annotated for causal language. The only prior work on construction-based semantic parsing that we know of is Causeway (Dunietz et al., 2017a), also based on the B ECAUSE corpus. Causeway detects causal connectives using lexico-syntactic patterns, then applies heuristics and classifiers to tag arguments and remove false positives. It achieves moderate performance, but requires extensive tuning and feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al"
D18-1196,W11-0906,0,0.700939,"transcripts fully annotated for causal language. The only prior work on construction-based semantic parsing that we know of is Causeway (Dunietz et al., 2017a), also based on the B ECAUSE corpus. Causeway detects causal connectives using lexico-syntactic patterns, then applies heuristics and classifiers to tag arguments and remove false positives. It achieves moderate performance, but requires extensive tuning and feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al"
D18-1196,J14-1002,0,0.0192042,"pular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 2015; FitzGerald et al., 2015; Roth, 2016). Systems in both paradigms are constrained by their underlying representations. PropBank covers only verbs and certain nominal and adjectival predicates. FrameNet’s frame-evoking elements are broader, including verbs, prepositions, adverbs, conjunction"
D18-1196,Q17-1009,1,0.799106,"er, including verbs, prepositions, adverbs, conjunctions, and even some MWEs, but must still be single words or MWEs that act like words. As Table 1 demonstrates, some semantic domains, such as causality, demand a more flexible approach. 2.2 Construction grammar CxG, which posits that the fundamental units of language are CONSTRUCTIONS—pairings of meanings with arbitrary linguistic forms. For instance, so X that Y (example 3) is characterized by a single construction, where the form is so hadjective Xi hfinite clausal complement Y i and the meaning is X to an extreme that causes Y . Following Dunietz et al. (2017a,b), we borrow two core insights of CxG: first, that morphemes, words, MWEs, and grammar are all on equal footing as “learned pairings of form and function” (Goldberg, 2013); and second, that constructions pair patterns of surface forms directly with meanings. Thus, we can tag any surface realizations of constructions as meaning-bearing triggers (hence “surface construction labeling”). 2.3 Causal language To test the SCL approach, we examine causal language, which conveys essential semantic information and is especially rich in constructional triggers. Our data representation for causal langu"
D18-1196,W17-0812,1,0.836546,"er, including verbs, prepositions, adverbs, conjunctions, and even some MWEs, but must still be single words or MWEs that act like words. As Table 1 demonstrates, some semantic domains, such as causality, demand a more flexible approach. 2.2 Construction grammar CxG, which posits that the fundamental units of language are CONSTRUCTIONS—pairings of meanings with arbitrary linguistic forms. For instance, so X that Y (example 3) is characterized by a single construction, where the form is so hadjective Xi hfinite clausal complement Y i and the meaning is X to an extreme that causes Y . Following Dunietz et al. (2017a,b), we borrow two core insights of CxG: first, that morphemes, words, MWEs, and grammar are all on equal footing as “learned pairings of form and function” (Goldberg, 2013); and second, that constructions pair patterns of surface forms directly with meanings. Thus, we can tag any surface realizations of constructions as meaning-bearing triggers (hence “surface construction labeling”). 2.3 Causal language To test the SCL approach, we examine causal language, which conveys essential semantic information and is especially rich in constructional triggers. Our data representation for causal langu"
D18-1196,P15-1033,0,0.473103,"ased semantic parsing that we know of is Causeway (Dunietz et al., 2017a), also based on the B ECAUSE corpus. Causeway detects causal connectives using lexico-syntactic patterns, then applies heuristics and classifiers to tag arguments and remove false positives. It achieves moderate performance, but requires extensive tuning and feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and co"
D18-1196,D15-1112,0,0.0129848,"oth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 2015; FitzGerald et al., 2015; Roth, 2016). Systems in both paradigms are constrained by their underlying representations. PropBank covers only verbs and certain nominal and adjectival predicates. FrameNet’s frame-evoking elements are broader, including verbs, prepositions, adverbs, conjunctions, and even some MWEs, but must still be single words or MWEs that act like words. As Table 1 demonstrates, some semantic domains, such as causality, demand a more flexible approach. 2.2 Construction grammar CxG, which posits that the fundamental units of language are CONSTRUCTIONS—pairings of meanings with arbitrary linguistic form"
D18-1196,S15-1033,0,0.0166454,"ow semantic parsing SCL of course inherits from SSP, which has a venerable tagging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to sc"
D18-1196,W09-1201,0,0.0669418,"Missing"
D18-1196,W08-2122,0,0.0401636,"d feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and coreference resolution. Vilares and Gómez-Rodríguez (2018) develop a transition system for Abstract Meaning Representation parsing, and TUPA (Hershcovich et al., 2017) does the same for Universal Conceptual Cognitive Annotation. Both can handle discontinuous or reentrant graph structures. Most directly relevant to DeepCx is Choi and Pa"
D18-1196,P17-1104,0,0.0263383,"STM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and coreference resolution. Vilares and Gómez-Rodríguez (2018) develop a transition system for Abstract Meaning Representation parsing, and TUPA (Hershcovich et al., 2017) does the same for Universal Conceptual Cognitive Annotation. Both can handle discontinuous or reentrant graph structures. Most directly relevant to DeepCx is Choi and Palmer’s (2011b) work, which defines a novel transition system for PropBank parsing. Our similar scheme for parsing causal constructions builds on this one, extending it for cases where the spans are not contiguous. 3 The SCL task for causal language An SCL task closely resembles an SSP task, except that the triggers can be complex constructions. As a corollary, the arguments can also be discontinuous and/or overlap with each ot"
D18-1196,P14-5010,0,0.00441969,"Missing"
D18-1196,H94-1020,0,0.418482,".1 5.2.4 Embedding the action history During training, DeepCx learns vector representations of each action. To embed the action history, these action embeddings are fed as inputs into yet another LSTM cell. This LSTM’s output is the embedding of the history thus far. 5.3 Implementation details DeepCx is implemented using a refactored version of the LSTM parser codebase that performs identically to the original.2 The neural network framework, which also underlies the LSTM parser, is an early version of DyNet (Neubig et al., 2017). The LSTM parser model is pretrained on the usual Penn Treebank (Marcus et al., 1994) sections (training: 02–21; development: 22). For w, ˜ we use the same “structured skip ngram” word embeddings as the LSTM parser. See Dyer et al. (2015) for details about the embedding approach, hyperparameters, and training corpora. DeepCx gives no special treatment to out-of-vocabulary items, other than using the 0 vector for words not included in the pretrained embeddings. 1 This embedding is similar to that proposed by Roth and Lapata (2016). However, their dependency paths include the words encountered along the way and their POS tags. We experimented with adding these elements to our de"
D18-1196,W08-2121,0,0.0384448,"ion-based work on predicting causal frames (§7). Finally, we discuss how the transition system and tagger model could be adapted to more difficult SCL tasks (§8). 1691 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1691–1701 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 2.1 Background and related work Shallow semantic parsing SCL of course inherits from SSP, which has a venerable tagging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficu"
D18-1196,K16-1019,0,0.0122846,"ansition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and coreference resolution. Vilares and Gómez-Rodríguez (2018) develop a transition system for Abstract Meaning Representation parsing, and TUPA (Hershcovich et al., 2017) does the same for Universal Conceptual Cognitive Annotation. Both can handle discontinuous or reentrant graph structures. Most directly relevant to DeepCx is Choi and Palmer’s (2011b) work, which defi"
D18-1196,Q15-1003,0,0.0155725,"which has a venerable tagging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 201"
D18-1196,N18-2023,0,0.0136457,"-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and coreference resolution. Vilares and Gómez-Rodríguez (2018) develop a transition system for Abstract Meaning Representation parsing, and TUPA (Hershcovich et al., 2017) does the same for Universal Conceptual Cognitive Annotation. Both can handle discontinuous or reentrant graph structures. Most directly relevant to DeepCx is Choi and Palmer’s (2011b) work, which defines a novel transition system for PropBank parsing. Our similar scheme for parsing causal constructions builds on this one, extending it for cases where the spans are not contiguous. 3 The SCL task for causal language An SCL task closely resembles an SSP task, except that the triggers can"
D18-1196,J08-4003,0,0.00915874,"icles and Congressional hearing transcripts fully annotated for causal language. The only prior work on construction-based semantic parsing that we know of is Causeway (Dunietz et al., 2017a), also based on the B ECAUSE corpus. Causeway detects causal connectives using lexico-syntactic patterns, then applies heuristics and classifiers to tag arguments and remove false positives. It achieves moderate performance, but requires extensive tuning and feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsin"
D18-1196,P16-1113,0,0.0739347,"gging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 2015; FitzGerald et al., 20"
D18-1366,E17-1088,1,0.86445,"iment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, whi"
D18-1366,E17-2067,0,0.445484,"haracter ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embeddings are trained on morphological representations (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015). Avraham and Goldberg (2017) explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called “tags”) for training the word embeddings. Lemmas capture information about the lexical identity of a word and are closely correlated with the semantics of a word; tags capture information about the syntactic context of a word. See Figure 1 for an example. We take inspiration from the above work in adapting these subword units for cross-lingual transfer. Phonological units: Subword units other than tags might seem to be of no use in closely-related languages with diﬀerent scri"
D18-1366,D16-1153,1,0.878386,"., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of limited quantity and uncert"
D18-1366,D16-1136,0,0.0268979,"tations for the downstream task. These two regimes are described below: CT-Joint: This model explicitly maps the word representations of the two languages into the same space by training simultaneously on both. This is achieved simply by combining the corpora of both the high-resource and the low-resource language and training jointly using the skip-gram objective, discussed above. The central intuition is as follows: once two related languages are placed in the same phonological and morphological space, they will share many subword units in common and this will make joint training proﬁtable. Duong et al. (2016) and Gouws et al. (2015) have previously shown the advantages of joint training and we observe this to be true in our case as well. CT-FineTune: This model implicitly maps the word representations of the two languages into the same space. The model attempts this by taking the learned continuous representations of the high resource subword units, referred to by x?? ??? , and uses them to initialize the model for the low resource language. The model is ﬁrst trained using all subword units on the high resource language and the learned representations are then used for initializing the subword uni"
D18-1366,P08-1088,0,0.016977,"et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of lim"
D18-1366,W17-4110,0,0.0301722,"s. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword informa"
D18-1366,D17-1302,0,0.026782,"ive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword information from resource-rich languages, especially through phonological and morphological representations. CT-Joint and CTFineTune do not require morphological analyzers, but we have found that even a morphological analyzer built in 2-3 weeks can boost performance an"
D18-1366,Q15-1016,0,0.0420122,"t tokens, within a speciﬁed window of the focus word ?? and ?(?|?? ) is the probability of observing context word ? given focus word ?? . The skipgram was originally deﬁned using the softmax function: ?(?|?? ) = ??(?,?? ) ?(?? ,?) ∑? ?=1 ? (2) where ? is a scoring function mapping ? and ?? to ℝ. The summation in the denominator is over the entire vocabulary ? which makes this formulation computationally ineﬃcient as cost of gradient computation is proportional to ? which is quite large (∼ 106 ). Mikolov et al. (2013b) hence employ negative sampling to make this computation eﬃcient and robust (Levy et al., 2015) and give better representations for infrequent words4 , which is crucial for the low resource settings. Negative sampling represents the above objective function (Equation 1) using a binary logistic loss as shown below: ? 3. We produce continuous representations for each subword unit, giving researchers the ability to use them in their own tasks as they see ﬁt. The code 1 for training word embeddings and the embeddings 2 which produced the best results are publicly available. We also release morphological analyzers for Hindi and Bengali3 . 2 Skipgram Objective https://github.com/Aditi138/Embe"
D18-1366,D15-1176,0,0.0397207,"g approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of limited quantity and uncertain quality. In this paper, we take a diﬀerent task: focusing instead on the similarity of the surface forms, phonology, or morphology of the two transfer languages. Speciﬁcally, inspired by Ling et al. (2015), who demonstrate the eﬀectiveness of character-level modeling for knowledge sharing in multilingual scenarios, we propose two approaches to transfer word embeddings using diﬀerent types of linguistically-inspired subword-level information. Both approaches focus on mapping the low resource language embeddings closer to those of the high resource language and are executed using two diﬀerent training regimes. We explore the eﬀect of diﬀerent subword units— characters, lemmas, inﬂectional properties, and phonemes— as each one oﬀers a unique linguistic insight, discussed more in Section 3. Our pro"
D18-1366,W13-3512,0,0.534869,"haracter-level modeling by representing the focus word ?? as a set of its character ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embeddings are trained on morphological representations (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015). Avraham and Goldberg (2017) explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called “tags”) for training the word embeddings. Lemmas capture information about the lexical identity of a word and are closely correlated with the semantics of a word; tags capture information about the syntactic context of a word. See Figure 1 for an example. We take inspiration from the above work in adapting these subword units for cross-lingual transfer. Phonological units: Subword units other"
D18-1366,P16-1101,0,0.272374,"r approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU. 1 Introduction Continuous word representations have demonstrated utility in state-of-the-art neural models for several NLP tasks, such as named entity recognition (NER; Ma and Hovy (2016)), machine reading (Tan et al., 2017), sentiment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high depen"
D18-1366,I13-1136,0,0.066857,"Missing"
D18-1366,D17-1269,0,0.120208,"Missing"
D18-1366,L18-1429,1,0.883211,"Missing"
D18-1366,W18-1818,1,0.81319,"l trained with charngrams+lemma+morph ngrams+lemma+morph and monolingual: char-ngram+lemma+morph). The diﬀerence is striking—in the monolingual condition, the NEs are widely dispersed, but in the bilingual condition, the NEs cluster together. This suggests that phonologically-mediated transfer through Turkish is resulting in embeddings in which NEs are close to one another, relative to monolingual Uyghur embeddings. 5.4 Machine Translation Task In addition to NER, we test the performance of our proposed approaches on the MT task to test generality of our conclusions. We use XNMT toolkit 3292 (Neubig et al., 2018) to translate sentences from the low-resource language to English. We run similar transfer and monolingual experiments as done for NER. Due to space limitations, we use select subword combinations for the experiments, details of which can be found in Appendix. BLEU is used as the evaluation metric. From Table 6, we observe that the combination of character-ngrams and lemma performs the best for Uyghur (+0.1) and the combination of character-ngrams, lemma and morph gives the best performance for Bengali (+1.7), over the word baseline, which demonstrates the importance of subword units for low-r"
D18-1366,D14-1162,0,0.084187,"that of NER. We hypothesize that this is because the MT models were trained on a training set that did not have translation pairs from the high resource language. As Qi et al. (2018) note, when training MT systems on a single language pair, it is less necessary for the embeddings to be coordinated across the languages. 6 Related Work Word Embedding Models: Most algorithms for learning embeddings take inspiration from language modeling (Bengio et al., 2003), motivated by distributional hypothesis (Harris, 1954), and employ a shallow neural network to map the words into a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment a"
D18-1366,N18-1202,0,0.0322711,"ge. As Qi et al. (2018) note, when training MT systems on a single language pair, it is less necessary for the embeddings to be coordinated across the languages. 6 Related Work Word Embedding Models: Most algorithms for learning embeddings take inspiration from language modeling (Bengio et al., 2003), motivated by distributional hypothesis (Harris, 1954), and employ a shallow neural network to map the words into a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word represe"
D18-1366,C16-1018,0,0.0442983,"Missing"
D18-1366,K16-1022,0,0.0265297,"t al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword information from resource-rich languages, especially through phonological and morphological representations. CT-Joint and CTFineTune do"
D18-1366,D16-1157,0,0.0138983,"⟨⟩قارىيالمايدۇ /qarijalmajdu/ /qari-jal-ma-jdu/ qari+Verb+Pot+Neg+Pres+A3sg ‘s/he can’t care for’ Figure 1: Representations of a word in Uyghur morphologically rich languages such as Turkish, Uyghur, Hindi, and Bengali. Although, given a large enough training corpus, most or all morphological forms of a lexeme (of which there may be many) could theoretically learn to have similar vector representations, it will be vastly more data efﬁcient if we can take into account regularities of their form to model morphology explicitly. We explore the following methods for doing so: Orthographic units: Wieting et al. (2016) and Bojanowski et al. (2016) show the utility of character-level modeling by representing the focus word ?? as a set of its character ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embedd"
D18-1366,N16-1119,0,0.0259393,"a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al."
D18-1366,D16-1163,0,0.0326908,"guage. The model is ﬁrst trained using all subword units on the high resource language and the learned representations are then used for initializing the subword units for the low resource language. To elucidate which pretrained subword helped the most on the low resource language, we use the same model for diﬀerent experiments, which is trained using all subword units—phoneme-ngrams, lemma and morphological properties. The linguistic intuition behind CTFineTune is similar to that behind CT-Joint. This idea of transferring parameters from high resource language has been previously explored by Zoph et al. (2016) for low resource neural machine translation which showed considerable improvement. 5 Evaluation In this section, we ﬁrst describe the model setup for training word embeddings followed by details on NER and MT experiments. 5.1 Implementation details We base our model on the C++ implementation of fasttext5 (Bojanowski et al., 2016) with modiﬁcations as described above. Data: We represent a word in the training corpus using the format presented by Avraham and Goldberg (2017). For instance, the Uyghur word in Figure 1 is represented as follows: phoneme ipa: qarijalmajdu, lemma l:qari, and morphol"
D18-1366,N18-2084,1,0.921313,"ource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU. 1 Introduction Continuous word representations have demonstrated utility in state-of-the-art neural models for several NLP tasks, such as named entity recognition (NER; Ma and Hovy (2016)), machine reading (Tan et al., 2017), sentiment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown"
D18-1366,N15-1140,0,\N,Missing
D18-1538,W13-3820,0,0.0356453,"Missing"
D18-1538,P10-1025,0,0.213251,"as a separate subtask and would be formalized as a constrained optimization problem. To solve this problem ILP (Punyakanok et al., 2008), A* algorithm (He et al., 2017) and gradient-based inference (Lee et al., 2017) were employed. Further, all of these works leveraged syntactic parse during inference and was never used during training unless used as a cascaded system. To the best of our knowledge, this work is the first attempt towards SSL span-based SRL model. Nonetheless, there were few efforts in SSL in dependency-based SRL systems (F¨urstenau and Lapata, 2009; Deschacht and Moens, 2009; Croce et al., 2010). (F¨urstenau and Lapata, 2009) proposed to augment the dataset by finding similar unlabeled sentences to already labeled set and annotate accordingly. While interesting, the similar augmentation technique is harder to apply to spanbased SRL as one requires to annotate the whole span. (Deschacht and Moens, 2009; Croce et al., 2010) proposed to leverage the relation between words by learning latent word distribution over the context, i.e. language model. Our paper also incorporates this idea by using ELMo as it is trained via language model objective. 5 Conclusion and Future Work We presented a"
D18-1538,D09-1003,0,0.171862,"Missing"
D18-1538,E09-1026,0,0.0833578,"Missing"
D18-1538,P17-1044,0,0.223296,"es the arguments corresponding to each clause or proposition, i.e. its semantic roles, based on lexical and positional information. SRL labels non-overlapping text spans corresponding to typical semantic roles such as Agent, Patient, Instrument, Beneficiary, etc. This task finds its use in many downstream applications such as question-answering (Shen and Lapata, 2007), information extraction (Bastianelli et al., 2013), machine translation, etc. Several SRL systems relying on large annotated corpora have been proposed (Peters et al., 2018; ⇤ Equal contribution, name order decided by coin flip. He et al., 2017), and perform relatively well. A more challenging task is to design an SRL method for low resource scenarios (e.g. rare languages or domains) where we have limited annotated data but where we may leverage annotated data from related tasks. Therefore, in this paper, we focus on building effective systems for low resource scenarios and illustrate our system’s performance by simulating low resource scenarios for English. SRL systems for English are built using large annotated corpora of verb predicates and their arguments provided as part of the PropBank and OntoNotes v5.0 projects (Kingsbury and"
D18-1538,W05-0634,0,0.110259,"Missing"
D18-1538,W13-3516,0,0.126013,"Missing"
D18-1538,J08-2005,0,0.439912,"ibilities (⌦n ) using the y scoring function f (x, y), which incorporates log probability and structural penalty terms. The details of scoring function is on Appendix Eq.(7). ˆ = arg max f (x, y0 ) y y0 2⌦n 2.3 (2) Structural Constraints There are different types of structural constraints: BIO, SRL and syntactic constraints. BIO constraints define valid BIO transitions for sequence tagging. For example, B-ARG0 cannot be followed by I-ARG1. SRL constraints define rules on the role level and has three particular constraints: unique core roles (U), continuation roles (C) and reference roles (R) (Punyakanok et al., 2008). Lastly, syntactic constraints state that srlspans(y) have to be subset of parse-spans(x). (He et al., 2017) use BIO and syntactic constraints at decoding time by solving Eq.(2) where f (x, y) incorporates those constraints and report that SRL constraints do not show significant improvements over the ensemble model. In particular, by using syntactic constraints, (He et al., 2017) achieves up to +2 F1 score on CoNLL-2005 dataset via A* decoding. Improvements of SRL system via use of syntactic constraints is consistent with other observations (Punyakanok et al., 2008). However, all previous wor"
D18-1538,D07-1002,0,0.151293,"Missing"
D18-1538,P15-1109,0,0.0237199,"and their arguments provided as part of the PropBank and OntoNotes v5.0 projects (Kingsbury and Palmer, 2002; Pradhan et al., 2013). These corpora are built by adding semantic role annotations to the constituents of previously-annotated syntactic parse trees in the Penn Treebank (Marcus et al., 1993). Traditionally, SRL relies heavily on using syntactic parse trees either from shallow syntactic parsers (chunkers) or full syntactic parsers and Punyakanok et al. shows significant improvements by using syntactic parse trees. Recent breakthroughs motivated by end-to-end deep learning techniques (Zhou and Xu, 2015; He et al., 2017) achieve state-of-the-art performance without leveraging any syntactic signals, relying instead on ample role-label annotations. We hypothesize that by leveraging syntactic structure while training neural SRL models, we may achieve robust performance, especially for low resource scenarios. Specifically, we propose to leverage syntactic parse trees as hard constraints for the SRL task i.e., we explicitly enforce that the predicted argument spans of the SRL network must agree with the spans implied by the syntactic parse of the sentence via scoring function in the training obje"
D18-1538,kingsbury-palmer-2002-treebank,0,0.168432,"et al., 2017), and perform relatively well. A more challenging task is to design an SRL method for low resource scenarios (e.g. rare languages or domains) where we have limited annotated data but where we may leverage annotated data from related tasks. Therefore, in this paper, we focus on building effective systems for low resource scenarios and illustrate our system’s performance by simulating low resource scenarios for English. SRL systems for English are built using large annotated corpora of verb predicates and their arguments provided as part of the PropBank and OntoNotes v5.0 projects (Kingsbury and Palmer, 2002; Pradhan et al., 2013). These corpora are built by adding semantic role annotations to the constituents of previously-annotated syntactic parse trees in the Penn Treebank (Marcus et al., 1993). Traditionally, SRL relies heavily on using syntactic parse trees either from shallow syntactic parsers (chunkers) or full syntactic parsers and Punyakanok et al. shows significant improvements by using syntactic parse trees. Recent breakthroughs motivated by end-to-end deep learning techniques (Zhou and Xu, 2015; He et al., 2017) achieve state-of-the-art performance without leveraging any syntactic sig"
D18-1538,W05-0625,0,0.110745,"Missing"
D18-1538,J93-2004,0,0.0604661,"re we may leverage annotated data from related tasks. Therefore, in this paper, we focus on building effective systems for low resource scenarios and illustrate our system’s performance by simulating low resource scenarios for English. SRL systems for English are built using large annotated corpora of verb predicates and their arguments provided as part of the PropBank and OntoNotes v5.0 projects (Kingsbury and Palmer, 2002; Pradhan et al., 2013). These corpora are built by adding semantic role annotations to the constituents of previously-annotated syntactic parse trees in the Penn Treebank (Marcus et al., 1993). Traditionally, SRL relies heavily on using syntactic parse trees either from shallow syntactic parsers (chunkers) or full syntactic parsers and Punyakanok et al. shows significant improvements by using syntactic parse trees. Recent breakthroughs motivated by end-to-end deep learning techniques (Zhou and Xu, 2015; He et al., 2017) achieve state-of-the-art performance without leveraging any syntactic signals, relying instead on ample role-label annotations. We hypothesize that by leveraging syntactic structure while training neural SRL models, we may achieve robust performance, especially for"
D18-1538,N18-1202,0,0.247927,"ction Semantic role labeling (SRL), a.k.a shallow semantic parsing, identifies the arguments corresponding to each clause or proposition, i.e. its semantic roles, based on lexical and positional information. SRL labels non-overlapping text spans corresponding to typical semantic roles such as Agent, Patient, Instrument, Beneficiary, etc. This task finds its use in many downstream applications such as question-answering (Shen and Lapata, 2007), information extraction (Bastianelli et al., 2013), machine translation, etc. Several SRL systems relying on large annotated corpora have been proposed (Peters et al., 2018; ⇤ Equal contribution, name order decided by coin flip. He et al., 2017), and perform relatively well. A more challenging task is to design an SRL method for low resource scenarios (e.g. rare languages or domains) where we have limited annotated data but where we may leverage annotated data from related tasks. Therefore, in this paper, we focus on building effective systems for low resource scenarios and illustrate our system’s performance by simulating low resource scenarios for English. SRL systems for English are built using large annotated corpora of verb predicates and their arguments pr"
D19-1520,P17-1042,0,0.0334043,"iew can be seen in Figure 1. In the following sections, we describe each of these three steps in detail. 2.1 Cross-lingual Transfer Learning The goal of cross-lingual learning is to take a recognizer trained in a source language, and transfer it to a target language. Our approach to doing so for NER follows that of Xie et al. (2018), and we provide a brief review in this section. To begin with, we assume access to two sets of pre-trained monolingual word embeddings in the source and target languages, X and Y , one small bilingual lexicon, either provided or obtained in an unsupervised manner (Artetxe et al., 2017; Conneau et al., 2017a), and labeled training data in the source language. Using these resources, we train bilingual word embeddings (BWE) to create a word-to-word translation dictionary, and finally 5165 use this dictionary to translate the source training data into the target language, which we use to train an NER model. To learn BWE, we first obtain a linear mapping W by solving the following objective: W ∗ = arg min kW XD − YD kF s.t. W W &gt; = I, W where XD and YD correspond to the aligned word embeddings from the bilingual lexicon. F denotes the Frobenius norm. We can first compute the P"
D19-1520,D09-1031,0,0.0436874,"oaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation. The most s"
D19-1520,D16-1153,1,0.842591,"in the same amount of time, in the future we plan to explore mixed-mode annotation where we choose either full sequences or spans for annotation. 4 Related Work Cross-Lingual Transfer: Transferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, where"
D19-1520,D18-1366,1,0.834225,"me, in the future we plan to explore mixed-mode annotation where we choose either full sequences or spans for annotation. 4 Related Work Cross-Lingual Transfer: Transferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artie"
D19-1520,P17-1171,0,0.020423,"ation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data. The code is publicly available here.1 1 Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work"
D19-1520,N04-4028,0,0.0219771,"etup: We use cross-lingual transfer (§2.1) to train our initial NER model and test on the target language. This is the same setting as Xie et al. (2018) and serves as our baseline. Then we use several active learning strategies to select data for manual annotation using this trained NER model. We compare our proposed ETAL strategy with the following baseline strategies: SAL: Select whole sequences for which the model has least confidence in the most likely labeling (Culotta and McCallum, 2005). CFEAL: Select least confident spans within a sequence using the confidence field estimation method (Culotta and McCallum, 2004). RAND: Select spans randomly from the unlabeled set for annotation. In this experimental setting, we simulate manual annotation by using gold labels for the data selected by active learning. At each subsequent run, we annotate 200 tokens and fine-tune the NER model on all the data acquired so far, which is then used to select data for the next run of annotation. 3.2.1 Results Figure 2 summarizes the results for all datasets across the different experimental settings. Each data-point on the x-axis corresponds to the NER performance after annotating 200 additional tokens. CT denotes using cross"
D19-1520,R11-1017,0,0.0258073,": AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation. The most similar work to ours perhaps is that of 5171 (a) Selected spans using ETAL strategy are highlighted for the human annotator to annotate. (b) Human annotator correcting the span boundary and assigning the correct entity type. (c) Human annotator assigning the correct entity type only since selected span boundary is correct. (d) Partially-annotated sequences after being annotated by the human annotator."
D19-1520,P17-2093,0,0.0594355,"ges of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation. The most similar work to ours perhaps is that of 5171 (a) Selected spans using ETAL strategy are highlighted for the human annotator to annotate. (b) Human annotator correcting the span boundary and assigning the correct entity type. (c) Human annotator assigning the correct entity type only since selected span boundary is correct. (d) Partially-annotated sequences after being annotated by the human annotator. Figure 5: Example of the human annotation process for Hindi. Fang and Cohn (2017), which selects informative word types for low-resource POS tagging. However, their method requires the annotator to annotate single tokens, which is not trivially applicable for multi-word entities in practical settings. 5 Conclusion In this paper, we presented a study on how to efficiently bootstrap NER systems for low-resource languages using a combination of cross-lingual transfer learning and active learning. We conducted both simulated and human annotation experiments across different languages and found that: 1) cross-lingual transfer is a powerful tool, constantly beating systems witho"
D19-1520,N13-1014,0,0.0705967,"target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages of training a model on this partially labeled data. All above methods focus on either token or full sequence annotation. The most similar work to ours perhaps is that of 5171 (a) Selected spans using ETAL strategy are highlighted for th"
D19-1520,D12-1010,0,0.0273269,"Missing"
D19-1520,W04-3250,0,0.352316,"Missing"
D19-1520,N16-1030,0,0.0106021,"y available here.1 1 Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recogn"
D19-1520,W17-2314,0,0.0209704,"ap NER systems for low-resource languages using a combination of cross-lingual transfer learning and active learning. We conducted both simulated and human annotation experiments across different languages and found that: 1) cross-lingual transfer is a powerful tool, constantly beating systems without using transfer; 2) our proposed recipe works the best among known active learning baselines; 3) our proposed active learning strategy saves annotator much effort while ensuring high quality. In future, to account for different levels of annotator expertise, we plan to combine proactive learning (Li et al., 2017) with our proposed method. Acknowledgement The authors would like to thank Sachin Kumar, Kundan Krishna, Aldrian Obaja Muis, Shirley Anugrah Hayati, Rodolfo Vega and Ramon Sanabria for participating in the human annotation experiments. This work is sponsored by Defense Advanced Research Projects Agency Information Innovation Office (I2O). Program: Low Resource Languages for Emergent Incidents (LORELEI). Issued by DARPA/I2O under Contract No. HR0011-15-C0114. This research was supported also in part by DARPA grant FA875018-2-0018 funded under the AIDA program. The views and conclusions containe"
D19-1520,P16-1101,0,0.316991,"The code is publicly available here.1 1 Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-r"
D19-1520,D14-1097,0,0.123437,"dvances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Specifically, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efficiency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is fine-tuned on data obtained using active learning to improve accuracy in the target language. Within this recipe, the choice of specific method for choosing and annotating data within active learning is highly important to minimize human effort. One relatively standard method used in previous work on NER is to select full sequences based on a criterion for the uncertainty of the entities recognized therein (Culotta and McCallum, 2005"
D19-1520,D17-1269,0,0.0303035,"Missing"
D19-1520,P11-2093,1,0.898661,"सु ीम कोट नेमांगा जवाब BORGIORG Cross-Lingual Transfer Learning English labeled dataset label Target language unlabeled dataset Target Language labeled dataset query spans ू ल और श क क कमी पर सु ीम कोट नेमांगा जवाब Active Learning Figure 1: Our proposed recipe: cross-lingual transfer is used for projecting annotations from an English labeled dataset to the target language. Entity-targeted active learning is then used to select informative sub-spans which are likely entities for humans to annotate. Finally, the NER model is fine-tuned on this partially-labeled dataset. sentence is of interest (Neubig et al., 2011; Sperber et al., 2014). Inspired by this finding and considering the fact that named entities are both important and sparse, we propose an entity-targeted strategy to save annotator effort. Specifically, we select uncertain subspans of tokens within a sequence that are most likely named entities. This way, the annotators only need to assign types to the chosen subspans without having to read and annotate the full sequence. To cope with the resulting partial annotation of sequences, we apply a constrained version of conditional random fields (CRFs), partial CRFs, during training that only lear"
D19-1520,N18-1202,0,0.0119744,"Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use c"
D19-1520,D08-1112,0,0.75947,"ally, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Specifically, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efficiency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is fine-tuned on data obtained using active learning to improve accuracy in the target language. Within this recipe, the choice of specific method for choosing and annotating data within active learning is highly important to minimize human effort. One relatively standard method used in previous work on NER is to select full sequences based on a criterion for the uncertainty of the entities recognized t"
D19-1520,W17-2630,0,0.0230424,"ferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategies for lowresource languages. Settles and Craven (2008) present a nice survey on the different AL strategies for sequence labeling tasks, whereas Marcheggiani and Artieres (2014) discuss the strategies for acquiring partially labeled data. Wanvarie et al. (2011); Neubig et al. (2011); Sperber et al. (2014) show the advantages o"
D19-1520,C08-1113,0,0.249022,"ed by this finding and considering the fact that named entities are both important and sparse, we propose an entity-targeted strategy to save annotator effort. Specifically, we select uncertain subspans of tokens within a sequence that are most likely named entities. This way, the annotators only need to assign types to the chosen subspans without having to read and annotate the full sequence. To cope with the resulting partial annotation of sequences, we apply a constrained version of conditional random fields (CRFs), partial CRFs, during training that only learn from the annotated subspans (Tsuboi et al., 2008; Wanvarie et al., 2011). To evaluate our proposed methods, we conducted simulated active learning experiments on 5 languages: Spanish, Dutch, German, Hindi and Indonesian. Additionally, to study our method in a more practical setting, we conduct human annotation experiments on two low-resource languages, Indonesian and Hindi, and one simulated low-resource language, Spanish. In sum, this paper makes the following contributions: 1. We present a bootstrapping recipe for improving low-resource NER. With just onetenth of tokens annotated, our proposed entity-targeted active learning method provid"
D19-1520,D18-1034,1,0.560569,"“how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Specifically, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efficiency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is fine-tuned on data obtained using active learning to improve accuracy in the target language. Within this recipe, the choice of specific method for choosing and annotating data within active learning is highly important to minimize human effor"
D19-1520,N16-1033,0,0.0198852,"language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data. The code is publicly available here.1 1 Introduction Named entity recognition (NER) is the task of detecting and classifying named entities in text into a fixed set of pre-defined categories (person, location, etc.) with several downstream applications including machine reading (Chen et al., 2017), entity and event co-reference (Yang and Mitchell, 2016), and text mining (Han and Sun, 2012). Recent advances in deep learning have yielded stateof-the-art performance on many sequence labeling tasks, including NER (Collobert et al., 2011; 1 https://github.com/Aditi138/ EntityTargetedActiveLearning Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2018). However, the performance of these models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a"
D19-1520,H01-1035,0,0.231617,"ese models is highly dependent on the availability of large amounts of annotated data, and as a result their accuracy is significantly lower on languages that have fewer resources than English. In this work, we ask the question “how can we efficiently bootstrap a high-quality named entity recognizer for a low-resource language with only a small amount of human effort?” Specifically, we leverage recent advances in data-efficient learning for low-resource languages, proposing the following “recipe” for bootstrapping low-resource entity recognizers: First, we use cross-lingual transfer learning (Yarowsky et al., 2001; Ammar et al., 2016), which applies a model trained on another language to low-resource languages, to provide a good preliminary model to start the bootstrapping process. Specifically, we use the model of Xie et al. (2018), which reports strong results on a number of language pairs. Next, on top of this transferred model we further employ active learning (Settles and Craven, 2008; Marcheggiani and Artieres, 2014), which helps improve annotation efficiency by using model predictions to select informative, rather than random, data for human annotators. Finally, the model is fine-tuned on data o"
D19-1520,D08-1063,0,0.0801523,"for creating a highquality entity gazetteer under a short time budget. Since a naive strategy of SAL allows for more labelled data to be acquired in the same amount of time, in the future we plan to explore mixed-mode annotation where we choose either full sequences or spans for annotation. 4 Related Work Cross-Lingual Transfer: Transferring knowledge from high-resource languages has been extensively used for improving low-resource NER. More common approaches rely on annotation projection methods where annotations in source language are projected to the target language using parallel corpora (Zitouni and Florian, 2008; Ehrmann et al., 2011) or bilingual dictionaries (Xie et al., 2018; Mayhew et al., 2017). Crosslingual word embeddings (Bharadwaj et al., 2016; Chaudhary et al., 2018) also provide a way to leverage annotations from related languages. Active Learning (AL): AL has been widely explored for many NLP tasks- NER: Shen et al. (2017) explore token-level annotation strategies, Chen et al. (2015) present a study on AL for clinical NER; Baldridge and Palmer (2009) evaluate how well AL works with annotator expertise and label suggestions, Garrette and Baldridge (2013) study type and token based strategi"
D19-1520,Q14-1014,1,0.934275,"ब BORGIORG Cross-Lingual Transfer Learning English labeled dataset label Target language unlabeled dataset Target Language labeled dataset query spans ू ल और श क क कमी पर सु ीम कोट नेमांगा जवाब Active Learning Figure 1: Our proposed recipe: cross-lingual transfer is used for projecting annotations from an English labeled dataset to the target language. Entity-targeted active learning is then used to select informative sub-spans which are likely entities for humans to annotate. Finally, the NER model is fine-tuned on this partially-labeled dataset. sentence is of interest (Neubig et al., 2011; Sperber et al., 2014). Inspired by this finding and considering the fact that named entities are both important and sparse, we propose an entity-targeted strategy to save annotator effort. Specifically, we select uncertain subspans of tokens within a sequence that are most likely named entities. This way, the annotators only need to assign types to the chosen subspans without having to read and annotate the full sequence. To cope with the resulting partial annotation of sequences, we apply a constrained version of conditional random fields (CRFs), partial CRFs, during training that only learn from the annotated su"
D19-1520,W02-2024,0,0.192088,"ain. We also compare the NER performance using two other training schemes: C ORPUS AUG, where we train the model on the concatenated corpus of transferred data and the newly acquired data, and C ORPUS AUG +F INE T UNE, where we additionally fine-tune the model trained using C ORPUS AUG on just the newly acquired data. 3 Experiments In this section, we evaluate the effectiveness of our proposed strategy in both simulated (§3.2) and human-annotation experiments (§3.3). 3.1 Experimental Settings Datasets: The first evaluation set includes the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for Spanish (from the Romance family), Dutch and German (like English, from the Germanic family). We use the standard corpus splits for train/dev/test. The second evaluation set is for the low-resource setting where we use the Indonesian (from the Austronesian family), Hindi (from the Indo-Aryan family) and Spanish datasets released by the Linguistic Data Consortium (LDC).2 We generate the train/dev/test split by random sampling. Details of the corpus statistics are in the Appendix. English-transferred Data: We use the same experimental settings and resou"
D19-1520,W03-0419,0,\N,Missing
D19-1520,D14-1162,0,\N,Missing
D19-1621,N18-1016,0,0.0465217,"Missing"
D19-1621,N18-2011,0,0.571679,"018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018) have focused mostly on ad-hoc decoding procedures to generate reasonable poetry, often relying on pruning from a set of candidate outputs to encourage desired behavior such as presence of explicitly-defined rhyming patterns. We propose an adversarial approach to poetry generation that, by adding structure and inductive bias into the discriminator, is able to learn rhyming constraints directly from data without prior knowledge. The role of the discriminator is to try to distinguish between generated and real poems during training. We propose to add inductive bias via the choice of discriminato"
D19-1621,D16-1126,0,0.213792,"Missing"
D19-1621,P17-4008,0,0.16672,"Missing"
D19-1621,P18-1152,0,0.0251601,"using the discriminator to compare poems based only on a learned similarity matrix of pairs of line ending words, the proposed approach is able to successfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information. 1 Introduction Many existing approaches to text generation rely on recurrent neural networks trained using likelihood on sequences of words or characters. However, such models often fail to capture overall structure and coherency in multi-sentence or longform text (Bosselut et al., 2018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018)"
D19-1621,P17-1016,0,0.1272,"Missing"
D19-1621,P18-1181,0,0.251365,"criminator architecture: We require the discriminator to reason about poems through pairwise comparisons between line ending words. These learned word comparisons form a similarity matrix for the poem within the discriminator’s architecture. Finally, the discriminator evaluates the poem through a 2D convolutional classifier applied directly to this matrix. This final convolution is naturally biased to identify spatial patterns across word comparisons, which, in turn, biases learned word comparisons to pick up rhyming since rhymes are typically the most salient spatial patterns. Recent work by Lau et al. (2018) proposes a quatrain generation method that relies on specific domain knowledge about the dataset to train a classifier for learning the notion of rhyming: that a line ending word always rhymes with exactly one more ending word in the poem. This limits the applicability of their method to other forms of poetry with different rhyming patterns. They train the classifier along with a language model in a multi-task setup. Further, at generation time, they heavily rely on rejection sampling to produce quatrains which satisfy any valid rhyming pattern. In contrast, we find that generators trained us"
D19-1621,D16-1127,0,0.0434765,"ch is able to successfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information. 1 Introduction Many existing approaches to text generation rely on recurrent neural networks trained using likelihood on sequences of words or characters. However, such models often fail to capture overall structure and coherency in multi-sentence or longform text (Bosselut et al., 2018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018) have focused mostly on ad-hoc decoding procedures to generate reasonable poetry, often relying on pruning from a set of candi"
D19-1621,P19-1192,0,0.0283185,"Missing"
D19-1621,W17-3502,0,0.0565797,"sselut et al., 2018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018) have focused mostly on ad-hoc decoding procedures to generate reasonable poetry, often relying on pruning from a set of candidate outputs to encourage desired behavior such as presence of explicitly-defined rhyming patterns. We propose an adversarial approach to poetry generation that, by adding structure and inductive bias into the discriminator, is able to learn rhyming constraints directly from data without prior knowledge. The role of the discriminator is to try to distinguish between generated and real poems during training. We propose to add inductive bias v"
D19-1621,N19-1013,0,0.0656129,"Missing"
D19-1621,P11-2014,0,0.529802,"d representations g(.) to check if rhyming words are close-by in the learned manifold. We consider all pairs of words among the ending words in a quatrain/limerick, and label each pair to be rhyming or non-rhyming based on previously stated definition of rhyming. If the cosine similarity score between the representations of pairs of words is above a certain threshold, we predict that word pair as rhyming, else it is predicted as non-rhyming. We report F1 scores for the binary classification setup of predicting word-pairs to be rhyming or not. We consider some additional baselines: R HYM - EM (Reddy and Knight, 2011) uses latent variables to model rhyming schemes, and train parameters using EM. GRAPHEME - K baselines predict a word pair as rhyming only if the last K = {1, 2, 3} characters of the two words are same. For S ONNET data, we observe that R HYME GAN obtains a F1 score of 0.90 (Table 3) on the test split (threshold chosen to maximize f1 on dev split). We repeat the above analysis on the L IMERICK dataset and observe an F1 of 0.92 for R HYME - GAN. D EEP - SPEARE model reports F1 6028 Model GRAPHEME -1 GRAPHEME -2 GRAPHEME -3 R HYM - EM D EEP - SPEARE/M AX -M ARGIN R HYME - GAN - NS R HYME - GAN S"
D19-1621,N19-5001,0,0.0580344,"Missing"
D19-1621,P18-1083,0,0.0259984,"p words to satisfy poetry constraints. Ghazvininejad et al. (2018) worked on poetry translation using an unconstrained machine translation model and separately learned Finite State Automata for enforcing rhythm and rhyme. Similar to rhyming and rhythm patterns in poetry, certain types of musical compositions showcase rhythm and repetition patterns, and some prior works model such patterns in music generation (Walder and Kim, 2018; Jhamtani and BergKirkpatrick, 2019). Generative adversarial learning (Goodfellow et al., 2014) for text generation has been used in prior works (Fedus et al., 2018; Wang et al., 2018, 2019; Rao and Daum´e III, 2019), though has not been explored with regard to the similarity structure proposed in this paper. 5 Conclusions In this paper we have proposed a novel structured discriminator to learn a poem generator. The generator learned utilizing the structured adversary is able to identify rhyming structure patterns present in data, as demonstrated through the improved sampling efficiency. Through the rhyming classification probe, we demonstrate that the proposed discriminator is better at learning the notion of rhyming compared to baselines. Acknowledgements We are thankful"
D19-1621,D14-1074,0,0.0615192,"Missing"
D19-1621,D17-1062,0,0.0209209,"cessfully learn rhyming patterns in two different English poetry datasets (Sonnet and Limerick) without explicitly being provided with any phonetic information. 1 Introduction Many existing approaches to text generation rely on recurrent neural networks trained using likelihood on sequences of words or characters. However, such models often fail to capture overall structure and coherency in multi-sentence or longform text (Bosselut et al., 2018; Holtzman et al., 2018). To rectify this, prior work has proposed losses which encourage overall coherency or other desired behavior (Li et al., 2016; Zhang and Lapata, 2017; Bosselut et al., 2018). However, most of these approaches rely on manually provided definitions of what constitutes a good or suitable structure, thereby limiting their applicability. In this paper we propose a method for English poetry generation that directly learns higher-level rhyming constraints as part of a generator without requiring strong manual intervention. Prior works on poetry generation (Oliveira, 2017; Ghazvininejad et al., 2018) have focused mostly on ad-hoc decoding procedures to generate reasonable poetry, often relying on pruning from a set of candidate outputs to encourag"
font-llitjos-carbonell-2004-translation,2001.mtsummit-road.7,1,\N,Missing
font-llitjos-etal-2004-error,2001.mtsummit-road.7,1,\N,Missing
font-llitjos-etal-2004-error,A00-2018,0,\N,Missing
font-llitjos-etal-2004-error,font-llitjos-carbonell-2004-translation,1,\N,Missing
H89-2078,1997.mtsummit-papers.6,0,0.0773734,"Missing"
I08-1056,P07-1059,0,0.0350403,"tion that tailors the IR component to question answering systems focuses on query formulation and query expansion (Woods et al., 2001). Taxonomic conceptual indexing system based on morphological, syntactic, and semantic features can be used to expand queries with inflected forms, hypernyms, and semantically related terms. In subsequent research (Bilotti et al., 2004), stemming is compared to query expansion using inflectional variants. On a particular question answering controlled dataset, results show that expansion using inflectional variants produces higher recall than stemming. Recently (Riezler et al., 2007) used statistical machine translation for query expansion and took a step towards bridging the lexical gap between questions and answers. In (Terra et al., 2005) query expansion is studied using lexical affinities with different query formulation strategies for passage retrieval. When evaluated on TREC datasets, the affinity replacement method obtained significant improvements in precision, but did not outperform other methods in terms of recall. 2 Cluster-Based Retrieval for QA In order to explore retrieval under question answering, we employ a statistical system (SQA) that achieves good fact"
J83-3001,P79-1002,1,0.828697,"Missing"
J83-3001,P83-1025,1,0.70235,"must be addressed by any interface intended for serious use by real users. Empirical observations have shown that users of natural language interfaces employ A m e r i c a n J o u r n a l of Computational Linguistics, V o l u m e 9, N u m b e r s 3-4, J u l y - D e c e m b e r 1983 Jaime G. Carbonell and Philip J. Hayes Recovery Strategies for Parsing Extrammatical Language ellipsis and other a b b r e v i a t i n g devices (for example, a n a p h o r a , short definite noun phrases, cryptic language omitting semantically superfluous words, and lexical abbreviations) with alarming frequency (Carbonell 1983). The results of our empirical observations can be summarized as follows: Terseness principle: Users of natural language interfaces insist on being as terse as possible, independent of task, communication media, typing ability, or instructions to the contrary, without sacrificing the flexibility of expression inherent in natural language communication. Broadly speaking, one can classify ellipsis into intrasentential and intersentential ellipsis, with the latter category being far more prevalent in practical natural language interfaces. I n t r a s e n t e n t i a l ellipsis occurs most frequen"
J83-3001,P81-1033,1,0.77935,"Missing"
J83-3001,P84-1047,1,0.905851,"5.3.1. C o o r d i n a t i n g multiple s t r a t e g i e s t h r o u g h an e n t i t y - o r i e n t e d a p p r o a c h A major problem that arises in using multiple parsing strategies is coordination between the strategies. Questions of interaction and order of application are involved. In CASPAR and DYPAR, the problem was solved simply by &quot; h a r d - w i r i n g &quot; the interactions, but this is not satisfactory in general, especially if we wish to extend the set of strategies available in a smooth way. One alternative we have begun to explore involves the idea of entity-oriented parsing (Hayes 1984). The central notion behind entity-oriented parsing is that the primary task of a natural language interface is to recognize entities - objects, actions, states, commands, etc. - from the domain of discourse of the interface. This recognition may be recursive in the sense that descriptions of entities may contain descriptions of subsidiary entities (for example, commands refer to objects). In entity-oriented parsing, all the entities that a particular interface system needs to recognize are defined separately. These definitions contain information both about the way the entities will be manife"
J83-3001,J81-4002,1,0.529572,"Missing"
J83-3001,J81-2002,0,0.0261819,"viate f r o m its g r a m m a t i c a l and semantic expectations. M a n y researchers have made this observation and have taken initial steps towards coverage of certain classes of extragrammatical constructions. Since robust parsers must deal primarily with input that does m e e t their expectations, the various efforts at coping with extragrammaticality have been generally structured as extensions to existing parsing methods. P r o b a b l y the most popular approach has been to extend syntactically-oriented parsing techniques e m p l o y i n g A u g m e n t e d Transition Networks (ATNs) (Kwasny and Sondheimer 1981, Weischedel and Sondheimer 1984, Weischedel and Black 1980, Woods et al. 1976). Other researchers have a t t e m p t e d to deal with u n g r a m m a t ical input through n e t w o r k - b a s e d semantic g r a m m a r techniques (Hendrix 1977), through extensions to p a t t e r n matching parsing in which partial p a t t e r n matching is allowed ( H a y e s and M o u r a d i a n 1981), through conceptual case f r a m e instantiation ( D e j o n g 1979, Schank, L e b o w i t z , and B i r n b a u m 1980), and through a p p r o a c h e s involving multiple c o o p e r a t i n g parsing strat"
J83-3001,J80-1002,0,0.0177301,"Missing"
J83-3001,J83-3003,0,0.404705,"eding sections. These strategies generally require an interpretive ability to &quot; s t e p b a c k &quot; and take a b r o a d view of the situation when a parser&apos;s expectations are violated, and this is very hard to do when using networks. The underlying p r o b l e m is that a significant amount of state information during the parse is implicitly e n c o d e d by the position in the n e t w o r k ; in the case of ATNs, other aspects of the state are contained in the settings of scattered registers. As d e m o n s t r a t e d by the meta-rule a p p r o a c h to diagnosing parse failures described by Weischedel and Sondheimer (1983) elsewhere in this journal issue, these and other difficulties elaborated below do not preclude r e c o v e r y f r o m extragrammatical input. H o w e v e r , they do m a k e it difficult and often impractical, since m u c h of the procedurally encoded state must be made declarative and explicit to the recovery strategies. O f t e n an ATN parse will continue b e y o n d the point where the grammatical deviation, say an omitted word, occurred, and reach a node in the n e t w o r k f r o m which it can make no further progress (that is, no arcs can be traversed). At this point, the parser cann"
J83-3001,J80-1001,0,0.0247679,"ation can be brought to bear on network based parsing, either through the semantic g r a m m a r a p p r o a c h in which joint semantic and syntactic categories are used directly in the ATN, or by allowing the tests on ATN arcs to d e p e n d on semantic criteria ( B o b r o w 1978, B o b r o w and W e b b e r 1980). In the f o r m e r technique, the a p p r o p r i a t e semantic i n f o r m a t i o n for recovery can be applied only if the correct network node can be located - a sometimes difficult task as we have seen. In the latter technique, s o m e t i m e s k n o w n as cascaded ATNs (Woods 1980), the syntactic and semantic parts of the g r a m m a r are kept separate, thus giving the potential for a higher degree of interpretiveness in using the semantic information. H o w e v e r , the natural way to use this technique is to e m p l o y the semantic i n f o r m a t i o n only to c o n f i r m or disconfirm parses arrived at on syntactic grounds. So the rigidity of the n e t w o r k formalism m a k e s it very difficult to bring the available semantic information to bear effectively on extragrammatical input. A further disadvantage of the network a p p r o a c h for implementing flex"
J83-3001,J83-3001,1,0.106957,"Missing"
J83-3001,J80-2003,0,\N,Missing
levin-etal-2014-resources,khokhlova-zakharov-2010-studying,0,\N,Missing
levin-etal-2014-resources,D10-1004,0,\N,Missing
levin-etal-2014-resources,ivanova-etal-2008-evaluating,0,\N,Missing
levin-etal-2014-resources,P14-1024,1,\N,Missing
levin-etal-2014-resources,macwhinney-fromm-2014-two,1,\N,Missing
levin-etal-2014-resources,W13-0906,1,\N,Missing
levin-etal-2014-resources,feely-etal-2014-cmu,1,\N,Missing
marujo-etal-2012-supervised,N03-1033,0,\N,Missing
marujo-etal-2012-supervised,guthrie-etal-2010-efficient,0,\N,Missing
monson-etal-2004-data,2001.mtsummit-road.7,1,\N,Missing
monson-etal-2004-data,carbonell-etal-2002-automatic,1,\N,Missing
monson-etal-2008-linguistic,2001.mtsummit-road.7,1,\N,Missing
monson-etal-2008-linguistic,2004.tmi-1.1,1,\N,Missing
monson-etal-2008-linguistic,N06-2002,1,\N,Missing
monson-etal-2008-linguistic,J01-2001,0,\N,Missing
monson-etal-2008-linguistic,W07-1315,1,\N,Missing
N06-2011,P02-1040,0,0.0811447,"Missing"
N06-2011,J90-2002,0,0.552015,"Missing"
N06-2011,1997.tmi-1.13,1,0.685111,"o equivalence classes and provides an outline of the Standard GAC algorithm. Section 3 describes the spectral clustering algorithm used. Sec41 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41–44, c New York, June 2006. 2006 Association for Computational Linguistics tion 4 lists results obtained in a full evaluation of the algorithm. Section 5 concludes and discusses directions for future work. 2 Term vectors for clustering Using a bilingual dictionary, usually created using statistical methods such as those of (Brown et. al., 1990) or (Brown, 1997), and the parallel text, a rough mapping between source and target words can be created. This word pair is then treated as an indivisible token for future processing. For each such word pair we then accumulate counts for each token in the surrounding context of its occurrences (N words, currently 3, immediately prior to and N words immediately following). The counts are weighted with respect to distance from occurrence, with a linear decay (from 1 to 1/N) to give greatest importance to the words immediately adjacent to the word pair being examined. These counts form a pseudo-document for each"
N06-2011,C00-1019,1,\N,Missing
N07-1041,C02-1013,0,0.0140051,"., 1997; Heckerman et al., 2000). The estimated log-odds is computed from a Laplace correction to the empirical probability at a leaf node. Na¨ıve Bayes We use a multinomial na¨ıve Bayes (NB) and a multivariate Bernoulli na¨ıve Bayes classifier (MBNB) (McCallum and Nigam, 1998). For these classifiers, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively. Since these are probabilistic, they issue log-odds estimates directly. 3.4 Sentence-Level Classifiers Each e-mail is automatically segmented into sentences using RASP (Carroll, 2002). Since the corpus has fine grained labels, we can train classifiers to classify a sentence. Each classifier in Section 3.3 is also used to learn a sentence classifier. However, we then must make a document-level prediction. In order to produce a ranking score, the confidence that the document contains an action-item is: ( 1 P ˆ s∈d|π(s)=1 λ(s), ∃s∈d|π(s) = 1 n(d) ˆ λ(d) = 1 ˆ o.w. n(d) maxs∈d λ(s) 3 This rule is not guaranteed be optimal for a particular value of N but is motivated by theoretical results which show such a rule converges to the optimal classifier as the number of training poin"
N07-1041,W04-3240,0,0.033712,"f striving, stacking, and two of the most competitive base classifiers in Figure 4. We see that striving loses by a slight amount to stacking early in the curve but still 330 beats the base classifiers. Later in the curve, it dominates all the classifiers. If we examine the curves using error bars, we see that the variance of STRIVE drops faster than the other classifiers as we move further along the x-axis. Thus, STRIVE’s ranking quality varies less with changes to the training set. 4 Related Work Several researchers have considered text classification tasks similar to action-item detection. Cohen et al. (2004) describe an ontology of “speech acts”, such as “Propose a Meeting”, and attempt to predict when an e-mail contains one of these speech acts. Corston-Oliver et al. (2004) consider detecting items in e-mail to “Put on a To-Do List” using a sentence-level classifier. In earlier work (Bennett and Carbonell, 2005), we demonstrated that sentence-level classifiers typically outperform document-level classifiers on this problem and examined the underlying reasons why this was the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using the sy"
N07-1041,W04-1008,0,0.133032,"but still 330 beats the base classifiers. Later in the curve, it dominates all the classifiers. If we examine the curves using error bars, we see that the variance of STRIVE drops faster than the other classifiers as we move further along the x-axis. Thus, STRIVE’s ranking quality varies less with changes to the training set. 4 Related Work Several researchers have considered text classification tasks similar to action-item detection. Cohen et al. (2004) describe an ontology of “speech acts”, such as “Propose a Meeting”, and attempt to predict when an e-mail contains one of these speech acts. Corston-Oliver et al. (2004) consider detecting items in e-mail to “Put on a To-Do List” using a sentence-level classifier. In earlier work (Bennett and Carbonell, 2005), we demonstrated that sentence-level classifiers typically outperform document-level classifiers on this problem and examined the underlying reasons why this was the case. Furthermore, we presented user studies demonstrating that users identify action-items more rapidly when using the system. In terms of classifier combination, a wide variety of work has been done in the arena. The STRIVE metaclassification approach (Bennett et al., 2005) extended Wolper"
N13-1025,P08-1024,0,0.549714,"a specific rule is used in a translation. Early experiments (Liang et al., 2006) used the structured perceptron to tune a phrase-based system on a large subset of the training data, showing improvements when using rule indicator features, word alignment features, and POS tag features. Another early attempt (Tillmann and Zhang, 2006) used phrase pair and word features in a block SMT system trained using stochastic gradient descent for a convex loss function, but did not compare to MERT. Problems of overfitting and degenerate derivations were tackled with a probabilistic latent variable model (Blunsom et al., 2008) which used rule indicator features yet failed to improve upon the MERT baseline for the standard Hiero features. 249 Difficulties in Large-Scale Training Discriminative training for machine translation is complicated by several factors. First, both translation rules and feature weights are learned from parallel data. If the same data is used for both tasks, overfitting of the weights is very possible.1 Second, the standard MT cost function, BLEU (Papineni et al., 2002), does not decompose additively over training instances (because of the “brevity penalty”) and so approximations are used—thes"
N13-1025,N12-1047,0,0.372997,"2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of which has its own separately tuned weight, which count how oft"
N13-1025,D08-1024,0,0.352008,"ight 30.115 30.120 (a) Four representative frequent sparse features. 30.110 4 Held-Out Line Search Algorithm 30.105 BLEU Sentence Level Approximations to BLEU Finally, we note that discriminative training methods often use a sentence level approximation to BLEU. It has been shown that optimizing corpus level BLEU versus sentence level BLEU can lead to improvements of up to nearly .4 BLEU points on the test set (Nakov et al., 2012). Possible fixes to this problem include using a proper sentence level metric such a METEOR (Denkowski and Lavie, 2011) or a pseudo-corpus from the last few updates (Chiang et al., 2008). However, in light of the result from section 3.1 that tuning on the dev set is still better than tuning on a held-out portion of the training data, we observe that tuning a corpus level metric on a highquality dev set from the same domain as the test set probably leads to the best translation quality. Attempts to improve upon this strong baseline lead us to the development of the HOLS algorithm which we describe next. 28.0 BLEU 30.0 3.3 −2 30.100 poorly scaled for rule feature weights. Changing the weights for one of the common features changes the BLEU score by almost 2.5 BLEU points, while"
N13-1025,P05-1033,0,0.0932255,"e much sparser features without abandoning the proven dense features; however, extremely sparse features leads to problems of scaling in the optimization problem as we will show. 3.1 Training Data and Overfitting One of the big questions in discriminative training of machine translation systems is why standard machine learning techniques can perform so poorly when applied to large-scale learning on the training data. Figure 1 shows a good example of this. The structured SVM (Tsochantaridis et al., 2004; Cherry and Foster, 2012) was used to learn the weights for a Chinese-English Hiero system (Chiang, 2005) with just eight features, using stochastic gradient descent (SGD) for online learning (Bottou, 1998; Bottou, 2010). The weights were initialized from MERT values tuned on a 2k-sentence dev set (MT06), and the figure shows the progress of the online method during a single pass through the 300ksentence Chinese-English FBIS training set. As the training progresses in Figure 1, BLEU scores on the training data go up, but scores on the 1 Previous work has attempted to mitigate the risk of overfitting through careful regularization (Blunsom et al., 2008; Simianer et al., 2012). 30 Table 1: MERT on"
N13-1025,P11-2031,1,0.859433,"MT05 MT08nw MT05wb Train Dev Test Train (FBIS) Dev (MT06) Test (MT02-03) MT08 24M 1M 1797 1,056 813 547 89K 1,359 1,133 302K 1,664 1,797 1,357 Tokens Source Target 594M 7M 31M 13K 236K 7K 144K 5K 116K 5K 89K 2.1M 1.7M 34K 28K 29K 24K 1M 9.3M 4K 192K 5K 223K 4K 167K derivations for the input x, and cost(yi , y) is add one smoothing sentence level BLEU.7 Except where noted, all experiments are repeated 5 times and results are averaged, initial weights for the dense features are drawn from a standard normal, and initial weights for the sparse features are set to zero. We evaluate using MultEval (Clark et al., 2011) and report standard deviations across optimizer runs and significance at p = .05 using MultEval’s built-in permutation test. In the large-scale experiments for HOLS, we only run the full optimizer once, and report standard deviations using multiple runs of the last MERT run (i.e. the last line search on the dev data). 6.1 following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which us"
N13-1025,W11-2107,0,0.0302107,"st lists. The plots indicate that the BLEU score is 29.0 27.0 −1 0 1 2 Weight 30.115 30.120 (a) Four representative frequent sparse features. 30.110 4 Held-Out Line Search Algorithm 30.105 BLEU Sentence Level Approximations to BLEU Finally, we note that discriminative training methods often use a sentence level approximation to BLEU. It has been shown that optimizing corpus level BLEU versus sentence level BLEU can lead to improvements of up to nearly .4 BLEU points on the test set (Nakov et al., 2012). Possible fixes to this problem include using a proper sentence level metric such a METEOR (Denkowski and Lavie, 2011) or a pseudo-corpus from the last few updates (Chiang et al., 2008). However, in light of the result from section 3.1 that tuning on the dev set is still better than tuning on a held-out portion of the training data, we observe that tuning a corpus level metric on a highquality dev set from the same domain as the test set probably leads to the best translation quality. Attempts to improve upon this strong baseline lead us to the development of the HOLS algorithm which we describe next. 28.0 BLEU 30.0 3.3 −2 30.100 poorly scaled for rule feature weights. Changing the weights for one of the comm"
N13-1025,P10-4002,1,0.867269,"search on the dev data). 6.1 following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which uses only dense features. Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus. We use GIZA++ for word alignments (Och and Ney, 2003), Thrax (Weese et al., 2011) to extract the grammars, our decoder is cdec (Dyer et al., 2010) which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM (Stolcke, 2002). Our optimizer uses code implemented in the pycdec python interface to cdec (Chahuneau et al., 2012). To speed up decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e |f ). Statistics about the datasets we used are listed in Table 2. We use the “soft ramp 3” loss function (Gimpel, 2012; Gimpel and Smith, 2012) as the surrogate loss function for calculating the gradient in HOLS. It is defined as ˜= L n  X i=1 X − log ~ ~ f (xi ,y)−cost(yi ,y) ew· y∈Gen(xi ) + log X"
N13-1025,N12-1023,0,0.864849,"son to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of which has its own separately tuned we"
N13-1025,W11-2123,0,0.0251385,"g 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which uses only dense features. Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus. We use GIZA++ for word alignments (Och and Ney, 2003), Thrax (Weese et al., 2011) to extract the grammars, our decoder is cdec (Dyer et al., 2010) which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM (Stolcke, 2002). Our optimizer uses code implemented in the pycdec python interface to cdec (Chahuneau et al., 2012). To speed up decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e |f ). Statistics about the datasets we used are listed in Table 2. We use the “soft ramp 3” loss function (Gimpel, 2012; Gimpel and Smith, 2012) as the surrogate loss function for calculating the gradient in HOLS. It is defined as ˜= L n  X i=1 X − log ~ ~ f (xi ,y)−cost(yi ,y) ew· y∈Gen(xi ) + log X e w· ~ f~(xi ,y)+cost(yi ,y)  y∈G"
N13-1025,D11-1125,0,0.568108,"imianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of"
N13-1025,P06-1096,0,0.510523,"Missing"
N13-1025,C12-1121,0,0.401001,"Missing"
N13-1025,P02-1038,0,0.0582837,"e details of our algorithm that addresses these issues, give results on three language pairs, and conclude. Techniques for distributed learning and feature selection for the perceptron loss using rule indicator, rule shape, and source side-bigram features have recently been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in"
N13-1025,J03-1002,0,0.00723126,"and report standard deviations using multiple runs of the last MERT run (i.e. the last line search on the dev data). 6.1 following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which uses only dense features. Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus. We use GIZA++ for word alignments (Och and Ney, 2003), Thrax (Weese et al., 2011) to extract the grammars, our decoder is cdec (Dyer et al., 2010) which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM (Stolcke, 2002). Our optimizer uses code implemented in the pycdec python interface to cdec (Chahuneau et al., 2012). To speed up decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e |f ). Statistics about the datasets we used are listed in Table 2. We use the “soft ramp 3” loss function (Gimpel, 2012; Gimpel and Smith, 2012) as the surrogate loss function for calculating the gradient in HO"
N13-1025,P03-1021,0,0.587438,". Not only does this two-phase learning approach prevent overfitting, the second pass optimizes corpus-level BLEU of the Viterbi translation of the decoder. We demonstrate significant improvements using sparse rule indicator features in three different translation tasks. To our knowledge, this is the first large-scale discriminative training algorithm capable of showing improvements over the MERT baseline with only rule indicator features in addition to the standard MERT features. 1 Introduction This paper is about large scale discriminative training of machine translation systems. Like MERT (Och, 2003), our procedure directly optimizes the cost of the Viterbi output on corpus-level metrics, but does so while scaling to millions of features. The training procedure, which we call the Held-Out Line Search algorithm (HOLS), is a two-phase iterative batch optimization procedure consisting of (1) a gradient calculation on a differentiable approximation to the loss on a large amount of parallel training While sparse features are successfully used in many NLP systems, such parameterizations pose a number of learning challenges. First, since any one feature is likely to occur infrequently, a large a"
N13-1025,P02-1040,0,0.0870489,"t compare to MERT. Problems of overfitting and degenerate derivations were tackled with a probabilistic latent variable model (Blunsom et al., 2008) which used rule indicator features yet failed to improve upon the MERT baseline for the standard Hiero features. 249 Difficulties in Large-Scale Training Discriminative training for machine translation is complicated by several factors. First, both translation rules and feature weights are learned from parallel data. If the same data is used for both tasks, overfitting of the weights is very possible.1 Second, the standard MT cost function, BLEU (Papineni et al., 2002), does not decompose additively over training instances (because of the “brevity penalty”) and so approximations are used—these often have problems with the length (Nakov et al., 2012). Finally, state-of-the-art MT systems make extensive good use of “dense” features, such as the log probability of translation decisions under a simpler generative translation model. Our goal is to begin to use much sparser features without abandoning the proven dense features; however, extremely sparse features leads to problems of scaling in the optimization problem as we will show. 3.1 Training Data and Overfi"
N13-1025,W10-1748,0,0.0488262,"tly been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule"
N13-1025,2012.amta-papers.14,0,0.114435,"tive training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of which has its own separately tuned weight, which count how often a specific rule is"
N13-1025,P12-1002,1,0.93084,"poor scaling (since MT 248 Proceedings of NAACL-HLT 2013, pages 248–258, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics decoding is so expensive, it is not feasible to make many passes through large amounts of training data, so optimization must be efficient). We then present the details of our algorithm that addresses these issues, give results on three language pairs, and conclude. Techniques for distributed learning and feature selection for the perceptron loss using rule indicator, rule shape, and source side-bigram features have recently been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011"
N13-1025,P06-2101,0,0.0864597,"ape, and source side-bigram features have recently been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameteri"
N13-1025,P06-1091,0,0.0494195,"criminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translation model: every rule has a feature, each of which has its own separately tuned weight, which count how often a specific rule is used in a translation. Early experiments (Liang et al., 2006) used the structured perceptron to tune a phrase-based system on a large subset of the training data, showing improvements when using rule indicator features, word alignment features, and POS tag features. Another early attempt (Tillmann and Zhang, 2006) used phrase pair and word features in a block SMT system trained using stochastic gradient descent for a convex loss function, but did not compare to MERT. Problems of overfitting and degenerate derivations were tackled with a probabilistic latent variable model (Blunsom et al., 2008) which used rule indicator features yet failed to improve upon the MERT baseline for the standard Hiero features. 249 Difficulties in Large-Scale Training Discriminative training for machine translation is complicated by several factors. First, both translation rules and feature weights are learned from parallel"
N13-1025,D07-1080,0,0.295509,"ram features have recently been proposed (Simianer et al., 2012), but no comparison to MERT was made. 3 2 Related Work Discriminative training of machine translation systems has been a widely studied problem for the last ten years. The pattern of using small, highquality development sets to tune a relatively small number of weights was established early (Och and Ney, 2002; Och, 2003). More recently, standard structured prediction algorithms that target linearly decomposable approximations of translation quality metrics have been thoroughly explored (Liang et al., 2006; Smith and Eisner, 2006; Watanabe et al., 2007; Rosti et al., 2010; Hopkins and May, 2011; Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; Saluja et al., 2012). These have without exception used sentence-level approximations of BLEU to determine oracles and update weights using a variety of criteria and with a variety of different theoretical justifications. Despite advancements in discriminative training for machine translation, large-scale discriminative training with rule indicator features has remained notoriously difficult. Rule indicator features are an extremely sparse and expressive parameterization of the translati"
N13-1025,W11-2160,0,0.028091,"ions using multiple runs of the last MERT run (i.e. the last line search on the dev data). 6.1 following 8 dense features: LM, phrasal and lexical p(e|f ) and p(f |e), phrase and word penalties, and glue rule. The total number of features is 2.2M (Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The same features are used for all tuning methods, except MERT baseline which uses only dense features. Although we extract different grammars from various subsets of the training corpus, word alignments were done using the entire training corpus. We use GIZA++ for word alignments (Och and Ney, 2003), Thrax (Weese et al., 2011) to extract the grammars, our decoder is cdec (Dyer et al., 2010) which uses KenLM (Heafield, 2011), and we used a 4-gram LM built using SRILM (Stolcke, 2002). Our optimizer uses code implemented in the pycdec python interface to cdec (Chahuneau et al., 2012). To speed up decoding, for each source RHS we filtered the grammars to the top 15 rules ranked by p(e |f ). Statistics about the datasets we used are listed in Table 2. We use the “soft ramp 3” loss function (Gimpel, 2012; Gimpel and Smith, 2012) as the surrogate loss function for calculating the gradient in HOLS. It is defined as ˜= L n"
N16-1087,W13-2322,0,0.598809,"new method is trained statistically from AMRannotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-tostring transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at: http://github.com/jflanigan/jamr 1 Introduction We consider natural language generation from the Abstract Meaning Representation (AMR; Banarescu et al., 2013). AMR encodes the meaning of a sentence as a rooted, directed, acyclic graph, where concepts are nodes, and edges are relationships among the concepts. Because AMR models propositional meaning1 while abstracting away from surface syntactic realizations, and is designed with human annotation in mind, it suggests a separation of (i) engineering the application-specific propositions that need to be 1 In essence, AMR handles semantic roles, entity types, within-sentence coreference, discourse connectives, modality, negation, and some other phenomena. communicated about from (ii) general-purpose re"
N16-1087,W11-2832,0,0.013904,"y be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We ha"
N16-1087,C10-1012,0,0.0119508,"much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language gene"
N16-1087,P06-1130,0,0.0471353,"Missing"
N16-1087,W02-1001,0,0.145668,"ac and c. For i > c+1, li contains all words between ai−1 and ai , and for i = c + 1, li contains all words between c and ai . The tables for lex , left lex , and right lex are populated using the segmented basic rules. For each basic rule extracted from the training corpus and segmented according to the previous paragraph, f → c is added to lex , and Aki → hli , ri i is added to left lex for i ≤ c and right lex for i > c. The permutation ki is known during extraction in Eq. 8. The parameters ψ are trained using AdaGrad (Duchi et al., 2011) with the perceptron loss function (Rosenblatt, 1957; Collins, 2002) for 10 iterations over the basic rules. The features g are listed in Table 2. 7 Tokens 210,000 29,000 30,000 5,000 Table 3: Train/dev./test/MT09 split. Table 2: Synthetic rule model features. POS is the most common part-of-speech tag sequence for c, “dist” is the string “dist”, and side is “L” if i < c, “R” otherwise. + denotes string concatenation. l1 X1 r1 . . . c . . . lm Xm rm Sentences 10,000 1,400 1,400 204 Abstract Rules Like the synthetic rules, the abstract rules RA (G) generalize the basic rules. However, abstract rules 737 are much simpler generalizations which use partof-speech (P"
N16-1087,P10-4002,1,0.897274,"lly or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we describe our approach in §4. The transducer’s rules are extracted from the limited AMR corpus and learned general731 Proceedings of N"
N16-1087,P14-1134,1,0.394856,"ed by defining b(·) and e(·) recursively, bottom up: b(i) = min(aj , e(i) = max(a0j , Inducing Basic Rules min b(j)) max e(j)) j∈children(i) j∈children(i) The basic rules, denoted RB , are extracted from the training AMR data using an algorithm similar to extracting tree transucers from tree-string aligned parallel corpora (Galley et al., 2004). Informally, the rules are extracted from a sentence w = hw1 , . . . , wn i with AMR graph G as follows: Also define functions ˜b and e˜, from fragment indices to integers, as: 1. The AMR graph and the sentence are aligned; we use the JAMR aligner from Flanigan et al. (2014), which aligns non-overlapping subgraphs of the graph to spans of words. The subgraphs that JAMR aligns are called fragments. In JAMR’s aligner, all fragments are trees. For fragment i, let Ci = children(root(i)) − nodes(i), which is the children of the fragment’s root concept that are not included in the fragment. Let fi be the TI representation for fragment i.5 If Ci is empty, then the rule extracted for fragment i is: 2. G is replaced by its spanning tree by deleting relations that use a variable in the AMR annotation. 3. In the spanning tree, for each node i, we keep track of the word indi"
N16-1087,N04-1035,0,0.0173377,"e child nodes of a node. We consider a node aligned if it belongs to an aligned fragment. Let the span of an aligned node i be denoted by endpoints ai and a0i ; for unaligned nodes, ai = ∞ and a0i = −∞ (depicted with superscripts in Fig. 2). The node alignments are propagated by defining b(·) and e(·) recursively, bottom up: b(i) = min(aj , e(i) = max(a0j , Inducing Basic Rules min b(j)) max e(j)) j∈children(i) j∈children(i) The basic rules, denoted RB , are extracted from the training AMR data using an algorithm similar to extracting tree transucers from tree-string aligned parallel corpora (Galley et al., 2004). Informally, the rules are extracted from a sentence w = hw1 , . . . , wn i with AMR graph G as follows: Also define functions ˜b and e˜, from fragment indices to integers, as: 1. The AMR graph and the sentence are aligned; we use the JAMR aligner from Flanigan et al. (2014), which aligns non-overlapping subgraphs of the graph to spans of words. The subgraphs that JAMR aligns are called fragments. In JAMR’s aligner, all fragments are trees. For fragment i, let Ci = children(root(i)) − nodes(i), which is the children of the fragment’s root concept that are not included in the fragment. Let fi"
N16-1087,J99-4004,0,0.0965541,"rule extraction from an AMRannotated sentence. 736 where the max is over c ∈ 0 . . . m, k1 , . . . , km is any permutation of 1, . . . , m, and Ri ∈ left lex (Ai ) for i < c and Ri ∈ right lex (Ai ) for i > c. ∗ is used to denote the concept position.  is the empty string. The best solution to Eq. 10 is found exactly by brute force search over concept position c ∈ [0, m + 1] and the permutation k1 , . . . , km . With fixed concept position and permutation, each Ri for the arg max is found independently. To obtain the exact K-best solutions, we use dynamic programming with a K-best semiring (Goodman, 1999) to keep track of the K best sequences for each concept position and permutation, and take the best K sequences over all values of c and k· . The synthetic rule model’s parameters are estimated using basic rules extracted from the training data. Basic rules are put into the form of Eq. 9 by Feature name POS + Ai + “dist” POS + Ai + side POS + Ai + side + “dist” POS + Ai + Ri + side c + Ai + “dist” c + Ai + side c + Ai + side + “dist” c + POS + Ai + side + “dist” Value |c − i| 1.0 |c − i| 1.0 |c − i| 1.0 |c − i| |c − i| Split Train Dev. Test MT09 segmenting the RHS into the form (11) by choosin"
N16-1087,N04-1014,0,0.181127,"nd leaves underspecified many important details—including tense, number, definiteness, whether a concept should be referred to nominally or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we descri"
N16-1087,P13-2121,0,0.0197701,"ules is returned. 8 Handwritten Rules We have handwritten rules for dates, conjunctions, multiple sentences, and the concept have-org-role91. We also create pass-through rules for concepts by removing sense tags and quotes (for string literals). 9 Experiments We evaluate on the AMR Annotation Release version 1.0 (LDC2014T12) dataset. We follow the recommended train/dev./test splits, except that we remove MT09 data (204 sentences) from the training data and use it as another test set. Statistics for this dataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement ("
N16-1087,D07-1028,0,0.0567956,"Missing"
N16-1087,2006.amta-papers.8,0,0.0387566,"ual formatting of the TI representation in Fig. 1 is: (X want-01 (ARG0 (X boy)) (ARG1 (X ride-01 (ARG0 (X bicycle (mod (X red))))))) To ease notation, we use the function sort[] to lexicographically sort edge labels in a TI representation. Using this function, an equivalent way of representing the TI representation in Eq. 1, if the Li are unsorted, is: (X C sort[(L1 T1 ) . . . (Lm Tm )]) The TI representation is converted into a word sequence using a tree-to-string transducer. The tree transducer formalism we use is one-state extended linear, non-deleting tree-to-string (1-xRLNs) transducers (Huang et al., 2006; Graehl and Knight, 2004).3 Definition 1. (From Huang et al., 2006.) A 1xRLNs transducer is a tuple (N, Σ, W, R) where N 2 If there are duplicate child edge labels, then the conversion process is ambiguous and any of the conversions can be used. The ordering ambiguity will be handled later in the treetransducer rules. 3 Multiple states would be useful for modeling dependencies in the output, but we do not use them here. 732 boy ride-01 ARG0 X bicycle mod X red The boy wants to ride the red bicycle . Figure 1: The generation pipeline. An AMR graph (top), with a deleted re-entrancy (dashed), is"
N16-1087,C12-1083,0,0.0452424,"ataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation. 11 erence Bleu for the LCD2014T12 test set, and fourreference Bleu for the MT09 set. We report ablation experiments for different sources of rules. When ablating handwritten rules, we do not ablate passthrough rules. The full system achieves 22.1 Bleu on the test set, and 21.2 on MT09. Removing the synthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing t"
N16-1087,P07-2045,1,0.010277,"e referred to nominally or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we describe our approach in §4. The transducer’s rules are extracted from the limited AMR corpus and learned general"
N16-1087,P98-1116,0,0.174995,"ody of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, tree-transducer approach can be easily extended with rules and"
N16-1087,A00-2023,0,0.165467,"and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, tree-transducer approach can be easily extended with rules and features from othe"
N16-1087,W05-1510,0,0.0374539,"nthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing the basic and abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation"
N16-1087,P03-1021,0,0.0208502,"transduction of G0 : ! e=E 4 arg max score(d; θ) d∈D(G0 ,T ) (6) If fi is just a single concept with no children, then m = 0 and fi = (X C). Eq. 6 is solved approximately using the cdec decoder for machine translation (Dyer et al., 2010). The score of the transduction is a linear function (with coefficients θ) of a vector of features including the output sequence’s language model logprobability and features associated with the rules in the derivation (denoted f ; Table 1): X score(d; θ) = θLM log(pLM (E(d))) + θ > f (r) r∈d The feature weights are trained on a development dataset using MERT (Och, 2003). In the next four sections, we describe the rules extracted and generalized from the training corpus. 5 1, . . . , F . Let nodes : {1, . . . , F } → 2{1,...,N } and root : {1, . . . , F } → {1, . . . , N } be functions that return the nodes in a fragment and the root of a fragment, respectively, and let children : {1, . . . , N } → 2{1,...,N } return the child nodes of a node. We consider a node aligned if it belongs to an aligned fragment. Let the span of an aligned node i be denoted by endpoints ai and a0i ; for unaligned nodes, ai = ∞ and a0i = −∞ (depicted with superscripts in Fig. 2). Th"
N16-1087,J05-1004,0,0.00451229,"s statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, t"
N16-1087,P02-1040,0,0.126115,"have-org-role91. We also create pass-through rules for concepts by removing sense tags and quotes (for string literals). 9 Experiments We evaluate on the AMR Annotation Release version 1.0 (LDC2014T12) dataset. We follow the recommended train/dev./test splits, except that we remove MT09 data (204 sentences) from the training data and use it as another test set. Statistics for this dataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation."
N16-1087,2007.mtsummit-ucnlg.4,0,0.0605298,"abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been"
N16-1087,N06-1056,0,0.0309,"thetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation. 11 erence Bleu for the LCD2014T12 test set, and fourreference Bleu for the MT09 set. We report ablation experiments for different sources of rules. When ablating handwritten rules, we do not ablate passthrough rules. The full system achieves 22.1 Bleu on the test set, and 21.2 on MT09. Removing the synthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing the basic and abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic a"
N16-1087,C98-1112,0,\N,Missing
P10-2067,N03-1017,0,0.00497929,"rporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner. 1 Introduction Corpus-based approaches to machine translation have become predominant, with phrase-based statistical machine translation (PB-SMT) (Koehn et al., 2003) being the most actively progressing area. The success of statistical approaches to MT can be attributed to the IBM models (Brown et al., 1993) that characterize word-level alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of SMT systems for various language pairs, the quality of alignment is typically quite low for language pairs l"
P10-2067,C04-1046,0,0.0442399,"Missing"
P10-2067,P07-2045,0,0.00770914,"uncertainty. We will be exploring alternative formulations to this strategy. We observe that confidence based metrics perform significantly better than the baseline. From the scatter plots in Figure 1 1 we can say that using our best selection strategy one achieves similar performance to the baseline, but at a much lower cost of elicitation assuming cost per link is uniform. We also perform end-to-end machine translation experiments to show that our improvement of alignment quality leads to an improvement of translation scores. For this experiment, we train a standard phrase-based SMT system (Koehn et al., 2007) over the entire parallel corpus. We tune on the MT-Eval 2004 dataset and test on a subset of MT-Eval 2004 dataset consisting of 631 sentences. We first obtain the baseline score where no manual alignment was used. We also train a configuration using gold standard manual alignment data for the parallel corpus. This is the maximum translation accuracy that we can achieve by any link selection algorithm. We now take the best link selection criteria, which is the confidence M argin(i) = ˆ ij /S, T ) −Conf 1(a2 ˆ ij /S, T ) Conf 1(a1 5 5.1 Experiments Data Setup Our aim in this paper is to show th"
P10-2067,J93-2003,0,0.0116582,"ry-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner. 1 Introduction Corpus-based approaches to machine translation have become predominant, with phrase-based statistical machine translation (PB-SMT) (Koehn et al., 2003) being the most actively progressing area. The success of statistical approaches to MT can be attributed to the IBM models (Brown et al., 1993) that characterize word-level alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of SMT systems for various language pairs, the quality of alignment is typically quite low for language pairs like ChineseEnglish, Arabic-English that diverge from the independence assumptions made by the generative models. Increased parallel data enable"
P10-2067,W07-0734,0,0.0352709,"Missing"
P10-2067,P04-1023,0,0.0200747,"scored high by our query strategy. We seek manual corrections for the selected links and add the alignment data to the current labeled data set. The word-level aligned labeled data is provided to our semi-supervised word alignment algorithm for training an alignment model Mt+1 over U . linear model is trained on available labeled data to improve performance. They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. Callison-Burch et al. (2004) also improve alignment by interpolating human alignments with automatic alignments. They observe that while working with such data sets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated to use as a learner in the semi-supervised algorithm to improve word alignment. To our knowledge, there is no prior work that has looked at reducing human effort by selective elicitation of partial word alignment using a"
P10-2067,P06-2014,0,0.0339173,"Missing"
P10-2067,J03-1002,0,0.00582919,"duction Corpus-based approaches to machine translation have become predominant, with phrase-based statistical machine translation (PB-SMT) (Koehn et al., 2003) being the most actively progressing area. The success of statistical approaches to MT can be attributed to the IBM models (Brown et al., 1993) that characterize word-level alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of SMT systems for various language pairs, the quality of alignment is typically quite low for language pairs like ChineseEnglish, Arabic-English that diverge from the independence assumptions made by the generative models. Increased parallel data enables better estimation of the model parameters, but a large number of language pairs still lack such resources. 2 Related Work Researchers have begun to explore models that use both labeled and unlabeled data to build word-alignment models for MT. Fraser and Marcu (2006) pose the problem of alignment as a search probl"
P10-2067,P02-1040,0,0.0783219,"Missing"
P10-2067,P06-1097,0,0.0240295,"sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of SMT systems for various language pairs, the quality of alignment is typically quite low for language pairs like ChineseEnglish, Arabic-English that diverge from the independence assumptions made by the generative models. Increased parallel data enables better estimation of the model parameters, but a large number of language pairs still lack such resources. 2 Related Work Researchers have begun to explore models that use both labeled and unlabeled data to build word-alignment models for MT. Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. The log365 Proceedings of the ACL 2010 Conference Short Papers, pages 365–370, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics model Mt from current iteration t for scoring the links. Re-training and re-tuning an SMT system for each link at a time is computationally infeasible. We therefore perform batch learning by selecting a set of N links scored high by our query strategy. We seek manual corrections for the selected links and add the"
P10-2067,D07-1006,0,0.0187025,"ethod on the newly-labeled and previously-labeled instances to minimize prediction or translation error, repeating until either the maximal number of external queries is reached or a desired accuracy level is achieved. Several studies (Tong and Koller, 2002; Nguyen and Smeulders, 2004; Donmez and Carbonell, 2008) show that active learning greatly helps to reduce the labeling effort in various classification tasks. 3.1 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignment error rate (AER) (Fraser and Marcu, 2007b) in the case of word alignment. In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for IBM models, the training procedure requires for each source sentence positio"
P10-2067,J07-1003,0,0.0799212,"ths reduces to restricting the summation in EM. 4 links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-giventarget models. Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SMT (Huang, 2009). We use confidence metrics as an active learning sampling strategy to obtain most informative links. We also experimented with other confidence metrics as discussed in (Ueffing and Ney, 2007), especially the IBM 1 model score metric, but it did not show significant improvement in this task. Query Strategies for Link Selection We propose multiple query selection strategies for our active learning setup. The scoring criteria is designed to select alignment links across sentence pairs that are highly uncertain under current automatic translation models. These links are difficult to align correctly by automatic alignment and will cause incorrect phrase pairs to be extracted in the translation model, in turn hurting the translation quality of the SMT system. Manual correction of such l"
P10-2067,J07-3002,0,0.0158945,"ethod on the newly-labeled and previously-labeled instances to minimize prediction or translation error, repeating until either the maximal number of external queries is reached or a desired accuracy level is achieved. Several studies (Tong and Koller, 2002; Nguyen and Smeulders, 2004; Donmez and Carbonell, 2008) show that active learning greatly helps to reduce the labeling effort in various classification tasks. 3.1 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignment error rate (AER) (Fraser and Marcu, 2007b) in the case of word alignment. In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for IBM models, the training procedure requires for each source sentence positio"
P10-2067,P06-2117,0,0.0204919,"model Mt+1 over U . linear model is trained on available labeled data to improve performance. They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. Callison-Burch et al. (2004) also improve alignment by interpolating human alignments with automatic alignments. They observe that while working with such data sets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated to use as a learner in the semi-supervised algorithm to improve word alignment. To our knowledge, there is no prior work that has looked at reducing human effort by selective elicitation of partial word alignment using active learning techniques. 3 Algorithm 1 AL FOR W ORD A LIGNMENT 1: Unlabeled Data Set: U = {(Sk , Tk )} 2: Manual Alignment Set : A0 = {akij , ∀si ∈ Sk , tj ∈ Tk } 3: Train Semi-supervised Word Alignment using (U , A0 ) → M0 4: N : batch size 5: for t ="
P10-2067,W08-0509,1,0.787331,", 2004; Donmez and Carbonell, 2008) show that active learning greatly helps to reduce the labeling effort in various classification tasks. 3.1 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignment error rate (AER) (Fraser and Marcu, 2007b) in the case of word alignment. In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for IBM models, the training procedure requires for each source sentence position, the summation over all positions in the target sentence. The manual alignments allow for one-tomany alignments and many-to-many alignments in both directions. For each position i in the source sentence, there can be more than one manually aligned target word. The restricted"
P10-2067,P09-1021,0,0.0244571,"rcu, 2007a). The second is to use extra annotation, typically word-level human alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in a semi-supervised manner. Our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. Active learning for MT has not yet been explored to its full potential. Much of the literature has explored one task – selecting sentences to translate and add to the training corpus (Haffari and Sarkar, 2009). In this paper we explore active learning for word alignment, where the input to the active learner is a sentence pair (S, T ) and the annotation elicited from human is a set of links {aij , ∀si ∈ S, tj ∈ T }. Unlike previous approaches, our work does not require elicitation of full alignment for the sentence pair, which could be effort-intensive. We propose active learning query strategies to selectively elicit partial alignment information. Experiments in Section 5 show that our selection strategies reduce alignment error rates significantly over baseline. Semi-supervised word alignment aim"
P10-2067,P09-1105,0,0.0174733,"nks based on our active learning query strategy. The query strategy uses the automatically trained alignment 366 the manual alignments. Therefore, the restriction of the alignment paths reduces to restricting the summation in EM. 4 links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-giventarget models. Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SMT (Huang, 2009). We use confidence metrics as an active learning sampling strategy to obtain most informative links. We also experimented with other confidence metrics as discussed in (Ueffing and Ney, 2007), especially the IBM 1 model score metric, but it did not show significant improvement in this task. Query Strategies for Link Selection We propose multiple query selection strategies for our active learning setup. The scoring criteria is designed to select alignment links across sentence pairs that are highly uncertain under current automatic translation models. These links are difficult to align correct"
P13-2134,P80-1004,1,0.0803408,"gorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements. 1 Introduction A variety of NLP tasks have been addressed using selectional preferences or restrictions, including word sense disambiguation (see Navigli (2009)), semantic parsing (e.g., Shi and Mihalcea (2005)), and metaphor processing (see Shutova (2010)). These semantic problems are quite challenging; metaphor analysis, for instance, has long been recognized as requiring considerable semantic knowledge (Wilks, 1978; Carbonell, 1980). The advent of extensive lexical resources, annotated corpora, and a spectrum of NLP tools 2 The Preference Violation Detection Task DAVID builds on the insight of Wilks (1978) that the strongest indicator of metaphoricity is the violation of selectional preferences. For example, only plants can literally be pruned. If laws is the object of pruned, the verb is likely metaphorical. Flagging such semantic mismatches between verbs and arguments is the task of preference violation detection. 765 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 765–770"
P13-2134,D11-1022,0,0.0328976,"Missing"
P13-2134,J05-1004,0,0.0381744,"re also ignored. 3 pruned regulating created inspecting The politician laws The politician -- laws . . . bags plastic bags new fees dairy farms Table 1: SENNA’s SRL output for the example sentence above. Though this example demonstrates only two arguments, SENNA is capable of labeling up to six. Algorithm Design To identify violations, DAVID employs a simple algorithm based on several existing tools and resources: SENNA (Collobert et al., 2011), a semantic role labeling (SRL) system; VerbNet, a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: Restriction WordNet Synsets animate animate being.n.01 people.n.01 person.n.01 physical object.n.01 matter.n.01 substance.n.01 social group.n.01 district.n.01 concrete organization Table 2: DAVID’s mappings between some common VerbNet restriction types and WordNet synsets. 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). Each VerbNet restriction is interprete"
P13-2134,P10-1071,0,0.0312002,"Missing"
P13-2134,N03-1033,0,0.0042109,", a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: Restriction WordNet Synsets animate animate being.n.01 people.n.01 person.n.01 physical object.n.01 matter.n.01 substance.n.01 social group.n.01 district.n.01 concrete organization Table 2: DAVID’s mappings between some common VerbNet restriction types and WordNet synsets. 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). Each VerbNet restriction is interpreted as mandating or forbidding a set of WordNet hypernyms, defined by a custom mapping (see Table 2). For example, VerbNet requires both the Patient of a verb in carve-21.2-2 and the Theme of a verb in wipe manner-10.4.1-1 to be concrete. By empirical inspection, concrete nouns are hyponyms of the WordNet synsets physical object.n.01, matter.n.03, or substance.n.04. Laws (the Patient of prune) is a hyponym of none of these, so prune would be flagged as a violation. 2. Run SENNA to identify the se"
P14-1134,W11-0103,0,0.0422234,"Missing"
P14-1134,W06-1655,0,0.0374261,"Missing"
P14-1134,W13-2322,0,0.53377,"gorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints. Our approach is described in the general framework of structured prediction, allowing future incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source system, JAMR, is available at: http://github.com/jflanigan/jamr 1 Introduction Semantic parsing is the problem of mapping natural language strings into meaning representations. Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. Nodes represent concepts, and labeled directed edges represent the relationships between them–see Figure 1 for an example AMR graph. The formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). Although it does not encode quantifiers, tense, or modality, the set of semantic phenomena included in AMR were selected with natural language applications—in particular, machine translation—in mind. In this paper we"
P14-1134,P13-2131,0,0.425377,"alism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). Although it does not encode quantifiers, tense, or modality, the set of semantic phenomena included in AMR were selected with natural language applications—in particular, machine translation—in mind. In this paper we introduce JAMR, the first published system for automatic AMR parsing. The system is based on a statistical model whose parameters are trained discriminatively using annotated sentences in the AMR Bank corpus (Banarescu et al., 2013). We evaluate using the Smatch score (Cai and Knight, 2013), establishing a baseline for future work. The core of JAMR is a two-part algorithm that first identifies concepts using a semi-Markov model and then identifies the relations that obtain between these by searching for the maximum spanning connected subgraph (MSCG) from an edge-labeled, directed graph representing all possible relations between the identified concepts. To solve the latter problem, we introduce an apparently novel O(|V |2 log |V |) algorithm that is similar to the maximum spanning tree (MST) algorithms that are widely used for dependency parsing (McDonald et al., 2005). Our MSCG"
P14-1134,1992.tmi-1.20,1,0.743247,"Missing"
P14-1134,P13-1091,0,0.183068,"Missing"
P14-1134,W02-1001,0,0.542547,"Missing"
P14-1134,S12-1029,1,0.839695,"Missing"
P14-1134,J14-1002,1,0.256753,"Missing"
P14-1134,de-marneffe-etal-2006-generating,0,0.0716777,"Missing"
P14-1134,dorr-etal-1998-thematic,0,0.405692,"Missing"
P14-1134,J02-3001,0,0.064329,"Missing"
P14-1134,C12-1083,0,0.303236,"Missing"
P14-1134,P03-1054,0,0.0983673,"Missing"
P14-1134,D10-1119,0,0.0230971,"Missing"
P14-1134,P11-1060,0,0.0809238,"Missing"
P14-1134,P09-1039,1,0.766485,"Missing"
P14-1134,D11-1022,1,0.524395,"Missing"
P14-1134,P13-2109,1,0.858632,"Missing"
P14-1134,E06-1011,0,0.178173,"Missing"
P14-1134,H05-1066,0,0.221071,"Missing"
P14-1134,J08-2005,0,0.0599703,"Missing"
P14-1134,W09-1119,0,0.154302,"Missing"
P14-1134,W06-1616,0,0.00889884,"Missing"
P14-1134,D07-1071,0,0.0579116,"Missing"
P14-5001,D10-1036,0,0.108076,"al leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; … (Chinese) 義肢: 0.41; 物理治療師: 0.15; 康復:0.10; 阿富汗: 0.08, … English Keywords from Bilingual Perspectives: prosthesis, artificial, leg, rehabilitation, orthopedic, … Figure 1. An example BiKEA keyword analysis for an article. accommodate words with similar meaning. And Huang and Ku (2013) weigh PageRank edges based on nodes’ degrees of reference. In contrast, we bridge PageRank graphs of parallel articles to facilitate statistics re-distribution or interaction between the involved languages. In studies more closely related to our work, Liu et al. (2010) and Zhao et al. (2011) present PageRank algorithms leveraging article topic information for keyword identification. The main differences from our current work are that the article topics we exploit are specified by humans not by automated systems, and that our PageRank graphs are built and connected bilingually. In contrast to the previous research in keyword extraction, we present a system that automatically learns topical keyword preferences and constructs and inter-connects PageRank graphs in bilingual context, expected to yield better and more accurate keyword lists for articles. To the b"
P14-5001,W03-1726,0,0.0163731,"Missing"
P14-5001,I13-1117,1,0.809982,"我們只是給他們 提供義肢。 花了很多年的程序 才讓這計劃成為現在的模樣。… Word Alignment Information: physical (物理), therapist (治療師), social (社會), reintegration (重返), physical (身體), rehabilitation (康 復), prosthesis (義肢), … Scores of Topical Keyword Preferences for Words: (English) prosthesis: 0.32; artificial leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; … (Chinese) 義肢: 0.41; 物理治療師: 0.15; 康復:0.10; 阿富汗: 0.08, … English Keywords from Bilingual Perspectives: prosthesis, artificial, leg, rehabilitation, orthopedic, … Figure 1. An example BiKEA keyword analysis for an article. accommodate words with similar meaning. And Huang and Ku (2013) weigh PageRank edges based on nodes’ degrees of reference. In contrast, we bridge PageRank graphs of parallel articles to facilitate statistics re-distribution or interaction between the involved languages. In studies more closely related to our work, Liu et al. (2010) and Zhao et al. (2011) present PageRank algorithms leveraging article topic information for keyword identification. The main differences from our current work are that the article topics we exploit are specified by humans not by automated systems, and that our PageRank graphs are built and connected bilingually. In contrast to"
P14-5001,J00-2011,0,0.145924,"idging is to take language divergence into account and to allow for language-wise interaction over word statistics. BiKEA, then in bilingual context, iterates with learned word keyness scores to find keywords. In our prototype, BiKEA returns keyword candidates of the article for keyword evaluation (see Figure 1); alternatively, the keywords returned by BiKEA can be used as candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into"
P14-5001,J03-1002,0,0.00588597,"to [1/v,1/ v, …,1/v] repeat Figure 4. Constructing PageRank word graph. Step (3) in Figure 3 linearly combines word graphs EWe and EWc using α. We use α to balance language properties or statistics, and BiKEA backs off to monolingual KEA if α is one. In Step (4) of Figure 3 for each word alignment (wic, wje), we construct a link between the word nodes with the weight BiWeight. The inter-language link is to reinforce language similarities and respect language divergence while the weight aims to elevate the crosslanguage statistics interaction. Word alignments are derived using IBM models 1-5 (Och and Ney, 2003). The inter-language link is directed from wic to wje, basically from language c to e based on the directional word-aligning entry (wic, wje). The bridging is expected to help keyword extraction in language e with the statistics in language c. Although alternative approach can be used for bridging, our approach is intuitive, and most importantly in compliance with the directional spirit of PageRank. Step (6) sets KP of keyword preference model using topical preference scores learned from Section 3.2, while Step (7) initializes KN of PageRank scores or, in our case, word keyness scores. Then we"
P14-5001,N03-1017,0,0.00743078,"Missing"
P14-5001,H05-1059,0,0.0146916,"Missing"
P14-5001,N10-1101,0,0.0307704,"text, iterates with learned word keyness scores to find keywords. In our prototype, BiKEA returns keyword candidates of the article for keyword evaluation (see Figure 1); alternatively, the keywords returned by BiKEA can be used as candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu et al. (2010) construct PageRank synonym graph to 3 The BiKEA System Submitting natural language articles t"
P14-5001,W08-1404,0,0.0608117,"Missing"
P15-2036,S07-1018,0,0.0348331,"nce relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually select corpus examples and annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentence"
P15-2036,P98-1013,0,0.803968,"TIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annotation sources: ‡ t pan tici par ant r cip nce a"
P15-2036,W04-0817,0,0.0210726,"ING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually"
P15-2036,bonial-etal-2014-propbank,0,0.057978,"Missing"
P15-2036,W13-5503,0,0.0575005,"Missing"
P15-2036,J14-1002,1,0.781256,"ant, ACTIVITY _ FINISH by Afinish, and H OLD CTIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annot"
P15-2036,S12-1029,1,0.891611,"model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at mo"
P15-2036,P11-1144,1,0.598328,"sk.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4 Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs an"
P15-2036,N12-1086,1,0.904091,"Missing"
P15-2036,P07-1033,0,0.0592643,"termined by frame identification). We use the heuristic procedure described by (Das et al., 2014) for extracting candidate argument spans for the predicate; call this spans(x, p, f ). spans always includes a special span denoting an empty or nonovert role, denoted ∅. For each candidate argument a ∈ spans(x, p, f ) and each role r, a binary feature vector φ (a,x, p, f ,r) is extracted. We use the feature extractors from (Das et al., 2014) as a baseline, adding additional ones in our experiments (§3.2– §3.4). Each a is given a real-valued score by a linear model: Domain Adaptation and Exemplars Daumé (2007) proposed a feature augmentation approach that is now widely used in supervised domain adaptation scenarios. We use a variant of this approach. Let Dex denote the exemplars training data, and Dft denote the full text training data. For every feature φ (a,x, p, f ,r) in the base model, we add a new feature φft (⋅) that fires only if φ (⋅) fires and x ∈ Dft . The intuition is that each base feature contributes both a “general” weight and a “domain-specific” weight to the model; thus, it can exhibit a general preference for specific roles, but this general preference can be fine-tuned for the dom"
P15-2036,S15-1005,0,0.0135716,"mance upon combining the best approaches. Both use full-text and exemplars for training; the first uses PropBank SRL as guide features, and the second adds hierarchy features. The best result is the 221 0.2 Acknowledgments 100 50 0 Test Examples 150 0.4 over, the techniques discussed here could be further explored using semi-automatic mappings between lexical resources (such as UBY; Gurevych et al., 2012), and correspondingly, this task could be used to extrinsically validate those mappings. Ours is not the only study to show benefit from heterogeneous annotations for semantic analysis tasks. Feizabadi and Padó (2015), for example, successfully applied similar techniques for SRL of implicit arguments.9 Ultimately, given the diversity of semantic resources, we expect that learning from heterogeneous annotations in different corpora will be necessary to build automatic semantic analyzers that are both accurate and robust. 0 200 400 600 800 1000 1200 1400 Frame Element, ordered by test set frequency 0.8 (a) Frequency of each role appearing in the test set. The authors are grateful to Dipanjan Das for his assistance, and to anonymous reviewers for their helpful feedback. This research has been supported by the"
P15-2036,W03-1007,0,0.023046,"annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annot"
P15-2036,J02-3001,0,0.941676,"A0 want ING is evoked by want, ACTIVITY _ FINISH by Afinish, and H OLD CTIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We"
P15-2036,E12-1059,0,0.0207678,"Missing"
P15-2036,P14-1136,0,0.167275,"ts of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4 Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs and (as of recently) eventive noun and adjective predicates. An example with PB annotations is shown in figure 2. We use the mode"
P15-2036,N06-2015,0,0.136256,"Missing"
P15-2036,S12-1016,0,0.0167416,"d T RIAL . C RIMINAL _ PROCESS .Defendant, for instance, is mapped to A RREST.Suspect, T RIAL.Defendant, and S ENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel"
P15-2036,N13-1013,0,0.0347516,"r the domain. Regularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features We experiment with features shared between related roles of related frames in order to capture Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 200"
P15-2036,D14-1108,1,0.685009,"ularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features We experiment with features shared between related roles of related frames in order to capture Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonal"
P15-2036,C04-1179,0,0.0362007,"dicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train"
P15-2036,W04-0803,0,0.0508177,"espect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the docum"
P15-2036,D08-1017,1,0.640396,"case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of overt arguments must not overlap. Beam search, with a beam size of 100, is used to find this argmax.5 3.2 Hierarchy Features 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use two types of guide features: one encodes the"
P15-2036,P09-1003,0,0.0182029,"ED.Wrongdoer, and so forth. Subframe: This indicates a subevent within a complex event. E.g., the C RIMINAL _ PROCESS frame groups together subframes A RREST, A RRAIGN MENT and T RIAL . C RIMINAL _ PROCESS .Defendant, for instance, is mapped to A RREST.Suspect, T RIAL.Defendant, and S ENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been"
P15-2036,P08-1108,0,0.0186663,"Missing"
P15-2036,J05-1004,0,0.717357,"Missing"
P15-2036,P15-2067,0,0.0221741,"Missing"
P15-2036,J08-2005,0,0.587921,"Missing"
P15-2036,Q15-1003,0,0.23587,"domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of"
P15-2036,C98-1013,0,\N,Missing
P15-2105,P11-1038,0,0.0195409,"ical variants in Twitter (e.g., “cats vs. catz”). While variants tend to have the same meaning as their standardized form, the proposed model does not have this information and will not be able to generalize properly. For instance, if the term ”John” is labelled as keyword in the training set, the model would not be able to extract ”Jooohn” as keyword as it is in a different word form. One way to adThe corpus is submitted as supplementary material. 638 dress this would be using a normalization system either built using hand engineered rules (Gouws et al., 2011) or trained using labelled data (Han and Baldwin, 2011; Chrupała, 2014). However, these systems are generally limited as these need supervision and cannot scale to new data or data in other languages. Instead, we will used unsupervised methods that leverage large amounts of unannotated data. We used two popular methods for this purpose: Brown Clustering and Continuous Word Vectors. uments, such as a news article, contain approximately 3-5 keywords, so extracting 3 keywords per document is a reasonable option. However, this would not work in Twitter, since the number of keywords can be arbitrary small. In fact, many tweets contain less than three"
P15-2105,C10-2042,0,0.0355775,"selecting keywords that are chosen by at least three annotators. We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in (Gimpel et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including ne"
P15-2105,P12-1092,0,0.0163423,"11010, share the first three nodes in the hierarchically 110. Sharing more tree nodes tends to translate into better similarity between words within the clusters. Thus, a word a 11001 cluster is simultaneously in clusters 1, 11, 110, 1100 and 11001, and a feature can be extracted for each cluster. In our experiments, we used the dataset with 1,000 Brown clusters made available by Owoputi et al. (Owoputi et al., 2013)2 . 4.1.2 Continuous Word Vectors Word representations learned from neural language models are another way to learn more generalizable features for words (Collobert et al., 2011; Huang et al., 2012). In these models, a hidden layer is defined that maps words into a continuous vector. The parameters of this hidden layer are estimated by maximizing a goal function, such as the likelihood of each word predicting surrounding words (Mikolov et al., 2013; Ling et al., 2015). In our work, we used the structured skip-ngram goal function proposed in (Ling et al., 2015) and for each word we extracted its respective word vector as features. 5 Experiments Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3. As baselines, we reported re"
P15-2105,P14-2111,0,0.0188732,"r (e.g., “cats vs. catz”). While variants tend to have the same meaning as their standardized form, the proposed model does not have this information and will not be able to generalize properly. For instance, if the term ”John” is labelled as keyword in the training set, the model would not be able to extract ”Jooohn” as keyword as it is in a different word form. One way to adThe corpus is submitted as supplementary material. 638 dress this would be using a normalization system either built using hand engineered rules (Gouws et al., 2011) or trained using labelled data (Han and Baldwin, 2011; Chrupała, 2014). However, these systems are generally limited as these need supervision and cannot scale to new data or data in other languages. Instead, we will used unsupervised methods that leverage large amounts of unannotated data. We used two popular methods for this purpose: Brown Clustering and Continuous Word Vectors. uments, such as a news article, contain approximately 3-5 keywords, so extracting 3 keywords per document is a reasonable option. However, this would not work in Twitter, since the number of keywords can be arbitrary small. In fact, many tweets contain less than three words, in which c"
P15-2105,W12-3153,0,0.0173412,"tugal {luis.marujo,wang.ling,isabel.trancoso,david.matos,joao.neto}@inesc-id.pt {cdyer,awb,anatoleg,jgc}@cs.cmu.edu, Abstract These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters. The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts. In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging (Gimpel et al., 2011), Machine Translation (Jelh et al., 2012; Ling et al., 2013), Named Entity Recognition (Ritter et al., 2011; Liu et al., 2013), Information Retrieval (Efron, 2011) and Summarization (Duan et al., 2012; Chang et al., 2013). As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications. In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We i"
P15-2105,P13-1018,1,0.272005,"wang.ling,isabel.trancoso,david.matos,joao.neto}@inesc-id.pt {cdyer,awb,anatoleg,jgc}@cs.cmu.edu, Abstract These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters. The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts. In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging (Gimpel et al., 2011), Machine Translation (Jelh et al., 2012; Ling et al., 2013), Named Entity Recognition (Ritter et al., 2011; Liu et al., 2013), Information Retrieval (Efron, 2011) and Summarization (Duan et al., 2012; Chang et al., 2013). As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications. In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differen"
P15-2105,N15-1142,1,0.664689,"racted for each cluster. In our experiments, we used the dataset with 1,000 Brown clusters made available by Owoputi et al. (Owoputi et al., 2013)2 . 4.1.2 Continuous Word Vectors Word representations learned from neural language models are another way to learn more generalizable features for words (Collobert et al., 2011; Huang et al., 2012). In these models, a hidden layer is defined that maps words into a continuous vector. The parameters of this hidden layer are estimated by maximizing a goal function, such as the likelihood of each word predicting surrounding words (Mikolov et al., 2013; Ling et al., 2015). In our work, we used the structured skip-ngram goal function proposed in (Ling et al., 2015) and for each word we extracted its respective word vector as features. 5 Experiments Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3. As baselines, we reported results using a TF-IDF, the default MAUI toolkit, and our own implementation of (Li et al., 2010) framework. In all cases the IDF component was computed over a collection of 52 million tweets. Results are reported on rows 1 and 2 in Table 1, respectively. The parameter k (col"
P15-2105,W08-1404,0,0.00839334,"e frequently used in many occasions as indicators of important information contained in documents. These can be used by human readers to search for their desired documents, but also in many Natural Language Processing (NLP) applications, such as Text Summarization (Pal et al., ¨ ur et al., 2005), 2013), Text Categorization (Ozg¨ Information Retrieval (Marujo et al., 2011a; Yang and Nyberg, 2015) and Question Answering (Liu and Nyberg, 2013). Many automatic frameworks for extracting keywords have been proposed (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Litvak and Last, 2008). These systems were built for more formal domains, such as news data or Web data, where the content is still produced in a controlled fashion. The emergence of social media environments, such as Twitter and Facebook, has created a framework for more casual data to be posted online. 1. Provide a annotated keyword annotated dataset consisting of 1827 tweets. These tweets are obtained from (Gimpel et al., 2011), and also contain POS annotations. 2. Improve a state-of-the-art keyword extraction system (Marujo et al., 2011b; Marujo et al., 2013) for this domain by learning additional features in a"
P15-2105,N13-1039,1,0.474503,"Missing"
P15-2105,D10-1036,0,0.0132353,"l et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including new set of features and more robust classifier, remains the state-of-the-art system in the news domain (Marujo et al., 2012). To the best of our knowledge, only (Li et al., 2010) used a supervised key"
P15-2105,D11-1141,0,0.0220653,"Missing"
P15-2105,marujo-etal-2012-supervised,1,0.940917,"ihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including new set of features and more robust classifier, remains the state-of-the-art system in the news domain (Marujo et al., 2012). To the best of our knowledge, only (Li et al., 2010) used a supervised keyword extraction framework (based on KEA) with additional features, such as POS tags to performed keyword extraction on Facebook posts. However, at that time Facebook status updates or posts did not contained either hashtags or user mentions. The size of Facebook posts is frequently longer than tweets and has less abbreviations since it is not limited by number of character as in tweets. 3 4 There are many methods that have been proposed for keyword extraction. TF-IDF is one of the simplest approaches for this end (Salt"
P15-2105,N10-1101,0,0.0485415,"ly 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work formation (e.g., retweet). The annotations of each annotator are combined by selecting keywords that are chosen by at least three annotators. We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in (Gimpel et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and fi"
P15-2105,W04-3252,0,\N,Missing
P15-2105,W11-2210,0,\N,Missing
P15-2105,P11-1039,0,\N,Missing
P15-2105,P11-2008,0,\N,Missing
P15-2105,C12-1047,0,\N,Missing
P19-1285,P18-1027,0,0.0896094,"Yang et al. (2017). 1 https://github.com/kimiyoung/ transformer-xl Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), have been a standard solution to language modeling and obtained strong results on multiple benchmarks. Despite the wide adaption, RNNs are difficult to optimize due to gradient vanishing and explosion (Hochreiter et al., 2001), and the introduction of gating in LSTMs and the gradient clipping technique (Graves, 2013) might not be sufficient to fully address this issue. Empirically, previous work has found that LSTM language models use 200 context words on average (Khandelwal et al., 2018), indicating room for further improvement. On the other hand, the direct connections between long-distance word pairs baked in attention mechanisms might ease optimization and enable the learning of long-term dependency (Bahdanau et al., 2014; Vaswani et al., 2017). Recently, Al-Rfou et al. (2018) designed a set of auxiliary losses to train deep Transformer networks for character-level language modeling, which outperform LSTMs by a large margin. Despite the success, the LM training in Al-Rfou et al. (2018) is performed on separated fixed-length segments of a few hundred characters, without any"
P19-1285,N18-1202,0,0.597194,"he state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch1 . 1 Introduction Language modeling is among the important problems that require modeling long-term dependency, with successful applications such as unsupervised pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). However, it has been a challenge to equip neural networks with the capability to model long-term dependency in sequential data. Recurrent neural networks (RNNs), in particular Long Short⇤ Equal contribution. Order determined by swapping the one in Yang et al. (2017). 1 https://github.com/kimiyoung/ transformer-xl Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), have been a standard solution to language modeling and obtained strong results on multiple benchmarks. Despite the wide adaption, RNNs are difficult to optimize due to gradien"
P19-1285,N18-2074,0,0.222672,"relative posi2981 tional encodings R 2 RLmax ⇥d , where the i-th row Ri indicates a relative distance of i between two positions. By injecting the relative distance dynamically into the attention score, the query vector can easily distinguish the representations of x⌧,j and x⌧ +1,j from their different distances, making the state reuse mechanism feasible. Meanwhile, we won’t lose any temporal information, as the absolute position can be recovered recursively from relative distances. Previously, the idea of relative positional encodings has been explored in the context of machine translation (Shaw et al., 2018) and music generation (Huang et al., 2018). Here, we offer a different derivation, arriving at a new form of relative positional encodings, which not only has a one-to-one correspondence to its absolute counterpart but also enjoys much better generalization empirically (see Section 4). Firstly, in the standard Transformer (Vaswani et al., 2017), the attention score between query qi and key vector kj within the same segment can be decomposed as > > > > Aabs i,j = Exi Wq Wk Exj + Exi Wq Wk Uj {z } | {z } | (b) (a) + > U> i Wq Wk Exj | {z (c) } > + U> i Wq Wk Uj | {z (d) } . Following the idea of"
P19-1286,D16-1162,1,0.800355,"method does not depend on model architectures, which makes it orthogonal to these model-based methods. Our work shows that apart from strengthening the target-side decoder, direct supervision over the in-domain unseen words is essential for domain adaptation. Similar to this, a variety of methods focus on solving OOV problems in translation. Daum´e III and Jagarlamudi (2011) induce lexicons for unseen words and construct phrase tables for statistical machine translation. However, it is nontrivial to integrate lexicon into NMT models that lack explicit use of phrase tables. With regard to NMT, Arthur et al. (2016) use a lexicon to bias the probability of the NMT system and show promising improvements. Luong and Manning (2015) propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary. Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting. Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decodi"
P19-1286,Q17-1010,0,0.0129557,"te that these domains are very distant from each other. Following Koehn and Knowles (2017), we process all the data with byte-pair encoding (Sennrich et al., 2016b) to construct a vocabulary of 50K subwords. To build an unaligned monolingual corpus for each domain, we randomly shuffle the parallel corpus and split the corpus into two parts with equal numbers of parallel sentences. We use the target and source sentences of the first and second halves respectively. We combine all the unaligned monolingual source and target sentences on all five domains to train a skip-gram model using fasttext (Bojanowski et al., 2017). We obtain source and target word embeddings in 512 dimensions by running 10 epochs with a context window of 10, and 10 negative samples. Corpus Medical IT Subtitles Law Koran Words 12,867,326 2,777,136 106,919,386 15,417,835 9,598,717 Sentences 1,094,667 333,745 13,869,396 707,630 478,721 W/S 11.76 8.32 7.71 21.80 20.05 Table 2: Corpus statistics over five domains. 3.2 Main Results We first compare DALI with other adaptation strategies on both RNN-based and Transformerbased NMT models. Table 1 shows the performance of the two models when trained on one domain (columns) and tested on another"
P19-1286,W17-4712,0,0.0784365,"In this paper, we try to fill this gap, examining domain adaptation methods for NMT specifically focusing on correctly translating unknown words. As noted by Chu and Wang (2018), there are two important distinctions to make in adaptation methods for MT. The first is data requirements; supervised adaptation relies on in-domain parallel data, and unsupervised adaptation has no such requirement. There is also a distinction between model-based and data-based methods. Modelbased methods make explicit changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervis"
P19-1286,P16-1185,0,0.017489,"thetic parallel data. Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, model-based methods change model architectures to leverage monolingual corpus by introducing an extra learning objective, such as auto-encoder objective (Cheng et al., 2016) and language modeling objective (Ramachandran et al., 2017). Another line of research on using monolingual data is unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018b,a; Yang et al., 2018). These methods use word-for-word translation as a component, but require a careful design of model architectures, and do not explicitly tackle the domain adaptation problem. Our proposed data-based method does not depend on model architectures, which makes it orthogonal to these model-based methods. Our work shows that apart from strengthening the target-side decoder, direct supervi"
P19-1286,W17-4714,0,0.0624612,"Missing"
P19-1286,C18-1111,0,0.0594611,"ve domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines. 1 Introduction Neural machine translation (NMT) has demonstrated impressive performance when trained on large-scale corpora (Bojar et al., 2018). However, it has also been noted that NMT models trained on corpora in a particular domain tend to perform poorly when translating sentences in a significantly different domain (Chu and Wang, 2018; Koehn and Knowles, 2017). Previous work in the context of phrase-based statistical machine translation (Daum´e III and Jagarlamudi, 2011) has noted that unseen (OOV) words account for a large portion of translation errors when switching to new domains. However this problem of OOV words in cross-domain transfer is under-examined Code/scripts are released at https://github.com/ junjiehu/dali. in the context of NMT, where both training methods and experimental results will differ greatly. In this paper, we try to fill this gap, examining domain adaptation methods for NMT specifically focusing o"
P19-1286,W17-4715,0,0.379184,"chitecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017). However, while these methods have potential to strengthen the target-language decoder through addition of in-domain target data, they do not explicitly provide direct supervision of domainspecific words, which we ar"
P19-1286,P11-2071,0,0.0967527,"Missing"
P19-1286,D17-1158,0,0.0505273,"y focusing on correctly translating unknown words. As noted by Chu and Wang (2018), there are two important distinctions to make in adaptation methods for MT. The first is data requirements; supervised adaptation relies on in-domain parallel data, and unsupervised adaptation has no such requirement. There is also a distinction between model-based and data-based methods. Modelbased methods make explicit changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-trans"
P19-1286,P17-2090,0,0.019089,"to this, a variety of methods focus on solving OOV problems in translation. Daum´e III and Jagarlamudi (2011) induce lexicons for unseen words and construct phrase tables for statistical machine translation. However, it is nontrivial to integrate lexicon into NMT models that lack explicit use of phrase tables. With regard to NMT, Arthur et al. (2016) use a lexicon to bias the probability of the NMT system and show promising improvements. Luong and Manning (2015) propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary. Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting. Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decoding. Zhao et al. (2018) design a contextual memory module in an NMT system to memorize translations of rare words. Kothur et al. (2018) treats an annotated lexicon as parallel sentences and continues training the NMT system on the lexicon. Though all these works lever"
P19-1286,D15-1147,0,0.0704678,"Missing"
P19-1286,P17-4012,0,0.0999685,"Missing"
P19-1286,kobus-etal-2017-domain,0,0.0414302,"18), there are two important distinctions to make in adaptation methods for MT. The first is data requirements; supervised adaptation relies on in-domain parallel data, and unsupervised adaptation has no such requirement. There is also a distinction between model-based and data-based methods. Modelbased methods make explicit changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a),"
P19-1286,W17-3204,0,0.363591,"ty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines. 1 Introduction Neural machine translation (NMT) has demonstrated impressive performance when trained on large-scale corpora (Bojar et al., 2018). However, it has also been noted that NMT models trained on corpora in a particular domain tend to perform poorly when translating sentences in a significantly different domain (Chu and Wang, 2018; Koehn and Knowles, 2017). Previous work in the context of phrase-based statistical machine translation (Daum´e III and Jagarlamudi, 2011) has noted that unseen (OOV) words account for a large portion of translation errors when switching to new domains. However this problem of OOV words in cross-domain transfer is under-examined Code/scripts are released at https://github.com/ junjiehu/dali. in the context of NMT, where both training methods and experimental results will differ greatly. In this paper, we try to fill this gap, examining domain adaptation methods for NMT specifically focusing on correctly translating un"
P19-1286,W18-2708,0,0.0171436,"propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary. Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting. Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decoding. Zhao et al. (2018) design a contextual memory module in an NMT system to memorize translations of rare words. Kothur et al. (2018) treats an annotated lexicon as parallel sentences and continues training the NMT system on the lexicon. Though all these works leverage a lexicon to address the problem of OOV words, none specifically target translating in-domain OOV words under a domain adaptation setting. 5 Conclusion In this paper, we propose a data-based, unsupervised adaptation method that focuses on domain adaption by lexicon induction (DALI) for mitigating unknown word problems in NMT. We conduct extensive experiments to show consistent improvements of two popular NMT models through the usage of our proposed method. Fu"
P19-1286,W11-2132,0,0.0308622,"umvent the domain shift problem by jointly learning domain discrimination and the translation. Joty et al. (2015) and Wang et al. (2017) address the domain adaptation problem by assigning higher weight to out-ofdomain parallel sentences that are close to the indomain corpus. Our proposed method focuses on solving the adaptation problem with no in-domain parallel sentences, a strict unsupervised setting. Prior work on using monolingual data to do data augmentation could be easily adapted to the domain adaptation setting. Early studies on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the system on the synthetic parallel data. Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, model-based methods change"
P19-1286,J82-2005,0,0.551376,"Missing"
P19-1286,2015.iwslt-evaluation.11,0,0.70816,"nd unsupervised adaptation has no such requirement. There is also a distinction between model-based and data-based methods. Modelbased methods make explicit changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017). However, while these methods have pote"
P19-1286,J03-1002,0,0.0141981,"ord embeddings of the i-th translation pair (s, t)i by the ith column vectors of X(n) , Y(n) ∈ Rd×n respectively. Xing et al. (2015) show that by enforcing an orthogonality constraint on W ∈ Od (R), we can obtain a closed-form solution from a singular T value decomposition (SVD) of Y(n) X(n) : W∗ = arg max W∈Od (R) kY(n) − WX(n) kF = UVT T UΣVT = SVD(Y(n) X(n) ). (1) In a domain adaptation setting we have parallel out-of-domain data Dparallel-out , which can be used to extract a seed lexicon. Algorithm 1 shows the procedure of extracting this lexicon. We use the word alignment toolkit GIZA++ (Och and Ney, 2003) to extract word translation probabilities P (t|s) and P (s|t) in both forward and backward directions from Dparallel-out , and extract lexicons Lfw = {(s, t), ∀P (t|s) > 0} and Lbw = 2990 Algorithm 1 Supervised lexicon extraction Input: Parallel out-of-domain data Dparallel-out Output: Seed lexicon L = {(s, t)}ni=1 1: Run GIZA++ on Dparallel-out to get Lfw , Lbw 2: Lg = Lfw ∪ Lbw 3: Remove pairs with punctuation only in either s and t from Lg 4: Initialize a counter C[(s, t)] = 0 ∀(s, t) ∈ Lg 5: for (src, tgt) ∈ Dparallel-out do 6: for (s, t) ∈ Lg do 7: if s ∈ src and t ∈ tgt then 8: C[(s, t)"
P19-1286,D17-1039,0,0.0530467,"Missing"
P19-1286,P16-1162,0,0.716803,"changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017). However, while these methods have potential to strengthen the target-language decoder through addition of in-domain target data, they do not explicitly provide direct supervision of domainspec"
P19-1286,P17-2089,0,0.0459512,"rrectly translated in-domain keywords of the sentence. 4 Related Work There is much work on supervised domain adaptation setting where we have large out-of-domain parallel data and much smaller in-domain parallel data. Luong and Manning (2015) propose training a model on an out-of-domain corpus and do finetuning with small sized in-domain parallel data 2996 to mitigate the domain shift problem. Instead of naively mixing out-of-domain and in-domain data, Britz et al. (2017) circumvent the domain shift problem by jointly learning domain discrimination and the translation. Joty et al. (2015) and Wang et al. (2017) address the domain adaptation problem by assigning higher weight to out-ofdomain parallel sentences that are close to the indomain corpus. Our proposed method focuses on solving the adaptation problem with no in-domain parallel sentences, a strict unsupervised setting. Prior work on using monolingual data to do data augmentation could be easily adapted to the domain adaptation setting. Early studies on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the syst"
P19-1286,N15-1104,0,0.0431559,"müdigkeit: tiredness … !∗ Induction &apos; $ Pseudo-in-domain Source Corpus Figure 1: Work flow of domain adaptation by lexicon induction (DALI). data-based method for unsupervised adaptation that specifically focuses the unknown word problem: domain adaptation by lexicon induction (DALI). Our proposed method leverages large amounts of monolingual data to find translations of in-domain unseen words, and constructs a pseudo-parallel in-domain corpus via word-forword back-translation of monolingual in-domain target sentences into source sentences. More specifically, we leverage existing supervised (Xing et al., 2015) and unsupervised (Conneau et al., 2018) lexicon induction methods that project source word embeddings to the target embedding space, and find translations of unseen words by their nearest neighbors. For supervised lexicon induction, we learn such a mapping function under the supervision of a seed lexicon extracted from out-of-domain parallel sentences using word alignment. For unsupervised lexicon induction, we follow Conneau et al. (2018) to infer a lexicon by adversarial training and iterative refinement. In the experiments on German-to-English translation across five domains (Medical, IT,"
P19-1286,P18-1005,0,0.0225506,"of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, model-based methods change model architectures to leverage monolingual corpus by introducing an extra learning objective, such as auto-encoder objective (Cheng et al., 2016) and language modeling objective (Ramachandran et al., 2017). Another line of research on using monolingual data is unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018b,a; Yang et al., 2018). These methods use word-for-word translation as a component, but require a careful design of model architectures, and do not explicitly tackle the domain adaptation problem. Our proposed data-based method does not depend on model architectures, which makes it orthogonal to these model-based methods. Our work shows that apart from strengthening the target-side decoder, direct supervision over the in-domain unseen words is essential for domain adaptation. Similar to this, a variety of methods focus on solving OOV problems in translation. Daum´e III and Jagarlamudi (2011) induce lexicons for uns"
P19-1286,D16-1160,0,0.0287716,"s on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the system on the synthetic parallel data. Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, model-based methods change model architectures to leverage monolingual corpus by introducing an extra learning objective, such as auto-encoder objective (Cheng et al., 2016) and language modeling objective (Ramachandran et al., 2017). Another line of research on using monolingual data is unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018b,a; Yang et al., 2018). These methods use word-for-word translation as a component, but require a careful design of model architectures, and do not explicitly tackle the domain adaptation problem. O"
P19-1286,D18-1036,0,0.019901,"a lexicon to bias the probability of the NMT system and show promising improvements. Luong and Manning (2015) propose to emit OOV target words by their corresponding source words and do post-translation for those OOV words with a dictionary. Fadaee et al. (2017) propose an effective data augmentation method that generates sentence pairs containing rare words in synthetically created contexts, but this requires parallel training data not available in the fully unsupervised adaptation setting. Arcan and Buitelaar (2017) leverage a domainspecific lexicon to replace unknown words after decoding. Zhao et al. (2018) design a contextual memory module in an NMT system to memorize translations of rare words. Kothur et al. (2018) treats an annotated lexicon as parallel sentences and continues training the NMT system on the lexicon. Though all these works leverage a lexicon to address the problem of OOV words, none specifically target translating in-domain OOV words under a domain adaptation setting. 5 Conclusion In this paper, we propose a data-based, unsupervised adaptation method that focuses on domain adaption by lexicon induction (DALI) for mitigating unknown word problems in NMT. We conduct extensive ex"
P19-1286,2008.iwslt-papers.6,0,0.0298477,"al. (2017) circumvent the domain shift problem by jointly learning domain discrimination and the translation. Joty et al. (2015) and Wang et al. (2017) address the domain adaptation problem by assigning higher weight to out-ofdomain parallel sentences that are close to the indomain corpus. Our proposed method focuses on solving the adaptation problem with no in-domain parallel sentences, a strict unsupervised setting. Prior work on using monolingual data to do data augmentation could be easily adapted to the domain adaptation setting. Early studies on databased methods such as self-enhancing (Schwenk, 2008; Lambert et al., 2011) translate monolingual source sentences by a statistical machine translation system, and continue training the system on the synthetic parallel data. Recent databased methods such as back-translation (Sennrich et al., 2016a) and copy-based methods (Currey et al., 2017) mainly focus on improving fluency of the output sentences and translation of identical words, while our method targets OOV word translation. In addition, there have been several attempts to do data augmentation using monolingual source sentences (Zhang and Zong, 2016; ChineaRios et al., 2017). Besides, mod"
P19-1286,P16-1009,0,0.690315,"changes to the model architecture such as jointly learning domain discrimination and translation (Britz et al., 2017), interpolation of language modeling and translation (Gulcehre et al., 2015; Domhan and Hieber, 2017), and domain control by adding tags and word features (Kobus et al., 2017). On the other hand, data-based methods perform adaptation either by combining in-domain and out-of-domain parallel corpora for supervised adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016) or by generating pseudo-parallel corpora from indomain monolingual data for unsupervised adaptation (Sennrich et al., 2016a; Currey et al., 2017). Specifically, in this paper we tackle the task of data-based, unsupervised adaptation, where representative methods include creation of a pseudoparallel corpus by back-translation of in-domain monolingual target sentences (Sennrich et al., 2016a), or construction of a pseudo-parallel indomain corpus by copying monolingual target sentences to the source side (Currey et al., 2017). However, while these methods have potential to strengthen the target-language decoder through addition of in-domain target data, they do not explicitly provide direct supervision of domainspec"
P19-1286,D18-1549,0,\N,Missing
P19-1286,W18-6401,0,\N,Missing
P80-1004,P79-1002,1,0.846204,"Missing"
P81-1032,P79-1002,1,0.905428,"ctions. At the moment, the algorithm works only on a mixture of case constructions and linear patterns, but eventually we envisage a 144 can contain unknown words (e.g., Mr. Joe Gallen D&apos;Aguila is obviously a name with D&apos;Aguila as the surname) but subject to ordering constraints and morphological preferences. When unknown words are encountered in other positions in a sentence, the parser may try morphological decomposition, spelling correction, querying the user, or more complex processes to induce the probable meaning of unknown words, such as the project-and-integrate technique described in [3]. Clearly these unknown.word strategies ought to be suppressed in parsing person names. 3. A • Structured-object (e.g., a concept with subattributes) .- Case-frame parser, starting with the pattern-marcher invoked on the list of patterns corresponding to the names (or compound names) of the semantically permissible structured objects, followed by case-frame parsing of any present subattributes. • Case-Oriented Parsing Strategy Simple Object .- Apply the pattern matcher, using only the patterns indexed as relevant in the case-fillerinformation field. Special Object -- Apply the .parsing strateg"
P81-1032,P80-1024,0,0.375379,". 1. Introduction With these goals in mind, we proceed to give details of the kinds of difficulties that a uniform parsing strategy can lead to, and show how dynamically-selected construction.specific techniques can help. We list a number of such specific strategies, then we focus on our initial implementation of two of these strategies and the mechanism that dynamically selects between them while pm&apos;alng task-oriented natural language imperative constructions. Imperatives were chosen largely because commands and queries given to a task-oriented natural language front end often take that form [6]. When people use language spontaneously, they o~ten do not respect grammatical niceties. Instead of producing sequences of grammatically well-formed and complete sentences, they often miss out or repeat words or phrases, break off what they are .saying and rephrase or replace it, speak in fragments, or use otherwise incorrect grammar. While other people generally have little trouble co&apos;reprehending ungrammatical utterances, most&apos; natural language computer systems are unable to process errorful input at all. Such inflexibility in parsing is a serious impediment to the use of natural language i"
P81-1032,P79-1006,0,0.13141,"y well-formed and complete sentences, they often miss out or repeat words or phrases, break off what they are .saying and rephrase or replace it, speak in fragments, or use otherwise incorrect grammar. While other people generally have little trouble co&apos;reprehending ungrammatical utterances, most&apos; natural language computer systems are unable to process errorful input at all. Such inflexibility in parsing is a serious impediment to the use of natural language in interactive computer systems. Accordingly, we [6] and other researchers including Wemchedel and Black [14], and Kwasny and Sondhelmer [9], have attempted to produce flexible parsers, i.e. parsers that can accept ungrammatical input, correcting the errors whan possible, and generating several alternative interpretations if appropriate. 2. Problems with a Uniform Parsing Strategy Our present flexible parser, which we call RexP, is intended to parse correctly input that correaponds to a fixed grammar, and also to deal with input that deviates from that grammar by erring along certain classes of common ungrammaticalities. Because of these goals, the parser is based on the combination of two uniform parsing strategies: bottom-up par"
P81-1032,P80-1026,0,\N,Missing
P83-1025,T78-1017,0,0.0374125,"Missing"
P83-1025,P81-1032,1,0.918565,"Missing"
P83-1025,P79-1006,0,\N,Missing
P83-1025,J78-3024,0,\N,Missing
P84-1089,P81-1033,1,0.930363,"the example abcve, ""pror' has 16 possible corrections in a small online dictionary. However, domain semantics allow only one word in the same position as ""pror', so correction is most effective if the list of possible words is generated first. • The word is a proper name or a unique identifier, such as a catalogue part name/number, not heretofore encountered by the system, but recognizable by a combination of contextual expectations and morphological or orthographic features (e.g., capitalization). In the first situation, there is no meaningful recovery strategy other than focused interaction[15] to inform the user of the precise difficulty. In the third, little action is required beyond recognizing the proper name and recording it appropriately for future reference. The second situation is more complicated; three basic recovery strategies are possible: 1. Follow the KLAUS[14] approach where the system temporarily wrests initiative from the user and plays a well designed ""twenty questions"" game, classifying the unknown term syntactically, and relating it semantically to existing concepts encoded in an inheritance hierarchy. This method has proven successful for verbs, nouns and adject"
P84-1089,J81-4002,1,0.870223,"ons. Since robust parsers must deal primarily with input that does meet their expectations, the various efforts at coping with extragrammaticality have generally been structured as extensions to existing parsing methods. Probably the most popular approach has been to extend syntactically.oriented parsing techniques employing Augmented Transition Networks (ATNs) [21, 24, 25, 29]. Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques [19. 20j. through extensions to pattern matching parsing in which partial pattern matching is allowed [16], through conceptual case frame instantiafion [12, 22], and through approaches involving multiple cooperating parsing strategies [7, 9, 18]. Extragrammaticalities include patently ungrammatical constructions, which may nevertheless be semantically comprehensible, as well as lexical difficulties (e.g. misspellings), violations of semantic constraints, utterances that may be grammatically acceptable but are beyond the syntactic coverage of the system, ellipsed fragments and other dialogue phenomena, and any other difficulties that may arise in parsing individual utterances• An extragrammaticalit"
P84-1089,J81-2002,0,0.0840923,"able of processing input utterances that deviate from its grammatical and semantic expectations. Many researchers have made this observation and have taken initial steps towards coverage of certain classes of extragrammatical constructions. Since robust parsers must deal primarily with input that does meet their expectations, the various efforts at coping with extragrammaticality have generally been structured as extensions to existing parsing methods. Probably the most popular approach has been to extend syntactically.oriented parsing techniques employing Augmented Transition Networks (ATNs) [21, 24, 25, 29]. Other researchers have attempted to deal with ungrammatical input through network-based semantic grammar techniques [19. 20j. through extensions to pattern matching parsing in which partial pattern matching is allowed [16], through conceptual case frame instantiafion [12, 22], and through approaches involving multiple cooperating parsing strategies [7, 9, 18]. Extragrammaticalities include patently ungrammatical constructions, which may nevertheless be semantically comprehensible, as well as lexical difficulties (e.g. misspellings), violations of semantic constraints, utterances that may be"
P84-1089,J80-1001,0,0.0322015,"rsing found in the literature. We consider three basic classes: transition network approaches (including syntactic ATNs and network-based semantic grammars), pattern matching approaches, and approaches based on case frame instantiation. These classes cover the majority of current catsing systems for restricted domain languages. semantic criteria [2, 3]. In the former technique, the appropriate semantic information for recovery can be applied only if the correct network node can be located - - a sometimes difficult task as we have seen. In the latter technique, sometimes known as cascaded ATNs [27], the syntactic and semantic parts of the grammar are kept separate, thus giving the potential for a higher d~gree of interpretivem:ss in using the semantic information. However, semantic information represented in this fashion is generally only used to confirm or disconfirm parses arrived at on syntactic grounds and does not participate directly in the parsing process. All three approaches are able to cope with lexical level problems satisfactorily. However, as we have seen, the application of semantic constraints often makes the correction of lexical problems more efficient and less prone to"
P84-1089,P79-1002,1,0.820692,"art of its initial morphological phase. This process is triggered by failing to recognize the inflected form as a wind that is present in the dictionary. It operates by applying standard morphological rules (e.g. - t e s => +,y) to derive a root from the inflected form. It a simple matter to check first for inflected forms and then for misspellings. However, if a word is both inflected and misspelt, the expectation-based spelling correcter must be invoked from within the morphological decomposition routines on potentially misspelt roots or inflexions. 2. Apply the project and integrate method [6] to infer the meaning and syntactic category o.f the word from context. This method has proven useful for nouns and adjectives whose meaning can be viewed as a recombination of features present elsewhere in the input. Unlike the KLAUS method, it operates in the background, placing no major run-time burden on the user. However, it remains highly experimental and may not prove practical without user confirmation. 3. Interact with the user in a focused manner to provide a paraphrase of the segment of input containing the unknown word. If this paraphrase results in the desired action, it is stored"
P84-1089,J80-2003,0,\N,Missing
P84-1089,J80-1002,0,\N,Missing
P84-1089,J83-3003,0,\N,Missing
P84-1089,J83-3001,1,\N,Missing
P84-1089,P83-1025,1,\N,Missing
Q17-1009,S07-1018,0,0.0832889,"ang, 2005), which take a “constructions all the way down” approach. Some HPSG parsers and formalisms, particularly those based on the English Resource Grammar (Copestake and Flickinger, 2000; Flickinger, 2011) or Sign-Based Construction Grammar (Boas and Sag, 2012), also take constructions into account. Thus far, however, only a few attempts (e.g., Hwang and Palmer, 2015) have been made to 128 integrate constructions with robust, broad-coverage NLP tools/representations. Other aspects of our work are more closely related to previous NLP research. Our task is similar to frame-semantic parsing (Baker et al., 2007), the task of automatically producing FrameNet annotations. Lexical triggers of a frame correspond roughly to our causal connectives, and both tasks require identifying argument spans for each trigger. The tasks differ in that FrameNet covers a much wider range of semantics, with more frame-specific argument types, but its triggers are limited to lexical units, whereas we permit arbitrary constructions. Our multi-stage approach is also loosely inspired by SEMAFOR and subsequent FrameNet parsers (Das et al., 2014; Roth and Lapata, 2015; T¨ackstr¨om et al., 2015). Several representational scheme"
Q17-1009,P98-1013,0,0.183874,"Missing"
Q17-1009,D14-1159,0,0.0199751,"n extremely diverse ways, demanding an operationalized concept of constructions. Recognizing causal relations also requires a combination of linguistic analysis and broader world knowledge. Additionally, causal relations are ubiquitous, both in our thinking and in our language (see, e.g., Conrath et al., 2014). Recognizing these relations is thus invaluable for many semantics-oriented applications, including textual entailment and question answering (especially for “why” questions). They are especially helpful for domain-specific applications such as finance, politics, and biology (see, e.g., Berant et al., 2014), where extracting cause and effect relationships can help drive decision-making. More general applications like machine translation and summarization, which ought to preserve stated causal relationships, can also benefit. In the remainder of this paper, we suggest two related approaches for tagging causal constructions and their arguments. We first review an annotation scheme for causal language and present a new corpus annotated using that scheme (§2). We then define the task of tagging causal language, casting it as a construction recognition problem (§3). Because it is so hard to identify"
Q17-1009,bethard-etal-2008-building,0,0.0352546,"lity. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units (Vieu et al., 2016). Some constructions would still be too complex to represent, but under their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. A broader-coverage linguistic approach was taken by Mirza and Tonelli (2014). They enriched TimeML to include causal links and their lexical triggers, and built an SVM-based system for predicting them. Their work differs from ours in that it requires arguments to be TimeML events; it requires connectives to be contiguous spans; and their classifier relies on gold-standard TimeML annotations. More recently, Hidey a"
Q17-1009,P08-2045,0,0.370297,"one of these covers the full range of linguistic realizations of causality. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units (Vieu et al., 2016). Some constructions would still be too complex to represent, but under their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. A broader-coverage linguistic approach was taken by Mirza and Tonelli (2014). They enriched TimeML to include causal links and their lexical triggers, and built an SVM-based system for predicting them. Their work differs from ours in that it requires arguments to be TimeML events; it requires connectives to be contiguous spans; and their classifier"
Q17-1009,bonial-etal-2014-propbank,0,0.0264304,"84.5 33.1 43.2 40.0 52.3 62.1 73.4 31.8 35.7 (b) With gold-standard parses Pipeline P Connectives R F1 SC Causes HC JC SE Effects HE JE Causeway-S w/o classifier Causeway-S w/ classifier + SC filter 10.2 62.7 70.6 51.6 17.7 56.0 79.4 80.2 98.1 96.4 45.7 45.6 52.8 59.0 90.2 92.7 41.3 43.4 Causeway-L w/o classifier Causeway-L w/ classifier + SC filter 9.1 56.4 84.1 37.9 16.4 44.3 57.8 77.0 68.2 85.3 33.3 41.8 53.0 67.2 68.0 83.4 34.4 40.4 Table 8: Results for Experiment 3. structions in English (Fillmore et al., 2012). Similar efforts are underway for VerbNet (Bonial et al., 2011) and PropBank (Bonial et al., 2014). On the NLPtools side, some work has been done on parsing text directly into constructions, particularly through the formalisms of Fluid Construction Grammar (Steels, 2012) and Embodied Construction Grammar (Bergen and Chang, 2005), which take a “constructions all the way down” approach. Some HPSG parsers and formalisms, particularly those based on the English Resource Grammar (Copestake and Flickinger, 2000; Flickinger, 2011) or Sign-Based Construction Grammar (Boas and Sag, 2012), also take constructions into account. Thus far, however, only a few attempts (e.g., Hwang and Palmer, 2015) hav"
Q17-1009,W11-0910,0,0.050187,"Missing"
Q17-1009,C14-1206,0,0.0189312,"mena that directly carry meaning. Rather than specifying by hand the constraints and properties that characterize each construction, we allow machine learning algorithms to learn these characteristics. Causal relations present an ideal testbed for this approach. As noted above, causal relations are realized in extremely diverse ways, demanding an operationalized concept of constructions. Recognizing causal relations also requires a combination of linguistic analysis and broader world knowledge. Additionally, causal relations are ubiquitous, both in our thinking and in our language (see, e.g., Conrath et al., 2014). Recognizing these relations is thus invaluable for many semantics-oriented applications, including textual entailment and question answering (especially for “why” questions). They are especially helpful for domain-specific applications such as finance, politics, and biology (see, e.g., Berant et al., 2014), where extracting cause and effect relationships can help drive decision-making. More general applications like machine translation and summarization, which ought to preserve stated causal relationships, can also benefit. In the remainder of this paper, we suggest two related approaches fo"
Q17-1009,copestake-flickinger-2000-open,0,0.0516385,"only evaluate on instances where both arguments are present, and our algorithms check for spans or tokens that at least could be arguments.) We leave addressing both limitations to future work. Nonetheless, this task is more difficult than it may appear. Two of the reasons for this are familiar issues in NLP. First, there is a surprisingly long tail of causal constructions (as we finished annotating, we 4 We use the non-collapsed enhanced dependency representation. We could have selected a parser that produces both syntactic and semantic structures, such as the English Resource Grammar (ERG; Copestake and Flickinger, 2000) or another HPSG variant. Though these parsers can produce impressively sophisticated analyses, we elected to use dependency parsers because they proved significantly more robust; there were many sentences in our corpus that we could not parse with ERG. However, incorporating semantic information from such a system when it is available would be an interesting extension for future work. Another possible input would have been semantic role labeling (SRL) tags. SRL tags could not form the basis of our system the way syntactic relations can, because they only apply to limited classes of words (pri"
Q17-1009,J14-1002,0,0.040443,"elated to previous NLP research. Our task is similar to frame-semantic parsing (Baker et al., 2007), the task of automatically producing FrameNet annotations. Lexical triggers of a frame correspond roughly to our causal connectives, and both tasks require identifying argument spans for each trigger. The tasks differ in that FrameNet covers a much wider range of semantics, with more frame-specific argument types, but its triggers are limited to lexical units, whereas we permit arbitrary constructions. Our multi-stage approach is also loosely inspired by SEMAFOR and subsequent FrameNet parsers (Das et al., 2014; Roth and Lapata, 2015; T¨ackstr¨om et al., 2015). Several representational schemes have incorporated elements of causal language. PDTB includes reason and result relations; FrameNet frames often include Purpose and Explanation roles; preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanation-related senses; and VerbNet and PropBank include verbs of causation. As described in §1, however, none of these covers the full range of linguistic realizations of causality. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causal"
Q17-1009,de-marneffe-etal-2006-generating,0,0.0713493,"Missing"
Q17-1009,W15-1622,1,0.847105,"ers and construction-independent classifiers to determine when causal constructions are truly present. We report on three sets of experiments (§5) assessing the two systems’ performance, the impacts of various design features, and the effects of parsing errors. The results indicate the viability of the approach, and point to further work needed to improve construction recognition (§6). 2 Causal Language Annotation Scheme and Corpus Causation is a slippery notion (see Schaffer, 2014), so the parameters of annotating causal language require careful definition. We follow the annotation scheme of Dunietz et al. (2015), which we now briefly review. 2.1 Causal Language Annotation Scheme The scheme of Dunietz et al. (2015) focuses specifically on causal language – language used to appeal to psychological notions of cause and effect. It is not concerned with what causal relationships hold in the real world; rather, it represents what causal relationships are asserted by the text. For example, cancer causes smoking states a false causation, but it would nonetheless be annotated. On the other hand, the bacon pizza is delicious would not be annotated, even though bacon may in fact cause deliciousness, because the"
Q17-1009,W03-1210,0,0.380695,"ndicators. This was a somewhat easier task than ours, given their much larger dataset and that they limited their causal triggers to contiguous phrases. Their dataset and methods for constructing it, however, could likely be adapted to improve our systems. Our pattern-matching techniques are based on earlier work on LEXICO - SYNTACTIC PATTERNS. These patterns, similarly represented as fragments of dependency parse trees with slots, have proven useful for hypernym discovery (Hearst, 1992; Snow et al., 129 2005). They have also been used both for the more limited task of detecting causal verbs (Girju, 2003) and for detecting causation relations that are not exclusively verbal (Ittoo and Bouma, 2011). Our work extends this earlier research in several ways. We propose several methods (CRF-based argument ID and statistical classifiers) for overcoming the ambiguity inherent in such patterns. We also take care to ground our notion of causality in a principled annotation scheme for causal language. This avoids the difficulties of agreeing on what counts as real-world causation (see Grivaz, 2010). 8 Conclusion and Future Work With this work, we have demonstrated the viability of two approaches to taggi"
Q17-1009,grivaz-2010-human,0,0.447437,"; Snow et al., 129 2005). They have also been used both for the more limited task of detecting causal verbs (Girju, 2003) and for detecting causation relations that are not exclusively verbal (Ittoo and Bouma, 2011). Our work extends this earlier research in several ways. We propose several methods (CRF-based argument ID and statistical classifiers) for overcoming the ambiguity inherent in such patterns. We also take care to ground our notion of causality in a principled annotation scheme for causal language. This avoids the difficulties of agreeing on what counts as real-world causation (see Grivaz, 2010). 8 Conclusion and Future Work With this work, we have demonstrated the viability of two approaches to tagging causal constructions. We hope that the constructional perspective will prove applicable to other domains, as well. Our code and corpus are available at https://github.com/ duncanka/causeway and https://github. com/duncanka/BECauSE, respectively. In the immediate future, we plan to explore more sophisticated, flexible algorithms for tagging causal constructions that rely less on fixed patterns. Two promising directions for flexible matching are tree kernels and parse forests (Tomita, 1"
Q17-1009,C92-2082,0,0.117832,"et with PDTBstyle AltLex annotations for causality. Using this corpus, they achieved high accuracy in finding causality indicators. This was a somewhat easier task than ours, given their much larger dataset and that they limited their causal triggers to contiguous phrases. Their dataset and methods for constructing it, however, could likely be adapted to improve our systems. Our pattern-matching techniques are based on earlier work on LEXICO - SYNTACTIC PATTERNS. These patterns, similarly represented as fragments of dependency parse trees with slots, have proven useful for hypernym discovery (Hearst, 1992; Snow et al., 129 2005). They have also been used both for the more limited task of detecting causal verbs (Girju, 2003) and for detecting causation relations that are not exclusively verbal (Ittoo and Bouma, 2011). Our work extends this earlier research in several ways. We propose several methods (CRF-based argument ID and statistical classifiers) for overcoming the ambiguity inherent in such patterns. We also take care to ground our notion of causality in a principled annotation scheme for causal language. This avoids the difficulties of agreeing on what counts as real-world causation (see"
Q17-1009,P16-1135,0,0.179996,"Missing"
Q17-1009,P03-1054,0,0.00848877,"spans of causal connectives are annotated. A connective span may be any set of tokens from the sentence. This can be thought of as recognizing instantiations of causal constructions. 2. Argument identification (or argument ID), in which cause and effect spans are identified for each causal connective. This can be thought of as identifying the causal construction’s slot-fillers. We assume as input a set of sentences, each with POS tags, lemmas, NER tags, and a syntactic parse in the Universal Dependencies (UD; Nivre et al., 2016) scheme, all obtained from version 3.5.2 of the Stanford parser (Klein and Manning, 2003).4 This task is defined in terms of text spans. Still, to achieve a high score on it, a tagger must respond to the meaning of the construction and arguments in context, just as annotators do. This may be achieved by analyzing indirect cues that correlate with meaning, such as lexical information, dependency labels, and tense/aspect/modality information. Compared to the annotation scheme, the task is limited in two important ways: first, we do not distinguish between types or degrees of causation; and second, we only tag instances where both the cause and the effect are present. (Even for conne"
Q17-1009,levin-etal-2014-resources,1,0.807054,"n Matching For pattern matching, we use TRegex (Levy and Andrew, 2006), a grep-inspired utility for matching patterns against syntax trees. During training, the sys122 • t2 has a child t3 via a dependency labeled mark • t3 has the lemma because and a POS tag of IN At test time, TRegex matches these extracted patterns against the test sentences. Continuing with the same example, we would recover t1 as the effect head, t2 as the cause head, and {t3 } as the connective. TRegex is designed for phrase-structure trees, so we transform each dependency tree into a PTB-like parenthetical notation (see Levin et al., 2014). Patterns involving verbs vary systematically: the verbs can become passives or verbal modifiers (e.g., the disaster averted last week), which changes the UD dependency relationships. To generalize across these, we crafted a set of scripts for TSurgeon (Levy and Andrew, 2006), a tree-transformation utility built on TRegex. The scripts normalize passive verbs and past participial modifiers into their active forms. Each sentence is transformed before pattern extraction or matching. TRegex Pattern Extraction The algorithm for extracting TRegex patterns first preprocesses all training sentences w"
Q17-1009,levy-andrew-2006-tregex,0,0.0131048,"x-based approach relies on a simple intuition: each causal construction corresponds, at least in part, to a fragment of a dependency tree, where several nodes’ lemmas and POS tags are fixed (see Figure 1). Accordingly, the first stage of Causeway-S induces lexico-syntactic patterns from the training data. At test time, it matches the patterns against new dependency trees to identify possible connectives and the putative heads of their cause and effect arguments. The second stage then expands these heads into complete argument spans. TRegex Pattern Matching For pattern matching, we use TRegex (Levy and Andrew, 2006), a grep-inspired utility for matching patterns against syntax trees. During training, the sys122 • t2 has a child t3 via a dependency labeled mark • t3 has the lemma because and a POS tag of IN At test time, TRegex matches these extracted patterns against the test sentences. Continuing with the same example, we would recover t1 as the effect head, t2 as the cause head, and {t3 } as the connective. TRegex is designed for phrase-structure trees, so we transform each dependency tree into a PTB-like parenthetical notation (see Levin et al., 2014). Patterns involving verbs vary systematically: the"
Q17-1009,H94-1020,0,0.507861,"Missing"
Q17-1009,C14-1198,0,0.064584,"nder their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. A broader-coverage linguistic approach was taken by Mirza and Tonelli (2014). They enriched TimeML to include causal links and their lexical triggers, and built an SVM-based system for predicting them. Their work differs from ours in that it requires arguments to be TimeML events; it requires connectives to be contiguous spans; and their classifier relies on gold-standard TimeML annotations. More recently, Hidey and McKeown (2016) automatically constructed a large dataset with PDTBstyle AltLex annotations for causality. Using this corpus, they achieved high accuracy in finding causality indicators. This was a somewhat easier task than ours, given their much larger dat"
Q17-1009,W16-1007,0,0.023541,"oposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units (Vieu et al., 2016). Some constructions would still be too complex to represent, but under their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. A broader-coverage linguistic approach was taken by Mirza and Tonelli (2014). They enriched TimeML to include causal links and their lexical triggers, and built an SVM-based system for predicting them. Their work differs from ours in that it requires arguments to be TimeML events; it requires connectives to be contiguous spans; and their classifier relies on gold-standard TimeML annotations. More recently, Hidey and McKeown (2016) automatically constructed a large datase"
Q17-1009,L16-1262,0,0.0202501,"Missing"
Q17-1009,J05-1004,0,0.0409024,"he restrictions of the representational schemes they are based on. Many semantic annotation schemes limit themselves to the argument structures of particular word classes. For example, the Penn Discourse Treebank (PDTB; 117 Transactions of the Association for Computational Linguistics, vol. 5, pp. 117–133, 2017. Action Editor: Christopher Potts. Submission batch: 9/2016; Revision batch: 11/2016; Published 6/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Prasad et al., 2008) includes only conjunctions and adverbials as connectives,1 and PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005) focus on verb arguments. FrameNet (Baker et al., 1998; Fillmore, 2012) is less restrictive, allowing many parts of speech as triggers. Most importantly, though, all these representations share the fundamental simplifying assumption that the basic linguistic carrier of meaning is the lexical unit. Some (e.g., PDTB and FrameNet) allow MWEs as lexical units, and much work has been done on detecting and interpreting MWEs (see Baldwin and Kim, 2010). But even these schemes overlook essential linguistic elements that encode meanings. In example 9, for instance, a lexical"
Q17-1009,prasad-etal-2008-penn,0,0.17783,"icon and grammar. This diversity presents a problem for most semantic parsers, which inherit the restrictions of the representational schemes they are based on. Many semantic annotation schemes limit themselves to the argument structures of particular word classes. For example, the Penn Discourse Treebank (PDTB; 117 Transactions of the Association for Computational Linguistics, vol. 5, pp. 117–133, 2017. Action Editor: Christopher Potts. Submission batch: 9/2016; Revision batch: 11/2016; Published 6/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Prasad et al., 2008) includes only conjunctions and adverbials as connectives,1 and PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005) focus on verb arguments. FrameNet (Baker et al., 1998; Fillmore, 2012) is less restrictive, allowing many parts of speech as triggers. Most importantly, though, all these representations share the fundamental simplifying assumption that the basic linguistic carrier of meaning is the lexical unit. Some (e.g., PDTB and FrameNet) allow MWEs as lexical units, and much work has been done on detecting and interpreting MWEs (see Baldwin and Kim, 2010). But even these schemes over"
Q17-1009,Q15-1032,0,0.0257064,"NLP research. Our task is similar to frame-semantic parsing (Baker et al., 2007), the task of automatically producing FrameNet annotations. Lexical triggers of a frame correspond roughly to our causal connectives, and both tasks require identifying argument spans for each trigger. The tasks differ in that FrameNet covers a much wider range of semantics, with more frame-specific argument types, but its triggers are limited to lexical units, whereas we permit arbitrary constructions. Our multi-stage approach is also loosely inspired by SEMAFOR and subsequent FrameNet parsers (Das et al., 2014; Roth and Lapata, 2015; T¨ackstr¨om et al., 2015). Several representational schemes have incorporated elements of causal language. PDTB includes reason and result relations; FrameNet frames often include Purpose and Explanation roles; preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanation-related senses; and VerbNet and PropBank include verbs of causation. As described in §1, however, none of these covers the full range of linguistic realizations of causality. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more co"
Q17-1009,W16-1712,0,0.0368452,"Missing"
Q17-1009,W15-1612,0,0.0532723,"n, which marks the cause and effect spans. 3. A statistical filter to remove false matches. 4. A constraint-based filter to remove redundant connectives. Smaller connectives like to (which is causal in sentences like I left to get lunch)5 are usually spurious when a larger connective includes the same word, like cause X to Y . When a larger and a smaller connective both make it through Stage 3 together, we remove the smaller one. Because argument ID is done before filtering, the arguments output by Stage 2 do not quite represent the cause and effect arguments of a causal instance. 5 Following Schneider et al. (2015), B ECAUSE considers the “in order to” usage of the infinitive to to carry lexical meaning beyond just marking an infinitival clause. tem examines each causal language instance, generating a TRegex pattern that will match tree fragments with the same connective and argument structure. In the example from Figure 1, the generated pattern would match any tree meeting three conditions:6 worry/VBP nsubj advcl I/PRP care/VBP nsubj I/PRP mark • some token t1 has a child t2 via a dependency labeled advcl because/IN Figure 1: A UD parse for the sentence I worry because I care, with the tree fragment co"
Q17-1009,W14-2505,0,0.0304922,"Missing"
Q17-1009,Q15-1003,0,0.0797588,"Missing"
Q17-1009,L16-1603,0,0.120986,"ional schemes have incorporated elements of causal language. PDTB includes reason and result relations; FrameNet frames often include Purpose and Explanation roles; preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanation-related senses; and VerbNet and PropBank include verbs of causation. As described in §1, however, none of these covers the full range of linguistic realizations of causality. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units (Vieu et al., 2016). Some constructions would still be too complex to represent, but under their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a l"
Q17-1009,C98-1013,0,\N,Missing
S15-1020,P13-1020,0,0.015808,"ave to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-C ENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). When adapting a single-document summarization method to perform multi-document summarization, a possible strategy is to combine the summaries of each do"
S15-1020,N13-1136,0,0.0137019,"es state of the art results. 1 2 Introduction The use of the Internet to fulfill generic information needs motivated pioneer multi-document summarization efforts as NewsInEssence (Radev et al., 2005) or Newsblaster (McKeown et al., 2002), online since 2001. In general, multi-document summarization approaches have to address two different problems: passage selection and information ordering. Current multi-document systems adopt, for passage selection, approaches similar to the ones used in single-document summarization, and use the chronological order of the documents for information ordering (Christensen et al., 2013). The problem is that most approaches fail to generate summaries that cover generic topics which comprehend different, equally important, subtopics. We propose to extend a state-of-the-art single-document summarization method, KPC ENTRALITY (Ribeiro et al., 2013), capable of focusing on diverse important topics while ignoring unimportant ones, to perform multi-document summarization. We explore two hierarchical strategies to perform this extension. Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document"
S15-1020,W04-1017,0,0.0206532,"del based on the following mathematical model: h i arg max λ(Sim1 (Si , Q))−(1−λ)(max Sim2 (Si , Sj )) Si Sj where Sim1 and Sim2 are similarity metrics that do not have to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-C ENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content"
S15-1020,N10-1134,0,0.0290128,"max Sim2 (Si , Sj )) Si Sj where Sim1 and Sim2 are similarity metrics that do not have to be different; Si are the yet unselected passages and Sj are the previously selected ones; Q is the required query to apply the model; and, λ is a parameter that allows to configure the result to be from a standard relevance-ranked list (λ = 1) to a maximal diversity ranking (λ = 0). Coverage-based summarization defines a set of concepts that need to occur in the sentences selected for the summaries. The concepts are events (Filatova and Hatzivassiloglou, 2004), topics (Lin and Hovy, 2000), salient words (Lin and Bilmes, 2010; Sipos et al., 2012), and word n-grams (Gillick et al., 2008; Almeida and Martins, 2013). 3 Multi-Document Summarization Our multi-document approach is built upon a centrality and coverage-based single-document summarization method, KP-C ENTRALITY (Ribeiro et al., 2013). This method, through the use of key phrases, is easily adaptable and has been shown to be robust in the presence of noisy input. This is an important aspect considering that using as input several documents frequently increases the amount of unimportant content). When adapting a single-document summarization method to perform"
S15-1020,C00-1072,0,0.520174,"ng unimportant ones, to perform multi-document summarization. We explore two hierarchical strategies to perform this extension. Related Work Most of the current work in automatic summarization focuses on extractive summarization. The most popular baselines for multi-document summarization fall into one of the following general models: Centrality-based (Radev et al., 2004; Erkan and Radev, 2004; Wang et al., 2008; Ribeiro and de Matos, 2011), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998; Guo and Sanner, 2010; Sanner et al., 2011; Lim et al., 2012), and Coverage-base methods (Lin and Hovy, 2000; Sipos et al., 2012). Additionally, methods such as KP-C ENTRALITY (Ribeiro et al., 2013), which is centrality and coverage-based, follow more than one paradigm. In general, Centrality-based models are used to produce generic summaries, while the MMR family generates query-oriented ones. Coveragebase models produce summaries driven by words, topics or events. Centrality-as-relevance methods base the detection of the most salient passages on the identification of the central passages of the input source(s). One of the main representatives of this family is Passageto-Centroid Similarity-based C"
S15-1020,W04-1013,0,0.00903657,"ed methods. Expected n-call@k adapts and extends MMR as a probabilistic model (Probabilistic Latent MMR). The Portfolio Theory also extends MMR based on the idea of ranking under uncertainty. As baseline, we used the straightforward idea of combining all input documents into a single one, and then submit the document to the single-document summarization method. Considering that most coverage-based systems explore event information, we opted for not including them in this comparative analysis. To assess the informativeness of the summaries generated by our methods, we used ROUGE-1 and ROUGE-2 (Lin, 2004) on DUC 2007 and TAC 2009 datasets. The main summarization task in DUC 20071 is the generation of 250-word summaries of 45 clusters of 25 newswire documents (from the AQUAINT corpus) and 4 human reference summaries. The TAC 2009 Summarization task2 has 44 topic clusters. Each topic has 2 sets of 10 news documents obtained from the AQUAINT 2 corpus.There are 4 human 100-word reference summaries for each set, where the reference summaries for the first set are query-oriented, and for the second set are update summaries. In this work, we used the first set of reference summaries. We evaluate the"
S15-1020,marujo-etal-2012-supervised,1,0.844369,"s: the ones closer to the passage associated with the support set under construction and the ones further apart. These heuristics use a permutation, di1 , di2 , · · · , diN −1 , of the distances of the passages sk to the passage pi , related to the support set under construction, with dik = dist(sk , pi ), 1 ≤ k ≤ N − 1, where N is the number of passages, corresponding to the order of occurrence of passages sk in the input source. The metric that is normally used is the cosine distance. The KP-Centrality method consists of two steps. First, it extracts key phrases using a supervised approach (Marujo et al., 2012) and combines them with a bag-of-words model in a compact matrix representation, given by:   w(t1 , p1 ) . . . w(t1 , pN ) w(t1 , k1 ) . . . w(t1 , kM )   .. ..  , . . w(tT , p1 ) . . . w(tT , pN ) w(tT , k1 ) . . . w(tT , kM ) (1) where w is a function of the number of occurrences of term ti in passage pj or key phrase kl , T is the number of terms and M is the number of key phrases. Then, using a segmented information source I , p1 , p2 , . . . , pN , a support set Si is computed for each passage pi using: Si , {s ∈ I ∪ K : sim(s, qi ) &gt; εi ∧ s 6= qi }, (2) for i = 1, . . . , N + M . P"
S16-1186,P13-2131,0,0.308975,"Missing"
S16-1186,W02-1001,0,0.141888,"ke predictions as follows: yˆ = arg max w · f (x, y 0 ) y 0 ∈Y(x) To train the model parameters w, a function of the training data is minimized with respect to w. This function is a sum of individual training examples’ losses L, plus a regularizer: X L(D; w) = L(xi , yi ; w) + λkwk2 (xi ,yi )∈D    C(xi ,yi ) }| { z     L(xi , yi ; w) = −  lim max w · f (xi , y) + α ·  min cost(yi , y 00 ) −cost(yi , y) 00 α→∞ y∈Y(xi ) + max y 0 ∈Y(xi )  y ∈Y(xi )  w · f (xi , y 0 ) + cost(yi , y 0 ) (1) Figure 1: Infinite ramp loss. Typical loss functions are the structured perceptron loss (Collins, 2002): L(xi , yi ; w) = −w · f (xi , yi ) + max w · f (xi , y) y∈Y(xi ) (2) and the structured SVM loss (Taskar et al., 2003; Tsochantaridis et al., 2004), which incorporates margin using a cost function:1 L(xi , yi ; w) = −w · f (xi , yi ) + max y∈Y(xi ) w · f (xi , y) + cost(yi , y)  (3) Both (2) and (3) are problematic if example i is unreachable, i.e., yi ∈ / Y(xi ), due to imperfect data or an imperfect definition of Y. In this case, the model is trying to learn an output it cannot produce. In some applications, the features f (xi , yi ) cannot even be computed for these examples. This proble"
S16-1186,P14-1134,1,0.736854,"ss function for structured prediction called infinite ramp, which is a generalization of the structured SVM to problems with unreachable training instances. 1 • Frame file lookup: for every word in the input sentence, if the lemma matches the name of a frame in the AMR frame files (with sense tag removed), we add the lemma concatenated with “-01” as a candidate concept fragment. • Lemma: for every word in the input sentence, we add the lemma of the word as a candidate concept fragment. Introduction Our entry to the SemEval 2016 Shared Task 8 is a set of improvements to the system presented in Flanigan et al. (2014). The improvements are: a novel training loss function for structured prediction, which we call “infinite ramp,” new sources for concepts, improved features, and improvements to the rule-based aligner in Flanigan et al. (2014). The overall architecture of the system and the decoding algorithms for concept identification and relation identification are unchanged from Flanigan et al. (2014), and we refer readers seeking a complete understanding of the system to that paper. 2 New Concept Fragment Sources and Features The concept identification stage relies on a function called clex in Section 3 o"
S16-1186,N12-1023,1,0.842131,"d to ramp loss (Collobert et al., 2006; Chapelle et al., 2009; Keshet and McAllester, 2011): L(xi , yi ; w) =  w · f (xi , y) − α · cost(yi , y) y∈Y(xi )  + max w · f (xi , y 0 ) + cost(yi , y 0 ) − max y 0 ∈Y(xi ) (5) The parameter α is often set to zero, and controls the “height” of the ramp, which is α + 1. Taking α → ∞ in Eq. 5 corresponds roughly to Eq. 1, hence the name “infinite ramp loss”. However, Eq. 1 also includes C(xi , yi ) term to make the limit well defined even when miny∈Y(xi ) cost(yi , y) 6= 0. Like infinite ramp loss, ramp loss also handles unreachable training examples (Gimpel and Smith, 2012), but we have found ramp loss to be more difficult to optimize than infinite ramp loss in practice due to local minima. Both loss functions are nonconvex. However, infinite ramp loss is convex if arg miny∈Y(xi ) cost(yi , y) is unique. the AMR annotation scheme between the production of the LDC2015E86 training data and the SemEval test set. During that time, there were changes to the concept senses and the concept frame files. Because the improvements in our parser were due to boosting recall in concept identification (and using the frame files to our advantage), our approach does not show as"
W00-0405,W97-0711,0,0.0129024,"Missing"
W00-0405,W98-1501,0,0.00680959,"Missing"
W00-0405,W97-0703,0,0.0653767,"ary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., .!999; Stein et al., 1999). These include comparing templates filled in by extracting information - using specialized, domain specific knowledge sources - from the doc""ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com-• paring named-entities - extracted usin"
W00-0405,W97-0702,0,0.0569255,"Missing"
W00-0405,W97-0704,0,0.00863366,"Missing"
W00-0405,W97-0713,0,0.0526354,"Missing"
W00-0405,W97-0707,0,0.00639272,"Missing"
W00-0405,J98-3005,0,0.178312,".) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., .!999; Stein et al., 1999). These include comparing templates filled in by extracting information - using specialized, domain specific knowledge sources - from the doc""ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com-• paring named-entities - extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related"
W00-0405,C69-0401,0,0.786482,"Missing"
W00-0405,P95-1053,0,0.018067,"mpts to delete ""less important"" spans of text from the original document; the text that remains is deemed a summary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., .!999; Stein et al., 1999). These include comparing templates filled in by extracting information - using specialized, domain specific knowledge sources - from the doc""ument, and then"
W00-0405,W97-0710,0,0.0467122,"remains is deemed a summary. Work on automated document summarization by text span extraction dates back at least to work at IBM in the fifties (Luhn, 1958). Most of the work in sentence extraction applied statistical techniques (frequency analysis, variance analysis, etc.) to linguistic units such as tokens, names, anaphora, etc. More recently, other approaches have investigated the utility of discourse structure (Marcu, 1997), the combination of information extraction and language generation (Klavans and Shaw, 1995; McKeown et al., 1995), and using machine learning to find patterns in text (Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Strzalkowski et al., 1998). Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al., .!999; Stein et al., 1999). These include comparing templates filled in by extracting information - using specialized, domain specific knowledge sources - from the doc""ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com-• paring nam"
W00-0405,J97-1003,0,\N,Missing
W00-0405,X98-1026,0,\N,Missing
W04-0107,carbonell-etal-2002-automatic,1,0.892932,"Missing"
W04-0107,W99-0904,0,0.186112,"ame roam solve show sow saw sing ring -/z/ -/z/ -/z/ blames roams solves shows sows saws sings rings →sang/eI/ V -/d/ -/d/ blamed roamed solved showed sowed sawed Perfective or Passive -/d/ -/n/ blamed roamed solved shown sown sawn -/i / -/i / -/i / Progressive blaming roaming solving showing sowing sawing singing ringing Past ŋ ŋ rang →sung/Λ/ V rung ŋ Table 1: A few inflection classes of the English verb paradigm ity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and b"
W04-0107,J01-2001,0,0.314787,"m a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over a corpus. Goldsmith (2001), by searching over a space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography. Segmenting word forms in a corpus, Goldsmith creates an inventory of stems and suffixes. Suffixes which can interchangeably concatenate onto a set of stems form a signature. After defining the space of signatures, Goldsmith searches for that choice of word segmentations resulting in a minimum description length local optimum. Finally, the work of Harris (1955; 1967), and later Hafer and Weiss (1974), has direct bearing on the approach taken in this paper. Couched in"
W04-0107,P04-2012,1,0.332363,"rently pursuing MT systems with Mapudungun, an indigenous language spoken by 900,000 people in southern Chile and Argentina, and Aymara, spoken by 3 million people in Bolivia, Peru, and northern Chile, as lowdensity languages and Spanish the resource rich language. A vital first step in a rule-based machine translation system is morphological analysis. This paper outlines a framework for automatic natural language morphology induction inspired by the traditional and linguistic concept of inflection classes. Additional details concerning the candidate inflection class framework can be found in Monson (2004). This paper then goes on to describe one implemented search strategy within this framework, presenting both a simple summary of results and an in depth error analysis. While the intent of this research direction is to define techniques applicable to low-density languages, this paper employs English to illustrate the main conjectures and Spanish, a language with a reasonably complex morphological system, for quantitative analysis. All experiments detailed in this paper are over a Spanish newswire corpus of 40,011 tokens and 6,975 types. 2 Previous Work It is possible to organize much of the re"
W04-0107,W00-0712,0,0.261239,"phic shape of related word forms. Next along the spectrum of orthographic similarVerb Paradigm Basic 3rd Person Singular Non-past Inflection Classes A B C blame roam solve show sow saw sing ring -/z/ -/z/ -/z/ blames roams solves shows sows saws sings rings →sang/eI/ V -/d/ -/d/ blamed roamed solved showed sowed sawed Perfective or Passive -/d/ -/n/ blamed roamed solved shown sown sawn -/i / -/i / -/i / Progressive blaming roaming solving showing sowing sawing singing ringing Past ŋ ŋ rang →sung/Λ/ V rung ŋ Table 1: A few inflection classes of the English verb paradigm ity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work"
W04-0107,N01-1024,0,0.634042,"who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over a corpus. Goldsmith (2001), by searching over a space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography. Segmenting word forms in a corpus, Goldsmith creates an inventory of stems and suffixes. Suffixes which can interchangeably concatenate onto a set of stems form a signature. After defining the space of signatures, Goldsmith searches for that choice of word segmentations resulting in a minimum description le"
W04-0107,H01-1035,0,0.0603411,"efine techniques applicable to low-density languages, this paper employs English to illustrate the main conjectures and Spanish, a language with a reasonably complex morphological system, for quantitative analysis. All experiments detailed in this paper are over a Spanish newswire corpus of 40,011 tokens and 6,975 types. 2 Previous Work It is possible to organize much of the recent work on unsupervised morphology induction by considering the bias each approach has toward discovering morphologically related words that are also orthographically similar. At one end of the spectrum is the work of Yarowsky et al. (2001), who derive a morphological analyzer for a language, L, by projecting the morphological analysis of a resource-rich language onto L through a clever application of statistical machine translation style word alignment probabilities. The word alignments are trained over a sentence aligned parallel bilingual text for the language pair. While the probabilistic model they use to generalize their initial system contains a bias toward orthographic similarity, the unembellished algorithm contains no assumptions on the orthographic shape of related word forms. Next along the spectrum of orthographic s"
W04-3251,N03-1004,0,0.274338,"s - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., Jaime Carbonell Carnegie Mellon University jgc@cs.cmu.edu 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fl"
W04-3251,P03-1003,0,0.156483,"edge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., Jaime Carbonell Carnegie Mellon University jgc@cs.cmu.edu 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for principled, statistically based, e"
W04-3251,P03-1001,0,0.33619,"03; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., Jaime Carbonell Carnegie Mellon University jgc@cs.cmu.edu 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for principled, statistically based, easily re-trainable, language independent QA systems that take full advantage of large amounts of training data. We propose an instance-based, data-driven approach to Question Answering. Instead of classifying questions according to limited, predefined ontologies, we allow training data to shape the strategies for answering new questions. Answer models, query content models, and extr"
W04-3251,N03-1011,0,0.0367114,"u-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., Jaime Carbonell Carnegie Mellon University jgc@cs.cmu.edu 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for principled, statistically based, easily re-trainable, language independent QA systems that take full advantage of large amounts of training data. We propose an instance-based, data-driven approach to Question Answering. Instead of classifying questions according to limited, predefined ontologies, we allow training data to shape the strategies for answering new questions. Answer models, quer"
W04-3251,C02-1042,0,0.100184,"incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., Jaime Carbonell Carnegie Mellon University jgc@cs.cmu.edu 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dom"
W04-3251,P00-1071,0,0.0419281,"training data and use them to enhance the queries. Finally, we treat answer extraction as a binary classification problem in which text snippets are labeled as correct or incorrect answers. We present a basic implementation of these concepts that achieves a good performance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answeri"
W04-3251,N03-1022,0,0.0536613,"ance on TREC test data. 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics - as reflected by the TREC QA track (Voorhees, 2003). Starting from successful pipeline architectures (Moldovan et al., 2000; Hovy et al., 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., Jaime Carbonell Carnegie Mellon University jgc@cs.cmu.edu 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction"
W04-3251,N03-2029,0,0.3212,"task by incorporating knowledge resources (Hermjakob et al., 2000; Hovy et al., 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al., 2003; Nyberg et al., 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al., 2002; Xu et al., 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al., 2003; Burger et al., 2002). In recent years, learning components have started to permeate Question Answering (Clarke et al., Jaime Carbonell Carnegie Mellon University jgc@cs.cmu.edu 2003; Ravichandran et al., 2003; Echihabi and Marcu, 2003). Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods. At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al., 2003; Fleischman et al., 2003). Moreover, Question Answering is expanding to different languages (Magnini et al., 2003) and domains other than news stories (Zweigenbaum, 2003). These trends suggest the need for princip"
W04-3251,J03-4003,0,\N,Missing
W05-0813,2005.eamt-1.21,1,0.807615,"ignments. 1.4 Anchor Context If the adjacent words of the source fragment and the candidate target fragment are translations of each other, we expect that this alignment is more likely to be correct. We boost SFT with the anchor context alignment score SACp , SACp = P (si−1 ↔ tj−1 ) ∗ P (si+k ↔ tj+l ) (13) SFT ← (SFT )λ ∗ (SACp )1−λ (14) Empirically, we found this combination gives the best score for French-English when λ = 0.6 and for Romanian-English when λ = 0.8, and leads to better results than the similar formula SFT ← λ ∗ SFT + (1 − λ) ∗ SACp (15) 2 Experimental Design In previous work (Kim et al., 2005), we tested our alignment method on a set of French-English sentence pairs taken from the Canadian Hansard corpus and on a set of English-Chinese sentence pairs, and compared the results to human alignments. For the present workshop, we chose to use the RomanianEnglish data which had been made available. 89 Due to a lack of time prior to the period of the shared task, we merely re-used the parameters which had been tuned for French-English, rather than tuning the alignment parameters specifically for the development data. SPA was run under three experimental conditions. In the first, labeled “"
W05-0813,W02-1018,0,0.0296941,"ranslation (EBMT) system’s performance, since subsentential alignment is critical in locating the correct translation for a matched fragment of the input. Unlike most algorithms in the literature, this new Symmetric Probabilistic Alignment (SPA) algorithm treats the source and target languages in a symmetric fashion. 1. A fixed bilingual probabilistic dictionary is available. 2. Fragments (word sequences) are translated independently of surrounding context. 3. Contiguous fragments of source language text are translated into contiguous fragments in the target language text. Unlike the work of (Marcu and Wong, 2002), our alignment algorithm is not generative and does not use the idea of a bag of concepts from which the phrases in the sentence pair arise. It is, rather, intended to find the corresponding target-language phrase given a specific source-language phrase of interest, as required by our EBMT system after finding a match between the input and the training data (Brown, 2004). In this short paper, we outline our basic algorithm and some extensions for using context and positional information, and compare its alignment accuracy on the Romanian-English data for the shared task with IBM Model 4 and t"
W05-0813,W03-0301,0,0.143462,"Missing"
W05-0813,2003.mtsummit-papers.4,1,\N,Missing
W05-0813,brown-2004-modified,1,\N,Missing
W05-0813,C00-1019,1,\N,Missing
W05-0813,P02-1040,0,\N,Missing
W07-1315,J01-2001,0,0.538661,"scarding the large number of erroneous initially selected candidate inflection classes. Finally, with a strong grasp on the paradigm structure, ParaMor straightforwardly segments the words of a corpus into morphemes. 1.3 Related Work In this section we highlight previously proposed minimally supervised approaches to the induction of morphology that, like ParaMor, draw on the unique structure of natural language morphology. One facet of NL morphological structure commonly leveraged by morphology induction algorithms is that morphemes are recurrent building blocks of words. Brent et al. (1995), Goldsmith (2001), and Creutz (2006) emphasize the building block nature of morphemes when they each use recurring word segments to efficiently encode a corpus. These approaches then hypothesize that those recurring segments which most efficiently encode a corpus are likely morphemes. Another technique that exploits morphemes as repeating sub-word segments encodes the lexemes of a corpus as a character tree, i.e. trie, (Harris, 1955; Hafer and Weis, 1974), or as a finite state automaton (FSA) over characters (Johnson, H. and Martin, 119 2003; Altun and M. Johnson, 2001). A trie or FSA conflates multiple instan"
W07-1315,N01-1024,0,\N,Missing
W07-1315,N03-2015,0,\N,Missing
W08-0708,J01-2001,0,0.0655504,"al. (1995) take this approach. A third technique leverages inflectional paradigms as the organizational structure of morphology. The ParaMor algorithm, which this paper extends, joins Snover (2002), Zeman (2007), and Goldsmith’s Linguistica in building morphology models around the paradigm. ParaMor tackles three challenges that face morphology induction systems which Goldsmith&apos;s Linguistica algorithm does not yet address. First, section 2.2 of this paper introduces an agglutinative segmentation model. This agglutinative model segments words into as many morphemes as the data justify. Although Goldsmith (2001) and Goldsmith and Hu (2004) discuss ideas for segmenting individual words into more than two morphemes, the implemented Linguistica algorithm, as presented in Goldsmith (2006), permits at most a single morpheme boundary in each word. Second, ParaMor decouples the task of paradigm identification from that of word segmentation (Monson et al., 2007b). In contrast, morphology models in Linguistica inherently encode both a belief about paradigm structure on individual words as well as a segmentation of those words. Without ParaMor’s decoupling of paradigm structure from specific segmentation model"
W08-0708,W07-1315,1,0.826288,"ently lack morphological analysis systems. Unsupervised induction could facilitate, for these lesser-resourced languages, the quick development of morphological systems from raw text corpora. Unsupervised morphology induction has been shown to help NLP tasks including speech recognition (Creutz, 2006) and information retrieval (Kurimo et al., 2007b). In this paper we work with languages like Spanish, German, and Turkish for which morphological analysis systems already exist. The baseline ParaMor algorithm which we extend here competed in the English and German tracks of Morpho Challenge 2007 (Monson et al., 2007b). The peer operated competitions of the Morpho Challenge series standardize the evaluation of unsupervised morphology induction algorithms (Kurimo et al., 2007a; 2007b). The ParaMor algorithm showed promise in the 2007 Challenge, placing first in the linguistic evaluation of German. Developed after the close of Morpho Challenge 2007, our improvements to the ParaMor algorithm could not officially compete in this Challenge. However, the Morpho Challenge 2007 Organizing Committee (Kurimo et al., 2008) graciously oversaw the quantitative evaluation of our agglutinative version of ParaMor. Abstra"
W08-0708,N01-1024,0,\N,Missing
W08-0708,P07-1013,0,\N,Missing
W08-0708,N03-2015,0,\N,Missing
W08-0708,H05-1085,0,\N,Missing
W09-1908,W03-0403,0,0.0330357,"ion modules, generators etc. A detailed discussion of the comprehensive pipeline, may be out of the scope of this paper, more so because such resources can not be expected in a low-resource language scenario. We only focus on the quintessential set of modules for MT pipeline - data acquisition, word-alignment, syntactic analysis etc. The resources can broadly be cat59 • Grammar: VP: V NP → NP V 3 Active Learning for MT Modern syntax based MT rides on the success of both Statistical Machine Translation and Statistical Parsing. Active learning has been applied to Statistical Parsing (Hwa, 2004; Baldridge and Osborne, 2003) to improve sample selection for manual annotation. In case of MT, active learning has remained largely unexplored. Some attempts include training multiple statistical MT systems on varying amounts of data, and exploring a committee based selection for re-ranking the data to be translated and included for re-training. But this does not apply to training in a low-resource scenario where data is scarce. In the rest of the section we discuss the different scenarios that arise in gathering of annotation for MT under a traditional ‘active learning’ setup and discuss the characteristics of the task"
W09-1908,P05-1033,0,0.0308176,"challenges that still remain in applying proactive learning for MT. 2 • Source: John ate an apple • Target: John ne ek seb khaya • Alignment: (1,1),(2,5),(3,3),(4,4) • SourceParse: (S (NP (NNP John)) (VP (VBD ate) (NP (DT an) (NN apple)))) Syntax Based Machine Translation • Lexicon: (seb → apple),(ate → khaya) In recent years, corpus based approaches to machine translation have become predominant, with Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) being the most actively progressing area. Recent research in syntax based machine translation (Yamada and Knight, 2001; Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem faced by PB-SMT approaches. While traditional approaches to syntax based MT were dependent on availability of manual grammar, more recent approaches operate within the resources of PB-SMT and induce hierarchical or linguistic grammars from existing phrasal units, to provide better generality and structure for reordering (Yamada and Knight, 2001; Chiang, 2005; Wu, 1997). 2.1 egorized as ‘monolingual’ vs ‘bilingual’ depending upon whether it requires knowledge in one language or both languages for annotation. A sample of th"
W09-1908,J04-3001,0,0.0294266,"disambiguation modules, generators etc. A detailed discussion of the comprehensive pipeline, may be out of the scope of this paper, more so because such resources can not be expected in a low-resource language scenario. We only focus on the quintessential set of modules for MT pipeline - data acquisition, word-alignment, syntactic analysis etc. The resources can broadly be cat59 • Grammar: VP: V NP → NP V 3 Active Learning for MT Modern syntax based MT rides on the success of both Statistical Machine Translation and Statistical Parsing. Active learning has been applied to Statistical Parsing (Hwa, 2004; Baldridge and Osborne, 2003) to improve sample selection for manual annotation. In case of MT, active learning has remained largely unexplored. Some attempts include training multiple statistical MT systems on varying amounts of data, and exploring a committee based selection for re-ranking the data to be translated and included for re-training. But this does not apply to training in a low-resource scenario where data is scarce. In the rest of the section we discuss the different scenarios that arise in gathering of annotation for MT under a traditional ‘active learning’ setup and discuss th"
W09-1908,N03-1017,0,0.0762645,"slation (MT) between their native tongue and the dominant language of their region. But scarcity in capital and know-how has largely restricted machine translation to the dominant languages of first world nations. To lower the barriers surrounding MT system creation, we must reduce the time and resources needed to develop MT for new language pairs. Syntax based MT has proven to be a good choice for minority language scenario (Lavie et al., 2003). While the amount of parallel data required to build such systems is orders of magnitude smaller than corresponding phrase based statistical systems (Koehn et al., 2003), the variety of linguistic annotation required is greater. Syntax 58 We first consider ‘Active Learning’ (AL) as a framework for building annotated data for the task of MT. However, AL relies on unrealistic assumptions related to the annotation tasks. For instance, AL assumes there is a unique omniscient oracle. In MT, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. A literate bilingual speaker with no extra training can produce translations for word, phrase or sentences and even align them. But it requires a trained"
W09-1908,D08-1027,0,0.0260832,"Missing"
W09-1908,J97-3002,0,0.0376457,"on (PBSMT) (Koehn et al., 2003) being the most actively progressing area. Recent research in syntax based machine translation (Yamada and Knight, 2001; Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem faced by PB-SMT approaches. While traditional approaches to syntax based MT were dependent on availability of manual grammar, more recent approaches operate within the resources of PB-SMT and induce hierarchical or linguistic grammars from existing phrasal units, to provide better generality and structure for reordering (Yamada and Knight, 2001; Chiang, 2005; Wu, 1997). 2.1 egorized as ‘monolingual’ vs ‘bilingual’ depending upon whether it requires knowledge in one language or both languages for annotation. A sample of the different kinds of data and annotation that is expected by an MT system is shown below. Each of the additional information can be seen as extra annotations for the ‘Source’ sentence. The language of target in the example is ‘Hindi’. Resources for Syntax MT Syntax based approaches to MT seek to leverage the structure of natural language to automatically induce MT systems. Depending upon the MT system and the paradigm, the resource requirem"
W09-1908,P01-1067,0,0.0421969,"m. We conclude with some challenges that still remain in applying proactive learning for MT. 2 • Source: John ate an apple • Target: John ne ek seb khaya • Alignment: (1,1),(2,5),(3,3),(4,4) • SourceParse: (S (NP (NNP John)) (VP (VBD ate) (NP (DT an) (NN apple)))) Syntax Based Machine Translation • Lexicon: (seb → apple),(ate → khaya) In recent years, corpus based approaches to machine translation have become predominant, with Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003) being the most actively progressing area. Recent research in syntax based machine translation (Yamada and Knight, 2001; Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem faced by PB-SMT approaches. While traditional approaches to syntax based MT were dependent on availability of manual grammar, more recent approaches operate within the resources of PB-SMT and induce hierarchical or linguistic grammars from existing phrasal units, to provide better generality and structure for reordering (Yamada and Knight, 2001; Chiang, 2005; Wu, 1997). 2.1 egorized as ‘monolingual’ vs ‘bilingual’ depending upon whether it requires knowledge in one language or both languages for annotation."
W09-4633,P04-1075,0,0.099874,"gies are prone to outliers, which are common in MT systems. Instances can also be queried based on expected future error. This strategy is better resistant to outliers as it uses the unlabeled pool when estimating the future error. Density-weighted sampling strategy is also very common and is based on the idea that informative instances are those that are uncertain and representative of the input distribution. In this paper we will investigate these last two strategies. Although active learning has been well studied in many natural language processing tasks, such as, Named-Entity Recognition (Shen et al., 2004), Parsing (Thompson et al., 1999), Word-sense disambiguation (Chen et al., 2006), not much work has been done in using these techniques to improve machine translation. (Eck et al., 2005) used a weighting scheme to select more informative sentences, wherein the importance is estimated using the unseen n-grams in the sentences that were previously selected. The length of the source sentence and actual frequency of the n-grams is used in their weighting scheme. Their experiments were based on the assumption that target sentences are not available at selection time, hence, no information from the"
W09-4633,N06-1016,0,0.0181974,"e queried based on expected future error. This strategy is better resistant to outliers as it uses the unlabeled pool when estimating the future error. Density-weighted sampling strategy is also very common and is based on the idea that informative instances are those that are uncertain and representative of the input distribution. In this paper we will investigate these last two strategies. Although active learning has been well studied in many natural language processing tasks, such as, Named-Entity Recognition (Shen et al., 2004), Parsing (Thompson et al., 1999), Word-sense disambiguation (Chen et al., 2006), not much work has been done in using these techniques to improve machine translation. (Eck et al., 2005) used a weighting scheme to select more informative sentences, wherein the importance is estimated using the unseen n-grams in the sentences that were previously selected. The length of the source sentence and actual frequency of the n-grams is used in their weighting scheme. Their experiments were based on the assumption that target sentences are not available at selection time, hence, no information from the target half of the data was used. Sentences were also weighted based on TF-IDF w"
W09-4633,P02-1040,0,0.0862527,"o easily port an MT system onto small devices that have less memory and storage capacity. In this paper, we propose using Active Learning strategies to sample the most informative sentence pairs. There has not been much progress in the application of active learning theory in machine translation due to the complexity of the translation models. We use a poolbased strategy to selectively sample instances from a parallel corpora which not only outperformed a random selector but also a previously used sampling strategy (Eck et al., 2005) in an EBMT framework (Brown, 2000) by about one BLEU point (Papineni et al., 2002). 1 Introduction An EBMT system uses source-target sentence pairs present in a parallel corpus to translate new input source sentences. The input sentence to be translated is matched against the source sentences present in the corpus. When a match is found, the corresponding translation in the target language is obtained through sub-sentential alignKristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 227–230 Jaime Carbonell Carnegie Mellon University Pittsburgh, PA jgc@cs.cmu.edu ment. The translation is generated from the partial target phrasal matches using a d"
W09-4633,2005.iwslt-1.7,0,0.309278,"quality translations. Reducing the amount of training data also enables one to easily port an MT system onto small devices that have less memory and storage capacity. In this paper, we propose using Active Learning strategies to sample the most informative sentence pairs. There has not been much progress in the application of active learning theory in machine translation due to the complexity of the translation models. We use a poolbased strategy to selectively sample instances from a parallel corpora which not only outperformed a random selector but also a previously used sampling strategy (Eck et al., 2005) in an EBMT framework (Brown, 2000) by about one BLEU point (Papineni et al., 2002). 1 Introduction An EBMT system uses source-target sentence pairs present in a parallel corpus to translate new input source sentences. The input sentence to be translated is matched against the source sentences present in the corpus. When a match is found, the corresponding translation in the target language is obtained through sub-sentential alignKristiina Jokinen and Eckhard Bick (Eds.) NODALIDA 2009 Conference Proceedings, pp. 227–230 Jaime Carbonell Carnegie Mellon University Pittsburgh, PA jgc@cs.cmu.edu m"
W10-0102,C04-1046,0,0.0225497,"he bidirectional probabilities. The selection strategy selects the least scoring links according to the formula below which corresponds to links with maximum uncertainty: Score(aij /sI1 , tJ1 ) = 5.2 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignQuery Strategies for Link Selection 2 ∗ P (tj /si ) ∗ P (si /tj ) (10) P (tj /si ) + P (si /tj ) Confidence Based: Posterior Alignment probabilities Confidence estimation for MT output is an interesting area with meaningful initial exploration (Blatz et al., 2004; Ueffing and Ney, 2007). Given a sentence pair (sI1 , tJ1 ) and its word alignment, we compute two confidence metrics at alignment link level – based on the posterior link probability and a simple IBM Model 1 as seen in Equation 13. We select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-given-target models. Targeting some of the uncertain parts of word alignment has already been shown to improve t"
W10-0102,J93-2003,0,0.0865446,"information, usually syntactic, from the language pairs (Cherry and Lin, 2006). The second is to use extra annotation, typically word-level human alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in a semi-supervised manner. Our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. Introduction The success of statistical approaches to Machine Translation (MT) can be attributed to the IBM models (Brown et al., 1993) that characterize wordlevel alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of statistical machine translation (SMT) systems for various language pairs, the quality of alignment is typically quite low for language Active learning for MT has not yet been explored to its full potential. Much of the literature has explored one task"
W10-0102,P04-1023,0,0.801476,"Missing"
W10-0102,P06-2014,0,0.0208565,"ghly uncertain or most informative alignment links that are proposed under an unsupervised word alignment model. Manual correction of such informative links can then be applied to create a labeled dataset used by a semi-supervised word alignment model. Our experiments show that using active learning leads to maximal reduction of alignment error rates with reduced human effort. 1 Two directions of research have been pursued for improving generative word alignment. The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). The second is to use extra annotation, typically word-level human alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in a semi-supervised manner. Our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. Introduction The success of statistical approaches to Machine Translation (MT) can be attributed to the IBM models (Brown et al., 1993) that characterize wordlevel alignments in parallel corpora."
W10-0102,P06-1097,0,0.239443,"an interesting method that has been applied to clustering problems. Tomanek and Hahn (2009) applied active semi supervised learning to the sequence-labeling problem. Tur et al. (2005) describe active and semi-supervised learning methods for reducing labeling effort for spoken language understanding. They train supervised classification algorithms for the task of call classification and apply it to a large unlabeled dataset to select the least confident instances for human labeling. Researchers have begun to explore semisupervised word alignment models that use both labeled and unlabeled data. Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. The log-linear model is trained on the available labeled data to improve performance. They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. Callison-Burch et 11 al. (2004) also improve alignment by interpolating human alignments with automatic alignments. They observe that while working with"
W10-0102,J07-3002,0,0.0187241,"ed dataset. The word-level aligned labeled dataset is then provided to our semi-supervised word alignment algorithm, which uses it to produces the alignment model θt+1 for U . Algorithm 1 AL FOR W ORD A LIGNMENT 1: Unlabeled Data Set: U = {(sk , tk )} 2: Manual Alignment Set : A0 = {akij , ∀si ∈ Sk , tj ∈ Tk } 3: Train Semi-supervised Word Alignment using (U , A0 ) → θ0 4: N : batch size 5: for t = 0 to T do 6: Lt = LinkSelection(U ,At ,θt ,N ) 7: Request Human Alignment for Lt 8: At+1 = At + Lt 9: Re-train Semi-Supervised Word Alignment on (U, At+1 ) → θt+1 10: end for ment error rate (AER) (Fraser and Marcu, 2007) in the case of word alignment. In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. 5 We propose multiple query selection strategies for our active learning setup. The scoring criteria is designed to select alignment links across sentence pairs that are highly uncertain under current automatic translation models. These links are difficult to align correctly by automatic alignment and will cause incorrect phrase pairs to be extracted in the translation model, in turn hurting the translation quality"
W10-0102,W08-0509,1,0.833421,"stimation for both the directions. 12 As we will discuss in Section 5, the alignments and the computed lexicons form an important part of our link selection strategies. P ˆ count(ti , sj ; A) sP P (sj /ti ) = (8) s count(ti ) P ˆ count(ti , sj ; A) sP P (ti /sj ) = (9) s count(sj ) We perform all our experiments on a symmetrized alignment that combines the bidirectional alignments using heuristics as discussed in (Koehn et al., 2007). We represent this alignment as A = {aij : i = 0 · · · J ∈ sJ1 ; j = 0 · · · I ∈ tI1 }. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. To get full benefit from the manual alignments, MGIZA++ modifies all alignment models used in the standard training procedure, i.e. the IBM1, HMM, IBM3 and IBM4 models. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for IBM models, the training procedure requires for each source sentence position, the summation over all positions in the target sentence. The manual alignments allow for one-to-many alig"
W10-0102,N09-1047,0,0.0940585,"ters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of statistical machine translation (SMT) systems for various language pairs, the quality of alignment is typically quite low for language Active learning for MT has not yet been explored to its full potential. Much of the literature has explored one task – selecting sentences to translate and add to the training corpus (Haffari et al., 2009). In this paper we explore active learning for word alignment, where the input to the active learner is a sentence pair (sJ1 , tI1 ), present in two different languages S = {s∗ } and T = {t∗ }, and the annotation elicited from human is a set of links {(j, i) : j = 0 · · · J; i = 0 · · · I}. Unlike previous approaches, our work does not require elicitation of full alignment for the sentence pair, which could be effortintensive. We use standard active learning query strategies to selectively elicit partial alignment information. This partial alignment information is then fed into a semi-supervis"
W10-0102,P09-1105,0,0.0909898,"iven a sentence pair (sI1 , tJ1 ) and its word alignment, we compute two confidence metrics at alignment link level – based on the posterior link probability and a simple IBM Model 1 as seen in Equation 13. We select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-given-target models. Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SMT (Huang, 2009). In our current work, we use confidence metrics as an active learning sampling strategy to obtain most informative links. We also experiment with other confidence metrics as discussed in (Ueffing and Ney, 2007), especially the IBM 1 model score metric which showed some improvement as well. Ps2t (aij , sI1 /tJ1 ) = Conf (aij /S, T ) = 5.3 pt2s (tj /si , aij ∈ A) (11) PM i pt2s (tj /si ) ps2t (si /tj , aij ∈ A) (12) PN i pt2s (tj /si ) 2 ∗ Pt2s ∗ Ps2t (13) Pt2s + Ps2t Agreement Based: Query by Committee The generative alignments produced differ based on the choice of direction of the language p"
W10-0102,J04-3001,0,0.0195447,"lso improve alignment by interpolating human alignments with automatic alignments. They observe that while working with such datasets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated as a learner in the semi-supervised AdaBoost algorithm to improve word alignment. Active learning has been applied to various fields of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Tang et al., 2001; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For Statistical Machine Translation, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting"
W10-0102,P07-2045,0,0.00547955,"1 ) A (6) (7) Given the Viterbi alignment for each sentence pair in the parallel corpus, we can also compute the word-level alignment probabilities using simple relative likelihood estimation for both the directions. 12 As we will discuss in Section 5, the alignments and the computed lexicons form an important part of our link selection strategies. P ˆ count(ti , sj ; A) sP P (sj /ti ) = (8) s count(ti ) P ˆ count(ti , sj ; A) sP P (ti /sj ) = (9) s count(sj ) We perform all our experiments on a symmetrized alignment that combines the bidirectional alignments using heuristics as discussed in (Koehn et al., 2007). We represent this alignment as A = {aij : i = 0 · · · J ∈ sJ1 ; j = 0 · · · I ∈ tI1 }. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. To get full benefit from the manual alignments, MGIZA++ modifies all alignment models used in the standard training procedure, i.e. the IBM1, HMM, IBM3 and IBM4 models. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for I"
W10-0102,W07-0734,0,0.0547652,"Missing"
W10-0102,J03-1002,0,0.0274685,"and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. Introduction The success of statistical approaches to Machine Translation (MT) can be attributed to the IBM models (Brown et al., 1993) that characterize wordlevel alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of statistical machine translation (SMT) systems for various language pairs, the quality of alignment is typically quite low for language Active learning for MT has not yet been explored to its full potential. Much of the literature has explored one task – selecting sentences to translate and add to the training corpus (Haffari et al., 2009). In this paper we explore active learning for word alignment, where the input to the active learner is a sentence pair (sJ1 , tI1 ), present in two different languages S = {s∗ } and T = {t∗ }, and the annotation elicited from"
W10-0102,P02-1040,0,0.0833163,"Missing"
W10-0102,P04-1075,0,0.0213902,"polating human alignments with automatic alignments. They observe that while working with such datasets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated as a learner in the semi-supervised AdaBoost algorithm to improve word alignment. Active learning has been applied to various fields of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Tang et al., 2001; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For Statistical Machine Translation, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. To our knowledge, w"
W10-0102,P09-1117,0,0.0214544,"k. 2 Related Work Semi-supervised learning is a broader area of Machine Learning, focusing on improving the learning process by usage of unlabeled data in conjunction with labeled data (Chapelle et al., 2006). Many semi-supervised learning algorithms use co-training framework, which assumes that the dataset has multiple views, and training different classifiers on a non-overlapping subset of these features provides additional labeled data (Zhu, 2005). Active query selection for training a semi-supervised learning algorithm is an interesting method that has been applied to clustering problems. Tomanek and Hahn (2009) applied active semi supervised learning to the sequence-labeling problem. Tur et al. (2005) describe active and semi-supervised learning methods for reducing labeling effort for spoken language understanding. They train supervised classification algorithms for the task of call classification and apply it to a large unlabeled dataset to select the least confident instances for human labeling. Researchers have begun to explore semisupervised word alignment models that use both labeled and unlabeled data. Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear spa"
W10-0102,J07-1003,0,0.0155819,"babilities. The selection strategy selects the least scoring links according to the formula below which corresponds to links with maximum uncertainty: Score(aij /sI1 , tJ1 ) = 5.2 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignQuery Strategies for Link Selection 2 ∗ P (tj /si ) ∗ P (si /tj ) (10) P (tj /si ) + P (si /tj ) Confidence Based: Posterior Alignment probabilities Confidence estimation for MT output is an interesting area with meaningful initial exploration (Blatz et al., 2004; Ueffing and Ney, 2007). Given a sentence pair (sI1 , tJ1 ) and its word alignment, we compute two confidence metrics at alignment link level – based on the posterior link probability and a simple IBM Model 1 as seen in Equation 13. We select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-given-target models. Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SM"
W10-0102,P06-2117,0,0.0207555,"dels. The log-linear model is trained on the available labeled data to improve performance. They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. Callison-Burch et 11 al. (2004) also improve alignment by interpolating human alignments with automatic alignments. They observe that while working with such datasets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated as a learner in the semi-supervised AdaBoost algorithm to improve word alignment. Active learning has been applied to various fields of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Tang et al., 2001; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For Statistical Machine Translation, application of active learning has been focused on the task of selecting the most in"
W10-0102,P02-1016,0,\N,Missing
W11-1210,J93-2003,0,0.0188065,"Missing"
W11-1210,D09-1009,0,0.0237797,"m multiple learning tasks. There has been very less work in the area of multitask active learning. (Reichart et al., 2008) proposes an extension of the single-sided active elicitation task to a multi-task scenario, where data elicitation is performed for two or more independent tasks at the same time. (Settles et al., 2008) propose elicitation of annotations for image segmentation under a multi-instance learning framework. Active learning with multiple annotations also has similarities to the recent body of work in learning from instance feedback and feature feedback (Melville et al., 2005). (Druck et al., 2009) propose active learning extensions to the gradient approach of learning from feature and instance feedback. However, in the comparable corpora problem although the second annotation is geared towards learning better features by enhancing the coverage of the lexicon, the annotation itself is not on the features but for extracting training data that is then used to train the lexicon. 3 Supervised Comparable Sentence Classification In this section we discuss our supervised training setup and the classification algorithm. Our classifier tries to identify comparable sentences from among a large po"
W11-1210,P10-1074,0,0.0678486,"Missing"
W11-1210,P98-1069,0,0.0228526,"e discuss the supervised training setup for our classifier. In Section 4 we discuss the application of active learning to the classification task. Section 5 discusses the case of active learning with two different annotations and proposes an approach for combining them. Section 6 presents experimental results and the effectiveness of the active learning strategies. We conclude with further discussion and future work. 2 Related Work There has been a lot of interest in using comparable corpora for MT, primarily on extracting parallel sentence pairs from comparable sources (Zhao and Vogel, 2002; Fung and Yee, 1998). Some work has gone beyond this focussing on extracting subsentential fragments from noisier comparable data (Munteanu and Marcu, 2006; Quirk et al., 2007). The research conducted in this paper has two primary contributions and so we will discuss the related work as relevant to each of them. Our first contribution in this paper is the application of active learning for acquiring comparable 70 data in the low-resource scenario, especially relevant when working with low-resource languages. There is some earlier work highlighting the need for techniques to deal with low-resource scenarios.(Munte"
W11-1210,J05-4003,0,0.285922,"gual information retrieval techniques. Once we have identified a subset of documents that are potentially parallel, the second challenge is to identify comparable sentence pairs. This is an interesting challenge as the availability of completely parallel sentences on the internet is quite low in most language-pairs, but one can observe very few comparable sentences among comparable documents for a given language-pair. Our work tries to address this problem by posing the identification of comparable sentences from comparable data as a supervised classification problem. Unlike earlier research (Munteanu and Marcu, 2005) where the authors try to identify parallel sentences among a pool of comparable documents, we try to first identify comparable sentences in a pool with dominantly non-parallel sentences. We then build a supervised classifier that learns from user annotations for comparable corpora identification. Training such a classifier requires reliably annotated data that may be unavailable for low-resource language pairs. Involving a human expert to perform such annotations is expensive for low-resource languages and so we propose active learning as a suitable technique to reduce the labeling effort. Th"
W11-1210,P06-1011,0,0.0179755,"sification task. Section 5 discusses the case of active learning with two different annotations and proposes an approach for combining them. Section 6 presents experimental results and the effectiveness of the active learning strategies. We conclude with further discussion and future work. 2 Related Work There has been a lot of interest in using comparable corpora for MT, primarily on extracting parallel sentence pairs from comparable sources (Zhao and Vogel, 2002; Fung and Yee, 1998). Some work has gone beyond this focussing on extracting subsentential fragments from noisier comparable data (Munteanu and Marcu, 2006; Quirk et al., 2007). The research conducted in this paper has two primary contributions and so we will discuss the related work as relevant to each of them. Our first contribution in this paper is the application of active learning for acquiring comparable 70 data in the low-resource scenario, especially relevant when working with low-resource languages. There is some earlier work highlighting the need for techniques to deal with low-resource scenarios.(Munteanu and Marcu, 2005) propose bootstrapping using an existing classifier for collecting new data. However, this approach works when ther"
W11-1210,J03-1002,0,0.00350371,"t one other issue that needs to be solved in order for our classification based approach to work for truly low-resource language pairs. As we will describe later in the paper, our comparable sentence classifier relies on the availability of an ini69 Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 69–77, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics tial seed lexicon that can either be provided by a human or can be statistically trained from parallel corpora (Och and Ney, 2003). Experiments show that a broad coverage lexicon provides us with better coverage for effective identification of comparable corpora. However, availability of such a resource can not be expected in very low-resource language pairs, or even if present may not be of good quality. This opens an interesting research question - Can we also elicit such information effectively at low costs? We propose active learning strategies for identifying the most informative comparable sentence pairs which a human can then extract parallel segments from. While the first form of supervision provides us with clas"
W11-1210,2007.mtsummit-papers.50,0,0.0154183,"discusses the case of active learning with two different annotations and proposes an approach for combining them. Section 6 presents experimental results and the effectiveness of the active learning strategies. We conclude with further discussion and future work. 2 Related Work There has been a lot of interest in using comparable corpora for MT, primarily on extracting parallel sentence pairs from comparable sources (Zhao and Vogel, 2002; Fung and Yee, 1998). Some work has gone beyond this focussing on extracting subsentential fragments from noisier comparable data (Munteanu and Marcu, 2006; Quirk et al., 2007). The research conducted in this paper has two primary contributions and so we will discuss the related work as relevant to each of them. Our first contribution in this paper is the application of active learning for acquiring comparable 70 data in the low-resource scenario, especially relevant when working with low-resource languages. There is some earlier work highlighting the need for techniques to deal with low-resource scenarios.(Munteanu and Marcu, 2005) propose bootstrapping using an existing classifier for collecting new data. However, this approach works when there is a classifier of"
W11-1210,P08-1098,0,0.0288953,"ontribution of the paper is to extend the traditional active learning setup that is suitable for eliciting a single annotation. We highlight the needs of the comparable corpora scenario where we have two kinds of annotations - class label assignment and parallel segment extraction and propose strategies in active learning that involve multiple annotations. A relevant setup is multitask learning (Caruana, 1997) which is increasingly becoming popular in natural language processing for learning from multiple learning tasks. There has been very less work in the area of multitask active learning. (Reichart et al., 2008) proposes an extension of the single-sided active elicitation task to a multi-task scenario, where data elicitation is performed for two or more independent tasks at the same time. (Settles et al., 2008) propose elicitation of annotations for image segmentation under a multi-instance learning framework. Active learning with multiple annotations also has similarities to the recent body of work in learning from instance feedback and feature feedback (Melville et al., 2005). (Druck et al., 2009) propose active learning extensions to the gradient approach of learning from feature and instance feed"
W11-1210,J03-3002,0,0.0441443,"the annotations independently. 1 Introduction The state-of-the-art Machine Translation (MT) systems are statistical, requiring large amounts of parallel corpora. Such corpora needs to be carefully created by language experts or speakers, which makes building MT systems feasible only for those language pairs with sufficient public interest or financial support. With the increasing rate of social media creation and the quick growth of web media in languages other than English makes it relevant for language research community to explore the feasibility of Internet as a source for parallel data. (Resnik and Smith, 2003) show that parallel corpora for a variety of languages can be harvested on the Internet. It is to be observed that a major portion of the multilingual web documents are created independent of one another and so are only mildly parallel at the document level. There are multiple challenges in building comparable corpora for consumption by the MT systems. The first challenge is to identify the parallelism between documents of different languages which has been reliably done using cross lingual information retrieval techniques. Once we have identified a subset of documents that are potentially par"
W11-1210,C98-1066,0,\N,Missing
W15-1622,P98-1013,0,0.219728,"nguistics limit the complexity of annotation by focusing not on the hairy metaphysics of causation, but on the assertions about causation that are explicit in the language. We ultimately plan to use this scheme in an automated causal information extraction system. Our second contribution is to compare two approaches to annotating causality, one using an annotation manual only and the other using a constructicon developed by an expert along with an annotation manual. The constructicon-based methodology is similar to the two-stage methodology used in PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) annotations: an initial phase of corpus lexicography produces a lexicon, followed by a second phase in which annotators identify instances of the lexical frames in a corpus. In our case, the “lexicon” is a list of English constructions that conventionally express causality. We also offer suggestions for when such an approach may be appropriate. Finally, we discuss the broader implications of our experience for difficult annotation tasks. In particular, we address the concern of arbitrariness in schemes which can only be successfully be applied by experts or highly trained annotators. 2 2.1 Re"
W15-1622,C14-1206,0,0.0629989,"oting or hindering another (the effect). The cause and effect must be deliberately related by an explicit causal connective. (As emphasized above, the words “presented as” are essential to this definition.) Causal relations can be expressed in English in many different ways. In this project, we exclude: • Causal relationships with no lexical trigger. We do not annotate implicit causal relationships (“zero” discourse connectives). We expect our work to be compatible with other work on such relationships, such as the implicit relations in the PDTB and systems for recovering those relationships (Conrath et al., 2014). • Connectives that lexicalize the means or the result of the causation. For example, kill can be interpreted as “cause to die,” but it encodes the result, so we exclude it. This decision was made to allow the scheme to focus specifically on language that expresses causation. If lexical causatives were included, nearly every transitive verb in the English language would have to be considered causal; it would be impossible to disentangle causation as a semantic phenomenon with its own linguistic realizations. It would also be impossible to annotate the cause and effect separately from the conn"
W15-1622,S07-1003,0,0.166541,"Missing"
W15-1622,grivaz-2010-human,0,0.164499,", rather than causal language. SemEval 2007 included a task (Girju et al., 2007) concerning classifying semantic relations between nominals, including causal relations. As part of this task, the organizers provided a dataset tagged with noun-noun relations. However, this task relied on a less precise, common-sense notion of real-world causation, and the annotations do not indicate the causal connectives, presumably because real-world causal relationships may not be indicated in the text. The SemEval data also limited the causes and effects to nouns (in our experience, they are often clauses). Grivaz (2010) finds that human annotators struggle to apply standard philosophical tests to make binary decisions about the presence of causation in a text segment. She suggests alternative criteria, which we take into account in our coding manual. 189 Many of her criteria, however, are concerned with how people identify real-world causal relationships, rather than how speakers or writers explicitly invoke the concept of causality. The Richer Events Description schema has also incorporated cause/effect relations (Ikuta et al., 2014). This effort, too, is concerned with bringing annotators to agreement on w"
W15-1622,W14-2903,0,0.14281,"Missing"
W15-1622,W14-0702,0,0.197127,"several relation types that are relevant to causation (primarily C AUSE and R EASON). Its representation of causal relations is limited in three important ways that we attempt to overcome. First, it does not capture the subtleties of different types of causal relationships. Second, it is limited to discourse relations, and so excludes other realizations of the relationship (e.g., verb arguments). Finally, its relation hierarchy fails to capture overlaps between the semantics of different discourse phenomena (e.g., hypotheticals may also be causal). Closer to our work is the scheme proposed by Mirza et al. (2014), who base their representation on Talmy’s “force dynamics” model of causation (Talmy, 1988). Their model is rich enough to capture linguistic triggers of causation, as well as causes and effects. It particularly follows Wolff’s (2005) taxonomy of expressions of causation. However, like the PDTB, it does not distinguish the different types of causal relationships. It also does not rigorously define what it counts as causal, and like Ikuta’s work, it is limited to event-event relations. The project most similar in spirit to ours is BioCause (Mih˘ail˘a et al., 2013), which provides an annotation"
W15-1622,J05-1004,0,0.097244,"15 Association for Computational Linguistics limit the complexity of annotation by focusing not on the hairy metaphysics of causation, but on the assertions about causation that are explicit in the language. We ultimately plan to use this scheme in an automated causal information extraction system. Our second contribution is to compare two approaches to annotating causality, one using an annotation manual only and the other using a constructicon developed by an expert along with an annotation manual. The constructicon-based methodology is similar to the two-stage methodology used in PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) annotations: an initial phase of corpus lexicography produces a lexicon, followed by a second phase in which annotators identify instances of the lexical frames in a corpus. In our case, the “lexicon” is a list of English constructions that conventionally express causality. We also offer suggestions for when such an approach may be appropriate. Finally, we discuss the broader implications of our experience for difficult annotation tasks. In particular, we address the concern of arbitrariness in schemes which can only be successfully be applied by experts or h"
W15-1622,prasad-etal-2008-penn,0,0.340978,"2014). This effort, too, is concerned with bringing annotators to agreement on what counts as realworld causation. It is also limited to event-event relations, even though causal language often describes states or objects as causes or effects. 2.2 Annotating Causal Language Other projects have, to a greater or lesser extent, focused on annotating stated causal relationships, much as we have. In general, our scheme attempts to be more precise in its definitions, more general in its scope, and more rich in its representational capacity than these prior works. The Penn Discourse TreeBank (PDTB; Prasad et al., 2008) includes several relation types that are relevant to causation (primarily C AUSE and R EASON). Its representation of causal relations is limited in three important ways that we attempt to overcome. First, it does not capture the subtleties of different types of causal relationships. Second, it is limited to discourse relations, and so excludes other realizations of the relationship (e.g., verb arguments). Finally, its relation hierarchy fails to capture overlaps between the semantics of different discourse phenomena (e.g., hypotheticals may also be causal). Closer to our work is the scheme pr"
W15-1622,J14-1009,0,0.0173849,"rhaps the annotation scheme fails to capture meaningful semantic categories – perhaps it is merely a fiction of the minds of its designers. It is to this concern that we turn next. 7 What Does Low Non-Expert Agreement Say About Validity? What imparts validity to an annotation scheme is a fundamental question that haunts every annotation project. Even a well-thought-out scheme can include arbitrary, empirically meaningless decisions, which would seem to undermine the scheme’s value as a description of a real linguistic phenomenon.3 This risk of arbitrariness is precisely what appears to bother Riezler (2014) in his discussion of circularity in computational linguistics: it is entirely possible that an annotation scheme has high interannotator agreement and can even be reproduced by software, and yet the scheme is empirically empty. The agreement can be achieved simply by developing a shared body of implicit, arbitrary theoretical assumptions among expert or intensively trained coders. Meanwhile, the fact that the annotations can be reproduced automatically shows only that the theory can be expressed both as an annotation scheme and as an annotation machine, not that it encapsulates something mean"
W15-1622,E12-2021,0,0.0564496,"Missing"
W15-1622,C98-1013,0,\N,Missing
W17-0812,bethard-etal-2008-building,0,0.0271808,"ons, which are not expressed as discourse connectives. BECauSE 2.0 can be thought of as an adaptation of PDTB’s multiple-annotation approach. Instead of focusing on a particular type of construction (discourse relations) and annotating all the meanings it can convey, we start from a particular meaning (causality), find all constructions that express it, and annotate each instance in the text with all the meanings it expresses. Other projects have attempted to address causality more narrowly. For example, a small corpus of event pairs conjoined with and has been tagged as causal or not causal (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. Similarly, Richer Event Description (O’Gorman et al., 2016) integrates real-world temporal and causal relations between events into a unified framework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to b"
W17-0812,burchardt-etal-2006-salsa,0,0.0976747,"Missing"
W17-0812,W15-1622,1,0.948538,"ausality and Overlapping Relations Lori Levin and Jaime Carbonell Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {lsl,jgc}@cs.cmu.edu Jesse Dunietz Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213, USA jdunietz@cs.cmu.edu Abstract press), the corpus contains annotations for causal language. It also includes annotations for seven commonly co-present meanings when they are expressed using constructions shared with causality. To deal with the wide variation in linguistic expressions of causation (see Neeleman and Van de Koot, 2012; Dunietz et al., 2015), BECauSE draws on the principles of Construction Grammar (CxG; Fillmore et al., 1988; Goldberg, 1995). CxG posits that the fundamental units of language are constructions – pairings of meanings with arbitrarily simple or complex linguistic forms, from morphemes to structured lexico-syntactic patterns. Accordingly, BECauSE admits arbitrary constructions as the bearers of causal relationships. As long as there is at least one fixed word, any conventionalized expression of causation can be annotated. By focusing on causal language – conventionalized expressions of causation – rather than real-wo"
W17-0812,H94-1020,0,0.13477,"example, so offensive that I left would be annotated as both causal (M OTIVATION) and E XTREMITY /S UFFICIENCY. When causality is not present in a use of a sometimes-causal construction, the instance is annotated as N ON - CAUSAL, and the overlapping relations present are marked. It can be difficult to determine when language that expresses one of these relationships was also intended to convey a causal relationship. Annotators used a variety of questions to assess an ambiguous instance, largely based on Grivaz (2010): • 47 documents randomly selected4 from sections 2-23 of the Penn Treebank (Marcus et al., 1994) • 679 sentences5 transcribed from Congress’ Dodd-Frank hearings, taken from the NLP Unshared Task in PoliInformatics 2014 (Smith et al., 2014) • 10 newspaper documents (Wall Street Journal and New York Times articles, totalling 547 sentences) and 2 journal documents (82 sentences) from the Manually Annotated SubCorpus (MASC; Ide et al., 2010) The first three sets of documents are the same dataset that was annotated for BECauSE 1.0. • The “why” test: After reading the sentence, could a reader reasonably be expected to answer a “why” question about the potential effect argument? If not, it is n"
W17-0812,C14-1198,0,0.0630157,"meanings it expresses. Other projects have attempted to address causality more narrowly. For example, a small corpus of event pairs conjoined with and has been tagged as causal or not causal (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. Similarly, Richer Event Description (O’Gorman et al., 2016) integrates real-world temporal and causal relations between events into a unified framework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to be contiguous; and its guidelines define causality less precisely, relying on intuitive notions Related Work Several annotation schemes have addressed elements of causal language. Verb resources such as VerbNet (Schuler, 2005) and PropBank (Palmer et al., 2005) include verbs of causation. Likewise, preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanationrelated senses. No"
W17-0812,W16-1007,0,0.034078,". BECauSE 2.0 can be thought of as an adaptation of PDTB’s multiple-annotation approach. Instead of focusing on a particular type of construction (discourse relations) and annotating all the meanings it can convey, we start from a particular meaning (causality), find all constructions that express it, and annotate each instance in the text with all the meanings it expresses. Other projects have attempted to address causality more narrowly. For example, a small corpus of event pairs conjoined with and has been tagged as causal or not causal (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. Similarly, Richer Event Description (O’Gorman et al., 2016) integrates real-world temporal and causal relations between events into a unified framework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to be contiguous; and its guidelines define causality less pre"
W17-0812,W16-5706,0,0.0992062,"Missing"
W17-0812,grivaz-2010-human,0,0.091269,"Times corpus (Sandhaus, 2008) All relation types present in the instance are marked. For example, so offensive that I left would be annotated as both causal (M OTIVATION) and E XTREMITY /S UFFICIENCY. When causality is not present in a use of a sometimes-causal construction, the instance is annotated as N ON - CAUSAL, and the overlapping relations present are marked. It can be difficult to determine when language that expresses one of these relationships was also intended to convey a causal relationship. Annotators used a variety of questions to assess an ambiguous instance, largely based on Grivaz (2010): • 47 documents randomly selected4 from sections 2-23 of the Penn Treebank (Marcus et al., 1994) • 679 sentences5 transcribed from Congress’ Dodd-Frank hearings, taken from the NLP Unshared Task in PoliInformatics 2014 (Smith et al., 2014) • 10 newspaper documents (Wall Street Journal and New York Times articles, totalling 547 sentences) and 2 journal documents (82 sentences) from the Manually Annotated SubCorpus (MASC; Ide et al., 2010) The first three sets of documents are the same dataset that was annotated for BECauSE 1.0. • The “why” test: After reading the sentence, could a reader reaso"
W17-0812,J05-1004,0,0.104621,"6) integrates real-world temporal and causal relations between events into a unified framework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to be contiguous; and its guidelines define causality less precisely, relying on intuitive notions Related Work Several annotation schemes have addressed elements of causal language. Verb resources such as VerbNet (Schuler, 2005) and PropBank (Palmer et al., 2005) include verbs of causation. Likewise, preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanationrelated senses. None of these, however, unifies all linguistic realizations of causation into one framework; they are concerned with specific classes of words, rather than the semantics of causality. FrameNet (Ruppenhofer et al., 2016) is closer in spirit to BECauSE, in that it starts from meanings 96 of causing, preventing, and enabling. 2.1 cause of her illness was dehydration. But this is an unparsimonious account of the causal construction: the copula and pre"
W17-0812,P10-2013,0,0.0275138,"f these relationships was also intended to convey a causal relationship. Annotators used a variety of questions to assess an ambiguous instance, largely based on Grivaz (2010): • 47 documents randomly selected4 from sections 2-23 of the Penn Treebank (Marcus et al., 1994) • 679 sentences5 transcribed from Congress’ Dodd-Frank hearings, taken from the NLP Unshared Task in PoliInformatics 2014 (Smith et al., 2014) • 10 newspaper documents (Wall Street Journal and New York Times articles, totalling 547 sentences) and 2 journal documents (82 sentences) from the Manually Annotated SubCorpus (MASC; Ide et al., 2010) The first three sets of documents are the same dataset that was annotated for BECauSE 1.0. • The “why” test: After reading the sentence, could a reader reasonably be expected to answer a “why” question about the potential effect argument? If not, it is not causal. 5.2 Inter-Annotator Agreement Inter-annotator agreement was calculated between the two primary annotators on a sample of 260 • The temporal order test: Is the cause asserted to precede the effect? If not, it is not causal. 3 Publicly available, along with the constructicon, at https://github.com/duncanka/BECauSE. 4 We excluded WSJ d"
W17-0812,prasad-etal-2008-penn,0,0.283111,"Missing"
W17-0812,P03-1054,0,0.0281262,"Missing"
W17-0812,W16-1712,0,0.0525976,"Missing"
W17-0812,W15-1612,0,0.0258048,"ramework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to be contiguous; and its guidelines define causality less precisely, relying on intuitive notions Related Work Several annotation schemes have addressed elements of causal language. Verb resources such as VerbNet (Schuler, 2005) and PropBank (Palmer et al., 2005) include verbs of causation. Likewise, preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanationrelated senses. None of these, however, unifies all linguistic realizations of causation into one framework; they are concerned with specific classes of words, rather than the semantics of causality. FrameNet (Ruppenhofer et al., 2016) is closer in spirit to BECauSE, in that it starts from meanings 96 of causing, preventing, and enabling. 2.1 cause of her illness was dehydration. But this is an unparsimonious account of the causal construction: the copula and preposition do not contribute to the causal meaning, and other language could be used to ti"
W17-0812,W14-2505,0,0.0203412,"a use of a sometimes-causal construction, the instance is annotated as N ON - CAUSAL, and the overlapping relations present are marked. It can be difficult to determine when language that expresses one of these relationships was also intended to convey a causal relationship. Annotators used a variety of questions to assess an ambiguous instance, largely based on Grivaz (2010): • 47 documents randomly selected4 from sections 2-23 of the Penn Treebank (Marcus et al., 1994) • 679 sentences5 transcribed from Congress’ Dodd-Frank hearings, taken from the NLP Unshared Task in PoliInformatics 2014 (Smith et al., 2014) • 10 newspaper documents (Wall Street Journal and New York Times articles, totalling 547 sentences) and 2 journal documents (82 sentences) from the Manually Annotated SubCorpus (MASC; Ide et al., 2010) The first three sets of documents are the same dataset that was annotated for BECauSE 1.0. • The “why” test: After reading the sentence, could a reader reasonably be expected to answer a “why” question about the potential effect argument? If not, it is not causal. 5.2 Inter-Annotator Agreement Inter-annotator agreement was calculated between the two primary annotators on a sample of 260 • The t"
W17-0812,E12-2021,0,0.184861,"Missing"
W17-0812,L16-1603,0,0.0457577,"r Computational Linguistics tative Hall visit next week. and catalogs/annotates a wide variety of lexical items that can express those meanings. Our work differs in several ways. First, FrameNet represents causal relationships through a variety of unrelated frames (e.g., C AUSATION and T HWARTING) and frame roles (e.g., P URPOSE and E XPLANATION). As with other schemes, this makes it difficult to treat causality in a uniform way. (The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units [Vieu et al., 2016]. Merging their framework into mainline FrameNet would mitigate this issue.) Second, FrameNet does not allow a lexical unit to evoke more than one frame at a time (although SALSA [Burchardt et al., 2006], the German FrameNet, does allow this). Each sentence conveys a causal relation, but piggybacks it on a related relation type. (1) uses a temporal relationship to suggest causality. (3) employs a correlative construction, and (2) contains elements of both time and correlation in addition to causation. (4), meanwhile, is framed as bringing something into existence, and (5) suggests both permis"
W19-4208,A94-1024,0,\N,Missing
W19-4208,P98-1080,0,\N,Missing
W19-4208,C98-1077,0,\N,Missing
W19-4208,N15-1055,0,\N,Missing
W19-4208,N16-1030,0,\N,Missing
W19-4208,N16-1161,1,\N,Missing
W19-4208,P16-1184,0,\N,Missing
W19-4208,E17-5001,0,\N,Missing
W19-4208,E17-2002,1,\N,Missing
W19-4208,E17-1048,0,\N,Missing
W19-4208,D18-1039,0,\N,Missing
W19-4208,W18-6011,0,\N,Missing
W19-4208,K18-1036,0,\N,Missing
W19-4208,N19-1155,0,\N,Missing
W19-4208,D19-1279,0,\N,Missing
W19-4208,W17-4115,0,\N,Missing
W19-4208,N19-1423,0,\N,Missing
X98-1025,W97-0707,0,0.393146,"12] [303] AP880621-0089 [8] There was no immediate comment from South Africa, which in the past has staged cross-border raids on Botswana and other neighboring countries to attack suspected facilities of the Afdcan National Congress, which seeks to overthrow South Afdca's white-led government. [9] [24] [502] wsJg00510-0088 [24] While the membership of Inkatha, the religiously and politically conservative group that is the ANC's chief rival for power in black South Afdca, is overwhelmingly Zulu, Inkatha's leader, Mangosutho Buthelezi, has very seldom appealed to sectional tnbal loyalties. [10] [16] [593] AP890821-0092 [11] Besides ending the emergency and lifting bans on anti-apartheid groups and individual activists, the Harare summit's conditions included the removal of all troops from South Afnca's black townships, releasing all political prisoners and ending political tdals and executions, and a government commitment to free political discussion. Fig 4: ~ =l.0 Multi Document Summarization [Rank] Document ID [Sentence Number] Sentence Fig 5: ~ =.3 Multi Document Summarization. [Rank] [Previous Rank in X = 1.0 Version] Document ID [Sentence Number] Sentence 187 <TITLE&gt;Angola Rejects S"
X98-1025,W97-0704,0,0.544988,"in their summarization work [15]. As such our method is simpler, faster and more widely applicable, but yields potentially less cohesive summaries. All summary results in this paper use the SMART search engine with stopwords eliminated from the indexed data and stemming. Query: Delaunay refinement mesh generation finite element method foundations three dimension analysis; ~ = .3 [1] Delaunay refinement is a technique for generating unstructured meshes of triangles or tetrahedra suitable for use in the finite element method or other numerical methods for solving partial differential equations. [5] The purpose of this thesis is to further this progress by cementing the foundations of two-dimensional Delaunay refinement, and by extending the technique and its analysis to three dimensions. [15] Nevertheless, Delaunay refinement methods for tetrahedral mesh generation have the rare distinction that they offer strong theoretical bounds and frequently perform well in practice. [39] If one can generate meshes that are completely satisfying for numedcal techniques like the finite element method, the other applications fall easily in line. [131] Our understanding of the relative merit of differ"
X98-1025,I05-2047,0,\N,Missing
