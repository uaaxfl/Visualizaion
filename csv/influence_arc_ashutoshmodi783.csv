2020.finnlp-1.14,D16-1041,0,0.0357209,"Missing"
2020.finnlp-1.14,P14-1113,0,0.0748657,"Missing"
2020.finnlp-1.14,S18-1116,0,0.0286978,"Missing"
2020.finnlp-1.14,S15-2151,0,0.156186,"Missing"
2020.finnlp-1.14,S16-1168,0,0.0380091,"Missing"
2020.finnlp-1.14,E17-2036,0,0.0332687,"Missing"
2020.finnlp-1.14,J13-3007,0,0.0781514,"Missing"
2020.finnlp-1.14,C14-1212,0,0.150868,"Missing"
2020.semeval-1.150,W02-0109,0,0.152677,"Missing"
2020.semeval-1.150,P17-5002,0,0.0649006,"Missing"
2020.semeval-1.162,P19-4007,0,0.0336293,"Missing"
2020.semeval-1.162,C16-1234,0,0.0433113,"analysis before (Wang et al., 2016; Yoon and Kim, 2017), none of the previous works have used a self-attention based LSTM along with it. We found that while the CNN component worked well for positive and negative tweets, the self-attention component worked better for neutral tweets, necessitating an ensemble of the two. The implementation of our system is made available via Github1 . 2 Related Work Performing standard NLP tasks on code-mixed data has presented significant challenges. Vyas et al. (2014) attempted to find methods for POS tagging of code-mixed social media text. Another work by Joshi et al. (2016) used CNNs to learn subword level embeddings and then utilized these embeddings in a BiLSTM network to learn subword level information from social media text. Subword level representations are particularly important while dealing with noisy texts containing misspellings and punctuations. However, this work doesn’t capture information about word-level semantics. More recent work by Lal et al. (2019) uses two parallel BiLSTMs, which they call the Collective and Specific Encoder and an additional feature network. This approach combines recurrent neural networks utilizing attention mechanisms, whi"
2020.semeval-1.162,P19-2052,0,0.0139601,"Performing standard NLP tasks on code-mixed data has presented significant challenges. Vyas et al. (2014) attempted to find methods for POS tagging of code-mixed social media text. Another work by Joshi et al. (2016) used CNNs to learn subword level embeddings and then utilized these embeddings in a BiLSTM network to learn subword level information from social media text. Subword level representations are particularly important while dealing with noisy texts containing misspellings and punctuations. However, this work doesn’t capture information about word-level semantics. More recent work by Lal et al. (2019) uses two parallel BiLSTMs, which they call the Collective and Specific Encoder and an additional feature network. This approach combines recurrent neural networks utilizing attention mechanisms, which helps in evaluating the overall sentiment using attention weights when presented with a mixture of local sentiments. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. ∗ Authors equally contributed to this work. 1 https://github.com/keshav22bansal/BAKSA_IITK 1221 Proceedings of the 14th International"
2020.semeval-1.162,D14-1105,0,0.0250665,"ion based LSTM, utilizing the XLM-R embeddings (Conneau et al., 2019). While CNNs have been used for sentiment analysis before (Wang et al., 2016; Yoon and Kim, 2017), none of the previous works have used a self-attention based LSTM along with it. We found that while the CNN component worked well for positive and negative tweets, the self-attention component worked better for neutral tweets, necessitating an ensemble of the two. The implementation of our system is made available via Github1 . 2 Related Work Performing standard NLP tasks on code-mixed data has presented significant challenges. Vyas et al. (2014) attempted to find methods for POS tagging of code-mixed social media text. Another work by Joshi et al. (2016) used CNNs to learn subword level embeddings and then utilized these embeddings in a BiLSTM network to learn subword level information from social media text. Subword level representations are particularly important while dealing with noisy texts containing misspellings and punctuations. However, this work doesn’t capture information about word-level semantics. More recent work by Lal et al. (2019) uses two parallel BiLSTMs, which they call the Collective and Specific Encoder and an a"
2020.semeval-1.162,P16-2037,0,0.101719,"dia text. We only focus on two popular bilingual code-mixing styles namely Hinglish and Spanglish. Sentiment Analysis is a term broadly used to classify states of human affection and emotion. Interpreting code-mixed languages is difficult not only because the sentences may not fit a particular language model, but also because mixed text on social-media usually contains tokens such as hashtags, and usernames. In this paper, we present an ensemble of CNN and self-attention based LSTM, utilizing the XLM-R embeddings (Conneau et al., 2019). While CNNs have been used for sentiment analysis before (Wang et al., 2016; Yoon and Kim, 2017), none of the previous works have used a self-attention based LSTM along with it. We found that while the CNN component worked well for positive and negative tweets, the self-attention component worked better for neutral tweets, necessitating an ensemble of the two. The implementation of our system is made available via Github1 . 2 Related Work Performing standard NLP tasks on code-mixed data has presented significant challenges. Vyas et al. (2014) attempted to find methods for POS tagging of code-mixed social media text. Another work by Joshi et al. (2016) used CNNs to le"
2020.semeval-1.162,O17-1023,0,0.0280398,"ocus on two popular bilingual code-mixing styles namely Hinglish and Spanglish. Sentiment Analysis is a term broadly used to classify states of human affection and emotion. Interpreting code-mixed languages is difficult not only because the sentences may not fit a particular language model, but also because mixed text on social-media usually contains tokens such as hashtags, and usernames. In this paper, we present an ensemble of CNN and self-attention based LSTM, utilizing the XLM-R embeddings (Conneau et al., 2019). While CNNs have been used for sentiment analysis before (Wang et al., 2016; Yoon and Kim, 2017), none of the previous works have used a self-attention based LSTM along with it. We found that while the CNN component worked well for positive and negative tweets, the self-attention component worked better for neutral tweets, necessitating an ensemble of the two. The implementation of our system is made available via Github1 . 2 Related Work Performing standard NLP tasks on code-mixed data has presented significant challenges. Vyas et al. (2014) attempted to find methods for POS tagging of code-mixed social media text. Another work by Joshi et al. (2016) used CNNs to learn subword level emb"
2020.semeval-1.217,D16-1053,0,0.0245676,"dings of each word of a sentence in addition to the word embeddings (Figure 2a). The characters of the sentences are passed through a pair of forward and backward LSTM. For each word, the outputs of the forward and backward LSTMs at the position of the last character of the word are taken, concatenated, and then passed through a highway layer to obtain the character-level embeddings of that word. These character-level embeddings are then concatenated with word embeddings obtained using pre-trained models such as GloVe or ELMo and passed through a pair of BiLSTM Layers. A self-attention layer (Cheng et al., 2016) which helps in learning the dependencies between the words in the sentence and gives different importance to different words while predicting is finally added, followed by a neural-network-based classifier which gives the probability of emphasis for each word. We also concatenated the POS tag of the words to the output of the Attention layers. The representation for each word at the output of Attention layers (along with POS tag concatenation) is passed through the fully connected layers to output the emphasis probability of the word. 3.2 Transformers Approach In this approach, we use Transfo"
2020.semeval-1.217,W14-4012,0,0.176702,"Missing"
2020.semeval-1.217,N16-1030,0,0.0528191,"nable automated design assistance in authoring (Figure 1). The dataset provided for this task is in English, and task description paper (Shirani et al., 2020) provided by the organizers describes the task, data, evaluation, results, and a summary of participating systems. We tried two different approaches for the task. Our BiLSTM + Attention approach is inspired by the baseline paper (Shirani et al., 2019). In this approach, we tweaked the BiLSTM layers (Hochreiter and Schmidhuber, 1997), tried other layers like GRU (Cho et al., 2014), and used character embeddings along with word embeddings (Lample et al., 2016). Our Transformers approach involves transfer learning using Transformer based models (Vaswani et al., 2017). This approach involves two types of models, the first one being a transformer-based model with the BiLSTM layer, the attention layer (Bahdanau et al., 2014), and fully connected layers on top. The second type of model involves transformer-based models with fully connected layers. We used BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and GPT-2 (Radford et al., 2019) as transformer-based models. In the end, we tried the homogeneous and heterogeneous e"
2020.semeval-1.217,D14-1162,0,0.103329,"Missing"
2020.semeval-1.217,N18-1202,0,0.111387,"Missing"
2020.semeval-1.217,P19-1112,0,0.525151,"eved by laying emphasis on particular words to convey the intent better. The emphasis selection task is about designing automatic methods for choosing candidate words to be emphasized in short written texts, to enable automated design assistance in authoring (Figure 1). The dataset provided for this task is in English, and task description paper (Shirani et al., 2020) provided by the organizers describes the task, data, evaluation, results, and a summary of participating systems. We tried two different approaches for the task. Our BiLSTM + Attention approach is inspired by the baseline paper (Shirani et al., 2019). In this approach, we tweaked the BiLSTM layers (Hochreiter and Schmidhuber, 1997), tried other layers like GRU (Cho et al., 2014), and used character embeddings along with word embeddings (Lample et al., 2016). Our Transformers approach involves transfer learning using Transformer based models (Vaswani et al., 2017). This approach involves two types of models, the first one being a transformer-based model with the BiLSTM layer, the attention layer (Bahdanau et al., 2014), and fully connected layers on top. The second type of model involves transformer-based models with fully connected layers"
2020.semeval-1.217,2020.semeval-1.184,0,0.29721,"Missing"
2020.semeval-1.231,D19-1565,0,0.045609,"Missing"
2020.semeval-1.231,2020.semeval-1.186,0,0.0558672,"Missing"
2020.semeval-1.231,D19-5010,0,0.085643,"propaganda span or not. For the classification task, the same system employed an N+2 way classification, where N is the number of propaganda classes. For the technique ∗ Authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1764 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1764–1770 Barcelona, Spain (Online), December 12, 2020. classification, we make use of document context. For context inclusion, we build upon an approach by Hou and Chen (2019), where they use title and previous sentence as context. They pass the context concatenated with the original text for fine-tuning the BERT model. Our approach for span identification makes use of a state-of-the-art language model enhanced by tagging schemes inspired from Named Entity Recognition which aim to model spans better - logically and intuitively - by involving Begin and End tags (Ramshaw and Marcus, 1995) to better formulate spans. In this regard, we experimented with BERT, RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2019), GPT2 (Radford et al., 2019) language models and with"
2020.semeval-1.231,D19-5024,0,0.0348985,"Missing"
2020.semeval-1.231,W95-0107,0,0.098357,"on Semantic Evaluation, pages 1764–1770 Barcelona, Spain (Online), December 12, 2020. classification, we make use of document context. For context inclusion, we build upon an approach by Hou and Chen (2019), where they use title and previous sentence as context. They pass the context concatenated with the original text for fine-tuning the BERT model. Our approach for span identification makes use of a state-of-the-art language model enhanced by tagging schemes inspired from Named Entity Recognition which aim to model spans better - logically and intuitively - by involving Begin and End tags (Ramshaw and Marcus, 1995) to better formulate spans. In this regard, we experimented with BERT, RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2019), GPT2 (Radford et al., 2019) language models and with BIO, BIOE, BIOES tagging schemes. For the final technique classification model, we use RoBERTa language model to get the contextual sequence representation for the propaganda fragment and perform classification. Our best performing models ranked 13th for SI subtask and 5th for the TC subtask on an unknown test set. The implementation for our system is made available via Github1 . 2 Problem Statement For the SI sub"
2020.semeval-1.231,D19-5011,0,0.0155378,"a plain-text document, identify those specific fragments which are propagandistic, and (ii) Technique Classification(TC): Given a text fragment identified as propaganda and its document context, identify the applied propaganda technique in the fragment. Both the subtasks focus on English texts only. An illustration is shown in Figure 1. Figure 1: Example of subtasks span identification(SI) and technique classification(TC) The previous iteration of this task required identifying the propaganda spans and also identifying the propaganda technique for that fragment. The highest scoring system by Yoosuf and Yang (2019) employed an out-of-the-box BERT (Devlin et al., 2018) language model fine-tuned on the token classification task - whether the token is part of a propaganda span or not. For the classification task, the same system employed an N+2 way classification, where N is the number of propaganda classes. For the technique ∗ Authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1764 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1764–1770"
2020.semeval-1.282,S19-2121,0,0.0153499,"tion using an ensemble method with different dropout rates. Method BERT+Bi-GRU BERT+Bi-GRU+ BERT Hidden Layers Concatenated BERT+Bi-LSTM BERT+Bi-LSTM + BERT Hidden Layers Concatenated F1 Score 0.833 0.819 0.797 0.837 Accuracy 0.906 0.915 0.883 0.915 Table 8: Accuracy and macro F1 results on the sub-task A: Greek based on different techniques 10.2 Data Augmentation Techniques We are planning on trying other data augmentation techniques in addition to the ones proposed. One of them considers using back translations (English to language X to English, Greek to language Y to Greek) as suggested by Aggarwal et al. (2019) in their paper for English sub-tasks for OffensEval 2019. We are planning to use these methods for sub-tasks A and C to improve the macro F1 score. 11 Conclusion In this paper, we described the approaches that we used for sub-tasks A, B, and C. We used Transformer based approaches for all sub-tasks and a soft label LSTM based approach for sub-task C. We also described our future approaches based on BERT embedding with Bi-GRU and Bi-LSTM. We are planning to use the data augmentation techniques and try to improve the F1 score on the gold labels for each sub-task. We made use of google colab 13"
2020.semeval-1.282,2020.lrec-1.758,0,0.0289979,"Missing"
2020.semeval-1.282,D14-1179,0,0.0241538,"Missing"
2020.semeval-1.282,W18-4405,0,0.0357272,"Missing"
2020.semeval-1.282,W18-5104,0,0.0137907,"others (OTH). ∗ Authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 https://github.com/karishmaslaud/OffensEval 2123 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2123–2132 Barcelona, Spain (Online), December 12, 2020. 3 Related Work Offensive language detection has been studied at various levels of granularity in the form of abusive language detection (Waseem et al., 2017), hate speech detection (Schmidt and Wiegand, 2017; Kshirsagar et al., 2018) and cyberbullying (Huang et al., 2018). API’s like Perspective2 have been developed to detect content toxicity using machine learning models. Various deep learning and ensemble methods have been proposed (Pitsilis et al., 2018) to detect abusive content online. Sentiment based approaches (BrassardGourdeau and Khoury, 2019) have also been used to detect toxicity. Besides English, there have been various contributions to the detection of offensive content in other languages. For Greek, Pavlopoulos et al. (2017) describe an RNN based attention model trained on the Gazetta dataset for user conten"
2020.semeval-1.282,S19-2011,0,0.0123769,"nding labels for sub-tasks A, B, and C conducted in OffensEval 2019. We used this as a validation dataset for sub-task A and error analysis in sub-tasks B and C. During the post-evaluation phase of the competition the organizers also released gold labels, which were actual test labels corresponding to the dataset against which we were evaluated for in the competition. 5 Sub-task A 5.1 Preprocessing We tried various preprocessing techniques but went with those that gave us the best F1 score on the development set (validation set). Emoji replacement as a preprocessing technique, as suggested by Liu et al. (2019a) in OffensEval 2019, was used for Greek, Arabic, Turkish in sub-task A. We replaced emojis with their corresponding meaning using the emoji library3 (Liu et al., 2019a). Sentences were preprocessed before passing as input to the BERT model. For Greek, stop words and punctuation were removed by 2 3 https://www.perspectiveapi.com https://github.com/carpedm20/emoji 2124 preprocessing with Spacy4 . For Arabic and Turkish consecutive duplicate words like “@USER @USER” were reduced to a single word “@USER”. 5.2 Pre-evaluation Phase During the pre-evaluation phase, we implemented different classica"
2020.semeval-1.282,D17-1117,0,0.0157702,"ge detection (Waseem et al., 2017), hate speech detection (Schmidt and Wiegand, 2017; Kshirsagar et al., 2018) and cyberbullying (Huang et al., 2018). API’s like Perspective2 have been developed to detect content toxicity using machine learning models. Various deep learning and ensemble methods have been proposed (Pitsilis et al., 2018) to detect abusive content online. Sentiment based approaches (BrassardGourdeau and Khoury, 2019) have also been used to detect toxicity. Besides English, there have been various contributions to the detection of offensive content in other languages. For Greek, Pavlopoulos et al. (2017) describe an RNN based attention model trained on the Gazetta dataset for user content moderation. Sigurbergsson and Derczynski (2019) discuss implementation of Logistic Regression, Learned-BiLSTM, Fast-BiLSTM and AUX-Fast-BiLSTM models for Danish. For Arabic, an approach based on convolution neural network and bidirectional LSTM has been proposed (Mohaouchane et al., 2019). Abozinadah and Jones Jr (2017) use a statistical approach for detecting abusive user accounts. Alakrot et al. (2018) use an SVM classifier with n-gram features. For ¨ Turkish, Ozel et al. (2017) use Na¨ıve Bayes Multinomia"
2020.semeval-1.282,2020.lrec-1.629,0,0.0197594,"Missing"
2020.semeval-1.282,W17-1101,0,0.0181544,"dual (IND), group (GRP) or others (OTH). ∗ Authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 https://github.com/karishmaslaud/OffensEval 2123 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2123–2132 Barcelona, Spain (Online), December 12, 2020. 3 Related Work Offensive language detection has been studied at various levels of granularity in the form of abusive language detection (Waseem et al., 2017), hate speech detection (Schmidt and Wiegand, 2017; Kshirsagar et al., 2018) and cyberbullying (Huang et al., 2018). API’s like Perspective2 have been developed to detect content toxicity using machine learning models. Various deep learning and ensemble methods have been proposed (Pitsilis et al., 2018) to detect abusive content online. Sentiment based approaches (BrassardGourdeau and Khoury, 2019) have also been used to detect toxicity. Besides English, there have been various contributions to the detection of offensive content in other languages. For Greek, Pavlopoulos et al. (2017) describe an RNN based attention model trained on the Gazet"
2020.semeval-1.282,2020.lrec-1.430,0,0.0278143,"Missing"
2020.semeval-1.282,W17-3012,0,0.020335,"sified the targeted offensive tweet as individual (IND), group (GRP) or others (OTH). ∗ Authors equally contributed to this work. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 https://github.com/karishmaslaud/OffensEval 2123 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2123–2132 Barcelona, Spain (Online), December 12, 2020. 3 Related Work Offensive language detection has been studied at various levels of granularity in the form of abusive language detection (Waseem et al., 2017), hate speech detection (Schmidt and Wiegand, 2017; Kshirsagar et al., 2018) and cyberbullying (Huang et al., 2018). API’s like Perspective2 have been developed to detect content toxicity using machine learning models. Various deep learning and ensemble methods have been proposed (Pitsilis et al., 2018) to detect abusive content online. Sentiment based approaches (BrassardGourdeau and Khoury, 2019) have also been used to detect toxicity. Besides English, there have been various contributions to the detection of offensive content in other languages. For Greek, Pavlopoulos et al. (2017) describe"
2020.semeval-1.282,N19-1144,0,0.0133584,".82 0.82 0.82 0.81 0.85 0.90 0.90 0.88 0.91 0.85 Table 1: Accuracy and macro F1 results on the sub-task A: Greek and Danish based on validation set. Turkish Arabic English(Task A) English(Task B) Method Bert + Linear Layer Bert + Linear Layer Bert + Linear Layer Roberta + Linear Layer F1 score 0.75 0.82 0.80 0.85 Accuracy 0.86 0.90 0.83 0.91 Table 2: Accuracy and macro F1 results on the sub-task A:Turkish, Arabic, English and sub-task B based on validation set. Sub-task A:English We trained the English data on approx 7,500,000 tweets out of the total soft label tweets given and used the OLID (Zampieri et al., 2019a) dataset to check F1 macro. As we had resource constraints, we trained 1,000,000 data tweets in one go using a batch size of 32 tweets. We continued this procedure till 7,500,000 tweets. Result is shown in Table 2. Sub-task B: For the RoBERTa model, we trained the model for 2 epochs using 80:20 train-val split on the training data. We got the result on the validation set, as indicated in Table 2. Further training the model for 3 epochs reduced the F1 score on the validation set. Therefore, we selected the model that we obtained after 2 epochs as the final model. We also checked the F1 macro"
2020.semeval-1.282,S19-2010,0,0.0457554,"Missing"
2020.semeval-1.56,2020.tacl-1.5,0,0.0201675,"a high degree of overlap. Hence, we believe systems to detect and parse causal relationships would perform well on this task as well. With respect to the first subtask, fine-tuning neural models using information-rich word embeddings seems to form the state-of-the-art (Kyriakakis et al., 2019) and forms the backbone of our submitted approach as well. The second subtask bears a resemblance to relation-entity extraction (of which cause-effect may be considered a specific case), which has also been recently dominated by neural models (Li and Tian, 2020; Soares et al., 2019). We found span-based (Joshi et al., 2020) and discourse parsing-based (Son et al., 2018) approaches particularly interesting with regards to our task. 4 Our Approaches 4.1 Subtask 1 4.1.1 Classical machine learning We tried SVMs and gradient-boosted random forests on linguistic feature-based (POS tags, verb tense and aspect information, etc.) representations of the samples. The baseline provided by the organisers was an SVM classifier on TF-IDF features with stop words removed. We felt that this would result in the removal and down-weighting of terms critical for identifying counterfactuality, like ‘should have’ and ‘would have’. Ind"
2020.semeval-1.56,D14-1181,0,0.0131595,"Missing"
2020.semeval-1.56,W19-5031,0,0.0256014,"- one may consider sentences like “Even if you were the last person on the planet, I wouldn’t go out with you!” - they often imply the existence of other factors that drive the relationship. Thus, from a linguistic perspective, the vocabulary and structure seen in causal sentences and that seen in counterfactual sentences have a high degree of overlap. Hence, we believe systems to detect and parse causal relationships would perform well on this task as well. With respect to the first subtask, fine-tuning neural models using information-rich word embeddings seems to form the state-of-the-art (Kyriakakis et al., 2019) and forms the backbone of our submitted approach as well. The second subtask bears a resemblance to relation-entity extraction (of which cause-effect may be considered a specific case), which has also been recently dominated by neural models (Li and Tian, 2020; Soares et al., 2019). We found span-based (Joshi et al., 2020) and discourse parsing-based (Son et al., 2018) approaches particularly interesting with regards to our task. 4 Our Approaches 4.1 Subtask 1 4.1.1 Classical machine learning We tried SVMs and gradient-boosted random forests on linguistic feature-based (POS tags, verb tense a"
2020.semeval-1.56,P16-1101,0,0.044357,"Missing"
2020.semeval-1.56,D14-1162,0,0.0941749,"Missing"
2020.semeval-1.56,P19-1279,0,0.0247665,"that seen in counterfactual sentences have a high degree of overlap. Hence, we believe systems to detect and parse causal relationships would perform well on this task as well. With respect to the first subtask, fine-tuning neural models using information-rich word embeddings seems to form the state-of-the-art (Kyriakakis et al., 2019) and forms the backbone of our submitted approach as well. The second subtask bears a resemblance to relation-entity extraction (of which cause-effect may be considered a specific case), which has also been recently dominated by neural models (Li and Tian, 2020; Soares et al., 2019). We found span-based (Joshi et al., 2020) and discourse parsing-based (Son et al., 2018) approaches particularly interesting with regards to our task. 4 Our Approaches 4.1 Subtask 1 4.1.1 Classical machine learning We tried SVMs and gradient-boosted random forests on linguistic feature-based (POS tags, verb tense and aspect information, etc.) representations of the samples. The baseline provided by the organisers was an SVM classifier on TF-IDF features with stop words removed. We felt that this would result in the removal and down-weighting of terms critical for identifying counterfactuality"
2020.semeval-1.56,P17-2103,0,0.186519,"for the second subtask. As the datasets were imbalanced, our evaluation metrics were precision, recall, F1-score and exact match (exclusively for the second subtask). The test set had 7000 sentences for the first subtask and 1950 for the second. Some samples from the training set are given in Tables 1 and 2. For more details, please refer to Yang et al. (2020). 1 https://github.com/gargrohin/counterfactuals-nlp 459 3 Previous and Related Work Research on counterfactuals is a relatively new area in the NLP community, and consequently, limited research literature exists. The closest work is by Son et al. (2017) which aimed to detect counterfactual elements in tweets. They identify a set of rules to filter such examples based on grammatical form (sentences containing “wish”, “should have”, etc.) and train an SVM to classify filtered sentences on the basis of POS tags and n-gram features. While their method works well on tweets, we found that these rules had limited coverage and were not effective in filtering long sentences like those in our dataset. The filtering also removes several examples, which makes it harder to train further classifiers in an already data-constrained setting. Iatridou (2000)"
2020.semeval-1.56,D18-1372,0,0.0136962,"ms to detect and parse causal relationships would perform well on this task as well. With respect to the first subtask, fine-tuning neural models using information-rich word embeddings seems to form the state-of-the-art (Kyriakakis et al., 2019) and forms the backbone of our submitted approach as well. The second subtask bears a resemblance to relation-entity extraction (of which cause-effect may be considered a specific case), which has also been recently dominated by neural models (Li and Tian, 2020; Soares et al., 2019). We found span-based (Joshi et al., 2020) and discourse parsing-based (Son et al., 2018) approaches particularly interesting with regards to our task. 4 Our Approaches 4.1 Subtask 1 4.1.1 Classical machine learning We tried SVMs and gradient-boosted random forests on linguistic feature-based (POS tags, verb tense and aspect information, etc.) representations of the samples. The baseline provided by the organisers was an SVM classifier on TF-IDF features with stop words removed. We felt that this would result in the removal and down-weighting of terms critical for identifying counterfactuality, like ‘should have’ and ‘would have’. Indeed, we found that performance improved remarka"
2020.semeval-1.56,2020.semeval-1.40,0,0.0229892,"ed by the task organisers. The examples were segments from news articles in English and were quite varied in length, from around 5 words to around 400 words. There were 13000 examples for the first subtask, with an ∼88:12 negative-positive split, and 3500 for the second subtask. As the datasets were imbalanced, our evaluation metrics were precision, recall, F1-score and exact match (exclusively for the second subtask). The test set had 7000 sentences for the first subtask and 1950 for the second. Some samples from the training set are given in Tables 1 and 2. For more details, please refer to Yang et al. (2020). 1 https://github.com/gargrohin/counterfactuals-nlp 459 3 Previous and Related Work Research on counterfactuals is a relatively new area in the NLP community, and consequently, limited research literature exists. The closest work is by Son et al. (2017) which aimed to detect counterfactual elements in tweets. They identify a set of rules to filter such examples based on grammatical form (sentences containing “wish”, “should have”, etc.) and train an SVM to classify filtered sentences on the basis of POS tags and n-gram features. While their method works well on tweets, we found that these rul"
2020.semeval-1.61,W14-1606,1,0.822362,"Missing"
2020.semeval-1.61,Q17-1003,1,0.884589,"Missing"
2020.semeval-1.61,K16-1008,1,0.857692,"Missing"
2020.semeval-1.61,L18-1564,1,0.912684,"natural language understanding systems and evaluating whether a system has sense-making capability remains a fundamental question in the natural language processing field (Modi, 2017; Modi, 2016; Modi and Titov, 2014). One important difference between human and machine text understanding lies in the fact that humans have access to commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that is assumed to be common ground (Modi et al., 2017). For a computer system, inferring unmentioned facts is a non-trivial challenge (Ostermann et al., 2018a). For our problem, we have proposed methods to include common sense in the validation and reasoning paradigm (Wang et al., 2019). Task 4 of semeval 2020 (Wang et al., 2020) is a common-sense validation and explanation task. It consists of classifying against common sense sentences from sentences that make sense. Figure 1 shows examples from subtask A and subtask B. In subtask A, clearly sentence 1 is against common sense. Subtask B contains three options for reasons to explain why sentence 1 is against common sense. As orange juice does not taste good on cereal, but milk does, sentence 1 mak"
2020.semeval-1.61,S18-1119,1,0.94784,"natural language understanding systems and evaluating whether a system has sense-making capability remains a fundamental question in the natural language processing field (Modi, 2017; Modi, 2016; Modi and Titov, 2014). One important difference between human and machine text understanding lies in the fact that humans have access to commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that is assumed to be common ground (Modi et al., 2017). For a computer system, inferring unmentioned facts is a non-trivial challenge (Ostermann et al., 2018a). For our problem, we have proposed methods to include common sense in the validation and reasoning paradigm (Wang et al., 2019). Task 4 of semeval 2020 (Wang et al., 2020) is a common-sense validation and explanation task. It consists of classifying against common sense sentences from sentences that make sense. Figure 1 shows examples from subtask A and subtask B. In subtask A, clearly sentence 1 is against common sense. Subtask B contains three options for reasons to explain why sentence 1 is against common sense. As orange juice does not taste good on cereal, but milk does, sentence 1 mak"
2020.semeval-1.61,N18-1202,0,0.019089,"ymbolic and statistical approaches to recent approaches based on deep neural networks; which model context of language, take advantage of external data or knowledge resources, and achieve the state-of-the-art performance, and at times, even near or above human performance. A major landmark in NLP is the development of pre-trained models and embeddings that can be used as features or further fine-tuned for downstream tasks. These models are often trained based on large corpora of textual data to capture different word senses. The defining contribution of Embeddings from Language Models (ELMo) (Peters et al., 2018) is its contextual word embeddings, which are built relying on the entire input sentence that they belong to. The recent Bidirectional Encoder Representations from Transformers (BERT) model outperforms previous competitive approaches by better capturing the context. A more recent model, XLNET (Yang et al., 2019), exceeded the performance of the vanilla BERT variant on several benchmarks. Robustly Optimized BERT Approach (RoBERTa) (Liu et al., 2019) achieved further improvement by making changes to the pre-training approach used in BERT. It includes randomizing masked tokens in the cloze pre-tr"
2020.semeval-1.61,S18-1120,0,0.0133577,"iction pre-training task with an additional task which compels the model to also predict whether a candidate next sentence comes from the same document or not. A Lite BERT (ALBERT) (Lan et al., 2019) implements several novel parameter reduction techniques to increase the training speed and efficiency of BERT, enabling a much deeper scale-up than the original large variant of BERT while having fewer parameters. ELECTRA (Clark et al., 2020) is used to pre-train transformers with comparatively less computation. This model is similar to the discriminator of a GAN. Common sense validation : TriAN (Wang et al., 2018) achieved state-of-the-art performance for SemEval ’18 Task-11 : Machine Comprehension Using Commonsense Knowledge (Ostermann et al., 2018b). It proposed a threeway attention mechanism to model interactions between the text, question, and answers, on top of BiLSTMs. It incorporated relational features (based on ConceptNet). CommonsenseQA (Talmor et al., 2018) is a large multi-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers. The leaderboard provides a variety of ensembling techniques using different LMs and also several ins"
2020.semeval-1.61,P19-1393,0,0.0715178,"he natural language processing field (Modi, 2017; Modi, 2016; Modi and Titov, 2014). One important difference between human and machine text understanding lies in the fact that humans have access to commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that is assumed to be common ground (Modi et al., 2017). For a computer system, inferring unmentioned facts is a non-trivial challenge (Ostermann et al., 2018a). For our problem, we have proposed methods to include common sense in the validation and reasoning paradigm (Wang et al., 2019). Task 4 of semeval 2020 (Wang et al., 2020) is a common-sense validation and explanation task. It consists of classifying against common sense sentences from sentences that make sense. Figure 1 shows examples from subtask A and subtask B. In subtask A, clearly sentence 1 is against common sense. Subtask B contains three options for reasons to explain why sentence 1 is against common sense. As orange juice does not taste good on cereal, but milk does, sentence 1 makes less sense than sentence 2. We use the generated embedding from transformer based encoders like BERT (Devlin et al., 2018), RoB"
2020.semeval-1.61,2020.semeval-1.39,0,0.0336443,"2017; Modi, 2016; Modi and Titov, 2014). One important difference between human and machine text understanding lies in the fact that humans have access to commonsense knowledge while processing text, which helps them to draw inferences about facts that are not mentioned in a text, but that is assumed to be common ground (Modi et al., 2017). For a computer system, inferring unmentioned facts is a non-trivial challenge (Ostermann et al., 2018a). For our problem, we have proposed methods to include common sense in the validation and reasoning paradigm (Wang et al., 2019). Task 4 of semeval 2020 (Wang et al., 2020) is a common-sense validation and explanation task. It consists of classifying against common sense sentences from sentences that make sense. Figure 1 shows examples from subtask A and subtask B. In subtask A, clearly sentence 1 is against common sense. Subtask B contains three options for reasons to explain why sentence 1 is against common sense. As orange juice does not taste good on cereal, but milk does, sentence 1 makes less sense than sentence 2. We use the generated embedding from transformer based encoders like BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), AlBERT (Lan et al.,"
2021.acl-long.313,P19-1424,0,0.0679001,"eover, we plan to continue to grow, revise and upgrade ILDC. We release the ILDC and code for the prediction and explanation models via GitHub1 . 2 Related Work There has been extensive research on legal domain text, and various corpora and tasks have been proposed e.g., prior case retrieval (Jackson et al., 2003), summarization (Tran et al., 2019; Bhattacharya et al., 2019a), catchphrase extraction (Galgani et al., 2012), crime classification (Wang et al., 2019), and judgment prediction (Zhong et al., 2020). Why ILDC? The task of Legal Judgment Prediction (LJP) and its corresponding corpora (Chalkidis et al., 2019; Zhong et al., 2020; Yang et al., 2019a; Xiao et al., 2018) are related to our setting. In the LJP task, given the facts of a case, violations, charges (e.g., theft) and terms of penalty are predicted. However, the ILDC and the CJPE task introduced in this paper differ from the existing LJP corpora and task in multiple ways. Firstly, we require prediction algorithms to explain the decisions in the CJPE task, to evaluate the explanations we provide a separate test set annotated with gold explanations. Secondly, in the LJP task, typically, the facts of a case are explicitly provided. However, i"
2021.acl-long.313,D19-1667,0,0.0287517,"Missing"
2021.acl-long.313,N19-1376,1,0.864104,"Missing"
2021.acl-long.313,N19-1423,0,0.0084912,"s (ECHR). It contains facts, articles violated (if any), and an importance score for each case. ILDC contrasts with the existing LJP corpora, where mainly the civil law system and cases are considered. Though the proposed corpus focuses on Indian cases, our analysis reveals (§ 4.2) that the language used in the cases is quite challenging to process computationally and provides a good playground for developing realistic legal text understanding systems. Several different approaches and corpora have been proposed for the LJP task. Chalkidis et al. (2019) proposed a hierarchical version of BERT (Devlin et al., 2019) to alleviate BERT’s input token count limitation for the LJP task. Yang et al. (2019a) applied Multi-Perspective Bi-Feedback Network for predicting the relevant law articles, charges, and terms of penalty on Chinese AI and Law challenge (CAIL2018) datasets. Xu et al. (2020) proposed a system for distinguishing confusing law articles in the LJP task. Zhong et al. (2018) applied topological multi-task learning on a directed acyclic graph to predict charges like theft, traffic violation, intentional homicide on three Chinese datasets (CJO, PKU, and CAIL). Luo et al. (2017) proposed an attention-"
2021.acl-long.313,C18-1041,0,0.0348177,"Missing"
2021.acl-long.313,N19-1357,0,0.0260051,"ayerwise Relevance Propagation (LRP) (Bach et al., 2015) and DeepLIFT (Shrikumar et al., 2017) methods did not work in our case. Due to the long length of documents, model agnostic explainability methods like LIME (Ribeiro et al., 2016) and Anchors (Ribeiro et al., 2018) were not applicable. We also experimented with attention-based methods, and Integrated Gradients (Sundararajan et al., 2017) method using the CAPTUM library (Kokhlikyan et al., 2019). However, these highlighted only a few tokens or short phrases. Moreover, attention-based scores are not necessarily indicative of explanations (Jain and Wallace, 2019). To extract explanations, we propose a method inspired from Li et al. (2016) and Zeiler and Fergus (2014). The idea is to use the occlusion method at both levels of the hierarchy. For each document, for the BiGRU part of the model, we mask each complete chunk embedding one at a time. The masked input is passed through the trained BiGRU, and the output probability (masked probability) of the label obtained by the original unmasked model is calculated. The masked probability is compared with unmasked probability to calculate the chunk explainability score. Formally, for a chunk c, if the sigmoi"
2021.acl-long.313,C18-2032,0,0.0179934,"the LJP task on French Supreme Court cases. Katz et al. (2017) presented a random forest model to predict the “Reverse”, “Affirm”, and “Other” decisions of US Supreme Court judges. We also experiment with some of these models as baselines for the CJPE task (§ 5). Explainability in a system is of paramount importance in the legal domain. Zhong et al. (2020) presented a QA based model using reinforcement learning for explainable LJP task on three Chinese datasets (CJO, PKU, and CAIL). The model aims to predict the appropriate crime by asking relevant questions related to the facts of the case. Jiang et al. (2018) used a rationale augmented classification model for the charge prediction task. The model selects as rationale the relevant textual portions in the fact description. Ye et al. (2018) used labelconditioned Seq2Seq model for charge prediction on Chinese legal documents, and the interpretation comprise the selection of the relevant rationales in the text for the charge. We develop an explainability model based on the occlusion method (§ 5.2). 3 Indian Legal Document Corpus In this paper, we introduce the I NDIAN L EGAL D OCUMENTS C ORPUS (ILDC), a collection of case proceedings (in the English l"
2021.acl-long.313,D14-1181,0,0.0052279,"Missing"
2021.acl-long.313,2021.ccl-1.108,0,0.0519143,"Missing"
2021.acl-long.313,W02-0109,0,0.129873,"ed probability is compared with unmasked probability to calculate the chunk explainability score. Formally, for a chunk c, if the sigmoid outputs (of the BiGRU) are σm (when the chunk was not masked) and σm0 (when the chunk was masked) and the predicted label is y then the probabilities ( and chunk score sc = pm − pm0 and σm0 /m , y=1 pm0 /m = 1 − σm0 /m , y = 0 We obtain sentences that explain the decision from the transformer part of the model (XLNet) using the chunks that were assigned positive scores. Each chunk (length 512 tokens) is segmented into sentences using NLTK sentence splitter (Loper and Bird, 2002). Similar to BiGRU, each sentence is masked and the output of the transformer at the classification head (softmax logits) is compared 4053 Metric Jaccard Similarity Overlap-Min Overlap-Max ROUGE-1 ROUGE-2 ROUGE-L BLEU Meteor Explainability Model vs Experts Expert 1 2 3 4 5 0.333 0.317 0.328 0.324 0.318 0.744 0.39 0.444 0.303 0.439 0.16 0.22 0.589 0.414 0.517 0.295 0.407 0.28 0.3 0.81 0.36 0.401 0.296 0.423 0.099 0.18 0.834 0.35 0.391 0.297 0.444 0.093 0.177 0.617 0.401 0.501 0.294 0.407 0.248 0.279 Table 5: Machine explanations v/s Expert explanations with logits of the label corresponding to"
2021.acl-long.313,D17-1289,0,0.0354935,"Missing"
2021.acl-long.313,N18-1049,0,0.0198225,"Missing"
2021.acl-long.313,P02-1040,0,0.109512,"Missing"
2021.acl-long.313,D14-1162,0,0.0838052,"Missing"
2021.acl-long.313,N16-3020,0,0.0603208,"xperimented with a variety explainability algorithms as a post-prediction step. We experimented with the best judgment prediction model (Hierarchical Transformer (XLNet + BiGRU)) for all the explainable algorithms. We explored three class of explainability methods (Xie et al., 2020): attribution based, model agnostic, and attention-based. In the class of attribution based methods, Layerwise Relevance Propagation (LRP) (Bach et al., 2015) and DeepLIFT (Shrikumar et al., 2017) methods did not work in our case. Due to the long length of documents, model agnostic explainability methods like LIME (Ribeiro et al., 2016) and Anchors (Ribeiro et al., 2018) were not applicable. We also experimented with attention-based methods, and Integrated Gradients (Sundararajan et al., 2017) method using the CAPTUM library (Kokhlikyan et al., 2019). However, these highlighted only a few tokens or short phrases. Moreover, attention-based scores are not necessarily indicative of explanations (Jain and Wallace, 2019). To extract explanations, we propose a method inspired from Li et al. (2016) and Zeiler and Fergus (2014). The idea is to use the occlusion method at both levels of the hierarchy. For each document, for the BiGRU"
2021.acl-long.313,sulea-etal-2017-predicting,0,0.0603804,"Missing"
2021.acl-long.313,2020.acl-main.280,0,0.0120247,".2) that the language used in the cases is quite challenging to process computationally and provides a good playground for developing realistic legal text understanding systems. Several different approaches and corpora have been proposed for the LJP task. Chalkidis et al. (2019) proposed a hierarchical version of BERT (Devlin et al., 2019) to alleviate BERT’s input token count limitation for the LJP task. Yang et al. (2019a) applied Multi-Perspective Bi-Feedback Network for predicting the relevant law articles, charges, and terms of penalty on Chinese AI and Law challenge (CAIL2018) datasets. Xu et al. (2020) proposed a system for distinguishing confusing law articles in the LJP task. Zhong et al. (2018) applied topological multi-task learning on a directed acyclic graph to predict charges like theft, traffic violation, intentional homicide on three Chinese datasets (CJO, PKU, and CAIL). Luo et al. (2017) proposed an attention-based model to predict the charges given the facts of the case along with the relevant articles on a dataset of Criminal Law of the People’s Republic of China. Hu et al. (2018) used an attribute-attentive model in a fewshot setup for charge prediction from facts of the case."
2021.acl-long.313,N16-1174,0,0.110422,"Missing"
2021.acl-long.313,N16-1000,0,0.101511,"methods did not work in our case. Due to the long length of documents, model agnostic explainability methods like LIME (Ribeiro et al., 2016) and Anchors (Ribeiro et al., 2018) were not applicable. We also experimented with attention-based methods, and Integrated Gradients (Sundararajan et al., 2017) method using the CAPTUM library (Kokhlikyan et al., 2019). However, these highlighted only a few tokens or short phrases. Moreover, attention-based scores are not necessarily indicative of explanations (Jain and Wallace, 2019). To extract explanations, we propose a method inspired from Li et al. (2016) and Zeiler and Fergus (2014). The idea is to use the occlusion method at both levels of the hierarchy. For each document, for the BiGRU part of the model, we mask each complete chunk embedding one at a time. The masked input is passed through the trained BiGRU, and the output probability (masked probability) of the label obtained by the original unmasked model is calculated. The masked probability is compared with unmasked probability to calculate the chunk explainability score. Formally, for a chunk c, if the sigmoid outputs (of the BiGRU) are σm (when the chunk was not masked) and σm0 (when"
2021.eacl-main.71,D18-1316,0,0.0181972,"t due to the discrete nature of the text. The basic requirement of imperceptibility of perturbation by human judges is much more challenging in a language data setting. Therefore, the adversarial sample needs to be grammatically correct and semantically sound. Perturbations at word or character level that are perceptible to human judges have been explored in-depth (Ebrahimi et al., 2017; Belinkov and Bisk, 2017; Jia and Liang, 2017; Gao et al., 2018). Work on defense against misspellings based attacks (Pruthi et al., 2019) and use of optimization algorithms for attacks like genetic algorithm (Alzantot et al., 2018; Wang et al., 2019) and particle swarm optimization (Zang et al., 2020) have also been explored. With the rise of pre-trained language models, like BERT (Devlin et al., 2018) and other transformer-based models, generating human imperceptible adversarial examples has become more challenging. Wallace et al. (2019), Jin et al. (2019), and Pruthi et al. (2019) have explored these models from different perspectives. Adversarial examples can be generated using black-box, where no knowledge about the model is accessible, and white-box, where information about the technical details of models are know"
2021.eacl-main.71,2020.emnlp-main.500,0,0.0391097,"ple (Word Ranking) 2) Replacing the chosen word (Word Replacement). Word Ranking is necessary to ensure that the word that contributes the most to the output prediction is considered as the candidate for replacement in the next step. Other constraints like generating semantically similar adversarial samples, human imperceptibility, and minimal perturbation percentage are also considered. Previous work has obtained word ranking by performing deletion of words (e.g., BAE-R (Garg and Ramakrishnan, 2020), TextFooler (Jin et al., 2019)), and replacement of words with [UNK] token (e.g., BERTAttack (Li et al., 2020)) and then ranking the words based on the output logits difference. Recently in the model explainability domain, the method of Occlusion and Language Models (OLM) 841 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 841–849 April 19 - 23, 2021. ©2021 Association for Computational Linguistics (Harbecke and Alt, 2020) has been proposed, the authors argue that the data likelihood of the samples obtained after either deleting the token or replacement with [UNK] token is very low, which makes these methods unsuitable for determining"
2021.eacl-main.71,2021.ccl-1.108,0,0.084513,"Missing"
2021.eacl-main.71,D18-1407,0,0.034106,"Missing"
2021.eacl-main.71,2020.emnlp-main.498,0,0.0480501,"Missing"
2021.eacl-main.71,2020.acl-srw.16,0,0.0195178,"so considered. Previous work has obtained word ranking by performing deletion of words (e.g., BAE-R (Garg and Ramakrishnan, 2020), TextFooler (Jin et al., 2019)), and replacement of words with [UNK] token (e.g., BERTAttack (Li et al., 2020)) and then ranking the words based on the output logits difference. Recently in the model explainability domain, the method of Occlusion and Language Models (OLM) 841 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 841–849 April 19 - 23, 2021. ©2021 Association for Computational Linguistics (Harbecke and Alt, 2020) has been proposed, the authors argue that the data likelihood of the samples obtained after either deleting the token or replacement with [UNK] token is very low, which makes these methods unsuitable for determining relevance of the word towards the output probability. The authors propose the use of language models for calculating the relevance of the words in a sentence. Taking inspiration from OLM, we propose Adv-OLM, a black box attack method, that adapts the idea of OLM (as the Work Ranking Strategy) to find the relevant words to replace. We empirically show that OLM provides a better set"
2021.eacl-main.71,D17-1215,0,0.0322162,"o the original input, making them imperceptible to humans while fooling the deep learning models to give incorrect predictions. Adversarial attack on textual data is much more difficult due to the discrete nature of the text. The basic requirement of imperceptibility of perturbation by human judges is much more challenging in a language data setting. Therefore, the adversarial sample needs to be grammatically correct and semantically sound. Perturbations at word or character level that are perceptible to human judges have been explored in-depth (Ebrahimi et al., 2017; Belinkov and Bisk, 2017; Jia and Liang, 2017; Gao et al., 2018). Work on defense against misspellings based attacks (Pruthi et al., 2019) and use of optimization algorithms for attacks like genetic algorithm (Alzantot et al., 2018; Wang et al., 2019) and particle swarm optimization (Zang et al., 2020) have also been explored. With the rise of pre-trained language models, like BERT (Devlin et al., 2018) and other transformer-based models, generating human imperceptible adversarial examples has become more challenging. Wallace et al. (2019), Jin et al. (2019), and Pruthi et al. (2019) have explored these models from different perspectives"
2021.eacl-main.71,2020.emnlp-demos.16,0,0.0976592,"Missing"
2021.eacl-main.71,P19-1561,0,0.0183384,"dels to give incorrect predictions. Adversarial attack on textual data is much more difficult due to the discrete nature of the text. The basic requirement of imperceptibility of perturbation by human judges is much more challenging in a language data setting. Therefore, the adversarial sample needs to be grammatically correct and semantically sound. Perturbations at word or character level that are perceptible to human judges have been explored in-depth (Ebrahimi et al., 2017; Belinkov and Bisk, 2017; Jia and Liang, 2017; Gao et al., 2018). Work on defense against misspellings based attacks (Pruthi et al., 2019) and use of optimization algorithms for attacks like genetic algorithm (Alzantot et al., 2018; Wang et al., 2019) and particle swarm optimization (Zang et al., 2020) have also been explored. With the rise of pre-trained language models, like BERT (Devlin et al., 2018) and other transformer-based models, generating human imperceptible adversarial examples has become more challenging. Wallace et al. (2019), Jin et al. (2019), and Pruthi et al. (2019) have explored these models from different perspectives. Adversarial examples can be generated using black-box, where no knowledge about the model i"
2021.eacl-main.71,P19-1103,0,0.0233515,"Missing"
2021.eacl-main.71,2020.acl-main.263,0,0.0359769,"Missing"
2021.eacl-main.71,D19-1221,0,0.0198913,"ceptible to human judges have been explored in-depth (Ebrahimi et al., 2017; Belinkov and Bisk, 2017; Jia and Liang, 2017; Gao et al., 2018). Work on defense against misspellings based attacks (Pruthi et al., 2019) and use of optimization algorithms for attacks like genetic algorithm (Alzantot et al., 2018; Wang et al., 2019) and particle swarm optimization (Zang et al., 2020) have also been explored. With the rise of pre-trained language models, like BERT (Devlin et al., 2018) and other transformer-based models, generating human imperceptible adversarial examples has become more challenging. Wallace et al. (2019), Jin et al. (2019), and Pruthi et al. (2019) have explored these models from different perspectives. Adversarial examples can be generated using black-box, where no knowledge about the model is accessible, and white-box, where information about the technical details of models are known. Generation of textual adversarial samples in a black-box setting consists of two steps 1) Finding words to replace in a sample (Word Ranking) 2) Replacing the chosen word (Word Replacement). Word Ranking is necessary to ensure that the word that contributes the most to the output prediction is considered as th"
2021.eacl-main.71,W18-5446,0,0.0797958,"Missing"
2021.eacl-main.71,2020.acl-main.540,0,0.0296351,"tibility of perturbation by human judges is much more challenging in a language data setting. Therefore, the adversarial sample needs to be grammatically correct and semantically sound. Perturbations at word or character level that are perceptible to human judges have been explored in-depth (Ebrahimi et al., 2017; Belinkov and Bisk, 2017; Jia and Liang, 2017; Gao et al., 2018). Work on defense against misspellings based attacks (Pruthi et al., 2019) and use of optimization algorithms for attacks like genetic algorithm (Alzantot et al., 2018; Wang et al., 2019) and particle swarm optimization (Zang et al., 2020) have also been explored. With the rise of pre-trained language models, like BERT (Devlin et al., 2018) and other transformer-based models, generating human imperceptible adversarial examples has become more challenging. Wallace et al. (2019), Jin et al. (2019), and Pruthi et al. (2019) have explored these models from different perspectives. Adversarial examples can be generated using black-box, where no knowledge about the model is accessible, and white-box, where information about the technical details of models are known. Generation of textual adversarial samples in a black-box setting cons"
2021.semeval-1.175,W19-5034,0,0.0285271,"fully connected layer and a CRF layer. The work by Wu and He 2019 on Relation Extraction uses BERT to identify the different types of relations between pair of entities in the given text. The system does not automatically recognize the entities between which relation exists, rather entities of interest need to be manually specified. 2 https://github.com/elsevierlabs/ OA-STM-Corpus 3 https://github.com/akashgnr31/ Counts-And-Measurement System overview Pre Processing Since we are using the SciBERT model, a maximum of 512 tokens can be passed as input to the model. Therefore, we used SciSpaCy (Neumann et al. 2019) to split the paragraph into sentences, and these sentences were passed as input to the SciBERT model. 3.2 Subtask 1 (Quantity Extraction) Input sentences were tokenized using a SciBERT tokenizer from HuggingFace (Wolf et al. 2020) implementation. The Quantity span were transformed into BIO / IOB format (Ramshaw and Marcus 1995) and used as the true-labels for training the model. The tokenized sentence is passed through SciBERT. Tanh activation function is applied over the final hidden state of SciBERT i.e. 0 Hi = W1 [tanh(Hi )] + b1 i = 0, 1, ..., len Here Hi is the hidden units corresponding"
2021.semeval-1.175,D16-1264,0,0.0267492,"span. 4 Experimental Setup The dataset is split into two parts - train set and dev set in a ratio of 90:10. The models were trained on the train set and were validated on the dev set. The environment and packages used for training and pre-processing are listed in appendix B. 4.1 Evaluation Metrics The official metrics used by the SemEval organizer are F1-measure, F1-overlap, and Exact Match. Exact Match is a binary value of 0 or 1, while F1measure is a token level overlap ratio of submission to true spans, where tokenization is done using simple white space delimiters. F1-overlap is a SQuAD (Rajpurkar et al., 2016) style Overlap score based on F1-measure, which penalizes the negative submissions more strictly. The final evaluation is based on a global F1-overlap score averaged across all subtasks. 5 5.1 Results Model Variants Used We tried various models like BERT-Base, BERTMedium (Devlin et al., 2018), SciBERT, and BioBERT (Lee et al. 2019). We could not try BERTLarge due to computational limitations. The results for the top two models are shown in Table 2. We also experimented with Bi-LSTM layers on top of BERT, but the model was overfitting due 1235 Model SciBERT SciBERT BERT-Med. Data Set eval dev e"
2021.semeval-1.175,W95-0107,0,0.0735905,"pecified. 2 https://github.com/elsevierlabs/ OA-STM-Corpus 3 https://github.com/akashgnr31/ Counts-And-Measurement System overview Pre Processing Since we are using the SciBERT model, a maximum of 512 tokens can be passed as input to the model. Therefore, we used SciSpaCy (Neumann et al. 2019) to split the paragraph into sentences, and these sentences were passed as input to the SciBERT model. 3.2 Subtask 1 (Quantity Extraction) Input sentences were tokenized using a SciBERT tokenizer from HuggingFace (Wolf et al. 2020) implementation. The Quantity span were transformed into BIO / IOB format (Ramshaw and Marcus 1995) and used as the true-labels for training the model. The tokenized sentence is passed through SciBERT. Tanh activation function is applied over the final hidden state of SciBERT i.e. 0 Hi = W1 [tanh(Hi )] + b1 i = 0, 1, ..., len Here Hi is the hidden units corresponding to token i and len is the maximum length of the tokenized sentence. Similarly, [CLS] token is processed. 0 Hcls = W0 [tanh(H0 )] + b0 1233 Finally, we get the final representation for the 0 0 sentence by concatenating Hcls and Hi and this is used for prediction via the softmax. 00 0 representation of the Quantity. The averaged"
2021.semeval-1.19,P17-1168,0,0.0529323,"Missing"
2021.semeval-1.19,K16-1008,1,0.806755,"Missing"
2021.semeval-1.19,L18-1564,1,0.871783,"Missing"
2021.semeval-1.19,D14-1162,0,0.0907679,"Sentence Prediction task. We then experimented with the ensemble of BERT Large and ALBERT xxlarge-v2 model predictions. We experimented by assigning different weights to BERT and ALBERT models and found out that equal weights to both works better. We later did an ensemble of the fine-tuned ALBERT model with a non fine-tuned ALBERT model. It gave much improved results on Subtask 1 and Subtask 2. 6 Results and Analysis The results of all the transformer based approaches are given in Table 3. The recurrence based models did not work and predicted answers with a random probability. We used GloVe (Pennington et al., 2014) vector embeddings that are not contextualized, unlike BERT embeddings. Moreover, the task requires some world knowledge since we need to predict an abstract word whose meaning can possibly be encoded if it is trained on large English corpus. Transformer based models are trained on large corpora and implicitly learn concepts grounded in 179 Type Example Article WC Question Options Scores Article WN Question Options Scores Article CC Question Options Scores Article CN Question Options Scores ... ”Tennis chose me. It’s something I never fell in love with,” Tomic told Australia’s Channel Seven. ”"
2021.semeval-1.19,D19-1410,0,0.0137318,"nsformer models (Vaswani et al., 2017) as these capture the context better due to the self-attention mechanism. Moreover, pre-trained models are readily available. The task is somewhat similar to a multiple choice question answering task. We experimented with the MCQ based approach as mentioned by Radford (2018) where a linear layer is built over the transformer and the correct answer is predicted by applying softmax over the probabilities of each option. One approach to get context from the article is to extract the most relevant sentences to the question with sentence similarity techniques (Reimers and Gurevych, 2019). We experimented with this approach and extracted “Top-k” sentences that were most semantically similar to the given question from the article. One of the major challenge in this task is to handle the long length of the article. Pappagari et al. (2019) discuss the approach of using hierarchical transformers for text classification problem to tackle long passages. BERT is applied to text segments and an LSTM layer or transformer is applied to get document embedding. Another approach is to model the shared task as a masked language modeling task. The transformer based models like BERT (Devlin e"
2021.semeval-1.19,2021.semeval-1.4,0,0.0328046,"CAM-Imperceptibility) and Subtask 2 (ReCAM-Nonspecificity). For Subtask 3 (ReCAM-Intersection), we submitted the ALBERT model as it gives the best results. We tried multiple approaches and found that Masked Language Modeling(MLM) based approach works the best. 1 Introduction Computers’ ability to understand, represent, and express text with abstract meaning is a fundamental problem towards achieving true natural language understanding. In past decades, significant advancement has been achieved in representation learning. SemEval-2021 Task 4 : Reading Comprehension of Abstract Meaning (ReCAM) (Zheng et al., 2021) explores the ability of machines to understand abstract concepts and proposes to predict abstract words just as humans do while writing article summaries. In the shared task, text passages are provided to read and understand abstract meaning. It consists of three subtasks where the first two subtasks are based on two different definitions of abstractness 1) Imperceptibility (Spreen and Schulz, 1966) and 2) Non-specificity (Changizi et al., 2008) and the third subtask discusses their intersection. Many cloze-style reading comprehension datasets like CNN/Daily Mail (Hermann et al., 2015) and Ch"
2021.semeval-1.24,W00-0726,0,0.383307,"ensEval (Zampieri et al., 2019). The task organizers concluded that most top-performing teams either used BERT (Liu et al., 2019) or an ensemble model to achieve SOTA results. Interestingly, the task of locating toxic spans is relatively novel, and its successful completion can be groundbreaking. A recent approach with a narrower scope is by Mathew et al. (2020), who focused on the rationality of decision in the task of hate speech detection. Span Identification: Span detection/identification tasks include numerous tasks like named entity recognition (NER) (Nadeau and Sekine, 2007), chunking (Sang and Buchholz, 2000) and keyphrase detection (Augenstein et al., 2017). (Papay et al., 2020) analyzed the span identification tasks via performance prediction over various neural architectures and showed that the presence of BERT component in the model is the highest positive predictor for these tasks. Inspired by this observation, we have built our model based on the transformer architecture, further exploiting the benefits of semi-supervised learning and modified Dice Loss. 3 3.1 our approaches. We further split the training set into train, dev, and test sets for evaluation purposes using an 80:10:10 split (Div"
2021.semeval-1.24,S19-2126,0,0.0345786,"Missing"
2021.semeval-1.24,P95-1026,0,0.844892,"nging mainly due to the following reasons: a) small size of the dataset b) characteristics of text samples extracted from social media leading to difficulties such as out-of-vocabulary words and ungrammatical sentences c) class imbalance in the dataset d) inconsistencies in data annotations. We approached this task as a subtoken level sequence labeling task. Fine-tuned pretrained transformer language models (Qiu et al., 2020) are the backbone of all our approaches. We investigated two main techniques to enhance the results of the fine-tuned transformer models, namely Semi-Supervised Learning (Yarowsky, 1995; Liu et al., 2011) and fine-tuning with Self-Adjusting Dice Loss (Li et al., 2020). This paper reports the results of our experiments with these different techniques and pre-trained transformer models. Our submitted system consisted of an ensemble of different pre-trained transformer models and achieved an F1 score of 0.6895 on the test set and secured 9th position on the task leaderboard. All of our code is made publicly available on Github1 . The rest of this paper is organized as follows. Section 2 discusses the previous works in the fields of offensive language detection and span identifi"
2021.semeval-1.24,S19-2010,0,0.0255466,"Linguistics 2 Related Work As the task involves detecting toxic spans in a text, we present the related work in two parts: (i) Offensive Language Detection and (ii) Span Identification. Offensive Language Detection: Research work has been done on different abusive and offensive language identification problems, ranging from aggression (Kumar et al., 2018) to hate speech (Davidson et al., 2017), toxic comments (Saif et al., 2018), and offensive language (Laud et al., 2020; Pitsilis et al., 2018). Recent contributions to offensive language detection came from the SemEval-2019 Task 6 OffensEval (Zampieri et al., 2019). The task organizers concluded that most top-performing teams either used BERT (Liu et al., 2019) or an ensemble model to achieve SOTA results. Interestingly, the task of locating toxic spans is relatively novel, and its successful completion can be groundbreaking. A recent approach with a narrower scope is by Mathew et al. (2020), who focused on the rationality of decision in the task of hate speech detection. Span Identification: Span detection/identification tasks include numerous tasks like named entity recognition (NER) (Nadeau and Sekine, 2007), chunking (Sang and Buchholz, 2000) and ke"
2021.semeval-1.36,2020.acl-main.372,0,0.0934085,"al., 2017) in humor detection and outperformed the state of the art models on multiple datasets. Ismailov (2019); Annamoradnejad (2020) extended the use of BERT models to humor classification. Fles, can-Lovin-Arseni et al. (2017) did humor classification by comparing and ranking tweets while Docekal et al. (2020) edit the tweet and rank the extent of humor for the edited tweet on a scale of 0 to 3 (most funny). There has been extensive research in the area of text emotion prediction and generation (e.g., Witon et al. (2018); Colombo et al. (2019); Goswamy et al. (2020); Singh et al. (2021)). Demszky et al. (2020) curated a large scale emotion detection dataset and achieved SOTA results by finetuning a BERT model. However, none of these works delve into humor analysis’ subjectivity, which is a prime focus of this task. Sentiment and Pun Analysis Li et al. (2019); Maltoudoglou et al. (2020) study BERT based models for sentiment analysis. Ke et al. (2019) uses a combination of sentence embedding, POS tagging and word-level sentiment polarity scores for sentiment classification. Zhou et al. (2020) uses contextualized and pronunciation embeddings for each word and pass these through a neural network to det"
2021.semeval-1.36,S17-2068,0,0.0662852,"Missing"
2021.semeval-1.36,2020.coling-main.251,1,0.734366,"proposed the use of transformers (Vaswani et al., 2017) in humor detection and outperformed the state of the art models on multiple datasets. Ismailov (2019); Annamoradnejad (2020) extended the use of BERT models to humor classification. Fles, can-Lovin-Arseni et al. (2017) did humor classification by comparing and ranking tweets while Docekal et al. (2020) edit the tweet and rank the extent of humor for the edited tweet on a scale of 0 to 3 (most funny). There has been extensive research in the area of text emotion prediction and generation (e.g., Witon et al. (2018); Colombo et al. (2019); Goswamy et al. (2020); Singh et al. (2021)). Demszky et al. (2020) curated a large scale emotion detection dataset and achieved SOTA results by finetuning a BERT model. However, none of these works delve into humor analysis’ subjectivity, which is a prime focus of this task. Sentiment and Pun Analysis Li et al. (2019); Maltoudoglou et al. (2020) study BERT based models for sentiment analysis. Ke et al. (2019) uses a combination of sentence embedding, POS tagging and word-level sentiment polarity scores for sentiment classification. Zhou et al. (2020) uses contextualized and pronunciation embeddings for each word a"
2021.semeval-1.36,N19-1374,1,0.897913,"Missing"
2021.semeval-1.36,P19-4007,0,0.0538708,"Missing"
2021.semeval-1.36,P18-1031,0,0.0184054,"cus of this task. Sentiment and Pun Analysis Li et al. (2019); Maltoudoglou et al. (2020) study BERT based models for sentiment analysis. Ke et al. (2019) uses a combination of sentence embedding, POS tagging and word-level sentiment polarity scores for sentiment classification. Zhou et al. (2020) uses contextualized and pronunciation embeddings for each word and pass these through a neural network to detect and localize pun in the sentence. However, none of these works focus on the subjectivity of the underlying sentiment and pun in the text. 3 3.1 2.2 Related Works Transfer Learning ULMFiT (Howard and Ruder, 2018) used a novel neural network based method for transfer learning and achieved SOTA results on a small dataset. Devlin et al. (2018) introduced BERT to learn latent representations in an unsupervised manner, which can then be finetuned on downstream tasks to achieve SOTA results. Lan et al. (2019); Liu et al. (2019); Sanh et al. (2019); Sun et al. (2019) have proposed several improvements to the BERT model. In this paper, we analyze the effects of using these different base models in the context of humor and offense detection. 1 https://github.com/aishgupta/ Quantifying-Humor-Offensiveness Syste"
2021.semeval-1.36,2020.acl-main.703,0,0.0332067,"Missing"
2021.semeval-1.36,D19-5505,0,0.0540038,"Missing"
2021.semeval-1.36,2021.ccl-1.108,0,0.0677239,"Missing"
2021.semeval-1.36,2021.semeval-1.9,0,0.0265159,"96 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics and experiments via GitHub1 We organize the rest of the paper as: we begin with a description of the challenge tasks followed by a brief literature survey in section 2. We then describe all of our proposed models in section 3 with training details in section 4 and present the experimental results in section 5. Finally, we analyze our findings and conclude in section 6, and 7 respectively. 2 Background 2.1 Problem Description SemEval 2021 Task 7: HaHackathon: Detecting and Rating Humor and Offense (Meaney et al., 2021) involves two main tasks – humor detection and offense detection. The organizers further subdivide the task into following subtasks: 1. Humor detection tasks: (a) Task 1a involves predicting whether a given text is humorous. (b) Task 1b requires predicting the humor rating of a given humorous text. (c) Task 1c incorporates humor subjectivity by posing a classification problem of predicting whether the underlying humor is controversial or not. 2. Task 2 is an offense detection task and is posed as a bounded regression problem. Given a text, we need to predict a mean score denoting the text’s of"
2021.semeval-1.36,2021.wassa-1.9,1,0.626952,"ansformers (Vaswani et al., 2017) in humor detection and outperformed the state of the art models on multiple datasets. Ismailov (2019); Annamoradnejad (2020) extended the use of BERT models to humor classification. Fles, can-Lovin-Arseni et al. (2017) did humor classification by comparing and ranking tweets while Docekal et al. (2020) edit the tweet and rank the extent of humor for the edited tweet on a scale of 0 to 3 (most funny). There has been extensive research in the area of text emotion prediction and generation (e.g., Witon et al. (2018); Colombo et al. (2019); Goswamy et al. (2020); Singh et al. (2021)). Demszky et al. (2020) curated a large scale emotion detection dataset and achieved SOTA results by finetuning a BERT model. However, none of these works delve into humor analysis’ subjectivity, which is a prime focus of this task. Sentiment and Pun Analysis Li et al. (2019); Maltoudoglou et al. (2020) study BERT based models for sentiment analysis. Ke et al. (2019) uses a combination of sentence embedding, POS tagging and word-level sentiment polarity scores for sentiment classification. Zhou et al. (2020) uses contextualized and pronunciation embeddings for each word and pass these through"
2021.semeval-1.36,D19-1372,0,0.0279606,"ivide the task into following subtasks: 1. Humor detection tasks: (a) Task 1a involves predicting whether a given text is humorous. (b) Task 1b requires predicting the humor rating of a given humorous text. (c) Task 1c incorporates humor subjectivity by posing a classification problem of predicting whether the underlying humor is controversial or not. 2. Task 2 is an offense detection task and is posed as a bounded regression problem. Given a text, we need to predict a mean score denoting the text’s offensiveness on a scale of 0 to 5, with 5 being the most offensive. Humor & Emotion Detection Weller and Seppi (2019) first proposed the use of transformers (Vaswani et al., 2017) in humor detection and outperformed the state of the art models on multiple datasets. Ismailov (2019); Annamoradnejad (2020) extended the use of BERT models to humor classification. Fles, can-Lovin-Arseni et al. (2017) did humor classification by comparing and ranking tweets while Docekal et al. (2020) edit the tweet and rank the extent of humor for the edited tweet on a scale of 0 to 3 (most funny). There has been extensive research in the area of text emotion prediction and generation (e.g., Witon et al. (2018); Colombo et al. (2"
2021.semeval-1.36,W18-6236,1,0.83722,"tion Detection Weller and Seppi (2019) first proposed the use of transformers (Vaswani et al., 2017) in humor detection and outperformed the state of the art models on multiple datasets. Ismailov (2019); Annamoradnejad (2020) extended the use of BERT models to humor classification. Fles, can-Lovin-Arseni et al. (2017) did humor classification by comparing and ranking tweets while Docekal et al. (2020) edit the tweet and rank the extent of humor for the edited tweet on a scale of 0 to 3 (most funny). There has been extensive research in the area of text emotion prediction and generation (e.g., Witon et al. (2018); Colombo et al. (2019); Goswamy et al. (2020); Singh et al. (2021)). Demszky et al. (2020) curated a large scale emotion detection dataset and achieved SOTA results by finetuning a BERT model. However, none of these works delve into humor analysis’ subjectivity, which is a prime focus of this task. Sentiment and Pun Analysis Li et al. (2019); Maltoudoglou et al. (2020) study BERT based models for sentiment analysis. Ke et al. (2019) uses a combination of sentence embedding, POS tagging and word-level sentiment polarity scores for sentiment classification. Zhou et al. (2020) uses contextualize"
2021.semeval-1.40,2020.findings-emnlp.27,0,0.0618053,"Missing"
2021.semeval-1.40,2020.acl-main.398,0,0.127072,"Missing"
2021.semeval-1.40,2021.ccl-1.108,0,0.0613452,"Missing"
2021.semeval-1.40,P15-1142,0,0.0590976,"t al., 2005). Verification under structured and semi-structured Evidence, such as tables, graphs, and databases, remains unexplored. Tables are ubiquitous in documents and presentations for concisely conveying important information; however, Inference on structured data like tables or graphs is much more difficult than simple text format due to complex structure and non-universal schema for the representation of data. Though recently, there has been work on Tabular Inference problems ( Zhong et al., 2020; Cho et al., 2018; Sun et al., 2018; Wenhu Chen and Wang, 2020; Eisenschlos et al., 2020; Pasupat and Liang, 2015; Wang et al., 2018 ) explaining the prediction, evidence finding is still an unexplored area. Through the SemEval-2021 Task 9 (Wang et al., 2021) we have tried to solve the Tabular Inference problem over scientific tables by providing an answer as well as a solution to our reasoning. In other words, given the structured table data and statement, we aim to classify the statement as entailed, unknown (neutral), or contradiction. In addition, we also aim to classify each cell of the table whether it is relevant or irrelevant in making the aforementioned prediction. Our contribution is three-fold"
2021.semeval-1.40,D15-1075,0,0.0307997,"subtasks. Given a table and a statement/fact, subtask A determines whether the statement is inferred from the tabular data, and subtask B determines which cells in the table provide evidence for the former subtask. We make a comparison of the baselines and state-of-the-art approaches over the given SemTabFact dataset. We also propose a novel approach CellBERT to solve evidence finding as a form of the Natural Language Inference task. We obtain a 3way F1 score of 0.69 on subtask A and an F1 score of 0.65 on subtask B. 1 Introduction Textual Inference, also known as natural language inference (Bowman et al., 2015), plays an important role in the study of natural language understanding and semantic representation. Due to the unprecedented amount of information generated over the internet, it becomes essential for machines to comprehend new information based on previous knowledge. Recent social events like political elections and pandemic spread have also shown the need for intelligent fact-checking systems that majorly depends on textual Inference over the scientific data. Though Textual Inference is well explored, the current works mainly deal with unstructured Evidence in the form of sentences (Dagan"
2021.semeval-1.40,C18-1165,0,0.021965,"n under structured and semi-structured Evidence, such as tables, graphs, and databases, remains unexplored. Tables are ubiquitous in documents and presentations for concisely conveying important information; however, Inference on structured data like tables or graphs is much more difficult than simple text format due to complex structure and non-universal schema for the representation of data. Though recently, there has been work on Tabular Inference problems ( Zhong et al., 2020; Cho et al., 2018; Sun et al., 2018; Wenhu Chen and Wang, 2020; Eisenschlos et al., 2020; Pasupat and Liang, 2015; Wang et al., 2018 ) explaining the prediction, evidence finding is still an unexplored area. Through the SemEval-2021 Task 9 (Wang et al., 2021) we have tried to solve the Tabular Inference problem over scientific tables by providing an answer as well as a solution to our reasoning. In other words, given the structured table data and statement, we aim to classify the statement as entailed, unknown (neutral), or contradiction. In addition, we also aim to classify each cell of the table whether it is relevant or irrelevant in making the aforementioned prediction. Our contribution is three-fold: • We perform an e"
2021.semeval-1.40,2021.semeval-1.39,0,0.117577,"ous in documents and presentations for concisely conveying important information; however, Inference on structured data like tables or graphs is much more difficult than simple text format due to complex structure and non-universal schema for the representation of data. Though recently, there has been work on Tabular Inference problems ( Zhong et al., 2020; Cho et al., 2018; Sun et al., 2018; Wenhu Chen and Wang, 2020; Eisenschlos et al., 2020; Pasupat and Liang, 2015; Wang et al., 2018 ) explaining the prediction, evidence finding is still an unexplored area. Through the SemEval-2021 Task 9 (Wang et al., 2021) we have tried to solve the Tabular Inference problem over scientific tables by providing an answer as well as a solution to our reasoning. In other words, given the structured table data and statement, we aim to classify the statement as entailed, unknown (neutral), or contradiction. In addition, we also aim to classify each cell of the table whether it is relevant or irrelevant in making the aforementioned prediction. Our contribution is three-fold: • We perform an empirical study of current state-of-the-art models on the SemTabFact dataset for the task of statement verification (see Section"
2021.semeval-1.40,2020.acl-main.539,0,0.129732,"is well explored, the current works mainly deal with unstructured Evidence in the form of sentences (Dagan et al., 2005). Verification under structured and semi-structured Evidence, such as tables, graphs, and databases, remains unexplored. Tables are ubiquitous in documents and presentations for concisely conveying important information; however, Inference on structured data like tables or graphs is much more difficult than simple text format due to complex structure and non-universal schema for the representation of data. Though recently, there has been work on Tabular Inference problems ( Zhong et al., 2020; Cho et al., 2018; Sun et al., 2018; Wenhu Chen and Wang, 2020; Eisenschlos et al., 2020; Pasupat and Liang, 2015; Wang et al., 2018 ) explaining the prediction, evidence finding is still an unexplored area. Through the SemEval-2021 Task 9 (Wang et al., 2021) we have tried to solve the Tabular Inference problem over scientific tables by providing an answer as well as a solution to our reasoning. In other words, given the structured table data and statement, we aim to classify the statement as entailed, unknown (neutral), or contradiction. In addition, we also aim to classify each cell of the"
2021.semeval-1.53,2020.coling-main.603,0,0.0416332,"possible way to solve this problem is to train a machine learning model using an annotated ∗ Authors contributed equally to the work. Names in alphabetical order. source dataset to assist the annotation process over some unlabelled target dataset. However, there may be differences in the source and target domain distributions, which may lead to inaccuracies. Thus the challenge is to update the weights of the source classifier to generalize it well on the target domain. This aligns with the well studied problem of Unsupervised Domain Adaptation (UDA) (Kouw and Loog, 2019; Wang and Deng, 2018; Ramponi and Plank, 2020) A common denominator across many popular UDA methods is their dependence on large amounts of labelled source domain data (Ganin and Lempitsky, 2015; Saito et al., 2018). However, many a times it is not possible to release the source domain dataset because of privacy concerns. This problem becomes particularly relevant when working with clinical Natural Language Processing (NLP) datasets because they contain highly sensitive information which cannot be freely distributed. To tackle these data sharing constraints, the framework of Source-Free Domain Adaptation (SFDA) is gaining interest (Laparr"
2021.semeval-1.57,P15-1034,0,0.0133864,"ts (IU) prediction and triplet formation. The information unit prediction and triplet formation have been approached in the literature mainly using rule-based methods. Rusu et al. (2007) suggests using syntactic parsers for generating parse trees, followed by triplet extraction using parser dependent techniques. Jivani et al. (2011) proposed an algorithm that exhibits the relationship between subject and object in a sentence using Stanford parser. This rule-based algorithm can form multiple triplets from a sentence as compared to Rusu et al. (2007). Stanford OpenIE Relation triplet formation (Angeli et al., 2015) uses a classifier, which learns to extract selfcontained clauses from longer sentences to form the final triplets using heuristics. Hamoudi (2016) and Jaiswal and George (2015) are also rule-based methods for triplet formation using Stanford dependency parser and constituency parser, respectively. KG-Bert (Yao et al., 2019) uses the BERT language model and utilize entity and relation descriptions of a triplet to compute its scoring function. 3 System Overview The proposed system is shown in Figure 2 depicting the entire pipeline and its respective model. 469 Binary Classifier Linear Layer BiL"
2021.semeval-1.57,D19-1371,0,0.0623621,"Missing"
2021.semeval-1.57,P16-1046,0,0.0365816,"earch articles (Xu et al., 2020). Typically, most knowledge graphs are created with rule-based approaches, hence, limiting their performance and generalization. However, some recent approach such as Sang et al. (2018); Wang et al. (2020b) uses neural approach in biomedical literature. To the best of our knowledge, there is no available contributions-focused knowledge graph over NLP literature using the neural approach. Sub-task A: The sub-task of extracting contribution sentences can also be posed as an extractive summarization problem (Nallapati et al., 2016; Narayan et al., 2018; Liu, 2019; Cheng and Lapata, 2016; Zhou et al., 2018; Dong et al., 2018; Wang et al., 2020a). BERTSUM (Liu and Lapata, 2019) and MATCHSUM (Zhong et al., 2020) are the recent methods leveraging language models and uses ROUGE-1, ROUGE-2 and ROUGE-L scores (Lin, 2004) on DailyMail data-set (Hermann et al., 2015). However, this extractive summarization technique may not be applicable in our case due to a number of reasons. Firstly, extractive summarization alone will not give all the contribution sentences because some sentences may not be relevant to the summarization task. Secondly, extractive summarization models are not teste"
2021.semeval-1.57,D18-1409,0,0.0254929,"Missing"
2021.semeval-1.57,2021.semeval-1.44,0,0.0744368,"Missing"
2021.semeval-1.57,W03-1028,0,0.348092,"consider documents up to a length of 4096 tokens, however, in our case, documents have on an average ∼10,000 tokens. Some of the extractive summarization methods (Liu and Lapata, 2019; Miller, 2019) take the number of contribution sentences as a hyper-parameter, but in our case, this is a trainable parameter in our model. Sub-task B: Sub-task B closely resembles the phrase extraction problem and several neural methods (Zhu et al. (2020); Wang et al. (2016); Zhang et al. (2016), inter alia) and non-neural based methods (using n-grams and noun-phrases with certain Part-of-speech (POS) patterns (Hulth, 2003)) have been proposed. Gollapalli et al. (2017) have shown that CRF has the potential to improve the existing phrase extraction model. Alzaidy et al. (2019) jointly leverages CRF and BiLSTM to capture hidden semantics for phrase extraction. Zhu et al. (2020) extended the work of Alzaidy et al. (2019) with the idea of self-training and used word embeddings, POS embeddings, and dependency embeddings with a BILUO labelling scheme in the output. Our proposed model took inspiration from Zhu et al. (2020) and propose a SciBERT based model using CRF on top of BiLSTM layers using BILUO labelling scheme"
2021.semeval-1.57,W09-1119,0,0.0198815,"took inspiration from Zhu et al. (2020) and propose a SciBERT based model using CRF on top of BiLSTM layers using BILUO labelling scheme on tokens. Our model captures better semantics than the word embeddings based approach in Zhu et al. (2020) because of SciBERT, which is trained on the scientific corpus. Moreover, our model uses the WordPiece tokenizer and hence, robust to Out-of-Vocabulary (OOV) tokens. Sahrawat et al. (2020) used contextual embeddings to the BiLSTM and CRF model using BIO (B=start token of phrase, I=continuation tokens of phrase and O=Non-phrase tokens) labelling scheme. Ratinov and Roth (2009) discussed that the BILUO scheme is superior to the BIO scheme; hence we adopt the BILUO scheme for sub-task B. Recently, Lai et al. (2020) combined sequence labelling with joint learning inspired from self-distillation to boost model performance on unsupervised datasets. However, their model used BIO labelling scheme and gave a comparable or marginal improvement in a supervised setting. Sub-task C: The sub-task C can be divided into two parts - information units (IU) prediction and triplet formation. The information unit prediction and triplet formation have been approached in the literature"
2021.semeval-1.57,2020.coling-main.56,0,0.0286859,"ns. Our model captures better semantics than the word embeddings based approach in Zhu et al. (2020) because of SciBERT, which is trained on the scientific corpus. Moreover, our model uses the WordPiece tokenizer and hence, robust to Out-of-Vocabulary (OOV) tokens. Sahrawat et al. (2020) used contextual embeddings to the BiLSTM and CRF model using BIO (B=start token of phrase, I=continuation tokens of phrase and O=Non-phrase tokens) labelling scheme. Ratinov and Roth (2009) discussed that the BILUO scheme is superior to the BIO scheme; hence we adopt the BILUO scheme for sub-task B. Recently, Lai et al. (2020) combined sequence labelling with joint learning inspired from self-distillation to boost model performance on unsupervised datasets. However, their model used BIO labelling scheme and gave a comparable or marginal improvement in a supervised setting. Sub-task C: The sub-task C can be divided into two parts - information units (IU) prediction and triplet formation. The information unit prediction and triplet formation have been approached in the literature mainly using rule-based methods. Rusu et al. (2007) suggests using syntactic parsers for generating parse trees, followed by triplet extrac"
2021.semeval-1.57,N16-1030,0,0.0505865,"improve semantic information. Our proposed model stack BiLSTM layers on top of SciBERT, followed by CRF (see Figure 2). The word-level representation {x1 , x2 , ..., xN } from the input sentence passes through the SciBERT tokenizer. We used the representation of the first sub-token for every word as the input to the SciBERT. The tokenized input is passed into the SciBERT layer, followed by the BiLSTM layer. The final feature output is mapped to a hidden linear layer to get the score matrix Z, which is passed into the CRF layer for label prediction y. The CRF layer is the same as described in Lample et al. (2016). The output produced by the SciBERT + BiLSTM + Hidden Layer corresponds to a scoring matrix Z (n×l) where n denotes the number of words in the input sentence, and l is the number of labels (l=5). The score of an output sequence y using CRF is given by: Scr(s, y) = n X (Zi,yi + Tyi−1 ,yi ) (1) i=0 where Zi,j denotes the score of word wi with the jth label, Tyi−1 ,yi is the transition score from the label yi−1 to yi , y = {y1 , y2 , ...., yn } is the sequence of true labels and Scr(s, y) corresponds to output score for sentence s and true labels y. A softmax over all possible label sequences yi"
2021.semeval-1.57,W04-1013,0,0.0256846,"neural approach in biomedical literature. To the best of our knowledge, there is no available contributions-focused knowledge graph over NLP literature using the neural approach. Sub-task A: The sub-task of extracting contribution sentences can also be posed as an extractive summarization problem (Nallapati et al., 2016; Narayan et al., 2018; Liu, 2019; Cheng and Lapata, 2016; Zhou et al., 2018; Dong et al., 2018; Wang et al., 2020a). BERTSUM (Liu and Lapata, 2019) and MATCHSUM (Zhong et al., 2020) are the recent methods leveraging language models and uses ROUGE-1, ROUGE-2 and ROUGE-L scores (Lin, 2004) on DailyMail data-set (Hermann et al., 2015). However, this extractive summarization technique may not be applicable in our case due to a number of reasons. Firstly, extractive summarization alone will not give all the contribution sentences because some sentences may not be relevant to the summarization task. Secondly, extractive summarization models are not tested on large documents such as research articles due to the limitation of the input token length for transformer-based language models. Some long document transformer-based methods are proposed (e.g., Beltagy et al., 2020), and can co"
2021.semeval-1.57,D19-1387,0,0.0118018,"ed approaches, hence, limiting their performance and generalization. However, some recent approach such as Sang et al. (2018); Wang et al. (2020b) uses neural approach in biomedical literature. To the best of our knowledge, there is no available contributions-focused knowledge graph over NLP literature using the neural approach. Sub-task A: The sub-task of extracting contribution sentences can also be posed as an extractive summarization problem (Nallapati et al., 2016; Narayan et al., 2018; Liu, 2019; Cheng and Lapata, 2016; Zhou et al., 2018; Dong et al., 2018; Wang et al., 2020a). BERTSUM (Liu and Lapata, 2019) and MATCHSUM (Zhong et al., 2020) are the recent methods leveraging language models and uses ROUGE-1, ROUGE-2 and ROUGE-L scores (Lin, 2004) on DailyMail data-set (Hermann et al., 2015). However, this extractive summarization technique may not be applicable in our case due to a number of reasons. Firstly, extractive summarization alone will not give all the contribution sentences because some sentences may not be relevant to the summarization task. Secondly, extractive summarization models are not tested on large documents such as research articles due to the limitation of the input token len"
2021.semeval-1.57,2020.acl-main.553,0,0.0228931,"Missing"
2021.semeval-1.57,N18-1158,0,0.0209183,", only a handful are based on research articles (Xu et al., 2020). Typically, most knowledge graphs are created with rule-based approaches, hence, limiting their performance and generalization. However, some recent approach such as Sang et al. (2018); Wang et al. (2020b) uses neural approach in biomedical literature. To the best of our knowledge, there is no available contributions-focused knowledge graph over NLP literature using the neural approach. Sub-task A: The sub-task of extracting contribution sentences can also be posed as an extractive summarization problem (Nallapati et al., 2016; Narayan et al., 2018; Liu, 2019; Cheng and Lapata, 2016; Zhou et al., 2018; Dong et al., 2018; Wang et al., 2020a). BERTSUM (Liu and Lapata, 2019) and MATCHSUM (Zhong et al., 2020) are the recent methods leveraging language models and uses ROUGE-1, ROUGE-2 and ROUGE-L scores (Lin, 2004) on DailyMail data-set (Hermann et al., 2015). However, this extractive summarization technique may not be applicable in our case due to a number of reasons. Firstly, extractive summarization alone will not give all the contribution sentences because some sentences may not be relevant to the summarization task. Secondly, extractive"
2021.semeval-1.57,2021.naacl-demos.8,0,0.0789119,"Missing"
2021.semeval-1.57,D16-1080,0,0.0126839,"th for transformer-based language models. Some long document transformer-based methods are proposed (e.g., Beltagy et al., 2020), and can consider documents up to a length of 4096 tokens, however, in our case, documents have on an average ∼10,000 tokens. Some of the extractive summarization methods (Liu and Lapata, 2019; Miller, 2019) take the number of contribution sentences as a hyper-parameter, but in our case, this is a trainable parameter in our model. Sub-task B: Sub-task B closely resembles the phrase extraction problem and several neural methods (Zhu et al. (2020); Wang et al. (2016); Zhang et al. (2016), inter alia) and non-neural based methods (using n-grams and noun-phrases with certain Part-of-speech (POS) patterns (Hulth, 2003)) have been proposed. Gollapalli et al. (2017) have shown that CRF has the potential to improve the existing phrase extraction model. Alzaidy et al. (2019) jointly leverages CRF and BiLSTM to capture hidden semantics for phrase extraction. Zhu et al. (2020) extended the work of Alzaidy et al. (2019) with the idea of self-training and used word embeddings, POS embeddings, and dependency embeddings with a BILUO labelling scheme in the output. Our proposed model took"
2021.semeval-1.57,2020.acl-main.552,0,0.0227657,"performance and generalization. However, some recent approach such as Sang et al. (2018); Wang et al. (2020b) uses neural approach in biomedical literature. To the best of our knowledge, there is no available contributions-focused knowledge graph over NLP literature using the neural approach. Sub-task A: The sub-task of extracting contribution sentences can also be posed as an extractive summarization problem (Nallapati et al., 2016; Narayan et al., 2018; Liu, 2019; Cheng and Lapata, 2016; Zhou et al., 2018; Dong et al., 2018; Wang et al., 2020a). BERTSUM (Liu and Lapata, 2019) and MATCHSUM (Zhong et al., 2020) are the recent methods leveraging language models and uses ROUGE-1, ROUGE-2 and ROUGE-L scores (Lin, 2004) on DailyMail data-set (Hermann et al., 2015). However, this extractive summarization technique may not be applicable in our case due to a number of reasons. Firstly, extractive summarization alone will not give all the contribution sentences because some sentences may not be relevant to the summarization task. Secondly, extractive summarization models are not tested on large documents such as research articles due to the limitation of the input token length for transformer-based language"
2021.semeval-1.57,P18-1061,0,0.020108,"., 2020). Typically, most knowledge graphs are created with rule-based approaches, hence, limiting their performance and generalization. However, some recent approach such as Sang et al. (2018); Wang et al. (2020b) uses neural approach in biomedical literature. To the best of our knowledge, there is no available contributions-focused knowledge graph over NLP literature using the neural approach. Sub-task A: The sub-task of extracting contribution sentences can also be posed as an extractive summarization problem (Nallapati et al., 2016; Narayan et al., 2018; Liu, 2019; Cheng and Lapata, 2016; Zhou et al., 2018; Dong et al., 2018; Wang et al., 2020a). BERTSUM (Liu and Lapata, 2019) and MATCHSUM (Zhong et al., 2020) are the recent methods leveraging language models and uses ROUGE-1, ROUGE-2 and ROUGE-L scores (Lin, 2004) on DailyMail data-set (Hermann et al., 2015). However, this extractive summarization technique may not be applicable in our case due to a number of reasons. Firstly, extractive summarization alone will not give all the contribution sentences because some sentences may not be relevant to the summarization task. Secondly, extractive summarization models are not tested on large document"
2021.semeval-1.62,P19-4007,0,0.0351182,"Missing"
2021.semeval-1.62,D19-1355,0,0.0136713,"asks differ only in the language pairs, some general approaches apply to both settings. We finally submit an ensemble of models in all the tasks. The ensembling was done by taking average of the probability scores of the models (Probability Sum Ensemble). 4.1 Task Agnostic Proposals Signals: We use a data preprocessing step of applying a signal to indicate the word to be disambiguated. This can be done in two ways - (i) Signal 1: encoding the target word (the word to be disambiguated) in both the sentences of a pair within double quotes (e.g., Click the right “ mouse ” button) as suggested by Huang et al. (2019), or (ii) Signal 2: append the target word at the end of the second sentence, similar to what was done by Wang et al. (2019). Note, for the former method; we need the character spans of the target word to apply double quotes at the correct position. Sentence Reversal Augmentation: For the models proposed in this task, the sentence pair is fed to the model in a manner such that the results do depend on which order the sentences are fed (i.e. the network parameters are not symmetric with respect to the two sentences). In such a case we propose the following augmentation - for every data point (s"
2021.semeval-1.62,P19-1568,0,0.015731,"ents - (i) EN-EN (train and dev data available), (ii) Non-English Multilingual (only dev data available), and (iii) Cross-Lingual (neither train nor dev data available). Our models and implementations are available here1 . Figure 1: An demonstrative example for the EnglishFrench Cross-lingual dataset. This pair will be classified as a ’False’ pair. 2 Related Work Word Sense Disambiguation: The techniques for the WSD task are broadly divided into knowledge-based and supervised approaches. The supervised approaches include fine-tuning BERT for sequence classification (Wang et al., 2019), EWISE (Kumar et al., 2019), and BiLSTM with attention (Raganato et al., 2017b). The knowledgebased methods use the information present in sense inventories such as WordNet (Fellbaum, 2012), BabelNet (Navigli and Ponzetto, 2012) and Wikipedia to derive semantic knowledge, assisting in the task of WSD. These include building sense embeddings for each sense of the word and disambiguate the target word using the nearest neighbour sense embedding: SensEmBERT (Scarlini et al., 2020), (Loureiro and Jorge, 2019), or augmenting the pretraining objecting of BERT to take into account the sense information available in the WordNet"
2021.semeval-1.62,2021.ccl-1.108,0,0.0401754,"Missing"
2021.semeval-1.62,P19-1569,0,0.100147,"sed approaches. The supervised approaches include fine-tuning BERT for sequence classification (Wang et al., 2019), EWISE (Kumar et al., 2019), and BiLSTM with attention (Raganato et al., 2017b). The knowledgebased methods use the information present in sense inventories such as WordNet (Fellbaum, 2012), BabelNet (Navigli and Ponzetto, 2012) and Wikipedia to derive semantic knowledge, assisting in the task of WSD. These include building sense embeddings for each sense of the word and disambiguate the target word using the nearest neighbour sense embedding: SensEmBERT (Scarlini et al., 2020), (Loureiro and Jorge, 2019), or augmenting the pretraining objecting of BERT to take into account the sense information available in the WordNet: SenseBERT (Levine et al., 2019). There are two existing benchmarks to evaluate the performance of WSD systems. One method is linked to the sense inventories, and the task is framed as a multi-class classification among the senses of a word listed in the inventory (Raganato et al., 2017a). The other is the WiC framework, not tied to any sense inventory, and asks if a target word has the same sense or not in the two given sentences. 1 https://github.com/dipakamiitk/Crosslingual-"
2021.semeval-1.62,2021.semeval-1.3,0,0.0289823,"a target word from a list of senses listed in a sense inventory like WordNet (Fellbaum, 2012). Pilehvar and Camacho-Collados ∗ Authors equally contributed to this work. (2018) proposed a novel benchmark (WiC - Word in Context Disambiguation) for the task casting the problem as a binary classification task, wherein it has to be identified whether a word common to a sentence pair is used in the same sense or not. The WiC task frees up the word sense disambiguation task from being tied to any sense inventory. The SemEval 2021 Task 2: Multilingual and Cross-lingual Word-in-Context Disambiguation (Martelli et al., 2021) extends the WiC framework proposed by Pilehvar and Camacho-Collados (2018) to more languages. The task is divided into two subtasks - the Multilingual task and the CrossLingual task. The sentence pair, with a word in common, which is to be disambiguated, is drawn from the same language in the MultiLingual task, whereas the pair is drawn from two different languages in the Cross-Lingual Task. The task is posed as binary classification task over a pair of sentence wide contexts sent1 and sent2, containing word sequences w1 and w2 respectively. The word sequences, w1 and w2 , have a common word"
2021.semeval-1.62,H93-1061,0,0.712807,"data was collected automatically from WordNet and Wiktionaries of various 512 languages. We are only interested in a few languages, from the WordNet development sets, the ones with good human performance, because we often note that this data does not accurately represent human distinguishable senses, which our manually collected task data set does. Specifically, we use Chinese(ZH), Danish(DA), Croatian(HR), and Dutch(NL). Farsi(FA) also has good human performance but we could not include it due to a pre-processing error. AuSemCor: We created our own augmented dataset AuSemCor from the SemCor (Miller et al., 1993) dataset, which is a sense annotated corpora in English, with senses tagged using WordNet as its sense inventory. To generate data points for the same sense (T class), we pair up sentences containing a common lemma, whose WordNet senses are identical. For the other class (F class), we pair up sentences with a common lemma, but this lemma has different WordNet sense across the sentence pair. In addition, for the F class, we make sure that the WordNet supersense is also different for creating coarser sense distinctions as suggested by Pilehvar and Camacho-Collados (2018). We obtain 4986 datapoin"
2021.semeval-1.62,E17-1010,0,0.0194853,"(ii) Non-English Multilingual (only dev data available), and (iii) Cross-Lingual (neither train nor dev data available). Our models and implementations are available here1 . Figure 1: An demonstrative example for the EnglishFrench Cross-lingual dataset. This pair will be classified as a ’False’ pair. 2 Related Work Word Sense Disambiguation: The techniques for the WSD task are broadly divided into knowledge-based and supervised approaches. The supervised approaches include fine-tuning BERT for sequence classification (Wang et al., 2019), EWISE (Kumar et al., 2019), and BiLSTM with attention (Raganato et al., 2017b). The knowledgebased methods use the information present in sense inventories such as WordNet (Fellbaum, 2012), BabelNet (Navigli and Ponzetto, 2012) and Wikipedia to derive semantic knowledge, assisting in the task of WSD. These include building sense embeddings for each sense of the word and disambiguate the target word using the nearest neighbour sense embedding: SensEmBERT (Scarlini et al., 2020), (Loureiro and Jorge, 2019), or augmenting the pretraining objecting of BERT to take into account the sense information available in the WordNet: SenseBERT (Levine et al., 2019). There are two e"
2021.semeval-1.62,D17-1120,0,0.0156365,"(ii) Non-English Multilingual (only dev data available), and (iii) Cross-Lingual (neither train nor dev data available). Our models and implementations are available here1 . Figure 1: An demonstrative example for the EnglishFrench Cross-lingual dataset. This pair will be classified as a ’False’ pair. 2 Related Work Word Sense Disambiguation: The techniques for the WSD task are broadly divided into knowledge-based and supervised approaches. The supervised approaches include fine-tuning BERT for sequence classification (Wang et al., 2019), EWISE (Kumar et al., 2019), and BiLSTM with attention (Raganato et al., 2017b). The knowledgebased methods use the information present in sense inventories such as WordNet (Fellbaum, 2012), BabelNet (Navigli and Ponzetto, 2012) and Wikipedia to derive semantic knowledge, assisting in the task of WSD. These include building sense embeddings for each sense of the word and disambiguate the target word using the nearest neighbour sense embedding: SensEmBERT (Scarlini et al., 2020), (Loureiro and Jorge, 2019), or augmenting the pretraining objecting of BERT to take into account the sense information available in the WordNet: SenseBERT (Levine et al., 2019). There are two e"
2021.semeval-1.66,S16-1160,0,0.0129499,"0.06 Dummy Annotation Generation 0 Figure 1: Solution Pipeline 20 40 Epochs 60 80 100 Figure 2: Convergence of losses for finetuning ELECTRA with weak supervision the SemEval 2016 Task 11 (Paetzold and Specia (2016a)). However, it was a binary classification task. Most of the participating systems used Support Vector Machines such as Kuru (2016) and Choubey and Pateria (2016), decision trees and random forests (Choubey and Pateria (2016), Brooke et al. (2016), Ronzano et al. (2016)), and even basic threshold based approaches (Kauchak (2016), Malmasi et al. (2016)). Very few of them, including Bingel et al. (2016) used neural networks. The system by Wr´obel (2016) achieved an F1 score very close to the winning solution using only single feature - word frequency from Wikipedia. Most of these systems use word embeddings, POS information and word frequencies as features. The winning system by Paetzold and Specia (2016b) however uses 69 morphological, semantic and syntactic features. Another related shared task was presented at the BEA workshop at 2018 (Yimam et al., 2018). It had a probabilistic task as well as a binary classification task. Even there, the organizers conclude that feature engineering has"
2021.semeval-1.66,2020.coling-main.59,0,0.0205713,"rove to be much better. The model architectures that were tried out in earlier stages showed similar trends. For example, ELECTRA finetuning produced much better scores than BERT finetuning. Also, simpler models like a simple linear regression on GloVe embeddings showed promise, proving that simpler models with lesser parameters worked better. All these trends across those models are visually shown in Figure 4. It was observed that the model was underperforming on the tuples from Biomed corpus. However the scores did not improve using BERT variants like BioBERT (Lee et al., 2019), BioMedBERT (Chakraborty et al., 2020) and a few other transformer based models pretrained on biomedical texts. A variant of ELECTRA on biomedical texts could have improve on this, however due to its unavailability it could not be tried out. In majority of the prior work on LCP, there is abundance use of word frequency as a feature. However, in this system the scores got worse when frequency features were used along with others in ensemble. And the feature in itself could not produce competitive results. Previously, Gong et al. (2020) and Mu et al. (2018) have shown that frequency information causes significant distortion in the e"
2021.semeval-1.66,S16-1156,0,0.0211856,"2021), pages 541–547 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Proposed Approach: Pipeline 13 0.12 Electra Glove Feature Extraction 0.11 Logit Loss {Sentence, Token} Classifier Ensemble 0.09 0.07 0.06 Dummy Annotation Generation 0 Figure 1: Solution Pipeline 20 40 Epochs 60 80 100 Figure 2: Convergence of losses for finetuning ELECTRA with weak supervision the SemEval 2016 Task 11 (Paetzold and Specia (2016a)). However, it was a binary classification task. Most of the participating systems used Support Vector Machines such as Kuru (2016) and Choubey and Pateria (2016), decision trees and random forests (Choubey and Pateria (2016), Brooke et al. (2016), Ronzano et al. (2016)), and even basic threshold based approaches (Kauchak (2016), Malmasi et al. (2016)). Very few of them, including Bingel et al. (2016) used neural networks. The system by Wr´obel (2016) achieved an F1 score very close to the winning solution using only single feature - word frequency from Wikipedia. Most of these systems use word embeddings, POS information and word frequencies as features. The winning system by Paetzold and Specia (2016b) however uses 69 morphological, semantic and synt"
2021.semeval-1.66,N19-1423,0,0.0337289,"and Kochmar (2018) uses feature engineering and later random forest and linear regression models. System Overview Our proposed pipeline can be divided into the following 4 main componentsa) Feature Extraction b) Regression Pipeline c) Classification Pipeline d) Ensemble The pipeline is shown in Figure 3. 3.1 0.10 0.08 Complexity Score 3 Train Loss Validation Loss 0.13 Regressor Feature Extraction ELECTRA is a transformer based model, that is trained like a discriminator and not like generator. And in our case, this model performed exceptionally well on the validation data as compared to BERT (Devlin et al., 2019). We extracted context-dependent features using embeddings generated from the ELECTRA model and captured context-independent word-level features using static 200-dimensional GloVe embeddings of the tokens. In order to generate the embeddings of the target word through ELECTRA, we implemented the KMP pattern matching algorithm (Wikipedia, 2021) to find the indices of the sub-tokens of the target token in the tokenized sentence. Subsequently, we calculated an average across these sub-token embeddings generated by ELECTRA. While using GloVe embeddings, in the case of multi-word expressions in Sub"
2021.semeval-1.66,W18-0520,0,0.0118482,"ed an F1 score very close to the winning solution using only single feature - word frequency from Wikipedia. Most of these systems use word embeddings, POS information and word frequencies as features. The winning system by Paetzold and Specia (2016b) however uses 69 morphological, semantic and syntactic features. Another related shared task was presented at the BEA workshop at 2018 (Yimam et al., 2018). It had a probabilistic task as well as a binary classification task. Even there, the organizers conclude that feature engineering has worked better than neural networks. The winning system by Gooding and Kochmar (2018) uses feature engineering and later random forest and linear regression models. System Overview Our proposed pipeline can be divided into the following 4 main componentsa) Feature Extraction b) Regression Pipeline c) Classification Pipeline d) Ensemble The pipeline is shown in Figure 3. 3.1 0.10 0.08 Complexity Score 3 Train Loss Validation Loss 0.13 Regressor Feature Extraction ELECTRA is a transformer based model, that is trained like a discriminator and not like generator. And in our case, this model performed exceptionally well on the validation data as compared to BERT (Devlin et al., 201"
2021.semeval-1.66,D19-1355,0,0.0266314,"pipeline, a pretrained ELECTRA model was finetuned with a linear layer on top of it. We leveraged the model directly available at the Huggingface library (Wolf et al., 2020). Only the last transformer layer of ELECTRA was kept trainable. The remaining ones were kept frozen. For Sub-task 2, a fixed ELECTRA model (nontrainable weights) was used to generate token embeddings and a linear regression model was trained with these extracted embeddings. Weak Supervision: In order to have higher attention on the target word, the use of weak supervi542 sion signals proved useful. Inspired by GlossBert (Huang et al., 2019), the target word was wrapped with single inverted commas (’ ’s) as a weak signal to the transformer (Vaswani et al., 2017) model. This technique significantly improved the results obtained using the regression pipeline in subtask I. However, the same technique applied to subtask II made the scores worse. Method Val MAE Test MAE + signal - signal 0.06516 0.06990 0.06800 0.07118 c = α ∗ low + (1 − α) ∗ high Table 1: Variation of MAE scores with and without the signalling technique for Sub-task 1: the single word task. (’+ signal’ means weak supervision has been used and ’- signal’ means otherwi"
2021.semeval-1.66,S16-1164,0,0.0256332,"ion 0.11 Logit Loss {Sentence, Token} Classifier Ensemble 0.09 0.07 0.06 Dummy Annotation Generation 0 Figure 1: Solution Pipeline 20 40 Epochs 60 80 100 Figure 2: Convergence of losses for finetuning ELECTRA with weak supervision the SemEval 2016 Task 11 (Paetzold and Specia (2016a)). However, it was a binary classification task. Most of the participating systems used Support Vector Machines such as Kuru (2016) and Choubey and Pateria (2016), decision trees and random forests (Choubey and Pateria (2016), Brooke et al. (2016), Ronzano et al. (2016)), and even basic threshold based approaches (Kauchak (2016), Malmasi et al. (2016)). Very few of them, including Bingel et al. (2016) used neural networks. The system by Wr´obel (2016) achieved an F1 score very close to the winning solution using only single feature - word frequency from Wikipedia. Most of these systems use word embeddings, POS information and word frequencies as features. The winning system by Paetzold and Specia (2016b) however uses 69 morphological, semantic and syntactic features. Another related shared task was presented at the BEA workshop at 2018 (Yimam et al., 2018). It had a probabilistic task as well as a binary classificati"
2021.semeval-1.66,S16-1163,0,0.0226649,"uation (SemEval-2021), pages 541–547 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Proposed Approach: Pipeline 13 0.12 Electra Glove Feature Extraction 0.11 Logit Loss {Sentence, Token} Classifier Ensemble 0.09 0.07 0.06 Dummy Annotation Generation 0 Figure 1: Solution Pipeline 20 40 Epochs 60 80 100 Figure 2: Convergence of losses for finetuning ELECTRA with weak supervision the SemEval 2016 Task 11 (Paetzold and Specia (2016a)). However, it was a binary classification task. Most of the participating systems used Support Vector Machines such as Kuru (2016) and Choubey and Pateria (2016), decision trees and random forests (Choubey and Pateria (2016), Brooke et al. (2016), Ronzano et al. (2016)), and even basic threshold based approaches (Kauchak (2016), Malmasi et al. (2016)). Very few of them, including Bingel et al. (2016) used neural networks. The system by Wr´obel (2016) achieved an F1 score very close to the winning solution using only single feature - word frequency from Wikipedia. Most of these systems use word embeddings, POS information and word frequencies as features. The winning system by Paetzold and Specia (2016b) however uses 69 m"
2021.semeval-1.66,S16-1154,0,0.0285483,"oss {Sentence, Token} Classifier Ensemble 0.09 0.07 0.06 Dummy Annotation Generation 0 Figure 1: Solution Pipeline 20 40 Epochs 60 80 100 Figure 2: Convergence of losses for finetuning ELECTRA with weak supervision the SemEval 2016 Task 11 (Paetzold and Specia (2016a)). However, it was a binary classification task. Most of the participating systems used Support Vector Machines such as Kuru (2016) and Choubey and Pateria (2016), decision trees and random forests (Choubey and Pateria (2016), Brooke et al. (2016), Ronzano et al. (2016)), and even basic threshold based approaches (Kauchak (2016), Malmasi et al. (2016)). Very few of them, including Bingel et al. (2016) used neural networks. The system by Wr´obel (2016) achieved an F1 score very close to the winning solution using only single feature - word frequency from Wikipedia. Most of these systems use word embeddings, POS information and word frequencies as features. The winning system by Paetzold and Specia (2016b) however uses 69 morphological, semantic and syntactic features. Another related shared task was presented at the BEA workshop at 2018 (Yimam et al., 2018). It had a probabilistic task as well as a binary classification task. Even there, th"
2021.semeval-1.66,S16-1149,0,0.0170266,"buted to this work. 1 https://github.com/neilrs123/Lexical-ComplexityPrediction 541 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 541–547 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Proposed Approach: Pipeline 13 0.12 Electra Glove Feature Extraction 0.11 Logit Loss {Sentence, Token} Classifier Ensemble 0.09 0.07 0.06 Dummy Annotation Generation 0 Figure 1: Solution Pipeline 20 40 Epochs 60 80 100 Figure 2: Convergence of losses for finetuning ELECTRA with weak supervision the SemEval 2016 Task 11 (Paetzold and Specia (2016a)). However, it was a binary classification task. Most of the participating systems used Support Vector Machines such as Kuru (2016) and Choubey and Pateria (2016), decision trees and random forests (Choubey and Pateria (2016), Brooke et al. (2016), Ronzano et al. (2016)), and even basic threshold based approaches (Kauchak (2016), Malmasi et al. (2016)). Very few of them, including Bingel et al. (2016) used neural networks. The system by Wr´obel (2016) achieved an F1 score very close to the winning solution using only single feature - word frequency from Wikipedia. Most of these systems use w"
2021.semeval-1.66,D14-1162,0,0.0907679,"Missing"
2021.semeval-1.66,S16-1157,0,0.0630016,"Missing"
2021.semeval-1.66,2020.readi-1.9,0,0.0822875,"then combined the two using ensemble methods. We used the ELECTRA (Clark et al., 2020) model for extracting contextdependent features and GloVe embeddings (Pennington et al., 2014) for representing the word-level features. Additionally, we propose a classification pipeline that is trained on GloVe embeddings of the tokens. This pipeline can be interpreted as a model for capturing different annotators’ thought processes: overconfidence, under-confidence and randomness. We are making our code available for our models and experiments via GitHub1 . 2 Background This task uses the CompLex dataset (Shardlow et al., 2020), which is a lexical complexity prediction dataset in English for single and multi word expressions (2-grams). Sentences in this task consists of sentences taken from 3 corpora- Bible, Biomed and Europarl. The train, validation and test split of the data was 9179, 520, 1103 respectively. We used the trial data as the validation set. The aim of the task is to predict how complex a given token in a given sentence is. More mathematically, given a tuple [s, t, c], where s = [t1 , t2 , ...tn ] and t = tj , we have to give an estimate of the function σ, such that σ(s, t) = c. (s is the sentence, t i"
2021.semeval-1.66,2021.semeval-1.1,0,0.0424543,"gital pedagogy, English has become an extremely popular language. Although English is considered an easy language to learn and grasp, a person’s choice of words often affects texts’ readability. The use of difficult words can potentially lead to a communication gap, thus hampering language efficiency. Keeping these issues in mind, many Natural Language Processing tasks for text simplification have been recently proposed (Paetzold and Specia, 2017; Sikka and Mago, 2020). Our task of lexical complexity prediction is an important step in the process of simplifying texts. The SemEval 2021 Task 1 (Shardlow et al., 2021) focuses on lexical complexity prediction in English. Given a sentence and a token from it, we have to predict the complexity score of the token. The task has two Sub-TasksSub-Task 1: complexity prediction of single words Sub-Task 2: complexity prediction of multi word expressions (MWEs). A word might seem complex because of 2 major ∗ factorsa) The word is less common or complex in itself. b) The context in which the word is used makes it hard to comprehend. Observing the orthogonality of these two reasons, we captured the context-dependent features and independent features separately, trained"
2021.semeval-1.66,S16-1146,0,0.057981,"Missing"
2021.semeval-1.66,W18-0507,0,0.0130605,"Ronzano et al. (2016)), and even basic threshold based approaches (Kauchak (2016), Malmasi et al. (2016)). Very few of them, including Bingel et al. (2016) used neural networks. The system by Wr´obel (2016) achieved an F1 score very close to the winning solution using only single feature - word frequency from Wikipedia. Most of these systems use word embeddings, POS information and word frequencies as features. The winning system by Paetzold and Specia (2016b) however uses 69 morphological, semantic and syntactic features. Another related shared task was presented at the BEA workshop at 2018 (Yimam et al., 2018). It had a probabilistic task as well as a binary classification task. Even there, the organizers conclude that feature engineering has worked better than neural networks. The winning system by Gooding and Kochmar (2018) uses feature engineering and later random forest and linear regression models. System Overview Our proposed pipeline can be divided into the following 4 main componentsa) Feature Extraction b) Regression Pipeline c) Classification Pipeline d) Ensemble The pipeline is shown in Figure 3. 3.1 0.10 0.08 Complexity Score 3 Train Loss Validation Loss 0.13 Regressor Feature Extractio"
2021.wassa-1.9,W10-0206,0,0.331954,"s segmented into an ordered set of clauses D = [c1 , c2 , ..., cd ] and the ECPE task aims to extract a set of emotion-cause pairs P = {..., (ci , cj ), ...} (ci , cj ∈ D), where ci is an emotion clause and cj is the corresponding cause clause. In the ECE task, we are additionally given the annotations of emotion clauses and the goal is to detect (one or more) clauses containing the cause for each emotion clause. Related Work The problem of Emotion Cause Extraction (ECE) has been studied extensively over the past decade. ECE was initially proposed as a word-level sequence detection problem in Lee et al. (2010). Attempts to solve this task focused on either classical machine learning techniques (Ghazi et al., 2015), or on rule-based methods (Neviarouskaya and Aono, 2013; Gao et al., 2015). Subsequently, the problem was reframed as a clause-level classification problem (Chen et al., 2010) and the Chinese-language dataset introduced by Gui et al. (2016) has since become the benchmark dataset for ECE and the task has been an active area of research (Xia et al., 2019; Gui et al., 2017; Yu et al., 2019; Li et al., 2018, 2019; Fan et al., 2019). However, the main limitation of ECE remains that it requires"
2021.wassa-1.9,D19-1563,0,0.569418,"tially proposed as a word-level sequence detection problem in Lee et al. (2010). Attempts to solve this task focused on either classical machine learning techniques (Ghazi et al., 2015), or on rule-based methods (Neviarouskaya and Aono, 2013; Gao et al., 2015). Subsequently, the problem was reframed as a clause-level classification problem (Chen et al., 2010) and the Chinese-language dataset introduced by Gui et al. (2016) has since become the benchmark dataset for ECE and the task has been an active area of research (Xia et al., 2019; Gui et al., 2017; Yu et al., 2019; Li et al., 2018, 2019; Fan et al., 2019). However, the main limitation of ECE remains that it requires emotion annotations even during test time, which severely limits the applicability of ECE models. To address this, Xia and Ding (2019) introduced a new task called emotion-cause pair extraction (ECPE), which extracts both emotion and its cause without requiring the emotion annotation. They demonstrated the results of their two-stage architecture on the benchmark Chinese language ECE corpus (Gui et al., 2016). Following their work, several works have been proposed to address the limitations of the two-stage architecture (Ding et al."
2021.wassa-1.9,2020.acl-main.342,0,0.0361331,"of ECE remains that it requires emotion annotations even during test time, which severely limits the applicability of ECE models. To address this, Xia and Ding (2019) introduced a new task called emotion-cause pair extraction (ECPE), which extracts both emotion and its cause without requiring the emotion annotation. They demonstrated the results of their two-stage architecture on the benchmark Chinese language ECE corpus (Gui et al., 2016). Following their work, several works have been proposed to address the limitations of the two-stage architecture (Ding et al. (2020a), Ding et al. (2020b), Fan et al. (2020), Yuan et al. (2020), Cheng et al. (2020), Chen et al. (2020)) . In order to explore the corpus further and to encourage future work from a broader commu4 Approach We propose an end-to-end emotion cause pairs extraction model (Figure 2), henceforth referred to as E2E-PExtE (refer to section 6 for the naming convention). The model takes an entire document as its input and computes, for each ordered pair of clauses (ci , cj ), the probability of being a potential emotion-cause pair. To facilitate the learning of suitable clause representations required for this primary task, we train the model o"
2021.wassa-1.9,D18-1506,0,0.0830153,"ast decade. ECE was initially proposed as a word-level sequence detection problem in Lee et al. (2010). Attempts to solve this task focused on either classical machine learning techniques (Ghazi et al., 2015), or on rule-based methods (Neviarouskaya and Aono, 2013; Gao et al., 2015). Subsequently, the problem was reframed as a clause-level classification problem (Chen et al., 2010) and the Chinese-language dataset introduced by Gui et al. (2016) has since become the benchmark dataset for ECE and the task has been an active area of research (Xia et al., 2019; Gui et al., 2017; Yu et al., 2019; Li et al., 2018, 2019; Fan et al., 2019). However, the main limitation of ECE remains that it requires emotion annotations even during test time, which severely limits the applicability of ECE models. To address this, Xia and Ding (2019) introduced a new task called emotion-cause pair extraction (ECPE), which extracts both emotion and its cause without requiring the emotion annotation. They demonstrated the results of their two-stage architecture on the benchmark Chinese language ECE corpus (Gui et al., 2016). Following their work, several works have been proposed to address the limitations of the two-stage"
2021.wassa-1.9,I13-1121,0,0.108813,", ...} (ci , cj ∈ D), where ci is an emotion clause and cj is the corresponding cause clause. In the ECE task, we are additionally given the annotations of emotion clauses and the goal is to detect (one or more) clauses containing the cause for each emotion clause. Related Work The problem of Emotion Cause Extraction (ECE) has been studied extensively over the past decade. ECE was initially proposed as a word-level sequence detection problem in Lee et al. (2010). Attempts to solve this task focused on either classical machine learning techniques (Ghazi et al., 2015), or on rule-based methods (Neviarouskaya and Aono, 2013; Gao et al., 2015). Subsequently, the problem was reframed as a clause-level classification problem (Chen et al., 2010) and the Chinese-language dataset introduced by Gui et al. (2016) has since become the benchmark dataset for ECE and the task has been an active area of research (Xia et al., 2019; Gui et al., 2017; Yu et al., 2019; Li et al., 2018, 2019; Fan et al., 2019). However, the main limitation of ECE remains that it requires emotion annotations even during test time, which severely limits the applicability of ECE models. To address this, Xia and Ding (2019) introduced a new task call"
2021.wassa-1.9,D14-1162,0,0.0876286,"Missing"
2021.wassa-1.9,N18-2074,0,0.0930951,"NTCIR-13 Workshop (Gao et al., 2017) for the ECE challenge. As observed by Xia and Ding (2019), we also noticed that performance on the two auxiliary tasks could be improved if done in an interactive manner rather than independently. Hence, the Cause-Encoder also makes use of the corresponding emotion-detection prediction yie , when generating rci (Figure 2). For the primary task, every ordered pair (ci , cj ) is represented by concatenating rei , rcj and peij , wherein peij is the positional embedding vector representing the relative positioning between the two clauses i, j in the document (Shaw et al., 2018). The primary task is solved by passing this pairrepresentation through a fully-connected neural network to get the pair-predictions. rpij = [rei ⊕ rcj ⊕ peij ]; hpij p y ˆij = ReLU(Wp1 ∗ = softmax(W p2 rpij ∗ The corpus consists of 2843 documents taken from several English novels. Each document is annotated with the following information: i) emotioncause pairs present in the document, that is, the set of emotion clauses and their corresponding cause clauses; ii) emotion category of each clause; and iii) the keyword within the clause denoting the labeled emotion. We do not use the emotion cate"
2021.wassa-1.9,W18-6236,1,0.844504,"Missing"
2021.wassa-1.9,P19-1096,0,0.532612,"hreessh, saim}@iitk.ac.in ashutoshm@cse.iitk.ac.in Abstract applicability of models solving the ECE problem is limited by the fact that emotion annotations are required at test time. More recently, Xia and Ding (2019) introduced the Emotion-Cause Pair Extraction (ECPE) task i.e. extracting all possible emotion-cause clause pairs in a document with no emotion annotations. Thus, ECPE opens up avenues for applications of real-time sentiment-cause analysis in tweets and product reviews. ECPE builds on the existing and well studied ECE task. Figure 1 shows an example with ground truth annotations. Xia and Ding (2019) use a two-stage architecture to extract potential emotion-cause clauses. In Stage 1, the model extracts a set of emotion clauses and a set of cause clauses (not mutually exclusive) from the document. In Stage 2, it performs emotioncause pairing and filtering, i.e. eliminating pairs that the model predicts as an invalid emotion-cause pair. However, this fails to fully capture the mutual dependence between emotion and cause clauses since clause extraction happens in isolation from the pairing step. Thus, the model is never optimized using the overall task as the objective. Also, certain emotion"
2021.wassa-1.9,2020.emnlp-main.289,0,0.0321908,"it requires emotion annotations even during test time, which severely limits the applicability of ECE models. To address this, Xia and Ding (2019) introduced a new task called emotion-cause pair extraction (ECPE), which extracts both emotion and its cause without requiring the emotion annotation. They demonstrated the results of their two-stage architecture on the benchmark Chinese language ECE corpus (Gui et al., 2016). Following their work, several works have been proposed to address the limitations of the two-stage architecture (Ding et al. (2020a), Ding et al. (2020b), Fan et al. (2020), Yuan et al. (2020), Cheng et al. (2020), Chen et al. (2020)) . In order to explore the corpus further and to encourage future work from a broader commu4 Approach We propose an end-to-end emotion cause pairs extraction model (Figure 2), henceforth referred to as E2E-PExtE (refer to section 6 for the naming convention). The model takes an entire document as its input and computes, for each ordered pair of clauses (ci , cj ), the probability of being a potential emotion-cause pair. To facilitate the learning of suitable clause representations required for this primary task, we train the model on two other auxiliar"
K16-1008,P08-1090,0,0.193918,"their immediate arguments (i.e. syntactic dependents of the predicate). These approaches model statistical dependencies between events (or, more formally, mentions of events) in a document, often restricting their model to capturing dependencies only between events sharing at least one entity (a common protagonist). We generally follow this tradition in our approach. Much of this previous work has focused on count-based techniques using, for example, either the generative framework (Frermann et al., 2014) or relying on information-theoretic measures such as pointwise mutual information (PMI) (Chambers and Jurafsky, 2008). Some of these techniques treat predicate-argument structures as an atomic whole (e.g., Pichotta and Mooney (2014)), in other words their probability estimates are based on cooccurrences of entire (predicate, arguments) tuples. Clearly such methods fail to adequately take into account compositional nature of expressions used to refer to events and suffer from data sparsity. In this work our goal is to overcome the shortcomings of the count-based methods described above by representing events as real-valued vectors (event embeddings), with the embeddings computed in a compositional way relying"
K16-1008,E14-1006,0,0.0171121,"2014; Rudinger et al., 2015a). Most of these methods represent events as verbal predicates along with tuples of their immediate arguments (i.e. syntactic dependents of the predicate). These approaches model statistical dependencies between events (or, more formally, mentions of events) in a document, often restricting their model to capturing dependencies only between events sharing at least one entity (a common protagonist). We generally follow this tradition in our approach. Much of this previous work has focused on count-based techniques using, for example, either the generative framework (Frermann et al., 2014) or relying on information-theoretic measures such as pointwise mutual information (PMI) (Chambers and Jurafsky, 2008). Some of these techniques treat predicate-argument structures as an atomic whole (e.g., Pichotta and Mooney (2014)), in other words their probability estimates are based on cooccurrences of entire (predicate, arguments) tuples. Clearly such methods fail to adequately take into account compositional nature of expressions used to refer to events and suffer from data sparsity. In this work our goal is to overcome the shortcomings of the count-based methods described above by repr"
K16-1008,W14-1606,1,0.727897,"re learned during training. We also experimented with different matrices T for subject and object positions, in order to take into account the positional information. Empirically, this had negligible effect on the final results. For event representation, one could use a sophisticated compositional model based on recursive neural networks (Socher et al., 2012) , we take a simpler approach and choose a feedforward network based compositional model as this is easier to train and more robust to choice of hyperparameters. Our event representations model is inspired from the event ordering model of Modi and Titov (2014). Their model uses distributed word representations for representing verb and argument lemmas constituting the event. Distributed word representations (also known as word embeddings) encode semantic and syntactic properties of a word in a vector of real values (Bengio et al., 2001; Turian et al., 2010; Collobert et al., 2011). Word embeddings have been shown to be beneficial in many NLP applications Turian et al. (2010); Collobert et al. (2011). The event model is a simple compositional model representing an event. The model is shown in Figure 1. Given an event, e = (v, d, a1 , a2 ), (here v i"
K16-1008,E12-1034,0,0.112911,"Missing"
K16-1008,E14-1024,0,0.061664,"es between events (or, more formally, mentions of events) in a document, often restricting their model to capturing dependencies only between events sharing at least one entity (a common protagonist). We generally follow this tradition in our approach. Much of this previous work has focused on count-based techniques using, for example, either the generative framework (Frermann et al., 2014) or relying on information-theoretic measures such as pointwise mutual information (PMI) (Chambers and Jurafsky, 2008). Some of these techniques treat predicate-argument structures as an atomic whole (e.g., Pichotta and Mooney (2014)), in other words their probability estimates are based on cooccurrences of entire (predicate, arguments) tuples. Clearly such methods fail to adequately take into account compositional nature of expressions used to refer to events and suffer from data sparsity. In this work our goal is to overcome the shortcomings of the count-based methods described above by representing events as real-valued vectors (event embeddings), with the embeddings computed in a compositional way relying on the predicate and its arguments. These embeddings capture semantic properties of events: events which differ in"
K16-1008,P10-1100,0,0.0948192,"Missing"
K16-1008,S15-1024,1,0.907338,"Missing"
K16-1008,P14-5010,0,0.00543248,"ents and Analysis 5.1 Data There is no standard dataset for evaluating script models. We experimented with movies summary corpus1 (Bamman et al., 2014). The corpus is created by extracting 42,306 movie summaries from November, 2012 dump of Wikipedia2 . Each document in the corpus concisely describes a movie plot along with descriptions of various characters involved in the plot. Average length of a document in the corpus is 176 words. But more popular movies have much more elaborate descriptions going up to length of 1,000 words. The corpus has been processed by the Stanford Corenlp pipeline (Manning et al., 2014). The texts in the corpus were tokenized and annotated with POS Θ = {C, T, R, A, Cp , Cf , We , Wp , Ws , Wo , (in) (in) (in) (in) Wpr , Wp , Ws , Wo , Wpr , B} is the parameter vector to be learned. Parameters are learned using mini-batch (size=1000) stochastic gradient descent with adagrad (Duchi et al., 2011) learning schedule. During training, the error in1 2 79 http://www.cs.cmu.edu/˜ark/personas/ http://dumps.wikimedia.org/enwiki/ Data Set No. of Scripts Train Set Dev Set Test Set 104,041 15,169 29,943 No. of Unique Events 856,823 119,302 231,539 given the context events by maximizing th"
K16-1008,D15-1195,0,0.276669,"Missing"
K16-1008,P09-1025,0,0.0161285,"Missing"
K16-1008,D12-1110,0,0.0578032,"Missing"
K16-1008,P10-1040,0,0.215556,"urrence counts of events, the number of instances required to model the joint probability distribution of events grows exponentially. For example, if event vocabulary size is 10 and number of events occurring in a chain are 5, then number of instances required to model the joint distribution of events is 510 − 1. This is so because the number of instances required are di76 event embedding e rectly proportional to number of free parameters in the model. To counter the shortcomings of count based script models, we propose a script model based on distributed representations (Bengio et al., 2001; Turian et al., 2010; Collobert et al., 2011). Our model tries to overcome the curse of dimensionality and sparsity by representing events as vector of real values. Both verbs and arguments are represented as a vector of real values (a.k.a embeddings). Verb and argument embeddings are composed to get event vector (event embedding). The model automatically learns these embeddings from the data itself and in the process encodes semantic properties in the event representations. 3 Ah hidden layer h T a1 dep embedding a1 = Csubj Rp T a2 predicate embedding p = Cembark subj embarked arg embedding a2 = Cbatmobile batmob"
K16-1008,L16-1556,0,0.117991,"Missing"
K16-1008,L16-1555,1,0.88609,"Missing"
L16-1555,P08-1090,0,0.630155,"typical human activity such as going to a restaurant or visiting a doctor” (Barr and Feigenbaum, 1981). Script events describe an action/activity along with the involved participants. For example, in the script describing A VISIT TO A RESTAURANT , typical events are ENTERING THE RESTAURANT, ORDERING FOOD or EATING . Participants in this scenario can include animate objects like the WAITER and the CUSTOMER , as well as inanimate objects such as CUTLERY or FOOD. Script knowledge has been shown to play an important role in text understanding (Cullingford, 1978; Miikkulainen, 1995; Mueller, 2004; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Modi and Titov, 2014; Rudinger et al., 2015). It guides the expectation of the reader, supports coreference resolution as well as common-sense knowledge inference and enables the appropriate embedding of the current sentence into the larger context. Figure 1 shows the first few sentences of a story describing the scenario TAKING A BATH . Once the TAKING A BATH scenario is evoked by the noun phrase (NP) “a bath”, the reader can effortlessly interpret the definite NP “the faucet” as an implicitly present standard participant of the TAKING A BATH script. Although in"
L16-1555,P09-1068,0,0.316802,"s going to a restaurant or visiting a doctor” (Barr and Feigenbaum, 1981). Script events describe an action/activity along with the involved participants. For example, in the script describing A VISIT TO A RESTAURANT , typical events are ENTERING THE RESTAURANT, ORDERING FOOD or EATING . Participants in this scenario can include animate objects like the WAITER and the CUSTOMER , as well as inanimate objects such as CUTLERY or FOOD. Script knowledge has been shown to play an important role in text understanding (Cullingford, 1978; Miikkulainen, 1995; Mueller, 2004; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Modi and Titov, 2014; Rudinger et al., 2015). It guides the expectation of the reader, supports coreference resolution as well as common-sense knowledge inference and enables the appropriate embedding of the current sentence into the larger context. Figure 1 shows the first few sentences of a story describing the scenario TAKING A BATH . Once the TAKING A BATH scenario is evoked by the noun phrase (NP) “a bath”, the reader can effortlessly interpret the definite NP “the faucet” as an implicitly present standard participant of the TAKING A BATH script. Although in this story, “entering the ba"
L16-1555,W14-1606,1,0.851867,"siting a doctor” (Barr and Feigenbaum, 1981). Script events describe an action/activity along with the involved participants. For example, in the script describing A VISIT TO A RESTAURANT , typical events are ENTERING THE RESTAURANT, ORDERING FOOD or EATING . Participants in this scenario can include animate objects like the WAITER and the CUSTOMER , as well as inanimate objects such as CUTLERY or FOOD. Script knowledge has been shown to play an important role in text understanding (Cullingford, 1978; Miikkulainen, 1995; Mueller, 2004; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Modi and Titov, 2014; Rudinger et al., 2015). It guides the expectation of the reader, supports coreference resolution as well as common-sense knowledge inference and enables the appropriate embedding of the current sentence into the larger context. Figure 1 shows the first few sentences of a story describing the scenario TAKING A BATH . Once the TAKING A BATH scenario is evoked by the noun phrase (NP) “a bath”, the reader can effortlessly interpret the definite NP “the faucet” as an implicitly present standard participant of the TAKING A BATH script. Although in this story, “entering the bath room”, “turning on"
L16-1555,P10-1100,1,0.763942,"“turning on the water” event, even if it was not explicitly mentioned in the text. Table 1 gives an example of typical events and participants for the script describing the scenario TAKING A BATH . A systematic study of the influence of script knowledge in texts is far from trivial. Typically, text documents (e.g. narrative texts) describing various scenarios evoke many different scripts, making it difficult to study the effect of a single script. Efforts have been made to collect scenariospecific script knowledge via crowdsourcing, for example the OMICS and SMILE corpora (Singh et al., 2002; Regneri et al., 2010; Regneri, 2013), but these corpora describe script events in a pointwise telegram style rather than in full texts. I was sitting on my couch when I decided that I hadn’t taken a bath in a while so I stood up and walked to the bathroom where I turned on the faucet in the sink and began filling the bath with hot water. While the tub was filling with hot water I put some bubble bath into the stream of hot water coming out of the faucet so that the tubbed filled with not only hot water[...] Figure 1: An excerpt from a story on the TAKING A BATH script. This paper presents the InScript 1 corpus (N"
L16-1555,S15-1024,1,0.825923,"Missing"
L16-1555,L16-1556,1,0.846082,"ith coreference information in order to facilitate the study of the interdependence between script structure and coreference. The InScript corpus is a unique resource that provides a basis for studying various aspects of the role of script knowledge in language processing by humans. The acquisition of this corpus is part of a larger research effort that aims at using script knowledge to model the surprisal and information density in written text. Besides InScript, this project also released a corpus of generic descriptions of script activities called DeScript (for Describing Script Structure, Wanzare et al. (2016)). DeScript contains a range of short and textually simple phrases that describe script events in the style of OMICS or SMILE (Singh et al., 2002; Regneri et al., 2010). These generic telegram-style descriptions are 1 The corpus can be downloaded at: http://www. sfb1102.uni-saarland.de/?page_id=2582 2 https://www.mturk.com 3485 2. Get Ingredients – gather all ingredients – get ingredients –… get ingred. Add Ingredients – pour ingredients in bowl – add ingredients to bowl –… add ingred. prepare ingred. Prepare Ingredients – mix ingredients together in bowl – stir ingredient –… […] 2.1. I gotget"
L16-1555,P13-4001,0,0.0727339,"Missing"
L18-1011,A00-1031,0,0.529327,"van Nooten and Holland (1994). This tagger produces all possible tokenizations of the input text that consist of morphologically and lexically valid word forms. Tokenization of Sanskrit is a challenging task, because individual words are merged by a set of phonetic rules called Sandhi (“connection”), whose resolution is non-deterministic and, therefore, guided by the morphological, lexical, and semantic composition of a sentence.1 This tokenization step results in a trellis of possible readings for each line of text. A dynamic programming approach that operates with a trigram language model (Brants, 2000) is used to find the most probable lexical path through this trellis. Final fine-grained morphological decisions are made by applying a Conditional Random Field (Lafferty et al., 2001) model to the most probable lexical path. The solutions are ordered by decreasing linguistic probability, given the data from the language model. The first author of this paper finally validated all proposed system analyses in a manual correction step, resulting in a morphological and lexical gold annotation of the complete R.V. Figure 1 shows a schematic overview of annotation levels for a part of hymn R.V, 1.13"
L18-1011,J08-2001,0,0.107844,"Missing"
L18-1011,Q16-1003,0,0.0308804,"nguistic domain. 2. Due to the chronological distance of approximately 1,000 years and fundamental differences in genres and topics, Vedic and Classical Sanskrit use rather different vocabularies (Hellwig, 2017). Vedic texts in general and especially the R.V contain many words that have disappeared in Classical Sanskrit. In addition, lexical semantics differ strongly between Vedic and Classical Sanskrit. The noun vadha, for example, can denote a tool for killing in the R.V (e.g., R.V 10.102.3), while it only denotes the act of killing in Classical Sanskrit. Bayesian models of semantic change (Frermann and Lapata, 2016) or diachronically motivated word embeddings (Hamilton et al., 2016) are not easily applicable, because the Vedic subcorpus is small,3 and the text historical research in older Sanskrit literature is full of uncertainties (Fosse, 1997). The lexical database of the tagger was adapted to the Vedic vocabulary using the specialized dictionary of Grassmann (1873), and Geldner’s German translation of the text (GeldSystem Adaptation Although Classical Sanskrit developed out of a late form of Vedic Sanskrit, which was described by the grammarian P¯an.ini, they represent two separate layers of Old Indo"
L18-1011,J02-3001,0,0.315927,"the remaining ones constitute sentences with missing copulae. Each verb has an average of 2.2 arguments (verbs without arguments: 6,399; with one arg.: 7,350; with 2-4 args.: 7,222; with more than 4 args.: 247).6 6. us.asah. An Algorithm for Argument Identification As mentioned in Sec. 4. and 5., the verb-argument annotation is selective. Therefore, we designed a basic argument identification algorithm that supports the re-annotation of non-oblique cases. Semantic role labeling is an active field of research in CL, and distinguishes between argument identification and argument classification (Gildea and Jurafsky, 2002). A wide range of learning algorithms such as probabilistic frameworks (Gildea and Jurafsky, 2002), 4 The verbal roots are referenced by strings in the VA annotation and by unique numeric IDs on the morpho-lexical level. The 67 mapping rules need to disambiguate homonymous verbal roots such as vas, which can mean “to dwell” (vasati), “to wear” (vaste), or “to shine” (ucchati). 5 The VA annotation indicates that a line of text contains a copula construction, but does not disambiguate the involved nominatives. – Use of copulae is optional in Sanskrit, with a strong tendency of not using it. So,"
L18-1011,P16-1141,0,0.0162552,",000 years and fundamental differences in genres and topics, Vedic and Classical Sanskrit use rather different vocabularies (Hellwig, 2017). Vedic texts in general and especially the R.V contain many words that have disappeared in Classical Sanskrit. In addition, lexical semantics differ strongly between Vedic and Classical Sanskrit. The noun vadha, for example, can denote a tool for killing in the R.V (e.g., R.V 10.102.3), while it only denotes the act of killing in Classical Sanskrit. Bayesian models of semantic change (Frermann and Lapata, 2016) or diachronically motivated word embeddings (Hamilton et al., 2016) are not easily applicable, because the Vedic subcorpus is small,3 and the text historical research in older Sanskrit literature is full of uncertainties (Fosse, 1997). The lexical database of the tagger was adapted to the Vedic vocabulary using the specialized dictionary of Grassmann (1873), and Geldner’s German translation of the text (GeldSystem Adaptation Although Classical Sanskrit developed out of a late form of Vedic Sanskrit, which was described by the grammarian P¯an.ini, they represent two separate layers of Old IndoAryan. Therefore, we needed to perform domain adaptation of the tagg"
L18-1011,W17-6811,1,0.854185,"e extended the morphological rule base and the full form dictionary of the tagger on per case basis, using Macdonell (1916). Figure 2 sets the number of newly added verbal forms (y-axis) in relation to the progress of annotation (x-axis). The plot shows that the number of cases in which we had to extend the full form database manually decreases over time, indicating improving adaptation to the new linguistic domain. 2. Due to the chronological distance of approximately 1,000 years and fundamental differences in genres and topics, Vedic and Classical Sanskrit use rather different vocabularies (Hellwig, 2017). Vedic texts in general and especially the R.V contain many words that have disappeared in Classical Sanskrit. In addition, lexical semantics differ strongly between Vedic and Classical Sanskrit. The noun vadha, for example, can denote a tool for killing in the R.V (e.g., R.V 10.102.3), while it only denotes the act of killing in Classical Sanskrit. Bayesian models of semantic change (Frermann and Lapata, 2016) or diachronically motivated word embeddings (Hamilton et al., 2016) are not easily applicable, because the Vedic subcorpus is small,3 and the text historical research in older Sanskrit"
L18-1011,J08-2006,0,0.0208715,"Missing"
L18-1011,P16-1113,0,0.0605806,"Missing"
L18-1564,P09-1068,0,0.0612194,"spects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each ques"
L18-1564,P16-1223,0,0.0634014,"computed in the same way. We use different weight matrices for a, t and q, respectively. A combined representation p for the text–question pair is then constructed using a bilinear transformation matrix W: p = t> Wq (1) We compute a score for each answer by using the dot product and pass the scores for both answers through a softmax layer for prediction. The probability p for an answer a to be correct is thus defined as: Attentive Reader The attentive reader is a well-established machine comprehension model that reaches good performance e.g. on the CNN/Daily Mail corpus (Hermann et al., 2015; Chen et al., 2016). We use the model formulation by Chen et al. (2016) and Lai et al. (2017), who employ bilinear weight functions to compute both attention and answer-text fit. Bidirectional GRUs are used to encode questions, texts and answers into hidden representations. For a question q and an answer a, the last state of the GRUs, q and a, are used as representations, while the text is encoded as a sequence of hidden states t1 ...tn . We then compute an attention score sj for each hidden state tj using the question representation q, a weight matrix Wa , and an attention bias b. Last, a text representation t"
L18-1564,P17-1147,0,0.0871477,"Missing"
L18-1564,D17-1082,0,0.139289,"Missing"
L18-1564,W14-1606,1,0.892066,"to tell who ate the food: Rachel or the waitress. In contrast, if we utilize commonsense knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney,"
L18-1564,L16-1555,1,0.938737,"scribe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section 2.1.). In Section 2.2., we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk1 (henceforth MTurk). Section 2.3. gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section 2.4. gives statistics about the final dataset. 2.1. Pilot Study As a starting point for our pilots, we made use of texts from the InScript corpus (Modi et al., 2016), which provides stories centered around everyday situations (see Section 2.2.2.). We conducted three different pilot studies to determine the best way of collecting questions that require inference over commonsense knowledge: The most intuitive way of collecting reading comprehension questions is to show texts to workers and let them formulate questions and answers on the texts, which is what we tried internally in a first pilot. Since our focus is to provide an evaluation framework for inference over commonsense knowledge, we manually assessed the number of questions that indeed require comm"
L18-1564,K16-1008,1,0.87344,"i and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of approx. 14,00"
L18-1564,D14-1162,0,0.0796453,"Missing"
L18-1564,E14-1024,0,0.0215786,"ly, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx."
L18-1564,P16-1027,0,0.0137069,"(Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements. MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of"
L18-1564,D16-1264,0,0.0964981,"Missing"
L18-1564,P10-1100,1,0.940718,"e utilize commonsense knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connectio"
L18-1564,D13-1020,0,0.338565,"Missing"
L18-1564,W17-2623,0,0.0482171,"rdson et al., 2013), BAbI (Weston et al., 2015), the Children’s Book Test (CBT, Hill et al. (2015)), CNN/Daily Mail (Hermann et al., 2015), the Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)), and RACE (Lai et al., 2017). These datasets differ with respect to text type (Wikipedia texts, examination texts, etc.), mode of answer selection (span-based, multiple choice, etc.) and test systems regarding different aspects of language understand3572 ing, but they do not explicitly address commonsense knowledge. Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA (Trischler et al., 2017) is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text’s title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. The NewsQA text collection differs from ours in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge re"
L18-1564,L16-1556,1,0.888375,"rio and that can be answered from different texts (about that scenario), but for which a text does not need to provide the answer explicitly. The next section will describe the mode of collection chosen for the final dataset, based on the third pilot, in more detail. 2.2. Data Collection 2.2.1. Scenario Selection As mentioned in the previous section, we decided to base the question collection on script scenarios rather than specific texts. As a starting point for our data collection, we use 1 www.mturk.com scenarios from three script data collections (Regneri et al., 2010; Singh et al., 2002; Wanzare et al., 2016). Together, these resources contain more than 200 scenarios. To make sure that scenarios have different complexity and content, we selected 80 of them and came up with 20 new scenarios. Together with the 10 scenarios from InScript, we end up with a total of 110 scenarios. 2.2.2. Texts For the collection of texts, we followed Modi et al. (2016), where workers were asked to write a story about a given activity “as if explaining it to a child”. This results in elaborate and explicit texts that are centered around a single scenario. Consequently, the texts are syntactically simple, facilitating ma"
L18-1564,W17-0901,1,0.858709,"knowledge, in particular, script knowledge about the EATING IN A RESTAURANT scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel. Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering (Modi and Titov, 2014a; Modi and Titov, 2014b), event paraphrasing (Regneri et al., 2010; Wanzare et al., 2017) or event prediction (namely, the narrative cloze task (Chambers and Jurafsky, T I wanted to plant a tree. I went to the home and garden store and picked a nice oak. Afterwards, I planted it in my garden. Q1 What was used to dig the hole? a. a shovel b. his bare hands Q2 When did he plant the tree? a. after watering it b. after taking it home Figure 1: An example for a text snippet with two reading comprehension questions. 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Pichotta and Mooney, 2016; Modi, 2016)). These evaluation methods lack a clear connection to real-world tasks."
L18-1564,P08-1090,0,\N,Missing
N19-1374,W08-0312,0,0.0748449,"Missing"
N19-1374,N18-2008,0,0.118124,"meters to optimize, and it is faster to train. In order to overcome the problem of generating trivial or mundane responses, there have been developments in inference techniques for encoderdecoder systems. Use of beam search has been shown to improve the general quality of generated answers, while Maximum Mutual Information (MMI) (Li et al., 2016) has improved the diversity of generated answers, leading to more meaningful output. We build on these techniques during affective inference. Emotion-based (affective) dialog generation systems have received increasing attention in the past few years. Huang et al. (2018) use emotion tokens (special “words” in a dictionary representing specific emotions) at either the encoder or decoder side, forcing the decoder to output a sentence with one specific emotion. Zhou et al. (2018) build their system using external and internal memory, where the former forces the network to generate emotional words, and the latter measures how emotional a generated sequence is compared to a target sequence. Lubis et al. (2018) modeled emotions in Valence-Arousal (VA) space for response generation. We extend this idea by using a ValenceArousal-Dominance (VAD) Lexicon (Mohammad, 201"
N19-1374,N16-1014,0,0.631148,"Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Unit (GRU) (Serban et al., 2015), but in general it is difficult to conclude which gating mechanism performs better (Chung et al., 2014). In our model, we use GRU because it has fewer parameters to optimize, and it is faster to train. In order to overcome the problem of generating trivial or mundane responses, there have been developments in inference techniques for encoderdecoder systems. Use of beam search has been shown to improve the general quality of generated answers, while Maximum Mutual Information (MMI) (Li et al., 2016) has improved the diversity of generated answers, leading to more meaningful output. We build on these techniques during affective inference. Emotion-based (affective) dialog generation systems have received increasing attention in the past few years. Huang et al. (2018) use emotion tokens (special “words” in a dictionary representing specific emotions) at either the encoder or decoder side, forcing the decoder to output a sentence with one specific emotion. Zhou et al. (2018) build their system using external and internal memory, where the former forces the network to generate emotional words"
N19-1374,W11-0609,0,0.12363,"Missing"
N19-1374,P82-1020,0,0.749869,"Missing"
N19-1374,L18-1008,0,0.0612569,"Missing"
N19-1374,P18-1017,0,0.216744,"et al. (2018) use emotion tokens (special “words” in a dictionary representing specific emotions) at either the encoder or decoder side, forcing the decoder to output a sentence with one specific emotion. Zhou et al. (2018) build their system using external and internal memory, where the former forces the network to generate emotional words, and the latter measures how emotional a generated sequence is compared to a target sequence. Lubis et al. (2018) modeled emotions in Valence-Arousal (VA) space for response generation. We extend this idea by using a ValenceArousal-Dominance (VAD) Lexicon (Mohammad, 2018), as it has been shown by Broekens (2012) that the third dimension (Dominance) is useful for modeling affect. Asghar et al. (2017) used the VAD Lexicon, but they let the neural network choose the emotion to generate (by maximizing or minimizing the affective dissonance) and their system cannot generate different emotional outputs for the same input, nor generate a specified emotion. 3 System Architecture Our system (see overview in Figure 1) is divided into three main components: (1) Emotion Labeling – automatic labeling of sentences according to the emotional content they express, using an em"
N19-1374,P02-1040,0,0.103945,"ee you. What are you doing here? I’m not here. Good to see you. You want to talk to me? Don’t worry about it. I’m just trying to get out of here. I’m going to kill you. I’m going to marry you. I’m just going to the party. There’s a lot of people here. What are you going to do with me? You’re not the only one who can help me. I can protect you. You’ve got to be kidding me. Table 1: Example responses from the baseline (seq2seq) model and the four EMOTICONS models with different emotions. 2015). Prior work has focused on designing architectures that lead to the best performance in terms of BLEU (Papineni et al., 2002) and Perplexity scores. Most seq2seq models are based on gated recurrent neural networks, either Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Unit (GRU) (Serban et al., 2015), but in general it is difficult to conclude which gating mechanism performs better (Chung et al., 2014). In our model, we use GRU because it has fewer parameters to optimize, and it is faster to train. In order to overcome the problem of generating trivial or mundane responses, there have been developments in inference techniques for encoderdecoder systems. Use of beam search has bee"
N19-1374,N15-1020,0,0.0272553,"rporating social intelligence by: (1) avoiding interaction problems that may arise when the system does not understand the user’s request (e.g., inappropriate responses that cause user anger) (Maslowski et al., 2017), and (2) building rapport ∗ Both authors contributed equally to this work. with the user (Strohkorb et al., 2016). Our method makes such conversational systems more social by outputting responses expressing emotion in a controlled manner, without sacrificing grammatical correctness, coherence, or relevance. Existing sequence-to-sequence (seq2seq) architectures, either recurrent- (Sordoni et al., 2015; Serban et al., 2015), attention- (Vaswani et al., 2017) or convolutional neural network (CNN)-based (Fan et al., 2018), do not provide a straightforward way to generate emotionally relevant output in a controlled manner. We introduce EMOTIonal CONversational System (EMOTICONS), which generates emotion-specific responses. It is based on novel contributions presented in this paper which fall in two main categories: explicit models which allow a controlled emotion-based response generation (e.g., methods based on emotion embeddings, affective sampling, and affective re-ranking), and implicit mo"
N19-1374,W18-6236,1,0.70255,"to a 3D vector of VAD values, ranging from 0 (lowest) to 1 (highest) (v ∈ [0, 1]3 ). Valence measures the positivity/negativity, Arousal the excitement/calmness, and Dominance the powerfulness/weakness of the emotion expressed by a word. This expands the work of Lubis et al. (2018), who modeled emotions only in VA space. In the following sections we describe different versions of the proposed model. 3.2.1 Emotion Classifier Affective training requires E0 , the emotion representation of the target sequence. In order to label all sentences of the corpus with E0 , we use an Emotion Classifier by Witon et al. (2018). The classifier predicts a probability distribution over class of six emotions. The classifier predictions for Cornell Movie-Dialogs Corpus (Cornell) have been shown to be highly correlated with human predictions (Witon et al., 2018). 3.2.2 Sequence-Level Explicit Encoder Model (SEE) To explicitly generate responses with emotion, this version of the model includes an emotion embedding at the encoder side. We feed the encoder with S 0 = (eSEE , s1 , s2 , . . . , s|S |), where eSEE = ASEE E0 is an Emotion Embedding (eSEE ∈ R3 ), and ASEE ∈ R3×6 is a mapping (learned during training) from E0 int"
N19-1376,P15-1033,0,0.0328709,"ic labels. We learn a prediction model by minimizing Negative Log Likelihood (N LL) of the data. 3.1 Model Architecture We propose a hierarchical architecture as shown in Figure 1. An utterance encoder takes each 3756 utterance in the dialog and outputs the corresponding utterance representation. A dialog encoder processes the utterance representations to give a compact vector representation for the dialog which is used to predict the topic of the dialog. Utterance Encoder: Each utterance in the dialog is processed sequentially using single layer Bidirectional Long Short Term Memory (BiLSTM) (Dyer et al., 2015) network and self-attention mechanism (Vaswani et al., 2017) to get the utterance representation. In particular, given an utterance with one-hot encoding for the tokens, uk = {wk,1 , wk,2 , ...., wk,L }, each token is mapped to a vector vk,i = Ewk,i ; i = 1, 2, ...L using pretrained embeddings (matrix E). Utterance representation (sk = aT H(1) ) is the weighted sum of the forward and backward direction concatenated hidden states at each step (1) (1) of the BiLSTM (H(1) = [h1 , ...., hL ]T where → − (1) ← −(1) (1) hi = [ hi : hi ] = BiLSTM(vk,i ) ). The (2) weights of the combination (a = softm"
N19-1376,P14-1062,0,0.0134601,"itly model the dependencies between utterances via self attention mechanism and hierarchical structure. Topic spotting has been explored in depth in the speech processing community (see for example, Wright et al. (1996); Kuhn et al. (1997); N¨oth et al. (1997); Theunissen (2002)). Researchers in this community have attempted to predict the topic directly from the audio signals using phoneme based features. However, the performance of word based models supersedes those of audio models (Hazen et al., 2007). Recently, there has been lot of work in deep learning community for text classification (Kalchbrenner et al., 2014; Zhang et al., 2015; Lai et al., 2015; Lin et al., 2015; Tang et al., 2015). These deep learning models use either RNNLSTM based neural networks (Hochreiter and Schmidhuber, 1997) or CNN based neural networks (Kim, 2014) for learning representation of words/sentences. We follow similar approach for topic spotting. Our model is related to the Hierarchical Attention Network (HN-ATT) model proposed by Yang et al. (2016) for document classification. HN-ATT models the document hierarchically by composing words (with weights determined by first level of attention mechanism) to get sentence represen"
N19-1376,D14-1181,0,0.0113218,"Missing"
N19-1376,D15-1106,0,0.0207455,"mechanism and hierarchical structure. Topic spotting has been explored in depth in the speech processing community (see for example, Wright et al. (1996); Kuhn et al. (1997); N¨oth et al. (1997); Theunissen (2002)). Researchers in this community have attempted to predict the topic directly from the audio signals using phoneme based features. However, the performance of word based models supersedes those of audio models (Hazen et al., 2007). Recently, there has been lot of work in deep learning community for text classification (Kalchbrenner et al., 2014; Zhang et al., 2015; Lai et al., 2015; Lin et al., 2015; Tang et al., 2015). These deep learning models use either RNNLSTM based neural networks (Hochreiter and Schmidhuber, 1997) or CNN based neural networks (Kim, 2014) for learning representation of words/sentences. We follow similar approach for topic spotting. Our model is related to the Hierarchical Attention Network (HN-ATT) model proposed by Yang et al. (2016) for document classification. HN-ATT models the document hierarchically by composing words (with weights determined by first level of attention mechanism) to get sentence representations and then combines the sentence representations w"
N19-1376,D14-1162,0,0.0807063,"Missing"
N19-1376,D15-1167,0,0.0147122,"rarchical structure. Topic spotting has been explored in depth in the speech processing community (see for example, Wright et al. (1996); Kuhn et al. (1997); N¨oth et al. (1997); Theunissen (2002)). Researchers in this community have attempted to predict the topic directly from the audio signals using phoneme based features. However, the performance of word based models supersedes those of audio models (Hazen et al., 2007). Recently, there has been lot of work in deep learning community for text classification (Kalchbrenner et al., 2014; Zhang et al., 2015; Lai et al., 2015; Lin et al., 2015; Tang et al., 2015). These deep learning models use either RNNLSTM based neural networks (Hochreiter and Schmidhuber, 1997) or CNN based neural networks (Kim, 2014) for learning representation of words/sentences. We follow similar approach for topic spotting. Our model is related to the Hierarchical Attention Network (HN-ATT) model proposed by Yang et al. (2016) for document classification. HN-ATT models the document hierarchically by composing words (with weights determined by first level of attention mechanism) to get sentence representations and then combines the sentence representations with help of second l"
N19-1376,N16-1174,0,0.0728775,"rmance of word based models supersedes those of audio models (Hazen et al., 2007). Recently, there has been lot of work in deep learning community for text classification (Kalchbrenner et al., 2014; Zhang et al., 2015; Lai et al., 2015; Lin et al., 2015; Tang et al., 2015). These deep learning models use either RNNLSTM based neural networks (Hochreiter and Schmidhuber, 1997) or CNN based neural networks (Kim, 2014) for learning representation of words/sentences. We follow similar approach for topic spotting. Our model is related to the Hierarchical Attention Network (HN-ATT) model proposed by Yang et al. (2016) for document classification. HN-ATT models the document hierarchically by composing words (with weights determined by first level of attention mechanism) to get sentence representations and then combines the sentence representations with help of second level attention to get document representation which is then used for classification. The aim of this paper is not to improve text classification but to improve topic spotting. Topic spotting and text classification differ in various aspects. We are among the first to show the use of hierarchical self attention (HN-SA) model for topic spotting."
Q17-1003,N16-1067,1,0.720394,"depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution duri"
Q17-1003,J10-4006,0,0.0235405,"ree-valued feature indicates whether the previous mention of the candidate DR d is a pronoun, a non-pronominal noun phrase, or has never been observed before. 4.2.2 Selectional Preferences Feature The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as"
Q17-1003,P14-1023,0,0.0594907,"ture The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as a stand-in for a semantic link. The values for each (w0 , r, w1 ) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of larg"
Q17-1003,P08-1090,0,0.722488,"uman expectations. • testing the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz"
Q17-1003,P09-1068,0,0.121207,"the hypothesis of Tily and Piantadosi that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Sc"
Q17-1003,E14-1006,1,0.888764,"chank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, en"
Q17-1003,E12-1034,0,0.242357,"Missing"
Q17-1003,W14-1606,1,0.882589,"), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure i"
Q17-1003,L16-1555,1,0.929396,"have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation. They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge. We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge. InScript is a crowdsourced corpus of simple narrative texts. Participants were asked to write about a specific activity (e.g., a restaurant visit, a bus ride, or a grocery shopping event) which they personally experienced, and they were instructed to tell the story as if explaining the activity to a child. This resulted in stories that are centered around a specific scenario and that explicitly mention mundane details. Thus, they generally realize longer event chains associated with a single script, which makes them particularly appropriate"
Q17-1003,K16-1008,1,0.856447,"or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications f"
Q17-1003,N16-1098,0,0.0419051,"that were used; specifically, we would expect that larger predictability effects might be observable at script boundaries, rather than within a script, as is the case in our stories. A next step in moving our participant prediction model towards NLP applications would be to replicate our modelling results on automatic textto-script mapping instead of gold-standard data as done here (in order to approximate human level of processing). Furthermore, we aim to move to more complex text types that include reference to several scripts. We plan to consider the recently published ROC Stories corpus (Mostafazadeh et al., 2016), a large crowdsourced collection of topically unrestricted short and simple narratives, as a basis for these next steps in our research. Acknowledgments We thank the editors and the anonymous reviewers for their insightful suggestions. We would like to thank Florian Pusse for helping with the Amazon Mechanical Turk experiment. We would also like to thank Simon Ostermann and Tatjana Anikina for helping with the InScript corpus. This research was partially supported by the German Research Foundation (DFG) as part of SFB 1102 ‘Information Density and Linguistic Encoding’, European Research Counc"
Q17-1003,N15-1082,0,0.014094,"anical Turk experiment (Figure 2), our referent prediction model is asked to guess the upcoming DR. relation r, we collect all the predicates in the training set which have the participant type p in the position r. The embedding of the DR xp,r is given by the average embedding of these predicates. The feature is computed as the dot product of xp,r and the word embedding of the predicate v. Predicate schemas The following feature captures a specific aspect of knowledge about prototypical sequences of events. This knowledge is called predicate schemas in the recent co-reference modeling work of Peng et al. (2015). In predicate schemas, the goal is to model pairs of events such that if a DR d participated in the first event (in a specific role), it is likely to participate in the second event (again, in a specific role). For example, in the restaurant scenario, if one observes a phrase John ordered, one is likely to see John waited somewhere later in the document. Specific arguments are not that important (where it is John or some other DR), what is important is that the argument is reused across the predicates. This would correspond to the rule X-subject-of-order → X-subject-of-eat.4 Unlike the previo"
Q17-1003,E14-1024,0,0.0438992,"hoice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent. 1.1 Scripts Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown"
Q17-1003,D12-1071,0,0.0196634,"nowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation. They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge. We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge. InScript is a crowdsourced corpus of simple narrative"
Q17-1003,P10-1100,1,0.908959,"cal event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant. Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest. Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014). Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language 33 comprehension (Sch¨utz-Bosbach and Prinz, 2007). Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012). 2 Data: The InScript Corpus Ordinary texts, i"
Q17-1003,S15-1024,1,0.925336,"Missing"
Q17-1003,W16-2518,1,0.85711,"preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v. It is computed as the cosine similarity simcos (xTd , xv,r ) of a vector-space representation of the DR xd and a structured vector-space representation of the predicate xv,r . The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010). Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task. The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010). This is a count-based, third-order cooccurrence tensor whose indices are a word w0 , a second word w1 , and a complex syntactic relation r, which is used as a stand-in for a semantic link. The values for each (w0 , r, w1 ) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of large corpora (ukWaC, BNC,"
S15-1024,P08-1090,0,0.577726,"m the website “Dinners from Hell.” Our results suggest that applying these techniques to a domain-specific dataset may be reasonable way to learn domain-specific scripts. The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson’s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called “Dinners from Hell.” Our models learn narrative chains, script-like structures that we evaluate with the “narrative cloze” task (Chambers and Jurafsky, 2008). 1 2 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et"
S15-1024,P09-1068,0,0.325579,"” task (Chambers and Jurafsky, 2008). 1 2 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attemp"
S15-1024,P11-1098,0,0.0291101,"lar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for"
S15-1024,J90-1003,0,0.34642,"y exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ord"
S15-1024,de-marneffe-etal-2006-generating,0,0.0309946,"Missing"
S15-1024,E12-1034,0,0.444975,"Missing"
S15-1024,P14-5010,0,0.00819738,"arrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e0 whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit stories about their terrible restaurant experiences. For an example story, see Figure 1. To process the raw data, we stripped all HTML and other non-story content from each file and processed the remaining text with the Stanford CoreNLP pipeline version 3.3.1 (Manning et al., 2014). Of the 237 stories obtained, we manually filtered out 94 stories that were “off-topic” (e.g., letters to the webmaster, dinners not at restaurants), leaving a total of 143 stories. The average story length is 352 words. 4.1 Annotation For the purposes of evaluation only, we hired four undergraduates to annotate every non-copular verb in each story as either corresponding to an event “related to the experience of eating in a restaurant” (e.g., ordered a steak), “unrelated to the experience of eating in a restaurant” (e.g., answered the phone), or uncertain. We used the WebAnno platform for an"
S15-1024,N04-1041,0,0.0726893,"al models are introduced by Jans et al. (2012) and we use them here, as well. First, the ordered pmi model, eˆ = arg max e∈V k X pmi(ei , e) + i=1 n X pmi(e, ei ) i=k+1 (4) where C(e1 , e2 ) is asymmetric, i.e., C(e1 , e2 ) counts only cases in which e1 occurs before e2 . Second, the bigram probability model: eˆ = arg max e∈V where p(e2 |e1 ) = metric. k Y p(e|ei ) i=1 C(e1 ,e2 ) C(e1 ,∗) n Y p(ei |e) (5) i=k+1 and C(e1 , e2 ) is asymDiscounting For each model, we add an option for discounting the computed scores. In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by Chambers and Jurafsky (2008). For the bigram probability model, this PMI discount score would be inappropriate, so we instead use absolute discounting. 207 Document Threshold We include a document threshold parameter, D, that ensures that, in any narrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e0 whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit"
S15-1024,E14-1024,0,0.534804,"uction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by"
S15-1024,P10-1100,1,0.740772,"Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by eliciting event sequence descriptions (ESDs) from humans to which they apply multiple sequence alignment (MSA) to yield one global structure per script. (Orr et al. (2014) learn similar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learnin"
S15-1024,H91-1059,0,0.0679119,"at we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers"
S15-1024,P13-4001,0,0.0436435,"Missing"
S17-1015,D15-1177,0,0.0439185,"Missing"
S17-1015,P16-1191,0,0.00990687,"Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that they assign the same weight to ever"
S17-1015,P16-1101,0,0.00456847,"ord. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word correspond"
S17-1015,J15-4004,0,0.0198179,"Missing"
S17-1015,P12-1092,0,0.0244217,"ptures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most suitable topic of a word"
S17-1015,P15-1010,0,0.0522639,"Nguyen1 , Dat Quoc Nguyen2 , Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that the"
S17-1015,K16-1008,1,0.408695,"enses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense o"
S17-1015,W14-1606,1,0.616494,"is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense"
S17-1015,N15-1070,0,0.0357356,"Missing"
S17-1015,Q17-1003,1,0.820726,"model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinge"
S17-1015,D14-1113,0,0.0546105,"Missing"
S17-1015,Q15-1016,0,0.0359882,"he word type w. The probability Pr(˜ v wd,m+j |swd,m ) is defined using the softmax function as follows: The mixture model λd,m,t = Pr(wd,m |t) × Pr(t|d) Pd P M 3 Experiments We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works. Note that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; Levy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a v wd,m + λd,m,t0 × v t0 1 + λd,m,t0 P v wd,m + Tt=1 λd,m,t × v t = P 1 + Tt=1 λd,m,t = where swd,m is the compositional vector representation of the mth word wd,m and the topics in document d; v w is the target vector representation of a word type w in vocabulary V ; v t is the vector representation of topic t; T is the number of topics; λd,m,t is defined as in Equation 1, and in MSWE -1 we define t0 = arg max λd,m,t . 1 We use an unigram distribution raised"
S17-1015,Q15-1022,1,0.436169,"into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, whe"
S17-1015,D15-1200,0,0.0397679,"t size. In addition, v tor representation of the word type w. The probability Pr(˜ v wd,m+j |swd,m ) is defined using the softmax function as follows: The mixture model λd,m,t = Pr(wd,m |t) × Pr(t|d) Pd P M 3 Experiments We evaluate MSWE on two different tasks: word similarity and word analogy. We also provide experimental results obtained by the baseline Word2Vec Skip-gram model and other previous works. Note that not all previous results are mentioned in this paper for comparison because the training corpora used in most previous research work are much larger than ours (Baroni et al., 2014; Li and Jurafsky, 2015; Schwartz et al., 2015; Levy et al., 2015). Also there are differences in the pre-processing steps that could affect the results. We could also improve obtained results by using a v wd,m + λd,m,t0 × v t0 1 + λd,m,t0 P v wd,m + Tt=1 λd,m,t × v t = P 1 + Tt=1 λd,m,t = where swd,m is the compositional vector representation of the mth word wd,m and the topics in document d; v w is the target vector representation of a word type w in vocabulary V ; v t is the vector representation of topic t; T is the number of topics; λd,m,t is defined as in Equation 1, and in MSWE -1 we define t0 = arg max λd,m,"
S17-1015,K17-3014,1,0.253101,"we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific"
S17-1015,U15-1014,1,0.855226,"into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, whe"
S17-1015,D14-1162,0,0.126941,"ntal results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Co"
S17-1015,W13-3512,0,0.17894,"Missing"
S17-1015,C14-1016,0,0.133176,"Missing"
S17-1015,D16-1174,0,0.425139,"Missing"
S17-1015,C14-1015,0,0.0135853,"Missing"
S17-1015,N15-1058,0,0.0460639,"Missing"
S17-1015,N10-1013,0,0.0136891,"dding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, approaches have been proposed to learn multi-sense word embeddings, where each sense of a word corresponds to a sense-specific embedding. Reisinger and Mooney (2010), Huang et al. (2012) and Wu and Giles (2015) proposed methods to cluster the contexts of each word and 121 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 121–127, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics its context words. MSWE thus is different from the topic-based models (Cheng et al., 2015; Liu et al., 2015b,a; Zhang and Zhong, 2016), in which we do not use the topic assignments when jointly learning vector representations of words and topics. Here we not only learn vectors based on the most sui"
S17-1015,P15-1173,0,0.0914243,"Missing"
S17-1015,D15-1036,0,0.0294844,"Missing"
S17-1015,K15-1026,0,0.0326275,"c Nguyen2 , Ashutosh Modi1 , Stefan Thater1 , Manfred Pinkal1 1 Department of Computational Linguistics, Saarland University, Germany {daiquocn, ashutosh, stth, pinkal}@coli.uni-saarland.de 2 Department of Computing, Macquarie University, Australia dat.nguyen@students.mq.edu.au Abstract then using cluster centroids as vector representations for word senses. Neelakantan et al. (2014), Tian et al. (2014), Li and Jurafsky (2015) and Chen et al. (2015) extended Word2Vec models (Mikolov et al., 2013a,b) to learn a vector representation for each sense of a word. Chen et al. (2014), Iacobacci et al. (2015) and Flekova and Gurevych (2016) performed word sense induction using external resources (e.g., WordNet, BabelNet) and then learned sense embeddings using the Word2Vec models. Rothe and Sch¨utze (2015) and Pilehvar and Collier (2016) presented methods using pre-trained word embeddings to learn embeddings from WordNet synsets. Cheng et al. (2015), Liu et al. (2015b), Liu et al. (2015a) and Zhang and Zhong (2016) directly opt the Word2Vec Skipgram model (Mikolov et al., 2013b) for learning the embeddings of words and topics on a topicassigned corpus. One issue in these previous works is that the"
S17-1015,D13-1170,0,0.00363313,"r words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks. 1 Introduction Word embeddings have shown to be useful in various NLP tasks such as sentiment analysis, topic models, script learning, machine translation, sequence labeling and parsing (Socher et al., 2013; Sutskever et al., 2014; Modi and Titov, 2014; Nguyen et al., 2015a,b; Modi, 2016; Ma and Hovy, 2016; Nguyen et al., 2017; Modi et al., 2017). A word embedding captures the syntactic and semantic properties of a word by representing the word in a form of a real-valued vector (Mikolov et al., 2013a,b; Pennington et al., 2014; Levy and Goldberg, 2014). However, usually word embedding models do not take into account lexical ambiguity. For example, the word bank is usually represented by a single vector representation for all senses including sloping land and financial institution. Recently, appr"
S17-1015,D14-1110,0,\N,Missing
S17-1015,P15-2003,0,\N,Missing
S17-1015,P14-1023,0,\N,Missing
S17-1015,L16-1046,0,\N,Missing
S18-1119,D15-1075,0,0.0806933,"Missing"
S18-1119,P08-1090,0,0.586286,"corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper is organized as follows: In Section 2, we give an overview of other machine compreh"
S18-1119,P09-1068,0,0.104067,"Missing"
S18-1119,P16-1223,0,0.0969874,"Missing"
S18-1119,P17-1168,0,0.024165,"are employed to enrich word token representations. ZMU (Li and Zhou, 2018) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings. ECNU (Sheng et al., 2018) use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader (Dhingra et al., 2017)). ´ ELiRF-UPV (Jos´e -Angel Gonz´alez et al., 2018) employs a BiLSTM with attention to find similarities between texts, questions, and answers. Each word is represented based on Numberbatch embeddings, which encode information from ConceptNet. 752 YNU AI1799 (Liu et al., 2018) submitted an ensemble of neural network models based on LSTMs, RNNs, and BiLSTM/CNN combinations, with attention mechanisms. In addition to word2vec embeddings, positional embeddings are used that are generated based on word embeddings. Rank Team name y/n what why who where when 1 2 3 4 5 6 7 8 9 10 11 Yuanfudao MITRE J"
S18-1119,S18-1174,0,0.0156272,"2: The accuracy of participating systems and the two baselines in total, on commonsense-based questions (CS), text-based questions (TXT) and on out-of-domain questions (from the 5 held-out testing scenarios). The best performance for each column is marked in bold print. Significant differences in results between two adjacent lines are marked by an asterisk (* p&lt;0.05) in the upper line. The last line shows the human upper bound (Ostermann et al., 2018) as comparison. (based on ConceptNet). The model is pretrained on another large machine comprehension dataset, namely the RACE corpus. YNU Deep (Ding and Zhou, 2018) test different LSTMs and BiLSTMs variants to encode questions, answers and texts. A simple attention mechanism is applied between question–answer and text–answer pairs. The final submission is an ensemble of five model instances. MITRE (Merkhofer et al., 2018) use a combination of 3 systems - two LSTMs with attention mechanisms, and one logistic regression model using patterns based on the vocabulary of the training set. The two neural models use different word embeddings - one trained on GoogleNews, another one trained on Twitter, which were enriched with word overlap features. Interestingly"
S18-1119,S18-1176,0,0.0287908,"Missing"
S18-1119,S18-1172,0,0.0613083,"Missing"
S18-1119,P17-1147,0,0.0637147,"Missing"
S18-1119,S18-1180,0,0.0405537,"other one trained on Twitter, which were enriched with word overlap features. Interestingly, the simple logistic regression model achieves competitive performance and would have ranked 4th as an individual system. Jiangnan (Xia, 2018) applies a BiLSTM over GloVe and CoVe embeddings (McCann et al., 2017) with an additional attention mechanism. The attention mechanism computes soft word alignment between words in the question and the text or answer. Manual features, including part-of-speech tags, named entitity types, and term frequencies, are employed to enrich word token representations. ZMU (Li and Zhou, 2018) consider a wide variety of neural models, ranging from CNNs, LSTMs and BiLSTMs with attention, together with pretrained Word2Vec and GloVe embeddings. They also employ data augmentation methods typically used in image processing. Their best performing model is a BiLSTM with attention mechanism and combined GloVe and Word2Vec embeddings. ECNU (Sheng et al., 2018) use BiGRUs and BiLSTMs to encode questions, answers and texts. They implement a multi-hop attention mechanism from question to text (a Gated Attention Reader (Dhingra et al., 2017)). ´ ELiRF-UPV (Jos´e -Angel Gonz´alez et al., 2018) e"
S18-1119,S18-1120,0,0.229149,"Missing"
S18-1119,S18-1173,0,0.0325421,"Missing"
S18-1119,S18-1181,0,0.0662618,"Missing"
S18-1119,K16-1008,1,0.821428,"ate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper is organized as follows: In Section 2, we give an overview of other machine comprehension datasets. In Section 3, we describe"
S18-1119,L16-1555,1,0.854976,"riety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993). Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proce"
S18-1119,W14-1606,1,0.907601,"the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the durat"
S18-1119,Q17-1003,1,0.839109,"she thus refers to Rachel. This shared task assesses how the inclusion of commonsense knowledge benefits natural language understanding systems. In particular, we focus on commonsense knowledge about everyday activities, referred to as scripts. Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake, taking a bus, etc. (Schank and Abelson, 1975). The concept of scripts has its underpinnings in cognitive psychology and has been shown to be an important component of the human cognitive system (Bower et al., 1979; Schank, 1982; Modi et al., 2017). From an application perspective, scripts have been shown to be useful for a variety of tasks, including story understanding (Schank, 1990), information extraction (Rau et al., 1989), and drawing inferences from texts (Miikkulainen, 1993). Factual knowledge is mentioned explicitly in texts from sources such as Wikipedia and news papers. On the contrary, script knowledge is often implicit in the texts as it is assumed to be known to the comprehender. Because of this implicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative"
S18-1119,N16-1098,0,0.0711233,"mind think about nothing but peaceful, happy thoughts. I stayed in there for only about ten minutes because it was so hot and steamy. When I got out, I turned the sauna off to save energy and took a cool shower. I got out of the shower and dried off. After that, I put on my extra set of clean clothes I brought with me, and got in my car and drove home. 2 Q1 Where did they sit inside the sauna? a. on the floor b. on a bench Q2 How long did they stay in the sauna? a. about ten min- b. over thirty utes minutes Figure 1: An example for a text from MCScript with 2 reading comprehension questions. (Mostafazadeh et al., 2016)). These tasks test a system’s ability to learn script knowledge from a text but they do not provide a mechanism to evaluate how useful script knowledge is in natural language understanding tasks. Our shared task bridges this gap by directly relating commonsense knowledge and language comprehension. The task has a machine comprehension setting: A machine is given a text document and asked questions based on the text. In addition to what is mentioned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions r"
S18-1119,L18-1564,1,0.88915,"ms with regard to specific question types and based on whether a question is directly answerable, or only inferable from the text. 4.2 Baselines We provide results of two baseline systems as lower bounds for comparison: a rule-based baseline (Sliding Window) and a neural end-to-end system (Attentive Reader). Both baselines are described in 2 IUCM cluster MCScript texts and try to find answers also in other texts, that are topically similar. In that sense, MCScript itself is used to represent commonsense knowledge. more detail below. For details about the tuning of hyperparameters, we refer to Ostermann et al. (2018). Sliding Window The sliding window baseline is a simple rule-based method that answers a question on a text by predicting the answer option with the highest similarity to the text. The intuition underlying this method is that answers similar to a text should be more plausible than answer options that are different from the text (independent of the question). In our baseline implementation, we compute similarity using a sliding window that compares each answer option to any possible “window” of w tokens of the text. For comparison, each window and each answer is represented by an average vecto"
S18-1119,D14-1162,0,0.0806018,"Missing"
S18-1119,D16-1264,0,0.12393,"atasets have been proposed for machine comprehension. One example is MCTest (Richardson et al., 2013), a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE (Lai et al., 2017), which contains 100,000 questions on reading examination data. Rajpurkar et al. (2016) have proposed the Stanford Question Answering Dataset (SQuAD), a data set of 100,000 questions on Wikipedia articles collected via crowdsourcing. In that dataset, the answer to a question corresponds to a segment/span from the reading passage. Since Wikipedia articles mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. Weston et al. (2015) have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reaso"
S18-1119,P10-1100,1,0.886473,"plicitness, learning script knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally i"
S18-1119,S18-1179,0,0.0225614,"ion of the softmax function over both answer options for the question: p(a|t, q) = sof tmax(t> Ws a) 5 embeddings (Speer et al., 2017). One participating system made use of script knowledge in the form of event sequence descriptions. Resources commonly used by participants include pretrained word embeddings such as GloVe (Pennington et al., 2014) or word2vec (Mikolov et al., 2013), and preprocessing pipelines such as NLTK4 . In the following, we provide short summaries of the participants’ systems and we give an overview of models and resources used by them (Table 1). Non-neural methods IUCM (Reznikova and Derczynski, 2018) applied an unsupervised approach that assigns the correct answer to a question based on text overlap. Text overlap is computed based on the given passage and text sources of the same topic. Different clustering and topic modeling techniques are used to identify such text sources in MCScript and DeScript. (2) Participants We ran our shared task through the CodaLab platform3 . 24 teams submitted results during the evaluation period, out of which 11 teams provided system descriptions: 8 teams from China, and one team each from Spain, Russia and the US. The full leader board containing all 24 sub"
S18-1119,D13-1020,0,0.150799,"ned in the text, answering the questions requires knowledge beyond the facts mentioned in the text. In particular, a substantial subset of questions requires inference over commonsense knowledge via scripts. For example, consider the short narrative in (1). For the first question, the correct choice for an answer requires commonsense knowledge about the activity of going to the sauna, which goes beyond what is mentioned in the text: Usually, people sit on benches inside a sauna, an Related Work Recently, a number of datasets have been proposed for machine comprehension. One example is MCTest (Richardson et al., 2013), a small curated dataset of 660 stories, with 4 multiple choice questions per story. The stories are crowdsourced and not limited to a domain. Answering questions in MCTest requires drawing inferences from multiple sentences from the text passage. In our dataset, in contrast, answering requires drawing inferences using knowledge not explicit in the text. Another recently published multiple choice dataset is RACE (Lai et al., 2017), which contains 100,000 questions on reading examination data. Rajpurkar et al. (2016) have proposed the Stanford Question Answering Dataset (SQuAD), a data set of"
S18-1119,S15-1024,1,0.874914,"Missing"
S18-1119,D15-1195,0,0.0608887,"Missing"
S18-1119,S18-1175,0,0.0341503,"Missing"
S18-1119,W17-2623,0,0.0302808,"s mostly contain factual knowledge, SQuAD does not assess how in practice, language comprehension relies on implicit and underrepresented knowledge about everyday activities i.e. script knowledge. Weston et al. (2015) have created the BAbI dataset. BAbI is a synthetic reading comprehension data set testing different types of reasoning to solve different tasks. In contrast to our dataset, the artificial texts in BAbI are not reflective of a typically occurring narrative text. Two recently published datasets that also have a larger focus on commonsense reasoning are NewsQA and TriviaQA. NewsQA (Trischler et al., 2017) contains newswire texts from CNN with crowdsourced questions and answers. During the question collection, workers were only presented with the title of the text, and a short summary. This 748 method ensures that literal repetitions of the text are avoided and the generation of non-trivial questions requiring background knowledge is supported. The NewsQA text collection differs from MCScript in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant. Tr"
S18-1119,W17-0901,1,0.841966,"cript knowledge from texts is very challenging. There are few exceptions of corpora containing narrative texts that explicitly instantiate script knowledge. An example is the InScript (Modi et al., 2016), which contains short and simple narratives, that very explicitly mention script events and participants. The Dinners from Hell corpus (Rudinger et al., 2015a) is a similar dataset centered around the EATING IN A RESTAURANT scenario. In the past, script modeling systems have been evaluated using intrinsic tasks such as event ordering (Modi and Titov, 2014), paraphrasing (Regneri et al., 2010; Wanzare et al., 2017), event prediction (namely, the narrative cloze task) (Chambers and Jurafsky, 2008, 2009; Rudinger et al., 2015b; Modi, 2016) or story completion (e.g. the story cloze task 747 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 747–757 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics information that is not given in the text. The dataset also comprises questions that can just be answered from the text, as the second question: The information about the duration of the stay is given literally in the text. The paper i"
S18-1119,C00-2137,0,0.237567,"Missing"
S18-1119,S18-1177,0,0.0482581,"Missing"
S19-1032,D12-1048,0,\N,Missing
S19-1032,D15-1162,0,\N,Missing
S19-1032,P15-1034,0,\N,Missing
S19-1032,W14-1606,1,\N,Missing
S19-1032,K16-1008,1,\N,Missing
S19-1032,Q16-1029,0,\N,Missing
S19-1032,Q17-1003,1,\N,Missing
S19-1032,P17-1044,0,\N,Missing
S19-1032,W04-0908,0,\N,Missing
W12-1901,bauer-etal-2012-dependency,0,0.0695217,"Missing"
W12-1901,burchardt-etal-2006-salsa,0,0.0326741,"wn to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association"
W12-1901,W05-0620,0,0.181807,"Missing"
W12-1901,P11-1144,0,0.0418046,"dividual verb, or even individual senses of a verb. The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively. However, the SRL task is known to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core"
W12-1901,erk-pado-2006-shalmaneser,0,0.147556,"ntations (Palmer et al., 2005) where roles are defined for each individual verb, or even individual senses of a verb. The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively. However, the SRL task is known to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More acc"
W12-1901,D09-1002,0,0.0674574,"Missing"
W12-1901,W06-1601,0,0.168802,"small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However"
W12-1901,D08-1008,0,0.0196437,"ther words, our method jointly induces both frames and frame-specific semantic roles. We experiment only with verbal predicates and evaluate the performance of the model with respect to some natural baselines. Though the scores for frame induction are not high, we argue that this is primarily due to very high granularity of FrameNet frames which is hard to reproduce for unsupervised systems, as the implicit supervision signal is not capable of providing these distinctions. 2 Task Definition In this work, we use dependency representations of frame semantics. Dependency representations for SRL (Johansson and Nugues, 2008) were made popular by CoNLL-2008 and CoNLL-2009 shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), but for English were limited to PropBank. Recently, English FrameNet was also released in the dependency format (Bauer et al., 2012). Instead of predicting argument spans, in dependency representation the goal is, roughly, to predict the syntactic head of the argument. The semantic dependency representation for sentence (a) is shown in Figure 1, labels on edges denote roles and labels on words denote frames. Note that in practice the structures can be more complex, as, for example, argume"
W12-1901,N10-1137,0,0.626335,"ly representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However, all these approaches"
W12-1901,P11-1112,0,0.275083,"ntion (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However, all these approaches have focused on PropBank-style representations. This may seem somewhat unnatural as FrameNet representations, though arguably more powerful, are harder to learn in the supervised setting, harder to annotate, and annotated data is available for a considerably fewer languages. This is the gap which we address in this preliminary study. More specifically, we extend an existing stateof-the-art Bayesian model for unsupervised semantic role labeling and apply it to support FrameNetstyle semantics. In other words, our method jointly induce"
W12-1901,D11-1122,0,0.227355,"ntion (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However, all these approaches have focused on PropBank-style representations. This may seem somewhat unnatural as FrameNet representations, though arguably more powerful, are harder to learn in the supervised setting, harder to annotate, and annotated data is available for a considerably fewer languages. This is the gap which we address in this preliminary study. More specifically, we extend an existing stateof-the-art Bayesian model for unsupervised semantic role labeling and apply it to support FrameNetstyle semantics. In other words, our method jointly induce"
W12-1901,S10-1011,0,0.0319159,"verbal predicates only, the first stage would be trivial and the second stage could be handled with heuristics as in much of previous work on unsupervised SRL (Lang and Lapata, 2011a; Titov and Klementiev, 2012). Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as (a) the distributions across frames (i.e. senses) are generally highly skewed, (b) current state-of-the-art techniques for word-sense induction hardly beat mostfrequent-sense baselines in accuracy metrics (Manandhar et al., 2010). This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011). From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference. 3 Model and Inference We follow previous work on unsupervised semantic role labeling (Lang and Lapata, 2011a; Titov and Klementiev, 2012) and associate arguments with their frame specific syntactic signatures which we refer"
W12-1901,C10-2107,0,0.06723,"oles are defined for each individual verb, or even individual senses of a verb. The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively. However, the SRL task is known to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al., 2010; Palmer and Sporleder, 2010; Das and Smith, 2011). Another reason for reduced interest in predicting FrameNet representations is the lack of annotated resources for most languages, with annotated corpora available or being developed only for English (Ruppenhofer et al., 2006), German (Burchardt et al., 2006), Spanish (Subirats, 2009) and Japanese (Ohara et al., 2004). Due to scarcity of labeled data, purely unsupervised set-ups recently started to receive considerable attention (Swier and Stevenson, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and nonco"
W12-1901,J05-1004,0,0.306755,"heat]. the same semantic frame Apply Heat is evoked by verbs cook and sautee, and roles COOK and F OOD in the sentence (a) are filled by Mary and the broccoli, respectively. Note that roles are specific to the frame, not to the individual lexical units (verbs cook and sautee, in the example).1 Most approaches to predicting these representations, called semantic role labeling (SRL), have relied on large annotated datasets (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). By far, most of this work has focused on PropBank-style representations (Palmer et al., 2005) where roles are defined for each individual verb, or even individual senses of a verb. The only exceptions are modifiers and roles A0 and A1 which correspond to proto-agent (a doer, or initiator of the action) and proto-patient (an affected entity), respectively. However, the SRL task is known to be especially hard for the FrameNetstyle representations for a number of reasons, including, the lack of cross-frame correspondence for most roles, fine-grain definitions of roles and frames in FrameNet, and relatively small amounts of statistically representative data (Erk and Pado, 2006; Das et al."
W12-1901,D09-1001,0,0.328307,"on unsupervised SRL (Lang and Lapata, 2011a; Titov and Klementiev, 2012). Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as (a) the distributions across frames (i.e. senses) are generally highly skewed, (b) current state-of-the-art techniques for word-sense induction hardly beat mostfrequent-sense baselines in accuracy metrics (Manandhar et al., 2010). This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011). From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference. 3 Model and Inference We follow previous work on unsupervised semantic role labeling (Lang and Lapata, 2011a; Titov and Klementiev, 2012) and associate arguments with their frame specific syntactic signatures which we refer to as argument keys: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relati"
W12-1901,P10-1031,0,0.02664,"g and Lapata, 2011a; Titov and Klementiev, 2012). Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as (a) the distributions across frames (i.e. senses) are generally highly skewed, (b) current state-of-the-art techniques for word-sense induction hardly beat mostfrequent-sense baselines in accuracy metrics (Manandhar et al., 2010). This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011). From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference. 3 Model and Inference We follow previous work on unsupervised semantic role labeling (Lang and Lapata, 2011a; Titov and Klementiev, 2012) and associate arguments with their frame specific syntactic signatures which we refer to as argument keys: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relation to its governor. • Pre"
W12-1901,W08-2121,0,0.12004,"Missing"
W12-1901,P11-1145,1,0.926856,"v and Klementiev, 2012). Additionally to considering only verbal predicates, we also assume that every verb belongs to a single frame. This assumption, though restrictive, may be reasonable in practice as (a) the distributions across frames (i.e. senses) are generally highly skewed, (b) current state-of-the-art techniques for word-sense induction hardly beat mostfrequent-sense baselines in accuracy metrics (Manandhar et al., 2010). This assumption, or its minor relaxations, is relatively standard in work on unsupervised semantic parsing tasks (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011). From the modeling prospective, there are no major obstacles to relaxing this assumption, but it would lead to a major explosion of the search space and, as a result, slow inference. 3 Model and Inference We follow previous work on unsupervised semantic role labeling (Lang and Lapata, 2011a; Titov and Klementiev, 2012) and associate arguments with their frame specific syntactic signatures which we refer to as argument keys: • Active or passive verb voice (ACT/PASS). • Argument position relative to predicate (LEFT/RIGHT). • Syntactic relation to its governor. • Preposition used for argument re"
W12-1901,E12-1003,1,0.623157,"on, 2004; Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and 1 More accurately, FrameNet distinguishes core and noncore roles with non-core roles mostly corresponding to modifiers, e.g., M AN N ER in sentence (b). Non-core roles are expected to generalize across frames. 1 NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1–7, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics CONTAINER COOK Mary cooks Apply_Heat FOOD the broccoli in a small pan Figure 1: An example of a semantic dependency graph. Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). However, all these approaches have focused on PropBank-style representations. This may seem somewhat unnatural as FrameNet representations, though arguably more powerful, are harder to learn in the supervised setting, harder to annotate, and annotated data is available for a considerably fewer languages. This is the gap which we address in this preliminary study. More specifically, we extend an existing stateof-the-art Bayesian model for unsupervised semantic role labeling and apply it to support FrameNetstyle semantics. In other words, our method jointly induces both frames and frame-specif"
W12-1901,W04-3213,0,\N,Missing
W12-1901,N10-1138,0,\N,Missing
W12-1901,W09-1201,0,\N,Missing
W12-1901,J02-3001,0,\N,Missing
W14-1606,W12-1901,1,0.797826,"ript information, induced from large unannotated corpora, should be highly beneficial. Our current model uses a fairly naive semantic composition component, we plan to extend it with more powerful recursive embedding methods which should be especially beneficial when considering very large text collections. Related Work Additionally to the work on script induction discussed above (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010), other methods for unsupervised learning of event semantics have been proposed. These methods include unsupervised frame induction techniques (O’Connor, 2012; Modi et al., 2012). Frames encode situations (or objects) along with their participants and properties (Fillmore, 1976). Events in these unsupervised approaches are represented with categorical latent variables, and they are induced relying primarily on the selectional preferences’ signal. The very recent work of Cheung et al. (2013) can be regarded as their extension but Cheung et al. also model transitions between events with Markov models. However, neither of these approaches considers (or directly optimizes) the discriminative objective of learning to order events, and neither of them uses distributed repre"
W14-1606,P11-1062,0,0.0398983,"Missing"
W14-1606,P09-1068,0,0.729939,"resentation Learning and exploiting distributed word representations (i.e. vectors of real values, also known as embeddings) have been shown to be beneficial in many NLP applications (Bengio et al., 2001; Turian et al., 2010; Collobert et al., 2011). These representations encode semantic and syntactic properties of a word, and are normally Our model is solely focusing on the ordering task, and admittedly does not represent all the information encoded by a script graph structure. For example, it cannot be directly used to predict a missing event given a set of events (the narrative cloze task (Chambers and Jurafsky, 2009)). Nev50 learned in the language modeling setting (i.e. learned to be predictive of local word context), though they can also be specialized by learning in the context of other NLP applications such as PoS tagging or semantic role labeling (Collobert et al., 2011). More recently, the area of distributional compositional semantics have started to emerge (Baroni and Zamparelli, 2011; Socher et al., 2012), they focus on inducing representations of phrases by learning a compositional model. Such a model would compute a representation of a phrase by starting with embeddings of individual words in t"
W14-1606,P08-1090,0,0.913237,"be used to answer, but doing this without explicitly representing the knowledge as a graph. In our method, the distributed representations (i.e. vectors of real numbers) of event realizations are computed based on distributed representations of predicates and their arguments, and then the event representations are used in a ranker to predict the prototypical ordering of events. Both the parameters of the compositional process for computing the event representation and the rankInduction of common sense knowledge about prototypical sequence of events has recently received much attention (e.g., Chambers and Jurafsky (2008); Regneri et al. (2010)). Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated. We show that this approach results in a substantial boost in performance on the event ordering task with r"
W14-1606,N13-1104,0,0.035405,"d Work Additionally to the work on script induction discussed above (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010), other methods for unsupervised learning of event semantics have been proposed. These methods include unsupervised frame induction techniques (O’Connor, 2012; Modi et al., 2012). Frames encode situations (or objects) along with their participants and properties (Fillmore, 1976). Events in these unsupervised approaches are represented with categorical latent variables, and they are induced relying primarily on the selectional preferences’ signal. The very recent work of Cheung et al. (2013) can be regarded as their extension but Cheung et al. also model transitions between events with Markov models. However, neither of these approaches considers (or directly optimizes) the discriminative objective of learning to order events, and neither of them uses distributed representations to encode semantic properties of events. As we pointed out before, our embedding approach is similar (or, in fact, a simplification of) the phrase embedding methods studied in the recent work on distributional compositional semantics (Baroni and Zamparelli, 2011; Socher et al., 2012). However, they have n"
W14-1606,W04-3205,0,0.0110602,"ks, we considered a few unseen predicate pairs where the EEverb model was correctly predicting their order. For many of these pairs there were no inferTable 3: Results on the Gigaword data for the verb-frequency baseline (BL), the verb-only embedding model (EEverb ), the full model (EE) and CJ08 rules. training. This selection strategy was chosen to create a negative bias for our model which is more expressive than the baseline methods and, consequently, better at memorizing examples. As a rule-based temporal classifier, we used high precision “happens-before” rules from the VerbOcean system (Chklovski and Pantel, 2004). Consider “to �verb-x� and then �verb-y�” as one example of such rule. We used predicted collapsed Stanford dependencies (de Marneffe et al., 2006) to extract arguments of the verbs, and used only a subset of dependents of a verb.7 This preprocessing ensured that (1) clues which form part of a pattern are not observable by our model both at train and test time; (2) there is no systematic difference between both events (e.g., for collapsed dependencies, the noun subject is attached to both verbs even if the verbs are conjoined); (3) no information about the order of events in text is available"
W14-1606,P10-1100,0,0.19924,"ugh representing the script knowledge as graphs is attractive from the human interpretability perspective, it may not be optimal from the application point of view. More specifically, these representations (1) require a model designer to choose an appropriate granularity of event mentions (e.g., whether nodes in the graph should be associated with verbs, or also their arguments); (2) do not provide a mechanism for deciding which scenario applies in a given discourse context and (3) often do not associate confidence levels with information encoded in the graph (e.g., the precedence relation in Regneri et al. (2010)). Instead of constructing a graph and using it to provide information (e.g., prototypical event ordering) to NLP applications, in this work we advocate for constructing a statistical model which is capable to “answer” at least some of the questions these graphs can be used to answer, but doing this without explicitly representing the knowledge as a graph. In our method, the distributed representations (i.e. vectors of real numbers) of event realizations are computed based on distributed representations of predicates and their arguments, and then the event representations are used in a ranker"
W14-1606,N13-1008,0,0.0162647,"Missing"
W14-1606,de-marneffe-etal-2006-generating,0,0.0409424,"Missing"
W14-1606,D12-1110,0,0.0333476,"Missing"
W14-1606,P10-1040,0,0.0151946,"Missing"
W14-1606,E14-1006,1,0.543514,"amount of training data, variability in the used vocabulary, optionality of events (e.g., going to the coffee machine may not be mentioned in a ESD), different granularity of events and variability in the ordering (e.g., coffee may be put in the filter before placing it in the coffee maker). Unlike our work, Regneri et al. (2010) relies on WordNet to provide extra signal when using the Multiple Se3.1.2 Results and discussion We evaluated our event embedding model (EE) against baseline systems (BL , MSA and BS). MSA is the system of Regneri et al. (2010). BS is a hierarchical Bayesian model by Frermann et al. (2014). BL chooses the order of events based on the preferred order of the corresponding verbs in the training set: (e1 , e2 ) is predicted to be in the 4 The event pairs are not coming from the same ESDs making the task harder as the events may not be in any temporal relation. 52 stereotypical order if the number of times the corresponding verbs v1 and v2 appear in this order in the training ESDs exceeds the number of times they appear in the opposite order (not necessary at adjacent positions); a coin is tossed to break ties (or if v1 and v2 are the same verb). This frequency counting method was p"
W14-1606,S13-2001,0,0.046004,"verb pair frequency counts are available at www.usna.edu/Users/cs/nchamber/data/schemas/acl09/verbpair-orders.gz 7 The list of dependencies not considered: aux, auxpass, attr, appos, cc, conj, complm, cop, dep, det, punct, mwe. 55 96.0 96.0 94.1   82.4  83.1 81.8 77.8 81.2 ity to the methods proposed in this work but they are mostly limited to binary relations and deal with predicting missing relations rather than with temporal reasoning of any kind. Identification of temporal relations within a text is a challenging problem and an active area of research (see, e.g., the TempEval task (UzZaman et al., 2013)). Many rule-based and supervised approaches have been proposed in the past. However, integration of common sense knowledge induced from large-scale unannotated resources still remains a challenge. We believe that our approach will provide a powerful signal complementary to information exploited by most existing methods. CJ08 BL EEverb EE 71.0   57.2 50.0 62.7      Figure 3: Results for different frequency bands: unseen, medium frequency (between 1 and 10) and high frequency (> 10) verb pairs. 5 ence chains of length 2 (e.g., chain of length 2 was found for the pair acc"
W14-1606,D10-1115,0,\N,Missing
W18-6236,L18-1252,0,0.0387976,"embedding (Peters et al., 2018). Our emotion prediction model is also based on deep learning techniques. Recently, fastText (Joulin et al., 2016) has been proposed for generating word representations which have shown state-of-the-art performance on a number of text related tasks. Our model makes use of a fastText model for emotion classification. 248 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 248–253 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chen et al. (2018) introduce an emotion corpus based on conversations taken from Friends TV scripts and propose a similar emotion classification model using a CNN-BiLSTM. Our model is similar to the model proposed by (Chen et al., 2018), but we use a pre-trained ELMo instead of a BiLSTM. Mohammad (2018) have proposed a VAD lexicon for emotion detection systems. We use VAD features together with ELMo (Peters et al., 2018). Recently, the ELMo model has been shown to boost performance on a number of Natural Language Processing (NLP) tasks. To the best of our knowledge, we are the first to make use of VAD features"
W18-6236,W11-0609,0,0.0260934,"e first experiment, we take the top 2 emotions predicted by the final system on D1 and check if at least one of the predicted labels matches one of the golden labels. F1 Scores are presented in Table 4. 7.3 Figure 4: Confusion Matrix for the Ensemble Classifier. depending on whether the trigger word was “happy” or “surprised”). 7 Model Generalization In order to have a better understanding of the performance of our system for real world applications, we tested our system on an explicit emotion prediction task. 7.1 Dataset and Task For our experiments, we used the Cornell Movie Corpus built by Danescu-Niculescu-Mizil and Lee (2011), which is composed of around 300,000 utterances extracted from 600 movies. A group of internal annotators manually annotated a subset of 58,000 lines, with at most 2 of 7 emotion labels (fear, surprise, anger, disgust, joy, sad, neutral). We use this data for two experiments. In the first experiment, we measure how well the classifier predictions correlate with human annotation for the 6 emotions. For this experiment we create the dataset D1 by randomly sampling 4800 lines consisting of 800 samples for each emotion class (except for the neutral class). In the second experiment, we measure how"
W18-6236,W18-6206,0,0.0615988,"Missing"
W18-6236,P18-1017,0,0.0199937,"el makes use of a fastText model for emotion classification. 248 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 248–253 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chen et al. (2018) introduce an emotion corpus based on conversations taken from Friends TV scripts and propose a similar emotion classification model using a CNN-BiLSTM. Our model is similar to the model proposed by (Chen et al., 2018), but we use a pre-trained ELMo instead of a BiLSTM. Mohammad (2018) have proposed a VAD lexicon for emotion detection systems. We use VAD features together with ELMo (Peters et al., 2018). Recently, the ELMo model has been shown to boost performance on a number of Natural Language Processing (NLP) tasks. To the best of our knowledge, we are the first to make use of VAD features in a deep learning setting for emotion prediction. 3 Task Description classifier requires a fixed length input. Since tweets have a variable number of words, padding is typically added to the shorter word sequences in order to have equal lengths across the mini-batch. In practice, havi"
W18-6236,N18-1202,0,0.287556,"pproaches to Subjectivity, Sentiment and Social Media Analysis, pages 248–253 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Chen et al. (2018) introduce an emotion corpus based on conversations taken from Friends TV scripts and propose a similar emotion classification model using a CNN-BiLSTM. Our model is similar to the model proposed by (Chen et al., 2018), but we use a pre-trained ELMo instead of a BiLSTM. Mohammad (2018) have proposed a VAD lexicon for emotion detection systems. We use VAD features together with ELMo (Peters et al., 2018). Recently, the ELMo model has been shown to boost performance on a number of Natural Language Processing (NLP) tasks. To the best of our knowledge, we are the first to make use of VAD features in a deep learning setting for emotion prediction. 3 Task Description classifier requires a fixed length input. Since tweets have a variable number of words, padding is typically added to the shorter word sequences in order to have equal lengths across the mini-batch. In practice, having long sequences may not work well due to noise introduced by padding. Based on tweet length distribution (see Figure 1"
