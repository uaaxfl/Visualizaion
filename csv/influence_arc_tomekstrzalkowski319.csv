1993.iwpt-1.23,J90-1003,0,0.0132017,"le and Rooth, 1991 ) parsed 13 million words of Associated Press news messages, while MIT's parser ( de Marcken, 1990) was used to process the 1 million word Lan caster/ Oslo/Bergen (LOB) corpus. In both cases, the parsers were designed to do partial process ing only, that is, they would never attempt a com plete analysis of certain constructions, such as the attachment of pp-adjuncts, subordinate clauses, or coordinations. This kind of partial analysis may be sufficient in some applications because of a relatively high precision of identifying cor rect syntactic dependencies, for example Church and Hanks (1990) used partial parses generated by Fidditch to study word co-occurrence patterns in syntactic contexts. On the other hand, ap plications involving information extraction or re trieval from text will usually require more accu rate parsers. An alternative is to create a parser that would attempt to produce a complete parse, and would resort to partial or approximate analysis only under exceptional conditions such as an extra grammatical input or a severe time pressure. En countering a construction that it couldn't han dle, the parser would first try to produce an ap proximate analysis of t"
1993.iwpt-1.23,P91-1030,0,0.0208623,"accuracy. For example, we may have to settle for partial parsing that would recognize only selected grammatical structures ( e.g. noun phrases; Ruge et al. , 1991 ) , or would avoid mak ing difficult decisions ( e.g. pp-attachment; Hin dle, 1983 ) . Much of the overhead and inefficiency comes from the fact that the lexical and struc tural ambiguity of natural language input can only be dealt with using limited context infor mation available to the parser. Partial parsing techniques have been used with a considerable success in processing large volumes of text, for example AT&T's Fidditch (Hindle and Rooth, 1991 ) parsed 13 million words of Associated Press news messages, while MIT's parser ( de Marcken, 1990) was used to process the 1 million word Lan caster/ Oslo/Bergen (LOB) corpus. In both cases, the parsers were designed to do partial process ing only, that is, they would never attempt a com plete analysis of certain constructions, such as the attachment of pp-adjuncts, subordinate clauses, or coordinations. This kind of partial analysis may be sufficient in some applications because of a relatively high precision of identifying cor rect syntactic dependencies, for example Church and Hanks ("
1993.iwpt-1.23,H91-1065,0,0.137383,"Missing"
1993.iwpt-1.23,P92-1014,1,0.830705,"Missing"
2020.figlang-1.23,W13-0904,0,0.062018,"Missing"
2020.figlang-1.23,P80-1004,0,0.350738,"e relations if they also carry high imageability scores. Imageability ratings of most lexical items are looked up in an expanded MRC psycholinguistic database, which were built for several languages (Liu et al., 2014). The candidate relations are then used to compute and rank possible source domains in an emerging conceptual metaphor. Full details of the metaphor extraction process can be found in the cited papers. Our approach to metaphor extraction is contrasted with more traditional computational approaches based on selectional restriction violations (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010 for an overview) which do not scale well due to their heavy reliance on domain knowledge. More recent variants of this general approach (e.g., Rosen, 2018) utilize more robust deep learning methods but their utility remains limited to only some forms of text metaphors. heterosexual couples is an incorrect interpretation of the concept of marriage. Finally, the seventh stance, labeled infringement, focuses on preventing emerging legal definitions of marriage and family from infringing on personal and religious lib"
2020.figlang-1.23,W18-0912,0,0.0140058,"ns are then used to compute and rank possible source domains in an emerging conceptual metaphor. Full details of the metaphor extraction process can be found in the cited papers. Our approach to metaphor extraction is contrasted with more traditional computational approaches based on selectional restriction violations (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010 for an overview) which do not scale well due to their heavy reliance on domain knowledge. More recent variants of this general approach (e.g., Rosen, 2018) utilize more robust deep learning methods but their utility remains limited to only some forms of text metaphors. heterosexual couples is an incorrect interpretation of the concept of marriage. Finally, the seventh stance, labeled infringement, focuses on preventing emerging legal definitions of marriage and family from infringing on personal and religious liberties. These last two stances can be grouped into the more general category of the traditional community, or those who believe that marriage and family should be reserved for heterosexual couples and that it is not the place of the gove"
2020.figlang-1.23,shutova-teufel-2010-metaphor,0,0.0138164,"bility scores. Imageability ratings of most lexical items are looked up in an expanded MRC psycholinguistic database, which were built for several languages (Liu et al., 2014). The candidate relations are then used to compute and rank possible source domains in an emerging conceptual metaphor. Full details of the metaphor extraction process can be found in the cited papers. Our approach to metaphor extraction is contrasted with more traditional computational approaches based on selectional restriction violations (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010 for an overview) which do not scale well due to their heavy reliance on domain knowledge. More recent variants of this general approach (e.g., Rosen, 2018) utilize more robust deep learning methods but their utility remains limited to only some forms of text metaphors. heterosexual couples is an incorrect interpretation of the concept of marriage. Finally, the seventh stance, labeled infringement, focuses on preventing emerging legal definitions of marriage and family from infringing on personal and religious liberties. These last two stances can be grouped int"
2020.figlang-1.23,W13-0905,0,0.0195024,"ey are based is so vast that it is believed to be free of sampling limitations plaguing earlier research. And yet, the models that can be derived are only as good as the data from which they are built; the data, however vast, may still be biased. For one, people who post on the internet are not necessarily representative of the general population. Furthermore, society is composed of various communities and groups 165 Proceedings of the Second Workshop on Figurative Language Processing, pages 165–175 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Wilks et al., 2013; Mohan et al., 2013). Furthermore, within each linguistic-cultural society, various communities project their views on weighty social issues onto metaphorical language (Charteris-Black, 2002). The objectives of the current Computational Ethnography (COMETH) project are thus twofold: (1) confirm experimentally that computational models of figurative language use capture communities’ uniquely biased worldviews; and (2) demonstrate experimentally that such models, when used generatively, can mimic communities’ reactions to novel information. Accordingly, the COMETH project developed an automated"
2020.findings-emnlp.247,P17-1017,0,0.0224341,"find that the approach produces better responses per automated metrics and detailed human evaluations. • We propose the use of LCS-inspired representations based on asks and framings, which in turn are grounded in conversation analysis literature, to generate plans, instead of using dialogue acts. • We release corpora annotated with plans for all utterances, using three planners, including symbolic planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning representations for open-domain dialogue systems based on lexical conceptual structures (explained in Section 3.1). 2 3 R"
2020.findings-emnlp.247,W17-5525,0,0.0184266,"he following contributions: • We investigate the impact of separating planning and realization in open-domain dialogue and find that the approach produces better responses per automated metrics and detailed human evaluations. • We propose the use of LCS-inspired representations based on asks and framings, which in turn are grounded in conversation analysis literature, to generate plans, instead of using dialogue acts. • We release corpora annotated with plans for all utterances, using three planners, including symbolic planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning"
2020.findings-emnlp.247,N18-2012,0,0.0460113,"Missing"
2020.findings-emnlp.247,P02-1040,0,0.107449,"y in a ratio of 80/10/10 for the training, testing, and validation set. 4.1 Planning Phase Evaluation This evaluation focuses on investigating the efficacy of the two automated planners (Context Attention (CTX) and Pseudo-Self Attention (PSA)) in learning to generate response plans. 4.1.1 Automated Metrics Are the automated planners able to faithfully learn how to generate the response utterance plans? To investigate, we compare the performance of the CTX and the PSA planner with the symbolic planner output (which is our silver standard reference) using common automated metrics Table 2: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015) on the test set. We use the library by Sharma et al. (2017). We find that PSA was able to achieve higher word overlap metrics with respect to the silver standard. We conducted an indepth analysis of the CTX and PSA planner output on the entire testing set. We found that the PSA model was more likely to produce ask actions that matched the ground truth, resulting in higher scores on the automated metrics. 4.1.2 Human Evaluation Evaluation using automated metrics provides limited evidence for the ability to aut"
2020.findings-emnlp.247,D19-1460,0,0.0215155,"planners and attention-based planners. dialogue systems (He et al., 2018). Novikova et al. (2017) propose the E2E challenge and use MRs to show lexical richness and syntactic variation. Similarly, Gardent et al. (2017) focus on structured data (e.g. DBpedia) to generate text in the WebNLG framework. Moryossef et al. (2019) use an explicit symbolic component for planning in a neural data to text generation system that allows controllable generation. Along with conversational intents, dialogue acts are also used for natural language understanding (NLU) in task-oriented systems (Li et al., 2019; Peskov et al., 2019). In contrast to these prior approaches, our work uses more in-depth meaning representations for open-domain dialogue systems based on lexical conceptual structures (explained in Section 3.1). 2 3 Related Work Open-Ended Dialogue Systems: Transformer models (Vaswani et al., 2017) and large transformer-based language models such as GPT, GPT-2, XLNet, BERT (Radford et al., 2018, 2019; Yang et al., 2019; Devlin et al., 2019) have helped achieve the SOTA performance across several natural language tasks. However, these models do not achieve the same level of consistent performance on generative mo"
2020.findings-emnlp.247,W94-0319,0,0.558422,"o Self Attention. Introduction Recent advancements in the area of generative modeling have helped increase the fluency of generative models. However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data (Moryossef et al., 2019; Wiseman et al., 2017). One reason could be that the generation task is typically construed as an end-to-end system. This is in contrast to traditional approaches, which incorporate a sequence of steps in the NLG system, including content determination, sentence planning, and surface realization (Reiter, 1994; Reiter and Dale, 2000). A review of literature from psycholinguistics and cognitive science also provides strong empirical evidence that the human language production process is not a monolith (Dell, 1985; Bock, 1996; Bock et al., 2007; Kennison, 2018). Prior approaches have indeed incorporated content planning into the NLG system, for example data-to-text generation problems (Puduppully et al., 2019; Moryossef et al., 2019) as well as classic works that include planning, based on speech acts (Cohen and Perrault, 1979) (for an in-depth review c.f. (Garoufi, 2014)). Our work closely follows t"
2020.findings-emnlp.247,W19-8610,1,0.842372,"92 0.0065 7.553 0.838 PSG 0.1253 0.0045 15.128 0.847 No Plan Symbolic Planner Table 5: Automated metric results on the responses generated on the test set of both corpora. input utterance but no plan as input; (2) Symbolic Planner based Generation: This model receives the plan from symbolic planner output; (3) CTX Planner-Based Generation: This model receives the CTX plan; (4) PSA Planner-Based Generation: This model receives the PSA plan. 4.2.1 Automated Metrics Prior research has shown that most automated metrics have little to no correlation to human ratings on NLG tasks (Liu et al., 2016; Santhanam and Shaikh, 2019); however, they may provide some standard of reference to evaluate performance. We report the following metrics: (i) BLEU (Papineni et al., 2002) (ii) length of responses, with the understanding that models that are able to generate longer responses are better (iii) following, Mei et al (2017), we report the diversity metric (Li et al., 2016a). Diversity is calculated as the number of distinct unigrams in the generation scaled by the total number of generated tokens (Mei et al., 2017; Li et al., 2016b). (iv) BERT-Score (Zhang* et al., 2020) metric, an embedding-based score which has shown grea"
2020.findings-emnlp.247,P16-1056,0,0.0435254,"Missing"
2020.findings-emnlp.247,J00-3003,0,0.725047,"Missing"
2020.findings-emnlp.247,D19-1062,0,0.0290866,"derstanding tasks (Ziegler et al., 2019; Edunov et al., 2019). Wolf et al. (2019) propose a transfer learning approach that fine tunes large pretrained language models and achieves SOTA scores on the PERSONA-chat dataset (Golovanov et al., 2019) and in the CONVAI2 competition (Dinan et al., 2019; Yusupov and Kuratov, 2018). Keskar et al. (2019) introduce a large-scale conditional transformer model that improves generation based on control codes. Our training paradigm is consistent with existing research that constrains large-scale language models across generation tasks (Rashkin et al., 2019; Urbanek et al., 2019) and yields controllable text generation (Shen et al., 2019; Zhou et al., 2017), with one key difference: we learn to plan and realize separately. Accordingly, we overview planning based approaches next. Planning-Based Approaches: A standard component of traditional NLG systems is a planner (Reiter and Dale, 2000). Prior work leverages intent and meaning representations (MR) to understand the content of the message (Young et al., 2013), but largely in task-oriented as opposed to open-ended 3.1 Approach NLU using Asks and Framing The representation we use to generate plans leverages asks and fr"
2020.findings-emnlp.247,C18-1312,0,0.0208261,", GPT-2, XLNet, BERT (Radford et al., 2018, 2019; Yang et al., 2019; Devlin et al., 2019) have helped achieve the SOTA performance across several natural language tasks. However, these models do not achieve the same level of consistent performance on generative modeling tasks as opposed to language understanding tasks (Ziegler et al., 2019; Edunov et al., 2019). Wolf et al. (2019) propose a transfer learning approach that fine tunes large pretrained language models and achieves SOTA scores on the PERSONA-chat dataset (Golovanov et al., 2019) and in the CONVAI2 competition (Dinan et al., 2019; Yusupov and Kuratov, 2018). Keskar et al. (2019) introduce a large-scale conditional transformer model that improves generation based on control codes. Our training paradigm is consistent with existing research that constrains large-scale language models across generation tasks (Rashkin et al., 2019; Urbanek et al., 2019) and yields controllable text generation (Shen et al., 2019; Zhou et al., 2017), with one key difference: we learn to plan and realize separately. Accordingly, we overview planning based approaches next. Planning-Based Approaches: A standard component of traditional NLG systems is a planner (Reiter and"
2020.findings-emnlp.247,P19-1566,0,0.0198767,"on of responses that are constrained by the response plan. In this phase, we only experiment with the Pseudo Self attention (PSA) model, based on Ziegler et al. (2019), who demonstrate that PSA outperforms other approaches on text generation tasks. We use nucleus sampling to overcome some of the drawbacks of beam search (Holtzman et al., 2020). 3.3 Corpora Our choice of corpora is driven by the presence of information elicitation and persuasive strategies in the utterances (i.e., asks and framings). Accordingly, we experiment with the AntiScam (Li et al., 2019) and Persuasion for Social Good (Wang et al., 2019) corpora. AntiScam contains dialogues about a customer service scenario and is specifically crowdsourced to understand human elicitation strategies. Persuasion for Social Good corpus contains interactions between workers who are assigned the roles of persuader and persuadee, 2738 AntiScam PSG 220 1017 Avg. Conversation Length 12.45 10.43 Avg. Utterance Length 11.13 19.36 Number of GIVE 2192 11587 Number of PERFORM 1681 7335 Number of GAIN 70 399 Number of LOSE 73 588 4376 8078 Number of Dialogues Number of RESPOND pling both in the planning and realization phase. All models are trained on two"
2020.findings-emnlp.247,D17-1239,0,0.0192667,"te that decoupling the process into planning and realization performs better than an end-to-end approach. 1 Figure 1: Example conversation between two speakers A & B where the response for the speaker B is generated based on the response plan from two learned planners: Context Attention and Pseudo Self Attention. Introduction Recent advancements in the area of generative modeling have helped increase the fluency of generative models. However, several issues persist: coherence of output and the semblance of mere repetition/hallucination of tokens from the training data (Moryossef et al., 2019; Wiseman et al., 2017). One reason could be that the generation task is typically construed as an end-to-end system. This is in contrast to traditional approaches, which incorporate a sequence of steps in the NLG system, including content determination, sentence planning, and surface realization (Reiter, 1994; Reiter and Dale, 2000). A review of literature from psycholinguistics and cognitive science also provides strong empirical evidence that the human language production process is not a monolith (Dell, 1985; Bock, 1996; Bock et al., 2007; Kennison, 2018). Prior approaches have indeed incorporated content planni"
2020.stoc-1.1,N03-1013,1,0.118022,"dentify the main action and arguments. For example, click here yields click as the ask and its argument here. Additional constraints are imposed through the use of a lexicon based on Lexical Conceptual Structure (LCS) (Dorr and Olsen, 2018; Dorr and Voss, 2018), derived from a pool of team members’ collected suspected scam/impersonation emails. Verbs from these emails were grouped as follows: • PERFORM: connect, copy, refer • GIVE: administer, contribute, donate • LOSE: deny, forget, surrender • GAIN: accept, earn, grab, win Additional linguistic processing includes: (1) categorial variation (Habash and Dorr, 2003) to map between different parts of speech, e.g., reference(N) → refer(V) enables detection of an explicit ask from you can reference your gift card; and (2) verbal processing to eliminate spurious asks containing verb forms such as sent or signing in sent you this email because you are signing up. 4.2.2 Motive Detection In addition to the use of distinct tools for detecting linguistic knowledge, Panacea extracts the attacker’s intention, or motive. Leveraging the attacker’s demands (asks), goals (framings) and message attack types (from the threat type classifier), the Motive Detection module"
2020.stoc-1.1,W18-3808,1,0.788683,"Missing"
2020.stoc-1.1,W18-1404,1,0.721686,"Missing"
2020.stoc-1.1,D09-1096,0,0.0526194,"Missing"
2020.stoc-1.1,P19-1247,0,0.0282452,"age the attacker to elicit identifying information, is the next advance in this arena. Prior work extracts information from email interactions (Dada et al., 2019), applies supervised learning to identify email signatures and forwarded messages (Carvalho and Cohen, 2004), and classifies email content into different structural sections (Lampert et al., 2009). Statistical and rulebased heuristics extract users’ names and aliases (Yin et al., 2011) and structured script representations determine whether an email resembles a password reset email typically sent from an organization’s IT department (Li and Goldwasser, 2019). Analysis of chatbot responses (Prakhar Gupta and Bigham, 2019) yields human-judgement correlation improvements. Approaches above differ from ours in that they require extensive model training. 1.1.1 Monitoring and Detection Panacea includes an initial protection layer based on the analysis of incoming messages. Conceptual users include end users and IT security professionals. Each message is processed and assigned a label of friend, foe, or unknown, taking into account headers and textual information of each message. The data obtained from this analysis is converted into threat intelligence"
2020.stoc-1.1,P14-5010,0,0.0108818,"d by state-ofthe-art systems in cyber threat intelligence. MISP (Wagner et al., 2016) focuses on information sharing from a community of trusted organizations. MITRE’s Collaborative Research Into Threats (CRITs) (Goffin, 2020) platform is, like Panacea, built on top of the Structured Threat Intelligence eXchange (STIX) specification. Panacea differs from these in that it is part of operational active defenses, rather than solely an analytical tool for incident response and threat reporting. 2 3 System Overview Panacea’s processing workflow is inspired by Stanford’s CoreNLP annotator pipeline (Manning et al., 2014a), but with a focus on using NLP to power active defenses against SE. A F3EAD-inspired phased analysis and engagement cycle is employed to conduct active defense operations. The cycle is triggered when a message arrives and is deconstructed into STIX threat intelligence objects. Object instances for the identities of the sender and all recipients are found or created in the knowledge base. Labeled relationships are created between those identity objects and the message itself. Once a message is ingested, plug-in components process the message in the find phase, yielding a response as a JSON o"
2020.stoc-1.1,W19-5944,0,0.0205723,"Missing"
2020.stoc-1.1,W18-5030,0,0.0283298,"ker, bulwarked by hopes of eventual payoff. Such requests are implemented as a collection of flag seeking strategies built on top of a conversational theory of asks. Flags are collected using information extraction techniques. Future work includes inferential logic and deception detection to unmask an attacker and separate them from feigned identities used to gain trust. 2 Our approach relates to work on conversational agents, e.g., response generation using neural models (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models (Dziri et al., 2018), self-disclosure for targeted responses (Ravichander and Black, 2018), topic models (Bhakta and Harris, 2015), and other NLP analysis (Sawa et al., 2016). All such approaches are limited to a pre-defined set of topics, constrained by the training corpus. Other prior work focuses on persuasion detection/prediction (Hidey and McKeown, 2018) but for judging when a persuasive attempt might be successful, whereas Panacea aims to achieve effective dialogue for countering (rather than adopting) persuasive attempts. Text-based semantic analysis is also used for SE detection (Kim et al., 2018), but not for engaging with an attacker. Whereas a bot might be employed to wa"
2020.stoc-1.2,W18-1404,1,0.779842,"ain through compliance or lack thereof. It should be noted that there is no one-to-one ratio between ask and framing in the ask/framing detection output. Given the content, there may be none, one or more asks and/or framings in the output. Our lexical organization is based on Lexical Conceptual Structure (LCS), a formalism that supports resource construction and extensions to new applications such as SE detection and response generation. Semantic classes of verbs with similar meanings (give, donate) are readily augmented through adoption of the STYLUS variant of LCS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018). We derive LCS+ from asks/framings and employ CATVAR (Habash and Dorr, 2003) to relate word variants (e.g., reference and refer). Table 1 illustrates LCS+ Ask/Framing output for three (presumed) SE emails: two PERFORM asks and one GIVE ask.1 Parentheses () refer to ask arguments, often a link that the potential victim might choose to click. Ask/framing outputs are provided to downstream response generation. For example, a possible response for Table 1(a) is I will contact asap. A comparison of LCS+ to two related resources shows that our lexical organization supports refinements, improves ask"
2020.stoc-1.2,habash-dorr-2002-handling,1,0.360012,"the SE task is to waste the attacker’s time, play along, and possibly extract information that could unveil their identity. GAIN: 13.5.1 Get: You are a winner of 1M Eu. 13.5.2 Obtain: You can recover your credit rating 4. Table 2: Lexical organization of LCS+ relies on Ask Categories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Ou"
2020.stoc-1.2,2006.amta-papers.7,1,0.45115,".2 Obtain: You can recover your credit rating 4. Table 2: Lexical organization of LCS+ relies on Ask Categories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic model"
2020.stoc-1.2,W18-5030,0,0.0251272,"abash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models produce focused responses (Dziri et al., 2018), self-disclosure yields targeted responses (Ravichander and Black, 2018), and SE detection employs topic models (Bhakta and Harris, 2015) and NLP of conversations (Sawa et al., 2016). However, all such approaches are limited to a pre-defined set of topics, constrained by the training corpus. Other prior work focuses on persuasion detection/ prediction (Hidey and McKeown, 2018) by leveraging argument structure, but for the purpose of judging when a persuasive attempt might be successful in subreddit discussions dedicated to changing opinions (ChangeMyView). Our work aims to achieve effective dialogue for countering (rather than adopting) persuasive attempts. Text-b"
2020.stoc-1.2,1985.tmi-1.17,0,0.202151,"Missing"
2020.stoc-1.2,W00-0207,0,0.0954748,"ategories (PERFORM, GIVE) and Framing Categories (GIVE, LOSE). Italicized exemplars with boldfaced triggers illustrate usage for each class. Boldfaced class numbers indicate those STYLUS classes that were modified to yield the LCS+ resource. Related Work LCS is used in interlingual machine translation (Voss and Dorr, 1995; Habash and Dorr, 2002), lexical acquisition 7 8 12 For brevity, excerpts are shown in lieu of full emails. LCS+ detects both GIVE/send and PERFORM/respond. Acknowledgments (Habash et al., 2006), cross-language information retrieval (Levow et al., 2000), language generation (Traum and Habash, 2000), and intelligent language tutoring (Dorr, 1997). STYLUS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018) systematizes LCS based on several studies (Levin and Rappaport Hovav, 1995; Rappaport Hovav and Levin, 1998), but to our knowledge our work is the first use of LCS in a conversational context, within a cyber domain. Our approach relates to work on conversational agents (CAs), where neural models automatically generate responses (Gao et al., 2019; Santhanam and Shaikh, 2019), topic models produce focused responses (Dziri et al., 2018), self-disclosure yields targeted responses (Ravichander"
2020.stoc-1.2,N03-1013,1,0.592437,"one-to-one ratio between ask and framing in the ask/framing detection output. Given the content, there may be none, one or more asks and/or framings in the output. Our lexical organization is based on Lexical Conceptual Structure (LCS), a formalism that supports resource construction and extensions to new applications such as SE detection and response generation. Semantic classes of verbs with similar meanings (give, donate) are readily augmented through adoption of the STYLUS variant of LCS (Dorr and Voss, 2018) and (Dorr and Olsen, 2018). We derive LCS+ from asks/framings and employ CATVAR (Habash and Dorr, 2003) to relate word variants (e.g., reference and refer). Table 1 illustrates LCS+ Ask/Framing output for three (presumed) SE emails: two PERFORM asks and one GIVE ask.1 Parentheses () refer to ask arguments, often a link that the potential victim might choose to click. Ask/framing outputs are provided to downstream response generation. For example, a possible response for Table 1(a) is I will contact asap. A comparison of LCS+ to two related resources shows that our lexical organization supports refinements, improves ask/framing detection and top ask identification, and yields qualitative improve"
2020.stoc-1.2,2003.mtsummit-systems.9,1,\N,Missing
2020.stoc-1.8,1985.tmi-1.17,0,0.174811,"Missing"
A00-1005,P96-1009,0,0.0452231,"Missing"
A00-1005,J99-3003,0,0.0989104,"ct syntax. Although these systems have been quite successful, they use detailed models of the domain and therefore cannot be used for diverse applications such as the ones required for customer service centers. Other related work on dialogue include (Carberry, 1990; Grosz and Sidner, 1986; Reichman, 1981). 1. Previous Work As mentioned earlier, some customer service centers now allow users to say either the option number or a keyword from a list of options/descriptions. However, the only known work which automates part of a customer service center using natural language dialogue is the one by Chu-Carroll and Carpenter (1999). The system described here is used as the front-end of a bank's customer service center. It routes calls by extracting key phrases from a user utterance and then by statistically comparing these phrases to phrases extracted from utterances in a training corpus consisting of pre-recorded calls where the routing was done by a human. The call is routed to the destination of the utterance from the training corpus that is most &quot;similar&quot; to the current utterance. On occasion, the system will interact with the user to clarify the user's request by asking a question. For example, if the user wishes t"
A00-1005,J86-3001,0,0.0265838,"gh a dialogue with the system, the TRIPS and the TRAINS projects allow users to plan their itineraries through dialogue. Duke's Pascal tutoring system helps students in an introductory programming class debug their programs by allowing them to analyze their syntax errors, get additional information on the error, and learn the correct syntax. Although these systems have been quite successful, they use detailed models of the domain and therefore cannot be used for diverse applications such as the ones required for customer service centers. Other related work on dialogue include (Carberry, 1990; Grosz and Sidner, 1986; Reichman, 1981). 1. Previous Work As mentioned earlier, some customer service centers now allow users to say either the option number or a keyword from a list of options/descriptions. However, the only known work which automates part of a customer service center using natural language dialogue is the one by Chu-Carroll and Carpenter (1999). The system described here is used as the front-end of a bank's customer service center. It routes calls by extracting key phrases from a user utterance and then by statistically comparing these phrases to phrases extracted from utterances in a training co"
A94-1028,H93-1071,0,0.0548045,"Missing"
A94-1028,H93-1070,0,0.111804,"ies. 5 Term Weighting Issues Finding a proper term weighting scheme is critical in term-based retrieval since the rank of a document is determined by the weights of the terms it shares with the query. One popular term weighting scheme, known as ffidf, weights terms proportionately to their inverted document frequency scores and to their in-document frequencies (tO. The in-document frequency factor is usually normalized by the document length, that is, it is more significant for a term to occur in a short 100-word abstract, than in a 5000-word article. 7 A standard ff.idf weighting scheme (see Buckley, 1993 for details) may be inappropriate for mixed term sets, consisting of ordinary concepts, proper names, and phrases, because: (1) It favors terms that occur fairly frequently in a document, which supports only general-type queries (e.g., &quot;all you know about X&quot;). Such queries were not typical in TREC. (2) It attaches low weights to infrequent, highly specific terms, such as names and phrases, whose only occurrences in a document are often decisive for relevance. Note that such terms cannot be reliably distinguished using their distribution in the database as the sole factor, and therefore syntac"
A94-1028,C92-1033,1,0.801964,"lt of this strategy is an approximate parse, partially fitted using top-down predictions. In runs with approximately 130 million words of TREC&apos;s Wall Street Journal and San Jose Mercury texts, the parser&apos;s speed averaged 30 minutes per Megabyte or about 80 words per second, on a Sun SparcStationl0. In addition, T I P has been shown to produce parse structures which are no worse 170 than those generated by full-scale linguistic parsers when compared to hand-coded parse trees. 5 Full details of TTP parser have been described in the TREC-1 report (Strzalkowski, 1993a), as well as in other works (Strzalkowski, 1992; Strzalkowski & Scheyen, 1993). As may be expected, the skip-and-fit strategy will only be effective if the input skipping can be performed with a degree of determinism. This means that most of the lexical level ambiguity must be removed from the input text, prior to parsing. We achieve this using a stochastic parts of speech tagger to preprocess the text prior to parsing. In order to streamline the processing, we also perform morphological normalization of words on the tagged text, before parsing. This is possible because the part-of-speech tags retain the information about each word&apos;s origi"
A94-1028,W93-0302,1,0.861256,"es (e.g., routing runs) low-quality terms had to be removed (or inhibited) before similar terms could be added to the query or else the effect of query expansion was all but drowned out by the increased noise. 2 Overall Design We have established the general architecture of a NLP-IR system, depicted schematically below, in which an advanced NLP module is inserted between the textual input (new documents, user queries) and the database search engine (in our case, NIST&apos;s PRISE system). This design has already shown some promise in producing a better performance than the base statistical system (Strzalkowski, 1993b). We would like to point out at the outset that this system is completely automated, including the statistical core, and the natural language processing components, and no human intervention or manual encoding is required. NIP: TAGGER PARSER After the final query is constructed, the database search follows, and a ranked list of documents is returned. It should be noted that all the processing steps, those performed by the backbone system, and those performed by the natural language processing components, are fully automated, and no human intervention or manual encoding is required. 3 Fast Pa"
A94-1028,1993.iwpt-1.23,1,0.721807,"is an approximate parse, partially fitted using top-down predictions. In runs with approximately 130 million words of TREC&apos;s Wall Street Journal and San Jose Mercury texts, the parser&apos;s speed averaged 30 minutes per Megabyte or about 80 words per second, on a Sun SparcStationl0. In addition, T I P has been shown to produce parse structures which are no worse 170 than those generated by full-scale linguistic parsers when compared to hand-coded parse trees. 5 Full details of TTP parser have been described in the TREC-1 report (Strzalkowski, 1993a), as well as in other works (Strzalkowski, 1992; Strzalkowski & Scheyen, 1993). As may be expected, the skip-and-fit strategy will only be effective if the input skipping can be performed with a degree of determinism. This means that most of the lexical level ambiguity must be removed from the input text, prior to parsing. We achieve this using a stochastic parts of speech tagger to preprocess the text prior to parsing. In order to streamline the processing, we also perform morphological normalization of words on the tagged text, before parsing. This is possible because the part-of-speech tags retain the information about each word&apos;s original form. Thus the sentence The"
A94-1028,J90-1003,0,\N,Missing
A94-1028,H91-1068,1,\N,Missing
A94-1028,P90-1034,0,\N,Missing
A97-1044,H93-1070,0,0.0288142,"head of the subject phrase and the main verb. These types of pairs account for most of the syntactic variants for relating two words (or simple phrases) into pairs carrying compatible semantic content. This also gives the pair-based representation sufficient flexibility to effectively capture content elements even in complex expressions. Long, complex phrases are similarly decomposed into collections of pairs, using corpus statistics to resolve structural ambiguities. 301 3.2 Linguistic Phrase Stream sion, whereas lnc.ntc slightly sacrifices the average precision, but gives better recall (see Buckley, 1993). We used a regular expression pattern matcher on the part-of-speech tagged text to extract noun groups and proper noun sequences. The major rules we used are: 1. a sequence of modifiers (vbnlvbgljj) followed by at least one noun, such as: &quot;cryonic suspend&quot;, &quot;air traffic control system&quot;; 2. proper noun(s) modifying a noun, such as: &quot;u.s. citizen&quot;, &quot;china trade&quot;; 3. proper noun(s) (might contain &apos;&&apos;), such as: &quot;warren commission&quot;, &quot;national air traffic controller&quot;. In these experiments, the length of phrases was limited to maximum 7 words. 3.3 N a m e S t r e a m Proper names, of people, places"
C04-1189,M98-1007,0,0.128606,"Missing"
C04-1189,N03-1004,0,0.0326839,"Missing"
C04-1189,W00-0303,0,0.115113,"Missing"
C04-1189,N04-4014,1,0.094326,"Missing"
C04-1189,C96-2157,1,0.726879,"cues and the expected entity types, which are domain adaptable. Domain adaptation is desirable for obtaining more focused dialogue, but it is not necessary for HITIQA to work. We used both setups under different conditions: the generic frames were used with TREC document collection to measure impact of IR precision on QA accuracy (Small et al., 2004). The domain-adapted frames were used for sessions with intelligence analysts working with the WMD Domain (see below). Currently, the adaptation process includes manual tuning followed by corpus bootstrapping using an unsupervised learning method (Strzalkowski & Wang, 1996). We generally rely on BBN’s Identifinder for extraction of basic entities, and use bootstrapping to define additional entity types as well as to assign roles to attributes. The version of HITIQA reported here and used by analysts during the evaluation has been adapted to the Weapons of Mass Destruction NonProliferation domain (WMD domain, henceforth). Figure 3 contains an example passage from this data set. In the WMD domain, the typed frames were mapped onto WMDTransfer 3-role frame, and two 2-role frames WMDTreaty and WMDDevelop. Adapting the frames to the WMD domain required very minimal m"
C10-1117,J99-1001,0,0.241255,"and classify social language uses in multi-party dialogue. 2. Related Research Issues related to linguistic manifestation of social phenomena have not been systematically researched before in computational linguistics; indeed, most of the effort thus far was directed towards the communicative dimension of discourse. While the Speech Acts theory (Austin, 1962; Searle, 1969) provides a generalized framework for multiple levels of discourse analysis (locution, illocution and perlocution), most current approaches to dialogue focus on information content and structural components (Blaylock, 2002; Carberry & Lambert, 1999; Stolcke, et al., 2000) in dialogue; few take into account the effects that speech acts may have upon the social 1038 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038–1046, Beijing, August 2010 roles of discourse participants. Also relevant is research on modeling sequences of dialogue acts – to predict the next one (Samuel et al. 1998; Ji & Bilmes, 2006 inter alia) – or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), which are attempts to formalize participants’ roles in conversation (e.g., Linell, 1"
C10-1117,P05-2014,0,0.0137442,"Missing"
C10-1117,N06-1036,0,0.051279,"Missing"
C10-1117,W03-2118,0,0.0235703,"Missing"
C10-1117,shaikh-etal-2010-mpc,1,0.886958,"Missing"
C10-1117,J00-3003,0,0.178483,"Missing"
C10-1117,webb-etal-2008-cross,1,0.880529,"Missing"
C10-1117,C10-2150,1,\N,Missing
C10-1117,H05-1044,0,\N,Missing
C10-1117,E03-1072,0,\N,Missing
C10-1117,P98-2188,0,\N,Missing
C10-1117,C98-2183,0,\N,Missing
C12-1155,J99-1001,0,0.0607346,"ons. Any member of the expert panel may exhibit the sociolinguistic behavior consistent with being an influencer. In a peer-oriented group discussion however, it could occur that the task and thought leader (leader and influencer) are the same person. Human-human interaction affords a rich resource for research. Much prior work has been done in communication that focuses on the communicative dimension of discourse. For example, the Speech Act theory (Austin, 1962; Searle 1969) provides a generalized framework of multiple levels of discourse analysis; work on dialogue analysis (Blaylock, 2002; Carberry and Lambert, 1999; Stolcke et al., 2000) focuses on information content and structure of dialogues. Somewhat more relevant to social roles is research that models sequences of dialogue acts (Bunt, 1994), in order to predict the next dialogue act (Samuel et al. 1998; Stolcke, et al., 2000; Ji & Bilmes, 2006, inter alia) or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), from which participants’ functional roles in conversation (though not social roles) may be extrapolated (e.g., Linell, 1990; Poesio and Mikheev, 1998; Field et al., 2008). However, the effects of speech acts"
C12-1155,H92-1086,0,0.379027,"Missing"
C12-1155,P11-2059,0,0.0128008,"ying online chat relies on the more explicit linguistic devices necessary to convey social and cultural nuances than is typical in face-to-face or telephonic conversations. The use of language by participants as a feature to determine interpersonal relations has been studied by Bracewell et al. (2011) who developed a learning framework to determine collegiality between discourse participants. Their approach, however, looks at singular instances of linguistic markers or single utterances rather than a sustained demonstration of sociolinguistic behavior over the 2536 course of entire discourse. Freedman et al. (2011) have developed an approach that takes into account the entire discourse to detect behaviors such as persuasion; however their analysis is conducted on and models developed upon online discussion threads where the social phenomena of interest may be rare. By contrast, we build our models based on analysis of a data corpus of online chat discourse, where data collection experiments were specifically designed so that the resulting corpus may be rich in sociolinguistic phenomena. Our research extends the work of Strzalkowski et al. (2010) and Broadwell et al. (2012), who first proposed the two-ti"
C12-1155,liu-etal-2012-extending,1,0.822134,"Missing"
C12-1155,P98-2188,0,0.0301205,"n-human interaction affords a rich resource for research. Much prior work has been done in communication that focuses on the communicative dimension of discourse. For example, the Speech Act theory (Austin, 1962; Searle 1969) provides a generalized framework of multiple levels of discourse analysis; work on dialogue analysis (Blaylock, 2002; Carberry and Lambert, 1999; Stolcke et al., 2000) focuses on information content and structure of dialogues. Somewhat more relevant to social roles is research that models sequences of dialogue acts (Bunt, 1994), in order to predict the next dialogue act (Samuel et al. 1998; Stolcke, et al., 2000; Ji & Bilmes, 2006, inter alia) or to map them onto subsequences or “dialogue games” (Carlson 1983; Levin et al., 1998), from which participants’ functional roles in conversation (though not social roles) may be extrapolated (e.g., Linell, 1990; Poesio and Mikheev, 1998; Field et al., 2008). However, the effects of speech acts on social behaviors and roles of conversation participants have not been systematically studied. Research in anthropology and communication has concentrated on how certain social norms and behaviors may be reflected in language (e.g., Scollon and"
C12-1155,shaikh-etal-2010-mpc,1,0.928668,"nor role to play in computing Influence, and hence we do not include them while combining behaviors. Similarly, while Task Control and Disagreement are most indicative of Task Leadership, other behaviours such as Network Centrality and Argument Diversity do not correlate with this role. Hence, we do not include them in computation of Leadership. We shall elaborate on this in Section 5, Evaluation and Results. 2540 3 Corpus, Annotation and Computational Modules The models described in this paper are derived from online chat dialogues. The corpus we use for this analysis is the MPC chat corpus (Shaikh et al., 2010, Liu et al., 2012). This is a corpus of over 90 hours of online chat dialogues in English, Urdu and Mandarin. Participants in these chats are native speakers of these languages. Each chat session is a task-oriented dialogue around 90 minutes in length, with at least 4 participants. This corpus is particularly useful for the type of sociolinguistic analysis we are interested in due to the characteristics of interaction in each chat session – the participants are focused on some task, they form a fairly stable group and the dynamics of conversation unfold naturally through discourse. Other corp"
C12-1155,W04-2319,0,0.0395466,"over 90 hours of online chat dialogues in English, Urdu and Mandarin. Participants in these chats are native speakers of these languages. Each chat session is a task-oriented dialogue around 90 minutes in length, with at least 4 participants. This corpus is particularly useful for the type of sociolinguistic analysis we are interested in due to the characteristics of interaction in each chat session – the participants are focused on some task, they form a fairly stable group and the dynamics of conversation unfold naturally through discourse. Other corpora exist such as the ICSI-MRDA corpus (Shriberg et al., 2004) and the AMI meeting corpus (Carletta, 2007), however these are spoken language resources rather than online chat. Where other corpora of online chat do exist, like the NPS Internet chat corpus (Forsyth and Martell, 2007) and StrikeCom corpus (Twitchell et al., 2004), they do not contain any information about the participants themselves or their reactions to the discussion. In order to create a ground truth of assessments of sociolinguistic behavior, we needed certain information to be captured through questionnaires or survey following each data collection session. In the data that comprise M"
C12-1155,J00-3003,0,0.308716,"Missing"
C12-1155,C10-1117,1,0.852338,"olinguistic behavior over the 2536 course of entire discourse. Freedman et al. (2011) have developed an approach that takes into account the entire discourse to detect behaviors such as persuasion; however their analysis is conducted on and models developed upon online discussion threads where the social phenomena of interest may be rare. By contrast, we build our models based on analysis of a data corpus of online chat discourse, where data collection experiments were specifically designed so that the resulting corpus may be rich in sociolinguistic phenomena. Our research extends the work of Strzalkowski et al. (2010) and Broadwell et al. (2012), who first proposed the two-tiered approach to sociolinguistic modeling and have demonstrated that a subset of mid-level sociolinguistic behaviors may be accurately inferred by a combination of low-level language features. We have adopted their approach and extended it to modeling of leadership and influence. Furthermore, we enhanced the method by adding the evidence learnt from correlations of indices and measures to compute weights through which sociolinguistic behaviors may be combined appropriately to infer higher-level social phenomena. In this paper, we descr"
C12-1155,C10-2150,0,0.023678,"Missing"
C92-1033,A88-1019,0,0.18188,"Missing"
C92-1033,J90-1003,0,0.0757036,"Missing"
C92-1033,P90-1031,0,0.0662819,"Missing"
C92-1033,P91-1030,0,0.0182559,"of the purser&apos;s accuracy. For example, we may have to settle for partial parsing that would recognize only selected grammatical structures (e.g. noun phrases; Ruge et al., 1991), or would avoid making difficult decisions (e.g. pp-attachment; Hindle, 1983). Much of the overhead and inefficiency comes from the fact that the lexical and structural ambiguity of natmal language input can only be dealt with using limited context information available to the parser. Partial parsing techniques have been used with a considerable success in processing large volumes of text, for example AT&T&apos;s Fidditch (Hindle and Rooth, 1991) parsed 13 million words of Associated Press news messages, while MIT&apos;s parser (de Marcken, 1990) was used to process the 1 million word Lancaster/Oslo/Bergen (LOB) corpus. In both cases, the parsers were designed to do partial processing only, that is, they would never attempt a complete analysis of certain constructions, such as the attachment of pp-adjuncts, subordinate clauses, or coordinations. This kind of partial analysis may be sufficient in some applications because of a relatively high precision of identifying correct syntactic dependencies. 2 However, the ratio at which these depend"
C92-1033,J83-3002,0,0.0689659,"Missing"
C92-1033,H91-1065,0,0.122669,"Missing"
C92-1033,J81-4005,0,0.111166,"er selections. In the following example, chinese, which does not appear in the dictionary, is tagged as &quot;j.j&quot;:~ this(dO papca&apos;(nn) dates(vbz) back(rb) the(d0 genesis(nn) of(in) binary(j/) conception(nn) circa(/n) 5000(cd) years(nns) ago(rb) ,(corn) as(rb) derived(vbn) by(m) the(d0 chinese(if) ancients(nns) .(per) We use a stochastic tagger to process the input text prior to parsing. The tagger is based upon a bigram model; it selects most likely tag for a word given co-occurrence probabilities computed from a small training SgL7 TTP is based on the Linguistic String G r a m m a r developed by Sager (1981). Written in Quintus Prolog, the parser currently encompasses more than 400 grammar productions, s T I P produces a regularized representation of each lmrsed sentence that reflects the sentence&apos;s logical structure. This representation may differ considerably from a standard Imrse tree, in that the constituents get moved around (e.g., de. passivization, de--dativization), and the phrases are organized recursively around their head elements. An important novel feature of T I P parser is that it is equipped with a time-out mechanism that allows for fast closing of more difficult sub-constituents"
C92-1033,P92-1014,1,0.685633,"abyte of text (approx. 150 million words) in about 40 days, on a 21 MIPS computer. The parser is based on a wide coverage grammar for English, and it contains a powerful skip-and-fit recovery mechanism that allows it to deal with unexpected input and to perform effectively under a severe time pressure. Prior to parsing, the input text is tagged with a stochastic tagger that assigns part-of-speech labels to every word, thus resolving lexical level ambiguity. T I P has been used as front-end of a natural language processing component to a traditional document-based information retrieval system (Strzalkowski and Vauthey, 1992). The parse structures were further analyzed to extract word and phrase dependency relations which were in turn used as input to various statistical and indexing processes. The results obtained were generally satisfactory: an improvement in both recall and precision of document retrieval have been observed. At present, we are also conducting experiments with large corpora of technical computer ,science texts in order to extract domain-specific ,7Some sentences(1 in 5000)mmystill fail to parseif tagging errorsare.compotmdedin In unexpectedway. ts Although parsing of some sentences may now appro"
C92-1033,J93-1005,0,\N,Missing
C94-1100,H94-1072,1,0.885568,"Missing"
C94-1100,J90-1003,0,\N,Missing
C94-1100,J86-3002,0,\N,Missing
C94-1100,H93-1102,1,\N,Missing
C94-1100,P90-1034,0,\N,Missing
C94-1100,C92-1033,1,\N,Missing
C94-1100,W93-0302,1,\N,Missing
C96-2157,H92-1022,0,0.0400949,"Missing"
C96-2157,P91-1034,0,0.0481289,"Missing"
C96-2157,H91-1065,0,0.0475964,"Missing"
C96-2157,P95-1026,0,0.210527,"tes neutral evidence, which is of little or no consequeuce to the spotter. In general, we take SW(t) &gt; e &gt; 0 as a piece of positive evidence, and SW(t) &lt; - e as a piece of negative evidence, as provided by item t. Weights of evidence items within an evidence set are then combined to arrive at the compound context weight which is used to accept or reject candidate phrase. At this time, we make no claim as to whether (1) is an optimal fornmla for cah:ulating evidence weights. An alternative method we considered was to estimate certain conditional probabilities, similarly to the formula used in (Yarowsky, 1995): The set of evidence items generated for this fl&apos;aginent, i.e., E1 UE2 UEaUE4, contains the following elements: (boys, p), (kicked, p), (the, s), (door, s), (with, f), (rage , f), ((boys, kicked), p), ((the, door)), s), ((with, ,&apos;age), f), (boys, p, 2), (ki&ed, p, 1), (the, s, 2), (door, s, 1), (with, f, 1), (rage, f, 2), ((boys, kicked), p, 1), ((the, door)), s, 1), ((with, ,&apos;age), f, 1) towards or against the hyphothesis that the central unit belongs to the semantic category of interest to the spotter. The significance weights are acquired through corpus-based training. Training Evidence it"
C98-2200,C96-2157,1,0.756279,"here Sh is a minor score calculated using metric h; wh is the weight reflecting how effective this metric is in general; l is the length of the segment. The following metrics are used to score passages considered for the main news section of the summary DMS. We list here only the criteria which are the 4Refer to (Euhn 1958) (Paice 1990) (Kau, Brandow & Mitze 1994) (Kupiec, Pedersen & Chen 1995) for sentence-based summarization approaches. nThe weights Wh are trainable in a supervised mode, given a corpus of texts and their summaries, or in an unsupervised mode as described in (Strzalkowski &: Wang 1996). For the purpose of tile experiments described here, these weights have been set manually. 1261 most relevant for generating summaries in context of an information retrieval system. Both the French and Iranian g o v e r n m e n t s acknowledged the I r a n i a n role in the release of the three French hostages, J e a n - P a u l Kauffmann, Marcel C a r t o n and Marcel Fontaine, 1. Words and phrases frequently occurring in a text are likely to be indicative of its content, especially if such words or phrases do not occur ofien elsewhere in the database. A weighted frequency score, similar to"
C98-2200,C94-1056,0,\N,Missing
E12-1030,P11-1113,0,0.283624,"Missing"
E12-1030,P08-1030,0,0.423182,"Missing"
E12-1030,P10-1081,0,0.295444,"Missing"
E12-1030,W02-1028,0,0.146322,"Missing"
E12-1030,A00-1039,0,0.0605015,"(Lin, 1998), that recognizes dependency relations between words is quite sufficient for deriving head-modifier relations and thus for construction of event templates. Event templates are obtained by stripping the parse tree of modifiers while preserving the basic dependency structure as shown in Figure 1, which is a stripped down parse tree of, “Also Monday, Israeli soldiers fired on four diplomatic vehicles in the northern Gaza town of Beit Hanoun, said diplomats” The model proposed here represents a significant advance over the current methods for relation extraction, such as the SVO model (Yangarber, et al. 2000) and its extension, e.g., the chain model (Sudo, et al. 2001) and other related variants (Riloff, 1996) all of which lack the expressive power to accurately recognize and represent complex event descriptions and to support successful machine learning. While Sudo’s subtree model (2003) overcomes some of the limitations of the chain models and is thus conceptually closer to our method, it nonetheless lacks efficiency required for practical applications. We represent complex relations as tree-like structures anchored at an event trigger (which is usually but not necessarily the main verb) with br"
E12-1030,H01-1009,0,0.766344,"quite sufficient for deriving head-modifier relations and thus for construction of event templates. Event templates are obtained by stripping the parse tree of modifiers while preserving the basic dependency structure as shown in Figure 1, which is a stripped down parse tree of, “Also Monday, Israeli soldiers fired on four diplomatic vehicles in the northern Gaza town of Beit Hanoun, said diplomats” The model proposed here represents a significant advance over the current methods for relation extraction, such as the SVO model (Yangarber, et al. 2000) and its extension, e.g., the chain model (Sudo, et al. 2001) and other related variants (Riloff, 1996) all of which lack the expressive power to accurately recognize and represent complex event descriptions and to support successful machine learning. While Sudo’s subtree model (2003) overcomes some of the limitations of the chain models and is thus conceptually closer to our method, it nonetheless lacks efficiency required for practical applications. We represent complex relations as tree-like structures anchored at an event trigger (which is usually but not necessarily the main verb) with branches extending to the event attributes (which are usually n"
E12-1030,P03-1029,0,0.652116,"Missing"
E12-1030,C96-2157,1,0.608192,"will occur in a variety of linguistic contexts, and some of these contexts may provide support for creating alternative extraction rules. When the new rules are subsequently applied to the text corpus, additional instances of the target concepts will be identified, some of which will be positive and some not. As this process continues to iterate over, the system acquires more extraction rules, fanning out from the seed set until no new rules can be learned. Thus defined, bootstrapping has been used in natural language processing research, notably in word sense disambiguation (Yarowsky, 1995). Strzalkowski and Wang (1996) were first to demonstrate that the technique could be applied to adaptive learning of named entity extraction 296 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 296–305, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics rules. For example, given a “naïve” rule for identifying company names in text, e.g., “capitalized NP followed by Co.”, their system would first find a large number of (mostly) positive instances of company names, such as “Henry Kauffman Co.” From the context surrounding eac"
E12-1030,M91-1028,0,\N,Missing
E12-1030,P95-1026,0,\N,Missing
E12-1030,P07-1074,0,\N,Missing
E12-1030,H92-1045,0,\N,Missing
E12-1030,M98-1011,0,\N,Missing
H91-1068,J90-1003,0,0.216643,"s Advanced Learner&apos;s Dictionary (OALD) or Longman&apos;s Dictionary of Contemporary English (LDOCE), both available on-line, are usually quite limited in their coverage of domain specific vocabulary, including domain-specific use of common words as well as technical terminology. Statistical methods for word clustering may provide a partial solution to this problem given a sufficient amount of textual data that display a certain uniformity of subject matter and style. These problems have been studied to some extent within the sublanguage paradigm [4,5], and also using elements of informarion theory [2,6]. One general problem with the latter approach is that information theory, which deals with code transmission, 5 Lewis and Croft [71 define the syntactic phrase as &quot;any pxir of non-function words in a sentence that are heads of syntactic structures connected by a grammatical relation.&quot; 6 Partial processing may include tagging and/or a limited parsing, see, for example [7], and also [9] for a more comprehensive view. SKN&apos;r~Nc~: The techniques are discussed and related to a general tape manipulation routine. parallel than to the word system. This relationship can change if the phrase is found in"
H91-1068,J86-3002,0,\N,Missing
H91-1068,P85-1037,0,\N,Missing
H91-1068,P90-1034,0,\N,Missing
H92-1040,J90-1003,0,\N,Missing
H92-1040,H91-1068,1,\N,Missing
H92-1040,C92-1033,1,\N,Missing
H94-1072,P92-1014,1,0.904319,"Missing"
H94-1072,H93-1070,0,0.0687692,"Missing"
H94-1072,C92-1033,1,\N,Missing
H94-1072,A94-1028,1,\N,Missing
H94-1072,H94-1110,1,\N,Missing
H94-1110,J90-1003,0,\N,Missing
H94-1110,H93-1071,0,\N,Missing
H94-1110,H91-1068,1,\N,Missing
H94-1110,H93-1070,0,\N,Missing
H94-1110,P90-1034,0,\N,Missing
H94-1110,C92-1033,1,\N,Missing
H94-1110,W93-0302,1,\N,Missing
J89-3003,C86-1086,1,0.557288,"Missing"
L16-1180,bestgen-2008-building,0,0.785704,"(2004) used WordNet (Miller et al. 1995) to assign positive or negative polarity to words using synonyms and antonyms for a small set of seed words, however, such a method is limited by the set of seed words chosen. Esuli and Sebastini (2006) used semi-supervised learning to create SentiWordNet, where potentially every word in WordNet would be assigned a sentiment score, although many words actually may not be sentiment-bearing (cf. Taboada, 2011 for further discussion). A number of approaches use semantic proximity of words in variations of Latent Semantic Analysis (Turney and Littman, 2003; Bestgen, 2008; Bestgen and Vincze, 2012); however, their self-reported correlations of proposed expansions against human ratings are not sufficiently robust. Neilson (2011) created a new ANEW specifically geared towards detecting sentiment in microblog posts, but it only contains 2477 words scored manually on a scale of +2 (positive) to -2 (negative). 3. Approach Our expansion method follows that adopted by Liu et al. (2014) for their automated expansion of the MRC psycholinguistic database. We use WordNet (Miller, 1995), a large English lexical database with over 150,000 words, hierarchically organized in"
L16-1180,esuli-sebastiani-2006-sentiwordnet,0,0.112714,"Missing"
L16-1180,C04-1200,0,0.0155118,"e our method of automatically expanding an existing affective lexicon for English. Our approach is general enough to apply to any specialized lexicon in any language where a partial resource exists. We also describe a method of automatically creating lexicons for a new language where no prior resources exist or are very limited; specifically, for Spanish, Russian and Farsi. We also describe the validation procedures used to ensure validity of these lexicons1. 2. Related Work There has been prior work in the automatic construction and expansion of affective lexicons using different techniques. Kim and Hovy (2004) used WordNet (Miller et al. 1995) to assign positive or negative polarity to words using synonyms and antonyms for a small set of seed words, however, such a method is limited by the set of seed words chosen. Esuli and Sebastini (2006) used semi-supervised learning to create SentiWordNet, where potentially every word in WordNet would be assigned a sentiment score, although many words actually may not be sentiment-bearing (cf. Taboada, 2011 for further discussion). A number of approaches use semantic proximity of words in variations of Latent Semantic Analysis (Turney and Littman, 2003; Bestge"
L16-1180,J11-2001,0,0.0249183,"Missing"
L16-1180,1996.amta-1.36,0,0.492001,"Missing"
L16-1594,W13-0909,1,0.901166,"Missing"
L18-1110,C12-1155,1,0.637315,"nversation The participants were ranked based on their influence, thus helping us select the influencer in each group along with scoring of every participant on various component metrics such as Network Centrality, Disagreement Measure, Topic Control, Involvement, among others, that contribute to the assessment of the degree of influence (Strzalkowski et al., 2013). All these attributes were computed for each conversation and participant. We describe the key metrics below. For a more detailed explanation, and how the scores are combined, the reader is referred to (Broadwell et al., 2012) and (Strzalkowski et al., 2012). Network Centrality (NC): It is the measure of degree to which a participant in the conversation is a “center” of communication in that group. A participant has a high degree of Network Centrality when other participants address more of their utterances towards her or him, and whose topics are most discussed by others. Disagreement Measure (DM): Disagreements with others is a way to control the topic of conversation by way of identifying or correcting what the participant sees as a problem. The more disagreement a participants shows, relative to other participants, the higher his/her Disagree"
L18-1110,C10-1117,1,0.861218,"Missing"
lin-etal-2012-revealing,W99-0705,0,\N,Missing
lin-etal-2012-revealing,W00-1308,0,\N,Missing
lin-etal-2012-revealing,J95-4004,0,\N,Missing
liu-etal-2012-extending,shaikh-etal-2010-mpc,1,\N,Missing
liu-etal-2012-extending,W09-3404,0,\N,Missing
liu-etal-2012-extending,song-etal-2010-enhanced,0,\N,Missing
liu-etal-2014-automatic-expansion,W13-0909,1,\N,Missing
N04-4014,M98-1007,0,0.818866,"Missing"
N04-4014,C96-2157,1,0.525869,"cues and the expected entity types, which are domain adaptable. Domain adaptation is desirable for obtaining more focused dialogue, but it is not necessary for HITIQA to work. We used both setups under different conditions: the generic frames were used with TREC document collection to measure impact of IR precision on QA accuracy (Small et al., 2004). The domain-adapted frames were used for sessions with intelligence analysts working with the WMD Domain (see below). Currently, the adaptation process includes manual tuning followed by corpus bootstrapping using an unsupervised learning method (Strzalkowski & Wang, 1996). We generally rely on BBN’s Identifinder for extraction of basic entities, and use bootstrapping to define additional entity types as well as to assign roles to attributes. The version of HITIQA reported here and used by analysts during the evaluation has been adapted to the 2 Scalability is certainly an outstanding issue here, and we are working on effective frame acquisition methods, which is outside of the scope of this paper. Weapons of Mass Destruction Non-Proliferation domain (WMD domain, henceforth). Figure 1b contains an example passage from this data set. In the WMD domain, the typed"
N04-4014,N04-4014,1,0.0512688,"Missing"
N04-4014,N03-1004,0,\N,Missing
P04-1010,W00-0303,0,0.0445695,"DARPA Communicator program has been instrumental in bringing about practical implementations of spoken dialogue systems. Systems developed under this program include CMU’s script-based dialogue manager, in which the travel itinerary is a hierarchical composition of frames (Xu and Rudnicky, 2000). The AT&T mixed-initiative system uses a sequential decision process model, based on concepts of dialog state and dialog actions (Levin et al., 2000). MIT’s Mercury flight reservation system uses a dialogue control strategy based on a set of ordered rules as a mechanism to manage complex interactions (Seneff and Polifroni, 2000). CU’s dialogue manager is event-driven, using a set of hierarchical forms with prompts associated with fields in the forms. Decisions are based not on scripts but on current context (Ward and Pellom, 1999). Our data-driven strategy is similar in spirit to that of CU. We take a statistical approach, in which a large body of transcribed, annotated conversations forms the basis for task identification, dialogue act recognition, and form filling for task completion. 3 System Architecture and Components The Amitiés system uses the Galaxy Communicator Software Infrastructure (Seneff et al., 1998)."
P04-1010,W00-0309,0,0.043477,"ialogue research efforts include the TRAINS project and the DARPA Communicator program. The classic TRAINS natural-language dialogue project (Allen et al., 1995) is a plan-based system which requires a detailed model of the domain and therefore cannot be used for a wide-ranging application such as financial services. The US DARPA Communicator program has been instrumental in bringing about practical implementations of spoken dialogue systems. Systems developed under this program include CMU’s script-based dialogue manager, in which the travel itinerary is a hierarchical composition of frames (Xu and Rudnicky, 2000). The AT&T mixed-initiative system uses a sequential decision process model, based on concepts of dialog state and dialog actions (Levin et al., 2000). MIT’s Mercury flight reservation system uses a dialogue control strategy based on a set of ordered rules as a mechanism to manage complex interactions (Seneff and Polifroni, 2000). CU’s dialogue manager is event-driven, using a set of hierarchical forms with prompts associated with fields in the forms. Decisions are based not on scripts but on current context (Ward and Pellom, 1999). Our data-driven strategy is similar in spirit to that of CU."
P04-1010,J99-3003,0,\N,Missing
P04-1010,W03-0704,1,\N,Missing
P06-1147,P01-1052,0,0.0139152,"escribes how to build a batch QA system by augmenting the TREC QA system with question clustering and answer co-occurrence maximization. Section 4.3 describes the experiments and explains the experimental results. Finally we conclude with the discussion of future work. 2 Related Work During recent years, many automatic QA systems have been developed and the techniques used in these systems cover logic inference, syntactic relation analysis, information extraction and proximity search, some systems also utilize pre-compiled knowledge base and external online knowledge resource. The LCC system (Moldovan & Rus, 2001; Harabagiu et al. 2004) uses a logic prover to select answer from related passages. With the aid of extended WordNet and knowledge base, the text terms are converted to logical forms that can be proved to match the question logical forms. The IBM’s PIQUANT system (Chu-Carroll et al, 2003; Prager et al, 2004) adopts a QA-byDossier-with-Constraints approach, which utilizes the natural constraints between the answer to the main question and the answers to the auxiliary questions. Syntactic dependency matching has also been applied in many QA systems (Cui et al, 2005; Katz and Lin 2003). The synt"
P06-1147,P04-1073,0,0.024007,"automatic QA systems have been developed and the techniques used in these systems cover logic inference, syntactic relation analysis, information extraction and proximity search, some systems also utilize pre-compiled knowledge base and external online knowledge resource. The LCC system (Moldovan & Rus, 2001; Harabagiu et al. 2004) uses a logic prover to select answer from related passages. With the aid of extended WordNet and knowledge base, the text terms are converted to logical forms that can be proved to match the question logical forms. The IBM’s PIQUANT system (Chu-Carroll et al, 2003; Prager et al, 2004) adopts a QA-byDossier-with-Constraints approach, which utilizes the natural constraints between the answer to the main question and the answers to the auxiliary questions. Syntactic dependency matching has also been applied in many QA systems (Cui et al, 2005; Katz and Lin 2003). The syntactic dependency relations of a candidate sentence are matched against the syntactic dependency relations in the question in order to decide if the candidate sentence contains the answer. Although surface text pattern matching is a comparatively simple method, it is very efficient for simple factoid questions"
P06-1147,P03-1003,0,0.0607764,"Missing"
P06-1147,P02-1006,0,0.0952812,"Missing"
P06-1147,H05-1038,0,\N,Missing
P11-1018,W01-1515,0,0.0399394,"notations. This has the added advantage of allowing the annotator to see the relationships between chat, behavior, and location/movement. This paper will describe our annotation process and the RAT tool. 2 Related Work Annotation tools have been built for a variety of purposes. The CSLU Toolkit (Sutton et al., 1998) is a suite of tools used for annotating spoken language. Similarly, the EMU System (Cassidy and Harrington, 2001) is a speech database management system that supports multi-level annotations. Systems have been created that allow users to readily build their own tools such as AGTK (Bird et al., 2001). The multi-modal tool DAT (Core and Allen, 1997) was developed to assist testing of the DAMSL annotation scheme. With DAT, annotators were able to listen to the actual dialogues as well as view the transcripts. While these tools are all highly effective for their respective tasks, ours is unique in its synchronized view of both event action and chat utterances. Although researchers studying online communication use either off-the shelf qualitative data analysis programs like Atlas.ti or NVivo, a few studies have annotated chat using custom-built tools. One approach uses computer-mediated disc"
P11-1018,shaikh-etal-2010-mpc,1,0.830772,"game action on the 2D display, the associated utterances and events populated the Chat and Event panel. Figure 6: Chat & Event Panel 4.3 The Annotator Panels The Annotator Panels (Figures 7 and 10) contains all features needed for the annotator to quickly annotate the events and dialogue. Annotators could choose from a number of categories to label each dialogue utterance. Coding categories included communicative links, dialogue acts, and selected multi-avatar actions. In the following we briefly outline each of these. A more detailed description of the chat annotation scheme is available in (Shaikh et al., 2010). 4.3.1 Communicative Links One of the challenges in multi-party dialogue is to establish which user an utterance is directed towards. Users do not typically add addressing information in their utterances, which leads to ambiguity while creating a communication link between users. With this annotation level, we asked the annotators to determine whether each utterance was addressed to some user, in which case they were asked to mark which specific user it was addressed to; was in response to another prior utterance by a different user, which required marking the specific utterance responded to;"
P90-1027,1988.tmi-1.12,0,0.386905,"tor, on the other hand, will accept well-formed expressions of the semantic representation language and produce corresponding expressions in the source natural language. Among the arguments for adopting the bidirectional design in NLP the following are perhaps the most widely shared: In the work reported here we concenlrated on unification-based formalisms, in particular Definite Clause Grammars (Pereira & Warren, 1980), which can be compiled dually into PROLOGparser and generator, where the generator is obtained from the parser's code with the inversion procedure described below. As noted by Dymetman and Isabelle (1988), this transformation must involve rearranging the order of literals on the right-hand side of some clauses. We noted that the design of the string grammar (Sager, 1981) makes it more suitable as a basis of a reversible system than other grammar designs, although other grammars can be &quot;normalized&quot; (Strzalkowski, 1989). We also would like to point out that our main emphasis is on the problem of • A bidirectional NLP system, or a system whose inverse can be derived by a fully automated process, greatly reduces effort required for the system development, since we need to write only one 212 revers"
P90-1027,J81-4005,0,0.298173,"ents for adopting the bidirectional design in NLP the following are perhaps the most widely shared: In the work reported here we concenlrated on unification-based formalisms, in particular Definite Clause Grammars (Pereira & Warren, 1980), which can be compiled dually into PROLOGparser and generator, where the generator is obtained from the parser's code with the inversion procedure described below. As noted by Dymetman and Isabelle (1988), this transformation must involve rearranging the order of literals on the right-hand side of some clauses. We noted that the design of the string grammar (Sager, 1981) makes it more suitable as a basis of a reversible system than other grammar designs, although other grammars can be &quot;normalized&quot; (Strzalkowski, 1989). We also would like to point out that our main emphasis is on the problem of • A bidirectional NLP system, or a system whose inverse can be derived by a fully automated process, greatly reduces effort required for the system development, since we need to write only one 212 reversibility rather than generation, the latter involving many problems that we don't deal with here (see, e.g. Derr & McKeown, 1984; McKeown, 1985). goals (i.e., those which"
P90-1027,C88-2128,0,0.297101,"Missing"
P90-1027,P89-1002,0,0.322643,"Missing"
P90-1027,C88-2150,0,0.145298,"Missing"
P90-1027,P84-1065,0,0.0312602,". We noted that the design of the string grammar (Sager, 1981) makes it more suitable as a basis of a reversible system than other grammar designs, although other grammars can be &quot;normalized&quot; (Strzalkowski, 1989). We also would like to point out that our main emphasis is on the problem of • A bidirectional NLP system, or a system whose inverse can be derived by a fully automated process, greatly reduces effort required for the system development, since we need to write only one 212 reversibility rather than generation, the latter involving many problems that we don't deal with here (see, e.g. Derr & McKeown, 1984; McKeown, 1985). goals (i.e., those which are not responsible for making a rule a &quot;chain rule&quot;), or resort to dynamic ordering of such goals, putting the goal freezing back into the picture. RELATED WORK In contrast with the above, the parser inversion procedure described in this paper does not require a run-time overhead and can be performed by an offline compilation process. It may, however, require that the grammar is normalized prior to its inversion. We briefly discuss the grammar normalization problem at the end of this paper. The idea that a generator for a language might be considered"
P92-1011,1988.tmi-1.12,0,0.25165,"Missing"
P92-1011,P84-1018,0,0.0862116,"Missing"
P92-1011,C90-2060,0,0.0171214,"ide literal has the same semantics as the root (the so called non-chain rule), the production is expanded, and the algorithm is reeursively applied to every literal on its right-hand side. When the evaluation of a non-chain rule is completed, SHDGA connects its left-hand side literal (called the pivot) to the initial root using (in a backward manner) a series of appropriate chain rules. At this time, all remaining literals in the chain rules are expanded in a fixed order (left-to-right). 1. I N T R O D U C T I O N Recently, two important new algorithms have been published ([SNMP89], [SNMP90], [S90a], [S90b] and [$91]) that address the problem of automated generation of natural language expressions from a structured representation of meaning. Both algorithms follow the same general principle: given a grammar, and a structured representation of meaning, produce one or more corresponding surface strings, and do so with a minimal possible effort. In this paper we limit our analysis of the two algorithms to unification-based formalisms. The first algorithm, which we call here the Semantic-Head-Driven Generation Algorithm (SHDGA), uses information about semantic heads ~ in grammar rules to obt"
P92-1011,W91-0112,1,0.775138,"Missing"
P92-1011,J90-1004,0,0.329745,"into serious inefficiency and nondeterminism, and which EAA will handle in an efficient and deterministic manner. We also point out that only EAA allows to treat the underlying grammar in a truly multi-directional manner. Both algorithms have resolved several outstanding problems in dealing with natural language grammars, including handling of left recursive rules, non-monotonic compositionality of representation, and deadlockprone rules2. In this paper we attempt to compare these two algorithms along their generality and efficiency lines. Throughout this paper we follow the notation used in [SNMP90]. 2. MAIN C H A R A C T E R I S T I C S OF SHDGA'S AND E A A ' S T R A V E R S A L S SHDGA traverses the derivation tree in the semantic-head-first fashion. Starting from the goal predicate node (called the root), containing a structured representation (semantics) from which to generate, it selects a production whose leg-hand side semantics unifies with the semantics of the root. If the selected production passes the semantics unchanged from the left to some nonterminal on the right (the so-called chain rule), this later nonterminal becomes the new root and the algorithm is applied recursively"
P92-1011,C88-2150,0,0.0619308,"Missing"
P92-1011,P89-1002,0,0.0604855,", if no right-hand side literal has the same semantics as the root (the so called non-chain rule), the production is expanded, and the algorithm is reeursively applied to every literal on its right-hand side. When the evaluation of a non-chain rule is completed, SHDGA connects its left-hand side literal (called the pivot) to the initial root using (in a backward manner) a series of appropriate chain rules. At this time, all remaining literals in the chain rules are expanded in a fixed order (left-to-right). 1. I N T R O D U C T I O N Recently, two important new algorithms have been published ([SNMP89], [SNMP90], [S90a], [S90b] and [$91]) that address the problem of automated generation of natural language expressions from a structured representation of meaning. Both algorithms follow the same general principle: given a grammar, and a structured representation of meaning, produce one or more corresponding surface strings, and do so with a minimal possible effort. In this paper we limit our analysis of the two algorithms to unification-based formalisms. The first algorithm, which we call here the Semantic-Head-Driven Generation Algorithm (SHDGA), uses information about semantic heads ~ in gr"
P92-1011,C90-3017,0,\N,Missing
P92-1011,W89-0206,0,\N,Missing
P92-1014,J81-4005,0,0.084356,"is supported by the experiments performed by the NYU group (Strzalkowski and Vauthey, 1991; Grishman and Strzalkowski, 1991), and by the group at the University of Massachussetts (Croft et al., 1991). We explore this possibility further in this paper. It should be noted that all the processing steps, those performed by the backbone system, and these performed by the natural language processing components, are fully automated, and no human intervention or manual encoding is required. FAST PARSING WITH TI&apos;P PARSER T I P flagged Text Parser) is based on the Linguistic String Grammar developed by Sager (1981). Written in Quintus Prolog, the parser currently encompasses more than 400 grammar productions. It produces regularized parse tree representations for each sentence that reflect the sentence&apos;s logical structure. The parser is equipped with a powerful skip-and-fit recovery mechanism that allows it to operate effectively in the face of illformed input or under a severe time pressure. In the recent experiments with approximately 6 million words of English texts,3 the parser&apos;s speed averaged between 0.45 and 0.5 seconds per sentence, or up to 2600 words per minute, on a 21 MIPS SparcStation ELC."
P92-1014,H91-1068,1,0.817753,"dback are usually far more effective, but they require user&apos;s manual intervention in the retrieval process. In this paper, we are concerned with fully automated retrieval only. 104 &quot;program language&quot; via a specification link. After the final query is constructed, the database search follows, and a ranked list of documents is returned. We believe that linguistic processing of both the database and the user&apos;s queries need to be done for a maximum benefit, and moreover, the two processes must be appropriately coordinated. This prognosis is supported by the experiments performed by the NYU group (Strzalkowski and Vauthey, 1991; Grishman and Strzalkowski, 1991), and by the group at the University of Massachussetts (Croft et al., 1991). We explore this possibility further in this paper. It should be noted that all the processing steps, those performed by the backbone system, and these performed by the natural language processing components, are fully automated, and no human intervention or manual encoding is required. FAST PARSING WITH TI&apos;P PARSER T I P flagged Text Parser) is based on the Linguistic String Grammar developed by Sager (1981). Written in Quintus Prolog, the parser currently encompasses more than 400 gr"
P92-1014,C92-1033,1,0.690013,"&quot;natural&quot; is deleted from a query already containing &quot;natural language&quot; because &quot;natural&quot; occurs in many unrelated contexts: &quot;natural number&quot;, &quot;natural logarithm&quot;, &quot;natural approach&quot;, etc. At the same time, other terms may be added, namely those which are linked to some query term through admissible similarity relations. For example, &quot;fortran&quot; is added to a query containing the compound term 3 These include CACM-3204, MUC-3, and a selection of nearly 6,000 technical articles extracted from Computer Library database (a Ziff Communications Inc. CD-ROM). 4 A complete description can be found in (Strzalkowski, 1992). 105 The method is illustrated by the automatic construction of beth recursive and iterative programs opera~-tg on natural numbers, lists, and trees, in order to construct a program satisfying certain specifications a theorem induced by those specifications is proved, and the desired program is extracted from the proof. is defined using the new root (&quot;store&quot;) or one of its standard inflexional forms (e.g., &quot;storing&quot;). For example, the following definitions are excerpted from the Oxford Advanced Learner&apos;s Dictionary (OALD): storage n [U] (space used for, money paid for) the storing of goods .."
P92-1014,J86-3002,0,0.136724,"Missing"
P92-1014,P90-1034,0,0.0830037,"Missing"
P92-1014,J90-1003,0,\N,Missing
P98-2205,C96-2157,1,0.760378,"where Sa is a minor score calculated using metric h; wh is the weight reflecting how effective this metric is in general; l is the length of the segment. The following metrics are used to score passages considered for the main news section of the summary DMS. We list here only the criteria which are the 4Kefer to (Euhn 1958) (Paice 1990) (l~u, Brandow & Mitze 1994) (Kupiec, Pedersen & Chen 1995) for sentence-based summarization approaches. SThe weights w~ are trainable in a supervised mode, given a corpus of texts and their summaries, or in an unsupervised mode as described in (Strzalkowski & Wang 1996). For the purpose of the experiments described here, these weights have been set manually. 1261 most relevant for generating summaries in contex~ of an information retrieval system. Both the French and Iranian governments acknowledged the Iranian role in the release ot"" the three French hostages, Jean-Paul Kauffmann, Marcel Carton and Marcel Fontaine. 1. Words and phrases frequergly occurring in a tex~ are likely to be indicative of its content, especially if such words or phrases do not occur olden elsewhere in the database. A weighted frequency score, similar to tf~df used in automatic tex~"
P98-2205,C94-1056,0,\N,Missing
S15-1009,P98-1013,0,0.149614,"the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals"
S15-1009,baker-etal-2010-modality,0,0.0482113,"Missing"
S15-1009,W13-2322,0,0.0158937,"xt passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, groups, artifacts, etc., which"
S15-1009,W09-3012,1,0.690346,"space we do not provide an overview over all definitions. While at first the terms “belief” and “factuality” appear to relate to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Saur´ı and Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would"
S15-1009,doddington-etal-2004-automatic,1,0.834711,"sity/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany. The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner. Such a meaning representation is useful for many applications; in our project we are specifically interested in knowledge base population. A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) (Doddington et al., 2004). The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content? Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation. The structure of the paper is as follows: we start out by situating our notion of “belief” with respect to other notions of"
S15-1009,W10-3001,0,0.293703,"Missing"
S15-1009,P11-2102,0,0.0364363,"rotates around the earth, as was his (presumably) honest communicative intention. Therefore, to us as researchers interested in describing how language 2 Sarcasm and irony differ from lying in that the communicative intention and the cognitive state are aligned, but they do not align with the standard interpretation of the utterance. Here, the intention is that the reader recognizes that the form of the utterance does not literally express the cognitive state. We leave aside sarcasm and irony in this paper; for current computational work on sarcasm detection, see for example (Gonz´alez-Ib´an˜ ez et al., 2011). is used to communicate, it does not matter that astronomers now believe that Ptolemy was wrong, it does not change our account of communication and it does not change the communication that happened two millennia ago. And since we do not need to make the assumption that the writer knows what she is talking about, we choose not to make this assumption. In the case of Ptolemy, we leave this determination – what is actually true – to astronomers. In other cases, we typically have models of trustworthiness: if a writer sends her spouse a text message saying she is hungry, the spouse has no reaso"
S15-1009,P09-2078,0,0.0256017,"d, we could assume that the writer knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumptio"
S15-1009,P11-1032,0,0.0145471,"er knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumption may be false in c"
S15-1009,C10-2117,1,0.747479,"on-committed belief in the annotations, the heuristic rules (mainly based on the presence of modal auxiliaries) that we added for the purpose of classifying the beliefs (CB, NCB, ROB, NA) did not work reliably in all cases. 4.3 System C System C uses a supervised learning approach to identify tokens denoting the heads of propositions that denote author’s expressed beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM"
S15-1009,W12-3807,1,0.915772,"Missing"
S15-1009,J12-2002,0,0.157236,"Missing"
S15-1009,W15-1304,1,0.70443,"ore, the FactBank annotation is basically compatible with ours. Our annotation is much simpler than that of FactBank in order to allow for a quicker annotation. We summarize the main points of simplification here. • We have taken the source always to be the writer. As we will discuss in Section 7.1, we will adopt the FactBank annotation in the next iteration of our annotation. • We do not distinguish between possible and probable; this distinction may be hard to annotate and not too valuable. • We ignore negation. If present, we simply assume it is part of the proposition which is the target. Werner et al. (2015) study the relation between belief and factuality in more detail. They provide an automatic way of mapping the annotations in FactBank to the 4-way distinction of speaker/writer’s belief that we present in this paper. 3.3 Corpus and Annotation Results The annotation effort for this phase of belief annotation for DEFT produced a training corpus of 852,836 words and an evaluation corpus of 100,037 words. All annotated data consisted of English text from discussion forum threads. The discussion forum threads were originally collected for the DARPA BOLT program, and were harvested from a wide vari"
S15-1009,C98-1013,0,\N,Missing
shaikh-etal-2010-mpc,W03-2118,0,\N,Missing
shaikh-etal-2010-mpc,P05-2014,0,\N,Missing
stein-etal-2000-evaluating,J98-3005,0,\N,Missing
stein-etal-2000-evaluating,P98-2213,0,\N,Missing
stein-etal-2000-evaluating,C98-2208,0,\N,Missing
stein-etal-2000-evaluating,X98-1028,1,\N,Missing
stein-etal-2000-evaluating,P98-2205,1,\N,Missing
stein-etal-2000-evaluating,C98-2200,1,\N,Missing
W03-0704,J99-3003,0,0.147909,"Missing"
W03-0704,devillers-etal-2002-annotations,0,0.0238155,"Missing"
W03-0704,A00-1005,1,\N,Missing
W03-1206,C00-1043,0,0.0155195,"mation piece needed to fulfill the statement. Recent automated systems for answering factual questions deduct this expected answer type from the form of the question and a finite list of possible answer types. For example, “Who was the first man in space” expects a “person” as the answer, while “How long was the Titanic?” expects some length measure as an answer, probably in yards and feet, or meters. This is generally a very good strategy, that has been exploited successfully in a number of automated QA systems that appeared in recent years, especially in the context of TREC QA1 evaluations (Harabagiu et al., 2000; Hovy et al., 2000; Prager at al., 2001). This process is not easily applied to analytical questions. This is because the type of an answer for analytical questions cannot always be anticipated due to their inherently exploratory character. In contrast to a factual question, an analytical question has an unlimited variety of syntactic forms with only a loose connection between their syntax and the expected answer. Given the unlimited potential of the formation of analytical questions, it would be counter-productive to restrict them to a limited number of question/answer types. Even finding a"
W03-1206,M98-1007,0,0.0373134,"locating and extracting instances of this attribute in the running text. The extractors are implemented using information extraction utilities which form the kernel of Sheffield’s GATE2 system. We have modified GATE to separate organizations into companies and other organizations, and we have also expanded by adding new concepts such as industries. Therefore, the framing process resembles strongly the template filling task in information extraction (cf. MUC3 evaluations), with one significant exception: while the MUC task was to fill in a template using potentially any amount of source text (Humphreys et al., 1998), the framing is essentially an inverse process. In framing, potentially multiple frames can be associated with a small chunk of text (a passage or a short paragraph). Furthermore, this chunk of text is part of a cluster of very similar text chunks that further reinforce some of the most salient features of these texts. This makes the frame filling a significantly less error-prone task – our experience has been far more positive than the MUC evaluation results may indicate. This is because, rather than trying to find the most appropriate values for attributes from among many potential candidat"
W03-1206,P02-1048,0,0.0221625,"Missing"
W03-1206,H01-1006,0,0.0530186,"Missing"
W03-1206,W00-0303,0,0.0953278,"Missing"
W03-1206,W00-1501,0,\N,Missing
W03-1206,W02-0209,0,\N,Missing
W03-1206,A00-1005,1,\N,Missing
W04-2507,M98-1007,0,0.13033,"Missing"
W10-2708,webb-etal-2010-evaluating,1,0.875558,"Missing"
W13-0909,W10-0303,0,0.588845,"Missing"
W13-0909,J91-1003,0,0.409834,"aphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method reli"
W13-0909,W06-3506,0,0.850487,"Missing"
W13-0909,P03-1054,0,0.00653924,"g is presented in a separate, future publication. Words that have an imageability rating lower than an experimentally determined threshold are further excluded from consideration. In the exam70 ple shown in Figure 1, words that have sufficiently high imageability scores are “labyrinthine”, “port”, “rail” and “airline”. We shall consider them as candidate relations, to be further investigated, as explained in the dependency parsing step described next. 3.3 Relation Extraction Dependency parsing reveals the syntactic structure of the sentence with the Target concept. We use the Stanford parser (Klein and Manning, 2003) for English language data. We identify candidate metaphorical relations to be any verbs that have the Target concept in direct dependency path (other than auxiliary and modal verbs). We exclude verbs of attitude (“think”, “say”, “consider”), since these have been found to be more indicative of metonymy than of metaphor. This list of attitude verbs is automatically derived from WordNet. From the example shown in Figure 1, one of the candidate relations extracted would be the verb “navigate”. In addition, we have a list of candidate relations from Step 3.2, which are the highly imageable nouns"
W13-0909,W07-0103,0,0.160031,"ig, 1995) who have focused on the 68 way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Science underpinning (Musolff, 2008; Lakoff, 2001). In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Naraya"
W13-0909,P80-1004,0,0.840031,"ed. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data,"
W13-0909,shutova-teufel-2010-metaphor,0,0.033737,"Net (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities"
W13-0909,C12-2109,0,0.305578,"Missing"
W13-0909,C10-1117,1,\N,Missing
W13-0909,D11-1063,0,\N,Missing
W13-1105,P03-1054,0,0.00392165,"in gaps. The lengths of these gaps are also important to measures for some behaviors. Meso-topics are the most persistent local topics, topics that are widely cited through long stretches of discourse. A selection of meso-topics is closely associated with the task in which the discourse participants are engaged. Short “gaps” in the chain are permitted (up to 10 turns, to accommodate digressions, obscure references, noise, etc.). Meso-topics can be distinguished from the local topics because the participants often make polarized statements about them. We use the Stanford part-of-speech tagger (Klein and Manning, 2003) to automatically detect nouns and noun phrases in dialogue and select those with subsequent mentions as local topics using a fairly simple pronoun resolution method based primarily on presence of specific lexical features as well as temporal distance between utterances. Princeton Wordnet (Fellbaum et al., 2006) is consulted to identify synonyms and other related words commonly used in co-references. The local topics that form sufficiently long co-reference chains are designated as meso-topics. 3.2 Topical Positioning Topical Positioning is defined as the attitude a speaker has towards the mes"
W13-1105,C10-1117,1,0.802734,"that constructs a model of a communitywide sentiment towards certain common issues discussed in social media, particularly forums and open blogs. This model is then used to assess whether a new post would fit into the targeted community by comparing the sentiment polarities about the concepts in the message and in the model. Potential posters are then guided in ways to shape their communication so that it minimizes the number of conflicting concept sentiments, while still preserving the intended message. 42 Another related research domain is about modeling the social phenomena in discourse. (Strzalkowski et al., 2010, Broadwell et al., 2012) proposed a two-tier approach that relies on extracting observable linguistic features of conversational text to detect mid-level social behaviors such as Topic Control, Disagreement and Involvement. These social behaviors are then used to infer higher-level social roles such as Leader and Influencer, which may have impact on how other participants’ opinions form and change. 3 System Modules In this section, we describe a series of modules in our system, which include meso-topic extraction, topical positioning and topical positioning map, and explain how we capture opi"
W14-2306,W10-0303,0,0.0169498,"age. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008). It is expressed through multiple means, many"
W14-2306,P80-1004,0,0.702834,"rishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to"
W14-2306,J91-1003,0,0.690231,"Missing"
W14-2306,W06-3506,0,0.0751735,"Missing"
W14-2306,C04-1200,0,0.115008,"metimes referred to as intensity. Our approach to affect in metaphor has been vetted not only by our core linguistic team but also by an independent team of linguist-analysts with whom we work to understand metaphor across several language-culture groups. Our research continues to show no difficulties in comprehension or disagreement across languages concerning the concept of linguistic affect, of its application to metaphor, and of its having both polarity and intensity. 5 Related Research: sentiment and affect There is a relatively large volume of research on sentiment analysis in language (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; inter alia) that aim at detecting polarity of text, but is not specifically concerned with metaphors. A number of systems were developed to automatically extract writer’s senti1 The First Workshop on Metaphor in NLP. http://aclweb.org/anthology//W/W13/W13-09.pdf 43 ment towards specific products or services such as movies or hotels, from online reviews (e.g., Turney, 2002; Pang and Lee, 2008) or social media messages (e.g., Thelwall et al., 2010). None of these techniques has been applied specifically to metaphorical language, and it is"
W14-2306,P13-1067,0,0.356958,"egative categories. However, the presence of largely negative concepts such as “poverty” in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2. While presence of affect in metaphorical language is well documented in linguistic and psycholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who proposed various models of metaphor affect classification based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approaches, which are closely related to sentiment analysis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it. In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the metaphoric expression"
W14-2306,W07-0103,0,0.0258652,"loran, 2007) that attempt to correlate metaphor semantics with their usage in naturally occurring text but generally lack robust tools to do so; and (3) social science approaches, particularly in psychology and anthropology that seek to explain how people produce and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated examples. In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & N"
W14-2306,S13-2053,0,0.216251,"m a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O’Halloran, 2007) that attempt to correlate metaphor semantics with their usag"
W14-2306,W13-0904,0,0.0153035,"interest are those that operate between the concepts within a Source domain and can be “borrowed” to link concepts within the Target domain, e.g., “Crime(TARGET) spread to(RELATION) previously safe areas” may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors a"
W14-2306,shutova-teufel-2010-metaphor,0,0.0617997,"Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van d"
W14-2306,P10-1071,0,0.0174283,"between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a system that can recognize metaphor; however their approach is only shown to work in a narrow domain (The Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met4 Affect in Metaphors Affect in language is understood to mean the attitude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008). It i"
W14-2306,C12-2109,0,0.0359731,"Missing"
W14-2306,S07-1013,0,0.0701286,"Missing"
W14-2306,W13-0909,1,0.73941,"may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O’Halloran, 2007) that attempt to correlate metaphor semantics with their usag"
W14-2306,P02-1053,0,0.0132888,"Missing"
W14-2306,P12-3002,0,0.0922262,"sentences into positive/negative categories. However, the presence of largely negative concepts such as “poverty” in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2. While presence of affect in metaphorical language is well documented in linguistic and psycholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who proposed various models of metaphor affect classification based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approaches, which are closely related to sentiment analysis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it. In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the"
W14-2306,W13-0905,0,0.020242,"at operate between the concepts within a Source domain and can be “borrowed” to link concepts within the Target domain, e.g., “Crime(TARGET) spread to(RELATION) previously safe areas” may be borrowing from a DISEASE or a PARASITE source domain. 3 aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1 workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different approach to metaphor understanding based on lexical semantics and discourse analysis was introduced by Strzalkowski et al. (2013). Space constraints limit our discussion about their work in this article, however in the foregoing, our discussion is largely consistent with their framework. Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language"
W14-4725,P80-1004,0,0.355181,"ed. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data,"
W14-4725,J91-1003,0,0.171713,"Missing"
W14-4725,W06-3506,0,0.0458493,"Missing"
W14-4725,W07-0103,0,0.0290505,"errig, 1995) who have focused on the way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Science underpinning (Musolff, 2008; Lakoff, 2001). In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Naraya"
W14-4725,shutova-teufel-2010-metaphor,0,0.0214182,"Net (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantitie"
W14-4725,P10-1071,0,0.014226,"tiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example). Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generate"
W14-4725,C12-2109,0,0.0320502,"Missing"
W14-4725,W13-0905,0,0.016931,"initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. More recently, several important approaches to metaphor extraction have emerged from the IARPA Metaphor program, including Broadwell et al (2013), Strzalkowski et al. (2014), Wilks et al (2013), Hovy et al (2013) inter alia. These papers concentrate on the algorithms for detection and classification of individual linguistic metaphors in text rather than formation of conceptual metaphors in a broader cultural context. Taylor et al (2014) outlines the rationale why conceptual level metaphors may provide important insights into cross-cultural contrasts. Our work described here is a first attempt at automatic discovery of conceptual metaphors operating within a culture directly from the linguistic evidence in language. 3 Our Approach The process of discovering conceptual metaphors is ne"
W14-4725,W13-0907,0,\N,Missing
W14-4725,shaikh-etal-2014-multi,1,\N,Missing
W14-4725,W13-0909,1,\N,Missing
W15-1408,W10-0303,0,0.0227642,"pages 67–76, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Understanding conflicts in this manner may allow policy-makers facilitate negotiations and discussions across different communities and help bridge contrasting viewpoints and cultural values. 2 Relevant Research The underlying core of our research is automated, large-scale metaphor extraction. Computational approaches to metaphor to date have yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our"
W15-1408,C12-2016,0,0.0515284,"Missing"
W15-1408,J91-1003,0,0.629185,"Missing"
W15-1408,W13-1105,1,0.941007,"ssessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen et al. (2013), who look at nonparametric topic modeling as a measure of influence; and Bracewell et al. (2012), who look at a category of social acts to determine measures of leadership; among others. Analysis of positions held by discourse participants has been studied in the realm of political science and computational sociolinguistics (Laver, Benoit & Garry, 2003; Slapin & Proksch, 2008; Lin et al., 2013; Pang & Lee, 2008) and our approach draws parallels from such prior work. Our topical positioning approach is a departure from existing approaches to sentiment analysis (Wiebe, Wilson and Cardie, 2005; Strapparava and Mihalcea, 2008) in looking at a larger context of discourse rather than individual utterances. 68 3 The Conflict – U.S. Gun Debate The main hypothesis, and an open research question, is then: can this new technology be effectively applied to understanding of a broad cultural conflict such as may arise in any society where potentially divisive issues exist? To answer this questio"
W15-1408,W13-0904,0,0.020107,"ave yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant"
W15-1408,shutova-teufel-2010-metaphor,0,0.0163255,"e Third Workshop on Metaphor in NLP, pages 67–76, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Understanding conflicts in this manner may allow policy-makers facilitate negotiations and discussions across different communities and help bridge contrasting viewpoints and cultural values. 2 Relevant Research The underlying core of our research is automated, large-scale metaphor extraction. Computational approaches to metaphor to date have yielded only limited scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extract"
W15-1408,C10-1117,1,0.822159,"approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen et al. (2013), who look at nonparametric topic modeling as a measure of influence; and Bracewell et al. (2012), who look at a category of social acts to determine measures of leadership; among others. Analysis of positions held by discourse participants has been studied in the realm of political science and computational sociolinguistics (Laver, Benoit & Garry, 2003; Slapin & Proksch, 2008; Lin et al., 2013; Pang & Lee, 2008) and our approach draws parallels from such prior work. Our topical positioning approach is a depa"
W15-1408,W13-0909,1,0.870627,"the listener (Perloff, 2014). Metaphors, which are mapping systems that allow the semantics of a familiar Source domain to be applied to a Target domain so that new frameworks of reasoning can emerge in the Target domain, are pervasive in discourse. Metaphorically rich language is considered highly influential. Persuasion and influence literature (Soppory and Dillard, 2002) indicates messages containing metaphorical language produce somewhat greater attitude change than messages that do not. Metaphors embody a number of elements of persuasive language, including concreteness and imageability (Strzalkowski et al., 2013, Broadwell et al., 2013, Charteris-Black, 2005). Using this line of investigation, we aim to understand the motivations of a group or of a political faction through their discourse, as part of the answer to such questions as: What are the key differences in protagonists’ positions? How extensive is a protagonists’ influence? Who dominates the discourse? Where is the core of the groups’ support? Our goal is to provide a basis for the analysis of cross-cultural conflicts by viewing the conflict as an ongoing debate or a “dialogue” between protagonists or participants. In this interpretation, ea"
W15-1408,W13-0905,0,0.027816,"ted scale, often hand-designed systems (Wilks, 1975; Fass, 1991; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. Shutova (2010) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Several other similar approaches were reported at the Meta4NLP workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Strzalkowski et al. (2013) developed a data-driven approach towards the automated extraction of metaphors from text and our approach builds upon their work. The use of metaphor, along with sociocultural aspects of language to understand cross-cultural conflict is novel to our approach. Recent research in computational sociolinguistic has developed methods for automatic assessment of leadership, influence and power in conversation (Broadwell et al., 2012; Shaikh et al., 2012; Strzalkowski et al., 2010) and we draw largely upon this work. Other relevant work includes Nguyen"
W91-0112,E89-1032,0,0.0148012,"between their analysis and synthesis capabilities. These properties are important in any linguistic system, especially in machine translation, and in various interactive natural language systems where the direction of communication frequently changes. In this paper we are primarily interested in the computational aspects I of reversibility that include bi-directional evaluation and dual compilation of computer grammars, inversion of parsers into efficient generators, and derivation of ""generating-versions"" of existing parsing algorithms. Some of the recent resea~h in this area is reported in (Calder et al., 1989; Dymetman and Isabelle, 1988; Dymetman et al., 1990; Estival, 1990; Hasida and Isizaki, 1987; Ishizaki, 1990; Shieber, 1988; Shieber et al., 1990; Strzalkowski, 1990a-c; Strzalkowski and Peng, 1990; van Noord, 1990; and Wedekind, 1988). Dymetman and Isabelle (1988) describe a top-down interpreter for definite clause grammars that statically reorders clause literals according to a hand-eoded specification, and further allows for dynamic selection of AND goals 2 during execution, using the technique known as the goal freezing (Colmerauer, 1982; Naish, 1986). Shieber et al. (1990) propose a mixe"
W91-0112,C88-2128,0,0.0784547,"Missing"
W91-0112,1988.tmi-1.12,0,0.322631,"Missing"
W91-0112,C90-3017,0,0.128786,"These properties are important in any linguistic system, especially in machine translation, and in various interactive natural language systems where the direction of communication frequently changes. In this paper we are primarily interested in the computational aspects I of reversibility that include bi-directional evaluation and dual compilation of computer grammars, inversion of parsers into efficient generators, and derivation of ""generating-versions"" of existing parsing algorithms. Some of the recent resea~h in this area is reported in (Calder et al., 1989; Dymetman and Isabelle, 1988; Dymetman et al., 1990; Estival, 1990; Hasida and Isizaki, 1987; Ishizaki, 1990; Shieber, 1988; Shieber et al., 1990; Strzalkowski, 1990a-c; Strzalkowski and Peng, 1990; van Noord, 1990; and Wedekind, 1988). Dymetman and Isabelle (1988) describe a top-down interpreter for definite clause grammars that statically reorders clause literals according to a hand-eoded specification, and further allows for dynamic selection of AND goals 2 during execution, using the technique known as the goal freezing (Colmerauer, 1982; Naish, 1986). Shieber et al. (1990) propose a mixed top-down/bottom-up interpretation, in which certai"
W91-0112,C90-2019,0,0.0120737,"mportant in any linguistic system, especially in machine translation, and in various interactive natural language systems where the direction of communication frequently changes. In this paper we are primarily interested in the computational aspects I of reversibility that include bi-directional evaluation and dual compilation of computer grammars, inversion of parsers into efficient generators, and derivation of ""generating-versions"" of existing parsing algorithms. Some of the recent resea~h in this area is reported in (Calder et al., 1989; Dymetman and Isabelle, 1988; Dymetman et al., 1990; Estival, 1990; Hasida and Isizaki, 1987; Ishizaki, 1990; Shieber, 1988; Shieber et al., 1990; Strzalkowski, 1990a-c; Strzalkowski and Peng, 1990; van Noord, 1990; and Wedekind, 1988). Dymetman and Isabelle (1988) describe a top-down interpreter for definite clause grammars that statically reorders clause literals according to a hand-eoded specification, and further allows for dynamic selection of AND goals 2 during execution, using the technique known as the goal freezing (Colmerauer, 1982; Naish, 1986). Shieber et al. (1990) propose a mixed top-down/bottom-up interpretation, in which certain goals, namely"
W91-0112,C90-2022,0,0.0114763,"ing execution unless L's value is known. Likewise, a request to generate a main verb in a sentence when the only information we have is its root form (or ""logical form"") may lead to repeated access to the lexicon until the ""correct"" surface form is chosen. Therefore, for a lexicon access goal, say acclex (Word,Feats,Root), it is reasonable to require that both Feats and Root are the essential arguments, in other words, that the set {Feat,Root} is a minimal set of essential arguments, or a MSEA, for acclex. The following procedure computes the set of active * Some concern has also been voiced (Gardent and Plainfosse, 1990) about the termination conditions of this algorithm. 5 Some programs m a y in fact be multi-directional, and therefore may have several 'inverses' or 'modes'. 6 Shieber et al. (1990) have shown that some recursive clauses c.annot be executed using top.down evaluation thus motivating the use of a mixed top-down/bouom-up evaluation of their 'head.driven' compilation. At present the grammar consists of 400+ productions. 92 i (6) For MSEAS (MS,MSEA,VP,s+I,OUT), i.e., for i = s + l , d o M S := {MSEA}. MSEA's in a clause head literal, s PROCEDURE MSEAS(MS,MSEA,VP,i,OUT) As a simple example consider"
W91-0112,J90-1004,0,0.457563,"d in various interactive natural language systems where the direction of communication frequently changes. In this paper we are primarily interested in the computational aspects I of reversibility that include bi-directional evaluation and dual compilation of computer grammars, inversion of parsers into efficient generators, and derivation of ""generating-versions"" of existing parsing algorithms. Some of the recent resea~h in this area is reported in (Calder et al., 1989; Dymetman and Isabelle, 1988; Dymetman et al., 1990; Estival, 1990; Hasida and Isizaki, 1987; Ishizaki, 1990; Shieber, 1988; Shieber et al., 1990; Strzalkowski, 1990a-c; Strzalkowski and Peng, 1990; van Noord, 1990; and Wedekind, 1988). Dymetman and Isabelle (1988) describe a top-down interpreter for definite clause grammars that statically reorders clause literals according to a hand-eoded specification, and further allows for dynamic selection of AND goals 2 during execution, using the technique known as the goal freezing (Colmerauer, 1982; Naish, 1986). Shieber et al. (1990) propose a mixed top-down/bottom-up interpretation, in which certain goals, namely those whose expansion is defined by the so-called ""chain rules"",3 are not expa"
W91-0112,C90-2060,0,0.570043,"ve natural language systems where the direction of communication frequently changes. In this paper we are primarily interested in the computational aspects I of reversibility that include bi-directional evaluation and dual compilation of computer grammars, inversion of parsers into efficient generators, and derivation of ""generating-versions"" of existing parsing algorithms. Some of the recent resea~h in this area is reported in (Calder et al., 1989; Dymetman and Isabelle, 1988; Dymetman et al., 1990; Estival, 1990; Hasida and Isizaki, 1987; Ishizaki, 1990; Shieber, 1988; Shieber et al., 1990; Strzalkowski, 1990a-c; Strzalkowski and Peng, 1990; van Noord, 1990; and Wedekind, 1988). Dymetman and Isabelle (1988) describe a top-down interpreter for definite clause grammars that statically reorders clause literals according to a hand-eoded specification, and further allows for dynamic selection of AND goals 2 during execution, using the technique known as the goal freezing (Colmerauer, 1982; Naish, 1986). Shieber et al. (1990) propose a mixed top-down/bottom-up interpretation, in which certain goals, namely those whose expansion is defined by the so-called ""chain rules"",3 are not expanded during the top-"
W91-0112,P90-1027,1,0.896819,"(Calder et al., 1989; Dymetman and Isabelle, 1988; Dymetman et al., 1990; Estival, 1990; Hasida and Isizaki, 1987; Ishizaki, 1990; Shieber, 1988; Shieber et al., 1990; Strzalkowski, 1990a-c; Strzalkowski and Peng, 1990; van Noord, 1990; and Wedekind, 1988). Dymetman and Isabelle (1988) describe a top-down interpreter for definite clause grammars that statically reorders clause literals according to a hand-eoded specification, and further allows for dynamic selection of AND goals 2 during execution, using the technique known as the goal freezing (Colmerauer, 1982; Naish, 1986). Shieber et al. (1990) propose a mixed top-down/bottom-up interpretation, in which certain goals, namely those whose expansion is defined by the so-called ""chain rules"",3 are not expanded during the top-down phase of the interpreter, but instead they are passed over until a nearest non-chain rule is reached. In the bottom-up phase the missing parts of the goal-expansion tree will be filled in by applying A reversible grammar is usually understood as a computational or linguistic system that can be used both for analysis ~nd generation of the language it defines. For example, a directive pars_gen (Sent,For~n) would"
W91-0112,C90-2052,0,0.021173,"Missing"
W91-0112,C88-2150,0,0.0559829,"ly changes. In this paper we are primarily interested in the computational aspects I of reversibility that include bi-directional evaluation and dual compilation of computer grammars, inversion of parsers into efficient generators, and derivation of ""generating-versions"" of existing parsing algorithms. Some of the recent resea~h in this area is reported in (Calder et al., 1989; Dymetman and Isabelle, 1988; Dymetman et al., 1990; Estival, 1990; Hasida and Isizaki, 1987; Ishizaki, 1990; Shieber, 1988; Shieber et al., 1990; Strzalkowski, 1990a-c; Strzalkowski and Peng, 1990; van Noord, 1990; and Wedekind, 1988). Dymetman and Isabelle (1988) describe a top-down interpreter for definite clause grammars that statically reorders clause literals according to a hand-eoded specification, and further allows for dynamic selection of AND goals 2 during execution, using the technique known as the goal freezing (Colmerauer, 1982; Naish, 1986). Shieber et al. (1990) propose a mixed top-down/bottom-up interpretation, in which certain goals, namely those whose expansion is defined by the so-called ""chain rules"",3 are not expanded during the top-down phase of the interpreter, but instead they are passed over until"
W91-0112,C90-2033,0,0.010722,"ally in machine translation, and in various interactive natural language systems where the direction of communication frequently changes. In this paper we are primarily interested in the computational aspects I of reversibility that include bi-directional evaluation and dual compilation of computer grammars, inversion of parsers into efficient generators, and derivation of ""generating-versions"" of existing parsing algorithms. Some of the recent resea~h in this area is reported in (Calder et al., 1989; Dymetman and Isabelle, 1988; Dymetman et al., 1990; Estival, 1990; Hasida and Isizaki, 1987; Ishizaki, 1990; Shieber, 1988; Shieber et al., 1990; Strzalkowski, 1990a-c; Strzalkowski and Peng, 1990; van Noord, 1990; and Wedekind, 1988). Dymetman and Isabelle (1988) describe a top-down interpreter for definite clause grammars that statically reorders clause literals according to a hand-eoded specification, and further allows for dynamic selection of AND goals 2 during execution, using the technique known as the goal freezing (Colmerauer, 1982; Naish, 1986). Shieber et al. (1990) propose a mixed top-down/bottom-up interpretation, in which certain goals, namely those whose expansion is defined by the s"
W91-0112,P84-1018,0,0.159071,"chased (Fido,John)) to the sentence Fido chased John in Toronto, or it would produce one of the several possibly paraphrases of this sentence given its representation. In the last several years, there have been a growing amount of research activity in reversibi¢ grammars for natural language, particularly in condecfion with machine translation work, and in natural language generation. Development of reversible 'grammar systems is considered desirable for variet), of reasons that include their immediate use in both parsing and generation, a i For linguistic aspects of reversible grammars, see (Kay, 1984; Landsbergen, 1987; Neuman, 1990; Steedman, 1987). 2 Literals on the fight-hand side of a clause create AND goals; literalswiththe samepredicatenameson the left-handsides of differentclausescreateOR goals. 3 A chainrule is one wherethe main binding.carryingargument (the ""head"")is passed unchangedfrom the left-handside to the fight. For example, assert(P) --> subj(Pl),verb(P2),obj(PI,P2,P),is a chainrulewith respectto the argumentP. assumingthatP is the 'head' argument. 91 the chain rules in a backward manner. This technique, known as 'head-driven' evaluation, can be applied quite profitably t"
W91-0112,C90-2051,0,0.02194,"Missing"
W91-0112,J81-4005,0,0.0331873,"o shown to deal adequately with recursive clauses that created problems in purely top-down compilation. 6 The inter-clausal inversion procedure discussed here effects global changes in goal ordering by moving selected goals between clauses and even creating new clauses. The net effect is similar to that achieved in the head-driven evaluation, except that no explicit concept of 'head' or 'chain-rule' is used. The algorithm has been tested on a substantial coverage PROLOG grammar for English derived form the PROTEUS Parser Grammar (Grishman, 1986), and the Linguistic String Grammar for English (Sager, 1981). 7 ""In"" and ""out"" status of arguments in a PROLOG program can be computed statically at compile time. The general algorithm has been described in (StrTalkowski, 1990c; Strzalkowski and Peng, 1990). ESSENTIAL ARGUMENTS: AN EXTENSION The notion of an essential argument in a PROLOG literal has been first introduced in (Strzalkowski, 1989), and subsequently extended in (Strzalkowski, 1990bc; Sttzalkowski and Peng, 1990). In short, X is an essential argument in a literal p ("" .- X • -- ) if X is required to be ""in"" for a successful evaluation of this literal. By a successful evaluation of a litera"
W93-0302,H93-1071,0,0.0901442,"o be discarded, as we are unable to tell synonymous or near synonymous relationships from those which are primarily complementary, e.g., man and woman. SUMMARY OF RESULTS We have processed the total of 500 MBytes of articles from Wall Street Journal section of T R E C database. Retrieval experiments involved 50 user information requests (topics) (TREC topics 51-100) consisting of several fields that included both text and user supplied keywords. A typical topic is shown below: Other results on the impact of different fields in TREC topics on the final recall/precision results were reported by Broglio and Croft (1993) at the ARPA HLT workshop, although text-only runs were not included. One of the most striking observations they have made is that the narrative field is entirely disposable, and moreover that its inclusion in the query actually hurts the system's performance. It has to be pointed out, however, that they do little language processing.15 <:top> <head> Tipster Topic Description < h u m > Number:. 059 <dora> Domain: Environment <title> Topic: Weather Related Fatalities <desc> Description: Document will report a type of weather event which has directly caused at least one fatality in some location"
W93-0302,J90-1003,0,0.0855473,"Missing"
W93-0302,J86-3002,0,0.27384,"Missing"
W93-0302,H91-1065,0,0.0621077,"Missing"
W93-0302,J81-4005,0,0.0106028,"ust close the currently open constituent (i.e., reduce a program satisfying certain specifications to NP), and possibly a few of its parent constituents, removing corresponding productions from further consideration, until an appropriate production is reactivated. In this case, T I P may force the following reductions: SI -~ to V N P , SA --~ SI; S -.~ N P V NP SA, until the production S ~ S and S is reached. Next, the parser skips input to find and, and resumes normal processing. FAST PARSING WITH T T P PARSER &quot;I'I'P (Tagged Text Parser) is based on the Linguistic String Grammar developed by Sager (1981). The parser currently encompasses some 400 grammar productions, but it is by no means complete. The parser's output is a regularized parse tree representation of each sentence, that is, a representation that reflects the sentence's logical predicateargument structure. For example, logical subject and logical object are identified in both passive and active sentences, and noun phrases are organized around their head elements. The significance of this representation will be discussed below. The parser is equipped with a powerful skip-and-fit recovery mechanism that allows it to operate effectiv"
W93-0302,H91-1068,1,0.821174,"Missing"
W93-0302,P92-1014,1,0.527537,"Missing"
W93-0302,C92-1033,1,0.842325,"-and-fit recovery mechanism that allows it to operate effectively in the faze of ill-formed input or under a severe time pressure. In the runs with approximately 83 million words of TREC's Wall Street Journal texts~ the parser's As may be expected, the skip-and-fit strategy will only be effective if the input skipping can be performed with a degree of determinism. This means that most of the iexical level ambiguity must be removed from the input text. prior to parsing. We achieve this using a stochastic parts of speech tagger to preprocess the text. Full details of the parser can be found in (Strzalkowski, 1992). 4 Approximately 0.5 GBytes of text. over 4 million senteilc¢~. 11 PART OF SPEECH T A G G E R is tagged as -jj,,:6 One way of dealing with lexical ambiguity is to use a tagger to preprocess the input marking each word with a tag that indicates its syntactic categorization: a part of speech with selected morphological features such as number, tense, mode, case and degree. The following are tagged sentences from the CACM-32(M collection: 5 this~dr paper/nn dates/vbz back/rb the~dr genesis/nn of~in binary/jj conception/nn circa~in 5000/cd years/nns ago/rb ,~corn as/rb derived/vbn by~in the~dr ch"
W97-0117,H92-1022,0,0.035931,"Missing"
W97-0117,J95-4004,0,0.231982,"Missing"
W97-0117,P91-1022,0,0.0252197,"Missing"
W97-0117,P93-1002,0,0.0242597,"Missing"
W97-0117,P96-1010,0,0.0512833,"Missing"
W97-0117,H94-1034,0,0.0303942,"lleviate the need for parallel correct transcriptions in the training sample, as multiple hypotheses can be aligned in order to postulate correction rules. Using multiple SRS in parallel may also increase the likelihood of locating and correcting spurious transcription errors. Finally, we may consider an open-box solution where the information encoded in C-Box rules is fed back into the SRS language model to improve its baseline performance. At this time, we made no attempt to address any of the problems related to spontaneous speech, such as disfluencies and self-repairs (e.g., Oviatt, 1994; Heeman & Allen, 1994). In dictation, where the speaker normally has an option of backing up and re-recording, such things are less of an issue than the word-for-word accuracy of the final transcription, since there are serious liability considerations to be reckoned with. Acknowledgements. This research is based upon work supported in part under a cooperative agreement between the National Institute of Standards and Technology Advanced Technology Program (under the I-IITECC contract, number 70NANB5Hl195) and the Healtheare Open Systems and Trials, Inc. consortium. The authors would like to thank all members of the"
W97-0117,H91-1057,0,0.0317718,"Missing"
W97-0117,H92-1017,0,0.0598081,"Missing"
W97-0117,H94-1040,0,0.0256006,"Missing"
W97-0117,H91-1013,0,0.0212854,"Missing"
W97-0117,H94-1041,0,0.0149757,"ses may also alleviate the need for parallel correct transcriptions in the training sample, as multiple hypotheses can be aligned in order to postulate correction rules. Using multiple SRS in parallel may also increase the likelihood of locating and correcting spurious transcription errors. Finally, we may consider an open-box solution where the information encoded in C-Box rules is fed back into the SRS language model to improve its baseline performance. At this time, we made no attempt to address any of the problems related to spontaneous speech, such as disfluencies and self-repairs (e.g., Oviatt, 1994; Heeman & Allen, 1994). In dictation, where the speaker normally has an option of backing up and re-recording, such things are less of an issue than the word-for-word accuracy of the final transcription, since there are serious liability considerations to be reckoned with. Acknowledgements. This research is based upon work supported in part under a cooperative agreement between the National Institute of Standards and Technology Advanced Technology Program (under the I-IITECC contract, number 70NANB5Hl195) and the Healtheare Open Systems and Trials, Inc. consortium. The authors would like to t"
W97-0117,W95-0203,0,\N,Missing
W97-0117,H89-1054,0,\N,Missing
wacholder-etal-2004-designing,W03-1206,1,\N,Missing
X96-1030,1993.iwpt-1.23,1,0.720237,"5) containing the compound term &quot;illegal activity&quot; via a synonymy link between &quot;illegal&quot; and &quot;unlawful&quot;. After the final query is constructed, the database search follows, and a ranked list of documents is returned. In TREC-4, the automatic query expansion has been limited to to routing runs, where we refined our version of massive expansion using relevenace information wrt. the training database. Query expansion via automatically generated domain map was not usd in offical ad-hoc runs. Full details of T I P parser have been described in the TREC-1 report [8], as well as in other works [6,7], [9,10,11,12]. As in TREC-3, we used a randomized index splitting mechanism which creates not one but several balanced sub-indexes. These sub-indexes can be searched independently and the results can be merged meaningfully into a single ranking. LINGUISTIC TERMS Syntactic phrases extracted from TTP parse trees are head-modifier pairs. The head in such a pair is a central element of a phrase (main verb, main noun, etc.), while the modifier is one of the adjunct arguments of the head. In the TREC experiments reported here we extracted head-modifier word and fixed-phrase pairs only. The following types of pai"
X96-1030,H91-1065,0,0.0336713,"Missing"
X96-1030,J81-4005,0,0.133244,"Missing"
X96-1030,P92-1014,1,0.896729,"opic 055) containing the compound term &quot;illegal activity&quot; via a synonymy link between &quot;illegal&quot; and &quot;unlawful&quot;. After the final query is constructed, the database search follows, and a ranked list of documents is returned. In TREC-4, the automatic query expansion has been limited to to routing runs, where we refined our version of massive expansion using relevenace information wrt. the training database. Query expansion via automatically generated domain map was not usd in offical ad-hoc runs. Full details of T I P parser have been described in the TREC-1 report [8], as well as in other works [6,7], [9,10,11,12]. As in TREC-3, we used a randomized index splitting mechanism which creates not one but several balanced sub-indexes. These sub-indexes can be searched independently and the results can be merged meaningfully into a single ranking. LINGUISTIC TERMS Syntactic phrases extracted from TTP parse trees are head-modifier pairs. The head in such a pair is a central element of a phrase (main verb, main noun, etc.), while the modifier is one of the adjunct arguments of the head. In the TREC experiments reported here we extracted head-modifier word and fixed-phrase pairs only. The followin"
X96-1030,C92-1033,1,0.875021,"opic 055) containing the compound term &quot;illegal activity&quot; via a synonymy link between &quot;illegal&quot; and &quot;unlawful&quot;. After the final query is constructed, the database search follows, and a ranked list of documents is returned. In TREC-4, the automatic query expansion has been limited to to routing runs, where we refined our version of massive expansion using relevenace information wrt. the training database. Query expansion via automatically generated domain map was not usd in offical ad-hoc runs. Full details of T I P parser have been described in the TREC-1 report [8], as well as in other works [6,7], [9,10,11,12]. As in TREC-3, we used a randomized index splitting mechanism which creates not one but several balanced sub-indexes. These sub-indexes can be searched independently and the results can be merged meaningfully into a single ranking. LINGUISTIC TERMS Syntactic phrases extracted from TTP parse trees are head-modifier pairs. The head in such a pair is a central element of a phrase (main verb, main noun, etc.), while the modifier is one of the adjunct arguments of the head. In the TREC experiments reported here we extracted head-modifier word and fixed-phrase pairs only. The followin"
X96-1036,P91-1034,0,0.0276899,"Missing"
X96-1036,P95-1026,0,0.0171038,"Missing"
X98-1020,J81-4005,0,0.146062,"of these relationships do convey semantic dependencies, e.g., in Poland is attacked by Germany the subject+verb and verb+object relationships uniquely capture the semantic relationship of who attacked whom. The surface word-order alone cannot be relied on to determine which relationship holds. From the onset, we assumed that capturing semantic dependencies may be critical for accurate text indexing. One way to approach this is to exploit the syntactic structures produced by a fairly comprehensive parser. T T P (Tagged Text Parser) is based on the Linguistic String Grammar developed by Sager (Sager 1981) . The parser currently encompasses some 400 grammar productions, but it is by no means complete. The parser's output is a regularized parse tree representation of each sentence, that is, a representation that reflects the sentence's logical predicateargument structure. For example, logical subject and logical object are identified in both passive and active sentences, and noun phrases are organized around their head elements. The parser is equipped with a powerful skip-and-fit recovery mechanism that allows it to operate effectively in the face of illformed input or under a severe time pressu"
X98-1020,A97-1044,1,\N,Missing
X98-1028,W97-0713,0,0.0958664,"nown dailies) is to comprehend it in its entirety, and then write a summary ""in your own words."" W h a t this amounts to, computationally, is a full linguistic analysis to extract key text components from which a summary could be built. One previously explored approach, e.g., (Ono, Sumita, & Miike 1994) (McKeown & Radev 1995), was to extract discourse structure elements and then generate the summary within this structure. In another approach, e.g., (DeJong 1982) (Lehnert 1981) pre-defined summary templates were filled with text elements obtained using information extraction techniques. Marcu (Marcu 1997a) uses rhetorical structure analysis to guide the selection of text segments for the summary; similarly Teufel and Moens (Teufel & Moens 1997) analyze argumentative structure of discourse to extract appropriate sentences. While these approaches can produce very good results, they are yet to be demonstrated in a practical system applied to a reasonable size domain. The main difficulty is the lack of an efficient and reliable method of computing the required discourse structure. Our Approach The approach we adopted in our work falls somewhere between simple sentence extraction and textunderstan"
X98-1028,C94-1056,0,0.0601719,"Missing"
X98-1028,C96-2157,1,0.875798,"Missing"
X98-1028,W97-0710,0,0.106062,"ally, is a full linguistic analysis to extract key text components from which a summary could be built. One previously explored approach, e.g., (Ono, Sumita, & Miike 1994) (McKeown & Radev 1995), was to extract discourse structure elements and then generate the summary within this structure. In another approach, e.g., (DeJong 1982) (Lehnert 1981) pre-defined summary templates were filled with text elements obtained using information extraction techniques. Marcu (Marcu 1997a) uses rhetorical structure analysis to guide the selection of text segments for the summary; similarly Teufel and Moens (Teufel & Moens 1997) analyze argumentative structure of discourse to extract appropriate sentences. While these approaches can produce very good results, they are yet to be demonstrated in a practical system applied to a reasonable size domain. The main difficulty is the lack of an efficient and reliable method of computing the required discourse structure. Our Approach The approach we adopted in our work falls somewhere between simple sentence extraction and textunderstanding, although philosophically we are closer to NYT cut-and-paste editors. We overcome the shortcomings of sentence-based summarization by work"
X98-1028,J97-1003,0,\N,Missing
X98-1028,P97-1013,0,\N,Missing
