2020.acl-main.384,N19-1378,1,0.844249,"fer to, and that they are able to track entities across a discourse. Parvez et al. (2018) show that LSTM-based models have poor results on texts with a high presence of entities; Paperno (2014) that they cannot predict the last word of text fragments that require a context of a whole passage (as opposed to the last sentence only), with data that mostly contain nominal elements. Several models (Henaff et al., 2019; Yang et al., 2017; 4178 Ji et al., 2017) were developed as an augmentation of RNN LMs to deal better with entities, with the implicit assumption that standard models do that poorly. Aina et al. (2019) achieved good results on an entity-linking task, but showed that the network was not acquiring entity representations. As for Transformer-based architectures, recent research suggests that they give same or better contextualized representations in comparison with LSTM language models, and that they better encapsulate syntactic information (Goldberg, 2019; Wolf, 2019). On the other hand, van Schijndel et al. (2019) show that big Transformer model representations perform on par or even poorer than smaller LSTMs on tasks such as number agreement or coordination, and that, like LSTMs, they have t"
2020.acl-main.384,P19-1285,0,0.0747272,"Missing"
2020.acl-main.384,W18-5426,0,0.221635,"ds related to studying or working more likely to follow than other kinds of words. That is, referential information helps language models do their task. The cited work includes explicit coreference guidance; however, since referential information is useful for language modeling, we expect language models to learn referential information even without explicit supervision. Here we analyze to what extent this is the case. We carry out our analysis using probe tasks, or tasks that check whether certain information is encoded in a model (Adi et al., 2016; Linzen et al., 2016; Conneau et al., 2018; Giulianelli et al., 2018). The reasoning is as follows: Even if a linguistic property is encoded in the network, it is not necessarily directly accessible through the model output; therefore, we train a probe model to predict a feature of interest, in this case anaphoric coreference, given the model’s hidden representations as input. We focus on the two main linguistic levels that are relevant for coreference: morphosyntax, with 4177 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4177–4189 c July 5 - 10, 2020. 2020 Association for Computational Linguistics grammatical co"
2020.acl-main.384,N18-1108,1,0.929653,"eferential information (the fact that pronoun and antecedent refer to the same entity). Instead, we find evidence that models capture referential aspects to some extent –though they are still much better at grammar. The Transformer outperforms the LSTM in all analyses, and exhibits in particular better semantico-referential abilities. 1 Introduction Neural network-based language models (LMs) have been shown to learn relevant properties of language without being explicitly trained for them. In particular, recent work suggests that they are able to capture syntactic relations to a large extent (Gulordava et al., 2018; Kuncoro et al., 2018; Wilcox et al., 2018). In this paper, we extend this line of research to analyze whether they are able to capture referential aspects of language, focusing on anaphoric relations (pronoun-antecedent relations, as in sheYeping Wang in Figure 1). Previous work, such as Ji et al. (2017), Yang et al. (2017) and Cheng and Erk (2019), showed that augmenting language models with a component that uses an objective based on entity or coreference information improves their performance at language modeling. Intuitively, in the example in Figure 1, understanding that the first she r"
2020.acl-main.384,D19-1275,0,0.0977296,"Missing"
2020.acl-main.384,N06-2015,0,0.012695,"TMs. The Transformer outperforms the LSTM in all the analyses. For morphosyntax, the Transformer and the LSTM have the same behavior with a performance difference; instead, they show different behavior with regard to semantico-referential information. 2 Related work Coreference and anaphora resolution (Mitkov, 2002; Poesio et al., 2016) are among the oldest topics in computational linguistics and have continued to receive a lot of attention in the last decade, as manifested by several shared tasks (Pradhan et al., 2011, 2012; Poesio et al., 2018). In our analysis we use the OntoNotes dataset (Hovy et al., 2006; Pradhan et al., 2012), developed within the coreference resolution community. Our probe tasks are related to coreference resolution; however, our goal is not to train a coreference system but to analyse whether language models extract features relevant for reference without explicit supervision. A recent line of work has focused on demonstrating that neural networks trained on language modeling, without any linguistic annotation, learn syntactic properties and relations such as agreement or filler-gap dependencies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018; Wilcox et"
2020.acl-main.384,D17-1195,0,0.13896,"-referential abilities. 1 Introduction Neural network-based language models (LMs) have been shown to learn relevant properties of language without being explicitly trained for them. In particular, recent work suggests that they are able to capture syntactic relations to a large extent (Gulordava et al., 2018; Kuncoro et al., 2018; Wilcox et al., 2018). In this paper, we extend this line of research to analyze whether they are able to capture referential aspects of language, focusing on anaphoric relations (pronoun-antecedent relations, as in sheYeping Wang in Figure 1). Previous work, such as Ji et al. (2017), Yang et al. (2017) and Cheng and Erk (2019), showed that augmenting language models with a component that uses an objective based on entity or coreference information improves their performance at language modeling. Intuitively, in the example in Figure 1, understanding that the first she refers to Yeping Wang makes words related to studying or working more likely to follow than other kinds of words. That is, referential information helps language models do their task. The cited work includes explicit coreference guidance; however, since referential information is useful for language modelin"
2020.acl-main.384,K19-1001,0,0.0782214,"esentations (Adi et al., 2016; Conneau et al., 2018; Hupkes et al., 2018; Lakretz et al., 2019; Giulianelli et al., 2018), as we do here —applying it to referential information. There is less work on referential information than on syntactic properties such as subject-verb agreement. As for anaphoric reference, Peters et al. (2018) include a limited test using 904 sentences from OntoNotes. Their results suggest that LMs are able to do unsupervised coreference resolution to a certain extent; our first probe task can be seen as an extended version of their task obtaining more specific insights. Jumelet et al. (2019) analyze the kind of information that LSTM-based LMs use to make decisions in within-sentence anaphora. They find a strong male bias encoded in the network’s weights, while the information in the input word embeddings only plays a role in the case of feminine pronouns. We analyze anaphora in longer spans (60 tokens / whole document) and include also a Transformer. The above work suggests that LMs capture morphosyntactic facts about anaphora to a large extent. There is much less evidence that LMs can capture a notion of entity, as that which nominal elements refer to, and that they are able to"
2020.acl-main.384,P18-1132,0,0.056701,"Missing"
2020.acl-main.384,N19-1002,0,0.0137671,"ocused on demonstrating that neural networks trained on language modeling, without any linguistic annotation, learn syntactic properties and relations such as agreement or filler-gap dependencies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018; Wilcox et al., 2018; Futrell et al., 2018). This is typically done by analysing the predictions of LMs on controlled sets of data. Part of this research uses probe models (also known as diagnostic models) to analyse the information contained in their hidden representations (Adi et al., 2016; Conneau et al., 2018; Hupkes et al., 2018; Lakretz et al., 2019; Giulianelli et al., 2018), as we do here —applying it to referential information. There is less work on referential information than on syntactic properties such as subject-verb agreement. As for anaphoric reference, Peters et al. (2018) include a limited test using 904 sentences from OntoNotes. Their results suggest that LMs are able to do unsupervised coreference resolution to a certain extent; our first probe task can be seen as an extended version of their task obtaining more specific insights. Jumelet et al. (2019) analyze the kind of information that LSTM-based LMs use to make decision"
2020.acl-main.384,Q16-1037,0,0.565743,"e first she refers to Yeping Wang makes words related to studying or working more likely to follow than other kinds of words. That is, referential information helps language models do their task. The cited work includes explicit coreference guidance; however, since referential information is useful for language modeling, we expect language models to learn referential information even without explicit supervision. Here we analyze to what extent this is the case. We carry out our analysis using probe tasks, or tasks that check whether certain information is encoded in a model (Adi et al., 2016; Linzen et al., 2016; Conneau et al., 2018; Giulianelli et al., 2018). The reasoning is as follows: Even if a linguistic property is encoded in the network, it is not necessarily directly accessible through the model output; therefore, we train a probe model to predict a feature of interest, in this case anaphoric coreference, given the model’s hidden representations as input. We focus on the two main linguistic levels that are relevant for coreference: morphosyntax, with 4177 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4177–4189 c July 5 - 10, 2020. 2020 Associa"
2020.acl-main.384,P18-1221,0,0.0229921,"at LSTM-based LMs use to make decisions in within-sentence anaphora. They find a strong male bias encoded in the network’s weights, while the information in the input word embeddings only plays a role in the case of feminine pronouns. We analyze anaphora in longer spans (60 tokens / whole document) and include also a Transformer. The above work suggests that LMs capture morphosyntactic facts about anaphora to a large extent. There is much less evidence that LMs can capture a notion of entity, as that which nominal elements refer to, and that they are able to track entities across a discourse. Parvez et al. (2018) show that LSTM-based models have poor results on texts with a high presence of entities; Paperno (2014) that they cannot predict the last word of text fragments that require a context of a whole passage (as opposed to the last sentence only), with data that mostly contain nominal elements. Several models (Henaff et al., 2019; Yang et al., 2017; 4178 Ji et al., 2017) were developed as an augmentation of RNN LMs to deal better with entities, with the implicit assumption that standard models do that poorly. Aina et al. (2019) achieved good results on an entity-linking task, but showed that the n"
2020.acl-main.384,D18-1179,0,0.0338397,"8; Kuncoro et al., 2018; Wilcox et al., 2018; Futrell et al., 2018). This is typically done by analysing the predictions of LMs on controlled sets of data. Part of this research uses probe models (also known as diagnostic models) to analyse the information contained in their hidden representations (Adi et al., 2016; Conneau et al., 2018; Hupkes et al., 2018; Lakretz et al., 2019; Giulianelli et al., 2018), as we do here —applying it to referential information. There is less work on referential information than on syntactic properties such as subject-verb agreement. As for anaphoric reference, Peters et al. (2018) include a limited test using 904 sentences from OntoNotes. Their results suggest that LMs are able to do unsupervised coreference resolution to a certain extent; our first probe task can be seen as an extended version of their task obtaining more specific insights. Jumelet et al. (2019) analyze the kind of information that LSTM-based LMs use to make decisions in within-sentence anaphora. They find a strong male bias encoded in the network’s weights, while the information in the input word embeddings only plays a role in the case of feminine pronouns. We analyze anaphora in longer spans (60 to"
2020.acl-main.384,W12-4501,0,0.0437342,"r outperforms the LSTM in all the analyses. For morphosyntax, the Transformer and the LSTM have the same behavior with a performance difference; instead, they show different behavior with regard to semantico-referential information. 2 Related work Coreference and anaphora resolution (Mitkov, 2002; Poesio et al., 2016) are among the oldest topics in computational linguistics and have continued to receive a lot of attention in the last decade, as manifested by several shared tasks (Pradhan et al., 2011, 2012; Poesio et al., 2018). In our analysis we use the OntoNotes dataset (Hovy et al., 2006; Pradhan et al., 2012), developed within the coreference resolution community. Our probe tasks are related to coreference resolution; however, our goal is not to train a coreference system but to analyse whether language models extract features relevant for reference without explicit supervision. A recent line of work has focused on demonstrating that neural networks trained on language modeling, without any linguistic annotation, learn syntactic properties and relations such as agreement or filler-gap dependencies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018; Wilcox et al., 2018; Futrell et a"
2020.acl-main.384,W11-1901,0,0.0444017,"tial capabilities of current language models, and of the differences between Transformers and LSTMs. The Transformer outperforms the LSTM in all the analyses. For morphosyntax, the Transformer and the LSTM have the same behavior with a performance difference; instead, they show different behavior with regard to semantico-referential information. 2 Related work Coreference and anaphora resolution (Mitkov, 2002; Poesio et al., 2016) are among the oldest topics in computational linguistics and have continued to receive a lot of attention in the last decade, as manifested by several shared tasks (Pradhan et al., 2011, 2012; Poesio et al., 2018). In our analysis we use the OntoNotes dataset (Hovy et al., 2006; Pradhan et al., 2012), developed within the coreference resolution community. Our probe tasks are related to coreference resolution; however, our goal is not to train a coreference system but to analyse whether language models extract features relevant for reference without explicit supervision. A recent line of work has focused on demonstrating that neural networks trained on language modeling, without any linguistic annotation, learn syntactic properties and relations such as agreement or filler-ga"
2020.acl-main.384,D19-1592,0,0.070021,"Missing"
2020.acl-main.384,P19-1452,0,0.0295093,"ch suggests that they give same or better contextualized representations in comparison with LSTM language models, and that they better encapsulate syntactic information (Goldberg, 2019; Wolf, 2019). On the other hand, van Schijndel et al. (2019) show that big Transformer model representations perform on par or even poorer than smaller LSTMs on tasks such as number agreement or coordination, and that, like LSTMs, they have the problem that agreement accuracy decreases as the subject becomes more distant from its verb. Most recent work on analysis of linguistic phenomena in NNs focuses on BERT (Tenney et al., 2019; Clark et al., 2019; Reif et al., 2019; Broscheit, 2019). In this paper we chose to use TransformerXL (Dai et al., 2019) as our Transformer model, and not BERT, for comparability: We wanted to compare the two most standard architectures for LMs on as equal ground as possible, and the two chosen models, TransformerXL and AWD-LSTM (Merity et al., 2017), share the same training objective and are trained on the same data, with comparable vocabularies. 3 Morphosyntactic factors To shed light into which morphosyntactic information LMs encode that is useful for coreference, we train a simple anaphor"
2020.acl-main.384,2020.emnlp-main.14,0,0.0449479,"Missing"
2020.acl-main.384,W18-5423,0,0.155211,"and antecedent refer to the same entity). Instead, we find evidence that models capture referential aspects to some extent –though they are still much better at grammar. The Transformer outperforms the LSTM in all analyses, and exhibits in particular better semantico-referential abilities. 1 Introduction Neural network-based language models (LMs) have been shown to learn relevant properties of language without being explicitly trained for them. In particular, recent work suggests that they are able to capture syntactic relations to a large extent (Gulordava et al., 2018; Kuncoro et al., 2018; Wilcox et al., 2018). In this paper, we extend this line of research to analyze whether they are able to capture referential aspects of language, focusing on anaphoric relations (pronoun-antecedent relations, as in sheYeping Wang in Figure 1). Previous work, such as Ji et al. (2017), Yang et al. (2017) and Cheng and Erk (2019), showed that augmenting language models with a component that uses an objective based on entity or coreference information improves their performance at language modeling. Intuitively, in the example in Figure 1, understanding that the first she refers to Yeping Wang makes words related to"
2020.acl-main.384,D17-1197,0,0.166325,"ties. 1 Introduction Neural network-based language models (LMs) have been shown to learn relevant properties of language without being explicitly trained for them. In particular, recent work suggests that they are able to capture syntactic relations to a large extent (Gulordava et al., 2018; Kuncoro et al., 2018; Wilcox et al., 2018). In this paper, we extend this line of research to analyze whether they are able to capture referential aspects of language, focusing on anaphoric relations (pronoun-antecedent relations, as in sheYeping Wang in Figure 1). Previous work, such as Ji et al. (2017), Yang et al. (2017) and Cheng and Erk (2019), showed that augmenting language models with a component that uses an objective based on entity or coreference information improves their performance at language modeling. Intuitively, in the example in Figure 1, understanding that the first she refers to Yeping Wang makes words related to studying or working more likely to follow than other kinds of words. That is, referential information helps language models do their task. The cited work includes explicit coreference guidance; however, since referential information is useful for language modeling, we expect languag"
2020.coling-main.172,D14-1086,0,0.251912,"for a given object, it is important to verify that this apparent naming variation is not a mere consequence of, for instance, lexical errors or different people naming different objects in the same scene. In this work, we assess the factors that affect the collection of object naming data in the typical Language & Vision (L&V) setup, and test whether these factors impact the accuracy or apparent accuracy of a L&V object labeling model. Existing work in L&V has relied on data collection methods that prompt natural language users to freely talk about or refer to particular objects in an image (Kazemzadeh et al., 2014; Yu et al., 2016; Silberer et al., 2020). Common to these methods is the use of images as a proxy for the actual context of language use (i.e., the real world), and the use of bounding boxes drawn onto the image to indicate the target object that is to be named. These two common aspects, essentially simulating real-world linguistic reference, have enabled large-scale data collection leveraging existing Computer Vision datasets. However, both aspects also introduce potential confounding factors, such as referential uncertainty, where a bounding box may fail to uniquely identify the target obje"
2020.coling-main.172,2020.lrec-1.710,1,0.78992,"Missing"
2020.coling-main.172,D19-1514,0,0.0283849,"e the consistent response set), and inadequate names of various types. We now use it to define a diagnostic evaluation method for object naming models, one which is more fine-grained than the predominant single-label evaluation. It also allows to assess whether models are affected by the same issues as humans, in particular referential and visual uncertainty. We apply our evaluation to Bottom-Up (Anderson et al., 2018) as a representative L&V object naming model, which has been widely used for transfer learning in L&V (Lu et al., 2019; Gao et al., 2019; Chen et al., 2019; Cadene et al., 2019; Tan and Bansal, 2019, inter alia). In contrast to existing works in Computer Vision research (Hoiem et al., 2012) on diagnosing the effects of object or image characteristics on model performance, ManyNames allows us to compare the model against an upper bound of the human performance in object naming, estimated via the verification annotations. Our analysis focuses on two questions: First, can an object detector that was trained in a single-label setting (i.e., towards predicting unique ground truth names) account for the naming variation inherent in human object naming behavior? Second, does the model exhibit a"
2020.coling-main.172,P17-1023,1,0.808481,"ostly concerned with naming particular object instances situated in naturalistic images. In such a setting, naming preferences are more nuanced: humans may prefer different names for instances of the same class (e.g., Fig. 1a-b), and even disagree in their choice for the same instance (Graf et al., 2016; Silberer et al., 2020). While Graf et al. (2016) modeled object naming in a controlled setting, Ordonez et al. (2016) and Mathews et al. (2015) used ImageNet data (Deng et al., 2009), where images show more realistic, yet still isolated objects annotated with WordNet synsets (Fellbaum, 1998). Zarrieß and Schlangen (2017) train a classification-based model for names produced in referring expressions in the RefCOCO data (Yu et al., 2016), where objects are situated in complex scenes and names might be affected by context. In contrast to our work, existing approaches did not have access to name annotations from many different annotators 1894 (a) C bird 27/duck 8 I chicken E singleton (b) duck 33/bird 3 — — (c) (d) (e) bear 16/polar bear 6/animal 3 table 23/counter 5 dog 24/dalmatian 2 ball 5 |dog 3 food 6 |tabletop;desk wheel 4 |ornament;toy;statue other-obj |inadequate other-objs |singletons other-obj |singleto"
2020.lrec-1.710,P15-2017,0,0.192301,"chical variation (chihuahua vs. dog), which has been discussed at length in the theoretical literature, and other less well studied phenomena like cross-classification (cake vs. dessert). Keywords: object naming, language and vision, computer vision 1. Introduction A central issue in Language & Vision (L&V) is how speakers refer to objects. This is most prominent for referring expression generation and interpretation (Kazemzadeh et al., 2014; Mao et al., 2015; Yu et al., 2016), but it also pervades virtually any other L&V task, such as caption generation or visual dialogue (Fang et al., 2015; Devlin et al., 2015; Das et al., 2017; De Vries et al., 2017). One of the central components of referring expressions are object names; for instance, speakers may name the left object in Figure 1 cake, food, or dessert, a.o. This aspect of reference has been understudied in Computational Linguistics and L&V; as a consequence, it is not clear to what extent current L&V models capture human naming behavior. In the same way that it has proven useful to model referring expressions in visual scenes in an isolated fashion, for which systems are required to integrate and reason over the visual scene and context objects"
2020.lrec-1.710,D14-1086,0,0.834256,"ted with an object is much higher in our dataset than in existing corpora for Language and Vision, such that ManyNames provides a rich resource for studying phenomena like hierarchical variation (chihuahua vs. dog), which has been discussed at length in the theoretical literature, and other less well studied phenomena like cross-classification (cake vs. dessert). Keywords: object naming, language and vision, computer vision 1. Introduction A central issue in Language & Vision (L&V) is how speakers refer to objects. This is most prominent for referring expression generation and interpretation (Kazemzadeh et al., 2014; Mao et al., 2015; Yu et al., 2016), but it also pervades virtually any other L&V task, such as caption generation or visual dialogue (Fang et al., 2015; Devlin et al., 2015; Das et al., 2017; De Vries et al., 2017). One of the central components of referring expressions are object names; for instance, speakers may name the left object in Figure 1 cake, food, or dessert, a.o. This aspect of reference has been understudied in Computational Linguistics and L&V; as a consequence, it is not clear to what extent current L&V models capture human naming behavior. In the same way that it has proven u"
2020.lrec-1.710,J12-1006,0,0.071128,"Missing"
2020.lrec-1.710,P15-1027,0,0.031434,"d fashion, for which systems are required to integrate and reason over the visual scene and context objects in which an object is presented, we believe there is value in modeling object names on their own. Specifically, questions that need addressing regarding object naming are (1) how much naming variation is attested and what factors drive the choice of a name (object category? individual properties of the object? context?); see e.g. Rohde et al. (2012) and Graf et al. (2016); (2) how to make L&V models more human-like with respect to naming, improving the design of L&V architectures (e.g., Lazaridou et al. (2015); Ordonez et al., (2016); Zhao et al., (2017)). In this paper, we present a new dataset, ManyNames, to provide richer possibilities for both analysis and modeling of human naming behavior. Existing resources in L&V that provide object names can be exploited for this area of inquiry to a limited extent only, as we will detail in Section 3, because their low number of annotations per item prevents reliable assessment of naming preferences, on the one hand, and variation, on the other. We chose a creation methodology for ManyNames that overcome these shortcomings in particular. cake (53), food (1"
2020.lrec-1.710,P14-5010,0,0.00242758,"x contains several objects); (iv) the most frequent elicited name is of the same domain as the VG name. This yielded 25, 596 images (we discarded 5, 497). We then did 3 more collection rounds, obtaining a total of 36 annotations per object. Figure 5 shows the instructions for these rounds; they were accompanied by a FAQ solving common issues. We shuffled the set of images per task between rounds, and workers could only participate in one round, to avoid workers annotating an instance more than once. Overall 841 workers4 and image in VG is chosen at most once). 3 We obtained tags with CoreNLP (Manning et al., 2014). 4 5796 Participation was restricted to residents of the UK, USA, Figure 4: Instructions for AMT annotators for the first round (whole instructions showed more examples, see Figure 5). Figure 5: Instructions for AMT annotators for rounds 2 to 4. took part in the data elicitation, with a median of 261 instances (range = [9, 17K]) per worker. 5. Analysis As shown in Table 1 above, ManyNames gathers many more names per object than previous datasets: 35.3 on average, compared to 1−7. It also contains the most variability, since objects have on average 5.7 names (compared to 1 − 1.9). Figure 6 sho"
2020.lrec-1.710,J91-4003,0,0.703822,"ators often chose a more general name than the VG annotators. In a qualitative analysis, we found the following types of variation in the data, illustrated with examples in Figure 6: Cross-classification: a substantial group are names conceptualizing alternative aspects of the same object (e.g. toast/dessert, image C). Conceptual disagreement: as we did not filter objects for prototypicality, our data mirrors a certain amount of disagreement between speakers as to what an object is (bed/bench, image J). Metonymy: we find examples reminiscent of metonymy discussed in the linguistic literature (Pustejovsky, 1991) where logically related parts of an object stand in as its name (burger/basket, image B). Issues with WordNet: due to WordNet’s fine-grained hierarchy, it is difficult to retrieve certain loose synonyms or hypernyms (robe/dress, image not shown). 6. Conclusion The question of how people choose names for objects presented visually is relevant for Language and Vision, Computational Linguistics, Computer Vision, Cognitive Science, 5 To detect hypernyms, we use the hypernym closure of the synset with a depth of 10; the other relations are straightforward. The coverage of WordNet for our name data"
2021.blackboxnlp-1.37,2020.acl-main.385,0,0.0207045,"ational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to possess in order to process natural language, namely detecting one or m"
2021.blackboxnlp-1.37,Q19-1004,0,0.0253412,"/DiscreteSeq 468 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 468–478 Online, November 11, 2021. ©2021 Association for Computational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019)."
2021.blackboxnlp-1.37,P19-1283,0,0.0160308,"019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to possess in order to process natural language, namely detecting one or more features in a sequence of tokens, possibly in a context- and/or order-sensitive way. 3 3.1 Description of the tasks Task description it has 2 activated features. The tasks are meant to be incremental, in the sense that, when processing the input sequence, the model does not know what information it will need to retrieve from it. Some uses of Transformers in the NLP literature instead give access to all th"
2021.blackboxnlp-1.37,P18-1198,1,0.825581,"ber 11, 2021. ©2021 Association for Computational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to possess in order to proces"
2021.blackboxnlp-1.37,P19-1285,0,0.063215,"Missing"
2021.blackboxnlp-1.37,N19-1423,0,0.0335728,"tasks that highlight one of the core abilities that a model needs to possess in order to process natural language, namely detecting one or more features in a sequence of tokens, possibly in a context- and/or order-sensitive way. 3 3.1 Description of the tasks Task description it has 2 activated features. The tasks are meant to be incremental, in the sense that, when processing the input sequence, the model does not know what information it will need to retrieve from it. Some uses of Transformers in the NLP literature instead give access to all the information from the start, such as in BERT (Devlin et al., 2019) and its variants. Arguably, incrementality is necessary in most instances of language understanding “in the wild”: For some applications, like Machine Translation for documents, it is realistic to give access to all the input at once, but as we move towards real-time applications such as virtual assistants, models will need to act incrementally. T1: one-feature detection. T1 asks whether the active feature of the query is present in some token in the sequence. A linguistic example, relevant for syntactic processing, would be: did the plural feature occur in a span of tokens? In the example in"
2021.blackboxnlp-1.37,D19-1275,0,0.0730818,"r into account, with deelling made it harder to understand which model coder attention performing most of the work in components are relevant and how they work. This other cases. Positional encoding plays a counterinhas motivated the NLP community to develop new tuitive role, helping in tasks that do not involve ormethods for model analysis. der by providing beneficial noise, and harming perGiven the difficulties analyzing large models, formance when it is redundant with self-attention. some of this work (Hupkes et al., 2018; Lake and The LSTM only compares favorably to the TransBaroni, 2018; Hewitt and Liang, 2019; Chrupała former in short sequence lengths for simple tasks, and Alishahi, 2019; White and Cotterell, 2021) and decoder attention is highly beneficial for this has focused on controlled tasks that emulate spe- model. cific aspects of language. We propose a new set of 1 such controlled tasks to explore a crucial aspect of The associated dataset and code can be downloaded at: natural language processing that has not received https://github.com/sorodoc/DiscreteSeq 468 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 468–478 Online, Novem"
2021.blackboxnlp-1.37,Q16-1037,0,0.0345191,"468–478 Online, November 11, 2021. ©2021 Association for Computational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to poss"
2021.blackboxnlp-1.37,2021.emnlp-main.230,0,0.0931003,"Missing"
2021.blackboxnlp-1.37,E17-1001,0,0.070068,"Missing"
2021.blackboxnlp-1.37,P19-1580,0,0.0230843,"sociation for Computational Linguistics Figure 1: Schematic description of our tasks. Different tasks require different interpretations of the query. The answer is positive for all these instances. 2 Related Work The current study belongs in the newly developed area of interpretability and explainability of computational linguistics, which seeks to understand how models capture natural language phenomena (Alishahi et al., 2019; Belinkov and Glass, 2019; Rogers et al., 2021). Much of this literature has focused on model behaviour in complex NLP tasks (Linzen et al., 2016; Conneau et al., 2018; Voita et al., 2019; Abnar and Zuidema, 2020; Sinha et al., 2021; Lakretz et al., 2021). Some other work has used controlled tasks to analyze neural networks regarding specific aspects, such as reasoning skills (Weston et al., 2015), compositionality (Lake and Baroni, 2018; Hupkes et al., 2020), PoS tagging (Hewitt and Liang, 2019), inductive biases (White and Cotterell, 2021) or hierarchical structure (Hupkes et al., 2018; Chrupała and Alishahi, 2019). We follow this latter methodology, and design tasks that highlight one of the core abilities that a model needs to possess in order to process natural language,"
2021.blackboxnlp-1.37,2021.acl-long.38,0,0.180181,"f the work in components are relevant and how they work. This other cases. Positional encoding plays a counterinhas motivated the NLP community to develop new tuitive role, helping in tasks that do not involve ormethods for model analysis. der by providing beneficial noise, and harming perGiven the difficulties analyzing large models, formance when it is redundant with self-attention. some of this work (Hupkes et al., 2018; Lake and The LSTM only compares favorably to the TransBaroni, 2018; Hewitt and Liang, 2019; Chrupała former in short sequence lengths for simple tasks, and Alishahi, 2019; White and Cotterell, 2021) and decoder attention is highly beneficial for this has focused on controlled tasks that emulate spe- model. cific aspects of language. We propose a new set of 1 such controlled tasks to explore a crucial aspect of The associated dataset and code can be downloaded at: natural language processing that has not received https://github.com/sorodoc/DiscreteSeq 468 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 468–478 Online, November 11, 2021. ©2021 Association for Computational Linguistics Figure 1: Schematic description of our tasks."
2021.conll-1.36,2020.tacl-1.5,0,0.162029,"he context of a referring expression and the communicated with shorter, less informative words expression itself. Instead, we aim at obtaining preand be pronounced more quickly and flatly (Fer- dictability scores that are only conditioned on the rer i Cancho and Solé, 2003; Aylett and Turk, 2004; context, following the definition of predictability Levy and Jaeger, 2007; Piantadosi et al., 2011). used in psycholinguistics. To this end, we miniThis is posited to result from a trade-off between mally modify a state-of-the-art coreference system clarity and cost: when the context already clearly (Joshi et al., 2020) to also carry out what we call conveys certain information, making it explicit masked coreference resolution (Figure 1): computwould be inefficient. In the domain of referring ing referent probabilities without observing the tarexpressions, or mentions, the prediction is that pro- get mention. We show that the resulting model renouns such as “they” and “her” are used for more tains standard coreference resolution performance, * First two authors contributed equally. while yielding a better estimate of human-derived 454 Proceedings of the 25th Conference on Computational Natural Language Learn"
2021.conll-1.36,D19-1588,0,0.0114866,"= (antecedent[MASK] = none) = 0.2 Figure 1: An example of deriving referent probabilities from masked coreference resolution predictions. the NLP literature, such as cluster-ranking (Clark and Manning, 2016) or span-ranking architectures (Lee et al., 2017). We focus on span-ranking models, which output, for each mention, a probability distribution over its potential antecedents. We rely on an existing state-of-the-art ‘end-toend’ coreference resolution system based on the SpanBERT language model (Joshi et al., 2020), henceforth SpanBERT-coref. It builds directly on the coreference systems of Joshi et al. (2019) and Lee et al. (2018), the main innovation being its reliance on SpanBERT in place of (respectively) BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018). We give more details about the system in Section 3. SpanBERT and BERT are transformer encoders pretrained on masked language modeling (Devlin et al., 2018): a percentage of tokens is masked – substituted with a special token [MASK]; the model has to predict these tokens based on their surrounding context. For training SpanBERT, random contiguous sets of tokens –spans– are masked, and an additional Span Boundary Objective encourages mea"
2021.conll-1.36,D17-1018,0,0.0237112,"more-upf/ masked-coreference. deep learning approaches have been proposed in 455 0.2 none: 0.2 0.3 0.3 Meg congratulated the colleague. She was happy because [MASK] got promoted. P (E[MASK] = {Meg}) = P (antecedent[MASK] = Meg) = 0.2 P (E[MASK] = {the colleague, she}) = P (antecedent[MASK] = the colleague) + P (antecedent[MASK] = She) = 0.6 P (E[MASK] = new) = (antecedent[MASK] = none) = 0.2 Figure 1: An example of deriving referent probabilities from masked coreference resolution predictions. the NLP literature, such as cluster-ranking (Clark and Manning, 2016) or span-ranking architectures (Lee et al., 2017). We focus on span-ranking models, which output, for each mention, a probability distribution over its potential antecedents. We rely on an existing state-of-the-art ‘end-toend’ coreference resolution system based on the SpanBERT language model (Joshi et al., 2020), henceforth SpanBERT-coref. It builds directly on the coreference systems of Joshi et al. (2019) and Lee et al. (2018), the main innovation being its reliance on SpanBERT in place of (respectively) BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018). We give more details about the system in Section 3. SpanBERT and BERT are tra"
2021.conll-1.36,N18-2108,0,0.0295089,"Missing"
2021.conll-1.36,Q17-1003,0,0.246386,"eported the same pronominalization rates in contexts that favored different referents, and argued for a mismatch between speaker and listener strategies (e.g., Rohde and Kehler, 2014; Mayol, 2018). While the aforementioned psycholinguistic studies used tightly controlled stimuli, researchers also reported contradictory results when looking at cloze task responses in corpus data. Tily and Piantadosi (2009) considered newspaper texts from the OntoNotes corpus and found that pronouns and proper names were favoured over full NPs when subjects were able to guess the upcoming referent. By contrast, Modi et al. (2017b) did not find this effect on narrative texts in the InScript corpus (Modi et al., 2017a). Computational estimates of predictability. Probabilistic language models have been commonly adopted to study expectation-based human language comprehension (Armeni et al., 2017). Predictability scores obtained from language models have been shown to correlate with measures of cognitive cost at the lexical and the syntactic levels (Smith and Levy, 2013; Frank et al., 2013). In this work, predictability is typically measured with solely the preceding context in computational psycholinguistics (e.g., Levy,"
2021.conll-1.36,P15-1158,0,0.131722,"nal psycholinguistics (e.g., Levy, 2008). However, more recent work has also looked at predictability calculated based on both the previous and following contexts using bidirectional networks, like we do: Pimentel et al. (2020) used surprisal calculated from a cloze language model which takes both left and right pieces of context, as the operationalization of word predictability. They studied the trade-off between clarity and cost and reported a tendency for ambiguous words to appear in highly informative contexts. Previous work also used computational estimates of referent predictability. In Orita et al. (2015), they were used to explain referential choice as the result of an addressee-oriented strategy. Their measures of predictability, however, were based on simple features like frequency or recency. Modi et al. (2017b) built upcoming referent prediction models combining shallow linguistic features and script knowledge. This approach allowed them to disentangle the role of linguistic and common-sense knowledge, respectively, on human referent predictions. More recent research assessed the ability of autoregressive language models to mimick referential expectation biases that humans have shown in t"
2021.conll-1.36,N18-1202,0,0.0165586,"such as cluster-ranking (Clark and Manning, 2016) or span-ranking architectures (Lee et al., 2017). We focus on span-ranking models, which output, for each mention, a probability distribution over its potential antecedents. We rely on an existing state-of-the-art ‘end-toend’ coreference resolution system based on the SpanBERT language model (Joshi et al., 2020), henceforth SpanBERT-coref. It builds directly on the coreference systems of Joshi et al. (2019) and Lee et al. (2018), the main innovation being its reliance on SpanBERT in place of (respectively) BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018). We give more details about the system in Section 3. SpanBERT and BERT are transformer encoders pretrained on masked language modeling (Devlin et al., 2018): a percentage of tokens is masked – substituted with a special token [MASK]; the model has to predict these tokens based on their surrounding context. For training SpanBERT, random contiguous sets of tokens –spans– are masked, and an additional Span Boundary Objective encourages meaningful span representations at the span’s boundary tokens.2 3 Methods score is computed for each span (sm : how likely it is to be a mention), and a compatibi"
2021.conll-1.36,2020.emnlp-main.328,0,0.0885567,"Missing"
2021.conll-1.36,W12-4501,0,0.0305252,"umans have shown in the psycholinguistic literature (e.g., Upadhye et al., 2020). Davis and van Schijndel (2021) extended this assessment to non-autoregressive models, like the ones we use here, and reported results consistent with prior work in autoregressive models, showing that these models exhibited biases in line with existing evidence on human behavior, at least for English. Automatic coreference resolution. The goal of a standard coreference resolution system is to group mentions in a text according to the real-world en1 We make the code used to carry out this study tity they refer to (Pradhan et al., 2012). Several available at https://github.com/amore-upf/ masked-coreference. deep learning approaches have been proposed in 455 0.2 none: 0.2 0.3 0.3 Meg congratulated the colleague. She was happy because [MASK] got promoted. P (E[MASK] = {Meg}) = P (antecedent[MASK] = Meg) = 0.2 P (E[MASK] = {the colleague, she}) = P (antecedent[MASK] = the colleague) + P (antecedent[MASK] = She) = 0.6 P (E[MASK] = new) = (antecedent[MASK] = none) = 0.2 Figure 1: An example of deriving referent probabilities from masked coreference resolution predictions. the NLP literature, such as cluster-ranking (Clark and Man"
alsina-etal-2002-catcg,A97-2015,1,\N,Missing
C04-1161,P98-1013,0,0.0222953,"combined with other kinds of adjectives, mainly object adjectives, they will ap´ pear at the peripheria (an`alisi politica seriosa, ‘serious political analysis’). This parameter can again be used for basic tasks such as POS-tagging: Adjective-noun ambiguity is notoriously the most difficult one to solve, and the ordering restrictions on the classes of adjectives can help to reduce it. However, it is most useful for semantic tasks. For instance, object adjectives can evoke arguments when combined with predicative nouns (presidential visit - a president visits X). For projects such as FrameNet (Baker et al., 1998), 1 Note that we do not state that adjectives denote objects or events, but that they imply an object or event in their denotation. This kind of adjectives denotes properties or states, but with an embedded or “shadow” argument (Pustejovsky, 1995), similarly to verbs like to butter. these kinds of relationships could be automatically extracted if information on the class were available. The same applies to event adjectives, this time being predicates (flipping coin - a coin flips). 2.4 Morphology vs. syntax It could seem that the semantic classes established for the second parameter amount to"
C04-1161,J96-2004,0,0.186915,"Missing"
C04-1161,P93-1023,0,0.687346,"need for the automatic acquisition of lexical information arised from the so-called “lexical bottleneck” in NLP systems: no matter whether symbolic or statistical, all systems need more and more lexical information in order to be able to predict a word’s behaviour, and this information is very hard and costly to encode manually. In recent research in the field, the main effort has been to infer semantic classes for verbs, in English (Stevenson et al., 1999) and German (Schulte im Walde and Brew, 2002). In this paper, we concentrate on adjectives, which have received less attention (see though Hatzivassiloglou and McKeown (1993) and Lapata (2001)). Our aim is to establish semantic classes for adjectives in Catalan by means of clustering, using only shallow syntactic evidence. We compare the results with a set of adjectives classified by human judges according to semantic characteristics. Thus, we intend to induce semantic properties from syntactic distribution. We now justify each of the choices: why adjectives, why clustering, and why shallow features. Adjectives are predicates, equivalent to verbs when appearing in predicative environments. A Eloi Batlle Audiovisual Institute Universitat Pompeu Fabra Pg. Circumval."
C04-1161,N01-1009,0,0.160096,"xical information arised from the so-called “lexical bottleneck” in NLP systems: no matter whether symbolic or statistical, all systems need more and more lexical information in order to be able to predict a word’s behaviour, and this information is very hard and costly to encode manually. In recent research in the field, the main effort has been to infer semantic classes for verbs, in English (Stevenson et al., 1999) and German (Schulte im Walde and Brew, 2002). In this paper, we concentrate on adjectives, which have received less attention (see though Hatzivassiloglou and McKeown (1993) and Lapata (2001)). Our aim is to establish semantic classes for adjectives in Catalan by means of clustering, using only shallow syntactic evidence. We compare the results with a set of adjectives classified by human judges according to semantic characteristics. Thus, we intend to induce semantic properties from syntactic distribution. We now justify each of the choices: why adjectives, why clustering, and why shallow features. Adjectives are predicates, equivalent to verbs when appearing in predicative environments. A Eloi Batlle Audiovisual Institute Universitat Pompeu Fabra Pg. Circumval.laci´o 8 08003 Bar"
C04-1161,J01-3003,0,0.247629,"Missing"
C04-1161,P02-1029,0,0.082731,"roperties from distributional evidence, taken as a generalisation of a word’s linguistic behaviour in corpora. The need for the automatic acquisition of lexical information arised from the so-called “lexical bottleneck” in NLP systems: no matter whether symbolic or statistical, all systems need more and more lexical information in order to be able to predict a word’s behaviour, and this information is very hard and costly to encode manually. In recent research in the field, the main effort has been to infer semantic classes for verbs, in English (Stevenson et al., 1999) and German (Schulte im Walde and Brew, 2002). In this paper, we concentrate on adjectives, which have received less attention (see though Hatzivassiloglou and McKeown (1993) and Lapata (2001)). Our aim is to establish semantic classes for adjectives in Catalan by means of clustering, using only shallow syntactic evidence. We compare the results with a set of adjectives classified by human judges according to semantic characteristics. Thus, we intend to induce semantic properties from syntactic distribution. We now justify each of the choices: why adjectives, why clustering, and why shallow features. Adjectives are predicates, equivalent"
C04-1161,W99-0503,0,0.0285326,"ition is that it is possible to infer lexical properties from distributional evidence, taken as a generalisation of a word’s linguistic behaviour in corpora. The need for the automatic acquisition of lexical information arised from the so-called “lexical bottleneck” in NLP systems: no matter whether symbolic or statistical, all systems need more and more lexical information in order to be able to predict a word’s behaviour, and this information is very hard and costly to encode manually. In recent research in the field, the main effort has been to infer semantic classes for verbs, in English (Stevenson et al., 1999) and German (Schulte im Walde and Brew, 2002). In this paper, we concentrate on adjectives, which have received less attention (see though Hatzivassiloglou and McKeown (1993) and Lapata (2001)). Our aim is to establish semantic classes for adjectives in Catalan by means of clustering, using only shallow syntactic evidence. We compare the results with a set of adjectives classified by human judges according to semantic characteristics. Thus, we intend to induce semantic properties from syntactic distribution. We now justify each of the choices: why adjectives, why clustering, and why shallow fe"
C04-1161,C98-1013,0,\N,Missing
C14-1097,W11-2501,0,0.672349,", proper nouns, adjectives and verbs) with a corpus frequency of 500 or larger. The resulting U+ corpus has roughly 133K word types and 2.8B word tokens. We created a vector space by counting co-occurrences of these word types within a window of two words on the left and the right, using the top 20k most frequent content words as dimensions. The space was transformed using Positive Pointwise Mutual Information (PPMI). U+Sent: The U+Sent space is constructed the same way as U+W2, but uses full sentence contexts instead of 2-word windows. TypeDM: This space is extracted from the TypeDM tensors (Baroni and Lenci, 2011). TypeDM contains a list of weighted tuples, hhw1 , l, w2 i, σi, where w1 and w2 are content words, l is a corpus-derived syntagmatic relationship between the words, and σ is a weight estimating saliency of the relationship. We construct vectors for every unique w1 using the set of hl, w2 i pairs as dimensions and corresponding σ values as dimension weights. We select TypeDM for its excellent performance in previous comparisons of distributional hypernymy measures (Lenci and Benotto, 2012). Reduced Spaces: In some experiments, we use dimensionality reduced spaces. We reduce all three spaces to"
C14-1097,E12-1004,0,0.261153,"ibutional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal (e.g. “animal rights”), but not so much for dog (e.g. #“dog rights”). This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy dete"
C14-1097,P99-1008,0,0.0266216,"imensions are context items (for example, other words) and the coordinates of the vector indicate the target’s degree of association with each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the D"
C14-1097,W04-3205,0,0.00966199,"uced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in whic"
C14-1097,W09-0215,0,0.0257716,"cision P (r) at every rank r among u’s dimensions – where precision is the fraction of dimensions shared with v –, and weighting by the rank of the same dimension in the broader term, rel0 (v, r, u). The final measure, balAPinc, smooths using the LIN similarity measure (Lin, 1998). (We only sketch this measure here due to its complexity; details are given in Kotlerman et al. (2010).) ( 1 if x &gt; 0; 1(x) = 0 otherwise Pn u · 1(vi ) Pn i W eedsP rec(u, v) = i=1 (1) i=1 ui P|1(u)| P (r) · rel0 (v, r, u)) APinc(u, v) = r=1 (2) |1(u)| p balAPinc(u, v) = APinc(u, v) · LIN(u, v) The ClarkeDE measure (Clarke, 2009) computes degree of entailment as the degree to which the narrower term u has lower values than v across all dimensions (eq. 3). Lenci and Benotto (2012) introduce the invCL measure, which uses ClarkeDE to measure both distributional inclusion of u in v and distributional non-inclusion of v in u (eq. 4). While all other measures interpret the Distributional Inclusion Hypothesis as the degree to which a ⊆ relation holds, Lenci and Benotto test the degree to which proper inclusion ( holds. They consider not only the degree to which the contexts of the narrower terms are included in the contexts"
C14-1097,C04-1036,0,0.0220452,"to distinguish between semantic relations: Typical nearest neighbors of dog are words like cat, animal, puppy, tail, or owner, all obviously related to dog, but through very different types of semantic relations. On these grounds, Murphy (2002) argues that distributional models cannot be a valid model of conceptual representation. Distinguishing semantic relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog"
C14-1097,N03-1011,0,0.0130184,"(for example, other words) and the coordinates of the vector indicate the target’s degree of association with each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclus"
C14-1097,C92-2082,0,0.516587,"been observed, usually in the form of a vector representation (Turney and Pantel, 2010). A target word is represented as a vector in a high-dimensional space in which the dimensions are context items (for example, other words) and the coordinates of the vector indicate the target’s degree of association with each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation indu"
C14-1097,P13-2078,0,0.0181,"relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appe"
C14-1097,S12-1012,0,0.387129,". Distinguishing semantic relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which sta"
C14-1097,N13-1090,0,0.00751658,"ifferences features5 are analogous to a supervised distributional inclusion measure. The difference between two words on a particular dimension captures the degree of distributional inclusion on that dimension. The primary distinction between the difference features and the unsupervised measures is that the supervised classifier learns to weight the importance of different dimensions. The f features encode directional aspects of distributional inclusion: that the hyponym contexts should be included in 4 After recent work using subtraction to represent analogy in certain neural-network spaces (Mikolov et al., 2013). We also tried variations, such as not normalizing vectors and removing the difference squared vector, but found this setting the best. We also tried the Diff features with an SVM and other nonlinear classifiers, but they performed worse. 5 1030 Data set Baseline Classifier U+W2300 U+Sent300 TypeDM300 B LESS .46 Concat Diff .76 .84 .73 .80 .82 E NTAILMENT .50 Concat Diff .81 .85 .78 .82 .65 .85 Table 2: Average accuracy of Concat and Diff on B LESS and E NTAILMENT using different spaces for feature generation. those of the hypernym (the weight learned is positive), and the hypernym contexts s"
C14-1097,P06-1015,0,0.0335869,"Missing"
C14-1097,P06-1101,0,0.0594759,"hat NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more distributionally general than hyponyms (u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy or the more loosely defined relation"
C14-1097,J06-3003,0,0.0391346,"nducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1 ] and other [NP2 ]” implies that NP2 is a hypernym of NP1 . Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more distributionally general than hypony"
C14-1097,W03-1011,0,0.0371289,"nymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more distributionally general than hyponyms (u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy or the more loosely defined relation of lexical entailment. Weeds and Weir (2003) measure distributional generality using a notion of precision (eq. 1). Here and in all equations below, u is the narrower term, and v the more general one. Abusing notation, we write u for both a word and its associated vector hu1 , . . . , un i. Kotlerman et al. (2010) predict lexical entailment with the balAPinc measure, a modification of the Average Precision (AP) measure (eq. 2). The general notion is that scores should increase with the number of dimensions of v that u shares, and also give more weight to the highly ranked dimensions (i.e. largest magnitude) of the narrower term u. This"
C14-1097,C04-1146,0,0.232308,"e a valid model of conceptual representation. Distinguishing semantic relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 200"
C14-1097,P05-1014,0,0.961617,"rity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal (e.g. “animal rights”), but not so much for dog (e.g. #“dog rights”). This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We show that the current best unsupervised approach is brittle in that their performance depends on"
C14-1097,J09-3004,0,0.0148978,"otlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal (e.g. “animal rights”), but not so much for dog (e.g. #“dog rights”). This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We show that the current best unsupervised approach is brittle in that their performance depends on the space they are applied to. This r"
C14-1097,P06-1085,0,\N,Missing
C14-1097,J06-1005,0,\N,Missing
D07-1018,alsina-etal-2002-catcg,1,0.719581,"of features across the three classes. Level morph func uni bi sem all Explanation morphological (derivational) properties syntactic function uni-gram distribution bi-gram distribution distributional cues of semantic properties combination of the 5 linguistic levels # Features 2 4 24 50 18 10.3 Table 1: Linguistic levels as feature sets. tion from the adjective database. Syntactic and semantic features encode distributional properties of adjectives. Syntactic features comprise three subtypes: (i) the syntactic function (level func) of the adjective, as assigned by a shallow Constraint Grammar (Alsina et al., 2002), distinguishing the modifier (pre-nominal or post-nominal) and predicative functions; (ii) a unigram distribution (level uni), independently encoding the parts of speech (POS) of the words preceding and following the adjective, respectively; and (iii) a bigram distribution (level bi), the POS bigram around the target adjective, considering only the 50 most frequent bigrams to avoid sparse features. Semantic features (level sem) expand syntactic features with heterogeneous shallow cues of semantic properties. Table 2 lists the semantic properties encoded in the features, as well as the number"
D07-1018,C04-1161,1,0.924213,"Missing"
D07-1018,W05-1009,1,0.884155,"Missing"
D07-1018,brants-2000-inter,0,0.233811,"Missing"
D07-1018,C00-1023,0,0.374331,"es (similar to determiners, like viele ‘many’), referential adjectives (heutige, ‘of today’), qualitative adjectives (equivalent to basic adjectives), classificatory adjectives (equivalent to object adjectives), and adjectives of origin (Stuttgarter, ‘from Stuttgart’). In a recent paper, Yallop et al. (2005) reported experiments on the acquisition of syntactic subcategorisation patterns for English adjectives. Apart from the above research with a classificatory flavour, other lines of research exploited lexical relations among adjectives for Word Sense Disambiguation (Justeson and Katz, 1995; Chao and Dyer, 2000). Work by Lapata (2001), contrary to the studies mentioned so far, focused on the meaning of adjective-noun combinations, not on that of adjectives alone. 8 Conclusion This paper has presented an architecture for the semantic classification of Catalan adjectives that explicitly includes polysemous classes. The focus of the architecture was on two issues: (i) finding an appropriate set of linguistic features, and (ii) defining an adequate architecture for the task. The investigation and comparison of features at various linguistic levels has shown that morphology plays a major role for the targ"
D07-1018,P93-1023,0,0.424175,"Missing"
D07-1018,P97-1023,0,0.543866,"Missing"
D07-1018,C00-1044,0,0.291835,"Missing"
D07-1018,H05-1124,0,0.0650012,"and as object in each of the binary decisions, it was deemed polysemous (BO). The motivation behind this approach was that polysemous adjectives should exhibit properties of all the classes involved. As a result, positive decisions on each binary classification can be made by the algorithm, which can be viewed as implicit polysemous assignments. This classification architecture is very popular in Machine Learning for multi-label problems, cf. (Schapire and Singer, 2000; Ghamrawi and McCallum, 2005), and has also been applied to NLP problems such as entity extraction and noun-phrase chunking (McDonald et al., 2005). The remainder of this section describes other methodological aspects of our experiments. 4.1 Classifier: Decision Trees As classifier for the binary decisions we chose Decision Trees, one of the most widely used Machine Learning techniques for supervised experiments (Witten and Frank, 2005). Decision Trees provide a transparent representation of the decisions made by the algorithm, and thus facilitate the inspection of results and the error analysis. The experiments were carried out with the freely available Weka software package. The particular algorithm chosen, Weka’s J48, is the latest op"
D07-1018,J01-3003,0,0.122499,"-the-art Machine Learning architecture for Multi-label Classification (Schapire and Singer, 2000; Ghamrawi and McCallum, 2005) and an Ensemble Classifier (Dietterich, 2002) with b) the definition of features at various levels of linguistic description. A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architec"
D07-1018,P93-1024,0,0.101669,"at various levels of linguistic description. A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architecture should allow related tasks to profit from our insights. As target classification for the experiments, a set of 210 Catalan adjectives was manually classified by experts into three simple and three p"
D07-1018,P97-1007,0,0.0156307,"ention in the Machine Learning community in the last decade (Dietterich, 2002). When building an ensemble classifier, several class proposals for each item are obtained, and one of them is chosen on the basis of majority voting, weighted voting, or more sophisticated decision methods. It has been shown that in most cases, the accuracy of the ensemble classifier is higher than the best individual classifier (Freund and Schapire, 1996; Dietterich, 2000; Breiman, 2001). Within NLP, ensemble classifiers have been applied, for instance, to genus term disambiguation in machinereadable dictionaries (Rigau et al., 1997), using a majority voting scheme upon several heuristics, and to part of speech tagging, by combining the class predictions of different algorithms (van Halteren et Levels morph+func+uni+bi+sem+all func+uni+bi+sem morph+func+sem+all bl all Full Ac. 84.0 ±0.06 81.5 ±0.04 72.4 ±0.03 51.0 ±0.0 62.3 ±2.3 Part. Ac. 95.7 ±0.02 95.9 ±0.01 89.3 ±0.02 65.2 ±0.0 90.7 ±1.6 Table 7: Results for ensemble classifier. al., 1998). The main reason for the general success of ensemble classifiers is that they gloss over the biases introduced by the individual systems. We implemented an ensemble classifier by usi"
D07-1018,P99-1014,0,0.0505561,"inguistic description. A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architecture should allow related tasks to profit from our insights. As target classification for the experiments, a set of 210 Catalan adjectives was manually classified by experts into three simple and three polysemous semantic cl"
D07-1018,J95-1001,0,0.496279,"een quantitative adjectives (similar to determiners, like viele ‘many’), referential adjectives (heutige, ‘of today’), qualitative adjectives (equivalent to basic adjectives), classificatory adjectives (equivalent to object adjectives), and adjectives of origin (Stuttgarter, ‘from Stuttgart’). In a recent paper, Yallop et al. (2005) reported experiments on the acquisition of syntactic subcategorisation patterns for English adjectives. Apart from the above research with a classificatory flavour, other lines of research exploited lexical relations among adjectives for Word Sense Disambiguation (Justeson and Katz, 1995; Chao and Dyer, 2000). Work by Lapata (2001), contrary to the studies mentioned so far, focused on the meaning of adjective-noun combinations, not on that of adjectives alone. 8 Conclusion This paper has presented an architecture for the semantic classification of Catalan adjectives that explicitly includes polysemous classes. The focus of the architecture was on two issues: (i) finding an appropriate set of linguistic features, and (ii) defining an adequate architecture for the task. The investigation and comparison of features at various linguistic levels has shown that morphology plays a m"
D07-1018,W03-0410,0,0.213867,"itecture for Multi-label Classification (Schapire and Singer, 2000; Ghamrawi and McCallum, 2005) and an Ensemble Classifier (Dietterich, 2002) with b) the definition of features at various levels of linguistic description. A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architecture should allow related tasks"
D07-1018,P03-1009,0,0.202383,". A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repreToni Badia GLiCom Universitat Pompeu Fabra 08003 Barcelona toni.badia@upf.edu sents a pervasive phenomenon in natural language. However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes). There are a few exceptions to this tradition, such as Pereira et al. (1993), Rooth et al. (1999), Korhonen et al. (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Our work addresses the lack of methodology in modelling a polysemous classification. We implement a multi-label classification architecture to handle polysemy. This paper concentrates on the classification of Catalan adjectives, but the general nature of the architecture should allow related tasks to profit from our insights. As target classification for the experiments, a set of 210 Catalan adjectives was manually classified by experts into three simple and three polysemous semantic classes. We deliberately d"
D07-1018,P98-1081,0,0.0824816,"Missing"
D07-1018,N01-1009,0,0.132463,"ke viele ‘many’), referential adjectives (heutige, ‘of today’), qualitative adjectives (equivalent to basic adjectives), classificatory adjectives (equivalent to object adjectives), and adjectives of origin (Stuttgarter, ‘from Stuttgart’). In a recent paper, Yallop et al. (2005) reported experiments on the acquisition of syntactic subcategorisation patterns for English adjectives. Apart from the above research with a classificatory flavour, other lines of research exploited lexical relations among adjectives for Word Sense Disambiguation (Justeson and Katz, 1995; Chao and Dyer, 2000). Work by Lapata (2001), contrary to the studies mentioned so far, focused on the meaning of adjective-noun combinations, not on that of adjectives alone. 8 Conclusion This paper has presented an architecture for the semantic classification of Catalan adjectives that explicitly includes polysemous classes. The focus of the architecture was on two issues: (i) finding an appropriate set of linguistic features, and (ii) defining an adequate architecture for the task. The investigation and comparison of features at various linguistic levels has shown that morphology plays a major role for the target classification, desp"
D07-1018,J93-2004,0,0.0461699,"Missing"
D07-1018,P05-1076,0,0.15006,"to automatically identify adjectival scales from corpora. Coordination information was used in Bohnet et al. (2002) for a classification task similar to the task we pursue, using a bootstrapping approach. The authors, however, pursued a classification that is not purely semantic, between quantitative adjectives (similar to determiners, like viele ‘many’), referential adjectives (heutige, ‘of today’), qualitative adjectives (equivalent to basic adjectives), classificatory adjectives (equivalent to object adjectives), and adjectives of origin (Stuttgarter, ‘from Stuttgart’). In a recent paper, Yallop et al. (2005) reported experiments on the acquisition of syntactic subcategorisation patterns for English adjectives. Apart from the above research with a classificatory flavour, other lines of research exploited lexical relations among adjectives for Word Sense Disambiguation (Justeson and Katz, 1995; Chao and Dyer, 2000). Work by Lapata (2001), contrary to the studies mentioned so far, focused on the meaning of adjective-noun combinations, not on that of adjectives alone. 8 Conclusion This paper has presented an architecture for the semantic classification of Catalan adjectives that explicitly includes p"
D07-1018,C98-1078,0,\N,Missing
D12-1112,J10-4006,0,0.0545233,"rectly harvested from the corpora (that is, based on the cooccurrence of, say, the phrase white towel with each of the 10K words in the space dimensions), without doing any composition. AN vectors obtained by composition will be examined in the following section. Though observed AN vectors should not be regarded as a gold standard in the sense of, for instance, Machine Learning approaches, because they are typically sparse10 and thus the vectors of their component adjective and noun will be richer, they are still useful for exploration and as a comparison point for the composition operations (Baroni and Lenci, 2010; Guevara, 2010). Figure 1 shows the distribution of the cosines between A, N, and AN vectors with intensional adjectives (I, white box), intersective uses of color terms (IE, lighter gray box), and subsective uses of color terms (S, darker gray box). In general, the similarity of the A and N vectors is quite low (cosine &lt; 0.2, left graph of Figure 1), and much lower than the similarities between both the AN and A vectors and the AN and N vectors. This is not surprising, given that adjectives and nouns describe rather different sorts of things. We find significant differences between the three"
D12-1112,D10-1115,0,0.270165,"representation of that meaning (Andrews et al., 2009). However, in order to have a complete theory of natural language meaning, these models must be supplied with or connected to a compositional semantics; otherwise, we will have no account of the recursive potential that natural language affords for the construction of novel complex contents. In the last 4-5 years, researchers have begun to introduce compositional operations on distributional semantic representations, for instance to combine verbs with their arguments or adjectives with nouns (Erk and Pad´o, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2011)1 . Although the proposed operations have shown varying degrees of success in a number of tasks such as detecting phrase similarity and paraphrasing, it remains unclear to what extent they can account for the full range of meaning composition phenomena found in natural language. Higher-order modification (that is, modification that cannot obviously be modeled as property intersection, in contrast to firstorder modification, which can) presents one such challenge, as we will detail in the next section. The goal of this paper is twofold. Fi"
D12-1112,P12-1015,1,0.876099,"Missing"
D12-1112,D08-1094,0,0.0609005,"Missing"
D12-1112,W11-0112,0,0.0350687,"r is twofold. First, we examine how the properties of different types of adjectival modifiers, both in isolation and in combination with nouns, are represented in distributional models. We take as a case study three groups of adjectives: 1) color terms used to ascribe true color properties (referred to here as intersective color terms), as prototypical representative of first-order modifiers; 2) color terms used to ascribe properties other than simple color (here, subsective color terms), as representatives of expressions that could in principle 1 In a complementary direction, Garrette et al. (2011) connect distributional representations of lexical semantics to logicbased compositional semantics. 1223 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1223–1233, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics be given a well-motivated first-order or higher-order analysis; and 3) intensional adjectives (e.g. former), as representative of modifiers that arguably require a higher-order analysis. Formal semantic models tend to group the second and third groups to"
D12-1112,W11-2507,0,0.169091,"Missing"
D12-1112,W10-2805,0,0.351891,"ion with wine, insofar as the former words are less likely than the latter to occur in contexts where white describes wine than in those where it describes dresses. In contrast, it is not immediately obvious how these operations would fare with intensional adjectives such as former. In particular, it is not clear what specific distributional features of the adjective would capture the effect that the ad1225 jective has on the meaning of the resulting modified nominal. Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. On such models, the distributional properties of observed occurrences of adjective-noun pairs are used to induce the effect of adjectives on nouns. Insofar as it is grounded in the intuition that adjective meanings should be modeled as mappings from noun meanings to adjective-noun meanings, the matrix analysis might be expected to perform bett"
D12-1112,2003.mtsummit-papers.42,0,0.061896,"ommonly used Log-Likelihood Ratio but is simpler to compute (Evert, 2005). Specifically, given a row element r, a column element c and a counting function C(r, c), then LM I = C(r, c) · log C(r, c)C(∗, ∗) C(r, ∗)C(∗, c) (1) where C(r, c) is how many times r cooccurs with c, C(r, ∗) is the total count of r, C(∗, c) is the total count of c, and C(∗, ∗) is the cumulative cooccurrence count of any r with any c. The dimensionality of the space was reduced using Singular Value Decomposition (SVD), as in Latent Semantic Analysis and related distributional semantic methods (Landauer and Dumais, 1997; Rapp, 2003; Sch¨utze, 1997). Both LMI and SVD were used for the core vocabulary, and the AN vectors were computed based on the values for the 1226 core vocabulary. All of the results discussed in the article are based on the SVD-reduced space, because it yielded consistently better results, except for those involving multiplicative composition, which was carried out on the non-reduced model because SVD reduction introduces negative values for the latent dimensions used for the reduced space. Some of the parameters of the space and composition functions were set based on performance on independent word s"
D12-1112,D11-1014,0,0.0149038,", in order to have a complete theory of natural language meaning, these models must be supplied with or connected to a compositional semantics; otherwise, we will have no account of the recursive potential that natural language affords for the construction of novel complex contents. In the last 4-5 years, researchers have begun to introduce compositional operations on distributional semantic representations, for instance to combine verbs with their arguments or adjectives with nouns (Erk and Pad´o, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2011)1 . Although the proposed operations have shown varying degrees of success in a number of tasks such as detecting phrase similarity and paraphrasing, it remains unclear to what extent they can account for the full range of meaning composition phenomena found in natural language. Higher-order modification (that is, modification that cannot obviously be modeled as property intersection, in contrast to firstorder modification, which can) presents one such challenge, as we will detail in the next section. The goal of this paper is twofold. First, we examine how the properties of different types of"
D15-1002,J10-4006,1,0.736996,"Missing"
D15-1002,J10-4007,1,0.625165,"Missing"
D15-1002,P09-1113,0,0.00429198,"-of::organization with the value world bank results in a binary attribute Referential Representations As our source of referential attributes, we use FreeBase (see footnote 1), a knowledge base of structured information on a wide range of entities of different semantic types (people, geographical entities, etc.). The information in FreeBase comes from various sources, including Wikipedia and domainspecific databases, plus user content generation and correction. FreeBase currently records at least 2 attributes for over 47 million entities, and it has been used fairly extensively in NLP before (Mintz et al., 2009; Socher et al., 2013a, among others). For each entity, FreeBase contains a list of attribute-value tuples (where values can in turn be entities, allowing a graph view of the data that we do not exploit here). Table 1 shows a sample of the attributes that FreeBase records for countries. Note that some attributes are simple (e.g., date founded), while other can be called complex, in the sense that they are attributes of attributes (e.g., geolocation::latitude). We use a double-colon notation to refer to complex attributes. The values of all attributes can be either numeric or categorical. The n"
D15-1002,W15-0107,0,0.0504358,"Missing"
D15-1002,S12-1019,0,0.0125682,"th Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., B"
D15-1002,J07-2002,1,0.247538,"Missing"
D15-1002,P15-2119,0,0.0494805,"neric noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., Buitelaar and Cimiano (2008) and references therein). This line of work, however, does not attempt to connect entity representations extracted from corpora and fro"
D15-1002,W15-0120,0,0.0479495,"e rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities. Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein). This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 201"
D15-1002,Q14-1023,0,0.0142536,"tional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a"
D15-1002,P14-1132,1,0.435548,"eferential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; F˘ag˘ar˘as¸an et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012). In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes. Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes. Our target features are conceptually very different from those of all these studies. Related Work There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to co"
D15-1002,D13-1136,0,0.0119385,"ose in spirit to what we do, except that, given an entity1-relation-entity2 tuple, we treat relation-entity2 as a binary attribute of entity1, and we try to induce such attributes on a larger scale (Socher et al. consider seven relations in total). Moreover, we rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities. Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein). This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes. The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015). However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities. Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such"
D15-1002,N13-1090,0,0.154617,"Missing"
D16-1123,W12-2703,0,0.0247794,"Missing"
D16-1123,P14-1129,0,0.0362421,"2009). The training subset contains 200 million words and the vocabulary consists of the 200K words that appear more than 5 times in the training subset. The validation and test sets are different subsets of the ukWaC corpus, both containing 120K words. We preprocessed the data similarly to what we did for Europarl-NC. We train our models using Stochastic Gradient Descent (SGD), which is relatively simple to tune compared to other optimization methods that involve additional hyper parameters (such as alpha in RMSprop) while being still fast and effective. SGD is commonly used in similar work (Devlin et al., 2014; Zaremba et al., 2014; Sukhbaatar et al., 2015). The learning rate is kept fixed during a single epoch, but we reduce it by a fixed proportion every time the validation perplexity increases by the end of the epoch. The values for learning rate, learning rate shrinking and mini-batch sizes as well as context size are fixed once and for all based on insights drawn from previous work (Hai Son et al., 2011; Sukhbaatar et al., 2015; Devlin et al., 2014) as well as experimentation with the Penn Treebank validation set. Specifically, the learning rate is set to 0.05, with mini-batch size of 128 (we"
D16-1123,W12-2701,0,0.0705501,"Missing"
D16-1123,P14-1062,0,0.582603,"nguage Models Ngoc-Quan Pham and German Kruszewski and Gemma Boleda Center for Mind/Brain Sciences University of Trento {firstname.lastname}@unitn.it Abstract ject recognition, scene parsing, and action recognition (Gu et al., 2015), but they have received less attention in NLP. They have been somewhat explored in static classification tasks where the model is provided with a full linguistic unit as input (e.g. a sentence) and classes are treated as independent of each other. Examples of this are sentence or document classification for tasks such as Sentiment Analysis or Topic Categorization (Kalchbrenner et al., 2014; Kim, 2014), sentence matching (Hu et al., 2014), and relation extraction (Nguyen and Grishman, 2015). However, their application to sequential prediction tasks, where the input is construed to be part of a sequence (for example, language modeling or POS tagging), has been rather limited (with exceptions, such as Collobert et al. (2011)). The main contribution of this paper is a systematic evaluation of CNNs in the context of a prominent sequential prediction task, namely, language modeling. Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Visio"
D16-1123,D14-1181,0,0.145405,"m and German Kruszewski and Gemma Boleda Center for Mind/Brain Sciences University of Trento {firstname.lastname}@unitn.it Abstract ject recognition, scene parsing, and action recognition (Gu et al., 2015), but they have received less attention in NLP. They have been somewhat explored in static classification tasks where the model is provided with a full linguistic unit as input (e.g. a sentence) and classes are treated as independent of each other. Examples of this are sentence or document classification for tasks such as Sentiment Analysis or Topic Categorization (Kalchbrenner et al., 2014; Kim, 2014), sentence matching (Hu et al., 2014), and relation extraction (Nguyen and Grishman, 2015). However, their application to sequential prediction tasks, where the input is construed to be part of a sequence (for example, language modeling or POS tagging), has been rather limited (with exceptions, such as Collobert et al. (2011)). The main contribution of this paper is a systematic evaluation of CNNs in the context of a prominent sequential prediction task, namely, language modeling. Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks. The"
D16-1123,P07-2045,0,0.0129905,"t sizes and genres, the first two of which have been used for language modeling evaluation before. The Penn Treebank contains one million words of newspaper text with 10K words in the vocabulary. We reuse the preprocessing and training/test/validation division from Mikolov et al. (2014). Europarl-NC is a 64-million word corpus that was developed for a Machine Translation shared task (Bojar et al., 2015), combining Europarl data (from parliamentary debates in the European Union) and News Commentary data. We preprocessed the corpus with tokenization and true-casing tools from the Moses toolkit (Koehn et al., 2007). The vocabulary is composed of words that occur at least 3 times in the training set and contains approximately 60K words. We use the validation and test set of the MT shared task. Finally, we took a subset of the ukWaC corpus, which was constructed by crawling UK websites (Baroni et al., 2009). The training subset contains 200 million words and the vocabulary consists of the 200K words that appear more than 5 times in the training subset. The validation and test sets are different subsets of the ukWaC corpus, both containing 120K words. We preprocessed the data similarly to what we did for E"
D16-1123,W15-1506,0,0.0957321,"iversity of Trento {firstname.lastname}@unitn.it Abstract ject recognition, scene parsing, and action recognition (Gu et al., 2015), but they have received less attention in NLP. They have been somewhat explored in static classification tasks where the model is provided with a full linguistic unit as input (e.g. a sentence) and classes are treated as independent of each other. Examples of this are sentence or document classification for tasks such as Sentiment Analysis or Topic Categorization (Kalchbrenner et al., 2014; Kim, 2014), sentence matching (Hu et al., 2014), and relation extraction (Nguyen and Grishman, 2015). However, their application to sequential prediction tasks, where the input is construed to be part of a sequence (for example, language modeling or POS tagging), has been rather limited (with exceptions, such as Collobert et al. (2011)). The main contribution of this paper is a systematic evaluation of CNNs in the context of a prominent sequential prediction task, namely, language modeling. Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks. Their application to language has received much less attention, and it has mainly focused on"
D16-1123,W15-3001,0,\N,Missing
D18-1323,J15-4004,0,0.0517185,"ly large vocabulary, 50K words, created from a Wikipedia dump (henceforth, Wiki).1 To allow comparison with previous work, we also evaluate on the Penn Treebank (PTB), which is small but has been used as a benchmark for LM since Mikolov et al. (2011). The PTB has approximately 1M tokens and is preprocessed to have 10K vocabulary words; we use the standard trainvalidation-test split. Furthermore, to evaluate the quality of the embeddings induced by the language models, as well as for the word representation experiments in Section 3.4, we use three standard word similarity datasets: SimLex-999 (Hill et al., 2015; SimLex), MEN (Bruni et al., 2014), and RareWords (Luong et al., 2013; RW). The performance on these datasets is evaluated in terms of Spearman corre1 https://dumps.wikimedia.org/enwiki/ 20180301/ 2938 Hid Emb Model Valid Test ∆ Size 200 200 non-tied tied tied+L 95.0 91.1 90.8 86.6 -4.5 89.8 85.8 -5.3 4.7M 2.7M 2.7M 400 200 non-tied tied+L 89.4 85.3 83.4 80.3 -5.0 8.3M 4.3M 400 non-tied tied tied+L 87.2 83.5 10.6M 82.0 78.2 -5.3 6.6M 81.9 78.0 -5.5 6.7M 600 400 non-tied tied+L 85.8 82.4 15.3M 79.0 76.0 -6.4 9.5M 600 non-tied tied tied+L 84.3 81.3 17.8M 79.7 76.1 -5.2 11.8M 78.7 75.5 -5.8 12.1"
D18-1323,W13-3512,0,0.0952921,"orth, Wiki).1 To allow comparison with previous work, we also evaluate on the Penn Treebank (PTB), which is small but has been used as a benchmark for LM since Mikolov et al. (2011). The PTB has approximately 1M tokens and is preprocessed to have 10K vocabulary words; we use the standard trainvalidation-test split. Furthermore, to evaluate the quality of the embeddings induced by the language models, as well as for the word representation experiments in Section 3.4, we use three standard word similarity datasets: SimLex-999 (Hill et al., 2015; SimLex), MEN (Bruni et al., 2014), and RareWords (Luong et al., 2013; RW). The performance on these datasets is evaluated in terms of Spearman corre1 https://dumps.wikimedia.org/enwiki/ 20180301/ 2938 Hid Emb Model Valid Test ∆ Size 200 200 non-tied tied tied+L 95.0 91.1 90.8 86.6 -4.5 89.8 85.8 -5.3 4.7M 2.7M 2.7M 400 200 non-tied tied+L 89.4 85.3 83.4 80.3 -5.0 8.3M 4.3M 400 non-tied tied tied+L 87.2 83.5 10.6M 82.0 78.2 -5.3 6.6M 81.9 78.0 -5.5 6.7M 600 400 non-tied tied+L 85.8 82.4 15.3M 79.0 76.0 -6.4 9.5M 600 non-tied tied tied+L 84.3 81.3 17.8M 79.7 76.1 -5.2 11.8M 78.7 75.5 -5.8 12.1M Inan2017 VD tied 650 Zaremba2014 1500 P&W2016 tied 1500 77.1 73.9 82"
D18-1323,E17-2025,0,0.502069,"ype of data (e.g., sentences or words) in different parts of the architecture can be a powerful way to aid learning: it reduces parameters, enabling more compact models and faster learning. Recurrent neural network (RNN) language models (Mikolov et al., 2010; Sundermeyer et al., 2012; Zaremba et al., 2014) have two word mappings: From the input word onto its embedding representation, and from the internal representation of the network (the hidden layer) to the weights for the prediction of the next word. In standard models, these representations are different. Recently, Inan et al. (2017) and Press and Wolf (2017) proposed to instead use a single word representation, tying the input and output mappings. Intuitively, both are representations of the same type of data (words), and information learnt when observing a word as input can be reused when predicting this word as output. Tied language models obtained better perplexity and better word similarity scores of embedding matrices while reducing the number of parameters. The models that achieve the latest state-of-the art results incorporate this technique (see, e.g., Merity et al., 2018). However, note that, by tying the output mapping to the input mapp"
E17-2013,D15-1003,0,0.0190069,"in terms of evaluation metric, dataset, and space, we leave it to future work to examine the influence of these factors. 5 Related Work Recent work has started exploring the representation of instances in distributional space: Herbe82 Figure 1: Performance by frequency bin lot and Vecchi (2015) and Gupta et al. (2015) extract quantified and specific properties of instances (some cats are black, Germany has 80 million inhabitants), and Kruszewski et al. (2015) seek to derive a semantic space where dimensions are sets of entities. We instead analyze instance vectors. A similar angle is taken in Herbelot and Vecchi (2015), for “artificial” entity vectors, whereas we explore “real” instance vectors extracted with standard distributional methods. An early exploration of the properties of instances and concepts, limited to a few manually defined features, is Alfonseca and Manandhar (2002). Some previous work uses distributional representations of instances for NLP tasks: For instance, Lewis and Steedman (2013) use the distributional similarity of named entities to build a type system for a semantic parser, and several works in Knowledge Base completion use entity embeddings (see Wang et al. (2014) and references"
E17-2013,W11-2501,0,0.227477,"ances are semantically more coherent than concepts, at least in our space. We believe a crucial reason for this is that instances share the same specificity, referring to one entity, while concepts are of widely varying specificity and size (compare president of the United States with artifact). Further work is required to probe this hypothesis. It is well established in lexical semantics that cosine similarity does not distinguish between hypernymy and other lexical relations, and in fact hyponyms and hypernyms are usually less similar than co-hyponyms like cat–dog or antonyms like good–bad (Baroni and Lenci, 2011). This result extends to instantiation: The average similarity of each instance to its concept is 0.110 (standard deviation: 0.12), very low compared to the figures in Table 3. The nearest neighbors of instances show a wide range of relations similar to those of concepts, further enriched by the instanceconcept axis: Tyre – Syria (location), Thames river – estuary (“co-hyponym class”), Luciano Pavarotti – soprano (“contrastive class”), Joseph Goebels – bolshevik (“antonym class”), and occasionally true instantiation cases like Sidney Poitier – actor. 4 reused across partitions. We report F-sco"
E17-2013,E12-1004,0,0.026698,"nal properties. We also establish that instantiation detection (“Mozart – composer”) is generally easier than hypernymy detection (“chemist – scientist”), and that results on the influence of input representation do not transfer from hyponymy to instantiation. 1 Introduction 2 Distributional semantics (Turney and Pantel, 2010), and data-driven, continuous approaches to language in general including neural networks (Bengio et al., 2003), are a success story in both Computational Linguistics and Cognitive Science in terms of modeling conceptual knowledge, such as the fact that cats are animals (Baroni et al., 2012), similar to dogs (Landauer and Dumais, 1997), and shed fur (Erk et al., 2010). However, distributional representations are notoriously bad at handling discrete knowledge (Fodor and Lepore, 1999; Smolensky, 1990), such as information about specific instances. For example, Beltagy et al. (2016) had to revert from a distributional to a symbolic knowledge source in an entailment task because the distributional component licensed unwarranted inferences (white man does not entail black man, even though the phrases are distributionally very similar). This partially explains that instances have recei"
E17-2013,Q15-1027,0,0.0133921,"Roller et al. (2014) report 0.85 maximum accuracy on a task analogous to N OT H YP, compared to our 0.57 F-score. Since our results are not directly comparable in terms of evaluation metric, dataset, and space, we leave it to future work to examine the influence of these factors. 5 Related Work Recent work has started exploring the representation of instances in distributional space: Herbe82 Figure 1: Performance by frequency bin lot and Vecchi (2015) and Gupta et al. (2015) extract quantified and specific properties of instances (some cats are black, Germany has 80 million inhabitants), and Kruszewski et al. (2015) seek to derive a semantic space where dimensions are sets of entities. We instead analyze instance vectors. A similar angle is taken in Herbelot and Vecchi (2015), for “artificial” entity vectors, whereas we explore “real” instance vectors extracted with standard distributional methods. An early exploration of the properties of instances and concepts, limited to a few manually defined features, is Alfonseca and Manandhar (2002). Some previous work uses distributional representations of instances for NLP tasks: For instance, Lewis and Steedman (2013) use the distributional similarity of named"
E17-2013,J16-4007,0,0.0627937,"semantics (Turney and Pantel, 2010), and data-driven, continuous approaches to language in general including neural networks (Bengio et al., 2003), are a success story in both Computational Linguistics and Cognitive Science in terms of modeling conceptual knowledge, such as the fact that cats are animals (Baroni et al., 2012), similar to dogs (Landauer and Dumais, 1997), and shed fur (Erk et al., 2010). However, distributional representations are notoriously bad at handling discrete knowledge (Fodor and Lepore, 1999; Smolensky, 1990), such as information about specific instances. For example, Beltagy et al. (2016) had to revert from a distributional to a symbolic knowledge source in an entailment task because the distributional component licensed unwarranted inferences (white man does not entail black man, even though the phrases are distributionally very similar). This partially explains that instances have received much less attention than concepts in distributional semantics. This paper addresses this gap and shows that distributional models can reproduce the age-old Datasets We focus on “public” named entities such as Abraham Lincoln or Vancouver, as opposed to “private” named entities like my neig"
E17-2013,N16-1030,0,0.015071,"andard distributional methods. An early exploration of the properties of instances and concepts, limited to a few manually defined features, is Alfonseca and Manandhar (2002). Some previous work uses distributional representations of instances for NLP tasks: For instance, Lewis and Steedman (2013) use the distributional similarity of named entities to build a type system for a semantic parser, and several works in Knowledge Base completion use entity embeddings (see Wang et al. (2014) and references there). The focus on public, named instances is shared with Named Entity Recognition (NER; see Lample et al. (2016) and references therein); however, we focus on the instantiation relation rather than on recognition per se. Also, in terms of modeling, NER is typically framed as a sequence labeling task to identify entities in text, whereas we do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a compa"
E17-2013,J10-4007,1,0.831074,"Missing"
E17-2013,S12-1012,0,0.0250573,"ities in text, whereas we do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional r"
E17-2013,N15-1098,0,0.0576241,"used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is that instantiation is easier to spot than hypernymy, consisten"
E17-2013,P05-1014,0,0.161924,"ing, NER is typically framed as a sequence labeling task to identify entities in text, whereas we do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and expe"
E17-2013,D15-1002,1,0.904283,"Missing"
E17-2013,D14-1167,0,0.0347687,"aken in Herbelot and Vecchi (2015), for “artificial” entity vectors, whereas we explore “real” instance vectors extracted with standard distributional methods. An early exploration of the properties of instances and concepts, limited to a few manually defined features, is Alfonseca and Manandhar (2002). Some previous work uses distributional representations of instances for NLP tasks: For instance, Lewis and Steedman (2013) use the distributional similarity of named entities to build a type system for a semantic parser, and several works in Knowledge Base completion use entity embeddings (see Wang et al. (2014) and references there). The focus on public, named instances is shared with Named Entity Recognition (NER; see Lample et al. (2016) and references therein); however, we focus on the instantiation relation rather than on recognition per se. Also, in terms of modeling, NER is typically framed as a sequence labeling task to identify entities in text, whereas we do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instanti"
E17-2013,C14-1212,0,0.172613,"do classification of previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The"
E17-2013,E14-1054,0,0.0766868,"f previously gathered candidates. In fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is t"
E17-2013,D16-1234,0,0.0586403,"sed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is that instantiation is easier to spot than hypernymy, consistent with it lying along a greater ontological"
E17-2013,C14-1097,1,0.918385,"Missing"
E17-2013,E14-4008,0,0.0385562,"n fact, the space we used was built on top of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is that instantiation is easier to spot than h"
E17-2013,L16-1723,0,0.0132118,"op of a corpus processed with a NER system. Named Entity Classification (Nadeau and Sekine, 2007) can be viewed as a limited form of the instantiation task. We analyze the entity representations themselves and tackle a wider set of tasks related to instantiation, with a comparative analysis with hypernymy. There is a large body of work on hypernymy and other lexical relations in distributional semantics (Geffet and Dagan, 2005; Kotlerman et al., 2010; Baroni and Lenci, 2011; Lenci and Benotto, 2012; Weeds et al., 2014; Rimell, 2014; Roller et al., 2014; Santus et al., 2014; Levy et al., 2015; Santus et al., 2016; Roller and Erk, 2016; Shwartz et al., 2016). Many studies, notably studies of textual entailment, include entities, but do not specifically investigate their properties and contrast them with concepts: This is the contribution of our paper. 6 Conclusions The ontological distinction between instances and concepts is fundamental both in theoretical studies and practical implementations. Our analyses and experiments suggest that the distinction is recoverable from distributional representations. The good news is that instantiation is easier to spot than hypernymy, consistent with it lying along"
E17-2013,P16-1226,0,0.100763,"Missing"
E17-2013,Q13-1015,0,\N,Missing
E17-2013,J06-1001,0,\N,Missing
J12-3005,W04-3221,0,0.0126322,"ent semantic classes in this article. The semantic properties of adjectives can also be exploited in advanced NLP tasks and applications such as Question Answering, Dialog Systems, Natural Language Generation, or Information Extraction. For instance, from a sentence like This maimai is round and sweet, we can quite safely infer that the (invented) object maimai is a physical object, probably edible. This type of process could be exploited in, for instance, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic classes (see Section 4.5). The"
J12-3005,A00-2006,0,0.574496,"0), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad), and in fact antonymy is the main organizing criterion for adjectives in WordNet (Miller 1998), the most widely used semantic resource in NLP. However, when examining a large scale lexicon, it becomes immediately apparent that there are many other types of adjectives that do not easily ﬁt in a scale-based or antonymy-based view of adjectives (Alonge et al. 2000). Some examples are pulmonary, former, and foldable. It is not clear, for instance, whether it makes sense to ask for an antonym of pulmonary, or to establish a “foldability” scale for foldable. These adjectives need a different treatment, and they are treated in terms of different semantic classes in this article. The semantic properties of adjectives can also be exploited in advanced NLP tasks and applications such as Question Answering, Dialog Systems, Natural Language Generation, or Information Extraction. For instance, from a sentence like This maimai is round and sweet, we can quite safe"
J12-3005,alsina-etal-2002-catcg,1,0.550013,"Missing"
J12-3005,W06-2911,0,0.0639609,"Missing"
J12-3005,J08-4004,0,0.09134,"Missing"
J12-3005,C04-1161,1,0.862482,"Missing"
J12-3005,brants-2000-inter,0,0.011141,"ticles on speciﬁcally this topic: Carvalho and Ranchhod (2003) used adjective classes similar to the ones explored here to disambiguate between nominal and adjectival readings in Portuguese. Adjective information, manually coded, served to establish constraints in a ﬁnite-state transducer part-of-speech tagger. Actually, POS tagging was also the initial motivation for the present research, as adjective–noun and adjective–verb (participle) ambiguities cause most difﬁculties to both humans and machines in languages such as English, German, and Catalan (Marcus, Santorini, and Marcinkiewicz 1993; Brants 2000; Boleda 2007). Bohnet, Klatt, and Wanner (2002) also has similar goals to the present research, as it is aimed at automatically classifying German adjectives. However, the classiﬁcation 577 Computational Linguistics Volume 38, Number 3 used is not purely semantic, polysemy is not taken into account, and the evidence and techniques used are more limited than the ones used here. Other research on adjectives within computational linguistics is oriented toward different goals than ours. Yallop, Korhonen, and Briscoe (2005) tackle syntactic, not semantic classiﬁcation, akin to the acquisition of s"
J12-3005,E09-1013,0,0.070879,"Missing"
J12-3005,C00-1023,0,0.0490761,"at automatically classifying German adjectives. However, the classiﬁcation 577 Computational Linguistics Volume 38, Number 3 used is not purely semantic, polysemy is not taken into account, and the evidence and techniques used are more limited than the ones used here. Other research on adjectives within computational linguistics is oriented toward different goals than ours. Yallop, Korhonen, and Briscoe (2005) tackle syntactic, not semantic classiﬁcation, akin to the acquisition of subcategorization frames for verbs. Another relevant line of research pursues WSD. Justeson and Katz (1995) and Chao and Dyer (2000) showed that adjectives are a very useful cue for disambiguating the sense of the nouns they modify. Adjective classes could be further exploited in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mini"
J12-3005,P10-1018,0,0.0531777,"Missing"
J12-3005,C96-1055,0,0.140226,"t of the work reported in this article was done while the ﬁrst author was a postdoctoral scholar at U. Polit`ecnica de Catalunya and a visiting researcher at U. Stuttgart. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another"
J12-3005,P10-2017,0,0.0693823,"Missing"
J12-3005,P93-1023,0,0.405696,"similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad), and in fact antonymy is the main organizing criterion for adjectives in WordNet (Miller 1998), the most widely used semantic resource in NLP. However, when examining a large scale lexicon, it becomes immediately apparent that there are many other"
J12-3005,P97-1023,0,0.562892,"Missing"
J12-3005,C00-1044,0,0.0159898,"(Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad), and in fact antonymy is the main organizing criterion for adjectives in WordNet (Miller 1998), the most widely used semantic resource in NLP. However, when examining a large scale lexicon, it becomes immediately apparent that there are many other types of adjectives that do not easily ﬁ"
J12-3005,P90-1034,0,0.837972,"for Computational Linguistics Computational Linguistics Volume 38, Number 3 1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another signiﬁcant challenge is posed by polysemy, or the fact that one and the same adjective can have multiple senses. Different senses may fall into different classes, such that it is no"
J12-3005,J95-1001,0,0.159979,"sent research, as it is aimed at automatically classifying German adjectives. However, the classiﬁcation 577 Computational Linguistics Volume 38, Number 3 used is not purely semantic, polysemy is not taken into account, and the evidence and techniques used are more limited than the ones used here. Other research on adjectives within computational linguistics is oriented toward different goals than ours. Yallop, Korhonen, and Briscoe (2005) tackle syntactic, not semantic classiﬁcation, akin to the acquisition of subcategorization frames for verbs. Another relevant line of research pursues WSD. Justeson and Katz (1995) and Chao and Dyer (2000) showed that adjectives are a very useful cue for disambiguating the sense of the nouns they modify. Adjective classes could be further exploited in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention,"
J12-3005,P05-1005,0,0.0610871,"mes for verbs. Another relevant line of research pursues WSD. Justeson and Katz (1995) and Chao and Dyer (2000) showed that adjectives are a very useful cue for disambiguating the sense of the nouns they modify. Adjective classes could be further exploited in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010),"
J12-3005,P03-1009,0,0.522368,"Missing"
J12-3005,N01-1009,0,0.248894,"Missing"
J12-3005,J04-1003,0,0.0606417,"a postdoctoral scholar at U. Polit`ecnica de Catalunya and a visiting researcher at U. Stuttgart. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another signiﬁcant challenge is posed by polysemy, or the fact that one and the sa"
J12-3005,P00-1012,0,0.0126806,"rms of different semantic classes in this article. The semantic properties of adjectives can also be exploited in advanced NLP tasks and applications such as Question Answering, Dialog Systems, Natural Language Generation, or Information Extraction. For instance, from a sentence like This maimai is round and sweet, we can quite safely infer that the (invented) object maimai is a physical object, probably edible. This type of process could be exploited in, for instance, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic cl"
J12-3005,J93-2004,0,0.0422735,"Missing"
J12-3005,A00-2034,0,0.0327602,"d in this article was done while the ﬁrst author was a postdoctoral scholar at U. Polit`ecnica de Catalunya and a visiting researcher at U. Stuttgart. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 3 1998). This article tackles precisely this task, that is, the semantic classiﬁcation of adjectives, for Catalan. We aim at automatically inducing the semantic class for an adjective given its linguistic properties, as extracted from corpora and other resources. The acquisition of semantic classes has been widely studied for verbs (Dorr and Jones 1996; McCarthy 2000; Korhonen, Krymolowski, and Marx 2003; Lapata and Brew 2004; Schulte im Walde 2006; Joanis, Stevenson, and James 2008) and, to a lesser extent, for nouns (Hindle 1990; Pereira, Tishby, and Lee 1993), but, with very few exceptions (Bohnet, Klatt, and Wanner 2002; Carvalho and Ranchhod 2003), not for adjectives. Furthermore, we cannot rely on a well-established classiﬁcation for adjectives. The classes themselves are subject to experimentation. We will test two different classiﬁcations, analyzing the empirical properties of the classes and the problems in their deﬁnition. Another signiﬁcant cha"
J12-3005,P04-1036,0,0.0788851,"Missing"
J12-3005,H05-1124,0,0.0246268,"Missing"
J12-3005,J01-3003,0,0.799967,"s round and sweet, we can quite safely infer that the (invented) object maimai is a physical object, probably edible. This type of process could be exploited in, for instance, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic classes (see Section 4.5). There is very little related work in empirical computational semantics in modeling regular polysemy. A pioneering piece of research is Buitelaar (1998), which tried to account for regular polysemy with the CoreLex resource. CoreLex, building on the Generative Lexicon theory (Pustejovsk"
J12-3005,P93-1024,0,0.443207,"Missing"
J12-3005,W05-0311,0,0.0336628,"in Section 3, including syntactic and semantic characteristics. The judges had a moderate degree of agreement, comparable to that obtained in other tasks on semantics or discourse, inter-annotator scores ranging between κ = 0.54 and 0.64 (see Artstein and Poesio [2008] for a discussion of agreement measures for computational linguistics). For comparison, V´eronis (1998) reported a mean pair-wise weighted κ = 0.43 for a word sense tagging task in French; and Merlo and Stevenson (2001) obtained κ = 0.53–0.66 for the task of classifying English verbs as unergative, unaccusative, or object-drop. Poesio and Artstein (2005) report κ values of 0.63–0.66 (0.45–0.50 if a trivial category is dropped) for the tagging of anaphoric relations. Our judges reported difﬁculties in tagging particular kinds of adjectives, such as deverbal adjectives. This issue will be retaken in Section 4.5. No intensional adjectives were identiﬁed in the data by the judges, and only one intensional-qualitative adjective was identiﬁed. Two intensional lemmata were manually added to be able to minimally track the class. This is clearly insufﬁcient for a quantitative approach, however, so the intensional class is dropped in the alternative cl"
J12-3005,C00-2094,0,0.200395,"Missing"
J12-3005,P99-1014,0,0.16525,"ce, Information Extraction and ontology population, although to our knowledge this possibility has received but little attention (Malouf 2000; Almuhareb and Poesio 2004). As for polysemy, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem, by biasing the experimental material to include monosemous words only, or by choosing an approach that ignores polysemy (Hindle 1990; Merlo and Stevenson 2001; Schulte im Walde 2006; Joanis, Stevenson, and James 2008). There are a few exceptions to this tradition, such as Pereira, Tishby, and Lee (1993), Rooth et al. (1999), and Korhonen, Krymolowski, and Marx (2003), who used soft clustering methods for multiple assignment to verb semantic classes (see Section 4.5). There is very little related work in empirical computational semantics in modeling regular polysemy. A pioneering piece of research is Buitelaar (1998), which tried to account for regular polysemy with the CoreLex resource. CoreLex, building on the Generative Lexicon theory (Pustejovsky 1995), groups WordNet senses into 39 “basic 578 Boleda, Schulte im Walde, and Badia Modeling Regular Polysemy in Catalan Adjectives types” (broad ontological categor"
J12-3005,sanroma-boleda-2010-database,1,0.793345,"Missing"
J12-3005,J06-2001,1,0.948191,"Missing"
J12-3005,J98-1004,0,0.616799,"Missing"
J12-3005,J04-3002,0,0.0440612,"d in WSD in at least two respects: (1) to establish an inventory of adjective senses (if polysemous instances are correctly detected; this is where sense induction and our own work ﬁts in), and (2) to exploit class-based properties for the disambiguation, similar to related work on verb classes (Resnik 1993; Prescher, Riezler, and Rooth 2000; Kohomban and Lee 2005). The application where adjectives have received most attention, however, is Opinion Mining and Sentiment Analysis (Pang and Lee 2008), as adjectives are known to convey much of the evaluative and subjective information in language (Wiebe et al. 2004). The typical goal of this kind of study has been to identify subjective adjectives and their orientation (positive, neutral, negative). This type of research, from pioneering work by Hatzivassiloglou and colleages (Hatzivassiloglou and McKeown 1993, 1997; Hatzivassiloglou and Wiebe 2000) to current research (de Marneffe, Manning, and Potts 2010), has thus focused on scalar adjectives, that is, adjectives like good and bad, which can be translated into values that can be ordered along a scale. These adjectives typically enter into antonymy relations (the semantic relation between good and bad)"
J12-3005,P05-1076,0,0.0708525,"Missing"
J12-3005,D07-1018,1,\N,Missing
J12-3005,J13-3008,0,\N,Missing
J12-3005,W11-0128,0,\N,Missing
J16-4002,E12-1004,0,0.0273098,"a such as negation and relative pronouns have started receiving treatment: In Hermann, Grefenstette, and Blunsom (2013), negation is defined as the distributional complement of a term in a particular domain; similarly, Kruszewski et al. (this issue) show that distributional representations can model alternatives in “conversational” negation; a tensor-based interpretation of relative pronouns has been proposed in Clark, Coecke, and Sadrzadeh (2013) and is further explored in this issue (Rimell et al.). Some aspects of logical operators, in fact, can even be directly derived from their vectors: Baroni et al. (2012) show that, to a reasonable extent, entailment relations between a range of quantifiers can be learned from their distributions. Defining distributional logical functions in a way that accounts for all phenomena traditionally catered for by Formal Semantics is, however, a challenging research program that is still in its first stages. D-first FDS also has to define new composition operators that act over word representations. Compositionality has been an area of active research in distributional semantics, and many composition functions have been suggested, ranging from simple vector addition"
J16-4002,P14-1023,0,0.296105,"Missing"
J16-4002,J10-4006,0,0.197488,"s matrices and tensors, where numerical values are abstractions on the contexts of use obtained from large amounts of natural language data (large corpora, image data sets, and so forth). The figure only shows values for two dimensions, but standard distributional representations range from a few dozen to hundreds of thousands of dimensions (cf. the dots in the figure). The semantic information is distributed across all the dimensions of the vector, and it is encoded in the form of continuous values, which allows for very rich and nuanced information to be expressed (Landauer and Dumais 1997; Baroni and Lenci 2010). One of the key strengths of Distributional Semantics is its use of well-defined algebraic techniques to manipulate semantic representations, which yield useful information about the semantics of the involved expressions. For instance, the collection of words in a lexicon forms a vector space or semantic space, in which semantic relations can be modeled as geometric relations: In a typical semantic space, postdoc is near student, and far from less related words such as wealth, as visualized in Figure 2. The words (represented with two dimensions dim1 and dim2; see Figure 2, left) can be plott"
J16-4002,D10-1115,0,0.089433,"he semantics of some of the near-synonyms discussed in Section 2. The table shows the nearest neighbors of (or, words that are closest to) man/chap/lad/dude/guy in the distributional model of Baroni, Dinu, and Kruszewski (2014). The representations clearly capture the fact that man is a more general, neutral word whereas the others are more informal, as well as other aspects of their semantics, such as the fact that lad is usually used for younger people. In recent years, distributional models have been extended to handle the semantic composition of words into phrases and longer constituents (Baroni and Zamparelli 2010; Mitchell and Lapata 2010; Socher et al. 2012), building on work that the Cognitive Science community had started earlier on (Foltz 1996; Kintsch 2001, among others). Although these models still do not account for the full range of composition phenomena that have been examined in Formal Semantics, they do encode relevant semantic information, as shown by their success in demanding semantic tasks such as predicting sentence similarity (Marelli et al. 2014). Compositional Distributional Semantics allows us to model semantic phenomena that are very challenging for Formal Semantics and more gener"
J16-4002,W15-2712,1,0.878635,"Missing"
J16-4002,W13-0104,1,0.932229,"Missing"
J16-4002,P12-1015,1,0.841124,"tion for a given linguistic expression is a function of the contexts in which it occurs. Context can be defined in various ways; the most usual one is the linguistic environment in which a word appears (typically, simply the words surrounding the target word, but some approaches use more sophisticated linguistic representations encoding, e.g., syntactic relations; Pado´ and Lapata [2007]). Figure 1(a) shows an example. Recently, researchers have started exploring other modalities, using, for instance, visual and auditory information extracted from images and sound files (Feng and Lapata 2010; Bruni et al. 2012; Roller and Schulte Im Walde 2013; Kiela and Clark 2015; Lopopolo and van Miltenburg 2015). Distributional representations are vectors (Figure 1(c)) or more complex algebraic objects such as matrices and tensors, where numerical values are abstractions on the contexts of use obtained from large amounts of natural language data (large corpora, image data sets, and so forth). The figure only shows values for two dimensions, but standard distributional representations range from a few dozen to hundreds of thousands of dimensions (cf. the dots in the figure). The semantic information is distribut"
J16-4002,W13-3005,0,0.0492442,"Missing"
J16-4002,S13-1001,0,0.210237,"ress in modeling the descriptive content of linguistic expressions in a cognitively plausible way (Lund, Burgess, and Atchley 1995; Landauer and Dumais 1997), but faces serious difficulties with many of the phenomena that Formal Semantics excels at, such as quantification and logical inference. 620 Boleda and Herbelot Formal Distributional Semantics Because of the complementary strengths of the two approaches, it has been suggested that much could be gained by developing an overarching framework (Coecke, Sadrzadeh, and Clark 2011; Beltagy et al. 2013; Erk 2013; Garrette, Erk, and Mooney 2014; Grefenstette 2013; Lewis and Steedman 2013; Baroni, Bernardi, and Zamparelli 2015). A Formal Distributional Semantics thus holds the promise of developing a more comprehensive model of meaning. However, given the fundamentally different natures of FS and DS, building an integrative framework poses theoretical and engineering challenges. This introductory article provides the necessary background to understand those challenges and to situate the articles that follow in the broader research context. 2. Formal Semantics Formal Semantics is a broad term that covers a range of approaches to the study of meaning, fr"
J16-4002,D11-1129,0,0.0241702,"presentations. Compositionality has been an area of active research in distributional semantics, and many composition functions have been suggested, ranging from simple vector addition (Kintsch 2001; Mitchell et al. 2008b) to matrix multiplication (where functional content words such as adjectives must be learned via machine learning techniques; Baroni and Zamparelli 2010, Paperno and Baroni 2016) to more complex operations (Socher et al. 2012). Some approaches to composition can actually be seen as sitting between the F-first and D-first approaches: In Coecke, Sadrzadeh, and Clark (2011) and Grefenstette and Sadrzadeh (2011), a CCG grammar is converted into a tensor-based logic relying on the direct composition of distributional representations. In contrast with their F-first counterparts, D-first FDSs regard distributions as the primary building blocks of the sentence, which must undergo composition rules to get at the meaning of longer constituents (see above). This set-up does not clearly distinguish the lexical from the logical level. This has advantages in cases where the lexicon has a direct influence over the structure of a constituent: Boleda et al. (2013), for instance, show that there is no fundamental"
J16-4002,D15-1002,1,0.0899114,"Missing"
J16-4002,W15-0120,1,0.732807,"Missing"
J16-4002,D15-1003,1,0.608722,"a particular framework (e.g., Erk [2016] that can be seen as a non-truth-theoretic, probabilistic semantics). We discuss this issue in more detail in Section 4.3. 4.2 D-first FDS In a D-first FDS, logical operators are directly expressed as functions over distributions, bypassing existing logics. The solutions proposed for this task vary widely. Quantifiers, for instance, may take the form of (a) operations over tensors in a truth-functional logic (Grefenstette 2013); (b) a mapping function from a distributional semantics space to some heavily underspecified form of a “model-theoretic” space (Herbelot and Vecchi 2015); (c) a function from sentences in a discourse to other sentences in the same discourse (Capetola 2013) (i.e., for universal quantification, we map Every color is good to Blue is good, Red is good. . . ). Other phenomena such as negation and relative pronouns have started receiving treatment: In Hermann, Grefenstette, and Blunsom (2013), negation is defined as the distributional complement of a term in a particular domain; similarly, Kruszewski et al. (this issue) show that distributional representations can model alternatives in “conversational” negation; a tensor-based interpretation of rela"
J16-4002,2016.lilt-13.2,1,0.790742,"Missing"
J16-4002,W13-3209,0,0.0349635,"Missing"
J16-4002,D15-1293,0,0.0313849,"of the contexts in which it occurs. Context can be defined in various ways; the most usual one is the linguistic environment in which a word appears (typically, simply the words surrounding the target word, but some approaches use more sophisticated linguistic representations encoding, e.g., syntactic relations; Pado´ and Lapata [2007]). Figure 1(a) shows an example. Recently, researchers have started exploring other modalities, using, for instance, visual and auditory information extracted from images and sound files (Feng and Lapata 2010; Bruni et al. 2012; Roller and Schulte Im Walde 2013; Kiela and Clark 2015; Lopopolo and van Miltenburg 2015). Distributional representations are vectors (Figure 1(c)) or more complex algebraic objects such as matrices and tensors, where numerical values are abstractions on the contexts of use obtained from large amounts of natural language data (large corpora, image data sets, and so forth). The figure only shows values for two dimensions, but standard distributional representations range from a few dozen to hundreds of thousands of dimensions (cf. the dots in the figure). The semantic information is distributed across all the dimensions of the vector, and it is en"
J16-4002,Q15-1027,0,0.0547561,"Missing"
J16-4002,Q15-1016,0,0.0364385,"Missing"
J16-4002,W15-0110,0,0.0638188,"Missing"
J16-4002,marelli-etal-2014-sick,0,0.0144454,"ent years, distributional models have been extended to handle the semantic composition of words into phrases and longer constituents (Baroni and Zamparelli 2010; Mitchell and Lapata 2010; Socher et al. 2012), building on work that the Cognitive Science community had started earlier on (Foltz 1996; Kintsch 2001, among others). Although these models still do not account for the full range of composition phenomena that have been examined in Formal Semantics, they do encode relevant semantic information, as shown by their success in demanding semantic tasks such as predicting sentence similarity (Marelli et al. 2014). Compositional Distributional Semantics allows us to model semantic phenomena that are very challenging for Formal Semantics and more generally symbolic approaches, especially concerning content words. Consider polysemy: In the first three sentences in Figure 1(a), postdoc refers to human beings, whereas in the fourth it refers to an event. Composing postdoc with an adjective such as tall will highlight the human-related information in the noun vector, bringing it closer to person, whereas composing it with long will highlight its eventive dimensions, bringing it closer to time (Baroni and Za"
J16-4002,N13-1090,0,0.120654,"Missing"
J16-4002,J07-2002,0,0.13737,"Missing"
J16-4002,J16-2006,0,0.0155724,"ts for all phenomena traditionally catered for by Formal Semantics is, however, a challenging research program that is still in its first stages. D-first FDS also has to define new composition operators that act over word representations. Compositionality has been an area of active research in distributional semantics, and many composition functions have been suggested, ranging from simple vector addition (Kintsch 2001; Mitchell et al. 2008b) to matrix multiplication (where functional content words such as adjectives must be learned via machine learning techniques; Baroni and Zamparelli 2010, Paperno and Baroni 2016) to more complex operations (Socher et al. 2012). Some approaches to composition can actually be seen as sitting between the F-first and D-first approaches: In Coecke, Sadrzadeh, and Clark (2011) and Grefenstette and Sadrzadeh (2011), a CCG grammar is converted into a tensor-based logic relying on the direct composition of distributional representations. In contrast with their F-first counterparts, D-first FDSs regard distributions as the primary building blocks of the sentence, which must undergo composition rules to get at the meaning of longer constituents (see above). This set-up does not"
J16-4002,D13-1115,0,0.0294999,"Missing"
J16-4002,C69-0201,0,0.379943,"roviding the necessary background for the articles that follow. 1. Introduction The 1960s and 1970s saw pioneering work in formal and computational semantics: On the formal side, Montague was writing his seminal work on the treatment of quantifiers (Montague 1974) and on the computational side, Sp¨arck-Jones and colleagues were developing vector-based representations of the lexicon (Sp¨arck-Jones 1967). At the same time, Cognitive Science and Artificial Intelligence were theorizing in which ways natural language understanding—seen as a broad, all-encompassing task—might be modeled and tested (Schank 1972; Winograd 1972). The experiments performed back then, however, suffered from a lack of resources (both in terms of data and computing capabilities) and thus only gave limited support to the developed theories. Fifty years later, Formal Semantics (FS) and vectorial models of meaning— commonly referred to as “Distributional Semantics” (DS)—have made substantial progress. Large machine-readable corpora are available and computing power has grown exponentially. These developments, together with the advent of improved ∗ CIMeC (Universit`a di Trento), Palazzo Fedrigotti, C.so Bettini 31, 38068 Rove"
J16-4002,D12-1110,0,0.468333,"phy 2004). How are distributional representations obtained? There are in fact many different versions of the function that maps contexts into distributional representations (represented as an arrow in Figure 1(b)). Traditional distributional models are count-based (Baroni, Dinu, and Kruszewski 2014): They are statistics over the observed contexts of use, corresponding for instance to how many times words occur with other words (like head, student, researcher, etc.) in a given sentence.1 More recent neural network–based models involve predicting the contexts instead (Collobert and Weston 2008; Socher et al. 2012; Mikolov, Yih, and Zweig 2013). In this type of approach, semantic representations are a by-product of solving a linguistic prediction task. Tasks that are general enough lead to general-purpose semantic representations; for instance, Mikolov, Yih, and Zweig (2013) used language modeling, the task of predicting words in a sentence (e.g., After graduating, Barbara kept working in the same institute, this time as a ). In a predictive setup, word vectors (called embeddings in the neural network literature) are typically initialized randomly, and iteratively refined as the model goes through the"
J16-4002,H89-1033,0,0.339256,"necessary background for the articles that follow. 1. Introduction The 1960s and 1970s saw pioneering work in formal and computational semantics: On the formal side, Montague was writing his seminal work on the treatment of quantifiers (Montague 1974) and on the computational side, Sp¨arck-Jones and colleagues were developing vector-based representations of the lexicon (Sp¨arck-Jones 1967). At the same time, Cognitive Science and Artificial Intelligence were theorizing in which ways natural language understanding—seen as a broad, all-encompassing task—might be modeled and tested (Schank 1972; Winograd 1972). The experiments performed back then, however, suffered from a lack of resources (both in terms of data and computing capabilities) and thus only gave limited support to the developed theories. Fifty years later, Formal Semantics (FS) and vectorial models of meaning— commonly referred to as “Distributional Semantics” (DS)—have made substantial progress. Large machine-readable corpora are available and computing power has grown exponentially. These developments, together with the advent of improved ∗ CIMeC (Universit`a di Trento), Palazzo Fedrigotti, C.so Bettini 31, 38068 Rovereto, Italy. E-m"
J16-4002,N10-1011,0,\N,Missing
J16-4002,J10-4007,0,\N,Missing
J16-4002,J02-2001,0,\N,Missing
J16-4002,S13-1002,1,\N,Missing
J16-4002,Q13-1015,0,\N,Missing
J16-4002,W11-0112,0,\N,Missing
J16-4002,2014.lilt-9.5,0,\N,Missing
J16-4002,W13-0109,0,\N,Missing
N19-1210,J08-4004,0,0.0331416,"Missing"
N19-1210,W17-6804,1,0.923893,"Missing"
N19-1210,C18-1135,1,0.891887,"Missing"
N19-1210,Q16-1003,0,0.041818,", 2017), and the relation between conventions in a community and the social standing of its members (Danescu-Niculescu-Mizil et al., 2013). None of these works has analyzed the ability of a distributional model to capture these phenomena, which is what we do in this paper for short-term meaning shift. Kulkarni et al. (2015) consider meaning shift in short time periods on Twitter data, but without providing an analysis of the observed shift nor systematically assessing the performance of the model, as we do here. Evaluation of semantic shift is difficult, due to the lack of annotated datasets (Frermann and Lapata, 2016). For this reason, even for long-term shift, evaluation is usually performed by manually inspecting the n words whose representation changes the most according to the model under investigation (Hamilton et al., 2016; Kim et al., 2014). Our dataset allows for a more systematic evaluation and analysis, and enables comparison in future studies. 3 Experimental Setup 3.1 Data We exploit user-generated language from an online forum of football fans, namely, the r/LiverpoolFC subreddit, one of the many communities hosted by the Reddit platform.2 Del Tredici and Fern´andez (2018) showed that this subr"
N19-1210,W11-2508,0,0.38761,"sis that a change in context of use mirrors a change in meaning. This in turn stems from the Distributional Hypothesis, that states that similarity in meaning results in similarity in context of use (Harris, 1954). Therefore, all models (including ours) spot semantic shift as a change in the word representation in different time periods. Among the most widely used techniques are Latent Semantic Analysis (Sagi et al., 2011; Jatowt and Duh, 2014), Topic Modeling (Wijaya and Yeniterzi, 2011), classic distributional representations based on co-occurence matrices of target words and context terms (Gulordava and Baroni, 2011). More recently, researchers have used 1 Data and code are available at: https://github. com/marcodel13/Short-term-meaning-shift. 2069 Proceedings of NAACL-HLT 2019, pages 2069–2075 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word embeddings computed using the skip-gram model by Mikolov et al. (2013). Since embeddings computed in different semantic spaces are not directly comparable, time related representations are usually made comparable either by aligning different semantic spaces through a transformation matrix (Kulkarni et al., 2015; Aza"
N19-1210,P16-1141,0,0.798273,"McConnell-Ginet, 1992). This paper is, to the best of our knowledge, the first exploration of the latter phenomenon—which we call short-term meaning shift—using distributional representations. More concretely, we focus on meaning shift arising within a period of 8 years, and explore it on data from an online community of speakers, because there the adoption of new meanings happens at a fast pace (Clark, 1996; Hasan, 2009). Indeed, short-term shift is usually hard to observe in standard language, such as the language of books or news, which has been the focus of long-term shift studies (e.g., Hamilton et al., 2016; Kulkarni et al., 2015), since it takes a long time for a new meaning to be widely accepted in the standard language. Our contribution is twofold. First, we create a small dataset of short-term shift for analysis and evaluation, and qualitatively analyze the types of meaning shift we find.1 This is necessary because, unlike studies of long-term shift, we cannot rely on material previously gathered by linguists or lexicographers. Second, we test the behavior of a standard distributional model of semantic change when applied to short-term shift. Our results show that this model successfully det"
N19-1210,W14-2517,0,0.777345,"meaning-shift. 2069 Proceedings of NAACL-HLT 2019, pages 2069–2075 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word embeddings computed using the skip-gram model by Mikolov et al. (2013). Since embeddings computed in different semantic spaces are not directly comparable, time related representations are usually made comparable either by aligning different semantic spaces through a transformation matrix (Kulkarni et al., 2015; Azarbonyad et al., 2017; Hamilton et al., 2016) or by initializing the embeddings at ti+1 using those computed at ti (Kim et al., 2014; Del Tredici et al., 2016; Phillips et al., 2017; Szymanski, 2017). We adopt the latter methodology (see Section 3.2). Unlike most previous work, we focus on the language of online communities. Recent studies of this type of language have investigated the spread of new forms and meanings (Del Tredici and Fern´andez, 2017, 2018; Stewart and Eisenstein, 2018), competing lexical variants (Rotabi et al., 2017), and the relation between conventions in a community and the social standing of its members (Danescu-Niculescu-Mizil et al., 2013). None of these works has analyzed the ability of a distrib"
N19-1210,C18-1117,0,0.282566,"du Abstract We present the first exploration of meaning shift over short periods of time in online communities using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard model for meaning shift detection on shortterm meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this. 1 Introduction Semantic change has received increasing attention in empirical Computational Linguistics / NLP in the last few years (Tang, 2018; Kutuzov et al., 2018). Almost all studies so far have focused on meaning shift in long periods of time—decades to centuries. However, the genesis of meaning shift and the mechanisms that produce it operate at much shorter time spans, ranging from the online agreement on words’ meaning in dyadic interactions (Brennan and Clark, 1996) to the rapid spread of new meanings in relatively small communities of people in (Wenger, 1998; Eckert and McConnell-Ginet, 1992). This paper is, to the best of our knowledge, the first exploration of the latter phenomenon—which we call short-term meaning shift—using distributional rep"
N19-1210,Q15-1016,0,0.0590412,"and includes 157k words. For Reddit13 , we include only words that occur at least 20 times in the sample, so as to ensure meaningful representations for each word, while for the other two samples we do not use any frequency threshold: Since the embeddings used for the initialization of LiverpoolFC13 encode community-independent meanings, if a word doesn’t occur in LiverpoolFC13 its representation will simply be as in Reddit13 , which reflects the idea that if a word is not used in a community, then its meaning is not altered within that community. We train with standard skip-gram parameters (Levy et al., 2015): window 5, learning rate 0.01, embedding dimension 200, hierarchical softmax. 3.3 Evaluation dataset Our dataset consists of 97 words from the r/LiverpoolFC subreddit with annotations by members of the subreddit —that is, community members with domain knowledge (needed for this task) but no linguistic background. To ensure that we would get enough cases of semantic shift to enable a meaningful analysis, we started out from content words that increase their relative frequency between t1 and t2 .5 A threshold of 2 standard deviations above the mean yielded ∼200 words. The first author manually"
N19-1210,W17-2624,0,0.0194587,"2019, pages 2069–2075 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word embeddings computed using the skip-gram model by Mikolov et al. (2013). Since embeddings computed in different semantic spaces are not directly comparable, time related representations are usually made comparable either by aligning different semantic spaces through a transformation matrix (Kulkarni et al., 2015; Azarbonyad et al., 2017; Hamilton et al., 2016) or by initializing the embeddings at ti+1 using those computed at ti (Kim et al., 2014; Del Tredici et al., 2016; Phillips et al., 2017; Szymanski, 2017). We adopt the latter methodology (see Section 3.2). Unlike most previous work, we focus on the language of online communities. Recent studies of this type of language have investigated the spread of new forms and meanings (Del Tredici and Fern´andez, 2017, 2018; Stewart and Eisenstein, 2018), competing lexical variants (Rotabi et al., 2017), and the relation between conventions in a community and the social standing of its members (Danescu-Niculescu-Mizil et al., 2013). None of these works has analyzed the ability of a distributional model to capture these phenomena, which i"
N19-1210,P17-2071,0,0.0143727,"c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word embeddings computed using the skip-gram model by Mikolov et al. (2013). Since embeddings computed in different semantic spaces are not directly comparable, time related representations are usually made comparable either by aligning different semantic spaces through a transformation matrix (Kulkarni et al., 2015; Azarbonyad et al., 2017; Hamilton et al., 2016) or by initializing the embeddings at ti+1 using those computed at ti (Kim et al., 2014; Del Tredici et al., 2016; Phillips et al., 2017; Szymanski, 2017). We adopt the latter methodology (see Section 3.2). Unlike most previous work, we focus on the language of online communities. Recent studies of this type of language have investigated the spread of new forms and meanings (Del Tredici and Fern´andez, 2017, 2018; Stewart and Eisenstein, 2018), competing lexical variants (Rotabi et al., 2017), and the relation between conventions in a community and the social standing of its members (Danescu-Niculescu-Mizil et al., 2013). None of these works has analyzed the ability of a distributional model to capture these phenomena, which is what we do in th"
N19-1378,E06-1002,0,0.0518163,"Missing"
N19-1378,K17-1023,0,0.105422,". 9 (replacing Eq. 8 of E NT L IB).6 Output scores oi are computed 6 (9) Two small changes with respect to the original model (motivated by empirical results in the hyperparameter search) (10) ˜ i,j , multiplied by the respective This information V gate gi,j , is added to the values to be used when processing the next (i + 1th ) token (Eq. 11), and the result is normalized (Eq. 12): ˜ i,j Vi+1,j = Vj + gi,j ∗ V Vi+1,j Vi+1,j = kVi+1,j k (11) (12) Our adaptation of the Recurrent Entity Network involves two changes. First, we use a biLSTM to process the linguistic utterance, while Henaff et al. (2017) used a simple multiplicative mask (we have natural dialogue, while their main evaluation was on bAbI, a synthetic dataset). Second, in the original model the gates were used to retrieve and output information about the query, whereas we use them directly as output scores because our task is referential. This also allows us to tie the keys to the characters of the Friends series as in the previous model, and thus have them represent entities (in the original model, the keys represented entity types, not instances). 4 Character Identification The training and test data for the task span the fir"
N19-1378,S18-1007,0,0.0299901,"e she.1 In the mentioned work, several models have been proposed that incorporate an explicit bias towards entity representations. Such entity-centric models have shown empirical success, but we still know little about what it is that they effectively learn to model. In this analysis paper, we adapt two previous entity-centric models (Henaff et al., 2017; Aina et al., 2018) for a recently proposed referential task and show that, despite their strengths, they are still very far from modeling entities.2 The task is character identification on multiparty dialogue as posed in SemEval 2018 Task 4 (Choi and Chen, 2018).3 Models are given dialogues from the TV show Friends and asked to link entity mentions (nominal expressions like I, she or the woman) to the characters to which they refer in each case. Figure 1 shows an example, where the mentions Ross and you are linked to entity 335, mention I to entity 183, etc. Since the TV series revolves around a set of entities that recur over many scenes and episodes, it is a good benchmark to analyze whether entity-centric models learn and use entity representations for referential tasks. Our contributions are three-fold: First, we adapt two previous entity-centric"
N19-1378,D18-1323,1,0.924602,"ten presents more complex models that incorporate e.g. hand-engineered features. In contrast, we keep our underlying model basic since we want to systematically analyze how certain architectural decisions affect performance. For the same reason we deviate from previous work to entity linking that uses a specialized coreference resolution module (e.g., Chen et al., 2017). Analysis of Neural Network Models. Our work joins a recent strand in NLP that systematically analyzes what different neural network models learn about language (Linzen et al., 2016; K´ad´ar et al., 2017; Conneau et al., 2018; Gulordava et al., 2018b; Nematzadeh et al., 2018, a.o.). This work, like ours, has yielded both positive and negative results: There is evidence that they learn complex linguistic phenomena of morphological and syntactic nature, like long distance agreement (Gulordava et al., 2018b; Giulianelli et al., 2018), but less evidence that they learn how language relates to situations; for instance, Nematzadeh et al. (2018) show that memory-augmented neural models fail on tasks that require keeping track of inconsistent states of the world. 3 Models We approach character identification as a classification task, and compare"
N19-1378,N18-1108,0,0.169287,"ten presents more complex models that incorporate e.g. hand-engineered features. In contrast, we keep our underlying model basic since we want to systematically analyze how certain architectural decisions affect performance. For the same reason we deviate from previous work to entity linking that uses a specialized coreference resolution module (e.g., Chen et al., 2017). Analysis of Neural Network Models. Our work joins a recent strand in NLP that systematically analyzes what different neural network models learn about language (Linzen et al., 2016; K´ad´ar et al., 2017; Conneau et al., 2018; Gulordava et al., 2018b; Nematzadeh et al., 2018, a.o.). This work, like ours, has yielded both positive and negative results: There is evidence that they learn complex linguistic phenomena of morphological and syntactic nature, like long distance agreement (Gulordava et al., 2018b; Giulianelli et al., 2018), but less evidence that they learn how language relates to situations; for instance, Nematzadeh et al. (2018) show that memory-augmented neural models fail on tasks that require keeping track of inconsistent states of the world. 3 Models We approach character identification as a classification task, and compare"
N19-1378,N10-1061,0,0.160795,"fosters good architectural decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed. 1 Introduction Modeling reference to entities is arguably crucial for language understanding, as humans use language to talk about things in the world. A hypothesis in recent work on referential tasks such as co-reference resolution and entity linking (Haghighi and Klein, 2010; Clark and Manning, 2016; Henaff et al., 2017; Aina et al., 2018; Clark et al., 2018) is that encouraging models to learn and use entity representations will help them better carry out referential tasks. To illustrate, creating an entity representation with the relevant information upon reading a woman should make it easier to ∗ denotes equal contribution. resolve a pronoun mention like she.1 In the mentioned work, several models have been proposed that incorporate an explicit bias towards entity representations. Such entity-centric models have shown empirical success, but we still know littl"
N19-1378,N18-1204,0,0.0262263,"y build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed. 1 Introduction Modeling reference to entities is arguably crucial for language understanding, as humans use language to talk about things in the world. A hypothesis in recent work on referential tasks such as co-reference resolution and entity linking (Haghighi and Klein, 2010; Clark and Manning, 2016; Henaff et al., 2017; Aina et al., 2018; Clark et al., 2018) is that encouraging models to learn and use entity representations will help them better carry out referential tasks. To illustrate, creating an entity representation with the relevant information upon reading a woman should make it easier to ∗ denotes equal contribution. resolve a pronoun mention like she.1 In the mentioned work, several models have been proposed that incorporate an explicit bias towards entity representations. Such entity-centric models have shown empirical success, but we still know little about what it is that they effectively learn to model. In this analysis paper, we ad"
N19-1378,P16-1061,0,0.100023,"l decisions. However, we also show that these models do not really build entity representations and that they make poor use of linguistic context. These negative results underscore the need for model analysis, to test whether the motivations for particular architectures are borne out in how models behave when deployed. 1 Introduction Modeling reference to entities is arguably crucial for language understanding, as humans use language to talk about things in the world. A hypothesis in recent work on referential tasks such as co-reference resolution and entity linking (Haghighi and Klein, 2010; Clark and Manning, 2016; Henaff et al., 2017; Aina et al., 2018; Clark et al., 2018) is that encouraging models to learn and use entity representations will help them better carry out referential tasks. To illustrate, creating an entity representation with the relevant information upon reading a woman should make it easier to ∗ denotes equal contribution. resolve a pronoun mention like she.1 In the mentioned work, several models have been proposed that incorporate an explicit bias towards entity representations. Such entity-centric models have shown empirical success, but we still know little about what it is that t"
N19-1378,P18-1198,0,0.196531,"t is not entitycentric, with the same model size. Second, through analysis we provide insights into how they achieve these improvements, and argue that making models entity-centric fosters architectural decisions that result in good inductive biases. Third, we create a dataset and task to evaluate the models’ ability to encode entity information such as gender, and show that models fail at it. More generally, our paper underscores the need for the analysis of model behavior, not only through ablation studies, but also through the targeted probing of model representations (Linzen et al., 2016; Conneau et al., 2018). 2 Related Work Modeling. Various memory architectures have been proposed that are not specifically for entitycentric models, but could in principle be employed in them (Graves et al., 2014; Sukhbaatar et al., 2015; Joulin and Mikolov, 2015; Bansal et al., 2017). The two models we base our results on (Henaff et al., 2017; Aina et al., 2018) were explicitly motivated as entity-centric. We show that our adaptations yield good results and provide a closer analysis of their behavior. Tasks. The task of entity linking has been formalized as resolving entity mentions to referential entity entries i"
N19-1378,N16-1150,0,0.07256,"e be employed in them (Graves et al., 2014; Sukhbaatar et al., 2015; Joulin and Mikolov, 2015; Bansal et al., 2017). The two models we base our results on (Henaff et al., 2017; Aina et al., 2018) were explicitly motivated as entity-centric. We show that our adaptations yield good results and provide a closer analysis of their behavior. Tasks. The task of entity linking has been formalized as resolving entity mentions to referential entity entries in a knowledge repository, mostly Wikipedia (Bunescu and Pas¸ca, 2006; Mihalcea and Csomai, 2007 and much subsequent work; for recent approaches see Francis-Landau et al., 2016; Chen et al., 2018). In the present entity linking task, only a list of entities is given, without associated encyclopedic entries, and information about the entities needs to be acquired from scratch through the task; note the analogy to how a human audience might get familiar with the TV show characters by watching it. Moreover, it addresses multiparty dialogue (as opposed to, typically, narrative text), where speaker information is crucial. A task closely related to entity linking is coreference resolution, i.e., predicting which portions of a text refer to the same entity (e.g., Marie Cur"
N19-1378,W18-5426,0,0.0500772,"linking that uses a specialized coreference resolution module (e.g., Chen et al., 2017). Analysis of Neural Network Models. Our work joins a recent strand in NLP that systematically analyzes what different neural network models learn about language (Linzen et al., 2016; K´ad´ar et al., 2017; Conneau et al., 2018; Gulordava et al., 2018b; Nematzadeh et al., 2018, a.o.). This work, like ours, has yielded both positive and negative results: There is evidence that they learn complex linguistic phenomena of morphological and syntactic nature, like long distance agreement (Gulordava et al., 2018b; Giulianelli et al., 2018), but less evidence that they learn how language relates to situations; for instance, Nematzadeh et al. (2018) show that memory-augmented neural models fail on tasks that require keeping track of inconsistent states of the world. 3 Models We approach character identification as a classification task, and compare a baseline LSTM (Hochreiter and Schmidhuber, 1997) with two models that enrich the LSTM with a memory module designed to learn and use entity representations. LSTMs are the workhorse for text processing, and thus a good baseline to assess the contribution of this module. The LSTM proce"
N19-1378,Q16-1037,0,0.23626,"counterpart model that is not entitycentric, with the same model size. Second, through analysis we provide insights into how they achieve these improvements, and argue that making models entity-centric fosters architectural decisions that result in good inductive biases. Third, we create a dataset and task to evaluate the models’ ability to encode entity information such as gender, and show that models fail at it. More generally, our paper underscores the need for the analysis of model behavior, not only through ablation studies, but also through the targeted probing of model representations (Linzen et al., 2016; Conneau et al., 2018). 2 Related Work Modeling. Various memory architectures have been proposed that are not specifically for entitycentric models, but could in principle be employed in them (Graves et al., 2014; Sukhbaatar et al., 2015; Joulin and Mikolov, 2015; Bansal et al., 2017). The two models we base our results on (Henaff et al., 2017; Aina et al., 2018) were explicitly motivated as entity-centric. We show that our adaptations yield good results and provide a closer analysis of their behavior. Tasks. The task of entity linking has been formalized as resolving entity mentions to refer"
N19-1378,N13-1090,0,0.041376,"top systems at SemEval 2018. Results in the second block marked with ∗ are statistically significantly better than BI LSTM at p &lt; 0.001 (approximate randomization tests, Noreen, 1989). one of the most interesting aspects of the SemEval data is the fact that it is dialogue (even if scripted), which allows us to explore the role of speaker information, one of the aspects of the extralinguistic context of utterance that is crucial for reference. We additionally used the publicly available 300dimensional word vectors that were pre-trained on a Google News corpus with the word2vec Skipgram model (Mikolov et al., 2013a) to represent the input tokens. Entity (speaker/referent) embeddings were randomly initialized. We train the models with backpropagation, using the standard negative log-likelihood loss function. For each of the three model architectures we performed a random search (> 1500 models) over the hyperparameters using cross-validation (see Appendix for details), and report the results of the best settings after retraining without cross-validation. The findings we report are representative of the model populations. Results. We follow the evaluation defined in the SemEval task. Metrics are macro-ave"
N19-1378,D18-1261,0,0.0314811,"models that incorporate e.g. hand-engineered features. In contrast, we keep our underlying model basic since we want to systematically analyze how certain architectural decisions affect performance. For the same reason we deviate from previous work to entity linking that uses a specialized coreference resolution module (e.g., Chen et al., 2017). Analysis of Neural Network Models. Our work joins a recent strand in NLP that systematically analyzes what different neural network models learn about language (Linzen et al., 2016; K´ad´ar et al., 2017; Conneau et al., 2018; Gulordava et al., 2018b; Nematzadeh et al., 2018, a.o.). This work, like ours, has yielded both positive and negative results: There is evidence that they learn complex linguistic phenomena of morphological and syntactic nature, like long distance agreement (Gulordava et al., 2018b; Giulianelli et al., 2018), but less evidence that they learn how language relates to situations; for instance, Nematzadeh et al. (2018) show that memory-augmented neural models fail on tasks that require keeping track of inconsistent states of the world. 3 Models We approach character identification as a classification task, and compare a baseline LSTM (Hochreit"
N19-1378,S18-1107,0,0.0410407,"Missing"
N19-1378,W11-1901,0,0.108381,"t associated encyclopedic entries, and information about the entities needs to be acquired from scratch through the task; note the analogy to how a human audience might get familiar with the TV show characters by watching it. Moreover, it addresses multiparty dialogue (as opposed to, typically, narrative text), where speaker information is crucial. A task closely related to entity linking is coreference resolution, i.e., predicting which portions of a text refer to the same entity (e.g., Marie Curie and the scientist). This typically requires clustering mentions that refer to the same entity (Pradhan et al., 2011). Mention clusters essentially correspond to entities, and recent work on coreference and language modeling has started exploiting an explicit notion of entity (Haghighi and Klein, 2010; Clark and Manning, 2016; Yang et al., 2017). Previous work both on entity linking and on coreference resolution (cited above, as well as Wiseman et al., 2016) often presents more complex models that incorporate e.g. hand-engineered features. In contrast, we keep our underlying model basic since we want to systematically analyze how certain architectural decisions affect performance. For the same reason we devi"
N19-1378,N16-1114,0,0.0279108,"cial. A task closely related to entity linking is coreference resolution, i.e., predicting which portions of a text refer to the same entity (e.g., Marie Curie and the scientist). This typically requires clustering mentions that refer to the same entity (Pradhan et al., 2011). Mention clusters essentially correspond to entities, and recent work on coreference and language modeling has started exploiting an explicit notion of entity (Haghighi and Klein, 2010; Clark and Manning, 2016; Yang et al., 2017). Previous work both on entity linking and on coreference resolution (cited above, as well as Wiseman et al., 2016) often presents more complex models that incorporate e.g. hand-engineered features. In contrast, we keep our underlying model basic since we want to systematically analyze how certain architectural decisions affect performance. For the same reason we deviate from previous work to entity linking that uses a specialized coreference resolution module (e.g., Chen et al., 2017). Analysis of Neural Network Models. Our work joins a recent strand in NLP that systematically analyzes what different neural network models learn about language (Linzen et al., 2016; K´ad´ar et al., 2017; Conneau et al., 201"
N19-1378,D17-1197,0,0.082213,"r, it addresses multiparty dialogue (as opposed to, typically, narrative text), where speaker information is crucial. A task closely related to entity linking is coreference resolution, i.e., predicting which portions of a text refer to the same entity (e.g., Marie Curie and the scientist). This typically requires clustering mentions that refer to the same entity (Pradhan et al., 2011). Mention clusters essentially correspond to entities, and recent work on coreference and language modeling has started exploiting an explicit notion of entity (Haghighi and Klein, 2010; Clark and Manning, 2016; Yang et al., 2017). Previous work both on entity linking and on coreference resolution (cited above, as well as Wiseman et al., 2016) often presents more complex models that incorporate e.g. hand-engineered features. In contrast, we keep our underlying model basic since we want to systematically analyze how certain architectural decisions affect performance. For the same reason we deviate from previous work to entity linking that uses a specialized coreference resolution module (e.g., Chen et al., 2017). Analysis of Neural Network Models. Our work joins a recent strand in NLP that systematically analyzes what d"
P12-1015,J10-4006,1,0.271139,"ounts by Pointwise Mutual Information, and it is a close approximation to the LogLikelihood Ratio (Evert, 2005). It counteracts the tendency of PMI to favour extremely rare events. 2 137 30K target words across the 30K documents in the concatenated corpus that have the largest cumulative LMI mass. This model is thus akin to traditional Latent Semantic Analysis (Landauer and Dumais, 1997), without dimensionality reduction. We add to the models we constructed the freely available Distributional Memory (DM) model,3 that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci, 2010). DM is an example of a more complex textbased model that exploits lexico-syntactic and dependency relations between words (see Baroni and Lenci’s article for details), and we use it as an instance of a grammar-based model. DM is based on the same corpora we used plus the 100M-word British National Corpus,4 and it also uses LMI scores. 2.2 Visual models The visual models use information extracted from images instead of textual corpora. We use image data where each image is associated with one or more words or tags (we use “tag” for each word associated to the image, and “label” for the set of"
P12-1015,R11-1055,0,0.0485659,"ext corpora (Turney and Pantel, 2010). These models (as well as virtually all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation (Louwerse, 2011), crucially on the information gathered through perception. Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision (Mooney, 2008). A few studies have begun to use visual information extracted from images as part of distributional semantic models (Bergsma and Van Durme, 2011; Bergsma and Goebel, 2011; Bruni et al., 2011; Feng and Lapata, 2010; Leong and Mihalcea, 2011). These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353. This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carry out a systematic comparison of models using textual, visual, and both types of information. (2) We evaluate the models on general semantic relatedness tasks and on two"
P12-1015,W11-2503,1,0.920391,"ntel, 2010). These models (as well as virtually all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation (Louwerse, 2011), crucially on the information gathered through perception. Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision (Mooney, 2008). A few studies have begun to use visual information extracted from images as part of distributional semantic models (Bergsma and Van Durme, 2011; Bergsma and Goebel, 2011; Bruni et al., 2011; Feng and Lapata, 2010; Leong and Mihalcea, 2011). These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353. This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carry out a systematic comparison of models using textual, visual, and both types of information. (2) We evaluate the models on general semantic relatedness tasks and on two specific tasks where"
P12-1015,N10-1011,0,0.589516,"odels (as well as virtually all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation (Louwerse, 2011), crucially on the information gathered through perception. Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision (Mooney, 2008). A few studies have begun to use visual information extracted from images as part of distributional semantic models (Bergsma and Van Durme, 2011; Bergsma and Goebel, 2011; Bruni et al., 2011; Feng and Lapata, 2010; Leong and Mihalcea, 2011). These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353. This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carry out a systematic comparison of models using textual, visual, and both types of information. (2) We evaluate the models on general semantic relatedness tasks and on two specific tasks where visual information is"
P12-1015,I11-1162,0,0.0842035,"ally all work in computational lexical semantics) rely on verbal information only, while human semantic knowledge also relies on non-verbal experience and representation (Louwerse, 2011), crucially on the information gathered through perception. Recent developments in computer vision make it possible to computationally model one vital human perceptual channel: vision (Mooney, 2008). A few studies have begun to use visual information extracted from images as part of distributional semantic models (Bergsma and Van Durme, 2011; Bergsma and Goebel, 2011; Bruni et al., 2011; Feng and Lapata, 2010; Leong and Mihalcea, 2011). These preliminary studies all focus on how vision may help text-based models in general terms, by evaluating performance on, for instance, word similarity datasets such as WordSim353. This paper contributes to connecting language and perception, focusing on how to exploit visual information to build better models of word meaning, in three ways: (1) We carry out a systematic comparison of models using textual, visual, and both types of information. (2) We evaluate the models on general semantic relatedness tasks and on two specific tasks where visual information is highly relevant, as they fo"
P12-1015,W11-0611,0,0.0285915,"Missing"
P12-1015,P10-1071,0,0.00642982,"0.3 Vision: green ● L 0.2 Vision: blue 0.30 0.30 Vision: black ● N L N 0.00 0.00 0.00 0.0 0.0 ● L L N L N L N Figure 2: Discrimination of literal (L) vs. nonliteral (N) uses by the best visual and textual models. termining the color of an object by the nearness of the noun denoting the object to the color term. In other words, we are trying to model the meaning of color terms and how they relate to other words, and not to directly extract the color of an object from pictures depicting them. Our second experiment is connected to the literature on the automated detection of figurative language (Shutova, 2010). There is in particular some similarity with the tasks studied by Turney et al. (2011). Turney and colleagues try, among other things, to distinguish literal and metaphorical usages of adjectives when combined with nouns, including the highly visual adjective dark (dark hair vs. dark humour). Their method, based on automatically quantifying the degree of abstractness of the noun, is complementary to ours. Future work could combine our approach and theirs. 7 Conclusion We have presented evidence that distributional semantic models based on text, while providing a good general semantic represen"
P12-1015,D11-1063,0,0.0167834,"0.00 0.0 0.0 ● L L N L N L N Figure 2: Discrimination of literal (L) vs. nonliteral (N) uses by the best visual and textual models. termining the color of an object by the nearness of the noun denoting the object to the color term. In other words, we are trying to model the meaning of color terms and how they relate to other words, and not to directly extract the color of an object from pictures depicting them. Our second experiment is connected to the literature on the automated detection of figurative language (Shutova, 2010). There is in particular some similarity with the tasks studied by Turney et al. (2011). Turney and colleagues try, among other things, to distinguish literal and metaphorical usages of adjectives when combined with nouns, including the highly visual adjective dark (dark hair vs. dark humour). Their method, based on automatically quantifying the degree of abstractness of the noun, is complementary to ours. Future work could combine our approach and theirs. 7 Conclusion We have presented evidence that distributional semantic models based on text, while providing a good general semantic representation of word meaning, can be outperformed by models using visual information for sema"
P16-1144,D15-1075,0,0.0561594,"n about one fifth of the cases, the annotators could not guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model’s ability to understand the broader context, which is instead the focus of LAMBADA. The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (Zweig and Burges, 2011). However, the latter limited context to single sentences, not attempting to measure broader passage understanding. Of course, text understanding can be tested through other tasks, including entailment detection (Bowman et al., 2015), answering questions about a text (Richardson et al., 2013; Weston et al., 2015) and measuring inter-clause coherence (Yin and Sch¨utze, 2015). While different tasks can provide complementary insights into the models’ abilities, we find word prediction particularly attractive because of its naturalness (it’s easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sen"
P16-1144,P82-1020,0,0.82838,"Missing"
P16-1144,C14-3002,0,0.0424602,"Missing"
P16-1144,P15-1007,0,0.0212447,"Missing"
P16-1144,D13-1020,0,0.058654,"guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model’s ability to understand the broader context, which is instead the focus of LAMBADA. The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (Zweig and Burges, 2011). However, the latter limited context to single sentences, not attempting to measure broader passage understanding. Of course, text understanding can be tested through other tasks, including entailment detection (Bowman et al., 2015), answering questions about a text (Richardson et al., 2013; Weston et al., 2015) and measuring inter-clause coherence (Yin and Sch¨utze, 2015). While different tasks can provide complementary insights into the models’ abilities, we find word prediction particularly attractive because of its naturalness (it’s easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sentences entail each other. Moreover, models can have access"
P16-1144,N15-1020,0,0.0259137,"ately, the system responses are appropriate for the respective questions. However, when taken together, they are incoherent. The system behaviour is somewhat parrot-like. It can locally produce perfectly sensible language fragments, but it fails to take the meaning of the broader discourse context into account. Much research effort has consequently focused on designing systems able to keep information from the broader context into memory, and possibly even perform simple forms of reasoning about it (Hermann et al., 2015; Hochreiter and Schmidhuber, 1997; Ji et al., 2015; Mikolov et al., 2015; Sordoni et al., 2015; Sukhbaatar et al., 2015; Wang and Cho, 2015, a.o.). In this paper, we introduce the LAMBADA dataset (LAnguage Modeling Broadened to Account for Discourse Aspects). LAMBADA proposes a word prediction task where the target item is difficult to guess (for English speakers) when only the sentence in which it appears is available, but becomes easy when a broader context is presented. Consider Example (1) in Figure 1. The sentence Do you honestly think that I would want you to have a ? has a multitude of possible continuations, but the broad context clearly indicates that the missing word is misca"
P19-1324,P18-1198,0,0.0611239,"Missing"
P19-1324,W10-2803,0,0.0305465,"thub.com/amore-upf/ LSTM_ambiguity word-level representations that do not change across contexts, that is, “static” word embeddings. These are then passed to further processing layers, such as the hidden layers in a recurrent neural network (RNN). Akin to classic distributional semantics (Erk, 2012), word embeddings are formed as an abstraction over the various uses of words in the training data. For this reason, they are apt to represent context-invariant information about a word —its lexical information— but not the contribution of a word in a particular context —its contextual information (Erk, 2010). Indeed, word embeddings subsume information relative to various senses of a word (e.g., mouse is close to words from both the animal and computer domain; Camacho-Collados and Pilehvar, 2018). Classic distributional semantics attempted to do composition to account for contextual effects, but it was in general unable to go beyond short phrases (Baroni, 2013); newer-generation neural network models have supposed a big step forward, as they can natively do composition (Westera and Boleda, 2019). In particular, the hidden layer activations in an RNN can be seen as putting words in context, as the"
P19-1324,D08-1094,0,0.159716,"Missing"
P19-1324,E14-1057,0,0.0499997,"Missing"
P19-1324,Q16-1037,0,0.0328091,"program, production, broadcast (3) . . . The inauguration of Dubai Internet City coincides with the opening of an annual IT show in Dubai. . . . exhibition, conference, convention, demonstration demonstrate, exhibit, indicate, offer, reveal conference, event, convention, symposium, exhibition conference, event, exhibition, symposium, convention Table 1: Examples from the LexSub dataset (Kremer et al., 2014) and nearest neighbors for target representations. Our work follows a recent strand of research that purport to identify what linguistic properties deep learning models are able to capture (Linzen et al., 2016; Adi et al., 2017; Gulordava et al., 2018; Conneau et al., 2018; Hupkes et al., 2018, a.o.). We train diagnostic models on the tasks of retrieving the embedding of a word and a representation of its contextual meaning, respectively —the latter obtained from a Lexical Substitution dataset (Kremer et al., 2014). Our results suggest that LSTM language models heavily rely on the lexical information in the word embeddings, at the expense of contextually relevant information. Although further analysis is necessary, this suggests that there is still much room for improvement to account for contextua"
P19-1324,N19-1112,0,0.0248669,"k word information. For predictive states, results improve closer to the output (from layer 1 to 3; they instead degrade for current states). We link this to the double objective that a LM has when it comes to word information: to integrate a word passed as input, and to predict one as output. Our results suggest that the hidden states keep track of information for both words, but lower layers focus more on the processing of the input and higher ones on the predictive aspect (see Fig. 1). This is in line with previous work showing that activations close to the output tend to be task-specific (Liu et al., 2019). Moreover, from predictive states, it is easier to retrieve contextual than lexical representations (.41/.43 vs. .29; the opposite was true for current states). Our hypothesis is that this is due to a combination of two factors. On the one hand, predictive states are based solely on contextual information, which highlights only certain aspects of a word; for instance, the context of Ex. (2) in Table 1 clearly signals that a noun is expected, and the predictive states in a LM should be sensitive to this kind of cue, as it affects the probability distribution over words. On the other hand, lexi"
P19-1324,K16-1006,0,0.133178,"(Westera and Boleda, 2019). In particular, the hidden layer activations in an RNN can be seen as putting words in context, as they combine the word embedding with information coming from the context (the adjacent hidden states). The empirical success of RNN models, and in particular LSTM architectures, at fundamental tasks like Language Modeling (Jozefowicz et al., 2015) suggests that they are indeed capturing relevant contextual properties. Moreover, contextualized representations derived from such models have been shown to be very informative as input for lexical disambiguation tasks (e.g. Melamud et al., 2016; Peters et al., 2018). We here present a method to probe the extent to which the hidden layers of an LSTM language trained on English data represent lexical and contextual information about words, in order to investigate how the model copes with lexical ambiguity. 3342 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3342–3348 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Examples LexSub w NN s NN w&s NN (1) . . . I clapped her shoulder to show I was not laughing at her. . . demonstrate, display, indic"
P19-1324,W19-0423,0,0.0158631,"on is to use Lexical Substitution data (McCarthy and Navigli, 2009) and, in particular, the large dataset by Kremer et al., 2014 (henceforth, LexSub; see Table 1). In this dataset, words in context (up to 3 sentences) are annotated with a set of paraphrases given by human subjects. Since contextual substitutes reflect differences among uses of a word (for instance, demonstrate paraphrases show in a context like Ex. (1), but not in Ex. (2)), this type of data is often used as an evaluation benchmark for contextual representations of words (e.g., Erk and Pad´o, 2008; Melamud et al., 2016; Gar´ı Soler et al., 2019). We leverage LexSub to build proxies for ground-truth representations of the contextual meaning of words. We define two types of representations, inspired by previous work that proposed simple vector operations to combine word representations (Mitchell and Lapata, 2010; Thater et al., 2011, a.o.): the average embedding of the substitute words (henceforth, s), and the average embedding of the union of the substitute words and the target word (w&s). As Table 1 qualitatively shows, the resulting representations tend to be close to the substitute words and reflect the contextual nuance conveyed b"
P19-1324,N18-1108,1,0.850332,". The inauguration of Dubai Internet City coincides with the opening of an annual IT show in Dubai. . . . exhibition, conference, convention, demonstration demonstrate, exhibit, indicate, offer, reveal conference, event, convention, symposium, exhibition conference, event, exhibition, symposium, convention Table 1: Examples from the LexSub dataset (Kremer et al., 2014) and nearest neighbors for target representations. Our work follows a recent strand of research that purport to identify what linguistic properties deep learning models are able to capture (Linzen et al., 2016; Adi et al., 2017; Gulordava et al., 2018; Conneau et al., 2018; Hupkes et al., 2018, a.o.). We train diagnostic models on the tasks of retrieving the embedding of a word and a representation of its contextual meaning, respectively —the latter obtained from a Lexical Substitution dataset (Kremer et al., 2014). Our results suggest that LSTM language models heavily rely on the lexical information in the word embeddings, at the expense of contextually relevant information. Although further analysis is necessary, this suggests that there is still much room for improvement to account for contextual meanings. Finally, we show that the hidd"
P19-1324,N18-1202,0,0.271815,"2019). In particular, the hidden layer activations in an RNN can be seen as putting words in context, as they combine the word embedding with information coming from the context (the adjacent hidden states). The empirical success of RNN models, and in particular LSTM architectures, at fundamental tasks like Language Modeling (Jozefowicz et al., 2015) suggests that they are indeed capturing relevant contextual properties. Moreover, contextualized representations derived from such models have been shown to be very informative as input for lexical disambiguation tasks (e.g. Melamud et al., 2016; Peters et al., 2018). We here present a method to probe the extent to which the hidden layers of an LSTM language trained on English data represent lexical and contextual information about words, in order to investigate how the model copes with lexical ambiguity. 3342 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3342–3348 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Examples LexSub w NN s NN w&s NN (1) . . . I clapped her shoulder to show I was not laughing at her. . . demonstrate, display, indicate, prove, clarify de"
P19-1324,I11-1127,0,0.0797317,"Missing"
peris-etal-2010-adn,W04-2610,0,\N,Missing
peris-etal-2010-adn,J02-3004,0,\N,Missing
peris-etal-2010-adn,S07-1003,0,\N,Missing
peris-etal-2010-adn,taule-etal-2008-ancora,1,\N,Missing
peris-etal-2010-adn,aparicio-etal-2008-ancora,1,\N,Missing
reese-etal-2010-wikicorpus,W07-0201,0,\N,Missing
reese-etal-2010-wikicorpus,agirre-de-lacalle-2004-publicly,0,\N,Missing
reese-etal-2010-wikicorpus,W08-2207,1,\N,Missing
reese-etal-2010-wikicorpus,widdows-ferraro-2008-semantic,0,\N,Missing
reese-etal-2010-wikicorpus,E09-1005,0,\N,Missing
reese-etal-2010-wikicorpus,S07-1015,1,\N,Missing
reese-etal-2010-wikicorpus,W01-0703,0,\N,Missing
reese-etal-2010-wikicorpus,atserias-etal-2006-freeling,0,\N,Missing
reese-etal-2010-wikicorpus,zesch-etal-2008-extracting,0,\N,Missing
reese-etal-2010-wikicorpus,atserias-etal-2008-semantically,0,\N,Missing
S12-1023,W06-2911,0,0.0195401,"d senses. We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992). A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular po"
S12-1023,J07-4004,0,0.0367893,"ntifying which lemmas of a given set instantiate a specific meta alternation. We let the model rank the lemmas through the score function (cf. Table (1) and Eq. (5)) and evaluate the ranked list using Average Precision. While an alternative would be to rank meta alternations for a given polysemous lemma, the method chosen here has the benefit of providing data on the performance of individual meta senses and meta alternations. 4.1 Data All modeling and data extraction was carried out on the written part of the British National Corpus (BNC; Burnage and Dunlop (1992)) parsed with the C&C tools (Clark and Curran, 2007). 6 For the evaluation, we focus on disemous words, words which instantiate exactly two meta senses according to WordNet. For each meta alternation (m, m0 ), we evaluate CAM on a set of disemous targets (lemmas that instantiate (m, m0 )) and disemous distractors (lemmas that do not). We define three types of distractors: (1) distractors sharing m with the targets (but not m0 ), (2) distractors sharing m0 with the targets (but not m), and (3) distractors sharing neither. In this way, we ensure that CAM cannot obtain good results by merely modeling the similarity of targets to either m or m0 , w"
S12-1023,P05-1004,0,0.0302659,"ty, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are interested in relations within words, namely between word senses. We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992). A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et a"
S12-1023,J10-4007,1,0.925057,"Missing"
S12-1023,H92-1045,0,0.231181,"ograph:camera). The framework defined in Section 2 conceptualizes our task in a way parallel to that of analogical reasoning, modeling not “first-order” semantic similarity, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are interested in relations within words, namely between word senses. We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992). A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also b"
S12-1023,P90-1034,0,0.617696,"been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular polysemy for adjectives. 7 Conclusions and Future Work We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics. We have presented a formal framewor"
S12-1023,E09-1045,0,0.0527305,"Missing"
S12-1023,P03-1009,0,0.0709775,"in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular polysemy for adjectives. 7 Conclusions and Future Work We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics. We have presented a formal framework to represent and operate with regular sense alternations, as well as a first simple instantiation of the framework. We have conducted an evaluation of different implementations of this model in the new task of determining whether words match a given sense alternation. All models signifi"
S12-1023,P99-1004,0,0.154155,"fore averaging. This gives equal weight to the each lemma or meta sense, respectively. Macro-averaging in repA thus assumes that senses are equally distributed, which is an oversimplification, as word senses are known to present skewed distributions (McCarthy et al., 2004) and vectors for words with a predominant sense will be similar to the dominant meta sense vector. Micro-averaging partially models sense skewedness under the assumption that word frequency correlates with sense frequency. Similarity measure. As the vector similarity measure in Eq. (5), we use the standard cosine similarity (Lee, 1999). It ranges between −1 and 1, with 1 denoting maximum similarity. In the current model where the vectors do not contain negative counts, the range is [0; 1]. 5 Results Effect of Parameters The four parameters of Section 4.3 (three space types, macro-/micro-averaging for repM and repA , and log-likelihood transformation) correspond to 24 instantiations of CAM. Figure 1 shows the influence of the four parameters. The only significant difference is tied to the use of lexicalized vector spaces (gramlex / lex are better than gram). The statistical significance of this difference was verified by a t"
S12-1023,W04-0837,0,0.028326,"erage, that is, a simple average over all instances. For repM and repA , there 155 is a design choice: The centroid can be computed by micro-averaging as well, which assigns a larger weight to more frequent lemmas (repM ) or meta senses (repA ). Alternatively, it can be computed by macro-averaging, that is, by normalizing the individual vectors before averaging. This gives equal weight to the each lemma or meta sense, respectively. Macro-averaging in repA thus assumes that senses are equally distributed, which is an oversimplification, as word senses are known to present skewed distributions (McCarthy et al., 2004) and vectors for words with a predominant sense will be similar to the dominant meta sense vector. Micro-averaging partially models sense skewedness under the assumption that word frequency correlates with sense frequency. Similarity measure. As the vector similarity measure in Eq. (5), we use the standard cosine similarity (Lee, 1999). It ranges between −1 and 1, with 1 denoting maximum similarity. In the current model where the vectors do not contain negative counts, the range is [0; 1]. 5 Results Effect of Parameters The four parameters of Section 4.3 (three space types, macro-/micro-averag"
S12-1023,J01-3003,0,0.089857,"previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular polysemy for adjectives. 7 Conclusions and Future Work We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics. We have presented a formal framework to represent and operate"
S12-1023,J07-2002,1,0.846005,"Missing"
S12-1023,P93-1024,0,0.797098,"Missing"
S12-1023,N10-1013,0,0.0762424,"of CAM is that it avoids word sense disambiguation, although it still relies on a predefined sense inventory (WordNet, through CoreLex). Our use of monosemous words to represent meta senses and meta alternations goes beyond previous work which uses monosemous words to disambiguate polysemous words in context (Izquierdo et al., 2009; Navigli and Velardi, 2005). Because of its focus on avoiding disambiguation, CAM simplifies the representation of meta alternations and polysemous words to single centroid vectors. In the future, we plan to induce word senses (Sch¨utze, 1998; Pantel and Lin, 2002; Reisinger and Mooney, 2010), which will allow for more flexible and realistic models. abs act agt anm art atr cel chm com con A BSTRACTION ACT AGENT A NIMAL A RTIFACT ATTRIBUTE C ELL C HEMICAL C OMMUNICATION C ONSEQUENCE ent evt fod frm grb grp grs hum lfr lme E NTITY E VENT F OOD F ORM B IOLOG . G ROUP G ROUPING S OCIAL G ROUP H UMAN L IVING B EING L INEAR M EASURE loc log mea mic nat phm pho plt pos pro L OCATION G EO . L OCATION M EASURE M ICROORGANISM NATURAL B ODY P HENOMENON P HYSICAL O BJECT P LANT P OSSESSION P ROCESS prt psy qud qui rel spc sta sub tme pro PART P SYCHOL . F EATURE D EFINITE Q UANTITY I NDEFINIT"
S12-1023,P99-1014,0,0.219419,"thing that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 2006; Joanis et al., 2008). However, in most of this research polysemy is ignored. A few exceptions use soft clustering for multiple assignment of verbs to semantic classes (Pereira et al., 158 1993; Rooth et al., 1999; Korhonen et al., 2003), and Boleda et al. (to appear) explicitly model regular polysemy for adjectives. 7 Conclusions and Future Work We have argued that modeling regular polysemy and other analogical processes will help improve current models of word meaning in empirical computational semantics. We have presented a formal framework to represent and operate with regular sense alternations, as well as a first simple instantiation of the framework. We have conducted an evaluation of different implementations of this model in the new task of determining whether words match a given sense alterna"
S12-1023,J06-2001,0,0.204394,"r more parameters to set. Definition of vector space. We instantiate the vecI function in three ways. All three are based on dependency-parsed spaces, following our intuition that topical similarity as provided by window-based spaces is insufficient for this task. The functions differ in the definition of the space’s dimensions, incorporating different assumptions about distributional differences among meta alternations. The first option, gram, uses grammatical paths of lengths 1 to 3 as dimensions and thus characterizes lemmas and meta senses in terms of their grammatical context (Schulte im Walde, 2006), with a total of 2,528 paths. The second option, lex, uses words as dimensions, treating the dependency parse as a co-occurrence filter (Pad´o and Lapata, 2007), and captures topical distinctions. The third option, gramlex, uses lexicalized dependency paths like obj–see to mirror more fine-grained semantic properties (Grefenstette, 1994). Both lex and gramlex use the 10,000 most frequent items in the corpus. Vector elements. We use “raw” corpus cooccurrence frequencies as well as log-likelihoodtransformed counts (Lowe, 2001) as elements of the co-occurrence vectors. Definition of centroid com"
S12-1023,J98-1004,0,0.778335,"Missing"
S12-1023,P08-1078,0,0.0728258,"Missing"
S12-1023,N01-1010,0,0.0363629,"5 0.10 0.15 0.20 0.25 coherence Figure 2: Average Precision and Coherence (κ) for each meta alternation. Correlation: r = 0.743 (p &lt; 0.001) 6 Related work As noted in Section 1, there is little work in empirical computational semantics on explicitly modeling sense alternations, although the notions that we have formalized here affect several tasks across NLP subfields. Most work on regular sense alternations has focused on regular polysemy. A pioneering study is Buitelaar (1998), who accounts for regular polysemy through the CoreLex resource (cf. Section 3). A similar effort is carried out by Tomuro (2001), but he represents regular polysemy at the level of senses. Recently, Utt and Pad´o (2011) explore the differences between between idiosyncratic and regular polysemy patterns building on CoreLex. Lapata (2000) focuses on the default meaning arising from word combinations, as opposed to the polysemy of single words as in this study. Meta alternations other than regular polysemy, such as metonymy, play a crucial role in Information Extraction. For instance, the meta alternation S OCIAL G ROUP -G EOGRAPHICAL L OCATION corresponds to an ambiguity between the L OCATIONO RGANIZATION Named Entity cl"
S12-1023,D11-1063,0,0.0447519,"CoreLex. Lapata (2000) focuses on the default meaning arising from word combinations, as opposed to the polysemy of single words as in this study. Meta alternations other than regular polysemy, such as metonymy, play a crucial role in Information Extraction. For instance, the meta alternation S OCIAL G ROUP -G EOGRAPHICAL L OCATION corresponds to an ambiguity between the L OCATIONO RGANIZATION Named Entity classes which is known to be a hard problem in Named Entity Recognition and Classification (Markert and Nissim, 2009). Metaphorical meta alternations have also received attention recently (Turney et al., 2011) On a structural level, the prediction of meta alternations shows a clear correspondence to analogy prediction as approached in Turney (2006) (carpenter:wood is analogous to mason:stone, but not to photograph:camera). The framework defined in Section 2 conceptualizes our task in a way parallel to that of analogical reasoning, modeling not “first-order” semantic similarity, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are"
S12-1023,J06-3003,0,0.0254949,"Meta alternations other than regular polysemy, such as metonymy, play a crucial role in Information Extraction. For instance, the meta alternation S OCIAL G ROUP -G EOGRAPHICAL L OCATION corresponds to an ambiguity between the L OCATIONO RGANIZATION Named Entity classes which is known to be a hard problem in Named Entity Recognition and Classification (Markert and Nissim, 2009). Metaphorical meta alternations have also received attention recently (Turney et al., 2011) On a structural level, the prediction of meta alternations shows a clear correspondence to analogy prediction as approached in Turney (2006) (carpenter:wood is analogous to mason:stone, but not to photograph:camera). The framework defined in Section 2 conceptualizes our task in a way parallel to that of analogical reasoning, modeling not “first-order” semantic similarity, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are interested in relations within words, namely between word senses. We cannot expect two different senses of the same noun to co-occur in the"
S12-1023,W11-0128,1,0.8629,"Missing"
S12-1023,C92-2070,0,0.492306,"emantic similarity, but “second-order” semantic relations. However, the two tasks cannot be approached with the same methods, as Turney’s model relies on contexts linking two nouns in corpus sentences (what does A do to B?). In contrast, we are interested in relations within words, namely between word senses. We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992). A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well. However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not. The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations. Meta senses also bear a close resemblance to the notion of semantic class as used in lexical acquisition (Hindle, 1990; Merlo and Stevenson, 2001; Schulte im Walde, 200"
S12-1023,J12-3005,1,\N,Missing
S13-1002,S12-1051,0,0.0454478,"fidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012’s competition was by B¨ar et al. (2012), an ensemble system that integrates many techniques including string similarity, n-gram overlap, WordNet similarity, vector space similarity and MT evaluation metrics. Weighted inference, and combined structuraldistributional representations One approach to weighted inference in NLP is that of Hobbs et al. (1993), who proposed viewing natural language interpretation as abductive inference. In this framework, problems like reference resolution and syntactic amb"
S13-1002,S12-1059,0,0.0221299,"Missing"
S13-1002,D10-1115,0,0.448308,"the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly"
S13-1002,P11-1062,0,0.0850071,"e make use of existing approaches that compute distributional representations for phrases. In particular, we compute the vector for a phrase from the vectors of the words in that phrase, using either vector addition (Landauer and Dumais, 1997) or component-wise multiplication (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). The inference-rule weight, wt(p1 , p2 ), for two phrases p1 and p2 is then determined using Eq. (5) in the same way as for words. Existing approaches that derive phrasal inference rules from distributional similarity (Lin and Pantel, 2001a; Szpektor and Dagan, 2008; Berant et al., 2011) precompile large lists of inference rules. By comparison, distributional phrase similarity can be seen as a generator of inference rules “on the fly”, as it is possible to compute distributional phrase vectors for arbitrary phrases on demand as they are needed for particular examples. Inference rules are generated for all pairs of constituents (c1 , c2 ) where c1 ∈ S1 and c2 ∈ S2 , a constituent is a single word or a phrase. The logical form provides a handy way to extract phrases, as they are generally mapped to one of two logical constructs. Either we have multiple single-variable predicate"
S13-1002,H05-1079,0,0.851584,"omputing the probability of a query literal given a set of weighted clauses as background knowledge and evidence. Tasks: RTE and STS Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, implies another, the hypothesis. Consider (1) below. (1) p: Oracle had fought to keep the forms from being released h: Oracle released a confidential document Here, h is not entailed. RTE directly tests whether a system can construct semantic representations that allow it to draw correct inferences. Of existing RTE approaches, the closest to ours is by Bos and Markert (2005), who employ a purely logical approach that uses Boxer to convert both the premise and hypothesis into first-order logic and then checks for entailment using a theorem prover. By contrast, our approach uses Markov logic with probabilistic inference. Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale from 0 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations. The best performer in 2012’s competition was by B¨ar et al. (2012), an ensemble system that integrates many techniques including string similarity,"
S13-1002,W08-2222,0,0.249599,"nd Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our app"
S13-1002,W11-2504,0,0.0394132,"Missing"
S13-1002,P11-1020,0,0.0148009,"demonstrates the advantage of using a model that operationalizes entailment between words and phrases as distributional similarity. 5 On other RTE datasets there are higher previous results. Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the combined RTE-2 and RTE-3 dataset. 17 5 5.1 Task 2: Semantic Textual Similarity Dataset The dataset we use in our experiments is the MSR Video Paraphrase Corpus (MSR-Vid) subset of the STS 2012 task, consisting of 1,500 sentence pairs. The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments (Chen and Dolan, 2011). The organizers of the STS 2012 task (Agirre et al., 2012) sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video. The gold standard score is the average of the Turkers’ annotations. In addition to the MSR Video Paraphrase Corpus subset, the STS 2012 task involved data from machine translation and sense descriptions. We do not use these because they do not consist of full grammatical sentences, which the parser does not handle well. In addition, the STS 2012 data included sentences from the MSR Paraphra"
S13-1002,P04-1014,0,0.100456,"(Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007). Specifically, we utilize Markov Logic Networks (MLNs) (Domingos and Lowd, 2009), which employ weighted formulas in first-order logic to compactly encode complex undirected probabilistic graphical models. MLNs are well suited for our approach since they provide an elegant framework for assigning weights to first-order logical rules, combining a diverse set of inference rules and performing sound probabilistic inference. An MLN consists of a s"
S13-1002,J12-1002,0,0.00930197,"s very sensitive to parsing errors, and the C&C parser, on which Boxer is based, produces many errors on this dataset, even for simple sentences. When the C&C CCG parse is wrong, the resulting logical form is wrong, and the resulting similarity score is greatly affected. Dropping variable binding makes the systems more robust to parsing errors. Second, in contrast to RTE, the STS dataset does not really test the important role of syntax and logical form in deter18 Future Work rectional similarity. We plan to incorporate directed similarity measures such as those of Kotlerman et al. (2010) and Clarke (2012). A primary problem for our approach is the limitations of existing MLN inference algorithms, which do not effectively scale to large and complex MLNs. We plan to explore “coarser” logical representations such as Minimal Recursion Semantics (MRS) (Copestake et al., 2005). Another potential approach to this problem is to trade expressivity for efficiency. Domingos and Webb (2012) introduced a tractable subset of Markov Logic (TML) for which a future software release is planned. Formulating the inference problem in TML could potentially allow us to run our system on longer and more complex sente"
S13-1002,D08-1094,1,0.834937,"Missing"
S13-1002,W11-0112,1,0.929827,"970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional models (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Sch¨utze, 1998; Erk and Pad´o, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional similarity rules, and only ev"
S13-1002,D11-1129,0,0.45908,"based representations (Montague, 1970; Kamp and Reyle, 1993) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms. Distributional models (Turney and Pantel, 2010) use contextual similarity to predict semantic similarity of words and phrases (Landauer and Dumais, 1997; Mitchell and Lapata, 2010), and to model polysemy (Sch¨utze, 1998; Erk and Pad´o, 2008; Thater et al., 2010). This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (Grefenstette and Sadrzadeh, 2011; Garrette et al., 2011), which encourages developing new techniques to combine them. Garrette et al. (2011; 2013) propose a framework for combining logic and distributional models in which logical form is the primary meaning representation. Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in Figure 1. Finally, Markov Logic Networks (Richardson and Domingos, 2006) (MLNs) are used to perform weighted inference on the resulting knowledge base. However, they only employed single-word distributional simil"
S13-1002,C08-1043,0,0.0498596,"Missing"
S13-1002,P08-1028,0,0.223342,"to handle both RTE and STS, we do not know of any other methods that have been explicitly tested on both problems. 2 Related work Distributional semantics Distributional models define the semantic relatedness of words as the similarity of vectors representing the contexts in which they occur (Landauer and Dumais, 1997; Lund and Burgess, 1996). Recently, such models have also been used to represent the meaning of larger phrases. The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010). Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Markov Logic In order to combine logical and probabilistic information, we d"
S13-1002,P12-1052,0,0.0289542,"vely combining logical representations with distributional information automatically acquired from text. In this section, we discuss some of limitations of the current work and directions for future research. As noted before, parse errors are currently a significant problem. We use Boxer to obtain a logical representation for a sentence, which in turn relies on the C&C parser. Unfortunately, C&C misparses many sentences, which leads to inaccurate logical forms. To reduce the impact of misparsing, we plan to use a version of C&C that can produce the top-n parses together with parse re-ranking (Ng and Curran, 2012). As an alternative to re-ranking, one could obtain logical forms for each of the topn parses, and create an MLN that integrates all of them (together with their certainty) as an underspecified meaning representation that could then be used to directly support inferences such as STS and RTE. We also plan to exploit a greater variety of distributional inference rules. First, we intend to incorporate logical form translations of existing distributional inference rule collections (e.g., (Berant et al., 2011; Chan et al., 2011)). Another issue is obtaining improved rule weights based on distributi"
S13-1002,J98-1004,0,0.284282,"Missing"
S13-1002,C08-1107,0,0.309922,"y describe similar entities and thus there is some degree of entailment between them. At the sentence level, however, we hold that a stricter, logic-based view of entailment is beneficial, and we even model sentence similarity (in STS) as entailment. There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data. First, we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words. In comparison to existing methods for creating textual inference rules (Lin and Pantel, 2001b; Szpektor and Dagan, 2008), these rules are computed on the fly as needed, rather than pre-compiled. Second, we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors. We replace deterministic conjunction by an average combiner, which encodes causal independence (Natarajan et al., 2010). We show that our framework is able to handle both sentence similarity (STS) and textual entailment (RTE) by making some simple adaptations to the MLN when switching between tasks. The framework achieves reasonable results on both tasks"
S13-1002,P10-1097,0,0.040878,"Missing"
S14-2141,S12-1051,0,0.0737721,"Explanation) finds the overall interpretation with the maximum probability given a set of evidence. This optimization problem is a second-order cone program (SOCP) (Kimmig et al., 2012) and can be solved in polynomial time. 2.5 Recognizing Textual Entailment Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, Entails, Contradicts, or is not related (Neutral) to another, the hypothesis. 2.6 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 3.1 Distributional Representation Approach Logical Representation The first component in the system is Boxer (Bos, 2008), which maps the input sentences into logical 797 phrases using vector addition across the component predicates. We also tried computing phrase vectors using component-wise vector multiplication (Mitchell and Lapata, 2010), but found it performed marginally worse than addition. 3.3 A general problem w"
S14-2141,W11-2501,0,0.029055,"n inference rule: a → b |w, where the rule weight w is → − − a function of sim(→ a , b ), and sim is a similarity → − − measure of the distributional vectors → a , b . We experimented with the symmetric similarity measure cosine, and asym, the supervised, asymmetric similarity measure of Roller et al. (2014). The asym measure uses the vector difference → − → − ( a − b ) as features in a logistic regression classifier for distinguishing between four different word relations: hypernymy, cohyponymy, meronomy, and no relation. The model is trained using the noun-noun subset of the BLESS data set (Baroni and Lenci, 2011). The final similarity weight is given by the model’s estimated probability that the word relationship is either hypernymy → − − or meronomy: asym(→ a , b ) = P (hyper(a, b))+ P (mero(a, b)). Distributional representations for words are derived by counting co-occurrences in the ukWaC, WaCkypedia, BNC and Gigaword corpora. We use the 2000 most frequent content words as basis dimensions, and count co-occurrences within a two word context window. The vector space is weighted using Positive Pointwise Mutual Information. Phrases are defined in terms of Boxer’s output to be more than one unary atom"
S14-2141,D10-1115,0,0.0521769,"d” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on dist"
S14-2141,S13-1002,1,0.910574,"Missing"
S14-2141,P14-1114,1,0.903726,"re of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for prob"
S14-2141,W14-2402,1,0.661264,"re of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for prob"
S14-2141,W08-2222,0,0.399373,"troduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Boxer (Bos, 2008), a wide-coverage semantic analysis tool to map natural sentences to logical form. Then, distributional information is encoded in the form of inference rules. We generate lexical and phrasal rules, and experiment with symmetric and asymmetric similarity measures. Finally, we use probabilistic logic frameworks to perform inference, Markov Logic Networks (MLN) for RTE, and Probabilistic Soft Logic (PSL) for STS. 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactl"
S14-2141,N13-1092,0,0.182293,"Missing"
S14-2141,S13-1001,0,0.0323607,"rated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system builds on our prior work (Beltagy et al., 2013; Beltagy et al., 2014a; Beltagy and Mooney, 2014; Beltagy et al., 2014b). We use Box"
S14-2141,C14-1097,1,0.904999,"rvation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical inference. Our system b"
S14-2141,S12-1012,0,0.024692,"2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability to handle soft logical inferences. Systems based on distributional semantics excel in lexical and soft reasoning, but are unable to handle phenomena like negation and quantifiers. We present a system which takes the best of both approaches by combining distributional semantics with probabilistic logical i"
S14-2141,S14-2001,0,0.0930159,"r RTE system was extremely conservative: we rarely confused the Entails and Contradicts classes, indicating we correctly predict the direction of entailment, but frequently misclassify examples as Neutral. An examination of these examples showed the errors were mostly due to missing or weakly-weighted distributional rules. On STS, our vector space baseline outperforms both PSL-based systems, but the ensemble outperforms any of its components. This is a testament to Evaluation The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014 (Marelli et al., 2014a; Marelli et al., 2014b). The dataset is 10,000 pairs of sentences, 5000 training and 5000 for testing. Sentences are annotated for both tasks. 799 the power of distributional models in their ability to predict word and sentence similarity. Surprisingly, we see that the PSL + Asym system slightly outperforms the PSL + Cosine system. This may indicate that even in STS, some notion of asymmetry plays a role, or that annotators may have been biased by simultaneously annotating both tasks. As with RTE, the major bottleneck of our system appears to be the knowledge base, which is built solely usin"
S14-2141,marelli-etal-2014-sick,0,0.0679359,"r RTE system was extremely conservative: we rarely confused the Entails and Contradicts classes, indicating we correctly predict the direction of entailment, but frequently misclassify examples as Neutral. An examination of these examples showed the errors were mostly due to missing or weakly-weighted distributional rules. On STS, our vector space baseline outperforms both PSL-based systems, but the ensemble outperforms any of its components. This is a testament to Evaluation The dataset used for evaluation is SICK: Sentences Involving Compositional Knowledge dataset, a task for SemEval 2014 (Marelli et al., 2014a; Marelli et al., 2014b). The dataset is 10,000 pairs of sentences, 5000 training and 5000 for testing. Sentences are annotated for both tasks. 799 the power of distributional models in their ability to predict word and sentence similarity. Surprisingly, we see that the PSL + Asym system slightly outperforms the PSL + Cosine system. This may indicate that even in STS, some notion of asymmetry plays a role, or that annotators may have been biased by simultaneously annotating both tasks. As with RTE, the major bottleneck of our system appears to be the knowledge base, which is built solely usin"
S14-2141,P08-1028,0,0.0432548,", and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. 2.2 Distributional Semantics Distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts. Words are represented as vectors in high dimensional spaces generated from their contexts. Also, it is possible to compute vector representations for larger phrases compositionally from their parts (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Kotlerman et al., 2010; Lenci and Benotto, 2012; Roller et al., 2014). Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). Introduction Textual Entailment systems based on logical inference excel in correct reasoning, but are often brittle due to their inability"
S17-1012,D15-1002,1,0.906343,"Missing"
S17-1012,D15-1038,0,0.15187,"Missing"
S17-1012,W15-0120,0,0.0313534,"These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instance of the concept politician. Consequently, entities are generally associated with a rich set of numeric and relational attributes (for politician instances: size, office, etc.). In contrast to concepts, the values of these attributes tend to be discrete (Herbelot, 2015): while the size of politician is best described by a probability distribution, the size of Donald Trump is 1.88m. Since distributional representations are notoriously bad at handling discrete knowledge (Fodor and Lepore, 1999; Smolensky, 1990), this raises the question of how well such models can capture entity-related knowledge. In our previous work (Gupta et al., 2015), we analysed distributional prediction of numeric attributes of entities, found a large variance in quality among attributes, and identified factors determining prediction difficulty. A corresponding analysis for relational ("
S17-1012,W15-0105,0,0.0274374,"Missing"
S17-1012,Q15-1019,0,0.0545797,"we analysed distributional prediction of numeric attributes of entities, found a large variance in quality among attributes, and identified factors determining prediction difficulty. A corresponding analysis for relational (categorial) attributes of entities is still missing, even though entities are highly relevant for NLP. This is evident from the highly active area of knowledge base completion (KBC), the task of extending incomplete entity information in knowledge bases such as Yago or Wikidata (e.g., Bordes et al., 2013; Freitas et al., 2014; Neelakantan and Chang, 2015; Guu et al., 2015; Krishnamurthy and Mitchell, 2015). In this paper, we assess to what extent relational attributes of entities are easily accessible from word embedding space. To this end, we define two models that predict, given a target entity (Star Wars) and a relation (director), a distributed representation for the relatum (George Lucas). We carry out a detailed per-relation analyses of their performance on seven 1 The original dataset by Mikolov et al. (2013) did contain a small number of entity-entity relations. 104 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 104–109, c Vancouver, Ca"
S17-1012,P14-2050,0,0.392568,"entral claim about distributed models of word meaning (e.g., Mikolov et al. (2013)) is that word embedding space provides easy access to semantic relations. E.g., Mikolov et al.’s space was shown to encode the “male-female relation” linearly, as a # » # # » − woman # » = king » ). vector (man − queen The accessibility of semantic relations was subsequently examined in more detail. Rei and Briscoe (2014) and Melamud et al. (2014) reported successful modeling of lexical relations such as hypernymy and synonymy. K¨oper et al. (2015) considered a broader range of relationships,with mixed results. Levy and Goldberg (2014b) developed an improved, nonlinear relation extraction method. These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instance of the concept politician. Consequently, entities are generally associated with a rich set of numeric and relational attributes (for politician instances: size, office, etc.). In contrast t"
S17-1012,W14-1618,0,0.258101,"entral claim about distributed models of word meaning (e.g., Mikolov et al. (2013)) is that word embedding space provides easy access to semantic relations. E.g., Mikolov et al.’s space was shown to encode the “male-female relation” linearly, as a # » # # » − woman # » = king » ). vector (man − queen The accessibility of semantic relations was subsequently examined in more detail. Rei and Briscoe (2014) and Melamud et al. (2014) reported successful modeling of lexical relations such as hypernymy and synonymy. K¨oper et al. (2015) considered a broader range of relationships,with mixed results. Levy and Goldberg (2014b) developed an improved, nonlinear relation extraction method. These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instance of the concept politician. Consequently, entities are generally associated with a rich set of numeric and relational attributes (for politician instances: size, office, etc.). In contrast t"
S17-1012,W14-1619,0,0.0160294,"es not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pick up by word embeddings. 1 Introduction A central claim about distributed models of word meaning (e.g., Mikolov et al. (2013)) is that word embedding space provides easy access to semantic relations. E.g., Mikolov et al.’s space was shown to encode the “male-female relation” linearly, as a # » # # » − woman # » = king » ). vector (man − queen The accessibility of semantic relations was subsequently examined in more detail. Rei and Briscoe (2014) and Melamud et al. (2014) reported successful modeling of lexical relations such as hypernymy and synonymy. K¨oper et al. (2015) considered a broader range of relationships,with mixed results. Levy and Goldberg (2014b) developed an improved, nonlinear relation extraction method. These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example, Donald Trump is an instanc"
S17-1012,N15-1054,0,0.0176546,"ge. In our previous work (Gupta et al., 2015), we analysed distributional prediction of numeric attributes of entities, found a large variance in quality among attributes, and identified factors determining prediction difficulty. A corresponding analysis for relational (categorial) attributes of entities is still missing, even though entities are highly relevant for NLP. This is evident from the highly active area of knowledge base completion (KBC), the task of extending incomplete entity information in knowledge bases such as Yago or Wikidata (e.g., Bordes et al., 2013; Freitas et al., 2014; Neelakantan and Chang, 2015; Guu et al., 2015; Krishnamurthy and Mitchell, 2015). In this paper, we assess to what extent relational attributes of entities are easily accessible from word embedding space. To this end, we define two models that predict, given a target entity (Star Wars) and a relation (director), a distributed representation for the relatum (George Lucas). We carry out a detailed per-relation analyses of their performance on seven 1 The original dataset by Mikolov et al. (2013) did contain a small number of entity-entity relations. 104 Proceedings of the 6th Joint Conference on Lexical and Computational"
S17-1012,W14-1608,0,0.0133266,"ifficulty for a relation does not result from low frequency, but from (a) one-to-many mappings; and (b) lack of context patterns expressing the relation that are easy to pick up by word embeddings. 1 Introduction A central claim about distributed models of word meaning (e.g., Mikolov et al. (2013)) is that word embedding space provides easy access to semantic relations. E.g., Mikolov et al.’s space was shown to encode the “male-female relation” linearly, as a # » # # » − woman # » = king » ). vector (man − queen The accessibility of semantic relations was subsequently examined in more detail. Rei and Briscoe (2014) and Melamud et al. (2014) reported successful modeling of lexical relations such as hypernymy and synonymy. K¨oper et al. (2015) considered a broader range of relationships,with mixed results. Levy and Goldberg (2014b) developed an improved, nonlinear relation extraction method. These studies were conducted primarily on concepts and their semantic relations, like hypernym(politician) = person. Meanwhile, entities and the relations they partake in are much less well understood.1 Entities are instances of concepts, i.e., they refer to specific individual objects in the real world, for example,"
S17-1012,D15-1174,0,0.154722,"Missing"
S18-1008,D17-1018,0,0.0453378,"997), the standard neural network model for sequential data like language. Training and evaluating this ∗ 2 denotes equal contribution. https://competitions.codalab.org/ competitions/17310 1 Related Work Source code for our model and for the training procedure is published on https://github.com/amore-upf/ semeval2018-task4. 65 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 65–69 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Inputs: lution (e.g., the aforementioned approaches, as well as Wiseman et al. 2016; Lee et al. 2017, Francis-Landau et al. 2016). For instance, we avoid feature engineering, focusing instead on the model’s ability to learn meaningful entity representations from the dialogue itself. Moreover, we deviate from the common strategy to entity linking of incorporating a specialized coreference resolution module (e.g., Chen et al. 2017). Ross & Rachel: guy { Ross & Rachel: was ... BiLSTM: Entity library: Model Description Ross Rachel guy Ws Ws Wt + xi ... ... (ﬁctional example) ... 3 ... Ross & Rachel: the tanh hi-1 E hi+1 hi ... Wo cos ei softmax We approach the task of character identification as"
S18-1008,W17-6904,1,0.893633,"Missing"
S18-1008,E06-1002,0,0.024698,"Missing"
S18-1008,K17-1023,0,0.0196918,"to choose from is given beforehand. Since our main aim is to test the benefits of having an entity library, in other respects our model is kept more basic than existing work both on entity linking and on coreference resoIntroduction SemEval 2018 Task 4 is an entity linking task on multiparty dialogue.1 It consists in predicting the referents of nominals that refer to a person, such as she, mom, Judy – henceforth mentions. The set of possible referents is given beforehand, as well as the set of mentions to resolve. The dataset used in this task is based on Chen and Choi (2016) and Chen et al. (2017), and consists of dialogue from the TV show Friends in textual form. Our main interest is whether deep learning models for tasks like entity linking can benefit from having an explicit entity library, i.e., a component of the neural network that stores entity representations learned during training. To that end, we add such a component to an otherwise relatively basic model – a bidirectional LSTM (long shortterm memory; Hochreiter and Schmidhuber 1997), the standard neural network model for sequential data like language. Training and evaluating this ∗ 2 denotes equal contribution. https://comp"
S18-1008,W11-1901,0,0.0415662,"sed on Chen and Choi 2016; Chen et al. 2017) only a list of entities is given, without any associated encyclopedic entries. This makes the task more similar to the way in which a human audience might watch the TV show, in that they are initially unfamiliar with the characters. What also sets the present task apart from most previous tasks is its focus on multiparty dialogue (as opposed to, typically, newswire articles). A task that is closely related to entity linking is coreference resolution, i.e., the task of clustering mentions that refer to the same entity (e.g., the CoNLL shared task of Pradhan et al. 2011). Since mention clusters essentially correspond to entities (an insight central to the approaches to coreference in Haghighi and Klein 2010; Clark and Manning 2016), the present task can be regarded as a type of coreference resolution, but one where the set of referents to choose from is given beforehand. Since our main aim is to test the benefits of having an entity library, in other respects our model is kept more basic than existing work both on entity linking and on coreference resoIntroduction SemEval 2018 Task 4 is an entity linking task on multiparty dialogue.1 It consists in predicting"
S18-1008,W16-3612,0,0.249113,"ues. It is a simple, standard model with one key innovation, an entity library. Our results show that this innovation greatly facilitates the identification of infrequent characters. Because of the generic nature of our model, this finding is potentially relevant to any task that requires effective learning from sparse or unbalanced data. 1 2 Previous entity linking tasks concentrate on linking mentions to Wikipedia pages (Bunescu and Pas¸ca 2006; Mihalcea and Csomai 2007 and much subsequent work; for a recent approach see FrancisLandau et al. 2016). By contrast, in the present task (based on Chen and Choi 2016; Chen et al. 2017) only a list of entities is given, without any associated encyclopedic entries. This makes the task more similar to the way in which a human audience might watch the TV show, in that they are initially unfamiliar with the characters. What also sets the present task apart from most previous tasks is its focus on multiparty dialogue (as opposed to, typically, newswire articles). A task that is closely related to entity linking is coreference resolution, i.e., the task of clustering mentions that refer to the same entity (e.g., the CoNLL shared task of Pradhan et al. 2011). Sin"
S18-1008,P16-1061,0,0.0304543,"he way in which a human audience might watch the TV show, in that they are initially unfamiliar with the characters. What also sets the present task apart from most previous tasks is its focus on multiparty dialogue (as opposed to, typically, newswire articles). A task that is closely related to entity linking is coreference resolution, i.e., the task of clustering mentions that refer to the same entity (e.g., the CoNLL shared task of Pradhan et al. 2011). Since mention clusters essentially correspond to entities (an insight central to the approaches to coreference in Haghighi and Klein 2010; Clark and Manning 2016), the present task can be regarded as a type of coreference resolution, but one where the set of referents to choose from is given beforehand. Since our main aim is to test the benefits of having an entity library, in other respects our model is kept more basic than existing work both on entity linking and on coreference resoIntroduction SemEval 2018 Task 4 is an entity linking task on multiparty dialogue.1 It consists in predicting the referents of nominals that refer to a person, such as she, mom, Judy – henceforth mentions. The set of possible referents is given beforehand, as well as the s"
S18-1008,N16-1150,0,0.0289365,"d neural network model for sequential data like language. Training and evaluating this ∗ 2 denotes equal contribution. https://competitions.codalab.org/ competitions/17310 1 Related Work Source code for our model and for the training procedure is published on https://github.com/amore-upf/ semeval2018-task4. 65 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 65–69 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics Inputs: lution (e.g., the aforementioned approaches, as well as Wiseman et al. 2016; Lee et al. 2017, Francis-Landau et al. 2016). For instance, we avoid feature engineering, focusing instead on the model’s ability to learn meaningful entity representations from the dialogue itself. Moreover, we deviate from the common strategy to entity linking of incorporating a specialized coreference resolution module (e.g., Chen et al. 2017). Ross & Rachel: guy { Ross & Rachel: was ... BiLSTM: Entity library: Model Description Ross Rachel guy Ws Ws Wt + xi ... ... (ﬁctional example) ... 3 ... Ross & Rachel: the tanh hi-1 E hi+1 hi ... Wo cos ei softmax We approach the task of character identification as one of multi-class classific"
S18-1008,D15-1002,1,0.811594,"Missing"
sanchez-marco-etal-2010-annotation,W09-0214,0,\N,Missing
vandeghinste-etal-2008-evaluation,habash-dorr-2002-handling,0,\N,Missing
vandeghinste-etal-2008-evaluation,P02-1040,0,\N,Missing
vandeghinste-etal-2008-evaluation,A00-1031,0,\N,Missing
vandeghinste-etal-2008-evaluation,N03-1017,0,\N,Missing
vandeghinste-etal-2008-evaluation,N03-1013,0,\N,Missing
vandeghinste-etal-2008-evaluation,2003.mtsummit-systems.9,0,\N,Missing
vandeghinste-etal-2008-evaluation,2005.mtsummit-papers.11,0,\N,Missing
vandeghinste-etal-2008-evaluation,2003.eamt-1.7,1,\N,Missing
vandeghinste-etal-2008-evaluation,2007.mtsummit-papers.74,0,\N,Missing
vandeghinste-etal-2008-evaluation,vandeghinste-etal-2006-metis,1,\N,Missing
W04-3214,P98-1013,0,0.0221795,"r is that this variance is to a large extent a result of differences in the underlying argument structure of the predicates in different frames. In a second experiment, we show that frame uniformity, which measures argument structure variation, correlates well with the performance figures, effectively explaining the variance. 1 Introduction Recent years have witnessed growing interest in corpora with semantic annotation, especially on the semantic role (or argument structure) level. A number of projects are working on producing such corpora through manual annotation, among which are FrameNet (Baker et al., 1998), the Prague Dependency Treebank (Hajiˇcová, 1998), PropBank (Kingsbury et al., 2002), and SALSA (Erk et al., 2003). For semantic role annotation to be widely useful for NLP, however, robust and accurate methods for automatic semantic role assignment are necessary. Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). This year (2004), semantic role labelling served as the shared task at two conferences, CoNLL1 and SENSEVAL2. However, almost all studies have co"
W04-3214,W04-0817,1,0.818377,"e into account that the relevant linking properties differ between individual predicates. Our results suggest that the variance caused by argument structure will not disappear with better classifiers, but that the problem of inadequate generalisations should be addressed in a principled way. There are several possible approaches to do so. First, the classic statistical approach: Combining evidence from different frame-specific roles to alleviate data sparseness. To this end, Gildea and Jurafsky (2002) developed a mapping from framespecific to syntactic roles, but results did not improve much. Baldewein et al. (2004) experiment with EM-driven generalisation, and obtain also only modest improvements. A second approach is to identify other levels, different from frames, at which regularities can be learnt better. One possibility is to identify smaller units within frames which have a more uniform structure and which can be learnt more easily. Since uniformity is defined in terms of a quality function, clustering would be the natural method to employ for this task. However, this method is only viable for frames with a large amount of annotation. A more general idea in this spirit is to construct an independe"
W04-3214,A00-1031,0,0.117153,"Missing"
W04-3214,P97-1003,0,0.159516,"Missing"
W04-3214,P03-1068,1,0.829712,"icates in different frames. In a second experiment, we show that frame uniformity, which measures argument structure variation, correlates well with the performance figures, effectively explaining the variance. 1 Introduction Recent years have witnessed growing interest in corpora with semantic annotation, especially on the semantic role (or argument structure) level. A number of projects are working on producing such corpora through manual annotation, among which are FrameNet (Baker et al., 1998), the Prague Dependency Treebank (Hajiˇcová, 1998), PropBank (Kingsbury et al., 2002), and SALSA (Erk et al., 2003). For semantic role annotation to be widely useful for NLP, however, robust and accurate methods for automatic semantic role assignment are necessary. Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). This year (2004), semantic role labelling served as the shared task at two conferences, CoNLL1 and SENSEVAL2. However, almost all studies have concentrated on the technical aspects of the models – identifying informative feature sets and suitable statistical f"
W04-3214,W03-1007,0,0.182197,"tic role (or argument structure) level. A number of projects are working on producing such corpora through manual annotation, among which are FrameNet (Baker et al., 1998), the Prague Dependency Treebank (Hajiˇcová, 1998), PropBank (Kingsbury et al., 2002), and SALSA (Erk et al., 2003). For semantic role annotation to be widely useful for NLP, however, robust and accurate methods for automatic semantic role assignment are necessary. Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). This year (2004), semantic role labelling served as the shared task at two conferences, CoNLL1 and SENSEVAL2. However, almost all studies have concentrated on the technical aspects of the models – identifying informative feature sets and suitable statistical frameworks – with the goal of optimising the performance of the models on the complete dataset. The only study we are aware of with a more detailed evaluation is Fleischman et al. (2003), who nevertheless come to the conclusion that either “new features”, 1 2 http://www.lsi.upc.es/~conll04st/ http://www.clres.com/SensSemRoles.html Gemma"
W04-3214,P00-1065,0,0.070522,"ely explaining the variance. 1 Introduction Recent years have witnessed growing interest in corpora with semantic annotation, especially on the semantic role (or argument structure) level. A number of projects are working on producing such corpora through manual annotation, among which are FrameNet (Baker et al., 1998), the Prague Dependency Treebank (Hajiˇcová, 1998), PropBank (Kingsbury et al., 2002), and SALSA (Erk et al., 2003). For semantic role annotation to be widely useful for NLP, however, robust and accurate methods for automatic semantic role assignment are necessary. Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). This year (2004), semantic role labelling served as the shared task at two conferences, CoNLL1 and SENSEVAL2. However, almost all studies have concentrated on the technical aspects of the models – identifying informative feature sets and suitable statistical frameworks – with the goal of optimising the performance of the models on the complete dataset. The only study we are aware of with a more detailed evaluation is Fleischman et al. (2003), who"
W04-3214,J02-3001,0,0.373578,"ontact” 3 . (1) a. b. c. [ Impactee His car] was struck [ Impactor by a third vehicle]. [ Impactor The door] slammed [ Result shut]. [ Impactors Their vehicles] collided [ Place at Pond Hill]. Note that the frame-specificity of semantic roles in FrameNet has important consequences for semantic role assignment, since there is no direct way to generalise across frames. Therefore, the learning for automatic assignment of semantic roles has to proceed frame-wise. Thus, the data sparseness problem is especially acute, and automatic assignment for frames with no training data is very difficult (see Gildea and Jurafsky (2002)). 3 Experiment 1: Frame-Wise Evaluation of Semantic Role Assignment In our first experiment, we perform a detailed (frame-wise) evaluation of semantic role assignment to discover general patterns in the data. Our aim is not to outperform existing models, but to replicate the workings of existing models so that our findings are representative for the task as it is currently addressed. To this end, we (a) use a standard dataset, the FrameNet data, (b) model the task with two different statistical frameworks, and (c) keep our models as generic as possible. 3.1 Data and experimental setup For thi"
W04-3214,J03-3005,0,0.0772868,"Missing"
W04-3214,P99-1004,0,0.0465873,", i.e. predicates in  a frame share a common set of arguments. What checks is whether the mapping from semantics to syntax is also similar. 6 The centroid of a cluster is “a point in  -dimensional space found by averaging the measurement values along each dimension” (Kaufman and Rousseeuw, 1990, p. 112), so that it is the point situated at the “center” of the cluster. In order to obtain an actual measure for frame uniformity, we take two further steps. First, we instantiate with the cosine similarity  , which has been found to be appropriate for a wide range of linguistic tasks (see e.g. Lee (1999)) and ranges between 0 (least similar) and 1 (identity):                             Second, we  normalise the values of , which !  , the number of vectors, to  , to grow in  make them interpretable analogously to values of the cosine similarity. Since this is possible in two different ways, we obtain two different measures for frame uniformity. The first one, which we call normalised quality-based  uniformity (""# ), simply divides the values by : ""#                          The second measure, weighted quality-based u"
W04-3214,W02-2018,0,0.312566,"Missing"
W04-3214,C98-1013,0,\N,Missing
W05-1009,alsina-etal-2002-catcg,1,0.759722,"as will be shown throughout the paper, and can be exploited in low-level NLP tasks (POStagging), and also in more demanding tasks, such as paraphrase detection and generation (e.g. exploiting the relationship tangible → can be touched, or deformació nasal → deformity affecting the nose). 2.2 Gold standard To perform the experiments, we built a set of annotated data based on this classification (gold standard from now on). We extracted the lemmata and data for the gold standard from a 16.5 million word Catalan corpus (Rafel, 1994), lemmatised, POS-tagged and shallow parsed with the CatCG tool (Alsina et al., 2002). The shallow parser gives information on the syntactic function of each word (subject, object, etc.), not on phrase structure. 186 lemmata were randomly chosen among all 2564 adjectives occuring more than 25 times in the corpus. 86 of the 186 lemmata were classified by 3 human judges into each of the classes (basic, object, event).2 In case of polysemy affecting the class as2 The 3 human judges were PhD students with training in linguistics, one of which had done research on adjectives. As it was defined, the level of training in linguistics needed for the signment, the judges were instructed"
W05-1009,C04-1161,1,0.913074,"pful to overcome the ceiling reached with morphology. 1 Introduction This paper fits into a broader effort addressing the automatic acquisition of semantic classes for Catalan adjectives. So far, no established standard of such semantic classes is available in theoretical or empirical linguistic research. Our aim is to reach a classification that is empirically adequate and theoretically sound, and we use computational techniques as a means to explore large amounts of data which would be impossible to explore by hand to help us define and characterise the classification. In previous research (Boleda et al., 2004), we developed a three-way classification according to generally accepted adjective properties (see Section 2), Decision trees are appropriate for our task, to test and compare sets of features, based on our gold standard. They are also known for their easy interpretation, by reading feature combinations off the tree paths. This property will help us get insight into relevant characteristics of our adjective classes, and in the error analysis. The paper is structured as follows: Section 2 presents the adjective classification and the gold standard used for the experiments. Sections 3 and 4 exp"
W05-1009,J96-2004,0,0.0307312,", ‘related to a point’ (usually, a point in time), as in això va ser un esdeveniment puntual, ‘this was a once-occuring event’. This is the meaning we would expect from the derivation punt (‘point’) + al, and is an object meaning. In this case, the judge should assign the adjective to two classes, primary basic, secondary object. Compositional meanings are thus those corresponding to active morphological processes, and can be predicted from the meaning of the noun and the derivation with the suffix (be it denominal, deverbal or participial). The judges had an acceptable 0.74 mean κ agreement (Carletta, 1996) for the assignment of the primary class, but a meaningless 0.21 for the secondary class (they did not even agree on which lemmata were polysemous). As a reaction to the low agreement about polysemy, we incorporated polysemy information from a Catalan dictionary (DLC, 1993). This information was incorporated only in addition to the gathered gold standard: In many cases the dictionary only lists the compositional sense. We added it as a second reading if our judges considered the noncompositional one as most frequent. One of the authors of the paper classified the remaining 100 lemmata accordin"
W05-1009,P93-1023,0,0.481006,"Missing"
W05-1009,P03-1009,0,0.160549,"Missing"
W05-1009,J01-3003,0,0.0900152,"Missing"
W05-1009,P02-1029,1,0.894544,"Missing"
W06-1704,J93-1001,0,0.0552394,"ifying Catalan verbs into syntactic classes (Mayol et al., 2006). Cluster analysis was applied to a 200 verb set, modeled in terms of 10 linguistically defined features. The data for the clustering were first extracted from a fragment of CTILC (14 million word). Using the manual tagging of the corpus, an average 0.84 f-score was obtained. Using CatCG, the performance decreased only 2 points (0.82 fscore). In a subsequent experiment, the data were extracted from the CUCWeb corpus. Given that it is 12 times larger than the traditional corpus, the question was whether “more data is better data” (Church and Mercer, 1993, 18-19). Banko and Brill (2001) present a case study on confusion set disambiguation that supports this slogan. Surprisingly enough, results using CUCWeb were significantly worse than those using the traditional corpus, even with automatic linguistic processing: CUCWeb lead to an average 0.71 f-score, so an 11 point difference resulted. These results somewhat question the quality of the CUCWeb corpus, particularly so as the authors attribute the difference to noise in the CUCWeb and difficulties in linguistic processing (see Section 4). However, 0.71 is still well beyond the 0.33 f-score base"
W06-1704,J03-3005,0,0.0702563,"Missing"
W06-1704,J03-3001,0,0.070647,"ct set up an architecture to retrieve a portion of the Web roughly corresponding to the Web in Spain, in order to study its formal properties (analysing its link distribution as a graph) and its characteristics in terms of pages, sites, and domains (size, kind of software used, language, among other aspects). One of the by-products of the project is a 166 million word corpus for Catalan.1 The biggest annotated Catalan corpus before CUCWeb is the CTILC corpus (Rafel, 1994), consisting of about 50 million words. In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). The most straightforward approach to using the Web as corpus is to gather data online (Grefenstette, 1998), or estimate counts 2 The WaCky project (http://wacky.sslmit.unibo.it/) aims at overcoming this challenge, by developing “a set of tools (and interfaces to existing tools) that will allow a linguist to crawl a section of the web, process the data, index them and search them”. 1 Catalan is a relatively minor language. There are currently about 10.8 million Catalan speakers, similar to Serbian (12), Greek (10.2), or Swedish (9.3). See http://www.upc.es/slt/alatac/cat/dades/catala-04.html"
W06-1704,alsina-etal-2002-catcg,1,0.882668,"Missing"
W06-1704,1999.tc-1.8,0,\N,Missing
W06-1704,P01-1005,0,\N,Missing
W11-1501,A00-1031,0,0.338641,"dictionary through an affixation module that checks whether they are derived forms, such as adverbs ending in -mente or clitic pronouns (-lo, -la) attached to verbs. This module has also been adapted, incorporating Old Spanish clitics (-gela, li) and other variants of derivation affixes (adverbs in -mientre or -mjentre). 5.3 Retraining the tagger FreeLing includes 2 different modules able to perform PoS tagging: a hybrid tagger (relax), integrating statistical and hand-coded grammatical rules, and a Hidden Markov Model tagger (hmm), which is a classical trigram markovian tagger, based on TnT (Brants, 2000). As mentioned in Section 4, the tagger for Standard Spanish has been used to pre-annotate the Gold Standard Corpus, which has subsequently been corrected to be able to carry out the retraining. The effort of correcting the corpus is much lower compared to annotating from scratch. In this paper we present the evaluation of the performance of the extended resource using the hmm tagger with the probabilities generated automatically from the trigrams in the Gold Standard Corpus. 6 Evaluation In this section we evaluate the dictionary (Section 6.1) and present the overall tagging results (Section"
W11-1501,padro-etal-2010-freeling,1,0.916541,"Missing"
W11-1501,W09-0214,0,0.059529,"Missing"
W11-1501,sanchez-marco-etal-2010-annotation,1,0.882435,"Missing"
W13-0104,E12-1004,1,0.86183,"re white possible free hypothetical naive potential safe impossible severe presumed vile likely hard probable nasty mere intelligent putative meagre mock ripe theoretical stable Table 1: Evaluated adjectives. Intensional (I) and non-intensional (N) adjectives are paired by frequency. over challenging foils. Turney (2012) shows how the observed vectors outperform any compositionallyderived model in a paraphrasing task. Grefenstette et al. (2013) reach state-of-the-art performance on widely used sentence similarity test sets with composition functions optimized on the observed vectors (see also Baroni et al., 2012; Baroni and Zamparelli, 2010; Boleda et al., 2012). Since we use the same criterion to evaluate the quality of the models, we are careful to separate training phrases from those used for evaluation (we introduce the test set in the next section). The weighted additive, dilation and full-additive models require one single set of parameters for all adjectives, and we thus use the top 10K most frequent phrases in our semantic space (excluding test items) for training. For the lexical function model, we need to train a separate weight matrix for each adjective. We do this by using as training dat"
W13-0104,D10-1115,1,0.512885,"he noun are relevant factors in the distributional representation of adjective phrases. 1 Introduction Distributional semantics (see Turney and Pantel, 2010, for an overview) has been very successful in modeling lexical semantic phenomena, from psycholinguistic facts such as semantic priming (McDonald and Brew, 2004) to tasks such as picking the right synonym on a TOEFL exercise (Landauer and Dumais, 1997). More recently, interest has increased in using distributional models to account not only for word meaning but also for phrase meaning, i.e. semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Garrette et al., 2012). Adjectival modification of nouns is a particularly useful and at the same time challenging testbed for different distributional models of composition, because syntactically it is very simple, while the semantic effect of the composition is very variable and potentially complex due to the frequent context dependence of the relation between the adjective and the noun (Asher, 2011, provides recent discussion). As a comparatively underexplored area of semantic theory, it is also an empirical domain where distributional models can give feedback to theo"
W13-0104,D12-1112,1,0.92692,"sues for the increasingly popular distributional approaches to semantics. First, as intensional adjectives cannot be modeled as first-order properties, it is hard to predict what their representations might look like or what their semantic effect would be in standard distributional models of composition based on vector addition or multiplication. This is so because addition and multiplication correspond to feature combination (see Section 2 for discussion), and it is not obvious what set of distinctive distributional features an intensional adjective would contribute on a consistent basis. In Boleda et al. (2012), we presented a first distributional semantic study of intensional adjectives. However, our study was limited in two ways. First, it compared intensional adjectives with a very narrow class of non-intensional adjectives, namely color terms; this raises doubts about the generality of our results. Second, the study had methodological weaknesses, as we did not separate training and test data, nor did we do any systematic parameter tuning prior to carrying out our experiments. This paper adresses these limitations by covering a wider variety of adjectives and using a better implementation of the"
W13-0104,W13-0112,0,0.0292883,"ative Matrix Factorization produces reduced dimensions that have no negative values, and are not fully dense. 4 I N I N alleged loose necessary modern former wide past black future white possible free hypothetical naive potential safe impossible severe presumed vile likely hard probable nasty mere intelligent putative meagre mock ripe theoretical stable Table 1: Evaluated adjectives. Intensional (I) and non-intensional (N) adjectives are paired by frequency. over challenging foils. Turney (2012) shows how the observed vectors outperform any compositionallyderived model in a paraphrasing task. Grefenstette et al. (2013) reach state-of-the-art performance on widely used sentence similarity test sets with composition functions optimized on the observed vectors (see also Baroni et al., 2012; Baroni and Zamparelli, 2010; Boleda et al., 2012). Since we use the same criterion to evaluate the quality of the models, we are careful to separate training phrases from those used for evaluation (we introduce the test set in the next section). The weighted additive, dilation and full-additive models require one single set of parameters for all adjectives, and we thus use the top 10K most frequent phrases in our semantic s"
W13-0104,W10-2805,0,0.495283,"experiments. This paper adresses these limitations by covering a wider variety of adjectives and using a better implementation of the composition functions, and performs several qualitative analyses on the results. Our results confirm that high quality adjective composition is possible in distributional models: Meaningful vectors can be composed, if we take phrase vectors directly extracted from the corpus as a benchmark. In addition, we find (perhaps unsurprisingly) that models that replicate higher-order predication within a distributional approach, such as Baroni and Zamparelli (2010) and Guevara (2010), fare better than models based on vector addition or multiplication (Mitchell and Lapata, 2010). However, unlike our previous study, we find no difference in the relative success of the different composition models on intensional vs. non-intensional modification, nor in relevant aspects of the distributional representations of corpus-harvested phrases. Rather, two relevant effects involve the polysemy of the noun and the extent to which the adjective denotes a typical attribute of the entity described by the noun. These results indicate that, in general, adjectival modification is more comple"
W13-0104,D11-1050,0,0.234325,"Missing"
W13-0104,J95-1001,0,0.181377,"l modification share properties that have gone unappreciated. If the type of modification does not explain the differences in the observed data, what does? An analysis reveals two relevant factors. The first one is the polysemy of the head noun. We find that, the more polysemous a noun is, the less similar its vector is to the corresponding phrase vector. It is plausible that modifying a noun has a larger impact when the noun is polysemous, as the adjective narrows down the meaning of the noun; indeed, adjectives have been independently shown to be powerful word sense disambiguators of nouns (Justeson and Katz, 1995). In distributional terms, the adjective notably “shifts” the vector of polysemous nouns, but for monosemous nouns there is just not much shifting room. This is reasonable but unsurprising; what is more worthy of attention is that this effect is invariant to adjective type. Both non-intensional and intensional adjectives have meaning modulating power, as shown in Table 3. For example, ripe selects for the sense of shock that has to do with a pile of sheaves of grain or corn. Similarly, past is incompatible with physical senses of range such as that referring to mountains or a cooking appliance"
W13-0104,P04-1003,0,0.0370005,"perform simple feature union or intersection, (3) contrary to what the theoretical literature might lead one to expect, do not yield a distinction between intensional and non-intensional modification, and (4) suggest that head noun polysemy and whether the adjective corresponds to a typical attribute of the noun are relevant factors in the distributional representation of adjective phrases. 1 Introduction Distributional semantics (see Turney and Pantel, 2010, for an overview) has been very successful in modeling lexical semantic phenomena, from psycholinguistic facts such as semantic priming (McDonald and Brew, 2004) to tasks such as picking the right synonym on a TOEFL exercise (Landauer and Dumais, 1997). More recently, interest has increased in using distributional models to account not only for word meaning but also for phrase meaning, i.e. semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Garrette et al., 2012). Adjectival modification of nouns is a particularly useful and at the same time challenging testbed for different distributional models of composition, because syntactically it is very simple, while the semantic effect of the composition is very"
W13-0104,D12-1110,0,0.217634,"in the distributional representation of adjective phrases. 1 Introduction Distributional semantics (see Turney and Pantel, 2010, for an overview) has been very successful in modeling lexical semantic phenomena, from psycholinguistic facts such as semantic priming (McDonald and Brew, 2004) to tasks such as picking the right synonym on a TOEFL exercise (Landauer and Dumais, 1997). More recently, interest has increased in using distributional models to account not only for word meaning but also for phrase meaning, i.e. semantic composition (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012; Garrette et al., 2012). Adjectival modification of nouns is a particularly useful and at the same time challenging testbed for different distributional models of composition, because syntactically it is very simple, while the semantic effect of the composition is very variable and potentially complex due to the frequent context dependence of the relation between the adjective and the noun (Asher, 2011, provides recent discussion). As a comparatively underexplored area of semantic theory, it is also an empirical domain where distributional models can give feedback to theoreticians about how a"
W15-2712,W13-3209,0,0.122551,"ifferent, identical, brown cardboard boxes. Adrian accidentally puts a pair of red socks in the box containing blue objects, and Barbara remarks ‘no, no, these belong in the red box’. Thus, even if red when modifying box (or indeed any noun denoting a physical ob(1) a. If you ate some of the cookies, then I won’t have enough for the party. ; some and possibly all b. A: Did you eat all the cookies? B: I ate some. ; some but not all Distributional models have so far not been particularly successful in modelling the meaning of function words (but see Baroni et al. (2012); Bernardi et al. (2013); Hermann et al. (2013)). We believe that discourse-aware distributional semantics may fare better in this respect. We elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is."
W15-2712,W13-0104,1,0.883712,"in the lexical substitution task (McCarthy and Navigli, 2007; Erk et al., 2013), which predicts one or more paraphrases for a word in a given sentence. Unlike Word Sense Disambiguation, word meaning in context is specific to a given use of a word, that is, it doesn’t assume a pre-defined list of senses and can account for highly specific contextual effects. However, in this tradition context is restricted to one sentence, so the semantic phenomena modeled do not extend to discourse or dialogue. (3) Compositional distributional semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013), which predicts the meaning of a phrase or sentence from the meaning of its component units. For instance, compositional distributional semantics accounts for how the generic distributional representation of, say, red makes different contributions when composed with nouns like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2"
W15-2712,W13-3214,0,0.0237673,"utions when composed with nouns like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2.2, we have highlighted the context update potential of utterances as a feature that should be captured by compositional distributional models beyond the word/phrase level. Recent work has evaluated such models on dialogue act tagging tasks (Kalchbrenner and Blunsom, 2013; Milajevs et al., 2014). However, these approaches consider utterances in isolation and rely on a predefined set of dialogue act types that are to a large extent arbitrary, and in any case of a metalinguistic nature. Similar comments would apply to the task of identifying discourse relations connecting isolated pairs of sentences. Instead, we argue that pragmatically-aware distributional models should help us to induce dialogue acts in an unsupervised way and to model them as context update functions. Thus, we suggest to adopt tasks that target coherence and the evolution of common ground — w"
W15-2712,D10-1113,0,0.0256717,"on discourse and dialogue context and then consider the dynamic meaning of sentences as context-change potential. 2.1 Word and Phrase Meaning As is well known, standard distributional models provide a single meaning representation for a word, which implicitly encodes all its possible senses and meaning nuances in general. A few recent models do account for some contextual effects within the scope of a sentence: For instance, the different shades of meaning that an adjective like red takes depending on the noun it modifies (e.g., car vs. cheek). However, such models, e.g. Erk and Pad´o (2008), Dinu and Lapata (2010), and Erk et al. (2013), typically use just a single word or sentence as context. They do not look into how word meaning gets progressively constrained by the common ground of the speakers as the discourse unfolds. A prominent type of “meaning adjustment” in discourse and dialogue is the interaction with the properties of the referent a particular word is associated to. For example, when we use a word like box, which a priori can be used for entities with very different properties, we typically use it to refer to a specific box in a given context, and this constrains its interpretation. The re"
W15-2712,D08-1094,0,0.0821638,"Missing"
W15-2712,P15-1029,0,0.0176232,"part. For instance, a vector for a phrase like red box in a context where red refers to the box’ contents should be mapped to different types of images depending on whether it has been constructed by a pragmatically aware model or not. Such a dataset could be constructed by creating images of referents of the same phrase used in different contexts, where the task would be to pick the best image for each context. A related task would be reference resolution in a situated visual dialogue context (which can be seen as a situated version of image retrieval). This task has recently been tackled by Kennington and Schlangen (2015), who present an incremental acWe propose to take a different look on what the distributional meaning of a sentence is. Sentences are part of larger communicative situations and, as highlighted in the Dynamic Semantic tradition, can be considered relations between the discourse so far and what is to come next. We thus challenge the distributional semantics community to develop dynamic distributional semantic models that are able to encode the “context change potential” that sentences and utterances bring about as well as their coherence within a discourse context, including but not limited to"
W15-2712,J13-3003,0,0.107601,"rnardi∗ Gemma Boleda∗ Raquel Fern´andez† Denis Paperno∗ ∗ Center for Mind/Brain Sciences University of Trento † Institute for Logic, Language and Computation University of Amsterdam Abstract their unawareness of the unfolding discourse context. Standardly, distributional models are constructed from large amounts of data in batch mode by aggregating information into a vector that synthesises the general distributional meaning of an expression. Some of the recent distributional models account for contextual effects within the scope of a phrase or a sentence, (e.g., (Baroni and Zamparelli, 2010; Erk et al., 2013)), but they are not intended to capture how the meaning depends on the incrementally built discourse context where an expression is used. Since words and sentences are not used in isolation but are typically part of a discourse, the traditional distributional view is not sufficient. We argue that, to grow into an empirically adequate, full-fledged theory of meaning and interpretation, distributional models must evolve to provide meaning representations for actual language use in discourse and dialogue. Specifically, we discuss how the type of information they encode needs to be extended, and p"
W15-2712,P14-1132,0,0.0273218,"nd and coherence, it is critical to capture the discourse context-dependent and incremental nature of meaning. Here we sketch out a series of tasks related to some of the main phenomena we have discussed, against which new models could be evaluated. In Section 2.1 we have considered the need to interface conceptual meaning with referential meaning incrementally built up as a discourse unfolds. A good testbed for evaluating these aspects is offered by the recent development of crossmodal distributional semantic frameworks that are able to map between language and vision (Karpathy et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Current models have shown that images representing a concept can be retrieved by mapping a word vector into a visual space, and more recently image generation systems that create images from word vectors have also been introduced (Lazaridou et al., 2015a; Lazaridou et al., 2015b). These frameworks could be used to test whether an incrementally constructed, discoursecontextualised word vector is able to retrieve and generate different, more contextually appropriate images than its out-of-context vector counterpart. For instance, a vector for a phrase like red box in a co"
W15-2712,D11-1129,0,0.157329,"Missing"
W15-2712,N15-1016,0,0.289155,"idered the need to interface conceptual meaning with referential meaning incrementally built up as a discourse unfolds. A good testbed for evaluating these aspects is offered by the recent development of crossmodal distributional semantic frameworks that are able to map between language and vision (Karpathy et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Current models have shown that images representing a concept can be retrieved by mapping a word vector into a visual space, and more recently image generation systems that create images from word vectors have also been introduced (Lazaridou et al., 2015a; Lazaridou et al., 2015b). These frameworks could be used to test whether an incrementally constructed, discoursecontextualised word vector is able to retrieve and generate different, more contextually appropriate images than its out-of-context vector counterpart. For instance, a vector for a phrase like red box in a context where red refers to the box’ contents should be mapped to different types of images depending on whether it has been constructed by a pragmatically aware model or not. Such a dataset could be constructed by creating images of referents of the same phrase used in differen"
W15-2712,J86-3001,0,0.186101,"tributional models mainly capture the latter stable conventions. The challenge is thus to be able to also capture the former, discourse-dependent meaning. Moreover, even function words, which are notreferential and are usually considered to have a precise (logical) meaning, are subject to pragmatic effects. For instance, the meaning of the determiner some is typically taken to be that of an existential quantifier (i.e., there exists at least one object with certain properties). Yet, its ‘at least one’ meaning may be refined in particular discourse contexts, as shown in the following examples: Grosz and Sidner, 1986; Kamp and Reyle, 1993; Asher and Lascarides, 2003; Ginzburg, 2012), namely, that the meaning of an expression consists in its context-change potential, where context is incrementally built up as a discourse proceeds. We contend that a distributional semantics for language use should account for the discourse context-dependent, dynamic, and incremental nature of language. Generic semantic knowledge won’t suffice: one needs to encode somehow the discourse state or common ground, which will enable modeling discourse and dialogue coherence. In this section, we first look into examples that illust"
W15-2712,W14-4321,1,0.874434,"Missing"
W15-2712,D12-1110,0,0.447686,"should capture, and propose concrete tasks on which they could be tested. 1 Introduction Distributional semantics has revolutionised computational semantics by representing the meaning of linguistic expressions as vectors that capture their co-occurrence patterns in large corpora (Turney et al., 2010; Erk, 2012). This strategy has been shown to be very successful for modelling word meaning, and it has recently been expanded to capture the meaning of phrases and even sentences in a compositional fashion (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). Distributional semantic models are often presented as a robust alternative to representing meaning, compared to symbolic and logic-based approaches in formal semantics, thanks to their flexible representations and their data-driven nature. However, current models fail to account for aspects of meaning that are central in formal semantics, such as the relation between linguistic expressions and their referents or the truth conditions of sentences. In this position paper we focus on one of the main limitations of current distributional approaches, namely, 2 Meaning in Discourse As we just poin"
W15-2712,S14-2001,1,0.826578,"e elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena discussed above is certainly challenging. However, we believe that, given the many recent advances in the field, the distributional semantics community is ready to take up this challenge. We have argued that, in order to account for the dynamics of situated common ground and coherence, i"
W15-2712,D13-1170,0,0.00520688,"d Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena discussed above is certainly challenging. However, we believe that, given the many recent advances in the field, the distributional semantics community is ready to take up this challenge. We have argued that, in order to account for the dynamics of situated common ground and coherence, it is critical to capture the discourse context-dependent and incremental nature of meaning. Here we sketch out a series of tasks rela"
W15-2712,S07-1009,0,0.0220861,"fic word occurrence in context. These approaches offer a very valuable starting point, but their scope differs from ours. In particular, we can identify the following three main traditions: (1) Word Sense Disambiguation (Navigli, 2009, offers an overview), which aims to assign one of the predefined list of word senses to a given word, depending on the context. These are typically dictionary senses, and so do not capture semantic nuances that depend on the specific use of the word in a given discourse or dialogue context. (2) Word meaning in context as modeled in the lexical substitution task (McCarthy and Navigli, 2007; Erk et al., 2013), which predicts one or more paraphrases for a word in a given sentence. Unlike Word Sense Disambiguation, word meaning in context is specific to a given use of a word, that is, it doesn’t assume a pre-defined list of senses and can account for highly specific contextual effects. However, in this tradition context is restricted to one sentence, so the semantic phenomena modeled do not extend to discourse or dialogue. (3) Compositional distributional semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013), which predicts the meaning of a phrase"
W15-2712,N15-1020,0,0.0682292,"Missing"
W15-2712,D14-1079,0,0.0132662,"like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2.2, we have highlighted the context update potential of utterances as a feature that should be captured by compositional distributional models beyond the word/phrase level. Recent work has evaluated such models on dialogue act tagging tasks (Kalchbrenner and Blunsom, 2013; Milajevs et al., 2014). However, these approaches consider utterances in isolation and rely on a predefined set of dialogue act types that are to a large extent arbitrary, and in any case of a metalinguistic nature. Similar comments would apply to the task of identifying discourse relations connecting isolated pairs of sentences. Instead, we argue that pragmatically-aware distributional models should help us to induce dialogue acts in an unsupervised way and to model them as context update functions. Thus, we suggest to adopt tasks that target coherence and the evolution of common ground — which is what discourse r"
W15-2712,P14-1009,1,0.848888,"not all Distributional models have so far not been particularly successful in modelling the meaning of function words (but see Baroni et al. (2012); Bernardi et al. (2013); Hermann et al. (2013)). We believe that discourse-aware distributional semantics may fare better in this respect. We elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena"
W15-2712,D10-1115,0,\N,Missing
W15-2712,E12-1004,1,\N,Missing
W15-2712,P83-1007,0,\N,Missing
W15-2712,Q14-1017,0,\N,Missing
W15-2712,2014.lilt-9.5,1,\N,Missing
W15-2712,P13-2010,1,\N,Missing
W15-2712,W15-0120,0,\N,Missing
W16-3211,D13-1202,0,0.0303441,"e show that memory networks perform well in this task, and that explicit counting is not necessary to the system’s performance, supporting psycholinguistic evidence on the acquisition of quantifiers. 1 Introduction Multimodal representations of meaning have recently gained a lot of attention in the computational semantics literature. It has been shown, in particular, that the meaning of content words can be modelled in a cognitively – and even neuroscientifically – plausible way by learning representations from both the linguistic and visual contexts in which a lexical item has been observed (Anderson et al., 2013; Lazaridou et al., 2015). Such work has been crucial to advance the development of both a) a computational theory of meaning rooted in situated language use, as pursued by the field of Distributional Semantics (Clark, 2012; Erk, 2012) and b) vision-based applications such as image caption generation and visual question answering (Antol et al., 2015), going towards genuine image understanding. Both distributional semantics and visual applications, however, struggle with providing plausible representations for function words. This has theoretical and practical consequences. On the ∗ This projec"
W16-3211,S13-1001,0,0.235575,", current vision systems are forced to rely on background language models instead of truly interpreting the words of a query or caption in the given visual context. As a consequence, if e.g. the sentence I see some cats is more frequent than I see no cat, language modelbased applications will tend to generate the first even when the second would be more appropriate. In this paper, we start remedying this situation by investigating one important class of function words: natural language quantifiers (e.g. no, some, all). Quantifiers are an emerging field of research in distributional semantics (Grefenstette, 2013; Herbelot and Vecchi, 2015) and, so far, haven’t been studied in relation with visual data and grounding. We make a first step in this direction by asking whether the meaning of quantifier words can be learnt by observing their use in the presence of visual information. We observe that in grounded contexts, children learn to make quantification estimates before being able to count (Feigenson et al., 2004; Mazzocco et al., 2011), using their Approximate Number Sense (ANS). We ask whether Neural Networks (NNs) can model this ability, and we evaluate several neural network models, with and witho"
W16-3211,D15-1003,1,0.759671,"tems are forced to rely on background language models instead of truly interpreting the words of a query or caption in the given visual context. As a consequence, if e.g. the sentence I see some cats is more frequent than I see no cat, language modelbased applications will tend to generate the first even when the second would be more appropriate. In this paper, we start remedying this situation by investigating one important class of function words: natural language quantifiers (e.g. no, some, all). Quantifiers are an emerging field of research in distributional semantics (Grefenstette, 2013; Herbelot and Vecchi, 2015) and, so far, haven’t been studied in relation with visual data and grounding. We make a first step in this direction by asking whether the meaning of quantifier words can be learnt by observing their use in the presence of visual information. We observe that in grounded contexts, children learn to make quantification estimates before being able to count (Feigenson et al., 2004; Mazzocco et al., 2011), using their Approximate Number Sense (ANS). We ask whether Neural Networks (NNs) can model this ability, and we evaluate several neural network models, with and without numerical processing abil"
W17-6904,P14-1023,1,0.760312,"Missing"
W17-6904,J10-4006,1,0.56304,"ion. Dataset. We have constructed a dataset for the task containing 40k sequences for training, 5k for validation and 10k for testing.1 It is assembled on the basis of 2k object categories with 50 ImageNet2 images each, sampled from a larger dataset (Lazaridou et al., 2015). These are natural images, which makes the task challenging. The object categories given in the queries are those specified in ImageNet. We build a set of linguistic attributes for each object by first extracting the 500 most associated, and thus plausible, syntactic neighbors for the category according to the DM resource (Baroni and Lenci, 2010). This excludes nonsensical combinations such as repair:dog. We further retain only (relatively) abstract verbs taking the target item as direct object.3 This is because (a) concrete verbs are likely to have strong visual correlates that could conflict with the image (cf. walk dog); and (b) referential expressions routinely successfully mix concrete and abstract cues (e.g., the dog I own). We remove all verbs with a score over 2.5 (on a 1–5 scale) in the concreteness norms of Brysbaert et al. (2014). We then construct each sequence as follows. First, we sample two random categories, and three"
W17-6904,J16-4002,1,0.840649,"that are already in the library. Our model shows promise: it beats traditional neural network architectures on the task. However, it is still outperformed by Memory Networks, another model with external memory. 1 Introduction Language combines discrete and continuous facets, as exemplified by the phenomenon of reference (Frege, 1892; Abbott, 2010): When we refer to an object in the world with the noun phrase the mug I bought, we use content words such as mug, which are notoriously fuzzy or vague in their meaning (Van Deemter, 2012; Murphy, 2002) and are best modeled through continuous means (Boleda and Herbelot, 2016). Once the referent for the mug has been established, however, it becomes a linguistic entity that we can manipulate in a largely discrete fashion, retrieving it and updating it with new information as needed (Remember the mug I bought? My brother stole it! Kamp and Reyle, 1993). Put differently, managing reference requires two distinct abilities: 1. The ability to categorize, that is, to recognize that different entities are equivalent with regard to some concept of interest (e.g. two mugs, two instances of the “things to take on a camping trip” category; Barsalou, 1983). This implies being a"
W17-6904,W08-2222,0,0.0549761,"wo referents), or to treat it as a new referent. Our second contribution is a neural network architecture with a module for referent representations: DIstributed model of REference, DIRE. DIRE uses the concept of external memory from deep learning (Joulin and Mikolov, 2015; Graves et al., 2016) to build an entity library for an exposure sequence that conceptually corresponds to the set of DRT discourse referents, using similarity-based reasoning on distributed representations to decide between aggregating and initializing entity representations. In contrast to symbolic implementations of DRT (Bos, 2008), which manipulate discourse referents on the basis of manually specified algorithms, DIRE learns to make these decisions directly from observing reference acts using end-to-end training. We see our paper as a first, modest step in the direction of data-driven learning of DRT-like behavior, and are of course still far from learning anything resembling a fully fledged DRT system. 2 Cross-modal Entity Tracking: Task and Data Task. Imagine an office, with a desk where there are three mugs and other objects. Adam tells Barbara that he just bought two of the mugs and he particularly likes the one o"
W17-6904,P12-1015,1,0.803766,"lities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 2020 programme under the Marie Skłodowska-Curie grant agreement No 655577 (LOVe"
W17-6904,W15-0120,0,0.0134861,"ented a new task, cross-modal entity tracking, that tests the categorization and individuation capabilities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 7"
W17-6904,D15-1003,0,0.0188569,", cross-modal entity tracking, that tests the categorization and individuation capabilities of computational models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 20"
W17-6904,J12-1006,0,0.0559812,"Missing"
W17-6904,N15-1016,1,0.845774,"ecent survey), but focuses on identifying language-external objects from images rather than mentions of a referent in text; to Visual Question Answering (Antol et al., 2015), but it cannot be solved with visual information alone; and to Referring Expression Generation (Krahmer and Van Deemter, 2012), but involves identification rather than generation. Dataset. We have constructed a dataset for the task containing 40k sequences for training, 5k for validation and 10k for testing.1 It is assembled on the basis of 2k object categories with 50 ImageNet2 images each, sampled from a larger dataset (Lazaridou et al., 2015). These are natural images, which makes the task challenging. The object categories given in the queries are those specified in ImageNet. We build a set of linguistic attributes for each object by first extracting the 500 most associated, and thus plausible, syntactic neighbors for the category according to the DM resource (Baroni and Lenci, 2010). This excludes nonsensical combinations such as repair:dog. We further retain only (relatively) abstract verbs taking the target item as direct object.3 This is because (a) concrete verbs are likely to have strong visual correlates that could conflic"
W17-6904,P13-1056,0,0.0187395,"nal models, and a challenging dataset for the task. Our project is related to several areas of active research. Reference is a classic topic in philosophy of language and linguistics (Frege, 1892; Abbott, 2010; Kamp and Reyle, 1993; Kamp, 2015); emulating discrete aspects of language and reasoning through continuous means is a long-standing goal in artificial intelligence (Smolensky, 1990; Joulin and Mikolov, 2015), and recent work focuses on reference (Baroni et al., 2017; Herbelot, 2015; Herbelot and Vecchi, 2015); grounding language in perception (Chen and Mooney, 2011; Bruni et al., 2012; Silberer et al., 2013), as well as reference and co-reference (Krahmer and Van Deemter, 2012; Poesio et al., 2017) are important subjects in Computational Linguistics. Our programme puts these different strands together. Acknowledgments: We thank Angeliki Lazaridou for help producing the visual vectors used in the paper. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 715154; AMORE); EU Horizon 2020 programme under the Marie Skłodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Ind"
W19-0410,J16-4005,0,0.0555558,"Missing"
W19-0410,2014.lilt-9.5,0,0.0383399,"2017) or audio (e.g., Lopopolo and Miltenburg 2015) – in principle any place where one may encounter a word could be used. Because of how distributional models work, words that appear in similar contexts end up being assigned similar representations. At present, all models need large amounts of data to compute high-quality representations. The closer these data resemble our experience as language learners, the more distributional semantics is expected to be able in principle to generate accurate representations of – as we will argue – expression meaning. As for the abstraction mechanism used, Baroni et al. (2014) distinguish between classic “countbased” methods, which work with co-occurrence statistics between words and contexts, and “predictionbased” methods, which instead apply machine learning techniques (artificial neural networks) to induce representations based on a prediction task, typically predicting the context given a word. For instance, the Skip-Gram model of Mikolov et al. (2013) would, applied to example (1), try to predict the words “the”, “red”, “is”, “chasing”, etc. from the presence of the word “cat” (more precisely, it would try to make these context words more likely than randomly"
W19-0410,P14-1023,0,0.390715,"2017) or audio (e.g., Lopopolo and Miltenburg 2015) – in principle any place where one may encounter a word could be used. Because of how distributional models work, words that appear in similar contexts end up being assigned similar representations. At present, all models need large amounts of data to compute high-quality representations. The closer these data resemble our experience as language learners, the more distributional semantics is expected to be able in principle to generate accurate representations of – as we will argue – expression meaning. As for the abstraction mechanism used, Baroni et al. (2014) distinguish between classic “countbased” methods, which work with co-occurrence statistics between words and contexts, and “predictionbased” methods, which instead apply machine learning techniques (artificial neural networks) to induce representations based on a prediction task, typically predicting the context given a word. For instance, the Skip-Gram model of Mikolov et al. (2013) would, applied to example (1), try to predict the words “the”, “red”, “is”, “chasing”, etc. from the presence of the word “cat” (more precisely, it would try to make these context words more likely than randomly"
W19-0410,J10-4006,0,0.201249,"nal semantics on its own is an adequate model of expression meaning. Our proposal sheds light on the role of distributional semantics in a broader theory of language and cognition, its relationship to formal semantics, and its place in computational models. Keywords: distributional semantics, expression meaning, formal semantics, speaker meaning, truth conditions, entailment, reference, compositionality, context 1 Introduction Distributional semantics has emerged as a promising model of certain ‘conceptual’ aspects of linguistic meaning (e.g., Landauer and Dumais 1997; Turney and Pantel 2010; Baroni and Lenci 2010; Lenci 2018) and as an indispensable component of applications in Natural Language Processing (e.g., reference resolution, machine translation, image captioning; especially since Mikolov et al. 2013). Yet its theoretical status within a general theory of meaning and of language and cognition more generally is not clear (e.g., Lenci 2008; Erk 2010; Boleda and Herbelot 2016; Lenci 2018). In particular, it is not clear whether distributional semantics can be understood as an actual model of expression meaning – what Lenci (2008) calls the ‘strong’ view of distributional semantics – or merely as"
W19-0410,S13-1002,1,0.387881,"the family as a whole, defined by the core tenet of associating with each word an abstraction over its use, is highly suitable in principle for modeling expression meaning. This makes the ‘strong’ view of distributional semantics attractive. An alternative to the ‘strong’ view is what Lenci (2008) calls the ‘weak’ view: that an abstraction over use may be part of what determines expression meaning, but that more is needed. This view underlies for instance the common assumption that a more complete model of expression meaning would require integrating distributional and formal semantics (e.g., Beltagy et al. 2013; Erk 2013; Baroni et al. 2014; Asher et al. 2016; Boleda and Herbelot 2016). But in section 4 we argue that the notions of formal semantic, like reference, truth conditions and entailment, do not belong at the level of expression meaning in the first place, and, accordingly, that distributional semantics can be sufficient as a model of expression meaning. Theoretical parsimony dictates that we opt for the least presumptive approach compatible with the empirical facts, i.e., with what a theory of expression meaning should account for. Some authors equate the meaning of an expression not with a"
W19-0410,J16-4002,1,0.692296,"lment, reference, compositionality, context 1 Introduction Distributional semantics has emerged as a promising model of certain ‘conceptual’ aspects of linguistic meaning (e.g., Landauer and Dumais 1997; Turney and Pantel 2010; Baroni and Lenci 2010; Lenci 2018) and as an indispensable component of applications in Natural Language Processing (e.g., reference resolution, machine translation, image captioning; especially since Mikolov et al. 2013). Yet its theoretical status within a general theory of meaning and of language and cognition more generally is not clear (e.g., Lenci 2008; Erk 2010; Boleda and Herbelot 2016; Lenci 2018). In particular, it is not clear whether distributional semantics can be understood as an actual model of expression meaning – what Lenci (2008) calls the ‘strong’ view of distributional semantics – or merely as a model of something that correlates with expression meaning in certain partial ways – the ‘weak’ view. In this paper we aim to resolve, in favor of the ‘strong’ view, the question of what exactly distributional semantics models, what its role should be in an overall theory of language and cognition, and how its contribution to state of the art applications can be understo"
W19-0410,W10-2803,0,0.463706,"ons, entailment, reference, compositionality, context 1 Introduction Distributional semantics has emerged as a promising model of certain ‘conceptual’ aspects of linguistic meaning (e.g., Landauer and Dumais 1997; Turney and Pantel 2010; Baroni and Lenci 2010; Lenci 2018) and as an indispensable component of applications in Natural Language Processing (e.g., reference resolution, machine translation, image captioning; especially since Mikolov et al. 2013). Yet its theoretical status within a general theory of meaning and of language and cognition more generally is not clear (e.g., Lenci 2008; Erk 2010; Boleda and Herbelot 2016; Lenci 2018). In particular, it is not clear whether distributional semantics can be understood as an actual model of expression meaning – what Lenci (2008) calls the ‘strong’ view of distributional semantics – or merely as a model of something that correlates with expression meaning in certain partial ways – the ‘weak’ view. In this paper we aim to resolve, in favor of the ‘strong’ view, the question of what exactly distributional semantics models, what its role should be in an overall theory of language and cognition, and how its contribution to state of the art ap"
W19-0410,W13-0109,0,0.353568,", defined by the core tenet of associating with each word an abstraction over its use, is highly suitable in principle for modeling expression meaning. This makes the ‘strong’ view of distributional semantics attractive. An alternative to the ‘strong’ view is what Lenci (2008) calls the ‘weak’ view: that an abstraction over use may be part of what determines expression meaning, but that more is needed. This view underlies for instance the common assumption that a more complete model of expression meaning would require integrating distributional and formal semantics (e.g., Beltagy et al. 2013; Erk 2013; Baroni et al. 2014; Asher et al. 2016; Boleda and Herbelot 2016). But in section 4 we argue that the notions of formal semantic, like reference, truth conditions and entailment, do not belong at the level of expression meaning in the first place, and, accordingly, that distributional semantics can be sufficient as a model of expression meaning. Theoretical parsimony dictates that we opt for the least presumptive approach compatible with the empirical facts, i.e., with what a theory of expression meaning should account for. Some authors equate the meaning of an expression not with an abstract"
W19-0410,N10-1011,0,0.0412971,"emantics on its own, i.e., a set of word vectors, combined perhaps with some basic algebraic operations or, at most, a simple classifier. By contrast, when distributional semantics is incorporated in a larger model (see section 2) the resulting system as a whole can be very successful. rences of words in contexts. Implementations of distributional semantics vary, primarily, in the notion of context and in the abstraction mechanism used. A context for a word is typically a text in which it occurs, such as a document, sentence or a set of neighboring words, but it can also contain images (e.g., Feng and Lapata 2010; Silberer et al. 2017) or audio (e.g., Lopopolo and Miltenburg 2015) – in principle any place where one may encounter a word could be used. Because of how distributional models work, words that appear in similar contexts end up being assigned similar representations. At present, all models need large amounts of data to compute high-quality representations. The closer these data resemble our experience as language learners, the more distributional semantics is expected to be able in principle to generate accurate representations of – as we will argue – expression meaning. As for the abstractio"
W19-0410,P05-1014,0,0.0546296,"en T HE W ORD C AT and C AT, or between linguistic and extralinguistic concepts, is not new, and word vectors are known to capture the more linguistic kind of information, and to be (at best) only a proxy for the extralinguistic concepts they are typically used to denote by a speaker (e.g., Miller and Charles 1991). But it appears to be sometimes overlooked. For instance, the assumption that the word vector for “cat” would (or should) model the extralinguistic concept C AT is made in work using distributional semantics to model entailment, e.g., that being a cat entails being an animal (e.g., Geffet and Dagan 2005; Roller et al. 2014; Vuli´c and Mrkˇsi´c 2017). But clearly the entailment relation holds between the extralinguistic concepts C AT and A NIMAL – being a cat entails being an animal – not between the linguistic concepts T HE W ORD C AT and T HE W ORDA NIMAL actually modeled by distributional semantics: being the word “cat” does not entail (in fact, it excludes) being the word “animal”. Hence these approaches are, strictly speaking, theoretically misguided – although their conflation of linguistic and extralinguistic concepts may be a defensible simplification for practical purposes. There hav"
W19-0410,W13-0112,0,0.0268031,"butional semantics works quite well for this type of effect in the composition of content words (e.g., Baroni et al. 2014; McNally and Boleda 2017), an area where formal semantics, which tends to leave the basic concepts unanalyzed, has struggled (despite efforts such as Pustejovsky 1995). Classic compositional distributional semantics, in which distributional representations are combined with some externally specified algorithm (which can be as simple as addition), also works reasonably well for short sentences, as measured for instance on sentence similarity (e.g., Mitchell and Lapata 2010; Grefenstette et al. 2013; Marelli et al. 2014). But for longer expressions distributional semantics on its own falls short (cf. our clarification of “on its own” in footnote 3), and this is part of what has inspired aforementioned works on integrating formal and distributional semantics (e.g., Coecke et al. 2011; Grefenstette and Sadrzadeh 2011; Beltagy et al. 2013; Erk 2013; Baroni et al. 2014; Asher et al. 2016). However, that distributional semantics falls short of accounting for full-fledged compositionality does not mean that it cannot be a sufficient model of expression meaning. For that, it should be establish"
W19-0410,D11-1129,0,0.023871,"stributional semantics, in which distributional representations are combined with some externally specified algorithm (which can be as simple as addition), also works reasonably well for short sentences, as measured for instance on sentence similarity (e.g., Mitchell and Lapata 2010; Grefenstette et al. 2013; Marelli et al. 2014). But for longer expressions distributional semantics on its own falls short (cf. our clarification of “on its own” in footnote 3), and this is part of what has inspired aforementioned works on integrating formal and distributional semantics (e.g., Coecke et al. 2011; Grefenstette and Sadrzadeh 2011; Beltagy et al. 2013; Erk 2013; Baroni et al. 2014; Asher et al. 2016). However, that distributional semantics falls short of accounting for full-fledged compositionality does not mean that it cannot be a sufficient model of expression meaning. For that, it should be established first that compositionality wholly resides at the level of expression meaning – and it is not clear that it does. Let us take a closer look at the main theoretical argument for compositionality, the argument from productivity.16 According to this argument, compositionality is necessary to explain how a competent speak"
W19-0410,D14-1086,0,0.0452063,"ces about) what it may be used by a given speaker to refer to. (To clarify: this does not imply that the actual or potential referents of a word are actually part of its meaning – see Section 4.) The same holds for the distributional semantic word vector for “cat”, although instantiations of distributional semantics may differ in how much referentially relevant information they encode. Presumably, more information of this sort is encoded when reference is prominent in the original data, for instance when a distributional semantic model is trained on referential expressions grounded in images (Kazemzadeh et al., 2014); otherwise such information needs to be induced from patterns in the text alone (like any other semantic information in text-only distributional semantics). the set of all cat-like things, C HASE a set of pairs where one chases the other, the variable x would be bound to a particular entity in the world, etc., and the logical connectives can have their usual truthconditional interpretation.10 In this way formal semantics accounts for reference to things in the world and it accounts for truth values (which is what sentences refer to; Frege 1892). Moreover, referents and truth values across pos"
W19-0410,W15-0110,0,0.0191632,"erhaps with some basic algebraic operations or, at most, a simple classifier. By contrast, when distributional semantics is incorporated in a larger model (see section 2) the resulting system as a whole can be very successful. rences of words in contexts. Implementations of distributional semantics vary, primarily, in the notion of context and in the abstraction mechanism used. A context for a word is typically a text in which it occurs, such as a document, sentence or a set of neighboring words, but it can also contain images (e.g., Feng and Lapata 2010; Silberer et al. 2017) or audio (e.g., Lopopolo and Miltenburg 2015) – in principle any place where one may encounter a word could be used. Because of how distributional models work, words that appear in similar contexts end up being assigned similar representations. At present, all models need large amounts of data to compute high-quality representations. The closer these data resemble our experience as language learners, the more distributional semantics is expected to be able in principle to generate accurate representations of – as we will argue – expression meaning. As for the abstraction mechanism used, Baroni et al. (2014) distinguish between classic “c"
W19-0410,marelli-etal-2014-sick,0,0.0511361,"uite well for this type of effect in the composition of content words (e.g., Baroni et al. 2014; McNally and Boleda 2017), an area where formal semantics, which tends to leave the basic concepts unanalyzed, has struggled (despite efforts such as Pustejovsky 1995). Classic compositional distributional semantics, in which distributional representations are combined with some externally specified algorithm (which can be as simple as addition), also works reasonably well for short sentences, as measured for instance on sentence similarity (e.g., Mitchell and Lapata 2010; Grefenstette et al. 2013; Marelli et al. 2014). But for longer expressions distributional semantics on its own falls short (cf. our clarification of “on its own” in footnote 3), and this is part of what has inspired aforementioned works on integrating formal and distributional semantics (e.g., Coecke et al. 2011; Grefenstette and Sadrzadeh 2011; Beltagy et al. 2013; Erk 2013; Baroni et al. 2014; Asher et al. 2016). However, that distributional semantics falls short of accounting for full-fledged compositionality does not mean that it cannot be a sufficient model of expression meaning. For that, it should be established first that composit"
W19-0410,N13-1090,0,0.490656,"formal semantics, and its place in computational models. Keywords: distributional semantics, expression meaning, formal semantics, speaker meaning, truth conditions, entailment, reference, compositionality, context 1 Introduction Distributional semantics has emerged as a promising model of certain ‘conceptual’ aspects of linguistic meaning (e.g., Landauer and Dumais 1997; Turney and Pantel 2010; Baroni and Lenci 2010; Lenci 2018) and as an indispensable component of applications in Natural Language Processing (e.g., reference resolution, machine translation, image captioning; especially since Mikolov et al. 2013). Yet its theoretical status within a general theory of meaning and of language and cognition more generally is not clear (e.g., Lenci 2008; Erk 2010; Boleda and Herbelot 2016; Lenci 2018). In particular, it is not clear whether distributional semantics can be understood as an actual model of expression meaning – what Lenci (2008) calls the ‘strong’ view of distributional semantics – or merely as a model of something that correlates with expression meaning in certain partial ways – the ‘weak’ view. In this paper we aim to resolve, in favor of the ‘strong’ view, the question of what exactly dis"
W19-0410,N18-1202,0,0.0331696,"r vector representations.4 Our arguments in this paper apply to both kinds of methods for distributional semantics. Word embeddings emerge not just from models that are expressly designed to yield word representations (such as Mikolov et al. 2013). Rather, any neural network model that takes words as input, trained on whatever task, must ‘embed’ these words in order to process them – hence any such model will result in word embeddings (e.g., Collobert and Weston 2008). Neural network models for language are trained for instance on language modeling (e.g., word prediction; Mikolov et al. 2010; Peters et al. 2018) or Machine Translation (Bahdanau et al., 2015). As long as the data on which these models are trained consist of word-context pairs, the resulting word embeddings qualify, for present purposes, as implementations of distributional semantics, and our proposal in the current paper applies also to them. Of course some implementations within this broad family may be better than others, and the type of task used is one parameter to be explored: It is expected that the more the task requires a human-like understanding of language, the better the resulting word embeddings will represent – as we will"
W19-0410,C14-1097,1,0.83108,"C AT, or between linguistic and extralinguistic concepts, is not new, and word vectors are known to capture the more linguistic kind of information, and to be (at best) only a proxy for the extralinguistic concepts they are typically used to denote by a speaker (e.g., Miller and Charles 1991). But it appears to be sometimes overlooked. For instance, the assumption that the word vector for “cat” would (or should) model the extralinguistic concept C AT is made in work using distributional semantics to model entailment, e.g., that being a cat entails being an animal (e.g., Geffet and Dagan 2005; Roller et al. 2014; Vuli´c and Mrkˇsi´c 2017). But clearly the entailment relation holds between the extralinguistic concepts C AT and A NIMAL – being a cat entails being an animal – not between the linguistic concepts T HE W ORD C AT and T HE W ORDA NIMAL actually modeled by distributional semantics: being the word “cat” does not entail (in fact, it excludes) being the word “animal”. Hence these approaches are, strictly speaking, theoretically misguided – although their conflation of linguistic and extralinguistic concepts may be a defensible simplification for practical purposes. There have been many proposal"
W19-0410,N18-1103,0,\N,Missing
