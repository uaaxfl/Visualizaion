2020.acl-main.278,N13-1073,0,0.179842,"Missing"
2020.acl-main.278,P16-1162,0,0.225361,"tion of translation errors. While TER only labels the mis-translation (“S”) and over-translation (“I”) errors, we describe a simple heuristic method to annotate the undertranslation error by mapping the label “D” from the ground-truth sequence to the generated sequence. 4 Miscalibration in NMT Data and Setup We carried out experiments on three different language pairs, including WAT17 English-Japanese (En-Jp), WMT14 EnglishGerman (En-De), and WMT17 Chinese-English (Zh-En). The training datasets consist of 1.9M, 4.5M, and 20.6M sentence pairs respectively. We employed Byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the three language pairs. We used BLEU (Papineni et al., 2001) to evaluate the NMT models. We used the TER toolkit (Snover et al., 2006) to label whether the tokens in NMT outputs are correctly translated. Normalization was not used, and the maximum shift distance was set to 50. The NMT model that we used in our experiments is Transformer (Vaswani et al., 2017). We used base model as default, which consists of a 6-layer encoder and a 6-layer decoder and the hidden size is 512. The model parameters are optimized by Adam (Kingma and Ba, 2015), with β1 = 0.9, β2"
2020.acl-main.278,2006.amta-papers.25,0,0.502853,"019) and Kumar and Sarawagi (2019) studied the calibration of NMT in the training setting, and found that NMT trained with label smoothing (Szegedy et al., 2016) is well-calibrated. We believe that this setting would cover up a central problem of NMT, the exposure bias (Ranzato et al., 2015) – the training-inference discrepancy caused by teacher forcing in the training of auto-regressive models. In response to this problem, this work focuses on the calibration of NMT in inference, which can better reflect the generative capacity of NMT models. To this end, we use translation error rate (TER) (Snover et al., 2006) to automatically annotate the correctness of generated tokens, which makes it feasible to evaluate calibration in infer3070 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3070–3079 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ence. Experimental results on several datasets across language pairs show that even trained with label smoothing, NMT models still suffer from miscalibration errors in inference. Figure 1 shows an example. While modern neural networks on classification tasks have been found to be miscalibrated in the"
2020.acl-main.278,D15-1182,0,0.06029,"Missing"
2020.acl-main.278,2001.mtsummit-papers.68,0,0.0152736,"(“I”) errors, we describe a simple heuristic method to annotate the undertranslation error by mapping the label “D” from the ground-truth sequence to the generated sequence. 4 Miscalibration in NMT Data and Setup We carried out experiments on three different language pairs, including WAT17 English-Japanese (En-Jp), WMT14 EnglishGerman (En-De), and WMT17 Chinese-English (Zh-En). The training datasets consist of 1.9M, 4.5M, and 20.6M sentence pairs respectively. We employed Byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the three language pairs. We used BLEU (Papineni et al., 2001) to evaluate the NMT models. We used the TER toolkit (Snover et al., 2006) to label whether the tokens in NMT outputs are correctly translated. Normalization was not used, and the maximum shift distance was set to 50. The NMT model that we used in our experiments is Transformer (Vaswani et al., 2017). We used base model as default, which consists of a 6-layer encoder and a 6-layer decoder and the hidden size is 512. The model parameters are optimized by Adam (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.98 and  = 10−9 . We used the same warmup strategy for learning rate as Vaswani et al. (2017"
2020.acl-main.278,P19-1176,0,0.132887,"bration. Szegedy et al. (2016) propose the label smoothing technique which can effectively reduce the calibration error. Ding et al. (2019) extend label smoothing to adaptive label regularization. Calibration on Structured Prediction Different from classification tasks, most natural language processing (NLP) tasks deal with complex structures (Kuleshov and Liang, 2015). Nguyen and O’Connor (2015) verified the finding of NiculescuMizil and Caruana (2005) in NLP tasks on loglinear structured models. For NMT, some works directed their attention to the uncertainty in prediction (Ott et al., 2018; Wang et al., 2019), Kumar and Sarawagi (2019) studied the calibration of several NMT models and found that the end of a sentence is severely miscalibrated. M¨uller et al. (2019) investigated the effect of label smoothing, finding that NMT models are well-calibrated in training. Different from previous works, we are interested in the calibration of NMT models in inference, given that the training and inference are discrepant for standard NMT models (Vaswani et al., 2017). 3 3.1 Definitions of Calibration Neural Machine Translation Training In machine translation task, an NMT model F : x → y maximizes the probabi"
2020.acl-main.278,D19-1073,1,0.935975,"bration. Szegedy et al. (2016) propose the label smoothing technique which can effectively reduce the calibration error. Ding et al. (2019) extend label smoothing to adaptive label regularization. Calibration on Structured Prediction Different from classification tasks, most natural language processing (NLP) tasks deal with complex structures (Kuleshov and Liang, 2015). Nguyen and O’Connor (2015) verified the finding of NiculescuMizil and Caruana (2005) in NLP tasks on loglinear structured models. For NMT, some works directed their attention to the uncertainty in prediction (Ott et al., 2018; Wang et al., 2019), Kumar and Sarawagi (2019) studied the calibration of several NMT models and found that the end of a sentence is severely miscalibrated. M¨uller et al. (2019) investigated the effect of label smoothing, finding that NMT models are well-calibrated in training. Different from previous works, we are interested in the calibration of NMT models in inference, given that the training and inference are discrepant for standard NMT models (Vaswani et al., 2017). 3 3.1 Definitions of Calibration Neural Machine Translation Training In machine translation task, an NMT model F : x → y maximizes the probabi"
2020.acl-main.278,D18-1396,0,0.0500661,"Missing"
2020.acl-main.278,P02-1040,0,\N,Missing
2020.acl-main.35,D16-1011,0,\N,Missing
2020.acl-main.35,D16-1249,0,\N,Missing
2020.acl-main.35,P17-1106,0,\N,Missing
2020.acl-main.35,N19-4009,0,\N,Missing
2020.acl-main.35,P19-1124,1,\N,Missing
2020.acl-main.35,W19-5201,0,\N,Missing
2020.acl-main.35,W19-4805,0,\N,Missing
2020.acl-main.35,N16-1082,0,\N,Missing
2020.acl-main.68,P19-1285,0,0.0473343,"Missing"
2020.acl-main.68,N19-1423,0,0.19865,"i/Shakespeare%27s sonnets we propose a simple and elegant framework named SongNet to address this challenging problem. The backbone of the framework is a Transformer-based auto-regressive language model. Considering the three folds characteristics mentioned above, we introduce sets of tailor-designed indicating symbols to improve the modeling performance, especially for the robustness of the format, rhyme, as well as sentence integrity. We improve the attention mechanism to impel the model to capture the future information on the format to further enhance sentence integrity. Inspired by BERT (Devlin et al., 2019) and GPT (Radford et al., 2018, 2019), a pretraining and fine-tuning framework is designed to further improve the generation quality. To verify the performance of our framework, we collect two corpora, SongCi and Sonnet, in Chinese and English respectively. Extensive experiments on the collected datasets demonstrate that our proposed framework can generate satisfying results in terms of both the tailor-designed automatic metrics including format accuracy, rhyming accuracy, sentence integrity, as well as the human evaluation results on relevance, fluency, and style. In summary, our contribution"
2020.acl-main.68,P18-1082,0,0.063576,"strict rhyming schemes and the rhyming words are labeled in red color and italic font. 2014; Gehring et al., 2017), Transformer and its variants (Vaswani et al., 2017; Dai et al., 2019), pre-trained auto-regressive language models such as XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), etc. Performance has been improved significantly in lots of tasks such as machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), dialogue systems (Vinyals and Le, 2015; Shang et al., 2015; Li, 2020), text summarization (Rush et al., 2015; Li et al., 2017; See et al., 2017), story telling (Fan et al., 2018; See et al., 2019), poetry writing (Zhang and Lapata, 2014; Lau et al., 2018; Liao et al., 2019), etc. Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) based sequence-tosequence (seq2seq) frameworks (Bahdanau et al., Code: http://github.com/lipiji/SongNet ۞ޗжަ▲Ѡ৩吷⽔澞னۨ䀲䬦ҁظՑ߄ݻײ澞 Or bends with the remover to remove. Introduction 1 橔ࠉލיଆㅓㅫޗڐП澞ٝީ㨡ࣞਘ㗘ੋչ澞 Let me not to the marriage of true minds Sonnet 1 SongCi"
2020.acl-main.68,D15-1044,0,0.0502208,"align with the tones of the notation. In SongCi and Sonnet, there are strict rhyming schemes and the rhyming words are labeled in red color and italic font. 2014; Gehring et al., 2017), Transformer and its variants (Vaswani et al., 2017; Dai et al., 2019), pre-trained auto-regressive language models such as XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), etc. Performance has been improved significantly in lots of tasks such as machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), dialogue systems (Vinyals and Le, 2015; Shang et al., 2015; Li, 2020), text summarization (Rush et al., 2015; Li et al., 2017; See et al., 2017), story telling (Fan et al., 2018; See et al., 2019), poetry writing (Zhang and Lapata, 2014; Lau et al., 2018; Liao et al., 2019), etc. Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) based sequence-tosequence (seq2seq) frameworks (Bahdanau et al., Code: http://github.com/lipiji/SongNet ۞ޗжަ▲Ѡ৩吷⽔澞னۨ䀲䬦ҁظՑ߄ݻײ澞 Or bends with the remover to remove. Introduction 1 橔ࠉލיଆㅓㅫޗڐП澞ٝީ"
2020.acl-main.68,P17-1099,0,0.03848,". In SongCi and Sonnet, there are strict rhyming schemes and the rhyming words are labeled in red color and italic font. 2014; Gehring et al., 2017), Transformer and its variants (Vaswani et al., 2017; Dai et al., 2019), pre-trained auto-regressive language models such as XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), etc. Performance has been improved significantly in lots of tasks such as machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), dialogue systems (Vinyals and Le, 2015; Shang et al., 2015; Li, 2020), text summarization (Rush et al., 2015; Li et al., 2017; See et al., 2017), story telling (Fan et al., 2018; See et al., 2019), poetry writing (Zhang and Lapata, 2014; Lau et al., 2018; Liao et al., 2019), etc. Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) based sequence-tosequence (seq2seq) frameworks (Bahdanau et al., Code: http://github.com/lipiji/SongNet ۞ޗжަ▲Ѡ৩吷⽔澞னۨ䀲䬦ҁظՑ߄ݻײ澞 Or bends with the remover to remove. Introduction 1 橔ࠉލיଆㅓㅫޗڐП澞ٝީ㨡ࣞਘ㗘ੋչ澞 Let me not to the marri"
2020.acl-main.68,K19-1079,0,0.0181914,"emes and the rhyming words are labeled in red color and italic font. 2014; Gehring et al., 2017), Transformer and its variants (Vaswani et al., 2017; Dai et al., 2019), pre-trained auto-regressive language models such as XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), etc. Performance has been improved significantly in lots of tasks such as machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), dialogue systems (Vinyals and Le, 2015; Shang et al., 2015; Li, 2020), text summarization (Rush et al., 2015; Li et al., 2017; See et al., 2017), story telling (Fan et al., 2018; See et al., 2019), poetry writing (Zhang and Lapata, 2014; Lau et al., 2018; Liao et al., 2019), etc. Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) based sequence-tosequence (seq2seq) frameworks (Bahdanau et al., Code: http://github.com/lipiji/SongNet ۞ޗжަ▲Ѡ৩吷⽔澞னۨ䀲䬦ҁظՑ߄ݻײ澞 Or bends with the remover to remove. Introduction 1 橔ࠉލיଆㅓㅫޗڐП澞ٝީ㨡ࣞਘ㗘ੋչ澞 Let me not to the marriage of true minds Sonnet 1 SongCi Neural text genera"
2020.acl-main.68,koen-2004-pharaoh,0,0.24808,"(yt |y<t ) (10) t=1 3.3 4.1 4.2 C 0 = {c0 , c0 , c0 , love, c1 , h/si bends, c0 , c0 , c0 , c0 , remove, c1 , h/si, heosi} where “love”, “bends” and “remove” are kept in the format C 0 . After the pre-training stage, we can conduct the fine-tuning procedure directly on the target corpus without adjusting any model structure. We can assign any format and rhyming symbols C to control the generation. Given C, we will obtain P and S automatically. And the model can conduct generation starting from the special token hbosi iteratively until meet the ending marker heosi. Both beam-search algorithm (Koehn, 2004) and truncated top-k sampling (Fan et al., 2018; Radford et al., 2019) method are utilized to conduct the decoding. Settings Datasets We conduct all the experiments on two collected corpus with different literary genres: SongCi and Sonnet, in Chinese and English respectively. The statistic number are shown in Table 3. We can see that Sonnet is in small size since we only utilize the samples from the Shakespeare’s Sonnets (Shakespeare, 2000). Since SongCi and Sonnet are in different languages, thus we conduct the pre-training procedure on two large scale corpus in the corresponding languages re"
2020.acl-main.68,P18-1181,0,0.014529,"lic font. 2014; Gehring et al., 2017), Transformer and its variants (Vaswani et al., 2017; Dai et al., 2019), pre-trained auto-regressive language models such as XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), etc. Performance has been improved significantly in lots of tasks such as machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), dialogue systems (Vinyals and Le, 2015; Shang et al., 2015; Li, 2020), text summarization (Rush et al., 2015; Li et al., 2017; See et al., 2017), story telling (Fan et al., 2018; See et al., 2019), poetry writing (Zhang and Lapata, 2014; Lau et al., 2018; Liao et al., 2019), etc. Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) based sequence-tosequence (seq2seq) frameworks (Bahdanau et al., Code: http://github.com/lipiji/SongNet ۞ޗжަ▲Ѡ৩吷⽔澞னۨ䀲䬦ҁظՑ߄ݻײ澞 Or bends with the remover to remove. Introduction 1 橔ࠉލיଆㅓㅫޗڐП澞ٝީ㨡ࣞਘ㗘ੋչ澞 Let me not to the marriage of true minds Sonnet 1 SongCi Neural text generation has made tremendous progress in various tasks. One co"
2020.acl-main.68,N16-1014,0,0.0849713,"Missing"
2020.acl-main.68,D17-1222,1,0.910061,"Missing"
2020.acl-main.68,P15-1152,0,0.0397645,". In lyrics, the syllables of the lyric words must align with the tones of the notation. In SongCi and Sonnet, there are strict rhyming schemes and the rhyming words are labeled in red color and italic font. 2014; Gehring et al., 2017), Transformer and its variants (Vaswani et al., 2017; Dai et al., 2019), pre-trained auto-regressive language models such as XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), etc. Performance has been improved significantly in lots of tasks such as machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), dialogue systems (Vinyals and Le, 2015; Shang et al., 2015; Li, 2020), text summarization (Rush et al., 2015; Li et al., 2017; See et al., 2017), story telling (Fan et al., 2018; See et al., 2019), poetry writing (Zhang and Lapata, 2014; Lau et al., 2018; Liao et al., 2019), etc. Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) based sequence-tosequence (seq2seq) frameworks (Bahdanau et al., Code: http://github.com/lipiji/SongNet ۞ޗжަ▲Ѡ৩吷⽔澞னۨ䀲䬦ҁظՑ߄ݻײ澞 Or bends with the re"
2020.acl-main.68,D14-1074,0,0.031466,"led in red color and italic font. 2014; Gehring et al., 2017), Transformer and its variants (Vaswani et al., 2017; Dai et al., 2019), pre-trained auto-regressive language models such as XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), etc. Performance has been improved significantly in lots of tasks such as machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), dialogue systems (Vinyals and Le, 2015; Shang et al., 2015; Li, 2020), text summarization (Rush et al., 2015; Li et al., 2017; See et al., 2017), story telling (Fan et al., 2018; See et al., 2019), poetry writing (Zhang and Lapata, 2014; Lau et al., 2018; Liao et al., 2019), etc. Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) based sequence-tosequence (seq2seq) frameworks (Bahdanau et al., Code: http://github.com/lipiji/SongNet ۞ޗжަ▲Ѡ৩吷⽔澞னۨ䀲䬦ҁظՑ߄ݻײ澞 Or bends with the remover to remove. Introduction 1 橔ࠉލיଆㅓㅫޗڐП澞ٝީ㨡ࣞਘ㗘ੋչ澞 Let me not to the marriage of true minds Sonnet 1 SongCi Neural text generation has made tremendous progress in var"
2020.acl-main.68,P16-1162,0,\N,Missing
2020.emnlp-main.502,D16-1039,0,0.019144,"fer to them by in-pattern (or IP for short) words. We refer to words without columns/rows or embeddings, i.e., V  VP , by out-of-pattern (or OOP) words. 108 All Nouns In-Pattern Nouns 106 Frequency et al. (2017) explore combinations of manual features and (un)supervised predictors, and suggest that unsupervised metrics are more robust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every word in a corpus. Nevertheless they have no ability to cope with the Type-II sparsity, which is the main focus of our work. Another line of retrofitting methods (Vuli´c et al., 2018; Vuli´"
2020.emnlp-main.502,E12-1004,0,0.0224723,"esults comparable, we align the settings as much as possible. 5.1 Corpora and Evaluation Corpora. We used the 431k is-a pairs (243k unique) released by Roller et al. (2018). We substitute the Gigaword corpus they used by uKWac (Ferraresi, 2007) because the former is not complimentary. This decision does not affect reproducing pattern-based approaches in Roller et al. (2018). Evaluation Tasks. The three sub-tasks include 1) ranked hypernym detection: given (xq , yq ) decide whether yq is a hypernym of xq . Five datasets i.e., B LESS (Baroni and Lenci, 2011), E VAL (Santus et al., 2015), L EDS (Baroni et al., 2012), S HWARTZ (Shwartz et al., 2016) and WBLESS (Weeds et al., 2014) are used. The positive predictions should be ranked higher over negative ones and Average Precision (AP) is used for evaluation. 2) hypernymy direction classification: determine which word in a pair has a broader meaning. Besides B LESS and WB LESS, we also use B I BLESS (Kiela et al., 2015) and Accuracy (Acc.) is reported for binary classification. 3) graded entailment: predict scalar scores on H YPERLEX (Vuli´c et al., 2017). Spearman’s correlation ρ between the labels and predicted scores is reported. The statistics of datase"
2020.emnlp-main.502,W11-2501,0,0.0409711,"oller et al. (2018); Le et al. (2019). To make experimental results comparable, we align the settings as much as possible. 5.1 Corpora and Evaluation Corpora. We used the 431k is-a pairs (243k unique) released by Roller et al. (2018). We substitute the Gigaword corpus they used by uKWac (Ferraresi, 2007) because the former is not complimentary. This decision does not affect reproducing pattern-based approaches in Roller et al. (2018). Evaluation Tasks. The three sub-tasks include 1) ranked hypernym detection: given (xq , yq ) decide whether yq is a hypernym of xq . Five datasets i.e., B LESS (Baroni and Lenci, 2011), E VAL (Santus et al., 2015), L EDS (Baroni et al., 2012), S HWARTZ (Shwartz et al., 2016) and WBLESS (Weeds et al., 2014) are used. The positive predictions should be ranked higher over negative ones and Average Precision (AP) is used for evaluation. 2) hypernymy direction classification: determine which word in a pair has a broader meaning. Besides B LESS and WB LESS, we also use B I BLESS (Kiela et al., 2015) and Accuracy (Acc.) is reported for binary classification. 3) graded entailment: predict scalar scores on H YPERLEX (Vuli´c et al., 2017). Spearman’s correlation ρ between the labels"
2020.emnlp-main.502,P14-1113,0,0.0219829,"form matrix factorization or embedding learning. Due to their nature, only words “seen” in P, or VP = {x | (x, y) ∈ P ∨ (y, x) ∈ P}, will have respective columns/rows or embeddings. We refer to them by in-pattern (or IP for short) words. We refer to words without columns/rows or embeddings, i.e., V  VP , by out-of-pattern (or OOP) words. 108 All Nouns In-Pattern Nouns 106 Frequency et al. (2017) explore combinations of manual features and (un)supervised predictors, and suggest that unsupervised metrics are more robust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every"
2020.emnlp-main.502,P05-1014,0,0.368832,"Although matrix factorization (Roller et al., 2018) or embedding techniques (Vendrov et al., 2016; Nickel and Kiela, 2017; Le et al., 2019) are widely adopted to implement pattern-based approaches, they only relieve the Type-I sparsity and cannot generalize to unseen words appearing in the Type-II pairs. On the other hand, distribu6208 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6208–6217, c November 16–20, 2020. 2020 Association for Computational Linguistics tional ones follow, or are inspired by, the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005), i.e., the set of the hyponym’s contexts should be roughly contained by the hypernym’s. Although applicable to any word in a corpus, they are suggested to be inferior to pattern-based ones fed with sufficient extracted pairs (Roller et al., 2018; Le et al., 2019). Since pattern-based methods have unresolved sparsity issues, while distributional ones are more broadly applicable but globally inferior, neither of them can dominate the other in every aspect. In this light, we are interested in two questions: • Is the Type-II sparsity severe in practice? • If so, how to complement pattern-based ap"
2020.emnlp-main.502,C18-1042,0,0.0132967,"efer. On several benchmark datasets, our framework achieves competitive improvements and the case study shows its better interpretability. 1 Figure 1: The overall framework of complementary methods for hypernymy detection from corpus. Different sparsity types of queried pairs are handled with pattern-based and distributional models respectively. Introduction A taxonomy is a semantic hierarchy of words or concepts organized w.r.t. their hypernymy (a.k.a. is-a) relationships. Being a well-structured resource of lexical knowledge, taxonomies are vital to various tasks such as question answering (Gupta et al., 2018), textual entailment (Dagan et al., 2013; Bowman et al., 2015; Yu et al., 2020b), and text generation (Biran and McKeown, 2013). When automatically building taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, res"
2020.emnlp-main.502,C92-2082,0,0.588063,"taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, resulting in two lines of approaches: patternbased and distributional. Pattern-based approaches employ pattern pairs (x, y) extracted via Hearst-like patterns (Hearst, 1992), e.g., “y such as x” and “x and other y”. An example of extracted pattern pairs from corpus are shown in Figure 1. Despite their high precision, the extracted pairs suffer from sparsity which comes in two folds i.e., Type-I: xq and yq separately appear in some extracted pairs, but the pair (xq , yq ) is absent e.g., (dog, animal); or Type-II: either xq or yq is not involved in any extracted pair e.g., (crocodile, animal). Although matrix factorization (Roller et al., 2018) or embedding techniques (Vendrov et al., 2016; Nickel and Kiela, 2017; Le et al., 2019) are widely adopted to implement p"
2020.emnlp-main.502,W19-4310,0,0.0260493,"Missing"
2020.emnlp-main.502,P15-2020,0,0.0348367,"Missing"
2020.emnlp-main.502,P19-1313,0,0.120901,"extracted via Hearst-like patterns (Hearst, 1992), e.g., “y such as x” and “x and other y”. An example of extracted pattern pairs from corpus are shown in Figure 1. Despite their high precision, the extracted pairs suffer from sparsity which comes in two folds i.e., Type-I: xq and yq separately appear in some extracted pairs, but the pair (xq , yq ) is absent e.g., (dog, animal); or Type-II: either xq or yq is not involved in any extracted pair e.g., (crocodile, animal). Although matrix factorization (Roller et al., 2018) or embedding techniques (Vendrov et al., 2016; Nickel and Kiela, 2017; Le et al., 2019) are widely adopted to implement pattern-based approaches, they only relieve the Type-I sparsity and cannot generalize to unseen words appearing in the Type-II pairs. On the other hand, distribu6208 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6208–6217, c November 16–20, 2020. 2020 Association for Computational Linguistics tional ones follow, or are inspired by, the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005), i.e., the set of the hyponym’s contexts should be roughly contained by the hypernym’s. Although applicable to any w"
2020.emnlp-main.502,I13-1095,0,0.017723,"nterpretability. 1 Figure 1: The overall framework of complementary methods for hypernymy detection from corpus. Different sparsity types of queried pairs are handled with pattern-based and distributional models respectively. Introduction A taxonomy is a semantic hierarchy of words or concepts organized w.r.t. their hypernymy (a.k.a. is-a) relationships. Being a well-structured resource of lexical knowledge, taxonomies are vital to various tasks such as question answering (Gupta et al., 2018), textual entailment (Dagan et al., 2013; Bowman et al., 2015; Yu et al., 2020b), and text generation (Biran and McKeown, 2013). When automatically building taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, resulting in two lines of approaches: patternbased and distributional. Pattern-based approaches employ pattern pairs (x, y) extrac"
2020.emnlp-main.502,K16-1006,0,0.012767,": X 1 xh = MLPh (c), |C(x)| c∈C(x) n 1X c= cj . n j=1 To obtain yH , a similar network is applied, though the two MLPs do not share parameters to reflect the asymmetry of hypernymy. We fix the embeddings of context word vectors during training because satisfactory performance is observed. Due to its simplicity, NB OW is efficient to train. However, it ignores the order of context words and may not well reserve semantics. CONTEXT 2 VEC with M EAN -Pooling. To study the impacts of positional information within the context, we also attempt to substitute the NB OW with the CONTEXT 2 VEC encoder (Melamud et al., 2016). In CONTEXT 2 VEC, two LSTMs are used −c and ← − to encode the left and right contexts → c of an occurrence of x, respectively. The two output vectors are concatenated as the final context representation c for the same transformation and averaging as for NB OW. Formally,  −−−−→ − ←−−−− ←  c = LSTM(→ c ); LSTM( − c) . Note that the encoder for y still has separate parameters from those of x. Hierarchical Attention Networks. NB OW and CONTEXT 2 VEC with M EAN -Pooling both aggregate every context word’s information into xh and yH . Given several long contexts and the fixed output dimension, i"
2020.emnlp-main.502,D15-1075,0,0.0258947,"ompetitive improvements and the case study shows its better interpretability. 1 Figure 1: The overall framework of complementary methods for hypernymy detection from corpus. Different sparsity types of queried pairs are handled with pattern-based and distributional models respectively. Introduction A taxonomy is a semantic hierarchy of words or concepts organized w.r.t. their hypernymy (a.k.a. is-a) relationships. Being a well-structured resource of lexical knowledge, taxonomies are vital to various tasks such as question answering (Gupta et al., 2018), textual entailment (Dagan et al., 2013; Bowman et al., 2015; Yu et al., 2020b), and text generation (Biran and McKeown, 2013). When automatically building taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, resulting in two lines of approaches: patternbased and distribut"
2020.emnlp-main.502,N18-1045,0,0.014936,", 2018) embed WordNet in low-dimensional space. Depending on vectors of words learnt from known is-a pairs, the above pattern-based methods cannot induce more hypernymy pairs whose words do not appear in any pattern. Distributional Approaches. Distributional models are inspired by DIH (Geffet and Dagan, 2005). They work on only word contexts rather than extracted pairs, thus are applicable to any word in a corpus. Early unsupervised models typically propose asymmetric similarity metrics over manual word feature vectors for entailment (Weeds et al., 2004; Clarke, 2009; Santus et al., 2014). In Chang et al. (2018) and Nguyen et al. (2017), the authors inject DIH into unsupervised embedding models to yield latent feature vectors with hypernymy information. Those feature vectors, manual or latent, may serve in unsupervised asymmetric metrics or to train supervised hypernymy classifiers. Shwartz 6209 3 Preliminaries We formally define the aforementioned two types of sparsity, and provide some statistical insights about their impacts on pattern-based methods. 3.1 Notations and Definitions Let V be the vocabulary of a corpus C. By applying Hearst patterns on C, a set of extracted pairs P ⊆ V × V , i.e., is-"
2020.emnlp-main.502,W09-0215,0,0.0891622,"kel and Kiela, 2017, 2018; Ganea et al., 2018) embed WordNet in low-dimensional space. Depending on vectors of words learnt from known is-a pairs, the above pattern-based methods cannot induce more hypernymy pairs whose words do not appear in any pattern. Distributional Approaches. Distributional models are inspired by DIH (Geffet and Dagan, 2005). They work on only word contexts rather than extracted pairs, thus are applicable to any word in a corpus. Early unsupervised models typically propose asymmetric similarity metrics over manual word feature vectors for entailment (Weeds et al., 2004; Clarke, 2009; Santus et al., 2014). In Chang et al. (2018) and Nguyen et al. (2017), the authors inject DIH into unsupervised embedding models to yield latent feature vectors with hypernymy information. Those feature vectors, manual or latent, may serve in unsupervised asymmetric metrics or to train supervised hypernymy classifiers. Shwartz 6209 3 Preliminaries We formally define the aforementioned two types of sparsity, and provide some statistical insights about their impacts on pattern-based methods. 3.1 Notations and Definitions Let V be the vocabulary of a corpus C. By applying Hearst patterns on C,"
2020.emnlp-main.502,D14-1162,0,0.0837142,"the subscripts to reflect the asymmetry. In practice, we adopt networks with separate parameters for C(x) and C(y), which is detailed in the next section. 6211 4.2 Encoding Queried Words To implement the distributional model, we encode C(x) and C(y) into hypernymy-specific representations xh and yH , respectively. There are various off-the-shelf models to encode sentential contexts. We take the following four approaches. Transformed Word Vector. Instead of working directly on the original contexts C(x) and C(y), this approach takes as input the pre-trained word vectors (Mikolov et al., 2013; Pennington et al., 2014) x and y of x and y, and apply two MultiLayer Perceptrons (MLPs), respectively: xh = MLPh (x), yH = MLPH (y). The intuition is that word vectors roughly depend on the contexts and encode the distributional semantics. To make the MLPs generalize to V rather than VP , the word vectors are fixed during training. Inspired by the post specialization in Vuli´c et al. (2018), it also takes a similar approach to generalize task-specific word vector transformations to unseen words, though their evaluation task is not hypernymy detection. NB OW with M EAN-Pooling. Given words {cj }nj=1 in a context c ∈"
2020.emnlp-main.502,P18-2101,0,0.025295,"Missing"
2020.emnlp-main.502,P18-2057,0,0.316437,"aches: patternbased and distributional. Pattern-based approaches employ pattern pairs (x, y) extracted via Hearst-like patterns (Hearst, 1992), e.g., “y such as x” and “x and other y”. An example of extracted pattern pairs from corpus are shown in Figure 1. Despite their high precision, the extracted pairs suffer from sparsity which comes in two folds i.e., Type-I: xq and yq separately appear in some extracted pairs, but the pair (xq , yq ) is absent e.g., (dog, animal); or Type-II: either xq or yq is not involved in any extracted pair e.g., (crocodile, animal). Although matrix factorization (Roller et al., 2018) or embedding techniques (Vendrov et al., 2016; Nickel and Kiela, 2017; Le et al., 2019) are widely adopted to implement pattern-based approaches, they only relieve the Type-I sparsity and cannot generalize to unseen words appearing in the Type-II pairs. On the other hand, distribu6208 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6208–6217, c November 16–20, 2020. 2020 Association for Computational Linguistics tional ones follow, or are inspired by, the Distributional Inclusion Hypothesis (DIH; Geffet and Dagan 2005), i.e., the set of the hypony"
2020.emnlp-main.502,E14-4008,0,0.109007,"2017, 2018; Ganea et al., 2018) embed WordNet in low-dimensional space. Depending on vectors of words learnt from known is-a pairs, the above pattern-based methods cannot induce more hypernymy pairs whose words do not appear in any pattern. Distributional Approaches. Distributional models are inspired by DIH (Geffet and Dagan, 2005). They work on only word contexts rather than extracted pairs, thus are applicable to any word in a corpus. Early unsupervised models typically propose asymmetric similarity metrics over manual word feature vectors for entailment (Weeds et al., 2004; Clarke, 2009; Santus et al., 2014). In Chang et al. (2018) and Nguyen et al. (2017), the authors inject DIH into unsupervised embedding models to yield latent feature vectors with hypernymy information. Those feature vectors, manual or latent, may serve in unsupervised asymmetric metrics or to train supervised hypernymy classifiers. Shwartz 6209 3 Preliminaries We formally define the aforementioned two types of sparsity, and provide some statistical insights about their impacts on pattern-based methods. 3.1 Notations and Definitions Let V be the vocabulary of a corpus C. By applying Hearst patterns on C, a set of extracted pai"
2020.emnlp-main.502,W15-4208,0,0.013496,"2019). To make experimental results comparable, we align the settings as much as possible. 5.1 Corpora and Evaluation Corpora. We used the 431k is-a pairs (243k unique) released by Roller et al. (2018). We substitute the Gigaword corpus they used by uKWac (Ferraresi, 2007) because the former is not complimentary. This decision does not affect reproducing pattern-based approaches in Roller et al. (2018). Evaluation Tasks. The three sub-tasks include 1) ranked hypernym detection: given (xq , yq ) decide whether yq is a hypernym of xq . Five datasets i.e., B LESS (Baroni and Lenci, 2011), E VAL (Santus et al., 2015), L EDS (Baroni et al., 2012), S HWARTZ (Shwartz et al., 2016) and WBLESS (Weeds et al., 2014) are used. The positive predictions should be ranked higher over negative ones and Average Precision (AP) is used for evaluation. 2) hypernymy direction classification: determine which word in a pair has a broader meaning. Besides B LESS and WB LESS, we also use B I BLESS (Kiela et al., 2015) and Accuracy (Acc.) is reported for binary classification. 3) graded entailment: predict scalar scores on H YPERLEX (Vuli´c et al., 2017). Spearman’s correlation ρ between the labels and predicted scores is repor"
2020.emnlp-main.502,L16-1056,0,0.0254213,"opose a framework of complementing patternbased approaches with distributional models where the former is invalid. 3) We systematically conduct comparisons on several common datasets, validating the superiority of our framework. 2 Related Work Pattern-Based Approaches. Taxonomies from experts (e.g., WordNet (Miller, 1995)) have proved effective in various reasoning applications (Song et al., 2011; Zhang et al., 2020). Meanwhile, Hearst patterns (Hearst, 1992) make large corpora a good resource of explicit is-a pairs, resulting in automatically built hypernymy knowledge bases (Wu et al., 2012; Seitner et al., 2016) of large scales. The coverage of both words and hypernymy pairs in those resources are far from complete. To infer unknown hypernymies between known words, e.g., implicit is-a pairs in transitive closures, pattern-based models are proposed. Roller et al. (2018) and Le et al. (2019) show that, on a broad range of benchmarks, simple matrix decomposition or embeddings on pattern-based word cooccurrence statistics provide robust performance. On Probase (Wu et al., 2012) - a Hearst-patternbased taxonomy, Yu et al. (2015) use embeddings to address the same sparsity problem. Some methods (Vendrov et"
2020.emnlp-main.502,P16-1226,0,0.037554,"Missing"
2020.emnlp-main.502,E17-1007,0,0.0140098,"on extracted pairs involving xq or yq . The separate contexts of xq and yq in corpus C turn out to serve as the basis and input of the distributional model, respectively. Given the superior performance of pattern-based models on IP pairs (Roller et al., 2018), the distributional model g is only responsible to answer OOP pairs. Various choices exist to implement the distributional model. We may apply unsupervised metrics (Weeds et al., 2004; Clarke, 2009; Santus et al., 2014) on manual features extracted from contexts of xq and yq , which are robust to the distribution change of training data (Shwartz et al., 2017). However, the scores of those metrics are not necessarily in the same scale with those output by the patternbased model f for IP pairs. Such inconsistency will harm downstream systems which involve the scores for ranking or calculation. Given sufficient supervision signals from f and the inherent noise of natural language, we implement the distributional model g by a supervised neural-network-based approach. Specifically, the network encodes the contexts of x and y in C, i.e., C(x) and C(y), to be xh and yH , respectively, and makes predictions by a dot product, i.e., g(x, y) = hxh , yH i. No"
2020.emnlp-main.502,N18-1056,0,0.0230058,"Missing"
2020.emnlp-main.502,E17-2087,0,0.0124122,"rization or embedding learning. Due to their nature, only words “seen” in P, or VP = {x | (x, y) ∈ P ∨ (y, x) ∈ P}, will have respective columns/rows or embeddings. We refer to them by in-pattern (or IP for short) words. We refer to words without columns/rows or embeddings, i.e., V  VP , by out-of-pattern (or OOP) words. 108 All Nouns In-Pattern Nouns 106 Frequency et al. (2017) explore combinations of manual features and (un)supervised predictors, and suggest that unsupervised metrics are more robust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every word in a corpus. Nev"
2020.emnlp-main.502,J17-4004,0,0.0306628,"Missing"
2020.emnlp-main.502,N18-1048,0,0.0326359,"Missing"
2020.emnlp-main.502,N18-1103,0,0.184824,"Missing"
2020.emnlp-main.502,2020.acl-main.334,0,0.0278634,"learning. Due to their nature, only words “seen” in P, or VP = {x | (x, y) ∈ P ∨ (y, x) ∈ P}, will have respective columns/rows or embeddings. We refer to them by in-pattern (or IP for short) words. We refer to words without columns/rows or embeddings, i.e., V  VP , by out-of-pattern (or OOP) words. 108 All Nouns In-Pattern Nouns 106 Frequency et al. (2017) explore combinations of manual features and (un)supervised predictors, and suggest that unsupervised metrics are more robust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every word in a corpus. Nevertheless they have"
2020.emnlp-main.502,N18-1102,0,0.0118353,"ust w.r.t. the distribution change of training instances. Projection learning (Fu et al., 2014; Ustalov et al., 2017; Wang and He, 2020) has been used for supervised hypernymy detection. Other Improved Methods. Due to weak generalization ability of Hearst patterns, Anh et al. (2016) and Shwartz et al. (2016) relieve the constraints from strict Hearst patterns to co-occurring contexts or lexico-syntactic paths between two words. They encode the co-occurring contexts or paths using word vectors to train hypernymy embeddings or classifiers. Although leading to better recall than Hearst patterns (Washio and Kato, 2018), they limit the trained embeddings or models from generalizing to every word in a corpus. Nevertheless they have no ability to cope with the Type-II sparsity, which is the main focus of our work. Another line of retrofitting methods (Vuli´c et al., 2018; Vuli´c and Mrkˇsi´c, 2018), i.e., adjusting distributional vectors to satisfy external linguistic constraints, has been applied to hypernymy detection. However, they strictly require more additional resources e.g., synonym and antonym to achieve better performance (Kamath et al., 2019). To the best of our knowledge, we are the first to propos"
2020.emnlp-main.502,C14-1212,0,0.0164794,"Corpora and Evaluation Corpora. We used the 431k is-a pairs (243k unique) released by Roller et al. (2018). We substitute the Gigaword corpus they used by uKWac (Ferraresi, 2007) because the former is not complimentary. This decision does not affect reproducing pattern-based approaches in Roller et al. (2018). Evaluation Tasks. The three sub-tasks include 1) ranked hypernym detection: given (xq , yq ) decide whether yq is a hypernym of xq . Five datasets i.e., B LESS (Baroni and Lenci, 2011), E VAL (Santus et al., 2015), L EDS (Baroni et al., 2012), S HWARTZ (Shwartz et al., 2016) and WBLESS (Weeds et al., 2014) are used. The positive predictions should be ranked higher over negative ones and Average Precision (AP) is used for evaluation. 2) hypernymy direction classification: determine which word in a pair has a broader meaning. Besides B LESS and WB LESS, we also use B I BLESS (Kiela et al., 2015) and Accuracy (Acc.) is reported for binary classification. 3) graded entailment: predict scalar scores on H YPERLEX (Vuli´c et al., 2017). Spearman’s correlation ρ between the labels and predicted scores is reported. The statistics of datasets are shown in Table 1. The three tasks require algorithms to ou"
2020.emnlp-main.502,C04-1146,0,0.268422,"Missing"
2020.emnlp-main.502,2020.emnlp-main.608,0,0.0183376,"Missing"
2020.emnlp-main.502,N16-1174,0,0.0136159,"ht contexts → c of an occurrence of x, respectively. The two output vectors are concatenated as the final context representation c for the same transformation and averaging as for NB OW. Formally,  −−−−→ − ←−−−− ←  c = LSTM(→ c ); LSTM( − c) . Note that the encoder for y still has separate parameters from those of x. Hierarchical Attention Networks. NB OW and CONTEXT 2 VEC with M EAN -Pooling both aggregate every context word’s information into xh and yH . Given several long contexts and the fixed output dimension, it is vital for encoders to capture the most useful information. Inspired by Yang et al. (2016), we incorporate attention on different words and contexts. We use a feed-forward network to estimate the importance, and combine the information, of each context word to obtain c: n   X αj = softmax wa> tanh(Wa cj ) , c = αj cj . j=1 Then, another similar network is applied to all c(i) ∈ C(x) to obtain the representation of xh : |C(x)|   X > (i) βi = softmax wb tanh(Wb c ) , xh = βi c(i) . i=1 For word y, the encoder is similar but still has separate parameters from those of x. 4.3 Training the Distributional Model We train the distributional model g’s parameters Φ with supervision signal"
2020.emnlp-main.502,2020.acl-main.336,1,0.692425,"ts and the case study shows its better interpretability. 1 Figure 1: The overall framework of complementary methods for hypernymy detection from corpus. Different sparsity types of queried pairs are handled with pattern-based and distributional models respectively. Introduction A taxonomy is a semantic hierarchy of words or concepts organized w.r.t. their hypernymy (a.k.a. is-a) relationships. Being a well-structured resource of lexical knowledge, taxonomies are vital to various tasks such as question answering (Gupta et al., 2018), textual entailment (Dagan et al., 2013; Bowman et al., 2015; Yu et al., 2020b), and text generation (Biran and McKeown, 2013). When automatically building taxonomies from scratch or populating manually crafted ones, the hypernymy detection task plays a central role. For a pair of queried words (xq , yq ), hypernymy detection requires inferring the existence of a hyponymhypernym relationship between xq and yq . Due to ∗ Work done when C. Yu, J. Han and P. Wang were with Tencent AI Lab. the good coverage and availability, free-text corpora are widely used to facilitate hypernymy detection, resulting in two lines of approaches: patternbased and distributional. Pattern-ba"
2020.emnlp-main.502,2020.emnlp-main.119,1,0.65306,"fferent mixtures of Type-I and -II sparsity. Our contributions are summarized as : 1) We confirm that a specific type of sparsity issue of current pattern-based approaches is non-negligible. 2) We propose a framework of complementing patternbased approaches with distributional models where the former is invalid. 3) We systematically conduct comparisons on several common datasets, validating the superiority of our framework. 2 Related Work Pattern-Based Approaches. Taxonomies from experts (e.g., WordNet (Miller, 1995)) have proved effective in various reasoning applications (Song et al., 2011; Zhang et al., 2020). Meanwhile, Hearst patterns (Hearst, 1992) make large corpora a good resource of explicit is-a pairs, resulting in automatically built hypernymy knowledge bases (Wu et al., 2012; Seitner et al., 2016) of large scales. The coverage of both words and hypernymy pairs in those resources are far from complete. To infer unknown hypernymies between known words, e.g., implicit is-a pairs in transitive closures, pattern-based models are proposed. Roller et al. (2018) and Le et al. (2019) show that, on a broad range of benchmarks, simple matrix decomposition or embeddings on pattern-based word cooccurr"
2020.emnlp-main.741,N19-1124,1,0.882499,"Missing"
2020.emnlp-main.741,D19-1195,1,0.734215,"l., 2018) for eliminating the effect of training noises. The learning approach maintains two matching models and makes them teach each other. Li et al. (2019) attempted to neglect the effect of false negatives and trivial true responses by adopting four negative sampling strategies to choose negative samples during training dynamically. Different from those previous works, our approach makes use of grayscale data from heterogeneous sources and learns progressive quality relationships. In addition, our work enhances retrieval models with generation models, which is on par with recent attempts (Cai et al., 2019a,b) to strengthen generation models via retrieval models. 7 Conclusions We presented a novel approach for training response selection models for multi-turn conversa9227 tions. It automatically constructs different types of grayscale data and uses a multi-level ranking objective. The proposed approach can teach a matching model to capture fine-grained quality differences better and reduce the train-test discrepancy in distractor strength. Experimental results on three benchmark datasets and four state-of-the-art models demonstrated the effectiveness of the proposed training approach. Acknowled"
2020.emnlp-main.741,P19-1370,0,0.435209,"for responses with less relevance to dialogue context. Results on Ubuntu show a completely different behavior: the performances grow in step with the margin. The reason may be that 2 We also tried to use different margins for different pairs but the improvements are limited. the response distractors of Ubuntu have relatively large margins in semantic and matching models need to make strong discrimination between the ground truth and other grayscale samples. As a result, models learned with the large margin can fit such data distribution. 5.5 Compatiblity with Co-teaching We have noticed that Feng et al. (2019) adopts the co-teaching framework to train a robust matching model. From their experiment, the co-teaching framework with dynamic margins is proven to eliminate the effect from random sampled noisy responses effectively. We believe that our approach and co-teaching framework can benefit each other. Therefore, we combine our training approach with the co-teaching framework taking margins strategy as an instance to train the matching models. From the results in Table 5, we can see that models trained with our approach outperform those trained with the co-teaching framework. More importantly, the"
2020.emnlp-main.741,D19-1193,0,0.102299,"done during internship at Tencent AI Lab. † Corresponding authors. the response selection problem, the trendy practice is to build neural matching models (Ji et al., 2014; Wang et al., 2015; Xu et al., 2016; Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019) for scoring the adequacy of individual response candidates in the dialogue context. Most prior works on this topic focus on fine-grained text encoding and better interactions between dialogue context and response candidates, typically via sophisticated and powerful matching networks (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, in almost all these previous works, the matching models are trained with binary classification objective. Each response in the training data is either labeled positive (i.e., a correct response to the dialogue context) or negative (i.e., an incorrect response). Often, the negative responses are automatically constructed by random sampling. One limitation of the above training strategy is that this formalization downplays the nuance of fine-grained response quality; the matching model is only informed to predict a binary label, either correct or incorrect. However,"
2020.emnlp-main.741,N18-3022,0,0.0747402,"sponse relevance difference and (2) reduce the traintest discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements. 1 + −−− − − Table 1: Dialogue context (conversation history) between Speakers A and B. R1 is a random sample used as a negative instance during training. R2 and R3 are real distractors during testing. Introduction Building intelligent conversation systems (Shum et al., 2018; Kollar et al., 2018) is gaining more and more attention in recent years. A core module in such kind of conversation systems is response selection (Ritter et al., 2011; Hu et al., 2014; Wu et al., 2017; Tao et al., 2019): Identifying the best response from a set of possible candidates given a dialogue context, i.e., conversation history. For ∗ Relevance Equal contribution. Work was done during internship at Tencent AI Lab. † Corresponding authors. the response selection problem, the trendy practice is to build neural matching models (Ji et al., 2014; Wang et al., 2015; Xu et al., 2016; Wu et al., 2017; Zhou et al."
2020.emnlp-main.741,D19-1128,0,0.607183,"negative responses and successfully select response 1 as the best response. 6 Related Work Some researchers also studied how to improve the performance of existing matching models with a better learning method. Wu et al. (2018) proposed to leverage a Seq2Seq model as a weak annotator to assign a score for each response candidate of the dialogue and learn matching models through the scores. Feng et al. (2019) introduced the coteaching framework (Han et al., 2018) for eliminating the effect of training noises. The learning approach maintains two matching models and makes them teach each other. Li et al. (2019) attempted to neglect the effect of false negatives and trivial true responses by adopting four negative sampling strategies to choose negative samples during training dynamically. Different from those previous works, our approach makes use of grayscale data from heterogeneous sources and learns progressive quality relationships. In addition, our work enhances retrieval models with generation models, which is on par with recent attempts (Cai et al., 2019a,b) to strengthen generation models via retrieval models. 7 Conclusions We presented a novel approach for training response selection models"
2020.emnlp-main.741,N16-1014,0,0.0441645,"text and those in the training corpus. Typically, the retrieval results are better than random responses because they are more or less relevant to the dialogue context. However, most retrieval results are worse than the ground truth. The retrieval results are ranked tier-2. Specifically, we split the multi-turn dialogue into a series of single-turn input-response pairs. Then we index the input-response pairs with the BM25 algorithm (Robertson and Zaragoza, 2009). We retrieve response candidates using the last utterance of the dialogue context. The generation-based models (Shang et al., 2015; Li et al., 2016) generate a new utterance from scratch after training. While those models have better generalization capacity in rare dialogue contexts, the generation responses tend to be universal and noninformative (e.g., I dont know, I think so etc.) (Li et al., 2016). Similar to the retrieval responses, the generation responses are usually better than the random responses but worse than the ground-truth responses. However, compared to retrieval models that merely rely on lexical overlapping, generation results can capture deeper semantic interactions. The different characteristics of retrieval and genera"
2020.emnlp-main.741,W15-4640,0,0.499799,"strong response distractors. Our method is simple, effective, and orthogonal to prior efforts for modeling designs. It can be conveniently implemented with most existing matching models. Experimental results on four state-of-the-art matching models and three benchmark datasets demonstrate that our new training approach leads to remarkable performance improvement consistently. 2 Background Early research for response selection is devoted to single-turn conversations (Wang et al., 2013; Tan et al., 2015; Yan et al., 2016). Recently, researchers have started to study on multi-turn conversations (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In the current literature, the task of response selection is formulated as follows. Given a dialogue dataset D = {(ci , ri )}, where ci represents a dialogue context, and ri is the human-written ground-truth response. The goal is to build a matching model s(·, ·) from D so that s(c, r) accurately measures the adequacy of a response candidate r for a dialogue context c. Rapid progress has been made for building such matching models in recent years. Concretely, various neural architectures (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Gu et al.,"
2020.emnlp-main.741,P19-1006,0,0.133005,"ing more and more attention in recent years. A core module in such kind of conversation systems is response selection (Ritter et al., 2011; Hu et al., 2014; Wu et al., 2017; Tao et al., 2019): Identifying the best response from a set of possible candidates given a dialogue context, i.e., conversation history. For ∗ Relevance Equal contribution. Work was done during internship at Tencent AI Lab. † Corresponding authors. the response selection problem, the trendy practice is to build neural matching models (Ji et al., 2014; Wang et al., 2015; Xu et al., 2016; Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019) for scoring the adequacy of individual response candidates in the dialogue context. Most prior works on this topic focus on fine-grained text encoding and better interactions between dialogue context and response candidates, typically via sophisticated and powerful matching networks (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, in almost all these previous works, the matching models are trained with binary classification objective. Each response in the training data is either labeled positive (i.e., a correct response to the dialogue contex"
2020.emnlp-main.741,D11-1054,0,0.232309,"Missing"
2020.emnlp-main.741,P15-1152,0,0.0290779,"ven dialogue con9222 text and those in the training corpus. Typically, the retrieval results are better than random responses because they are more or less relevant to the dialogue context. However, most retrieval results are worse than the ground truth. The retrieval results are ranked tier-2. Specifically, we split the multi-turn dialogue into a series of single-turn input-response pairs. Then we index the input-response pairs with the BM25 algorithm (Robertson and Zaragoza, 2009). We retrieve response candidates using the last utterance of the dialogue context. The generation-based models (Shang et al., 2015; Li et al., 2016) generate a new utterance from scratch after training. While those models have better generalization capacity in rare dialogue contexts, the generation responses tend to be universal and noninformative (e.g., I dont know, I think so etc.) (Li et al., 2016). Similar to the retrieval responses, the generation responses are usually better than the random responses but worse than the ground-truth responses. However, compared to retrieval models that merely rely on lexical overlapping, generation results can capture deeper semantic interactions. The different characteristics of re"
2020.emnlp-main.741,P19-1001,0,0.264115,"te-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements. 1 + −−− − − Table 1: Dialogue context (conversation history) between Speakers A and B. R1 is a random sample used as a negative instance during training. R2 and R3 are real distractors during testing. Introduction Building intelligent conversation systems (Shum et al., 2018; Kollar et al., 2018) is gaining more and more attention in recent years. A core module in such kind of conversation systems is response selection (Ritter et al., 2011; Hu et al., 2014; Wu et al., 2017; Tao et al., 2019): Identifying the best response from a set of possible candidates given a dialogue context, i.e., conversation history. For ∗ Relevance Equal contribution. Work was done during internship at Tencent AI Lab. † Corresponding authors. the response selection problem, the trendy practice is to build neural matching models (Ji et al., 2014; Wang et al., 2015; Xu et al., 2016; Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019) for scoring the adequacy of individual response candidates in the dialogue context. Most prior works on this topic focus on fine-grained text encoding and better interactions"
2020.emnlp-main.741,D13-1096,0,0.0590343,"world response distractors and thus reduces the gap between training and testing, leading to a better distinguishing ability for strong response distractors. Our method is simple, effective, and orthogonal to prior efforts for modeling designs. It can be conveniently implemented with most existing matching models. Experimental results on four state-of-the-art matching models and three benchmark datasets demonstrate that our new training approach leads to remarkable performance improvement consistently. 2 Background Early research for response selection is devoted to single-turn conversations (Wang et al., 2013; Tan et al., 2015; Yan et al., 2016). Recently, researchers have started to study on multi-turn conversations (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In the current literature, the task of response selection is formulated as follows. Given a dialogue dataset D = {(ci , ri )}, where ci represents a dialogue context, and ri is the human-written ground-truth response. The goal is to build a matching model s(·, ·) from D so that s(c, r) accurately measures the adequacy of a response candidate r for a dialogue context c. Rapid progress has been made for building such matching mod"
2020.emnlp-main.741,N16-1170,0,0.0369201,"al., 2019) pairs each utterance of a context with a response via stacking multiple interaction blocks and then aggregates matching information from all the pairs as a matching score in an iterative fashion. Baseline Models We compare with the following baseline models. Single-turn Matching Models These models concatenate all context utterances together into one single long utterance then compute the matching scores between the long utterance and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM (Kadlec et al., 2015), Match-LSTM (Wang and Jiang, 2016) and MVLSTM (Wan et al., 2016). Implementation Details • MSN (Yuan et al., 2019) utilizes a multi-hop selector to select the relevant utterances as context and then matches the filtered context with the given response candidate to obtain a matching score. 1 Related resources can be found at https: //ai.tencent.com/ailab/nlp/dialogue/ datasets/grayscale_data_release.zip 9224 Model RNN CNN LSTM BiLSTM MV-LSTM Match-LSTM DL2R Multi-View DUA SMN DAM IOI MSN G-SMN G-DAM G-IOI G-MSN M AP 0.390 0.417 0.485 0.479 0.498 0.500 0.488 0.505 0.551 0.529 0.550 0.573 0.587 0.564 0.588 0.591 0.599 M RR 0.422"
2020.emnlp-main.741,P18-2067,0,0.0992586,"re in Chinese (the left) and we also provide their translated version in English (the right). introduced in Section 2 that these models may mistake the fuzzy-candidate with few improper details for the best response due to the gap between training and testing. In contrast, after adopting our training approach, the G-SMN and G-DAM correctly identify the improper content in the negative responses and successfully select response 1 as the best response. 6 Related Work Some researchers also studied how to improve the performance of existing matching models with a better learning method. Wu et al. (2018) proposed to leverage a Seq2Seq model as a weak annotator to assign a score for each response candidate of the dialogue and learn matching models through the scores. Feng et al. (2019) introduced the coteaching framework (Han et al., 2018) for eliminating the effect of training noises. The learning approach maintains two matching models and makes them teach each other. Li et al. (2019) attempted to neglect the effect of false negatives and trivial true responses by adopting four negative sampling strategies to choose negative samples during training dynamically. Different from those previous w"
2020.emnlp-main.741,P17-1046,0,0.34042,"sets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements. 1 + −−− − − Table 1: Dialogue context (conversation history) between Speakers A and B. R1 is a random sample used as a negative instance during training. R2 and R3 are real distractors during testing. Introduction Building intelligent conversation systems (Shum et al., 2018; Kollar et al., 2018) is gaining more and more attention in recent years. A core module in such kind of conversation systems is response selection (Ritter et al., 2011; Hu et al., 2014; Wu et al., 2017; Tao et al., 2019): Identifying the best response from a set of possible candidates given a dialogue context, i.e., conversation history. For ∗ Relevance Equal contribution. Work was done during internship at Tencent AI Lab. † Corresponding authors. the response selection problem, the trendy practice is to build neural matching models (Ji et al., 2014; Wang et al., 2015; Xu et al., 2016; Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019) for scoring the adequacy of individual response candidates in the dialogue context. Most prior works on this topic focus on fine-grained text encoding and"
2020.emnlp-main.741,D19-1011,0,0.313761,"Missing"
2020.emnlp-main.741,C18-1317,0,0.463025,"hod is simple, effective, and orthogonal to prior efforts for modeling designs. It can be conveniently implemented with most existing matching models. Experimental results on four state-of-the-art matching models and three benchmark datasets demonstrate that our new training approach leads to remarkable performance improvement consistently. 2 Background Early research for response selection is devoted to single-turn conversations (Wang et al., 2013; Tan et al., 2015; Yan et al., 2016). Recently, researchers have started to study on multi-turn conversations (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). In the current literature, the task of response selection is formulated as follows. Given a dialogue dataset D = {(ci , ri )}, where ci represents a dialogue context, and ri is the human-written ground-truth response. The goal is to build a matching model s(·, ·) from D so that s(c, r) accurately measures the adequacy of a response candidate r for a dialogue context c. Rapid progress has been made for building such matching models in recent years. Concretely, various neural architectures (Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018; Gu et al., 2019; Tao et al., 2019; Yuan et al., 2"
2020.emnlp-main.741,D16-1036,0,0.137025,"Missing"
2020.emnlp-main.741,P18-1103,0,0.306861,"al., 2018) is gaining more and more attention in recent years. A core module in such kind of conversation systems is response selection (Ritter et al., 2011; Hu et al., 2014; Wu et al., 2017; Tao et al., 2019): Identifying the best response from a set of possible candidates given a dialogue context, i.e., conversation history. For ∗ Relevance Equal contribution. Work was done during internship at Tencent AI Lab. † Corresponding authors. the response selection problem, the trendy practice is to build neural matching models (Ji et al., 2014; Wang et al., 2015; Xu et al., 2016; Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019) for scoring the adequacy of individual response candidates in the dialogue context. Most prior works on this topic focus on fine-grained text encoding and better interactions between dialogue context and response candidates, typically via sophisticated and powerful matching networks (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, in almost all these previous works, the matching models are trained with binary classification objective. Each response in the training data is either labeled positive (i.e., a correct response to t"
2020.findings-emnlp.401,W19-4828,0,0.011255,"ome may may come test today The Figure 1: Constituency trees of a right-branching language and its reversed (left-branching) language. The tree at the bottom is obtained by reversing the tree at the top. Introduction Neural language models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in two stages. Firstly, it defines the feature scores based on a language model, namely, the feature definition stage. Secondly, it leverages the feature scores to build a constituency tree, namely, the parsing stage. However, the degree to which the extracted constituency trees match gold constituency annotations may impreci"
2020.findings-emnlp.401,N19-1423,0,0.0371162,"on the branching bias, namely parsing algorithms, feature definitions, and language models. Experiments show that several existing works exhibit branching biases, and some implementations of these three factors can introduce the branching bias. 1 The today test come may may come test today The Figure 1: Constituency trees of a right-branching language and its reversed (left-branching) language. The tree at the bottom is obtained by reversing the tree at the top. Introduction Neural language models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in two stages. Firstly, it defines the feature scores based on a"
2020.findings-emnlp.401,N19-1419,0,0.0306393,"g bias. 1 The today test come may may come test today The Figure 1: Constituency trees of a right-branching language and its reversed (left-branching) language. The tree at the bottom is obtained by reversing the tree at the top. Introduction Neural language models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in two stages. Firstly, it defines the feature scores based on a language model, namely, the feature definition stage. Secondly, it leverages the feature scores to build a constituency tree, namely, the parsing stage. However, the degree to which the extracted constituency trees match gold constituency ann"
2020.findings-emnlp.401,P19-1356,0,0.0262215,"Missing"
2020.findings-emnlp.401,2021.ccl-1.108,0,0.0240199,"Missing"
2020.findings-emnlp.401,J93-2004,0,0.0697929,"syntax extracting pipeline (i.e., both the feature definition and parsing algorithm are fair) and then calculate the branching gap using the well-trained language models on languages L and L0 . Since there is no branching bias within our selected extracting method, the branching bias can be attributed to the input itself, if a branching gap is observed. 3 3.1 Experiments Settings Data We choose English as the main language in our experiments. The English data used for training language models is the concatenation of 1M lines of Wikipedia data (Devlin et al., 2019) and the Penn TreeBank (PTB) (Marcus et al., 1993) training data. We use PTB-22 and PTB-23 for validation and test, respectively. Besides, to rule out the impact of other linguistic properties, we also conduct part of our experiments on German and Chinese. We use the German Treebank from the SPMRL (Seddah et al., 2014) and Penn Chinese TreeBank (CTB) (Xue et al., 2005) with their provided test sets to evaluate previous methods on those two languages, respectively. Language Models In our experiments, we train three different language models (i.e., BERT, GPT2, LSTM) for English and its reversed language 4 . The BERT and GPT2 models are trained"
2020.findings-emnlp.401,W18-5444,0,0.0443675,"Missing"
2020.findings-emnlp.401,W19-4827,0,0.0208798,"Missing"
2020.findings-emnlp.401,2020.acl-main.383,0,0.0887257,"uage models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in two stages. Firstly, it defines the feature scores based on a language model, namely, the feature definition stage. Secondly, it leverages the feature scores to build a constituency tree, namely, the parsing stage. However, the degree to which the extracted constituency trees match gold constituency annotations may imprecisely reflect the model’s competence of capturing syntax, since their final performance may benefit from the branching bias. For example, as pointed out by Dyer et al. (2019), the syntax extracted from the ordered neuron based"
2020.findings-emnlp.401,N18-1202,0,0.0191411,"methods. Furthermore, we analyze the impacts of three factors on the branching bias, namely parsing algorithms, feature definitions, and language models. Experiments show that several existing works exhibit branching biases, and some implementations of these three factors can introduce the branching bias. 1 The today test come may may come test today The Figure 1: Constituency trees of a right-branching language and its reversed (left-branching) language. The tree at the bottom is obtained by reversing the tree at the top. Introduction Neural language models such as LSTM (Merity et al., 2018; Peters et al., 2018), GPT2 (Radford et al., 2019), and BERT (Devlin et al., 2019; Liu et al., 2019) have achieved state-of-the-art performance in various downstream NLP tasks. Many recent works try to interpret their success by revealing the linguistic properties captured by these language models (Hewitt and Manning, 2019; Clark et al., 2019; Jawahar et al., 2019; Tenney et al., 2019). One interesting line of these works tries to extract discrete constituency trees from pre-trained language models (Mareˇcek and Rosa, 2018, 2019; Kim et al., 2020; Wu et al., 2020). The core of these works is to extract syntax in t"
2020.findings-emnlp.401,W14-6111,0,0.0353882,"Missing"
2020.findings-emnlp.432,D18-1457,1,0.829088,"ution of source words. Related to our work, Li et al. (2019) and Tang et al. (2019b) also conducted word alignment analysis on the same De-En and Zh-En datasets with Transformer models8 . We use similar techniques to examine word alignment in our context; however, we also introduce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed"
2020.findings-emnlp.432,W18-5431,0,0.0330701,"Missing"
2020.findings-emnlp.432,W19-5211,0,0.125059,"duce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed a graduated label smoothing that can improve the inference calibration. In this work, based on our information probing analysis, we simplified the decoder by removing the residual feedforward module in totality, with minimal loss of translation quality and a significant boost of"
2020.findings-emnlp.432,D16-1159,0,0.0466695,"Missing"
2020.findings-emnlp.432,2020.acl-main.269,1,0.828464,"peed (words per second) and “#Infer.” denotes the inference speed (sentences per second). Results are averages of three runs. preting the behaviors of attention modules. Previous studies generally focus on the self-attention in the encoder, which is implemented as multi-head attention. For example, Li et al. (2018) showed that different attention heads in the encoder-side self-attention generally attend to the same position. Voita et al. (2019) and Michel et al. (2019) found that only a few attention heads play consistent and often linguistically-interpretable roles, and others can be pruned. Geng et al. (2020) empirically validated that a selective mechanism can mitigate the problem of word order encoding and structure modeling of encoder-side self-attention. In this work, we investigated the functionalities of decoder-side attention modules for exploiting both source and target information. Interpreting Encoder Attention The encoderattention weights are generally employed to interpret the output predictions of NMT models. Recently, Jain and Wallace (2019) showed that atten4806 IFM -IFM -- 3.5 3.0 2.5 SEM 3.0 2 5.0 TEM 4.010: Comparison Figure of IFM information evolution between the standard and s"
2020.findings-emnlp.432,D18-1548,0,0.0198969,"and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance – a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed. 1 Introduction Transformer models have advanced the state-ofthe-art on a variety of natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019"
2020.findings-emnlp.432,D19-1149,0,0.26795,", 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the e"
2020.findings-emnlp.432,D19-1088,1,0.802727,"cePPL PPL Source AddAdd & Norm & Norm 5.0 5.0 En-Zh Source PPL 3.5 4.0 4.5 4.5 IFM SEM TEM En-De Source PPL 4.0 Softmax Softmax 4.5 5.5 5.5 IFM SEM TEM En-Fr 5.0 Output Output 4.5 Probabilities Probabilities Target TargetPPL PPL IFM SEM TEM EnDe-w/oFFN 5.5 2.0 1 2 3 Decoder 4 Model Standard (Base) Simplified (Base) Fluency 4.00 4.01 Adequacy 3.86 3.87 Table 5: Human evaluation of translation performance of both standard and simplified decoders on 100 samples from En-Zh test set, on the scale of 1 to 5. tion weights are weakly correlated with the contribution of source words to the prediction. He et al. (2019) used the integrated gradients to better estimate the contribution of source words. Related to our work, Li et al. (2019) and Tang et al. (2019b) also conducted word alignment analysis on the same De-En and Zh-En datasets with Transformer models8 . We use similar techniques to examine word alignment in our context; however, we also introduce a forced-decoding-based probing task to closely examine the information flow. Understanding and Improving NMT Recent work started to improve NMT based on the findings of interpretation. For instance, Belinkov et al. (2017, 2018) pointed out that different"
2020.findings-emnlp.432,R19-1136,0,0.340244,", 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the e"
2020.findings-emnlp.432,N19-1357,0,0.0626198,"Missing"
2020.findings-emnlp.432,P19-1452,0,0.0230096,".1 Representation Evolution Across Layers In order to quantify and visualize the representation evolution, we design a universal probing scheme to quantify the source (or target) information stored in network representations. Task Description Intuitively, the more the source (or target) information stored in a network representation, the more probably a trained reconstructor could recover the source (or target) sequence. Since the lengths of source sequence and decoder representations are not necessarily the same, the widely-used classification-based probing approaches (Belinkov et al., 2017; Tenney et al., 2019b) cannot be applied to this task. Accordingly, we cast this task as a generation problem – evaluating the likelihood of generating the word sequence conditioned on the input representation. Figure 2 illustrates the architecture of our probing scheme. Given a representation sequence from decoder H = {h1 , . . . , hM } and the source (or target) word sequence to be recovered x = {x1 , . . . , xN } the recovery likelihood is calculated as the perplexity (i.e. negative log-likelihood) of forced-decoding the word sequence: P P L(x|H) = N X − log P (xn |x&lt;n , H) n=1 3 More implementation details ar"
2020.findings-emnlp.432,P16-1008,1,0.925135,"e functionalities of the three modules are well-separated. 2.3 Research Questions Modern Transformer decoder is implemented as multiple identical layers, in which the source and target information are exploited and evolved layerby-layer. One research question arises naturally: RQ1. How do source and target information evolve within the decoder layer-by-layer and module-by-module? RQ2. How does SEM exploit the source information in different layers? In Section 3.2, we investigate how the SEMs transform the source information to the target side in terms of alignment accuracy and coverage ratio (Tu et al., 2016). Experimental results show that higher layers of SEM modules accomplish word alignment, while lower layer ones exploit necessary contexts. This also explains the representation evolution of source information: lower layers collect more source information to obtain a global view of source input, and higher layers extract less aligned source input for accurate translation. Of the three sub-layers, IFM modules conceptually appear to play a key role in merging source and target information – raising our final question: RQ3. How does IFM fuse source and target information on the operation level? I"
2020.findings-emnlp.432,P19-1580,0,0.0203504,"bell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that the output of the encoder is fed to the decoder and the training signals for the encoder are back-propaga"
2020.findings-emnlp.432,D19-1085,1,0.891412,"Missing"
2020.findings-emnlp.432,D17-1301,1,0.848009,"ope that our analysis and findings could inspire architectural changes for further improvements, such as 1) improving the word alignment of higher SEMs by incorporating external alignment signals; 2) exploring the stacking order of SEM, TEM and IFM sub-layers, which may provide a more effective way to transform information; 3) further pruning redundant sub-layers for efficiency. Since our analysis approaches are not limited to the Transformer model, it is also interesting to explore other architectures such as RNMT (Chen et al., 2018), ConvS2S (Gehring et al., 2017), or on document-level NMT (Wang et al., 2017, 2019). In addition, our analysis methods can be applied to other sequence-to-sequence tasks such as summarization and grammar error correction, whose source and target sides are in the same language. We leave those tasks for future work. Acknowledgments Tadepalli acknowledges the support of DARPA under grant number N66001-17-2-4030. The authors thank the anonymous reviewers for their insightful and helpful comments. References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine trans"
2020.findings-emnlp.432,2020.acl-main.278,1,0.810191,", 2018) pointed out that different layers prioritize different linguistic types, based on which Dou et al. (2018) and Yang et al. (2019) simultaneously exposed all of these signals to the subsequent process. Dalvi et al. (2017) explained why the decoder learns considerably less morphology than the encoder, and then explored to explicitly inject morphology in the decoder. Emelin et al. (2019) argued that the need to represent and propagate lexical features in each layer limits the model’s capacity, and introduced gated shortcut connections between the embedding layer and each subsequent layer. Wang et al. (2020) revealed that miscalibration remains a severe challenge for NMT during inference, and proposed a graduated label smoothing that can improve the inference calibration. In this work, based on our information probing analysis, we simplified the decoder by removing the residual feedforward module in totality, with minimal loss of translation quality and a significant boost of both training and inference speeds. 5 Conclusions In this paper, we interpreted NMT Transformer decoder by assessing the evolution of both source 8 We find our results are more similar to that of Tang et al. (2019b). Also, o"
2020.findings-emnlp.432,P19-1354,1,0.925372,"ion (Vaswani et al., 2017), natural language inference (Shen et al., 2018), semantic role labeling (Strubell et al., 2018), and language representation (Devlin et al., 2019). However, so far not much is known about the internal properties and functionalities it learns to achieve its superior performance, which poses significant challenges for human understanding of the model and potentially designing better architectures. ∗ Work done when interning at Tencent AI Lab. Recent efforts on interpreting Transformer models mainly focus on assessing the encoder representations (Raganato et al., 2018; Yang et al., 2019; Tang et al., 2019a) or interpreting the multi-head self-attentions (Li et al., 2018; Voita et al., 2019; Michel et al., 2019). At the same time, there have been few attempts to interpret the decoder side, which we believe is also of great interest, and should be taken into account while explaining the encoder-decoder networks. The reasons are threefold: (a) the decoder takes both source and target as input, and implicitly performs the functionalities of both alignment and language modeling, which are at the core of machine translation; (b) the encoder and decoder are tightly coupled in that"
2020.findings-emnlp.432,D19-1083,0,0.0287233,"To verify that, we remove TEM from the decoder (“SEM⇒IFM”), which significantly increases the alignment error from 0.37 to 0.54 (in Figure 5), and leads to a serious decrease of translation performance (BLEU: 27.45 ⇒ 22.76, in Table 1) on En-De, while results on En-Zh also confirms it (in Figure 6). This indicates that TEM is essential for building word alignment. However, reordering the stacking of TEM and SEM (“SEM⇒TEM⇒IFM”) does not affect the alignment or translation qualities (BLEU: 27.45 vs. 27.61). These results provide empirical support for recent work on merging TEM and SEM modules (Zhang et al., 2019). Robustness to Decoder Depth To verify the robustness of our conclusions, we vary the depth of NMT decoder and train it from scratch. Table 2 demonstrates the results on translation quality, which generally show that more decoder layers bring better performance. Figure 7 shows that SEM behaves similarly regardless of depth. These results demonstrate the robustness of our conclusions. 3.3 Information Fusion in Decoder We now turn to the analysis of IFM. Within the Transformer decoder, IFM plays the critical role of fusing the source and target information by merg4804 0.9 0.9 0.9 0.9 0.8 0.8 0."
2020.wmt-1.34,D18-1457,1,0.850332,"Missing"
2020.wmt-1.34,D18-1045,0,0.0270558,"arge-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT (Sennrich et al., 2016a). The method first trains an intermediate target-to-source system, which is used to translate target monolingual corpus into source. Then the synthetic parallel corpus is used to train models together with the bilingual data. In this work we apply the noise back-translations method as introduced in (Lample et al., 2018). When translating monolingual data we use an ensemble of two models to get better source translations. We follow (Edunov et al., 2018) to add noise to the synthetic source data. Furthermore, we use a tag at the head of each synthetic source sentence as Caswell et al. (2019) does. To filter the pseudo corpus, we translate the synthetic source into target and calculate a Round-Trip BLEU score, the synthetic pairs are dropped if the BLEU score is lower than 30. Notably, we only apply back translation to the English → German task. We find that back translation decrease the translation quality to Chinese ↔ English tasks in our experiments. 2 https://github.com/mosessmt/mosesdecoder/tree/master/scripts/tokenizer/tokenizer.perl 3 3"
2020.wmt-1.34,D19-1135,1,0.88946,"Missing"
2020.wmt-1.34,N19-4009,0,0.0304603,", 2019) encoder. • B IG D EEP T RANSFORMER is the T RANSFORMER - BIG model with 20 encoder layers. • L ARGER T RANSFORMER is similar to B IG D EEP model except that it uses 8192 as the FFN inner width. The main differences between these models are presented in Table 1. To stabilize the training of deep model, we use the Pre-Norm strategy (Li et al., 2019). The layer normalization was applied to the input of every sub-layer which the computation sequence could be expressed as: normalize → Transform → dropout → residual-add. All models are implemented on top of the open-source toolkit Fairseq3 (Ott et al., 2019). 3.2 Data Augmentation Data augmentation is a commonly used technique to improve the translation quality. There are various of methods to conduct data augmentation such as back-translation (Sennrich et al., 2016a), joint training (Zhang et al., 2018) etc. In this section, we will introduce the methods we used in WMT2020. 3.2.1 Large-scale Back-translation Back-translation is the most commonly used data augmentation technique to incorporate monolingual data into NMT (Sennrich et al., 2016a). The method first trains an intermediate target-to-source system, which is used to translate target mono"
2020.wmt-1.34,P16-1009,0,0.261753,"tion task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the back-translation manner (Sennrich et al., 2016b). Different from the standard backtranslation, we add noise to the synthetic source ∗ Equal contribution. Correspondence to {frostwu, brightxwang, vinnylywang, fangxuliu}@tencent.com. sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning (Sun et al., 2019) is very effective in our three experiments and specially, we propose a boosted finetuning method for English ↔ Chinese tasks. We also take advantage of the"
2020.wmt-1.34,P16-1162,0,0.542977,"tion task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the back-translation manner (Sennrich et al., 2016b). Different from the standard backtranslation, we add noise to the synthetic source ∗ Equal contribution. Correspondence to {frostwu, brightxwang, vinnylywang, fangxuliu}@tencent.com. sentence in order to take advantage of large-scale monolingual text. In addition, we add a special token to the synthetic source sentence to help the model better distinguish the bilingual data and synthetic data. The in-domain finetuning (Sun et al., 2019) is very effective in our three experiments and specially, we propose a boosted finetuning method for English ↔ Chinese tasks. We also take advantage of the"
2020.wmt-1.34,W19-5341,0,0.0876143,"to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in English → German task following the b"
2020.wmt-1.34,2020.wmt-1.60,1,0.813015,"and the “update-freq” parameter in Fairseq is set to 8. Specifically, for L ARGE settings, the batch size is 4096 and “update-freq” is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 4.2 3.7 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by Wang et al. (2020c). The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble 6 Input: Single models M1m , In-domain corpus D, E1n is n different ensemble combinations Output: Single models M1m Translate D with"
2020.wmt-1.34,W18-6429,1,0.42399,"boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in"
2020.wmt-1.34,W18-6430,0,0.0695928,"boosted in-domain finetuning method to improve single models. Ensemble is used to combine single models and we propose an iterative transductive ensemble method which can further improve the translation performance based on the ensemble results. We achieve a BLEU score of 36.8 and the highest chrF score of 0.648 on Chinese → English task. 1 Introduction Recently, Transformer (Vaswani et al., 2017), that depends on self-attention mechanism , has significantly improved the translation quality. It is widely used as basic Neural Machine Translation (NMT) models in previous WMT translation tasks (Wang et al., 2018b; Li et al., 2019; Sun et al., 2019). In this year’s translation task, our Tencent Translation team participated in three WMT2020 shared news translation tasks, including Chinese → English, English → Chinese and English → German. For the three tasks, we use similar model architectures and training strategies. Four structures are used and all of them are based on deep transformer which are proven more effective than the standard Transformer-big models (Li et al., 2019). In terms of data augmentation, we adopt R2L training (Zhang et al., 2019) to all the tasks. Monolingual data is only used in"
2020.wmt-1.34,P19-1176,0,0.0535969,"Missing"
2020.wmt-1.34,2020.wmt-1.97,1,0.841946,"and the “update-freq” parameter in Fairseq is set to 8. Specifically, for L ARGE settings, the batch size is 4096 and “update-freq” is 16. We set max learning rate to 0.0007 and warmup-steps to 4000. All the dropout probabilities are set to 0.1. We select the checkpoint with the lowest loss on development set as the final checkpoint in each training. We calculate sacreBLEU score 6 for all experiments which is officially recommended. The WMT2019 testset (test2019) is used as the development set for all the tasks. 4.2 3.7 Iterative Transductive Ensemble Transductive ensemble (TE) is proposed by Wang et al. (2020c). The key idea is that source input sentences from the validation and test sets are firstly translated to the target language space with multiple different well-trained NMT models, which results in a pretranslated synthetic dataset. Then individual models are finetuned on the generated synthetic dataset. We propose an variation of TE, the Iterative Transductive Ensemble (ITE) which is based on Ensemble, as following: Algorithm 1: Iterative Transductive Ensemble 6 Input: Single models M1m , In-domain corpus D, E1n is n different ensemble combinations Output: Single models M1m Translate D with"
2020.wmt-1.34,J82-2005,0,0.607939,"Missing"
2020.wmt-1.60,W18-6404,0,0.0212518,"this hybrid combination method outperforms solely combining checkpoints or models in terms of robustness and effectiveness. Approaches We integrated advanced techniques into our systems, including data selection, model ensemble, back/forward translation, larger batch learning, finetuning, and system combination. 4.1 Data Selection Inspired by Ding and Tao (2019), multi-feature language modelling can select high-quality data from a large monolingual or bilingual corpus. We present a four-feature selection criterion, which scoring each sentence by BERT LM (Devlin et al., 2019b), Transformer LM (Bei et al., 2018), N-gram LM (Stolcke, 2002) and FDA (Bic¸ici and Yuret, 2011). Three LMs are complement each other on measuring qualities of sentences while FDA can measure its domain relevance given a in-domain dataset. Sentence pairs in the out-of-domain corpus Checkpoint Average and Model Ensemble 4.3 Finetuning We employ various finetuning strategies at different phases of training. For Sent-Out→Sent-In finetune (same architecture but different data), we first train a sentence-level model on large pseudo-indomain data and then continuously train it on small in-domain data. We apply similar strategy for Do"
2020.wmt-1.60,W11-2131,0,0.0808625,"Missing"
2020.wmt-1.60,N12-1047,0,0.0441032,"decoding. X LM→S ENT B ERT→D OC M BART→S ENT Integration Models IN I N+O UT IN I N+O UT IN I N+O UT I N→I N I N +O UT ∗ I N +O UT Pretrain O UT→I N O UT→I N +O UT I N +O UT I N +O UT I N +O UT BLEU 42.56 59.81 41.87 58.62 45.65 51.12 51.93 54.01 54.59 49.77 51.58 59.61 56.01 57.48 Table 4: BLEU scores of S ENT, D OC, NAT and P RE T RAIN with different finetuning strategies on En⇒De. the sentence reranker contains the best left-to-right (L2R) translation model, R2L (right-to-left) translation model and T2S (target-to-source) translation model. They are integrated by K-best batch MIRA training (Cherry and Foster, 2012) on valid set. 5 Experimental Results Unless otherwise specified, reported BLEU scores are calculated based on combined and tokenized validation set by muti-bleu.perl, which is different from the official evaluation method. 5.1 Ablation Study Table 2 investigates effects of different settings on translation quality. We then apply the best hyperparameters to the models in Section 4 if applicable. Effects of Model Average and Ensemble Following Section 4.2, we averaged top-L checkpoints in S ENT-B model and found that it performs best when L = 5. We followed the same operation for S ENT-S model"
2020.wmt-1.60,N19-1423,0,0.432471,"em combination. Particularly, we proposed a multi-feature data selection on large general-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensemble, back/forward translation and system combination. 1 We experimented mBART after the official submission. 483 Proceedings of the 5th Conference on Machine Translation (WMT), pages 483–491 c Online, November 19–20, 2020. 2020 Associatio"
2020.wmt-1.60,W19-5314,1,0.774159,"tion set and generated a final checkpoint with averaged weights to avoid stochasticity. To combine different models (maybe different architectures), we further ensembled the averaged checkpoints in each model. In our preliminary experiments, we find that this hybrid combination method outperforms solely combining checkpoints or models in terms of robustness and effectiveness. Approaches We integrated advanced techniques into our systems, including data selection, model ensemble, back/forward translation, larger batch learning, finetuning, and system combination. 4.1 Data Selection Inspired by Ding and Tao (2019), multi-feature language modelling can select high-quality data from a large monolingual or bilingual corpus. We present a four-feature selection criterion, which scoring each sentence by BERT LM (Devlin et al., 2019b), Transformer LM (Bei et al., 2018), N-gram LM (Stolcke, 2002) and FDA (Bic¸ici and Yuret, 2011). Three LMs are complement each other on measuring qualities of sentences while FDA can measure its domain relevance given a in-domain dataset. Sentence pairs in the out-of-domain corpus Checkpoint Average and Model Ensemble 4.3 Finetuning We employ various finetuning strategies at dif"
2020.wmt-1.60,2020.wmt-1.3,0,0.0819563,"Missing"
2020.wmt-1.60,2020.tacl-1.47,0,0.105944,"l-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensemble, back/forward translation and system combination. 1 We experimented mBART after the official submission. 483 Proceedings of the 5th Conference on Machine Translation (WMT), pages 483–491 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Tes"
2020.wmt-1.60,W18-6311,0,0.171527,"transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while"
2020.wmt-1.60,D15-1130,0,0.0686515,"ask. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. W"
2020.wmt-1.60,W18-6301,0,0.136336,"y proposed evolved cross-attention (Ding et al., 2020). Technically, we used the most recent effective strategies including back/forward translation, data selection, domain adaptation, batch learning, finetuning, model ensemble and system combination. Particularly, we proposed a multi-feature data selection on large general-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms (FDA, Bic¸ici and Yuret, 2011) to select domain-relevant data. In addition, we explore large batching (Ott et al., 2018) for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT (Devlin et al., 2019a), bilingual XLM (Conneau and Lample, 2019) and multilingual mBART (Liu et al., 2020), of which knowledge can be transferred to chat translation models.1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensem"
2020.wmt-1.60,P16-1009,0,0.431512,"w.statmt. org/wmt20/chat-task_results_DA.html. 484 3 https://github.com/Unbabel/BConTrasT. http://www.statmt.org/wmt20/ translation-task.html. 5 https://github.com/ google-research-datasets/Taskmaster. 4 We do not use larger monolingual corpora (e.g. CommonCrawl) and leave this for future work. 2.2 Source Sentence Processing Pre-processing To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, Unicode conversation, punctuation normalization, tokenization and true-casing. After filtering steps, we generate subwords via Joint BPE (Sennrich et al., 2016b) with 32K merge operations. 3.1 Sentence-level NMT (S ENT) We use standard T RANSFORMER models (Vaswani et al., 2017) with two customized settings. Due to data limitation, we use the small settings (S ENT-S)6 with regular batch size (4096 tokens × 8 GPUs). Based on the base settings (S ENT-B),7 we also empirically adopt big batch learning (Ott et al., 2018) (16348 tokens × 4 GPUs) with larger dropout (0.3). 3.2 Document-level NMT (D OC) To improve discourse properties for chat translation, we re-implement our document-level model (Wang et al., 2017b) on top of T RANS FORMER . Its addition en"
2020.wmt-1.60,P16-1162,0,0.599377,"w.statmt. org/wmt20/chat-task_results_DA.html. 484 3 https://github.com/Unbabel/BConTrasT. http://www.statmt.org/wmt20/ translation-task.html. 5 https://github.com/ google-research-datasets/Taskmaster. 4 We do not use larger monolingual corpora (e.g. CommonCrawl) and leave this for future work. 2.2 Source Sentence Processing Pre-processing To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, Unicode conversation, punctuation normalization, tokenization and true-casing. After filtering steps, we generate subwords via Joint BPE (Sennrich et al., 2016b) with 32K merge operations. 3.1 Sentence-level NMT (S ENT) We use standard T RANSFORMER models (Vaswani et al., 2017) with two customized settings. Due to data limitation, we use the small settings (S ENT-S)6 with regular batch size (4096 tokens × 8 GPUs). Based on the base settings (S ENT-B),7 we also empirically adopt big batch learning (Ott et al., 2018) (16348 tokens × 4 GPUs) with larger dropout (0.3). 3.2 Document-level NMT (D OC) To improve discourse properties for chat translation, we re-implement our document-level model (Wang et al., 2017b) on top of T RANS FORMER . Its addition en"
2020.wmt-1.60,D19-1633,0,0.0234314,"ls that generate each target word conditioned on previously generated ones, NAT models break the autoregressive factorization and produce target words in parallel (Gu et al., 2018). Although NAT is proposed to speed up the inference, we exploit it to alleviate sequential error accumulation and improve the diversity in conversational translation. To adequately capture the source contexts, we proposed evolved cross-attention for NAT decoder by modeling the local and global attention simultaneously (Ding et al., 2020). Accordingly, we implement our method based on the advanced MaskPredict model (Ghazvininejad et al., 2019)8 , which uses the conditional mask LM (Devlin et al., 2019a) to iteratively generate the target sequence from the masked input. 3.4 Pretraining NMT (P RETRAIN) To transfer the general knowledge to chat translation models, we explore to initialize (part of) model parameters with different pretrained language/generation models. Li et al. (2019) showed 8 https://github.com/facebookresearch/ Mask-Predict. 485 #CP 1 5 10 15 20 ENS En-De 60.32 60.33 60.26 60.19 60.23 60.49 De-En 59.51 59.53 59.42 59.34 59.22 60.08 (a) Model average and ensemble. #BM 4 8 12 14 16 20 En-De 60.33 60.33 60.33 60.34 60."
2020.wmt-1.60,P14-6007,0,0.0288008,"the Tencent AI Lab’s entry into the WMT2020 Chat Translation Task. We explore a breadth of established techniques for building chat translation systems. The paper includes numerous models making use of sentence-level, document-level, non-autoregressive NMT. It also investigates a number of advanced techniques including data selection, model ensemble, finetuing, back/forward translation and initialization using a pretrained LMs. We present extensive experimental results and hope that this work could help both MT researchers and industries to boost the performance of discourse-aware MT systems (Hardmeier, 2014; Wang, 2019). Acknowledgments The authors wish to thank the organizers of WMT2020 Chat Translation for their prompt responses on our questions. The authors also specially thank Dr. Xuebo Liu (University of Macau) and Dr. Siyou Liu (Macao Polytechnic Institute), who kindly support us by their engineering and linguistic suggestions, respectively. Official Results The official automatic evaluation results of our submissions for WMT 2020 are presented in Table 8. For the primary submission, the S YS-1 combines S ENT (ensembled S ENT-B and S ENT-S), D OC and NAT models. As contrastive submissions,"
2020.wmt-1.60,D16-1139,0,0.0422246,"el model on pseudo-in-domain data and then use parts of corresponding parameters to warm-up a document-level model, which will be continuously trained on in-domain data. 4.4 Back/Forward Translation Following Section 2, we obtain processed monolingual data. For back translation (BT), we use the best backward translation model to translate from target to source side and produce the synthetic corpus, which is used to enhance the autoregressive NMT models (Sennrich et al., 2016a). About forward translation (FT), we employ forward translation model to perform sequence distillation for NAT models (Kim and Rush, 2016) . 4.5 System Combination As shown in Figure 1, in order to take full advantages of different systems (Model1 , Model2 and Model3 ), we explore both token- and sentencelevel combination strategies. Token-level We perform token-level combination with confusion network. Concretely, our method follows Consensus Network Minimum Bayes Risk (ConMBR) (Sim et al., 2007), which can be modeled as EConM BR = argminE 0 L(E 0 , Econ ), where Econ was obtained as backbone through performing consensus network decoding. X LM→S ENT B ERT→D OC M BART→S ENT Integration Models IN I N+O UT IN I N+O UT IN I N+O UT"
2020.wmt-1.60,D19-6503,0,0.128451,"9 https://github.com/google-research/ bert. 12 https://github.com/marian-nmt/marian. 13 https://github.com/bicici/FDA. https://github.com/facebookresearch/ XLM. 10 https://github.com/pytorch/fairseq/ tree/master/examples/mbart. 486 Method S ENT-B +Bi-FDA +Bi-FDA-XL +Mono-FDA-XL # Sent. 10K 300K 500K 1M 500K 800K 1M 800K 1M BLEU 41.87 59.36 59.81 59.96 59.86 59.95 59.68 60.36 59.80 Systems S ENT-B S ENT-S D OC NAT Table 3: BLEU scores of S ENT-BASE model on En⇒De task with different FDA variants (three LMs scoring are consistent). S ENT→D OC and we use “h/si” symbols as their pseudo contexts (Kim et al., 2019; Li et al., 2020). Besides, we conduct Sent-Out→Doc-In finetuning (different architectures and data). Specifically, we first train a sentence-level model on pseudo-in-domain data and then use parts of corresponding parameters to warm-up a document-level model, which will be continuously trained on in-domain data. 4.4 Back/Forward Translation Following Section 2, we obtain processed monolingual data. For back translation (BT), we use the best backward translation model to translate from target to source side and produce the synthetic corpus, which is used to enhance the autoregressive NMT mode"
2020.wmt-1.60,2020.acl-main.322,0,0.0386177,"Missing"
2020.wmt-1.60,I17-3009,1,0.817542,". Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth of established techniques for building Chat NMT systems. Specifica"
2020.wmt-1.60,D19-1085,1,0.805322,"owledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in Germa"
2020.wmt-1.60,D17-1301,1,0.937396,". Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth of established techniques for building Chat NMT systems. Specifica"
2020.wmt-1.60,L16-1436,1,0.906273,"resent extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our submission to the novel task in De⇔En. We explore a breadth"
2020.wmt-1.60,2020.wmt-1.97,1,0.642141,"s 483–491 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Test 2,100 Out-of-domain 46,074,573 +filter 33,293,382 +select 1,000,000 Monolingual Out-of-domain De 58,044,806 +filter 56,508,715 +select 1,000,000 Out-of-domain En 34,209,709 +filter 32,823,301 +select 1,000,000 According to the official evaluation results, our systems in De⇒En and De⇐En are respectively ranked 2nd and 4th.2 Furthermore, a number of advanced technologies reported in this paper are also adapted to our systems for biomedical translation (Wang et al., 2020) and news translation (Wu et al., 2020) tasks, which respectively achieve up to 1st and 2nd ranks in terms of BLEU scores. Though our empirical experiments, we gain some interesting findings on the chat translation task: 1. The presented data selection method improves the baseline model by up to +18.5 BLEU points. It helps a lot for small-scale data. 2. The large batch learning works well, which makes sentence-level NMT models perform the best among different NMT models. 5. It is difficult to transfer general knowledge from pretrained LMs to the downstream translation task. Data and Processing"
2020.wmt-1.60,2020.wmt-1.34,1,0.813015,"2020 Association for Computational Linguistics Data # Sents Parallel In-domain 13,845 Valid 1,902 Test 2,100 Out-of-domain 46,074,573 +filter 33,293,382 +select 1,000,000 Monolingual Out-of-domain De 58,044,806 +filter 56,508,715 +select 1,000,000 Out-of-domain En 34,209,709 +filter 32,823,301 +select 1,000,000 According to the official evaluation results, our systems in De⇒En and De⇐En are respectively ranked 2nd and 4th.2 Furthermore, a number of advanced technologies reported in this paper are also adapted to our systems for biomedical translation (Wang et al., 2020) and news translation (Wu et al., 2020) tasks, which respectively achieve up to 1st and 2nd ranks in terms of BLEU scores. Though our empirical experiments, we gain some interesting findings on the chat translation task: 1. The presented data selection method improves the baseline model by up to +18.5 BLEU points. It helps a lot for small-scale data. 2. The large batch learning works well, which makes sentence-level NMT models perform the best among different NMT models. 5. It is difficult to transfer general knowledge from pretrained LMs to the downstream translation task. Data and Processing 2.1 Data The parallel data we use to t"
2020.wmt-1.60,N19-1095,0,0.0264596,"dels to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German⇒English primary system is ranked the second in terms of BLEU scores. 1 Introduction Although neural machine translation (NMT, Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017) has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness (Maruf et al., 2018; Wang et al., 2019), informality (Wang et al., 2018; Yang et al., 2019) and personality (Mirkin et al., 2015; Wang et al., 2016). This is a task-oriented chat translation task (Wang et al., 2017a; Farajian et al., 2020), which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German ∗ This work was conducted when Li Ding and Liang Ding were interning at Tencent AI Lab. Li Ding is now working at OPPO Research Institute. and English, the systems should translate the customer’s utterances in German⇒English (De⇒En) while the agent’s in German⇐English (De⇐En). In this paper, we present our sub"
2020.wmt-1.97,D11-1033,0,0.0270018,"-domain monolingual data from bilingual data in other language pair. Specifically, we collect the English side of the bilingual sentence pairs from Biomedical Translation and UFAL Medical Corpus. The statistics of the in-domain bilingual and monolingual data is listed in Table 2. Bilingual Data 3.3 In-domain bilingual data The in-domain bilingual data is provided by WMT20 biomedical translation shared task. For German-English, we choose Biomedical Translation3 and UFAL Medical Corpus4 to use as the in-domain training data. For Chinese-English out-of-domain (OOD) data, we adopt data selection (Axelrod et al., 2011; Liu et al., 2014) to select the in-house data (8.5M sentence pairs) as the in-domain training data. General-domain bilingual data To alleviate the data scarce problem, we collect generaldomain bilingual data from WMT20 news translation shared task5 . For German-English, we use Europarl-v106 , ParaCrawl-v5.17 , News Commentary-v158 and Wiki Titles-v29 . For 2 https://github.com/pytorch/fairseq (Ott et al., 2019) 3 https://github.com/ biomedical-translation-corpora/corpora 4 https://ufal.mff.cuni.cz/ufal_ medical_corpus 5 http://www.statmt.org/wmt18/ translation-task.html 6 http://www.statmt.o"
2020.wmt-1.97,W19-5403,0,0.0635797,"n shuffled.cs-en shuffled.es-en shuffled.fr-en shuffled.hu-en shuffled.pl-en shuffled.ro-en shuffled.sv-en n/a n/a n/a n/a n/a n/a n/a n/a 37,814,533 n/a n/a n/a n/a n/a n/a n/a 37,814,533 48,243,170 92,999,169 88,526,658 48,783,611 39,442,076 62,034,179 23,142,661 Table 2: The detailed statistics of in-domain training data used in our system. “Zh/En” and “De/En” denote the Chinese-English and German-English bilingual data, respectively. “En” denotes the monolingual English data. use Moses scripts14 to preprocess15 the data and filter the bilingual data with following heuristics rules: Follow Bawden et al. (2019), we use multibleu.perl from Moses16 to compute BLEU scores and report case-sensitive BLEU scores on development and test sets. • Filter out duplicate sentence pairs (Khayrallah and Koehn, 2018; Ott et al., 2018). • Filter out sentence pairs with wrong language (Khayrallah and Koehn, 2018). • Filter out sentences pairs containing more than 120 tokens or fewer than 3. • Filter out sentence pairs with source/target length ratio exceeding 1.5 (Ott et al., 2018). 4.2 Evaluation For German-English, we use the Khresmoi development data as the development set, and use the sentence pairs with the corr"
2020.wmt-1.97,C18-1111,0,0.045754,"Missing"
2020.wmt-1.97,D18-1457,1,0.838269,"on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), p"
2020.wmt-1.97,D19-1082,1,0.890519,"Missing"
2020.wmt-1.97,D19-1135,1,0.809118,"the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), pages 881–886 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Encoder Layer Decoder Layer Attention Heads Emb"
2020.wmt-1.97,N19-1122,1,0.887317,"Missing"
2020.wmt-1.97,2020.emnlp-main.176,1,0.763879,"inal model for the testing. For model inference, the length penalty is set to 0.6 and the beam size is set to 4. The German-English results are listed in Table 3. Our observations are: • Due to the largest model capacity, L ARGE model obtains the best translation performance among the four model variants. • Ensemble decoding with different transformer architectures (E NSEMBLE in Table 3) achieves best translation performance. 4.5 • Leveraging in-domain bilingual data (“+Indomain”) and synthetic bilingual data (“+BT In-domain”) achieves significant translation improvement. Data rejuvenation18 (Jiao et al., 2020) is an approach which exploits the inactive training examples for neural machine translation on large-scale datasets. We adopt the data rejuvenation approach to German⇒English translation task. Experimental results are presented in Tale 7 and the data rejuvenation approach achieves significant improvement over the baseline L ARGE model. 4.4 train the models from scratch. Since the development set and test set have different data distribution, we save checkpoints every epoch and average the last 5 checkpoints rather than choose the model with best validation loss. For model inference, the lengt"
2020.wmt-1.97,W18-2709,0,0.0186969,"3 48,243,170 92,999,169 88,526,658 48,783,611 39,442,076 62,034,179 23,142,661 Table 2: The detailed statistics of in-domain training data used in our system. “Zh/En” and “De/En” denote the Chinese-English and German-English bilingual data, respectively. “En” denotes the monolingual English data. use Moses scripts14 to preprocess15 the data and filter the bilingual data with following heuristics rules: Follow Bawden et al. (2019), we use multibleu.perl from Moses16 to compute BLEU scores and report case-sensitive BLEU scores on development and test sets. • Filter out duplicate sentence pairs (Khayrallah and Koehn, 2018; Ott et al., 2018). • Filter out sentence pairs with wrong language (Khayrallah and Koehn, 2018). • Filter out sentences pairs containing more than 120 tokens or fewer than 3. • Filter out sentence pairs with source/target length ratio exceeding 1.5 (Ott et al., 2018). 4.2 Evaluation For German-English, we use the Khresmoi development data as the development set, and use the sentence pairs with the correct alignment in WMT19 biomedical translation task test set as our test set. For Chinese-English, we use the in-house bilingual test set (1,000 sentence pairs) and the sentence pairs with the c"
2020.wmt-1.97,W17-3204,0,0.0209667,"English bilingual data for the community. The rest of this paper is organized as follows. Section 2 presents our system with four different transformer architectures: D EEP, H YBRID, B IG, L ARGE Transformers. Section 3 describes the training data used in our system, including bilingual data, monolingual data and synthetic bilingual data. Section 4 reports experimental results in two language directions. Finally, we conclude our work in Section 5. Introduction Neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017, NMT) has achieved great progress in recent years. However, as Koehn and Knowles (2017) pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach ("
2020.wmt-1.97,W19-5325,0,0.0377866,"Missing"
2020.wmt-1.97,2020.wmt-1.60,1,0.704639,"Missing"
2020.wmt-1.97,P19-1176,0,0.0310034,"opt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder stacks 35-layer self-attention-based encoder on top of 5-layer bi-directional ONLSTM (Shen et al., 2019) encoder. • B IG T RANSFORMER is the T RANSFORMER BIG model as used by Vaswani et al. (2017). • L ARGE T RANSFORMER is similar to T RANSFORMER - BIG model except that it uses a 20-layer encoder. 881 Proceedings of the 5th Conference on Machine Translation (WMT), pages 881–886 c Onli"
2020.wmt-1.97,D17-1155,0,0.0610201,"Missing"
2020.wmt-1.97,P14-2093,1,0.694985,"Missing"
2020.wmt-1.97,N19-4009,0,0.0654804,"Missing"
2020.wmt-1.97,W19-5420,0,0.0561124,"e lack of sufficient in-domain bilingual data, we use an online translation system TranSmart13 to translate the in-domain monolingual English back to Chinese. For German-English, we train a English-German L ARGE model on the combination of in-domain and general-domain bilingual data, and use the model to generate synthetic bilingual data. 4 Experiment We report experimental results in four language pairs: German-English (de/en), English-German (en/de), Chinese-English (zh/en) and EnglishChinese (en/zh). 4.1 Experimental Setup Data Pre-Processing We follow previous work (Saunders et al., 2019; Peng et al., 2019) to 10 http://mteval.cipsc.org.cn:81/ agreement/description 11 https://conferences.unite.un.org/ UNCorpus/ 12 http://data.statmt.org/wikititles/v2/ 13 transmart.qq.com 882 Corpus File Zh/En De/En En Biomedical Translation wmt18training/es-en wmt18training/fr-en wmt18training/pt-en wmt19training/de-en wmt19training/fr-en wmt19training/es-en wmt19training/pt-en wmt20training/it-en wmt20training/ru-en n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 40,398 n/a n/a n/a n/a n/a 287,811 627,576 74,645 40,398 75,049 100,257 49,918 14,756 46,782 UFAL Medical Corpus shuffled.de-en shuffled.cs-en shuffle"
2020.wmt-1.97,W19-5421,0,0.0186626,"r Chinese-English, as we lack of sufficient in-domain bilingual data, we use an online translation system TranSmart13 to translate the in-domain monolingual English back to Chinese. For German-English, we train a English-German L ARGE model on the combination of in-domain and general-domain bilingual data, and use the model to generate synthetic bilingual data. 4 Experiment We report experimental results in four language pairs: German-English (de/en), English-German (en/de), Chinese-English (zh/en) and EnglishChinese (en/zh). 4.1 Experimental Setup Data Pre-Processing We follow previous work (Saunders et al., 2019; Peng et al., 2019) to 10 http://mteval.cipsc.org.cn:81/ agreement/description 11 https://conferences.unite.un.org/ UNCorpus/ 12 http://data.statmt.org/wikititles/v2/ 13 transmart.qq.com 882 Corpus File Zh/En De/En En Biomedical Translation wmt18training/es-en wmt18training/fr-en wmt18training/pt-en wmt19training/de-en wmt19training/fr-en wmt19training/es-en wmt19training/pt-en wmt20training/it-en wmt20training/ru-en n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a n/a 40,398 n/a n/a n/a n/a n/a 287,811 627,576 74,645 40,398 75,049 100,257 49,918 14,756 46,782 UFAL Medical Corpus shuffled.de-en sh"
2020.wmt-1.97,P16-1009,0,0.709327,"pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder sta"
2020.wmt-1.97,P16-1162,0,0.826805,"pointed out, NMT systems suffer from poor translation performance in out-ofdomain scenarios, which poses a great challenge for the biomedical translation task. In this paper, we present our submission to the WMT20 shared task on biomedical translation task. We participated in two language directions: German-English and Chinese-English. To address the domain problem, on one hand, we adopt model ensemble technique (Liu et al., 2018) with different transformer architectures to build a more robust model. On the other hand, we enlarge the in-domain bilingual corpus with back-translation approach (Sennrich et al., 2016a). Our contributions are as follows: • We adopt the model ensemble technique and the back-translation approach to achieve 1 Details of our systems are introduced in https:// github.com/hsing-wang/WMT2020_BioMedical 2 System In our systems, we adopt four different model architectures with T RANSFORMER (Vaswani et al., 2017): • D EEP T RANSFORMER (Dou et al., 2018; Wang et al., 2019a; Dou et al., 2019) is the T RANSFORMER - BASE model with the 40layer encoder. • H YBRID T RANSFORMER (Hao et al., 2019b) is the T RANSFORMER - BASE model with 40layer hybrid encoder. The 40-layer hybrid encoder sta"
2020.wmt-1.97,D19-1145,1,0.895388,"Missing"
2020.wmt-1.97,D17-1149,1,0.899559,"Missing"
2020.wmt-1.97,2020.wmt-1.34,1,0.89255,"Missing"
2021.acl-demo.1,C18-1139,0,0.0140491,"d the URL is available on the web page.4 The text matching API is used to calculate the similarity between a pair of sentences. Similar to the text understanding API, the text matching API also supports access via HTTP-POST and the URL is available on the web page.5 Coarse-grained NER The difference between fine-grained and coarse-grained NERs is that the former involves more entity types with a finer granularity. We implement coarse-grained NER using supervised learning methods, including conditional random field (CRF) (Lafferty et al., 2001) based and deep neural network (DNN) based models (Akbik et al., 2018; Liu et al., 2019; Li et al., 2020). Constituency Parsing We implement the constituency parsing model based on the work (Kitaev and Klein, 2018). Kitaev and Klein (2018) build the parser by combining a sentence encoder with a chart decoder based on the self-attention mechanism. Different from work (Kitaev and Klein, 2018) , we use pre-trained BERT model as the text encoder to extract features to support the subsequent decoder-based parsing. Our model achieves excellent performance and has low search complexity. Semantic Role Labeling Semantic role labeling (also called shallow semantic parsin"
2021.acl-demo.1,P81-1022,0,0.278718,"lar expressions or supervised sequence tagging methods to recognize time and quantity entities. However, it is difficult for those methods to derive structured or deep semantic information of entities. To overcome this problem, time and quantity entities are parsed in TexSmart by Context Free Grammar (CFG), which is more expressive than regular expressions. Its key idea is similar to that in Shi et al. (2015) and can be described as follows: First, CFG grammar rules are manually written according to possible natural language expressions of a specific entity type. Second, the Earley algorithm (Earley, 1970) is employed to parse a piece of text to obtain semantic trees of entities. Finally, deep semantic representations of entities are derived from the semantic trees. 2.2 Other Modules Deep Semantic Representation Word Segmentation In order to support different application scenarios, TexSmart provides word segmentation results of two granularity levels: word level (or basic level), and phrase level. For phraselevel segmentation, some phrases (especially noun phrases) may contained as a unit. An unsupervised algorithm is implemented in TexSmart for both English and Chinese word segmentation. We ch"
2021.acl-demo.1,U15-1010,0,0.0276516,"TTP API employs a larger knowledge base and supports more 4 3 https://ai.tencent.com/ailab/nlp/texsmart/ table_html/tc_label_set.html. 5 5 https://texsmart.qq.com/api https://texsmart.qq.com/api/match_text. Quality SE ZH EN 79.5 80.5 PTB for English and CTB 9.0 for Chinese. We use their corresponding test sets to evaluate all the models. Coarse-grained NER To ensure better generalization to industrial applications, we combine several public training sets together for English NER. They are CoNLL2003 (Sang and De Meulder, 2003), BTC (Derczynski et al., 2016), GMB (Bos et al., 2017), SEC_FILING (Alvarado et al., 2015), WikiGold (Balasuriya et al., 2009; Nothman et al., 2013), and WNUT17 (Derczynski et al., 2017). Since the label set for all these datasets are slightly different, we only maintain three common labels (Person, Location and Organization) for training and testing. For Chinese, we create a NER dataset including about 80 thousand sentences labeled with 12 entity types, by following a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms for coarse-grained NER: CRF and DNN. For DNN, we implement the"
2021.acl-demo.1,W18-2501,0,0.0662142,"Missing"
2021.acl-demo.1,W09-3302,0,0.0687657,"Missing"
2021.acl-demo.1,C92-2082,0,0.277702,"de web search (e.g., for query suggestion) and recommendation systems. Semantic expansion task was firstly introduced in Han et al. (2020), and it was addressed by a neural method. However, this method is not as efficient as one expected for some industrial applications. Therefore, we propose a light-weight alternative approach in TexSmart for this task. This approach includes two offline steps and two online ones, as illustrated in Figure 2. During the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster"
2021.acl-demo.1,C10-3004,0,0.154401,"Missing"
2021.acl-demo.1,P08-1067,0,0.029343,"ven by TexSmart for “24 months ago” is a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en"
2021.acl-demo.1,D14-1082,0,0.0308477,"rt for “24 months ago” is a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offl"
2021.acl-demo.1,D18-1536,0,0.0331353,"Missing"
2021.acl-demo.1,P18-1249,0,0.0167912,"he text understanding API, the text matching API also supports access via HTTP-POST and the URL is available on the web page.5 Coarse-grained NER The difference between fine-grained and coarse-grained NERs is that the former involves more entity types with a finer granularity. We implement coarse-grained NER using supervised learning methods, including conditional random field (CRF) (Lafferty et al., 2001) based and deep neural network (DNN) based models (Akbik et al., 2018; Liu et al., 2019; Li et al., 2020). Constituency Parsing We implement the constituency parsing model based on the work (Kitaev and Klein, 2018). Kitaev and Klein (2018) build the parser by combining a sentence encoder with a chart decoder based on the self-attention mechanism. Different from work (Kitaev and Klein, 2018) , we use pre-trained BERT model as the text encoder to extract features to support the subsequent decoder-based parsing. Our model achieves excellent performance and has low search complexity. Semantic Role Labeling Semantic role labeling (also called shallow semantic parsing) tries to assign role labels to words or phrases in a sentence. TexSmart takes a sequence labeling model with BERT as the text encoder for sema"
2021.acl-demo.1,2021.findings-emnlp.18,1,0.731087,"Missing"
2021.acl-demo.1,P17-1152,0,0.0965523,"Missing"
2021.acl-demo.1,C16-1111,0,0.0255628,"HTTP API and the SDK may be slightly different, because the HTTP API employs a larger knowledge base and supports more 4 3 https://ai.tencent.com/ailab/nlp/texsmart/ table_html/tc_label_set.html. 5 5 https://texsmart.qq.com/api https://texsmart.qq.com/api/match_text. Quality SE ZH EN 79.5 80.5 PTB for English and CTB 9.0 for Chinese. We use their corresponding test sets to evaluate all the models. Coarse-grained NER To ensure better generalization to industrial applications, we combine several public training sets together for English NER. They are CoNLL2003 (Sang and De Meulder, 2003), BTC (Derczynski et al., 2016), GMB (Bos et al., 2017), SEC_FILING (Alvarado et al., 2015), WikiGold (Balasuriya et al., 2009; Nothman et al., 2013), and WNUT17 (Derczynski et al., 2017). Since the label set for all these datasets are slightly different, we only maintain three common labels (Person, Location and Organization) for training and testing. For Chinese, we create a NER dataset including about 80 thousand sentences labeled with 12 entity types, by following a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms fo"
2021.acl-demo.1,2021.naacl-main.116,1,0.743751,"entation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will be added in v0.3.0. Programming langu"
2021.acl-demo.1,W96-0213,0,0.664796,"ime was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will"
2021.acl-demo.1,C18-1166,0,0.0468114,"s a structured string with a precise date in JSON format: {&quot;value&quot;: [2019, 3]} if the screenshot time was Mar. 2021. Deep semantic representation is important for applications like task-oriented chatbots, where the precise meanings of some entities are required. So far, most public text understanding tools do not provide such a feaPart-of-Speech Tagging Part-of-Speech (POS) denotes the syntactic role of each word in a sentence, also known as word classes or syntactic categories and it is helpful for many downstream text understanding tasks such as parsing (Huang, 2008; Chen and Manning, 2014; Liu et al., 2018a). We implement three models among many popular ones for part-of-speech tagging (Ratnaparkhi, 1996; Huang et al., 2015; Li et al., 2021b): Log-linear based model (Ratnaparkhi, 1996), conditional random field (CRF) based model (Lafferty et al., 2001) and deep neural network (DNN) based model (Akbik 2.1.3 4 et al., 2018; Liu et al., 2019). We denote them as: log_linear, crf and dnn, respectively. text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https://ai.tencent.com/ ailab/nlp/texsmart/en/instructions.html. Offline Toolkit (SDK)"
2021.acl-demo.1,W03-0419,0,0.678216,"Missing"
2021.acl-demo.1,D15-1135,1,0.740722,"ient. In this sense, both methods are general in practice. ture. As a result, applications using these tools have to implement deep semantic representation by themselves. Some NLP toolkits make use of regular expressions or supervised sequence tagging methods to recognize time and quantity entities. However, it is difficult for those methods to derive structured or deep semantic information of entities. To overcome this problem, time and quantity entities are parsed in TexSmart by Context Free Grammar (CFG), which is more expressive than regular expressions. Its key idea is similar to that in Shi et al. (2015) and can be described as follows: First, CFG grammar rules are manually written according to possible natural language expressions of a specific entity type. Second, the Earley algorithm (Earley, 1970) is employed to parse a piece of text to obtain semantic trees of entities. Finally, deep semantic representations of entities are derived from the semantic trees. 2.2 Other Modules Deep Semantic Representation Word Segmentation In order to support different application scenarios, TexSmart provides word segmentation results of two granularity levels: word level (or basic level), and phrase level."
2021.acl-demo.1,W02-0109,0,0.557609,"Missing"
2021.acl-demo.1,C10-1112,1,0.519815,"rocedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally, there may be multiple (ambiguous) clusters containing the target entity mention and thus it is necessary to pick the best cluster through disambiguation. Once the best cluster is chosen, its members (or instances) can be returned as the expansion results. Now the core challenge is how to calculate the System Modules Compared to most other public text understanding systems, TexSmart supports three unique modules, i.e., fine-grained NER, semantic"
2021.acl-demo.1,P14-5010,0,0.00644413,"Missing"
2021.acl-demo.1,N18-2028,1,0.809382,"uring the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally, there may be multiple (ambiguous) clusters containing the target entity mention and thus it is necessary to pick the best cluster through disambiguation. Once the best cluster is chosen, its members (or instances) can be returned as the expansion results. Now the core challenge is how to calculate the System Modules Compared to most other public text understanding systems, TexSmart supports three unique modules, i.e., fine-gra"
2021.acl-demo.1,J93-2004,0,0.0740651,"ng a similar guideline to that of the Ontonotes dataset. We randomly split it into a training set and a test set with ratio of 3:1. We evaluate two algorithms for coarse-grained NER: CRF and DNN. For DNN, we implement the RoBERTa-CRF and Flair models. As we found RoBERTa-CRF performs better on the Chinese dataset while Flair is better on the English dataset, we report results of RoBERTa-CRF for Chinese and Flair for English in our experiments. Constituency Parsing We conduct parsing experiments on both English and Chinese datasets. For English task, we use WSJ sections in Penn Treebank (PTB) (Marcus et al., 1993), and we follow the standard splits: the training data ranges from section 2 to section 21; the development data is section 24; and the test data is section 23. For Chinese task, we use the Penn Chinese Treebank (CTB) of the version 5.1 (Xue et al., 2005). The training data includes the articles 001-270 and articles 440-1151; the development data is the articles 301- 325; and the test data is the articles 271-300. SRL Semantic role labeling experiments are conducted on both English and Chinese datasets. We use the CoNLL 2012 datasets (Pradhan et al., 2013) and follow the standard splits for th"
2021.acl-demo.1,W13-3516,0,0.0334885,"e WSJ sections in Penn Treebank (PTB) (Marcus et al., 1993), and we follow the standard splits: the training data ranges from section 2 to section 21; the development data is section 24; and the test data is section 23. For Chinese task, we use the Penn Chinese Treebank (CTB) of the version 5.1 (Xue et al., 2005). The training data includes the articles 001-270 and articles 440-1151; the development data is the articles 301- 325; and the test data is the articles 271-300. SRL Semantic role labeling experiments are conducted on both English and Chinese datasets. We use the CoNLL 2012 datasets (Pradhan et al., 2013) and follow the standard splits for the training, development and test sets. The network parameters of our model are initialized using RoBERTa. The batch size is set to 32 and the learning rate is 5×10−5 . Text Matching Two text matching algorithms are evaluated: ESIM and Linkage. The datasets used in evaluating English text matching are MRPC6 and QUORA7 . For Chinese text matching, four datasets are involved: LCQMC (Liu et al., 2018b), AFQMC (Xu et al., 2020), BQ_CORPUS (Chen et al., 2018), and PAWSzh (Zhang et al., 2019). We evaluate the quality FGNER Base Hybrid 45.9 53.8 Table 1: Semantic"
2021.acl-demo.1,P11-1116,1,0.692266,"(e.g., for query suggestion) and recommendation systems. Semantic expansion task was firstly introduced in Han et al. (2020), and it was addressed by a neural method. However, this method is not as efficient as one expected for some industrial applications. Therefore, we propose a light-weight alternative approach in TexSmart for this task. This approach includes two offline steps and two online ones, as illustrated in Figure 2. During the offline procedure, Hearst patterns are first applied to a large-scale text corpus to obtain a is-a map (or called a hyponym-to-hypernym map) (Hearst, 1992; Zhang et al., 2011). Then a clustering algorithm is employed to build a collection of term clusters from all the hyponyms, allowing a hyponym to belong to multiple clusters. Each term cluster is labeled by one or more hypernyms (or called type names). Term similarity scores used in the clustering algorithm are calculated by a combination of word embedding, distributional similarity, and patternbased methods (Mikolov et al., 2013; Song et al., 2018; Shi et al., 2010). During the online testing time, clusters containing the target entity mention are first retrieved by referring to the cluster collection. Generally"
2021.acl-demo.1,P13-4009,0,0.0710732,"Missing"
2021.acl-demo.1,N19-1131,0,0.045871,"Missing"
2021.acl-long.137,N19-1423,0,0.113115,"LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset. 5.3 Implementation Details For all experiments, we set the value of pcc (0) in the corpus-level pacing function pcc (t) as 0.3, meaning that all models start training with the context-response pairs whose corpus-level difficulty is lower than 0.3. For the instance-level pacing function pic (t), the value of kT is set as 3, meaning that, after IC is completed, the negative responses of each training instance are sampled"
2021.acl-long.137,P19-1370,0,0.049549,"Response Selection. Early studies in this area devoted to the response selection for single-turn conversations (Wang et al., 2013; Tan et al., 2016; Su et al., 2020). Recently, researchers turned to the scenario of multi-turn conversations and many sophisticated neural network architectures have been devised (Wu et al., 2017; Gu et al., 2019; Zhou et al., 2018; Gu et al., 2020). There is an emerging line of research studying how to improve existing matching models with better learning algorithms. Wu et al. (2018) proposed to adopt a Seq2seq model as weak teacher to guide the training process. Feng et al. (2019) designed a co-teaching framework to eliminate the training noises. Similar to our work, Li et al. (2019) proposed to alleviate the problem of trivial negatives by sampling stronger negatives. Lin et al. (2020) attempted to create negative examples with a retrieval system and a pre-trained generation model. In contrast to their studies, we not only enlarge the set of negative examples but also arrange the negative examples in an easy-to-diffuclt fashion. Curriculum Learning. Curriculum Learning (Bengio et al., 2009) is reminiscent of the cognitive process of human being. Its core idea is first"
2021.acl-long.137,D19-1193,0,0.0320746,"lding intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the positive response scor"
2021.acl-long.137,2020.emnlp-main.550,0,0.0218947,"ext-response combinations. In this work, we construct G(·, ·) as an non-interaction matching model with dualencoder structure such that we can precompute all contexts and responses offline and store them in cache. For any context-response pair (c, r), its pairwise relevance G(c, r) is defined as G(c, r) = Ec (c)T Er (r), (4) where Ec (c) and Er (r) are the dense context and response representations produced by a context encoder Ec (·) and a response encoder Er (·)2 . Offline Index. After training the ranking model on the same response selection dataset D using the in-batch negative objective (Karpukhin et al., 2020), we compute the dense representations of all contexts and responses contained in D. Then, as described in Eq. (4), the relevance scores of all possible combinations of the contexts and responses in D can be easily computed through the dot product between their representations. After this step, we can compute the corpus-level and instance-level difficulty of all possible combinations and cache them in memory for a fast access in training. 2 The encoders can be any model, e.g., LSTM (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017). Related Work Dialogue Response Select"
2021.acl-long.137,N18-3022,0,0.0122841,"the dialogue context and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics. 1 Table 1: An example dialogue context between speakers A and B, where P1 and P2 are easy and difficult positives; N1 and N2 are easy and difficult negatives. Introduction Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the co"
2021.acl-long.137,D19-1128,0,0.384379,"during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the positive response scores are higher than the negative ones. Recently, some researchers (Li et al., 2019; Lin et al., 2020) have raised the concern that randomly sampled negative responses are often too trivial (i.e., totally irrelevant to the dialogue context). Models trained with trivial negative responses may fail to handle strong distractors in real-world scenarios. Essentially, the problem stems from the ignorance of the diversity in context-response matching degree. In other words, all random responses are treated as equally negative regardless of their different distracting strengths. For example, Ta1740 Proceedings of the 59th Annual Meeting of the Association for Computational Linguisti"
2021.acl-long.137,2020.emnlp-main.741,1,0.293051,"p at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the positive response scores are higher than the negative ones. Recently, some researchers (Li et al., 2019; Lin et al., 2020) have raised the concern that randomly sampled negative responses are often too trivial (i.e., totally irrelevant to the dialogue context). Models trained with trivial negative responses may fail to handle strong distractors in real-world scenarios. Essentially, the problem stems from the ignorance of the diversity in context-response matching degree. In other words, all random responses are treated as equally negative regardless of their different distracting strengths. For example, Ta1740 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Int"
2021.acl-long.137,W15-4640,0,0.0277167,"5 0.775 0.792 0.828 0.825 0.857 0.858 0.842 0.861 0.921 0.933 0.950 0.886 0.937 0.985 0.935 0.968 0.993 Table 2: Experimental results of different models trained with our approach on Douban, Ubuntu, and E-Commerce datasets. All results acquired using HCL outperforms the original results with a significance level p-value < 0.01. we report the results of Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at Position 1 (P@1). In addition, we also report the results of R10 @1, R10 @2, R10 @5, where Rn @k means recall at position k in n candidates. Ubuntu Dataset. This dataset (Lowe et al., 2015) contains multi-turn dialogues collected from chat logs of the Ubuntu Forum. The training, validation and test size are 500k, 50k and 50k. Each dialogue context is paired with 10 response candidates. Following previous studies, we use R2 @1, R10 @1, R10 @2 and R10 @5 as evaluation metrics. E-Commerce Dataset. This dataset (Zhang et al., 2018) consists of Chinese conversations between customers and customer service staff from Taobao4 . The size of training, validation and test set are 500k, 25k and 1k. In the test set, each dialogue context is paired with 10 candidate responses. Rn @k are emplo"
2021.acl-long.137,P19-1006,0,0.0173133,"Introduction Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensures that the posi"
2021.acl-long.137,D11-1054,0,0.126157,"Missing"
2021.acl-long.137,N10-1116,0,0.0106068,"ted to create negative examples with a retrieval system and a pre-trained generation model. In contrast to their studies, we not only enlarge the set of negative examples but also arrange the negative examples in an easy-to-diffuclt fashion. Curriculum Learning. Curriculum Learning (Bengio et al., 2009) is reminiscent of the cognitive process of human being. Its core idea is first learning easier concepts and then gradually transitioning to more complex concepts based on some predefined learning schemes. Curriculum learning (CL) has demonstrated its benefits in various machine learning tasks (Spitkovsky et al., 2010; Ilg et al., 2017; Li et al., 2017; Svetlik et al., 2017; Liu et al., 2018; Platanios et al., 2019). Recently, Penha and Hauff (2020) employed the idea of CL to tackle the response selection task. However, they only apply curriculum learning for the positive-side response selection, while ignoring the diversity of the negative responses. 5 5.1 Experiment Setups Datasets and Evaluation Metrics We test our approach on three benchmark datasets. Douban Dataset. This dataset (Wu et al., 2017) consists of multi-turn Chinese conversation data crawled from Douban group3 . The size of training, valida"
2021.acl-long.137,D19-1011,0,0.0429294,"utterance and then measures the relevance score between the context and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset. 5.3 Implementation Details For all experiments, we set the value of pcc (0) in the corpus-level pacing function pcc (t) as 0.3, meaning that all models start training with the context-response pairs whose corpus-level difficulty is lower"
2021.acl-long.137,P19-1001,0,0.0132239,"utterance and then measures the relevance score between the context and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset. 5.3 Implementation Details For all experiments, we set the value of pcc (0) in the corpus-level pacing function pcc (t) as 0.3, meaning that all models start training with the context-response pairs whose corpus-level difficulty is lower"
2021.acl-long.137,D13-1096,0,0.0289406,"cribed in Eq. (4), the relevance scores of all possible combinations of the contexts and responses in D can be easily computed through the dot product between their representations. After this step, we can compute the corpus-level and instance-level difficulty of all possible combinations and cache them in memory for a fast access in training. 2 The encoders can be any model, e.g., LSTM (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017). Related Work Dialogue Response Selection. Early studies in this area devoted to the response selection for single-turn conversations (Wang et al., 2013; Tan et al., 2016; Su et al., 2020). Recently, researchers turned to the scenario of multi-turn conversations and many sophisticated neural network architectures have been devised (Wu et al., 2017; Gu et al., 2019; Zhou et al., 2018; Gu et al., 2020). There is an emerging line of research studying how to improve existing matching models with better learning algorithms. Wu et al. (2018) proposed to adopt a Seq2seq model as weak teacher to guide the training process. Feng et al. (2019) designed a co-teaching framework to eliminate the training noises. Similar to our work, Li et al. (2019) propo"
2021.acl-long.137,N16-1170,0,0.033235,"alogue context is paired with 10 candidate responses. Rn @k are employed as the evaluation metrics. 5.2 Baseline Models In the experiments, we compare our approach with the following models that can be summarized into three categories. Single-turn Matching Models. This type of models treats all dialogue context as a single long utterance and then measures the relevance score between the context and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response"
2021.acl-long.137,C18-1317,0,0.0214328,"results of Mean Average Precision (MAP), Mean Reciprocal Rank (MRR) and Precision at Position 1 (P@1). In addition, we also report the results of R10 @1, R10 @2, R10 @5, where Rn @k means recall at position k in n candidates. Ubuntu Dataset. This dataset (Lowe et al., 2015) contains multi-turn dialogues collected from chat logs of the Ubuntu Forum. The training, validation and test size are 500k, 50k and 50k. Each dialogue context is paired with 10 response candidates. Following previous studies, we use R2 @1, R10 @1, R10 @2 and R10 @5 as evaluation metrics. E-Commerce Dataset. This dataset (Zhang et al., 2018) consists of Chinese conversations between customers and customer service staff from Taobao4 . The size of training, validation and test set are 500k, 25k and 1k. In the test set, each dialogue context is paired with 10 candidate responses. Rn @k are employed as the evaluation metrics. 5.2 Baseline Models In the experiments, we compare our approach with the following models that can be summarized into three categories. Single-turn Matching Models. This type of models treats all dialogue context as a single long utterance and then measures the relevance score between the context and response ca"
2021.acl-long.137,D16-1036,0,0.0232595,"ing Models. This type of models treats all dialogue context as a single long utterance and then measures the relevance score between the context and response candidates, including RNN (Lowe et al., 2015), CNN (Lowe et al., 2015), LSTM (Lowe et al., 2015), Bi-LSTM 4 www.taobao.com (Kadlec et al., 2015), MV-LSTM (Wan et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Multi-turn Matching Models. Instead of treating the dialogue context as one single utterance, these models aggregate information from different utterances in more sophisticated ways, including DL2R (Yan et al., 2016), Multi-View (Zhou et al., 2016), DUA (Zhang et al., 2018), DAM (Zhou et al., 2018), MRFN (Tao et al., 2019a), IOI (Tao et al., 2019b), SMN (Wu et al., 2017) and MSN (Yuan et al., 2019). BERT-based Matching Models. Given the recent advances of pre-trained language models (Devlin et al., 2019), Gu et al. (2020) proposed the SA-BERT model which adapts BERT for the task of response selection and it is the current state-ofthe-art model on the Douban and Ubuntu dataset. 5.3 Implementation Details For all experiments, we set the value of pcc (0) in the corpus-level pacing function pcc (t) as 0.3, meaning that all models start trai"
2021.acl-long.137,P18-1103,0,0.273013,"ifficult negatives. Introduction Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the training objective ensu"
2021.acl-long.137,P18-2067,0,0.030971,"r and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017). Related Work Dialogue Response Selection. Early studies in this area devoted to the response selection for single-turn conversations (Wang et al., 2013; Tan et al., 2016; Su et al., 2020). Recently, researchers turned to the scenario of multi-turn conversations and many sophisticated neural network architectures have been devised (Wu et al., 2017; Gu et al., 2019; Zhou et al., 2018; Gu et al., 2020). There is an emerging line of research studying how to improve existing matching models with better learning algorithms. Wu et al. (2018) proposed to adopt a Seq2seq model as weak teacher to guide the training process. Feng et al. (2019) designed a co-teaching framework to eliminate the training noises. Similar to our work, Li et al. (2019) proposed to alleviate the problem of trivial negatives by sampling stronger negatives. Lin et al. (2020) attempted to create negative examples with a retrieval system and a pre-trained generation model. In contrast to their studies, we not only enlarge the set of negative examples but also arrange the negative examples in an easy-to-diffuclt fashion. Curriculum Learning. Curriculum Learning"
2021.acl-long.137,P17-1046,0,0.35725,"N2 are easy and difficult negatives. Introduction Building intelligent conversation systems is a longstanding goal of artificial intelligence and has attracted much attention in recent years (Shum et al., 2018; Kollar et al., 2018). An important challenge for building such conversation systems is the response selection problem, that is, selecting the best response to a given dialogue context from a set of candidate responses (Ritter et al., 2011). To tackle this problem, different matching models are developed to measure the matching degree between a dialogue context and a response candidate (Wu et al., 2017; Zhou et al., 2018; Lu et al., 2019; Gu et al., 2019). Despite their differences, ∗ The main body of this work was done during internship at Tencent Inc. The first two authors contributed equally. Yan Wang is the corresponding author. most prior works train the model with data constructed by a simple heuristic. For each context, the human-written response is considered as positive (i.e., an appropriate response) and the responses from other dialogue contexts are considered as negatives (i.e., inappropriate responses). In practice, the negative responses are often randomly sampled and the trai"
2021.acl-long.221,W19-5301,0,0.0422466,"Missing"
2021.acl-long.221,2021.acl-long.567,0,0.0323651,"(NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large"
2021.acl-long.221,W19-5206,0,0.218083,"ems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied. Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava"
2021.acl-long.221,2020.acl-main.532,0,0.253958,"suggest that emphasizing the learning on uncertain monolingual sentences also brings additional benefits for the learning of low-frequency words at the target side. 4 Related Work Synthetic Parallel Data. Data augmentation by synthetic parallel data has been the most simple and effective way to utilize monolingual data for NMT, 2847 which can be achieved by self-training (He et al., 2019) and back-translation (Sennrich et al., 2016a). While back-translation has dominated the NMT area for years (Fadaee and Monz, 2018; Edunov et al., 2018; Caswell et al., 2019), recent works on translationese (Marie et al., 2020; Graham et al., 2019) suggest that NMT models trained with backtranslation may lead to distortions in automatic and human evaluation. To address the problem, starting from WMT2019 (Barrault et al., 2019), the test sets only include naturally occurring text at the sourceside, which is a more realistic scenario for practical translation usage. In this new testing setup, the forward-translation (Zhang and Zong, 2016), i.e., self-training in NMT, becomes a more promising method as it also introduces naturally occurring text at the source-side. Therefore, we focus on the data sampling strategy in"
2021.acl-long.221,D18-1040,0,0.361354,"tions; ST: pseudo-sentences) on WMT En⇒De newstest2019 and newstest2020. certainty. Another interesting finding is that using the pseudo-sentences outperforms using the manual translations (Rows 4 vs. 2, 5 vs. 3). One possible reason is that the T RANSFORMER - BIG model to construct the pseudo-sentences was trained on the whole WMT19 En-De data that contains the heldout data, which serves as self-training to decently improve the supervised baseline (He et al., 2019). Comparison with Related Work. We compared our sampling approach with two related works, i.e., difficult word by frequency (DWF, Fadaee and Monz, 2018) and source language model (S RC LM, Lewis, 2010). The former one was proposed for monolingual data selection for back-translation, in which sentences with lowfrequency words were selected to boost the performance of back-translation. The latter one was proposed for in-domain data selection for in-domain language models. Details of the implementation of related work are in Appendix A.3. Table 3 listed the results. For DWF, it brings no improvement over R AND S AMP, indicating that the Table 3: Comparison of the proposed uncertaintybased sampling strategy with related methods on WMT En⇒De newst"
2021.acl-long.221,N19-4007,0,0.0367251,"per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set. Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2 . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019). 2.3 Effect of Uncertain Data First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual sentences for the self-training. To ensure the quality of synthetic parallel data, we trained 2 BLEU+case.mixed+lang.[Task]+numrefs.1 +smooth.exp++test.wmt[Year]+tok.[Tok]+ver sion.1.4.14, Task=en-de/en-zh, Year=19/20, Tok=13a/zh 3 https://githu"
2021.acl-long.221,2020.emnlp-main.76,0,0.0302764,"one reflects the quality of synthetic sentence pairs. We also presented the results of the synthetic target sentences for reference. Details of the linguistic properties are in Appendix A.2. The results are reported in Figure 3. For the length property, we find that monolingual sentences with higher uncertainty are usually longer except for those with excessively high uncertainty (e.g., bin 5). The monolingual sentences in the last data bin noticeably contain more rare words than other bins in Figure 3(b), and the rare words in the sentences pose a great challenge in the NMT training process (Gu et al., 2020). In Figure 3(c), the overall coverage in bin 5 is the lowest among the self-training bins. In contrast, bin 1 with the lowest uncertainty has the highest coverage. These observations suggest that monolingual sentences in bin 1 indeed contain the easiest patterns while 2843 50 50 50 8.5 8.5 8.5 Source Source Source Target TargetTarget 1.001.001.00 Source Source Source Target TargetTarget Source Source Source Target TargetTarget 0.950.950.95 25 25 25 8.0 8.0 8.0 0.900.900.90 0 0 0 3Uncertainty UncertaintyDistribution Distribution 1 1 21 2 32 3 43 4 54 5 7.5 7.5 7.5 1 1 21 2 32 3 43 4 54 5 5 bin"
2021.acl-long.221,W11-2123,0,0.0291568,"Missing"
2021.acl-long.221,2020.findings-emnlp.435,1,0.846505,"lity. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.1 1 Introduction Leveraging large-scale unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020"
2021.acl-long.221,2020.emnlp-main.176,1,0.887935,"lity. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.1 1 Introduction Leveraging large-scale unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020"
2021.acl-long.221,D16-1139,0,0.0227609,"ct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied. Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al., 2016). Related work on computer 2840 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2840–2850 August 1–6, 2021. ©2021 Association for Computational Linguistics vision also reveals that easy patterns in unlabeled data with the deterministic prediction may not provide"
2021.acl-long.221,W04-3250,0,0.579118,"steps with 32K (4096 × 8) tokens per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set. Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2 . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019). 2.3 Effect of Uncertain Data First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual sentences for the self-training. To ensure the quality of synthetic parallel data, we trained 2 BLEU+case.mixed+lang.[Task]+numrefs.1 +smooth.exp++test.wmt[Year]+tok.[Tok]+ver sion.1.4.14, Task=en-de/en-zh"
2021.acl-long.221,P10-2041,0,0.131195,"newstest2020. certainty. Another interesting finding is that using the pseudo-sentences outperforms using the manual translations (Rows 4 vs. 2, 5 vs. 3). One possible reason is that the T RANSFORMER - BIG model to construct the pseudo-sentences was trained on the whole WMT19 En-De data that contains the heldout data, which serves as self-training to decently improve the supervised baseline (He et al., 2019). Comparison with Related Work. We compared our sampling approach with two related works, i.e., difficult word by frequency (DWF, Fadaee and Monz, 2018) and source language model (S RC LM, Lewis, 2010). The former one was proposed for monolingual data selection for back-translation, in which sentences with lowfrequency words were selected to boost the performance of back-translation. The latter one was proposed for in-domain data selection for in-domain language models. Details of the implementation of related work are in Appendix A.3. Table 3 listed the results. For DWF, it brings no improvement over R AND S AMP, indicating that the Table 3: Comparison of the proposed uncertaintybased sampling strategy with related methods on WMT En⇒De newstest2019 and newstest2020. technique developed for"
2021.acl-long.221,2020.tacl-1.47,0,0.0257659,"iao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model t"
2021.acl-long.221,W19-5333,0,0.0111278,"ges. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strate"
2021.acl-long.221,N19-4009,0,0.01743,"yj |xi ). yj ∈Ab (xi ) (4) 2.2 Experimental Setup Data. We conducted experiments on two large-scale benchmark translation datasets, i.e., WMT English⇒German (En⇒De) and WMT English⇒Chinese (En⇒Zh). The authentic parallel data for the two tasks consists of about 36.8M and 22.1M sentence pairs, respectively. The monolingual data we used is from newscrawl released by WMT2020. We combined the Model. We chose the state-of-the-art T RANS FORMER (Vaswani et al., 2017) network as our model, which consists of an encoder of 6 layers and a decoder of 6 layers. We adopted the open-source toolkit Fairseq (Ott et al., 2019) to implement the model. We used the T RANSFORMER -BASE model for preliminary experiments (§2.3) and the constrained scenario (§3.2) for efficiency. For the unconstrained scenario (§3.3), we adopted the T RANSFORMER -B IG model. Results on these models with different capacities can also reflect the robustness of our approach. For the T RANSFORMER BASE model, we trained it for 150K steps with 32K (4096 × 8) tokens per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16"
2021.acl-long.221,P02-1040,0,0.109387,"scenario (§3.3), we adopted the T RANSFORMER -B IG model. Results on these models with different capacities can also reflect the robustness of our approach. For the T RANSFORMER BASE model, we trained it for 150K steps with 32K (4096 × 8) tokens per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set. Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2 . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019). 2.3 Effect of Uncertain Data First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual s"
2021.acl-long.221,N19-1119,0,0.0226628,"These results corroborate with prior studies (Chang et al., 2017; Mukherjee and Awadallah, 2020) such that learning on certain examples brings little gain while on the excessively uncertain examples may also hurt the model training. 2.4 Linguistic Properties of Uncertain Data We further analyzed the differences between the monolingual sentences with varied uncertainty to gain a deeper understanding of the uncertain data. Specifically, we performed linguistic analysis on the five data bins in terms of three properties: 1) sentence length that counts the tokens in the sentence, 2) word rarity (Platanios et al., 2019) that measures the frequency of words in a sentence with a higher value indicating a more rare sentence, and 3) translation coverage (Khadivi and Ney, 2005) that measures the ratio of source words being aligned with any target words. The first two reflect the properties of monolingual sentences while the last one reflects the quality of synthetic sentence pairs. We also presented the results of the synthetic target sentences for reference. Details of the linguistic properties are in Appendix A.2. The results are reported in Figure 3. For the length property, we find that monolingual sentences"
2021.acl-long.221,W18-6319,0,0.016797,"-B IG model. Results on these models with different capacities can also reflect the robustness of our approach. For the T RANSFORMER BASE model, we trained it for 150K steps with 32K (4096 × 8) tokens per batch. For the T RANSFORMER -B IG model, we trained it for 30K steps with 460K (3600 × 128) tokens per batch with the cosine learning rate schedule (Wu et al., 2019a). We used 16 Nvidia V100 GPUs to conduct the experiments and selected the final model by the best perplexity on the validation set. Evaluation. We evaluated the models by BLEU score (Papineni et al., 2002) computed by SacreBLEU (Post, 2018)2 . For the En⇒Zh task, we added the option --tok zh to SacreBLEU. We measured the statistical significance of improvement with paired bootstrap resampling (Koehn, 2004) using compare-mt3 (Neubig et al., 2019). 2.3 Effect of Uncertain Data First of all, we investigated the effect of monolingual data uncertainty on the self-training performance in NMT. We conducted the preliminary experiments on the WMT En⇒De dataset with the T RANSFORMER -BASE model. We sampled 8M bilingual sentence pairs from the authentic parallel data and randomly sampled 40M monolingual sentences for the self-training. To"
2021.acl-long.221,P16-1009,0,0.652024,"le unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data aug"
2021.acl-long.221,P16-1162,0,0.714753,"le unlabeled data has become an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data aug"
2021.acl-long.221,2020.wmt-1.30,0,0.0364461,"Missing"
2021.acl-long.221,P19-1177,0,0.014441,"pectively. Generally, a high H(Y|X = x) denotes that a source sentence x would have more possible translation candidates. Equation (2) estimates the translation uncertainty of a source sentence with all possible translation candidates in the parallel corpus. It can not be directly applied to the sentences in monolingual data due to the lack of corresponding translation candidates. One potential solution to the problem is utilizing a trained model to generate multiple translation candidates. However, generation may lead to bias estimation due to the generation diversity issue (Li et al., 2016; Shu et al., 2019). More importantly, generation is extremely time-consuming for large-scale monolingual data. Monolingual Uncertainty. To address the problem, we modified Equation (2) to reflect the uncertainty of monolingual sentences. We estimate the target word distribution conditioned on each source word based on the authentic parallel corpus, and then use the distribution to measure the translation uncertainty of the monolingual example. Specifically, we measure the uncertainty of monolingual sentences based on the bilingual dictionary. 2841 37 38 newscrawl data from year 2011 to 2019 for the English mono"
2021.acl-long.221,2020.acl-main.252,0,0.0210192,"; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “t"
2021.acl-long.221,P16-1008,1,0.892962,"Missing"
2021.acl-long.221,D19-1073,0,0.0341507,"21). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied. Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al., 2016). Related work on computer 2840 Proceedings of the 59th Annu"
2021.acl-long.221,2020.acl-main.278,1,0.895865,"Missing"
2021.acl-long.221,2020.emnlp-main.216,0,0.0635692,"Missing"
2021.acl-long.221,D19-1430,0,0.0751742,"; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization (Wu et al., 2019b; Wang et al., 2019) in the last two steps can boost the self-training performance significantly. However, how to efficiently and effectively sample the subset from the large-scale monolingual data in the first step has not been well studied. Intuitively, self-training simplifies the complexity of generated target sentences (Kim and Rush, 2016; Zhou et al., 2019; Jiao et al., 2020b), and easy patterns in monolingual sentences with deterministic translations may not provide additional gains over the self-training “teacher” model (Shrivastava et al., 2016). Related work on computer 2840 Proceed"
2021.acl-long.221,2020.wmt-1.34,1,0.766284,"oaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, self-training is performed in three steps: (1) randomly sample a subset from the large-scale monolingual data; (2) use a “teacher” NMT model to translate the subset data into the target language to construct the synthetic parallel data; (3) combine the synthetic and authentic parallel data to train a “student” NMT model. Recent studies have shown that synthetic data manipulation (Edunov et al., 2018; Caswell et al., 2019) and training strategy optimization ("
2021.acl-long.221,D16-1160,0,0.321979,"come an effective approach for improving the performance of natural language processing (NLP) models (Devlin et al., 2019; Brown et al., 2020; Jiao et al., 2020a). As for neural machine translation ∗ Work was mainly done when Wenxiang Jiao was interning at Tencent AI Lab. 1 The source code is available at https://github. com/wxjiao/UncSamp (NMT), compared to the parallel data, the monolingual data is available in large quantities for many languages. Several approaches on boosting the NMT performance with the monolingual data have been proposed, e.g., data augmentation (Sennrich et al., 2016a; Zhang and Zong, 2016), semisupervised training (Cheng et al., 2016; Zhang et al., 2018; Cai et al., 2021), pre-training (Siddhant et al., 2020; Liu et al., 2020). Among them, data augmentation with the synthetic parallel data (Sennrich et al., 2016a; Edunov et al., 2018) is the most widely used approach due to its simple and effective implementation. It has been a de-facto standard in developing the large-scale NMT systems (Hassan et al., 2018; Ng et al., 2019; Wu et al., 2020; Huang et al., 2021). Self-training (Zhang and Zong, 2016) is one of the most commonly used approaches for data augmentation. Generally, se"
2021.acl-long.221,2020.acl-main.620,0,0.0581449,"Missing"
2021.acl-long.370,E14-2007,0,0.0614733,"Missing"
2021.acl-long.370,J09-1002,0,0.119679,"Missing"
2021.acl-long.370,N16-1148,0,0.0142305,"ient interaction ways of CAT have emerged (Vasconcellos and Le´on, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020). Among those different methods, the autocompletion is the most related to our work. Therefore, we will first describe previous works in both sentence-level and word-level autocompletion, then show the relation to other tasks and scenarios. Sentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxe"
2021.acl-long.370,P05-1033,0,0.0842133,"part is the completion generated by MT system. Both (b) and (c) are word-level autocompletion, underlined text “sp” is the human typed characters and the words in the rounded rectangles are word-level autocompletion candidates. Introduction Machine translation (MT) has witnessed great advancements with the emergence of neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT"
2021.acl-long.370,N19-1423,0,0.0497258,"h decomposes the whole word autocompletion process into two parts: model the distribution of the target word w based on the source sequence x and the translation context c, and find the most possible word w based on the distribution and human typed sequence s. (2) where h is the representation of [MASK], φ is a linear network that projects the hidden representation h to a vector with dimension of target vocabulary size V , and softmax(d)[w] takes the component regarding to w after the softmax operation over a vector d ∈ RV . Inspired by the attention-based architectures (Vaswani et al., 2017; Devlin et al., 2019)3 , we 3 Because the use of attention-based models has become ubiquitous recently, we omit an exhaustive background de4795 Bidirectional Masked Attention Key / Value [SOS] the aircraft [MASK] ... rapidly [EOS] ... &lt;latexit sha1_base64=&quot;cXNtnBINXcmwhkwKN8rhFMu8CHE=&quot;&gt;AAAB7XicbVDLSsNAFL3xWeur6tLNYBFclUQUXRbduKxgH9CGMplO2rGTSZi5EUroP7hxoYhb/8edf+OkzUJbDwwczrmHufcEiRQGXffbWVldW9/YLG2Vt3d29/YrB4ctE6ea8SaLZaw7ATVcCsWbKFDyTqI5jQLJ28H4NvfbT1wbEasHnCTcj+hQiVAwilZq9QYxmnK/UnVr7gxkmXgFqUKBRr/yZYMsjbhCJqkxXc9N0M+oRsEkn5Z7qeEJZWM65F1LFY248bPZtlNyapUBCWNtn0IyU38nMhoZM4kCOxlRHJlFLxf/87ophtd+JlSSIlds/lGYSoIxyU"
2021.acl-long.370,P19-1294,0,0.0125111,"tems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD. Word-level Autocompletion Word-level autocompletion for CAT is less studied than sentencelevel autocompletion. Langlais et al. (2000); Santy et al. (2019) consider to complete a target word based on human typed characters and a translation prefix. But they require the target word to be t"
2021.acl-long.370,D14-1130,0,0.260663,"statistical machine translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT technologies (such as translation memory, terminology management, sample sentence search, etc.), autocompletion plays an important role in a CAT system in enhancing translation efficiency. Autocompletion suggests translation results according to the text pieces provided by human translators. We note two limitations in previous research on the topic of autocompletion for CAT. First, most of previous studies aim to save human efforts by sentence-level autocompletion (Figure 1 a). Nevertheless, word-level autocompletion (Figure 1 b 479"
2021.acl-long.370,N18-2081,0,0.0245608,"Missing"
2021.acl-long.370,P17-1141,0,0.0583182,"onstruct the first public benchmark to facilitate research in this topic. • We propose a joint training strategy to optimize the model parameters on different types of contexts together. 2 2 Related Work Computer-aided translation (CAT) is a widely used practice when using MT technology in the industry. 2 This approach has been implemented into a humanmachine interactive translation system TranSmart (Huang et al., 2021) at www.transmart.qq.com. As the the MT systems advanced and improved, various efficient interaction ways of CAT have emerged (Vasconcellos and Le´on, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020). Among those different methods, the autocompletion is the most related to our work. Therefore, we will first describe previous works in both sentence-level and word-level autocompletion, then show the relation to other tasks and scenarios. Sentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT syst"
2021.acl-long.370,N19-1090,0,0.0330547,"Missing"
2021.acl-long.370,P18-4024,0,0.0267046,"the proposed word-level autocompletion is more general and can be applied to real-world scenarios such as post-editing (Vasconcellos and Le´on, 1985; 4793 Green et al., 2013) and LCD, where human translators need to input some words (corrections or constraints). Huang et al. (2015) propose a method to predict a target word based on human typed characters, however, this method only uses the source side information and does not consider translation context, leading to limited performance compared with our work. Others Our work may also be related to previous works in input method editors (IME) (Huang et al., 2018; Lee et al., 2007). However, they are in the monolingual setting and not capable of using the useful multilingual information. 3 Task and Benchmark In this section, we first describe why we need wordlevel autocompletion in real-world CAT scenarios. We then present the details of the GWLAN task and the construction of benchmark. Why GWLAN? Word level autocompletion is beneficial for improving input efficiency (Langlais et al., 2000). Previous works assume that the translation context should be a prefix and the desired word is next to the prefix as shown in Figure 1 (b), where the context is “W"
2021.acl-long.370,P19-1607,0,0.0132864,"(IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as"
2021.acl-long.370,2016.amta-researchers.9,0,0.144968,"translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT technologies (such as translation memory, terminology management, sample sentence search, etc.), autocompletion plays an important role in a CAT system in enhancing translation efficiency. Autocompletion suggests translation results according to the text pieces provided by human translators. We note two limitations in previous research on the topic of autocompletion for CAT. First, most of previous studies aim to save human efforts by sentence-level autocompletion (Figure 1 a). Nevertheless, word-level autocompletion (Figure 1 b 4792 Proceedings of the 59th"
2021.acl-long.370,P09-5002,0,0.109029,"Missing"
2021.acl-long.370,N03-1017,0,0.0432236,"ion, where the gray part is the completion generated by MT system. Both (b) and (c) are word-level autocompletion, underlined text “sp” is the human typed characters and the words in the rounded rectangles are word-level autocompletion candidates. Introduction Machine translation (MT) has witnessed great advancements with the emergence of neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), which is able to produce much higher quality translation results than statistical machine translation (SMT) models (Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019)."
2021.acl-long.370,W00-0507,0,0.365678,"letes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD. Word-level Autocompletion Word-level autocompletion for CAT is less studied than sentencelevel autocompletion. Langlais et al. (2000); Santy et al. (2019) consider to complete a target word based on human typed characters and a translation prefix. But they require the target word to be the next word of the translation prefix, which limits its application. In contrast, in our work the proposed word-level autocompletion is more general and can be applied to real-world scenarios such as post-editing (Vasconcellos and Le´on, 1985; 4793 Green et al., 2013) and LCD, where human translators need to input some words (corrections or constraints). Huang et al. (2015) propose a method to predict a target word based on human typed char"
2021.acl-long.370,N18-1119,0,0.0219315,"T focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 202"
2021.acl-long.370,D19-3018,0,0.0897867,"(Koehn et al., 2003; Chiang, 2005; Koehn, 1 The information of benchmark datasets is in https: //github.com/ghrua/gwlan 2009). In spite of this, MT systems cannot replace human translators, especially in the scenarios with rigorous translation quality requirements (e.g., translating product manuals, patent documents, government policies, and other official documents). Therefore, how to leverage the pros of MT systems to help human translators, namely, Computer-aided translation (CAT), attracts the attention of researchers (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019). Among all CAT technologies (such as translation memory, terminology management, sample sentence search, etc.), autocompletion plays an important role in a CAT system in enhancing translation efficiency. Autocompletion suggests translation results according to the text pieces provided by human translators. We note two limitations in previous research on the topic of autocompletion for CAT. First, most of previous studies aim to save human efforts by sentence-level autocompletion (Figure 1 a). Nevertheless, word-level autocompletion (Figure 1 b 4792 Proceedings of the 59th Annual Meeting of th"
2021.acl-long.370,N19-1044,0,0.0154186,"chieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than a hard manner as in LCD. Word-level Autocompletion Word-level autocompletion for CAT is less studied than sentencelevel autocompletion. Langlais et al. (2000); Santy et al. (2019) consider to complete a target word based on human typed characters and a translation prefix. But they require the target word to be the next word of the"
2021.acl-long.370,2020.acl-main.325,0,0.0111079,"ve machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its inference is typically less efficient compared with the standard NMT. Therefore, other works propose efficient methods (Li et al., 2020; Song et al., 2019) by using lexical constraints in a soft manner rather than"
2021.acl-long.370,J85-2003,0,0.645723,"Missing"
2021.acl-long.370,2020.aacl-main.1,1,0.736288,"cilitate research in this topic. • We propose a joint training strategy to optimize the model parameters on different types of contexts together. 2 2 Related Work Computer-aided translation (CAT) is a widely used practice when using MT technology in the industry. 2 This approach has been implemented into a humanmachine interactive translation system TranSmart (Huang et al., 2021) at www.transmart.qq.com. As the the MT systems advanced and improved, various efficient interaction ways of CAT have emerged (Vasconcellos and Le´on, 1985; Green et al., 2014; Hokamp and Liu, 2017; Weng et al., 2019; Wang et al., 2020). Among those different methods, the autocompletion is the most related to our work. Therefore, we will first describe previous works in both sentence-level and word-level autocompletion, then show the relation to other tasks and scenarios. Sentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translati"
2021.acl-long.370,P16-1007,0,0.0160925,"then show the relation to other tasks and scenarios. Sentence-level Autocompletion Most of previous work in autocompletion for CAT focus on sentence-level completion. A common use case in this line is interactive machine translation (IMT) (Green et al., 2014; Cheng et al., 2016; Peris et al., 2017; Knowles and Koehn, 2016; Santy et al., 2019). IMT systems utilize MT systems to complete the rest of a translation after human translators editing a prefix translation (Alabau et al., 2014; Zhao et al., 2020). For most IMT systems, the core to achieve this completion is prefix-constrained decoding (Wuebker et al., 2016). Another sentence-level autocompletion method, lexically constrained decoding (LCD) (Hokamp and Liu, 2017; Post and Vilar, 2018), recently attracts lots of attention (Hasler et al., 2018; Susanto et al., 2020; Kajiwara, 2019). Compared with IMT, LCD relaxes the constraints provided by human translators from prefixes to general forms: LCD completes a translation based on some unordered words (i.e., lexical constraints), which are not necessary to be continuous (Hokamp and Liu, 2017; Hu et al., 2019; Dinu et al., 2019; Song et al., 2019). Although it does not need additional training, its infer"
2021.acl-long.385,2020.acl-main.81,0,0.0130235,"chanism into seq2seq framework (Wang et al., 2019). FASPell incorporates BERT into the seq2seq for better performance (Hong et al., 2019). SoftMask-BERT firstly conducts error detection using a GRU-based model and then incorporating the predicted results with the BERT model using a soft-masked strategy (Zhang et al., 2020b). Note that the best results of SoftMask-BERT are obtained after pre-training on a large-scale dataset with 500M paired samples. SpellGCN proposes to incorporate phonological and visual similarity knowledge into language models via a specialized graph convolutional network (Cheng et al., 2020). Chunk proposes a chunk-based decoding method with global optimization to correct single character and multi-character word typos in a unified framework (Bao et al., 2020). We also implement some classical methods for comparison and ablation analysis, especially for the VarLen correction problem. Transformer-s2s is the typical Transformer-based seq2seq framework for sequence prediction (Vaswani et al., 2017). GPT2-finetune is also a sequence generation framework fine-tuned based on a pre-trained Chinese GPT2 model4 (Radford et al., 2019; Li, 2020). BERT-finetune is just fine-tune the Chinese"
2021.acl-long.385,W12-2006,0,0.0230168,"ei today! 〞 Type III ⠨ㄐ晝ⴷ〞ℯ槗漈I very feel happy today! Figure 1: Illustration for the three types of operations to correct the grammatical errors: Type I-substitution; Type II-deletion and insertion; Type III-local paraphrasing. in many natural language processing scenarios such as writing assistant (Ghufron and Rosyida, 2018; Napoles et al., 2017; Omelianchuk et al., 2020), search engine (Martins and Silva, 2004; Gao et al., 2010; Duan and Hsu, 2011), speech recognition systems (Karat et al., 1999; Wang et al., 2020a; Kubis et al., 2020), etc. Grammatical errors may appear in all languages (Dale et al., 2012; Xing et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015; Bryant et al., 2019), in this paper, we only focus to tackle the problem of Chinese Grammatical Error Correction (CGEC) (Chang, 1995). Introduction Grammatical Error Correction (GEC) aims to automatically detect and correct the grammatical errors that can be found in a sentence (Wang et al., 2020c). It is a crucial and essential application task 1 Code: https://github.com/lipiji/TtT ⠨ㄐ〞ℯ柝摾槗漈I feel fly long happy today! We investigate the problem of CGEC and the related corpora from SIGHAN (Tseng et al., 2015) and NLPCC (Zhao et a"
2021.acl-long.385,N19-1423,0,0.141376,"d be inserted into the original sentence X. Then, we will pad the special symbol &lt;mask&gt; to the tail of X to indicate that these positions possibly can be translated into some new real tokens: X = (x1 , x2 , x3 , &lt;eos&gt;, &lt;mask&gt;, &lt;mask&gt;). 2.3 Bidirectional Semantic Modeling Transformer layers (Vaswani et al., 2017) are particularly well suited to be employed to conduct the bidirectional semantic modeling and bottom-to-up information conveying. As shown in Figure 3, after preparing the input samples, an embedding layer and a stack of Transformer layers initialized with a pre-trained Chinese BERT (Devlin et al., 2019) are followed to conduct the semantic modeling. Specifically, for the input, we first obtain the representations by summing the word embeddings with the positional embeddings: H0t = Ewt + Ept Direct Prediction The objective of our model is to translate the input sentence X which contains grammatical errors into a correct sentence Y . Then, since we have obtained the sequence representation vectors HL , we can directly add a softmax layer to predict the results, just similar to the methods used in non-autoregressive neural machine translation (Gu and Kong, 2020) and BERT-based finetuning framew"
2021.acl-long.385,2020.findings-emnlp.184,0,0.0262119,"r detection using a GRU-based model and then incorporating the predicted results with the BERT model using a soft-masked strategy (Zhang et al., 2020b). Note that the best results of SoftMask-BERT are obtained after pre-training on a large-scale dataset with 500M paired samples. SpellGCN proposes to incorporate phonological and visual similarity knowledge into language models via a specialized graph convolutional network (Cheng et al., 2020). Chunk proposes a chunk-based decoding method with global optimization to correct single character and multi-character word typos in a unified framework (Bao et al., 2020). We also implement some classical methods for comparison and ablation analysis, especially for the VarLen correction problem. Transformer-s2s is the typical Transformer-based seq2seq framework for sequence prediction (Vaswani et al., 2017). GPT2-finetune is also a sequence generation framework fine-tuned based on a pre-trained Chinese GPT2 model4 (Radford et al., 2019; Li, 2020). BERT-finetune is just fine-tune the Chinese BERT model on the CGEC corpus directly. Beam search decoding strategy is employed to con4 4978 https://github.com/lipiji/Guyu Model NTOU (2015) NCTU-NTUT (2015) HanSpeller+"
2021.acl-long.385,2021.findings-acl.11,0,0.0401727,"Missing"
2021.acl-long.385,P16-1154,0,0.0372816,"q2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020). Seq2seq based translation models are easily to be trained and can handle all types of correcting operations above mentioned. However, considering the exposure bias issue (Ranzato et al., 2016; Zhang et al., 2019), the generated results usually suffer from the phenomenon of hallucination (Nie et al., 2019; Maynez et al., 2020) and cannot be faithful to the source text, even though copy mechanisms (Gu et al., 2016) are incorporated (Wang et al., 2019). Therefore, Omelianchuk et al. (2020) and Liang et al. (2020) propose to purely employ tagging to conduct the problem of GEC instead of generation. All correcting operations such as deletion, insertion, and substitution can be guided by the predicted tags. Nevertheless, the pure tagging strategy requires to extend the vocabulary V to about three times by adding “insertion-” and “substitution-” prefixes to the original tokens (e.g., “insertion-good”, “substitutionpaper”) which decrease the computing efficiency dramatically. Moreover, the pure tagging framew"
2021.acl-long.385,D19-5522,0,0.275682,"the predicted tags. Nevertheless, the pure tagging strategy requires to extend the vocabulary V to about three times by adding “insertion-” and “substitution-” prefixes to the original tokens (e.g., “insertion-good”, “substitutionpaper”) which decrease the computing efficiency dramatically. Moreover, the pure tagging framework needs to conduct multi-pass prediction until no more operations are predicted, which is inefficient and less elegant. Recently, many researchers fine-tune the pre-trained language models such as BERT on the task of CGEC and obtain reasonable results (Zhao et al., 2019; Hong et al., 2019; Zhang et al., 2020b). However, limited by the BERT framework, most of them can only address the fixed-length correcting scenarios and cannot conduct deletion, insertion, and local paraphrasing operations flexibly. Moreover, during the investigations, we also observe an obvious but crucial phenomenon for CGEC that most words in a sentence are correct and need not to be changed. This phenomenon is depicted in Figure 2, where the operation flow is from the bottom tail to the up tail. Grey dash lines represent the “Keep” operations and the red solid lines indicate those three types of correcting"
2021.acl-long.385,2020.acl-main.391,0,0.0182695,", over the past few years, although a number of methods have been developed to deal with the problem of CGEC, some crucial and essential aspects are still uncovered. Generally, sequence translation and sequence tagging are the two most typical technical paradigms to tackle the problem of CGEC. Benefiting from the development of neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), attention-based seq2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020). Seq2seq based translation models are easily to be trained and can handle all types of correcting operations above mentioned. However, considering the exposure bias issue (Ranzato et al., 2016; Zhang et al., 2019), the generated results usually suffer from the phenomenon of hallucination (Nie et al., 2019; Maynez et al., 2020) and cannot be faithful to the source text, even though copy mechanisms (Gu et al., 2016) are incorporated (Wang et al., 2019). Therefore, Omelianchuk et al. (2020) and Liang et al. (2020) propose to purely employ tagging to conduct the problem of GEC instead of generati"
2021.acl-long.385,2020.nlptea-1.8,0,0.0326008,"translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020). Seq2seq based translation models are easily to be trained and can handle all types of correcting operations above mentioned. However, considering the exposure bias issue (Ranzato et al., 2016; Zhang et al., 2019), the generated results usually suffer from the phenomenon of hallucination (Nie et al., 2019; Maynez et al., 2020) and cannot be faithful to the source text, even though copy mechanisms (Gu et al., 2016) are incorporated (Wang et al., 2019). Therefore, Omelianchuk et al. (2020) and Liang et al. (2020) propose to purely employ tagging to conduct the problem of GEC instead of generation. All correcting operations such as deletion, insertion, and substitution can be guided by the predicted tags. Nevertheless, the pure tagging strategy requires to extend the vocabulary V to about three times by adding “insertion-” and “substitution-” prefixes to the original tokens (e.g., “insertion-good”, “substitutionpaper”) which decrease the computing efficiency dramatically. Moreover, the pure tagging framework needs to conduct multi-pass prediction until no more operations are predicted, which is ineffic"
2021.acl-long.385,2020.acl-main.173,0,0.100939,"neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), attention-based seq2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020). Seq2seq based translation models are easily to be trained and can handle all types of correcting operations above mentioned. However, considering the exposure bias issue (Ranzato et al., 2016; Zhang et al., 2019), the generated results usually suffer from the phenomenon of hallucination (Nie et al., 2019; Maynez et al., 2020) and cannot be faithful to the source text, even though copy mechanisms (Gu et al., 2016) are incorporated (Wang et al., 2019). Therefore, Omelianchuk et al. (2020) and Liang et al. (2020) propose to purely employ tagging to conduct the problem of GEC instead of generation. All correcting operations such as deletion, insertion, and substitution can be guided by the predicted tags. Nevertheless, the pure tagging strategy requires to extend the vocabulary V to about three times by adding “insertion-” and “substitution-” prefixes to the original tokens (e.g., “insertion-good”, “substitutionpaper”"
2021.acl-long.385,E17-2037,0,0.0608152,"Missing"
2021.acl-long.385,W14-1701,0,0.0261018,"feel happy today! Figure 1: Illustration for the three types of operations to correct the grammatical errors: Type I-substitution; Type II-deletion and insertion; Type III-local paraphrasing. in many natural language processing scenarios such as writing assistant (Ghufron and Rosyida, 2018; Napoles et al., 2017; Omelianchuk et al., 2020), search engine (Martins and Silva, 2004; Gao et al., 2010; Duan and Hsu, 2011), speech recognition systems (Karat et al., 1999; Wang et al., 2020a; Kubis et al., 2020), etc. Grammatical errors may appear in all languages (Dale et al., 2012; Xing et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015; Bryant et al., 2019), in this paper, we only focus to tackle the problem of Chinese Grammatical Error Correction (CGEC) (Chang, 1995). Introduction Grammatical Error Correction (GEC) aims to automatically detect and correct the grammatical errors that can be found in a sentence (Wang et al., 2020c). It is a crucial and essential application task 1 Code: https://github.com/lipiji/TtT ⠨ㄐ〞ℯ柝摾槗漈I feel fly long happy today! We investigate the problem of CGEC and the related corpora from SIGHAN (Tseng et al., 2015) and NLPCC (Zhao et al., 2018) carefully, and we conclude"
2021.acl-long.385,P19-1256,0,0.0244286,"the development of neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), attention-based seq2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020). Seq2seq based translation models are easily to be trained and can handle all types of correcting operations above mentioned. However, considering the exposure bias issue (Ranzato et al., 2016; Zhang et al., 2019), the generated results usually suffer from the phenomenon of hallucination (Nie et al., 2019; Maynez et al., 2020) and cannot be faithful to the source text, even though copy mechanisms (Gu et al., 2016) are incorporated (Wang et al., 2019). Therefore, Omelianchuk et al. (2020) and Liang et al. (2020) propose to purely employ tagging to conduct the problem of GEC instead of generation. All correcting operations such as deletion, insertion, and substitution can be guided by the predicted tags. Nevertheless, the pure tagging strategy requires to extend the vocabulary V to about three times by adding “insertion-” and “substitution-” prefixes to the original tokens (e.g., “insertion-good"
2021.acl-long.385,2020.bea-1.16,0,0.0836859,"Missing"
2021.acl-long.385,W15-3204,0,0.0439627,"Figure 1: Illustration for the three types of operations to correct the grammatical errors: Type I-substitution; Type II-deletion and insertion; Type III-local paraphrasing. in many natural language processing scenarios such as writing assistant (Ghufron and Rosyida, 2018; Napoles et al., 2017; Omelianchuk et al., 2020), search engine (Martins and Silva, 2004; Gao et al., 2010; Duan and Hsu, 2011), speech recognition systems (Karat et al., 1999; Wang et al., 2020a; Kubis et al., 2020), etc. Grammatical errors may appear in all languages (Dale et al., 2012; Xing et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015; Bryant et al., 2019), in this paper, we only focus to tackle the problem of Chinese Grammatical Error Correction (CGEC) (Chang, 1995). Introduction Grammatical Error Correction (GEC) aims to automatically detect and correct the grammatical errors that can be found in a sentence (Wang et al., 2020c). It is a crucial and essential application task 1 Code: https://github.com/lipiji/TtT ⠨ㄐ〞ℯ柝摾槗漈I feel fly long happy today! We investigate the problem of CGEC and the related corpora from SIGHAN (Tseng et al., 2015) and NLPCC (Zhao et al., 2018) carefully, and we conclude that the grammatical err"
2021.acl-long.385,W96-0108,0,0.511219,"ble 1. SIGHAN15 (Tseng et al., 2015)2 This is a benchmark dataset for the evaluation of CGEC and it contains 2,339 samples for training and 1,100 samples for testing. As did in some typical previous works (Wang et al., 2019; Zhang et al., 2020b), we also use the SIGHAN15 testset as the benchmark dataset to evaluate the performance of our models as well as the baseline methods in fixed-length (FixLen) error correction settings. HybirdSet (Wang et al., 2018)3 It is a newly released dataset constructed according to a prepared confusion set based on the results of ASR (Yu and Deng, 2014) and OCR (Tong and Evans, 1996). This dataset contains about 270k paired samples and it is also a FixLen dataset. TtTSet Considering that datasets of SIGHAN15 and HybirdSet are all FixLen type datasets, in order to demonstrate the capability of our model TiT on the scenario of Variable-Length (VarLen) CGEC, based on the corpus of HybirdSet, we 2 http://ir.itc.ntnu.edu.tw/lre/ sighan8csc.html 3 https://github.com/wdimmy/ Automatic-Corpus-Generation Comparison Methods We compare the performance of TtT with several strong baseline methods on both FixLen and VarLen settings. NTOU employs n-gram language model with a reranking s"
2021.acl-long.385,D18-1273,0,0.0915139,"perations because they may change the sentence length. However, over the past few years, although a number of methods have been developed to deal with the problem of CGEC, some crucial and essential aspects are still uncovered. Generally, sequence translation and sequence tagging are the two most typical technical paradigms to tackle the problem of CGEC. Benefiting from the development of neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), attention-based seq2seq encoder-decoder frameworks have been introduced to address the CGEC problem in a sequence translation manner (Wang et al., 2018; Ge et al., 2018; Wang et al., 2019, 2020b; Kaneko et al., 2020). Seq2seq based translation models are easily to be trained and can handle all types of correcting operations above mentioned. However, considering the exposure bias issue (Ranzato et al., 2016; Zhang et al., 2019), the generated results usually suffer from the phenomenon of hallucination (Nie et al., 2019; Maynez et al., 2020) and cannot be faithful to the source text, even though copy mechanisms (Gu et al., 2016) are incorporated (Wang et al., 2019). Therefore, Omelianchuk et al. (2020) and Liang et al. (2020) propose to purely"
2021.acl-long.385,P19-1578,0,0.131555,"For complex correcting cases which require deletion, insertion, and local paraphrasing, the performance is unacceptable. This inferior performance phenomenon is also discussed in the tasks of non-autoregressive neural machine translation (Gu and Kong, 2020). One of the essential reasons causing the inferior performance is that the dependency information among the neighbour tokens are missed. Therefore, dependency modeling should be called back to improve the performance of generation. Naturally, linear-chain CRF (Lafferty et al., 2001) is introduced to fix this issue, and luckily, Sun et al. (2019) also employ CRF to address the problem of non-autoregressive sequence generation, which inspired us a lot. Dependency Modeling via CRF Then given the input sequence X, under the CRF framework, the likelihood of the target sequence Y with length T 0 4976 And the loss function Lcrf for CRF-based dependency modeling is: is constructed as: Pcrf (Y |X) = 0 0 T T X X 1 exp s(yt ) + t(yt−1 , yt ) Z(X) t=1 ! Lcrf = − log Pcrf (Y |X) t=2 (5) where Z(X) is the normalizing factor and s(yt ) represents the label score of y at position t, which can be obtained from the predicted logit vector st ∈ R|V |fro"
2021.acl-long.385,2020.aacl-main.20,0,0.031852,"of error Detection and Correction1 . 1 晝 ⴷ always happy when Type II ⠨ㄐℯ晝ⴷⴷ槗漈II am come to Fei today! 〞 Type III ⠨ㄐ晝ⴷ〞ℯ槗漈I very feel happy today! Figure 1: Illustration for the three types of operations to correct the grammatical errors: Type I-substitution; Type II-deletion and insertion; Type III-local paraphrasing. in many natural language processing scenarios such as writing assistant (Ghufron and Rosyida, 2018; Napoles et al., 2017; Omelianchuk et al., 2020), search engine (Martins and Silva, 2004; Gao et al., 2010; Duan and Hsu, 2011), speech recognition systems (Karat et al., 1999; Wang et al., 2020a; Kubis et al., 2020), etc. Grammatical errors may appear in all languages (Dale et al., 2012; Xing et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015; Bryant et al., 2019), in this paper, we only focus to tackle the problem of Chinese Grammatical Error Correction (CGEC) (Chang, 1995). Introduction Grammatical Error Correction (GEC) aims to automatically detect and correct the grammatical errors that can be found in a sentence (Wang et al., 2020c). It is a crucial and essential application task 1 Code: https://github.com/lipiji/TtT ⠨ㄐ〞ℯ柝摾槗漈I feel fly long happy today! We investigate the"
2021.acl-long.385,W13-3605,0,0.0204632,"I ⠨ㄐ晝ⴷ〞ℯ槗漈I very feel happy today! Figure 1: Illustration for the three types of operations to correct the grammatical errors: Type I-substitution; Type II-deletion and insertion; Type III-local paraphrasing. in many natural language processing scenarios such as writing assistant (Ghufron and Rosyida, 2018; Napoles et al., 2017; Omelianchuk et al., 2020), search engine (Martins and Silva, 2004; Gao et al., 2010; Duan and Hsu, 2011), speech recognition systems (Karat et al., 1999; Wang et al., 2020a; Kubis et al., 2020), etc. Grammatical errors may appear in all languages (Dale et al., 2012; Xing et al., 2013; Ng et al., 2014; Rozovskaya et al., 2015; Bryant et al., 2019), in this paper, we only focus to tackle the problem of Chinese Grammatical Error Correction (CGEC) (Chang, 1995). Introduction Grammatical Error Correction (GEC) aims to automatically detect and correct the grammatical errors that can be found in a sentence (Wang et al., 2020c). It is a crucial and essential application task 1 Code: https://github.com/lipiji/TtT ⠨ㄐ〞ℯ柝摾槗漈I feel fly long happy today! We investigate the problem of CGEC and the related corpora from SIGHAN (Tseng et al., 2015) and NLPCC (Zhao et al., 2018) carefully"
2021.acl-long.385,2020.acl-main.82,0,0.270691,". Nevertheless, the pure tagging strategy requires to extend the vocabulary V to about three times by adding “insertion-” and “substitution-” prefixes to the original tokens (e.g., “insertion-good”, “substitutionpaper”) which decrease the computing efficiency dramatically. Moreover, the pure tagging framework needs to conduct multi-pass prediction until no more operations are predicted, which is inefficient and less elegant. Recently, many researchers fine-tune the pre-trained language models such as BERT on the task of CGEC and obtain reasonable results (Zhao et al., 2019; Hong et al., 2019; Zhang et al., 2020b). However, limited by the BERT framework, most of them can only address the fixed-length correcting scenarios and cannot conduct deletion, insertion, and local paraphrasing operations flexibly. Moreover, during the investigations, we also observe an obvious but crucial phenomenon for CGEC that most words in a sentence are correct and need not to be changed. This phenomenon is depicted in Figure 2, where the operation flow is from the bottom tail to the up tail. Grey dash lines represent the “Keep” operations and the red solid lines indicate those three types of correcting operations mentione"
2021.acl-long.385,W15-3107,0,0.0294957,"n the scenario of Variable-Length (VarLen) CGEC, based on the corpus of HybirdSet, we 2 http://ir.itc.ntnu.edu.tw/lre/ sighan8csc.html 3 https://github.com/wdimmy/ Automatic-Corpus-Generation Comparison Methods We compare the performance of TtT with several strong baseline methods on both FixLen and VarLen settings. NTOU employs n-gram language model with a reranking strategy to conduct prediction (Tseng et al., 2015). NCTU-NTUT also uses CRF to conduct label dependency modeling (Tseng et al., 2015). HanSpeller++ employs Hidden Markov Model with a reranking strategy to conduct the prediction (Zhang et al., 2015). Hybrid utilizes LSTM-based seq2seq framework to conduct generation (Wang et al., 2018) and Confusionset introduces a copy mechanism into seq2seq framework (Wang et al., 2019). FASPell incorporates BERT into the seq2seq for better performance (Hong et al., 2019). SoftMask-BERT firstly conducts error detection using a GRU-based model and then incorporating the predicted results with the BERT model using a soft-masked strategy (Zhang et al., 2020b). Note that the best results of SoftMask-BERT are obtained after pre-training on a large-scale dataset with 500M paired samples. SpellGCN proposes to"
2021.acl-long.385,P19-1426,0,0.063807,"Missing"
2021.acl-long.385,N19-1014,0,0.0979922,"on can be guided by the predicted tags. Nevertheless, the pure tagging strategy requires to extend the vocabulary V to about three times by adding “insertion-” and “substitution-” prefixes to the original tokens (e.g., “insertion-good”, “substitutionpaper”) which decrease the computing efficiency dramatically. Moreover, the pure tagging framework needs to conduct multi-pass prediction until no more operations are predicted, which is inefficient and less elegant. Recently, many researchers fine-tune the pre-trained language models such as BERT on the task of CGEC and obtain reasonable results (Zhao et al., 2019; Hong et al., 2019; Zhang et al., 2020b). However, limited by the BERT framework, most of them can only address the fixed-length correcting scenarios and cannot conduct deletion, insertion, and local paraphrasing operations flexibly. Moreover, during the investigations, we also observe an obvious but crucial phenomenon for CGEC that most words in a sentence are correct and need not to be changed. This phenomenon is depicted in Figure 2, where the operation flow is from the bottom tail to the up tail. Grey dash lines represent the “Keep” operations and the red solid lines indicate those three"
2021.emnlp-main.210,D19-1641,0,0.243407,"representation r mc and type representation r t into a shared space by φ (r mc , A) : r mc → Ar mc θ (r t , B) : r t → Br t , (3) where A ∈ Rds ×2dm and B ∈ Rds ×dt are learnable matrices. The matching score is defined as yt = φ (r mc , A)·θ (r t , B) = (Ar mc )&gt; Br t (4) During training, we match mention m with all the types in Ttrain , so the loss function for m is: Type Hierarchy-Aware (HA) Module In HA module, we use Transformer encoder (Vaswani et al., 2017) with mask-self-attention to capture the hierarchical information for better type representations. Besides, we take the encoder from Lin and Ji (2019) to learn the features of mentions and contexts. Then a similarity function is defined to compute the matching score between a mention and a candidate type based on the context. 2.3.1 Mention-Context Encoder In the mention-context encoder, an entity mention and its context are represented as the weighted sum of their ELMo word representations. Then the mention representation r m and context representation r c are concatenated as the final representation: r mc = r m ⊕ r c , where r m , r c ∈ Rdm , r mc ∈ R2dm , ⊕ denotes concatenation. 2.3.2 Hierarchy-Aware Type Encoder Given a type set T = Ttr"
2021.emnlp-main.210,E17-1075,0,0.0550379,"Missing"
2021.emnlp-main.210,N19-1423,0,0.208547,"e is the surface form of a type, which is a word or a phase, e.g., type name of /organization/corporation is corporation. (ii) Type hierarchy is the ontology structure connecting seen and unseen types. (iii) Background knowledge provides the external prior information that depicts types in detail, e.g., prototypes (Ma et al., 2016) and descriptions (Obeidat et al., 2019). MSF is composed of three modules, with each targeting a specific information source. (i) In the CA (Context-Consistency Aware) module, we measure the context consistency by large-scale pretrained language models, e.g., BERT (Devlin et al., 2019). By masking mentions and predicting the names of ground truth types through finetuning on the data of seen types, CA is expected to measure the context consistency of unseen types more precisely. (ii) In the HA (Type-Hierarchy Aware) module, we use Transformer encoder (Vaswani et al., 2017) to model the hierarchical dependency among types. There have been substantial works exploring type hierarchy in the supervised typing task (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xiong et al., 2019), but only some preliminary research in ZFET (Ma et al., 2016; Zhang et al., 2020b). (iii) In the KA (B"
2021.emnlp-main.210,D19-1488,0,0.0282879,"with regard to macro F1 scores. More importantly, we further discuss the characteristics, merits and demerits of each information source and provide an intuitive understanding of the complementarity among them. Source2: Type Hierarchy Government … Overall Loss Fusion ?????? ?????? ?????? CA HA KA <Northwest and Midway are two of the …, /organization/corporation&gt; Fine-grained entity typing (FET) aims to detect the types of an entity mention given its context (Abhishek et al., 2017; Xu and Barbosa, 2018; Jin et al., 2019). The results of FET benefit lots of downstream tasks (Chen et al., 2020; Hu et al., 2019; Zhang et al., 2020a; Liu et al., 2021; Chu et al., 2020). In many scenarios, the type hierarchy is continuously evolving, which requires newly emerged types to be accounted into FET systems. As a result, zero-shot FET (ZFET) is welcomed to handle the new types which are unseen during training stage (Ma et al., 2016; Ren et al., 2020; Zhang et al., 2020b). The major challenge of ZFET is to build the semantic connections between the seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a bus"
2021.emnlp-main.210,C16-1017,0,0.308687,"ay are two of the …, /organization/corporation&gt; Fine-grained entity typing (FET) aims to detect the types of an entity mention given its context (Abhishek et al., 2017; Xu and Barbosa, 2018; Jin et al., 2019). The results of FET benefit lots of downstream tasks (Chen et al., 2020; Hu et al., 2019; Zhang et al., 2020a; Liu et al., 2021; Chu et al., 2020). In many scenarios, the type hierarchy is continuously evolving, which requires newly emerged types to be accounted into FET systems. As a result, zero-shot FET (ZFET) is welcomed to handle the new types which are unseen during training stage (Ma et al., 2016; Ren et al., 2020; Zhang et al., 2020b). The major challenge of ZFET is to build the semantic connections between the seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a business firm whose articles of incorporation have been approved in some state. Introduction ∗ Corporation Source3: Background Knowledge [Mention] 1 Organization [Context] [Type] Figure 1: Illustration of the proposed multi-source fusion model (MSF). Auxiliary information has been proved to be essential in this regard ("
2021.emnlp-main.210,N19-1087,0,0.0138639,"seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a business firm whose articles of incorporation have been approved in some state. Introduction ∗ Corporation Source3: Background Knowledge [Mention] 1 Organization [Context] [Type] Figure 1: Illustration of the proposed multi-source fusion model (MSF). Auxiliary information has been proved to be essential in this regard (Xian et al., 2019), with a variety of approaches focused on scattered information (Ma et al., 2016; Zhou et al., 2018; Obeidat et al., 2019; Ren et al., 2020; Zhang et al., 2020b). However, the power of auxiliary information has not been sufficiently exploited in existing solutions. Besides, the effects of each information source also remain to be clearly understood. In this paper, we propose a Multi-Source Fusion model (MSF) integrating three kinds of popular auxiliary information for ZFET, i.e., context consistency, type hierarchy, and background knowledge, as illustrated in Figure 1. (i) Context consistency means a correct type should be se2668 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process"
2021.emnlp-main.210,D16-1144,0,0.176978,"score vector from a module for mention m towards all types t ∈ Ttest with x as component. µx and σx denote the mean and standard deviation of the vector x. The final decision score by our fusion model for type t is: scoret = λ1 s0t + λ2 yt0 + λ3 p0t , (11) where λ1 , λ2 , λ3 ≥ 0 are hyper-parameters and λ1 + λ2 + λ3 = 1. 3 3.1 Experimental Setup Datasets and Evaluation Metrics We evaluate our model on two widely-used datasets: {˜ r mc , r˜ tp , r˜ td , r˜ h } = W {r mc , r tp , r td , r h } (6) BBN (Weischedel and Brunstein, 2005) and Wiki 2671 (Ling and Weld, 2012). The version processed by Ren et al. (2016) is adopted for our experiments. Detailed statistics on two datasets are listed in Table 1. We do not use OntoNotes (Gillick et al., 2014) since it is hard to define the name, description and hierarchy for its special type /other. Types of both BBN and Wiki are organized into a 2-level hierarchy. There are 47 types in BBN and 113 types in Wiki. Following Ma et al. (2016); Zhang et al. (2020b), we use the coarse-grained (Level-1) types such as /organization for training (denoted as seen types), while the fine-grained (Level-2) types such as /organization/corporation are reserved for testing (de"
2021.emnlp-main.210,E17-1119,0,0.0290272,"ource. (i) In the CA (Context-Consistency Aware) module, we measure the context consistency by large-scale pretrained language models, e.g., BERT (Devlin et al., 2019). By masking mentions and predicting the names of ground truth types through finetuning on the data of seen types, CA is expected to measure the context consistency of unseen types more precisely. (ii) In the HA (Type-Hierarchy Aware) module, we use Transformer encoder (Vaswani et al., 2017) to model the hierarchical dependency among types. There have been substantial works exploring type hierarchy in the supervised typing task (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xiong et al., 2019), but only some preliminary research in ZFET (Ma et al., 2016; Zhang et al., 2020b). (iii) In the KA (Background-Knowledge Aware) module, we introduce prototypes (Ma et al., 2016) and WordNet descriptions (Miller, 1995) as background knowledge of types. KA is embodied as natural language inference with a translation-based solution to better incorporate knowledge. Extensive experiments are carried out to verify the effectiveness of the proposed fusion model. We also conduct a deep analysis on the characteristics, merits and demerits of each information"
2021.emnlp-main.210,D19-1502,0,0.0474529,"Missing"
2021.emnlp-main.210,I17-1011,0,0.0226177,"in the KA module. Prototypes refer to the carefully selected mentions for a type based on Normalized Point-wise Mutual Information (NPMI), which provide a mention-level summary for types (Ma et al., 2016). Descriptions are queried from WordNet glosses (Miller, 1995) by type names, which provide a brief high-level summary for each type. 2.4.1 Inference from Background Knowledge We hope to infer whether a mention m matches a candidate type t, given the prototypes, type description and the context. In this work, we embody the KA module as natural language inference (NLI) from multiple premises (Lai et al., 2017). An ex1 The initialization details are presented in Appendix A ample is presented in Figure 2, with input the same 2670 Multiple Premises • • • Context-based premise: Northwest and Midway are two of the five airlines with which Budget has agreements. Prototypes-based premise: /organization/corporation has the following prototypes: western_union, … Description-based premise: /organization/corporation denotes a collection of business firms whose articles of incorporation have been approved in some state. where W ∈ Rdw ×2dm . We hope that r˜ mc + r˜ tp + r˜ td ≈ r˜ h when the hypothesis can be i"
2021.emnlp-main.210,D18-1121,0,0.0337611,"Missing"
2021.emnlp-main.210,N19-1084,0,0.0349593,"Missing"
2021.emnlp-main.210,N18-1002,0,0.0124422,"ontext-Consistency Aware) module, we measure the context consistency by large-scale pretrained language models, e.g., BERT (Devlin et al., 2019). By masking mentions and predicting the names of ground truth types through finetuning on the data of seen types, CA is expected to measure the context consistency of unseen types more precisely. (ii) In the HA (Type-Hierarchy Aware) module, we use Transformer encoder (Vaswani et al., 2017) to model the hierarchical dependency among types. There have been substantial works exploring type hierarchy in the supervised typing task (Shimaoka et al., 2017; Xu and Barbosa, 2018; Xiong et al., 2019), but only some preliminary research in ZFET (Ma et al., 2016; Zhang et al., 2020b). (iii) In the KA (Background-Knowledge Aware) module, we introduce prototypes (Ma et al., 2016) and WordNet descriptions (Miller, 1995) as background knowledge of types. KA is embodied as natural language inference with a translation-based solution to better incorporate knowledge. Extensive experiments are carried out to verify the effectiveness of the proposed fusion model. We also conduct a deep analysis on the characteristics, merits and demerits of each information source. We find that,"
2021.emnlp-main.210,2020.coling-main.7,0,0.102233,"acro F1 scores. More importantly, we further discuss the characteristics, merits and demerits of each information source and provide an intuitive understanding of the complementarity among them. Source2: Type Hierarchy Government … Overall Loss Fusion ?????? ?????? ?????? CA HA KA <Northwest and Midway are two of the …, /organization/corporation&gt; Fine-grained entity typing (FET) aims to detect the types of an entity mention given its context (Abhishek et al., 2017; Xu and Barbosa, 2018; Jin et al., 2019). The results of FET benefit lots of downstream tasks (Chen et al., 2020; Hu et al., 2019; Zhang et al., 2020a; Liu et al., 2021; Chu et al., 2020). In many scenarios, the type hierarchy is continuously evolving, which requires newly emerged types to be accounted into FET systems. As a result, zero-shot FET (ZFET) is welcomed to handle the new types which are unseen during training stage (Ma et al., 2016; Ren et al., 2020; Zhang et al., 2020b). The major challenge of ZFET is to build the semantic connections between the seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a business firm whose art"
2021.emnlp-main.210,D18-1231,0,0.0133239,"ctions between the seen types (during training) and the unseen ones (during inference). Corresponding Author … Prototypes: western_union, quebecor, merrill, rtc, … Description: a business firm whose articles of incorporation have been approved in some state. Introduction ∗ Corporation Source3: Background Knowledge [Mention] 1 Organization [Context] [Type] Figure 1: Illustration of the proposed multi-source fusion model (MSF). Auxiliary information has been proved to be essential in this regard (Xian et al., 2019), with a variety of approaches focused on scattered information (Ma et al., 2016; Zhou et al., 2018; Obeidat et al., 2019; Ren et al., 2020; Zhang et al., 2020b). However, the power of auxiliary information has not been sufficiently exploited in existing solutions. Besides, the effects of each information source also remain to be clearly understood. In this paper, we propose a Multi-Source Fusion model (MSF) integrating three kinds of popular auxiliary information for ZFET, i.e., context consistency, type hierarchy, and background knowledge, as illustrated in Figure 1. (i) Context consistency means a correct type should be se2668 Proceedings of the 2021 Conference on Empirical Methods in Na"
2021.emnlp-main.431,J92-4003,0,0.338507,"names, “company” and “fruit”, can be mapped to it. To resolve this problem, we cluster all mentions in M by an overlapping clustering algorithm and then assign a type name to each mention cluster, whose meaning may be less ambiguous. Standard overlapping clustering algorithms either need to pre-define the number of clusters or are inefficient (Jain et al., 1999). Instead, we design a new algorithm whose basic idea includes three steps: for each mention, it computes its neighbor set consisting of its top k mentions according to a term similarity score; then a hierarchical clustering algorithm (Brown et al., 1992) is applied to each neighbor set, leading to many small subsets in total; then it repeatedly merges a pair of subsets to a larger subset according to a pre-defined threshold and finally take all subsets as clusters. Since one neighbor set may overlap with another one, the resulted clusters can be overlapping. According to the clusters, we create a map from mention clusters to type names. For example, the cluster “[Apple, Microsoft, Google, Facebook, ...]” is mapped to “company”. There are unambiguous mentions such as “Google” and “Microsoft” in this cluster, so the ambiguity of “Apple” can be"
2021.emnlp-main.431,2020.acl-main.749,0,0.028741,"Missing"
2021.emnlp-main.431,2021.emnlp-main.210,1,0.771934,"re practical setting, i.e. Finegrained Entity Typing without Knowledge Base (FETw/oKB). That is, given an ontology L, we aim to infer the type set T for the mention x within a sentence c when there are no KBs available. Relation to Zero-shot FET Suppose the type ontology L is divided into two disjoint subontology L1 and L2 , and there is a labeled training dataset where the labels are all from L1 . Zero-shot FET aims to make a prediction on testing mentions whose labels are within L2 . A surge of efforts have been devoted to zero-shot FET (Ma et al., 2016; Xian et al., 2019; Ren et al., 2020; Chen et al., 2021), but all of them assume that training data is obtained from knowledge bases by distant supervision, similar to the conventional FET. Therefore, the proposed FETw/oKB is clearly different from zero-shot FET. Road Map Generally, it is challenging to train Suppose L is a fine-grained entity type ontology for a real-world application as illustrated in Figure 2, θ under the FETw/oKB setting. In the next sections, we propose a new framework to address which consists of a set of formal types organized as FETw/oKB. Its key idea is a two-step approach: hierarchical trees, and a dictionary used to expl"
2021.emnlp-main.431,P18-1009,0,0.0911923,"lts show that our method achieves competitive performance with respect to the models trained on the original KB-supervised datasets. Figure 1: A comparison between the previous FET framework (left) and our proposed FET framework (right). In the previous one, the type ontology is coupled with KB whereas it is not in ours. Despite its success, this framework has one limitation: The type ontology for training and testing is strongly restricted by the underlying KB. As a result, the application of the trained models under this framework can be hampered by the lack or the incompleteness of the KB (Choi et al., 2018). In real-world applications of FET, the ontology of the fine-grained types depends on the applications instead of the KB. Typically, there may not exist 1 Introduction a one-to-one mapping from the ontology of appliEntity Typing is a fundamental task in Natural cations to that of the existing KB. For example, Language Processing. Traditional entity typing re- a game-related application may need to identify search focuses on a limited number of entity types the named entities of some specific video games, while recent studies strive for finer granularity. A such as the heroes and items in DOTA"
2021.emnlp-main.431,2021.acl-long.141,0,0.0276714,"realworld application. Instead, the desired types and ontology depend on the specific application and usually there does not exist a one-to-one mapping from the desired types to KB types. 2.3 Free-form Entity Typing Choi et al. (2018) propose an alternative to the conventional FET framework. Their proposed ultrafine entity typing task aims to predict free-form types such as noun phrases that describe appropriate types for the role the target entity plays in the sentence. To address this free-form typing task, they automatically create a large dataset using some heuristic methods. In addition, Dai et al. (2021) extend the dataset in Choi et al. (2018) with pretrained langauge models and then train the typing task on the extended dataset. Since the entity types are replaced by free-form noun phrases, there is no a unified type ontology in their task. However, a formal, explicit specification of the target types plays an important role in the downstream tasks of fine-grained entity typing. Different from their work, our proposed task retains the type ontology as an input for the task, but disentangles it from the knowledge base so as to provide larger flexibility. 3 Problem Statement for FETw/oKB comm"
2021.emnlp-main.431,D15-1103,0,0.0614652,"Missing"
2021.emnlp-main.431,N19-1423,0,0.0332094,"to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create training data, since it is challenging to build the humanannotated data (Ling and Weld, 2012; Gillick et al., 2014; Li et al., 2021a) Our contributions in this work are orthogonal to the above research line. As stated in Section 1, the focus of this work is not to develop more advanced models under the conventional FET framework. Actually, these models can be freely migrated to our proposed framework. It is worth noti"
2021.emnlp-main.431,C92-2082,0,0.554423,"ramework disentangles the type ontology from the KB and thus the given ontology can be an arbitrary one, defined by a practical application that does not correspond to any KB or by a specific KB as the conventional FET setting. Therefore, our proposed framework is more flexible than the previous one. Our framework consists of two steps: creating pseudo data from an unlabeled dataset and (iteratively) training the FET model based on the pseudo dataset. In the first step, we aim to create a large pseudo dataset with fine-grained type labels. We propose an automatic method using Hearst patterns (Hearst, 1992) to achieve this goal. There exists a difference between the pseudo data and the testing data in that each sentence in the pseudo data satisfies a (relaxed) Hearst pattern while few testing data do. Consequently, directly training a FET model on the pseudo data may limit its generalization ability. We thereby propose a self-training method with weak supervision from a coarse-grained NER model in the second step to alleviate this issue. We do not focus on designing novel models under the conventional FET setting in this work. Actually, our proposed two-step framework does not set a limit on the"
2021.emnlp-main.431,2021.acl-long.160,0,0.0658572,"anced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create training data, since it is challenging to build the humanannotated data (Ling and Weld, 2012; Gillick et al., 2014; Li et al., 2021a) Our contributions in this work are orthogonal to the above research line. As stated in Section 1, the focus of this work is not to develop more advanced models under the conventional FET framework. Actually, these models can be freely migrated to our proposed framework. It is worth noting that using Hearst patterns for FET was pioneered by Del Corro et al. (2015). However, they employed Hear"
2021.emnlp-main.431,2021.findings-emnlp.18,1,0.917958,"pplication. Instead, the desired types and ontology depend on the specific application and usually there does not exist a one-to-one mapping from the desired types to KB types. 2.3 Free-form Entity Typing Choi et al. (2018) propose an alternative to the conventional FET framework. Their proposed ultrafine entity typing task aims to predict free-form types such as noun phrases that describe appropriate types for the role the target entity plays in the sentence. To address this free-form typing task, they automatically create a large dataset using some heuristic methods. In addition, Dai et al. (2021) extend the dataset in Choi et al. (2018) with pretrained langauge models and then train the typing task on the extended dataset. Since the entity types are replaced by free-form noun phrases, there is no a unified type ontology in their task. However, a formal, explicit specification of the target types plays an important role in the downstream tasks of fine-grained entity typing. Different from their work, our proposed task retains the type ontology as an input for the task, but disentangles it from the knowledge base so as to provide larger flexibility. 3 Problem Statement for FETw/oKB comm"
2021.emnlp-main.431,D19-1641,0,0.150896,"a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create training data, since it is challenging to build the humanannotated data (Ling and Weld, 2012; Gillick et al., 2014; Li et al., 2021a) Our contributions in this work are orthogonal to the above research line. As stated in Section 1, the focus of this work is not to develop more advanced models under the conventional FET framework. Actually, these models can be freely migrated to our proposed framework. It is worth noting that using Hearst patterns for FET was pioneered by Del Corro et"
2021.emnlp-main.431,W02-0109,0,0.461743,"oft, Google, Facebook, ...]” is mapped to “company”. There are unambiguous mentions such as “Google” and “Microsoft” in this cluster, so the ambiguity of “Apple” can be reduced. It is worth mentioning that Zhang et al. (2020); Liu et al. (2021) adopt a similar method to obtain a mention cluster map for fine-grained entity classification where entity mentions are unknown. In their work, the map is employed to infer the labels by exact matching during inference. Consequently, 2 their work can not address those mentions which We implement an in-house extraction algorithm similar to that in NLTK (Loper and Bird, 2002). are not covered in the map. Instead of directly us5312 ing the map for inference, we employ the map to generate pseudo data for training a typing model, as will be described later. In this way, the model is able to generalize on unseen mentions. nization/company” even if this sentence does not satisfy any Hearst patterns at all. In this sense, our generation method can be considered as a relaxed version of the Hearst pattern method. 4.2 5 Pseudo Data Generation by Matching & Mapping Self-Training via Weak Guidance Note that each sentence in the above pseudo data satisfies a relaxed Hearst pa"
2021.emnlp-main.431,C16-1017,0,0.0236827,"em Statement In this paper, we focus on FET under a more practical setting, i.e. Finegrained Entity Typing without Knowledge Base (FETw/oKB). That is, given an ontology L, we aim to infer the type set T for the mention x within a sentence c when there are no KBs available. Relation to Zero-shot FET Suppose the type ontology L is divided into two disjoint subontology L1 and L2 , and there is a labeled training dataset where the labels are all from L1 . Zero-shot FET aims to make a prediction on testing mentions whose labels are within L2 . A surge of efforts have been devoted to zero-shot FET (Ma et al., 2016; Xian et al., 2019; Ren et al., 2020; Chen et al., 2021), but all of them assume that training data is obtained from knowledge bases by distant supervision, similar to the conventional FET. Therefore, the proposed FETw/oKB is clearly different from zero-shot FET. Road Map Generally, it is challenging to train Suppose L is a fine-grained entity type ontology for a real-world application as illustrated in Figure 2, θ under the FETw/oKB setting. In the next sections, we propose a new framework to address which consists of a set of formal types organized as FETw/oKB. Its key idea is a two-step ap"
2021.emnlp-main.431,P18-1010,0,0.0152394,"ur proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create tra"
2021.emnlp-main.431,D14-1162,0,0.0865649,"Missing"
2021.emnlp-main.431,N18-1202,0,0.0186901,"014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to automatically create training data, since it is challenging to build the humanannotated data (Ling and Weld, 2012; Gillick et al., 2014; Li et al., 2021a) Our contributions in this work are orthogonal to the above research line. As stated in Section 1, the focus of this work is not to develop more advanced models under the conventional FET framework. Actually, these models can be freely migrated to our propo"
2021.emnlp-main.431,P17-2052,0,0.0146013,"o et al. (2015). However, they employed Hearst patterns as well as a knowledge base to build an FET system. Hence they did not prove that it is feasible to address FET without any knowledge bases as we do. 2.2 Fine-grained Type Ontology Recent work introduced fine-grained type ontologies based on knowledge bases. Ling and Weld (2012) derive the type ontology consisting of 122 types from Freebase types. Later work (Murty et al., 2017) introduces a even more fine-grained ontology, which consists of over 1,941 types, obtained by manually annotating a mapping from 1,081 Freebase types to WordNet. Rabinovich and Klein (2017) derive the ontology from the Wikipedia categories and WordNet graphs. Del Corro et al. (2015) com/lemaoliu/fet-data. Part of this work is implemented in the public system TexSmart at https:// texsmart.qq.com. 5310 use the ontology derived from the entire WordNet hierarchy. Although driving an ontology from the type hierarchy of a knowledge base is an efficient way to build type ontology, it is not practical in realworld application. Instead, the desired types and ontology depend on the specific application and usually there does not exist a one-to-one mapping from the desired types to KB type"
2021.emnlp-main.431,D16-1144,0,0.10483,"roach (pseudo data generation and model training) to tackle the FET task. • Compared with training on KB-supervised datasets, our proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance ("
2021.emnlp-main.431,W16-1313,0,0.323817,"KB-supervised datasets, our proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve better performance (Lin and Ji, 2019; Chen et al., 2020; Onoe et al., 2021). This research line relies on a knowledge base to"
2021.emnlp-main.431,P15-2048,0,0.0308514,"propose a two-step approach (pseudo data generation and model training) to tackle the FET task. • Compared with training on KB-supervised datasets, our proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have been employed to achieve be"
2021.emnlp-main.431,P13-4023,0,0.0336833,"ained entity typing based on a new setting: FET without a knowledge base. • Under the new setting, we propose a two-step approach (pseudo data generation and model training) to tackle the FET task. • Compared with training on KB-supervised datasets, our proposed approach achieves competitive performance on two popular FET testing datasets. • To facilitate future research under this new setting, we make the created data publicly available. 1 1 The dataset will be available at https://github. 2 2.1 Related Work Fine-grained Entity Typing Starting from hand-crafted features (Ling and Weld, 2012; Yosef et al., 2013; Gillick et al., 2014), research on FET has moved to distributed representations (Yogatama et al., 2015; Ren et al., 2016a,b; Zhang et al., 2018) and more advanced neural network models (Dong et al., 2015; Shimaoka et al., 2016a,b; Murty et al., 2018). Shimaoka et al. (2016a) propose the first attentive neural model that outperforms feature-based methods with a simple cross-entropy loss. They use LSTM (Hochreiter and Schmidhuber, 1997) for context encoding. Murty et al. (2018) employ Convolutional Neural Networks to encode the entity mention and context. More recently, pretrained language mod"
2021.emnlp-main.431,S18-2022,0,0.0248991,"Missing"
2021.findings-acl.220,2020.acl-main.10,0,0.0340331,"Metrics1 for open-domain dialogue systems. A prediction model is designed to estimate the reliability of the given reference set. We show how its predicted results can be helpful to augment the reference set, and thus improve the reliability of the metric. Experiments validate both the effectiveness of our prediction model and that the reliability of reference-based metrics improves with the augmented reference sets. 1 Introduction The lack of reliable automatic evaluation metrics is a major impediment to the development of opendomain dialogue systems (Li and Jurafsky, 2016; Gao et al., 2019; Li et al., 2020a). The underlying difficulty in evaluation lies in the diversity of the possible outcomings. Existing evaluation metrics for open-domain dialogue systems can be ∗ Corresponding authors Interested reader may contact the authors to obtain a copy of the code and the data. 1 Tencent AI Lab, Shenzhen, China shumingshi@tencent.com roughly divided into reference-based and referencefree metrics. Reference-based metrics usually measure how similar a generated response is to the reference responses. Reference-free metrics, on the other hand, measure the quality of a response without any reference and u"
2021.findings-acl.220,W04-1013,0,0.442918,"reference and usually focus on specific aspects of the responses. For example, much work often computes the perplexity of a generated response as a measure of fluency (Li et al., 2020b), and adopts Dist-1/2 (Li et al., 2016b) to measure the diversity of the response. In this work, we focus on reference-based metrics. BLEU (Papineni et al., 2002), originally for machine translation, is now a popular referencebased metric to evaluate open-domain dialog systems automatically. However, it has been shown that BLEU and other word-overlap metrics such as METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004), rely on surface-form similarities only without considering the semantic diversity, thus fail to correlate well with human judgements (Liu et al., 2016). Instead, embedding-based metrics are adopted to consider the semantic meaning of a word defined by a distributed representation. For example, Zhang et al. (2020) introduce an embedding-based metric BERTScore that computes the similarity between the generated response and reference responses using contextual embeddings obtained from BERT (Devlin et al., 2019). Intuitively, the reliability of a referenced-based metric depends on two factors: ("
2021.findings-acl.220,P04-1077,0,0.150156,"EAM](BS) model in interactive annotations, respectively. “HumanREAM](BS)” denotes the annotator with the model assistance and “Human” is the annotator without the model assistance. The most commonly used reference-based metrics for dialog systems were originally proposed for machine translation. They typically count the amount of word-overlap between the generated response and the reference response. BLEU (Papineni et al., 2002) is the most widely used metric in machine translation that calculates the geometric mean of the precision for n-gram. Other related word-overlap metrics such as NIST (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) also have been used for dialogue evaluation. Instead of using word-overlap based metrics, embedding-based metrics are adopted to consider the semantic meaning of a word as defined by a distributed representation. They typically compute the similarity between the generated response and reference response using approximated sentencelevel representations. The most commonly used word embedding based metrics use a heuristic to combine the vector representation of the individual word in the sentence. For example, Embedding Average (Foltz et a"
2021.findings-acl.220,D16-1230,0,0.11833,"Missing"
2021.findings-acl.220,2021.findings-acl.193,0,0.481841,"he correlation results using the augmented response set from annotators in the second group are not stable. Though the overall trend is increasing, the final obtained reference set has even worse correlations than a much small augmented set from the first group. This shows that our interactive annotation strategy is effective to help annotators avoid writing responses with unsatisfactory quality. 6 0.40 Related Work Automatic evaluation is crucial to the research of open-domain dialog systems (Li and Jurafsky, 2016; Li et al., 2017; Gao et al., 2019; Venkatesh et al., 2018; Chan et al., 2021; Xiang et al., 2021). Existing metrics can be broadly categorized into reference-based and reference-free metrics (Chen et al., 2021). In this work, we focus on referencebased metrics, which usually measure how similar a generated response is to the reference response. 40 60 Number of references 80 100 Figure 4: Pearson correlations of the reference sets constructed with/without the REAM](BS) model in interactive annotations, respectively. “HumanREAM](BS)” denotes the annotator with the model assistance and “Human” is the annotator without the model assistance. The most commonly used reference-based metrics for d"
2021.findings-acl.220,D18-1297,1,0.897301,"Missing"
2021.findings-acl.220,P17-1103,0,0.0750574,"evel representations. The most commonly used word embedding based metrics use a heuristic to combine the vector representation of the individual word in the sentence. For example, Embedding Average (Foltz et al., 1998; Mitchell and Lapata, 2008), Vector Extrema (Forgues and Pineau, 2014), and Greedy Matching (Rus and Lintean, 2012). Zhang et al. (2020) introduce a better embeddingbased metric BERTScore that computes token similarity using contextual embeddings that capture the specific use of a word in a sentence. A few reference-based metrics for dialog systems are learnable functions. ADEM (Lowe et al., 2017) which is based on neural networks is trained to predict a score of a response given its query and a reference response. RUBER (Tao et al., 2018) 2494 evaluates responses with a blending of scores from the referenced and unreferenced metrics. RUBER is learnable, but its training does not require human annotation scores. As discussed from the very beginning of our work, all the above work focus on designing a better metric. However, the reliability of the reference set is also a key to improve the correlation of reference-based metrics, but not investigated in detail in previous work. Therefore"
2021.findings-acl.220,P08-1028,0,0.35562,"Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) also have been used for dialogue evaluation. Instead of using word-overlap based metrics, embedding-based metrics are adopted to consider the semantic meaning of a word as defined by a distributed representation. They typically compute the similarity between the generated response and reference response using approximated sentencelevel representations. The most commonly used word embedding based metrics use a heuristic to combine the vector representation of the individual word in the sentence. For example, Embedding Average (Foltz et al., 1998; Mitchell and Lapata, 2008), Vector Extrema (Forgues and Pineau, 2014), and Greedy Matching (Rus and Lintean, 2012). Zhang et al. (2020) introduce a better embeddingbased metric BERTScore that computes token similarity using contextual embeddings that capture the specific use of a word in a sentence. A few reference-based metrics for dialog systems are learnable functions. ADEM (Lowe et al., 2017) which is based on neural networks is trained to predict a score of a response given its query and a reference response. RUBER (Tao et al., 2018) 2494 evaluates responses with a blending of scores from the referenced and unrefe"
2021.findings-acl.220,P02-1040,0,0.1293,"ngshi@tencent.com roughly divided into reference-based and referencefree metrics. Reference-based metrics usually measure how similar a generated response is to the reference responses. Reference-free metrics, on the other hand, measure the quality of a response without any reference and usually focus on specific aspects of the responses. For example, much work often computes the perplexity of a generated response as a measure of fluency (Li et al., 2020b), and adopts Dist-1/2 (Li et al., 2016b) to measure the diversity of the response. In this work, we focus on reference-based metrics. BLEU (Papineni et al., 2002), originally for machine translation, is now a popular referencebased metric to evaluate open-domain dialog systems automatically. However, it has been shown that BLEU and other word-overlap metrics such as METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004), rely on surface-form similarities only without considering the semantic diversity, thus fail to correlate well with human judgements (Liu et al., 2016). Instead, embedding-based metrics are adopted to consider the semantic meaning of a word defined by a distributed representation. For example, Zhang et al. (2020) introduce an embeddin"
2021.findings-acl.373,2020.acl-main.705,0,0.0141674,"copying penalty can play a greater role and bring a significant performance boost. This also verifies the effectiveness of the copying penalty. 4 4.1 Related Work tures which are then fed into NMT models; and 2) parameter initialization, where part/all of the parameters of an NMT model are initialized by a pre-trained model and then training the model on downstream datasets (i.e., parallel corpus). About knowledge extraction, Yang et al. (2020a) and Zhu et al. (2020) explore enhancing encoder and decoder representations by leveraging pretrained BERT models (Devlin et al., 2019). In addition, Chen et al. (2020) distill the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample,"
2021.findings-acl.373,N19-1423,0,0.355934,"ussein Tantawi war anwesend. Table 1: Training objective gap between Seq2Seq LM pre-training and NMT training. LM learns to reconstruct a few source tokens and copy most of them, while NMT learns more translation rather than copying. Underlines denote artificial noises, and highlights indicate expected copying tokens. 2020). As a range of surface, syntactic and semantic information has been encoded in the initialized parameters (Jawahar et al., 2019; Goldberg, 2019), they are expected to bring benefits to NMT models and hence the translation quality. Introduction Self-supervised pre-training (Devlin et al., 2019; Song et al., 2019), which acquires general knowledge from a large amount of unlabeled data to help better and faster learning downstream tasks, has an intuitive appeal for neural machine translation (NMT; Bahdanau et al., 2015; Vaswani et al., 2017). One direct way to utilize pre-trained knowledge is initializing the NMT model with a pre-trained language model (LM) before training it on parallel data (Conneau and Lample, 2019; Liu et al., ∗ Work was done when Xuebo Liu and Liang Ding were interning at Tencent AI Lab. However, there is a discrepancy between the training objective of sequence-"
2021.findings-acl.373,2021.acl-long.266,1,0.684327,"of different copying penalties in P RETRAINED. Penalizing copying (i.e., α &lt; 1 ) brings benefits to the translations of various sources. Translating source original sentences is more sensitive to copying behaviors, leading to a larger score degradation when encouraging copying (i.e., α &gt; 1 ). greater than 1, which verifies our claim. 3.3 Out-of-domain Robustness Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; Tu et al., 2020), but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021). In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models. Setup We followed M¨uller et al. (2020) to preprocess all the used data sets.8 We served the medical domain as the training domain (i.e., using the data from the medical domain for model training and validation), which consists of 1.1M training examples and 2,000 validation examples. The test set of the medical domain contains 1,691 examples, while the test sets of the IT, Koran, law, and subtitle domains are with 2,000 examples respectively. For training R ANDOM, we used the"
2021.findings-acl.373,2020.emnlp-main.6,0,0.476957,"Missing"
2021.findings-acl.373,P16-1154,0,0.027074,", 2019; Liu et al., 2019; Yang et al., 2019). Compared with training from scratch, fine-tuning a pre-trained model on downstream datasets usually pushes state-of-the-art performances, while reducing computational and labeling costs. Previous studies mainly investigate the effect of pre-training on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even t"
2021.findings-acl.373,2020.acl-main.244,0,0.0313561,"Missing"
2021.findings-acl.373,P19-1356,0,0.0328339,"Missing"
2021.findings-acl.373,W18-2709,0,0.0142241,"wles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding of copying behaviors in NMT models. We observe that the translation of proper nouns in the source original text contains more copying tokens, which sheds light upon future works. 5 Conclusion and Future Work We find that NMT models with pre-training are prone to generate more copying tokens. We introduce a copying ratio and a copying error rate to quantitatively analyze copying behaviors in NMT evaluation. In addition, a simple and effective copying penalty is proposed to enhance the copying behaviors during model inference. Experimental results prove th"
2021.findings-acl.373,D18-1339,0,0.0316042,"is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding of copying behaviors in NMT models. We observe that the translation of proper nouns in the source original text c"
2021.findings-acl.373,W17-3204,0,0.0160039,"raining on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah"
2021.findings-acl.373,D11-1034,0,0.0335393,"Missing"
2021.findings-acl.373,2020.acl-main.703,0,0.0349923,"re-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answerin"
2021.findings-acl.373,2020.emnlp-main.210,0,0.43631,"NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answering and natural lang"
2021.findings-acl.373,2020.tacl-1.47,0,0.0560822,"Missing"
2021.findings-acl.373,2021.ccl-1.108,0,0.0281972,"Missing"
2021.findings-acl.373,2020.amta-research.14,0,0.0673361,"Missing"
2021.findings-acl.373,W19-5333,0,0.0331562,"Missing"
2021.findings-acl.373,N19-4009,0,0.0143054,"of pre-training on NMT in the perspective of copying behaviors. We expect to provide more evidence for controlling the copying behaviors of NMT models. 2.1 Experimental Setup Data We conducted experiments on the widelyused WMT14 English-German benchmark. We used the processed data provided by Vaswani et al. (2017), which consists of 4.5M sentence pairs.1 We used all the training data for model training. The validation set is newstest2013 of 3,000 examples and the test set is newstest2014 of 3,003 examples. Models and Settings We implemented all the models by the open-sourced toolkit fairseq (Ott et al., 2019).2 We used 8 V100 GPUs for the experiments. We mainly compared two models: 1) R ANDOM, which is a vanilla NMT model whose weights are randomly initialized without pre-training; and 2) P RETRAINED, an NMT model using the weights of pre-trained mBART.cc253 for parameter initialization, which has shown its usability and reliability for translation tasks (Tran et al., 2020; Tang et al., 2020). For the training of R ANDOM, we used the Transformer big setting of Ott et al. (2018b) with a huge training batch size of 460K tokens.4 For P RETRAINED, we fine-tuned on the pre-trained mBART.cc25 with a tra"
2021.findings-acl.373,W18-6301,0,0.122055,"is newstest2014 of 3,003 examples. Models and Settings We implemented all the models by the open-sourced toolkit fairseq (Ott et al., 2019).2 We used 8 V100 GPUs for the experiments. We mainly compared two models: 1) R ANDOM, which is a vanilla NMT model whose weights are randomly initialized without pre-training; and 2) P RETRAINED, an NMT model using the weights of pre-trained mBART.cc253 for parameter initialization, which has shown its usability and reliability for translation tasks (Tran et al., 2020; Tang et al., 2020). For the training of R ANDOM, we used the Transformer big setting of Ott et al. (2018b) with a huge training batch size of 460K tokens.4 For P RETRAINED, we fine-tuned on the pre-trained mBART.cc25 with a training batch size of 131K tokens. The hyperparameters keep the same with R ANDOM except the 0.2 label smoothing, 2500 warm-up steps, and 1e-4 maximum learning rate. Evaluation For each model, we selected the checkpoint with the lowest perplexity on the validation set for testing. The beam size is 5 and the length penalty is 0.6. In addition to report1 https://drive.google.com/uc?id=0B_ bZck-ksdkpM25jRUN2X2UxMm8 2 https://github.com/pytorch/fairseq 3 https://github.com/pytor"
2021.findings-acl.373,P02-1040,0,0.11011,"rseq/ blob/master/examples/scaling_nmt/README. md#3-train-a-model 4266 Source Target R ANDOM P RETRAINED Military ruler Field Marshal Hussein Tantawi was in attendance. Der Milit¨arf¨uhrer Feldmarschall Hussein Tantawi war anwesend. Performance O RACLE R ANDOM P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of"
2021.findings-acl.373,N18-1202,0,0.00927479,"020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answering and natural language inference (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). Compared with training from scratch, fine-tuning a pre-trained model on downstream datasets usually pushes state-of-the-art performances, while reducing computational and labeling costs. Previous studies mainly investigate the effect of pre-training on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monoli"
2021.findings-acl.373,W18-6319,0,0.0179225,"P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of “copying token” by comparing each input and output sentence pair. The denominator is the total number of tokens in output sentences. In general, higher Ratio values indicate more copying behaviors produced by the NMT model, and vice versa. Copying E"
2021.findings-acl.373,2020.tacl-1.40,0,0.0209978,"ying errors and thus the BLEU scores get a sharp degradation when setting the copying penalty Figure 3: BLEU scores of different copying penalties in P RETRAINED. Penalizing copying (i.e., α &lt; 1 ) brings benefits to the translations of various sources. Translating source original sentences is more sensitive to copying behaviors, leading to a larger score degradation when encouraging copying (i.e., α &gt; 1 ). greater than 1, which verifies our claim. 3.3 Out-of-domain Robustness Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; Tu et al., 2020), but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021). In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models. Setup We followed M¨uller et al. (2020) to preprocess all the used data sets.8 We served the medical domain as the training domain (i.e., using the data from the medical domain for model training and validation), which consists of 1.1M training examples and 2,000 validation examples. The test set of the medical domain contains 1,691 examples, while the test se"
2021.findings-acl.373,2020.tacl-1.18,0,0.136916,"ll the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of f"
2021.findings-acl.373,W16-2323,0,0.0591289,"tion, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding"
2021.findings-acl.373,2006.amta-papers.25,0,0.166465,"6 Source Target R ANDOM P RETRAINED Military ruler Field Marshal Hussein Tantawi was in attendance. Der Milit¨arf¨uhrer Feldmarschall Hussein Tantawi war anwesend. Performance O RACLE R ANDOM P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of “copying token” by comparing each input and output sentence pair. T"
2021.findings-acl.373,N03-1033,0,0.0595807,"Missing"
2021.findings-acl.373,2020.emnlp-main.208,0,0.123844,"setting CP to 1.2). One possible reason is that the IT domain needs to copy more tokens from the source sentence than translating sentences from other domains, thus the copying penalty can play a greater role and bring a significant performance boost. This also verifies the effectiveness of the copying penalty. 4 4.1 Related Work tures which are then fed into NMT models; and 2) parameter initialization, where part/all of the parameters of an NMT model are initialized by a pre-trained model and then training the model on downstream datasets (i.e., parallel corpus). About knowledge extraction, Yang et al. (2020a) and Zhu et al. (2020) explore enhancing encoder and decoder representations by leveraging pretrained BERT models (Devlin et al., 2019). In addition, Chen et al. (2020) distill the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Tr"
2021.findings-acl.373,W19-5208,0,0.238508,"Missing"
2021.findings-acl.422,W19-5206,0,0.108278,"e bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019). company best family back policeAmerican work show US well later children country men women decision officer meeting Abstract game {wangshuo2018, zxtan, sms, liuyang2011}@tsinghua.edu.cn &gt; {zptu, shumingshi}@tencent.com despite ? next used (b) German-Original Figure 1: Example of language coverage bias illustrated by word clouds that are plotted at the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect"
2021.findings-acl.422,C18-1111,0,0.0181722,"performance drop of backtranslation, as well as the different performances between tagged forward-translation and tagged back-translation (Caswell et al., 2019). In addition, we show that our approach is also beneficial for data augmentation approaches, which can further improve the translation performance over both back-translation and forward-translation. 5.3 Domain Adaptation Since high-quality and domain-specific parallel data is usually scarce or even unavailable, domain adaptation approaches are generally employed for translation in low-resource domains by leveraging out-of-domain data (Chu and Wang, 2018). Languages can be also regarded as different domains, since articles in different languages cover different topics (Bogoychev and Sennrich, 2019). Starting from this intuition, we distinguish examples with different original languages with tagging (Aharoni et al., 2019) and fine-tuning (Luong and Manning, 2015), which are commonly-used in domain adaptation and multi-lingual translation tasks. Our work also benefits domain adaptation: distinguishing original languages in general domain 4785 data consistently improves translation performance of NMT models in several specific domains (Table 16 i"
2021.findings-acl.422,D18-1045,0,0.0469535,"this section, we aim to provide some insights where monolingual data augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6"
2021.findings-acl.422,N19-1388,0,0.0767514,"validation sets. the monolingual data augmentation scenario (Section 4.2), where the language coverage bias problem is more severe due to the newly introduced dataset in source or target language. 4.1 Bilingual Data Utilization In this section, we aim to improve bilingual data utilization through explicitly distinguishing between the source- and target-original training data. Methodology We distinguish original languages with two simple and effective methods: • Bias-Tagging: Tagging is a commonly-used approach to distinguishing between different types of examples, such as different languages (Aharoni et al., 2019; Riley et al., 2020) and synthetic vs authentic examples (Caswell et al., 2019). In this work, we attach a special tag to the source side of each target-original example, which enables NMT models to distinguish it from the source-original ones in training. • Fine-Tuning: Fine-tuning (Luong and Manning, 2015) is a useful method to help knowledge transmit among data from different distributions. We pre-train NMT models on the full training data that consists of both the source- and targetoriginal data, and then fine-tune them on only the source-original data. For fair comparison, the total trai"
2021.findings-acl.422,P07-2045,0,0.0080886,"Missing"
2021.findings-acl.422,2021.eacl-main.130,0,0.0136043,"(2020) show that the source-side translationese texts can potentially lead to distortions in automatic and human evaluations. Accordingly, the WMT competition starts to use only source-original test sets for most translation directions since 2019. Our study reconfirms the necessity of distinguishing the source- and target-original examples and takes one step further to distinguish examples in training data. Complementary to previous works, we investigate the effect of language coverage bias on machine translation, which is related to the content bias rather than the language style difference. Shen et al. (2021) also reveal the context mismatch between texts from different original languages. To alleviate this problem, they proposed to combine back- and forward-translation by introducing additional monolingual data, while we focus on better exploiting bilingual data by distinguishing the original languages, which is also helpful for back- and forward-translation. Lembersky et al. (2011, 2012) propose to adapt machine translation systems to generate texts that are more similar to human-translations, while Riley et al. (2020) propose to model human-translated texts and original texts as separate langua"
2021.findings-acl.422,2020.wmt-1.30,0,0.0343531,"Missing"
2021.findings-acl.422,D11-1034,0,0.080019,"y crabs are the most well-known image spokesmen of Bacheng. Both It is the best-known icon of Bacheng. Table 5: An example of the outputs of NMT models trained on different sets of data. Using the targetoriginal data tends to omit content words. naturally arises: can target-original bilingual data improve the fluency of NMT models? To answer the above question, we measure the fluency of outputs with language models trained on the monolingual data as described in Section 2. Previous study finds that different perplexities could be caused by specific contents rather than structural differences (Lembersky et al., 2011). Specifically, some source-original contents are of low frequency in the target-language monolingual data (e.g., “Bacheng” in Table 5), thus the language model trained on the target-language monolingual data tends to assign higher perplexities to outputs containing more source-original content words. To rule out this possibility and check whether the outputs are structurally different, we follow Lembersky et al. (2011) to abstract away from the contentspecific features of the outputs to measure their fluency at the syntactic level. Table 6 shows the results. Although using only the source-ori"
2021.findings-acl.422,W19-6627,0,0.115311,"t the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect of original languages in parallel data on neural machine translation (Barrault et al., 2020; Edunov et al., 2020; Marie et al., 2020). Several studies have shown that targetoriginal test examples1 can lead to distortions in automatic and human evaluations, which should be omitted from machine translation test sets (Barrault et al., 2019; Zhang and Toral, 2019; Graham 1 Target-original test examples are sentence pairs that are translated from the target language into the source language. et al., 2020). Another branch of studies report that target-original test data leads to discrepant conclusions: back-translation only benefits the translation of target-original test data while harms that of source-original test data (Edunov et al., 2020; Marie et al., 2020). They attribute these phenomena to the reason that human-translated texts (i.e., translationese) exhibit formal and stylistic differences that set them apart from the texts originally written i"
2021.findings-acl.422,E12-1026,0,0.0480962,"Missing"
2021.findings-acl.422,2015.iwslt-evaluation.11,0,0.170579,"ly distinguishing between the source- and target-original training data. Methodology We distinguish original languages with two simple and effective methods: • Bias-Tagging: Tagging is a commonly-used approach to distinguishing between different types of examples, such as different languages (Aharoni et al., 2019; Riley et al., 2020) and synthetic vs authentic examples (Caswell et al., 2019). In this work, we attach a special tag to the source side of each target-original example, which enables NMT models to distinguish it from the source-original ones in training. • Fine-Tuning: Fine-tuning (Luong and Manning, 2015) is a useful method to help knowledge transmit among data from different distributions. We pre-train NMT models on the full training data that consists of both the source- and targetoriginal data, and then fine-tune them on only the source-original data. For fair comparison, the total training steps of the pre-training and fine-tuning stages are the same as the baseline. Analysis Recent studies have shown that generating human-translation like texts as opposed to original texts can improve the BLEU score (Riley et al., 2020). To validate that the improvement is partially from alleviating the c"
2021.findings-acl.422,2020.acl-main.532,0,0.193192,"the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation (Marie et al., 2020). We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants (Caswell et al., 2019). company best family back policeAmerican work show US well later children country men women decision officer meeting Abstract game {wangshuo2018, zxtan, sms, liuyang2011}@tsinghua.edu.cn &gt; {zptu, shumingshi}@tencent.com despite ? next used (b) German-Original Figure 1: Example of language coverage bias illustrated by word clouds that are plotted"
2021.findings-acl.422,W19-5208,0,0.392807,"plotted at the English side of sentence pairs in the En-De test sets from WMT10 to WMT18. The test sets consist of Englishoriginal and German-original sentence pairs. Introduction In recent years, there has been a growing interest in investigating the effect of original languages in parallel data on neural machine translation (Barrault et al., 2020; Edunov et al., 2020; Marie et al., 2020). Several studies have shown that targetoriginal test examples1 can lead to distortions in automatic and human evaluations, which should be omitted from machine translation test sets (Barrault et al., 2019; Zhang and Toral, 2019; Graham 1 Target-original test examples are sentence pairs that are translated from the target language into the source language. et al., 2020). Another branch of studies report that target-original test data leads to discrepant conclusions: back-translation only benefits the translation of target-original test data while harms that of source-original test data (Edunov et al., 2020; Marie et al., 2020). They attribute these phenomena to the reason that human-translated texts (i.e., translationese) exhibit formal and stylistic differences that set them apart from the texts originally written i"
2021.findings-acl.422,2020.wmt-1.25,0,0.0648905,"Missing"
2021.findings-acl.422,N19-4007,0,0.0685171,"ly, if the language coverage bias exists, the vocabulary distributions of the source- and target-original data should differ greatly from each other, since the covered issues tend to have different frequencies between them (D’Alessio and Allen, 2000). We use the Jensen-Shannon (JS) divergence (Lin, 1991) to measure the difference between two vocabulary En⇒Zh En⇐Zh Origin noun verb adj noun verb adj Target Source Both 67.6 69.7 69.9 52.0 54.0 54.1 64.3 66.2 65.9 53.8 61.8 61.2 38.0 44.1 43.8 57.0 63.9 63.4 Table 4: Translation adequacy of different types of content words measured by F-measure (Neubig et al., 2019). The results are reported on the validation sets. distributions p and q:   p+q p+q 1 KL(p|| ) + KL(q|| ) , JS (p||q) = 2 2 2 where KL(·||·) is the KL divergence (Kullback and Leibler, 1951) of two distributions. Table 2 shows the JS divergence of the vocabulary distributions between the source- and targetoriginal data. We also divide the words into content words and functions words based on their POS tags, since content words are more related to the language coverage bias, while the function words are more related to the stylistic and structural differences between the translationese and or"
2021.findings-acl.422,W19-5333,0,0.0130493,"each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Section 3.3). To this end, we propose an automatic method to detect the original language of each training example (Section 3.1), which is often not available in large-scale parallel corpora (Riley et al., 2020). 3.1"
2021.findings-acl.422,W18-6301,0,0.0117812,". For En⇔De and En⇔Zh, we used newstest2019 as the validation sets. For En⇔Ja, we split the official validation set released by WMT20 into two parts by the original language and only used the corresponding part for each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Se"
2021.findings-acl.422,W18-6319,0,0.0201539,"nce pairs for En⇔De, En⇔Zh, and En⇔Ja, respectively. We used the monolingual data that is publicly available in WMT20 to train the proposed original language detection model (Section 3.1) and data augmentation (Section 4.2). The Appendix lists details about the data preprocessing. For En⇔De and En⇔Zh, we used newstest2019 as the validation sets. For En⇔Ja, we split the official validation set released by WMT20 into two parts by the original language and only used the corresponding part for each direction. We used newstest2020 as the test sets for all the six tasks. We reported the Sacre BLEU (Post, 2018), as recommended by WMT20. Model We used the Transformer-Big (Vaswani et al., 2017) model, which consists of a 6-layer encoder and a 6-layer decoder, and the hidden size is 1024. Recent studies showed that training on large batches can further boost model performance (Ott et al., 2018; Wu et al., 2018). Accordingly, we followed their settings to train models with batches of approximately 460k tokens. Please refer to the Appendix for more details about model training. We followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original la"
2021.findings-acl.422,2020.findings-emnlp.276,0,0.057633,"Missing"
2021.findings-acl.422,2020.acl-main.691,0,0.33119,"followed Ng et al. (2019) to use the Transformer-Big decoder as our language models, which are used to detect the original language and measure translation fluency. Language models are also trained with large batches (Ott et al., 2018). 3 Observing Language Coverage Bias In this study, we first establish the existence of language coverage bias (Section 3.2), and show how the bias affects NMT performance (Section 3.3). To this end, we propose an automatic method to detect the original language of each training example (Section 3.1), which is often not available in large-scale parallel corpora (Riley et al., 2020). 3.1 Detecting Original Languages Detection Method Intuitively, we use a largescale monolingual dataset to estimate the distribution of the contents covered by each language. For each training example, we compare its similarities 4779 WMT20 WMT20 En=&gt;Zh En=&gt;Zh En-Zh En-Ja En-De FT Ours 83.6 84.4 83.7 91.5 86.6 88.7 WMT20 WMT20 Zh=&gt;En Zh=&gt;En 37 37 28 28 36.5 36.5 32 32 Table 1: F1 scores of detecting original languages in the test sets. “FT” denotes the forward translation classifier proposed by Riley et al. (2020). 33.2 33.2 31.0 31.0 P (Ds )P (hx, yi|Ds ) , P (hx, yi) P (Dt )P (hx, yi|Dt ) P"
2021.findings-acl.422,P16-1009,0,0.0613289,"augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6 40.8 - 28.8 - 34.8 - 20.4 - 31.9 39.6 - Our Implemented Systems Baselin"
2021.findings-acl.422,P16-1162,0,0.0390796,"augmentation improves translation performance, and investigate whether our approach can further improve model performance in this scenario that potentially suffers more from the language coverage bias problem. For fair comparison across language pairs, we augment NMT models with the same English monolingual corpus as described in Section 2. We down-sample the large-scale monolingual corpus to the same amount as that of the bilingual corpus in each language pair, in order to rule out the effect of the scale of synthetic data (Edunov et al., 2018; Fadaee and Monz, 2018). We use backtranslation (Sennrich et al., 2016a) to augment the English monolingual data for the task of translating from another language to English (“X⇒En”), and use forward-translation for the task in the opposite translation direction (“En⇒X”). Table 10 lists the results, where several observations can be made. Explaining Data Augmentation with Language Coverage Bias Concerning the monolingual data augmentation methods (Rows 3-4), the vanilla 4783 En-Zh Method En-Ja En-De Average ⇒ ⇐ ⇒ ⇐ ⇒ ⇐ WMT20 Systems Shi et al. (2020) Zhang et al. (2020) Molchanov (2020) 38.6 40.8 - 28.8 - 34.8 - 20.4 - 31.9 39.6 - Our Implemented Systems Baselin"
2021.findings-acl.432,D19-1501,1,0.854308,"ogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NL"
2021.findings-acl.432,D19-1201,1,0.869864,"ogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NL"
2021.findings-acl.432,2020.emnlp-main.313,1,0.729602,"roved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is learned from tuples of d"
2021.findings-acl.432,2021.acl-long.34,0,0.469022,"ious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a re"
2021.findings-acl.432,N19-1021,0,0.0144273,"dding. posterior qθ (z|hq ), i.e. a conditional Gaussian distribution. Training procedures. Previous works (Bowman et al., 2015; Zhao et al., 2017) mentioned that VAE and CVAE training is challenging due to the KL vanishing issue, where the decoder ignores the conditional information and all the resulting posteriors almost collapse to a same Gaussian prior. To mitigate this issue, first, we initialize our model with Optimus (Li et al., 2020), a large-scale VAE-based PLM model, while optimizing Eq. 2. To mitigate the same issue while optimizing Eq. 3, we use the cyclical KL annealing schedule (Fu et al., 2019). Specifically, we add a hyperparameter α to control the weight of the KLdivergence in Eq. 3. We set α close to zero in the first half of cyclic schedule, linearly anneal α to 1 in the next one-fourth of cyclic schedule and kept α = 1 in the remaining cyclic schedule. Moreover, the Free Bits (Bowman et al., 2015) is also crucial for the training. It replaces the KLdivergence in Eq. 3 by a hinge loss max(γ, KL(qφ (z|hq )||pθ (z|hp ))) (4) where γ is a hyperparameter which controls the information space for the each dimension of the latent variable. Finally, an extra bag-of-word loss (Zhao et al"
2021.findings-acl.432,2021.findings-acl.220,1,0.534829,"Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is learned from tuples of data, {conversational context, response reference, negative sample}, in an unsuperv"
2021.findings-acl.432,2020.acl-main.124,0,0.101111,"ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to e"
2021.findings-acl.432,W19-2310,0,0.198948,"Missing"
2021.findings-acl.432,P19-1590,0,0.0127027,"rior distribution P (z|c) to capture the feasible latent reference information in the latent space. Specifically, when training CVAEs, P (z|c) is forced to be close to the posterior distribution Q(z|c, ri ) for any reference response ri as illustrated in Fig. 1. In this sense, if z is sampled from P (z|c), z may contain some information of any ri in some extent, and z can be used as a surrogate of {rk }K k=1 . Therefore, we can expect 4891 1 There is a brief proof in Apendix A. I(l; c, {rk }N k=1 ) ≥ I(l; c, ri , z) ≥ I(l; c, ri ). 3.2 3.3 Overall Architecture Previous works (Li et al., 2019; Gururangan et al., 2019; Li et al., 2020) concluded that VAEs can learn a smooth latent space through the regularization from the gaussian prior. Inspired by Li et al. (2020), we propose a novel architecture which can be regarded as a large-scale pretrained language model (PLM) based on VAEs/CVAEs. Encoder. Li et al. (2019) argue that the VAEs might benefit from initialization with a noncollapsed encoder, because the encoder provides useful information from the beginning of training. We use the Masked PLMs (Devlin et al., 2018; Liu et al., 2019) as the text encoder because of their impressive effectiveness in natura"
2021.findings-acl.432,N19-1169,0,0.0184016,"(2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimoto et al., 2019; Chan et al., 2020). Meanwhile, several works (Qiu et al., 2019; Chen et al., 2020; Gao et al., 2021) have shown their effectiveness in the open-domain dialogue systems. With the development of the large4890 Learning-based Metrics. Recent studies (Tao et al., 2018; Sinha et al., 2020) attempt to mitigate the one-to-many issue by considering the similarity of the generated response with the conversational contexts. The similarity is calculated by a designed discriminative model which learns to evaluate whether a response matches the conversational context well. The discriminative model is lear"
2021.findings-acl.432,D19-1370,0,0.23084,"ntation learning and dialogue modeling, we propose to learn the dialogue representations via VAEs/CVAEs for better evaluation. Equip with such dialogue representations, we obtain an Enhanced dialogue evaluation Metric in latent Space (EMS). EMS is a self-supervised evaluation metric with a two-stage training procedure. It represents dialogue sentences in a smooth latent space to both capture discourse-level context information and model more feasible latent references. Specifically, in the first stage, we build a VAE based model to map the dialogue sentences into a latent (or semantic) space. Li et al. (2019) showed that VAEs can be viewed as a regularized version of the auto-encoder and learn a smooth latent space through the regularization from the Gaussian prior. Then, we train our model by optimizing CVAEs’ objective which forces the prior distribution to capture the feasible latent references information (details in Section 3.3). In the second stage, we combine the dialogue representations and the captured feasible latent reference information to train a discriminative model. Meanwhile, we give a potential explanation of our motivation about why using feasible latent reference information can"
2021.findings-acl.432,2020.emnlp-main.378,0,0.250324,"to capture the feasible latent reference information in the latent space. Specifically, when training CVAEs, P (z|c) is forced to be close to the posterior distribution Q(z|c, ri ) for any reference response ri as illustrated in Fig. 1. In this sense, if z is sampled from P (z|c), z may contain some information of any ri in some extent, and z can be used as a surrogate of {rk }K k=1 . Therefore, we can expect 4891 1 There is a brief proof in Apendix A. I(l; c, {rk }N k=1 ) ≥ I(l; c, ri , z) ≥ I(l; c, ri ). 3.2 3.3 Overall Architecture Previous works (Li et al., 2019; Gururangan et al., 2019; Li et al., 2020) concluded that VAEs can learn a smooth latent space through the regularization from the gaussian prior. Inspired by Li et al. (2020), we propose a novel architecture which can be regarded as a large-scale pretrained language model (PLM) based on VAEs/CVAEs. Encoder. Li et al. (2019) argue that the VAEs might benefit from initialization with a noncollapsed encoder, because the encoder provides useful information from the beginning of training. We use the Masked PLMs (Devlin et al., 2018; Liu et al., 2019) as the text encoder because of their impressive effectiveness in natural language underst"
2021.findings-acl.432,I17-1099,0,0.0443873,"Missing"
2021.findings-acl.432,D16-1230,0,0.030509,"ning variational model to capture the feasible latent references; • Experiments performed on two large datasets demonstrate the effectiveness of our proposed model and outperform all baseline methods. 2 Related Work Word overlap-based Metrics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the"
2021.findings-acl.432,2021.ccl-1.108,0,0.0249961,"Missing"
2021.findings-acl.432,P17-1103,0,0.0589553,"del to capture the feasible latent references; • Experiments performed on two large datasets demonstrate the effectiveness of our proposed model and outperform all baseline methods. 2 Related Work Word overlap-based Metrics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional rep"
2021.findings-acl.432,P08-1028,0,0.819519,"ural language generation (NLG) tasks (Dathathri et al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic simila"
2021.findings-acl.432,P02-1040,0,0.116934,"an. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural language generation (NLG) tasks (Dathathri et al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019;"
2021.findings-acl.432,P19-1372,1,0.904042,", an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial for the development of natural lang"
2021.findings-acl.432,W12-2018,0,0.667018,"t al., 2019; Gu et al., 2019; Gao et al., 2019; Chan et al., 2019a, 2020). The lack of meaningful automatic evaluation metrics has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a genera"
2021.findings-acl.432,2020.tacl-1.52,0,0.402268,"for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) further proposed to enhance such discriminative evaluation metrics by finetuning on a few human-annotated data to improve the robustness. These discriminative metrics trained using a single relevant response and multiple negative samples. However, Sai et al. (2020) argued that such discriminative metrics should be trained on multiple relevant responses (i.e., positive samples) and multiple negative samples, to favor the one-to-many nature in open-domain dialogues. Therefore, they collected a new dataset which contains multiple relevant and irrelevant responses for any given conversational context to train their discriminative evaluation model and the model trained by multiple relevant responses shows impressive performance. However, there are no organized relevant multiple responses in most existing datasets. Collecting a new dataset is expensive and ti"
2021.findings-acl.432,2020.acl-main.704,0,0.0671024,"in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build aut"
2021.findings-acl.432,P18-1205,0,0.100736,"Missing"
2021.findings-acl.432,P17-1061,0,0.20045,"space. Specifically, we present EMS, an Enhanced dialogue evaluation Metric in latent Space. Experimental results on two real-world dialogue datasets confirm the superiority of our method for open-domain dialogue evaluation, where both Pearson and Spearman correlations with human judgments outperform all baselines. 1 Introduction With the surge of deep learning techniques, generation-based open-domain dialogue systems have witnessed significant improvement in recent years. Plenty of novel and effective models (Sutskever et al., 2014; Serban et al., 2016; Li et al., 2015; Serban et al., 2016; Zhao et al., 2017; Gu et al., 2018; Qiu et al., 2019; Chan et al., 2019b; Serban et al., 2017; Wolf et al., 2019; Hu et al., 2019; Chen et al., 2020) are proposed and have greatly promoted the development of the opendomain dialogue generation. Unlike the endless emergence of novel methods, however, there is still no meaningful and widely accepted automatic evaluation metric for dialogue generation yet. As we ∗ This work was done while Z. Chan was an intern at Tencent AI Lab. Corresponding Author: Rui Yan. know, automatic evaluation allows quick and effective comparison between different systems and is crucial"
2021.findings-acl.432,2020.acl-main.4,0,0.417553,"9; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by considering the similarity of the generated 4889 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4889–4900 August 1–6, 2021. ©2021 Association for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) further proposed to enhance such discriminative evaluation metrics by finetuning on a few human-annotated data to improve the robustness. These discriminative metrics trained using a single relevant response and multiple negative samples. However, Sai et al. (2020) argued that such discriminative metrics should be trained on multiple relevant responses (i.e., positive samples) and multiple negative samples, to favor the one-to-many nature in open-domain dialogues. Therefore, they collected a new dataset which contains multiple relevant and irrelevant responses for any given conversational con"
2021.findings-acl.432,D19-1053,0,0.201466,"n and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue eva"
2021.findings-acl.432,D18-1463,0,0.0149516,"s has become a significant impediment for open-domain dialog generation research. Over the past decade, many automatic evaluation metrics are proposed to evaluate the opendomain dialogue systems. Among them, the word overlap-based automatic evaluation metrics from NLG tasks, such as BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 201"
2021.findings-acl.432,2020.acl-main.220,0,0.122106,"ng model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by considering the similarity of the generated 4889 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4889–4900 August 1–6, 2021. ©2021 Association for Computational Linguistics responses with the conversational context. Specifically, these works design discriminative models which can judge whether the generated responses match the conversational context well, which learn from {conversational context, response reference, negative sample} pairs in unsupervised learning manner. Zhao et al. (2020) fur"
2021.findings-acl.432,2021.findings-acl.193,1,0.84362,"004) in text summarization, are popular. In addition, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) have been utilized to evaluate the open-domain dialogue systems (Gu et al., 2018; Chan et al., 2019b; Shen et al., 2018). Recently, with the fantastic development of the large-scale pre-training model (Devlin et al., 2018; Liu et al., 2019; Radford et al., 2019), researchers proposed to enhance the embedding metrics by converting the dialogue sentences to hidden space via pre-training model (Zhang et al., 2019; Sellam et al., 2020; Zhao et al., 2019; Xiang et al., 2021). The common idea behind these metrics is that they measure the semantic similarity between a reference response and a generated response, independent on the conversational context. However, due to the notorious one-to-many nature (Li et al., 2015; Zhao et al., 2017; Qiu et al., 2019; Gu et al., 2018) of open-domain dialogue, a good response should be related well to its context yet may be largely different from a reference response in semantics. Some other works (Tao et al., 2018; Ghazarian et al., 2019; Sinha et al., 2020) thereby proposed to build automatic dialogue evaluation metrics by co"
2021.findings-acl.432,2020.acl-srw.27,0,0.0191815,"rics. Several word overlap-based automatic evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely used to evaluate the quality of generated responses. These word overlap-based metrics measure how many words overlap in a given generated response when compared to a reference response. Liu et al. (2016); Lowe et al. (2017); Tao et al. (2018) argued that these word overlap-based metric scores are weakly correlated to human judgment due to ignoring the notorious one-to-many nature of the open-domain dialogues. Therefore, Yuma et al. (2020) proposed the improved BLEU, which compares the generated response with multiply diverse references. Embedding-based Metrics. Unlike word overlap-based metrics comparing two raw sentences, Embedding Metrics (Mitchell and Lapata, 2008; Forgues et al., 2014; Rus and Lintean, 2012) map sentences to a high dimensional space, and calculate similarity based on the high-dimensional representations. Embedding Metrics are recently popular for evaluating the generation tasks, such as text summarization (Gao et al., 2020; Chen et al., 2021), question answer (Gao et al., 2019) and text generation (Hashimo"
2021.findings-emnlp.18,D18-1217,0,0.0580215,"Missing"
2021.findings-emnlp.18,N19-1423,0,0.216612,"rew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these methods all train the senachieved even better results through incorpotence encoders from scratch, without exploiting the rating label correlations.1 knowledge from unlabeled corpora. Hence, none of them is even competitive with current best se1 Introduction quence labeling model. Plenty of tasks in natural language understanding In this paper, we propose lexical unit analysis (NLU), such as syntactic chunking, are essentially (LUA), a unified and effective span-based model a sequence segmentation problem, which partitions tha"
2021.findings-emnlp.18,C18-1139,0,0.153813,"an-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these methods all train the senachieved even better results through incorpotence encoders from scratch, without exploiting the rating label correlations.1 knowledge from unlabeled corpora. Hence, none of them is even competitive with current best se1 Introduction quence labeling model. Plenty of tasks in natural language understanding In this paper, we propose lexical unit analysis (NLU), such as syntactic chunking, are essentially (LUA), a unified and effective span-based model a sequence segmentation problem,"
2021.findings-emnlp.18,W06-1655,0,0.108807,"r, each of these methods suffers from normalized at span level, and therefore suffered its own defects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et"
2021.findings-emnlp.18,P16-1039,0,0.0283561,"Liu et al., 2019c; Luo et al., 2020). Every token we embed every span of the sentence, inspired by in a sentence, according to its position in the cor- the finding that pretraining language models are responding segment, is labeled with a tag (e.g., very robust to rare tokens and the low-resource setB-PER). A representative work is Bidirectional ting (Liu et al., 2019b). Then, we assign a score to LSTM-CRF (Huang et al., 2015). every segmentation candidate and use DP to globRecently, there is a surge of interest in develop- ally search for the candidate with the maximum ing span-based models (Cai and Zhao, 2016; Zhai score. The score of a segmentation is computed et al., 2017; Li et al., 2020a; Yu et al., 2020; Li from the segment scores predicted by LUA. We et al., 2021). They regard spans rather than tokens minimize the hinge loss, instead of cross-entropy, as the basic units for labeling. For example, Li et al. to train our models. (2020a) model named entity recognition (NER) We have performed extensive experiments on 1 syntactic chunking, Chinese part-of-speech (POS) The source code for our work is publicly available at https://github.com/LeePleased/LUA. tagging, and NER across 7 datasets. Our m"
2021.findings-emnlp.18,N16-1030,0,0.0408992,"Missing"
2021.findings-emnlp.18,2020.acl-main.519,0,0.142602,"inspired by in a sentence, according to its position in the cor- the finding that pretraining language models are responding segment, is labeled with a tag (e.g., very robust to rare tokens and the low-resource setB-PER). A representative work is Bidirectional ting (Liu et al., 2019b). Then, we assign a score to LSTM-CRF (Huang et al., 2015). every segmentation candidate and use DP to globRecently, there is a surge of interest in develop- ally search for the candidate with the maximum ing span-based models (Cai and Zhao, 2016; Zhai score. The score of a segmentation is computed et al., 2017; Li et al., 2020a; Yu et al., 2020; Li from the segment scores predicted by LUA. We et al., 2021). They regard spans rather than tokens minimize the hinge loss, instead of cross-entropy, as the basic units for labeling. For example, Li et al. to train our models. (2020a) model named entity recognition (NER) We have performed extensive experiments on 1 syntactic chunking, Chinese part-of-speech (POS) The source code for our work is publicly available at https://github.com/LeePleased/LUA. tagging, and NER across 7 datasets. Our model 181 Findings of the Association for Computational Linguistics: EMNLP 2021, pag"
2021.findings-emnlp.18,2020.acl-main.574,1,0.894314,"inspired by in a sentence, according to its position in the cor- the finding that pretraining language models are responding segment, is labeled with a tag (e.g., very robust to rare tokens and the low-resource setB-PER). A representative work is Bidirectional ting (Liu et al., 2019b). Then, we assign a score to LSTM-CRF (Huang et al., 2015). every segmentation candidate and use DP to globRecently, there is a surge of interest in develop- ally search for the candidate with the maximum ing span-based models (Cai and Zhao, 2016; Zhai score. The score of a segmentation is computed et al., 2017; Li et al., 2020a; Yu et al., 2020; Li from the segment scores predicted by LUA. We et al., 2021). They regard spans rather than tokens minimize the hinge loss, instead of cross-entropy, as the basic units for labeling. For example, Li et al. to train our models. (2020a) model named entity recognition (NER) We have performed extensive experiments on 1 syntactic chunking, Chinese part-of-speech (POS) The source code for our work is publicly available at https://github.com/LeePleased/LUA. tagging, and NER across 7 datasets. Our model 181 Findings of the Association for Computational Linguistics: EMNLP 2021, pag"
2021.findings-emnlp.18,P19-1524,0,0.114558,"fects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these methods all train the"
2021.findings-emnlp.18,P19-1233,0,0.519662,"fects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these methods all train the"
2021.findings-emnlp.18,W13-3516,0,0.24678,"Missing"
2021.findings-emnlp.18,W00-0726,0,0.720062,"Missing"
2021.findings-emnlp.18,W03-0419,0,0.513531,"Missing"
2021.findings-emnlp.18,I17-1018,0,0.193475,"Missing"
2021.findings-emnlp.18,P17-1076,0,0.13538,"Missing"
2021.findings-emnlp.18,P16-1101,0,0.0486329,"language understanding In this paper, we propose lexical unit analysis (NLU), such as syntactic chunking, are essentially (LUA), a unified and effective span-based model a sequence segmentation problem, which partitions that circumvents all above problems. Our segmena sequence of lexical units into multiple labeled tation of a natural language sentence contains two segments. A classical approach to sequence seg- steps. Firstly, we utilize BERT (Devlin et al., 2019), mentation is to cast it into a sequence labeling task a powerful pretraining language model, to get conusing IOB tagging scheme (Ma and Hovy, 2016; textualized token representations, and with them Liu et al., 2019c; Luo et al., 2020). Every token we embed every span of the sentence, inspired by in a sentence, according to its position in the cor- the finding that pretraining language models are responding segment, is labeled with a tag (e.g., very robust to rare tokens and the low-resource setB-PER). A representative work is Bidirectional ting (Liu et al., 2019b). Then, we assign a score to LSTM-CRF (Huang et al., 2015). every segmentation candidate and use DP to globRecently, there is a surge of interest in develop- ally search for the"
2021.findings-emnlp.18,2020.coling-main.187,0,0.0704473,"Missing"
2021.findings-emnlp.18,P18-2038,0,0.0153554,"suffered its own defects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic programming (DP) (Bellextract the candidate with the maximum score. man, 1966) to search for the optimal segmentation We have conducted extensive experiments on of a sentence. Unlike their counterparts (Clark 3 tasks, (e.g., syntactic chunking), across 7 datasets. LUA has established new state-ofet al., 2018; Akbik et al., 2018; Devlin et al., 2019; the-art performances on 6 of them. We have Li et al., 2021), these meth"
2021.findings-emnlp.18,2020.acl-main.577,0,0.348222,"{newmanli,redmondliu,shumingshi}@tencent.com Abstract as machine reading comprehension (MRC) (Seo et al., 2017), where entities are extracted as retrievThe span-based model enjoys great popularing answer spans. While span-based models have ity in recent works of sequence segmentation. achieved promising performances, they are locally However, each of these methods suffers from normalized at span level, and therefore suffered its own defects, such as invalid predictions. In from the label bias problem (Lafferty et al., 2001). this work, we introduce a unified span-based Moreover, some of them (Yu et al., 2020; Li et al., model, lexical unit analysis (LUA), that ad2021) rely on heuristic rules to correct invalid predresses all these matters. Segmenting a lexical unit sequence involves two steps. Firstly, we dictions (e.g., span conflicts between two entities). embed every span by using the representations Early span-based models (Andrew, 2006; Kong from a pretraining language model. Secondly, et al., 2016; Ye and Ling, 2018; Liu et al., 2019a) we define a score for every segmentation candibased on Semi-Markov CRF (Sarawagi and Cohen, date and apply dynamic programming (DP) to 2005) adopts dynamic p"
2021.findings-emnlp.18,2020.acl-main.302,0,0.0318771,"Missing"
2021.findings-emnlp.18,P18-1144,0,0.118475,"Missing"
2021.findings-emnlp.247,W19-5206,0,0.177765,"T and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and EnglishRussian benchmarks show that PT can nicely cowork with BT, leading to state-of-the-art model performances. Extensive analyses show that the tagging mechanism is helpful for enhancing the complementarity between PT and BT by improving the translation of source-original sentences and low-frequency words. Our main contributions are as follows: • We design two probing tasks to investigate the impact of PT and BT on NMT models. • We empirically demonstrate the complementarity between PT and BT. • We show that Tagged BT further improve"
2021.findings-emnlp.247,2021.emnlp-main.263,1,0.833334,"Missing"
2021.findings-emnlp.247,D18-1045,0,0.452877,"Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the"
2021.findings-emnlp.247,2020.acl-main.253,0,0.0509777,"Missing"
2021.findings-emnlp.247,D18-1040,0,0.0187186,"l parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences between PT and BT sity and complexity in synthetic data, showing that adding symbols (e.g., noises and tags) to the back- on improving model performance. We design two probing tasks to study the research question: Which translated source can help NMT distinguish the data from various sources and learn better represen- module of NMT do PT and BT respectively play a tations (Fadaee and Monz, 2018; Wang et al., 2019; greater role in enhancing translation quality? Edunov et al., 2018; Caswell et al., 2019; Marie 3.1 Effects of PT on NMT et al., 2020). The claims and understandings from these works are chiefly at the data-level rather than Given a pre-trained model, it is common to use its the model-level. part or all parameters to initialize the downstream There also exists some works that combine tasks. We design four NMT models, which differ PT and BT to further boost the model perfor- from the NMT components (Encoder vs. Decoder) mance (Conneau et al., 2020; Song et al., 2019; with p"
2021.findings-emnlp.247,2020.emnlp-main.6,0,0.0112886,"ed source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and BT improves the model performance, while adding tags to BT data further improves Liu et al., 2021a; Wang et al., 2021).4 Generally speaking, the translation of Src-Ori is more important than that of Tgt-Ori for practical NMT systems (Graham et al., 2020), thus its performance should be taken seriously. As shown in Table 3, PT performs better on Src-Ori than BT (33.8 vs. 31.9 BLEU) while BT achieves higher scores on TgtOri than PT (45.6 vs. 42.0 BLEU). Besides, simply combining PT and BT can improve the translation quality on both Src-Ori and Tgt-Ori sentences, but the improvement of Src-Ori is lower than only using PT. By introducing tagged BT, the model can achieve better performance than the simple one, especially on source-original sentences. Takeaway: 1) PT and BT complementary in terms of originality of sentences; 2) Tagged BT can allevi"
2021.findings-emnlp.247,2021.acl-long.221,1,0.810105,"Missing"
2021.findings-emnlp.247,2020.emnlp-main.210,0,0.605565,"laborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conducted experiments on the WMT16 English-Romanian (En-Ro) and English-Russia (En-Ru) translation tasks, which are widely-used benchmarks of data augmentation methods for NMT. The training/validation/test sets of the En-Ro include 612K/2K/2K sentence pairs, while those of En-Ru include 2M/3K/3K pairs. Towards better reproducibility, we directly used the BT data provided by Sennrich et al. (2016a)1 , consisting of 2.3M synt"
2021.findings-emnlp.247,2020.acl-main.41,1,0.673903,"o widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and En"
2021.findings-emnlp.247,2021.findings-acl.373,1,0.886854,"o learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conducted experiments on the WMT16 English-Romanian (En-Ro) and English-Russia (En-Ru) translation tasks, which are widely-used benchmarks of data augmentation methods for NMT. The training/validation/test sets of the En-Ro include 612K/2K/"
2021.findings-emnlp.247,2020.tacl-1.47,0,0.265899,"o widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and En"
2021.findings-emnlp.247,2020.acl-main.532,0,0.0613034,"Missing"
2021.findings-emnlp.247,N19-4007,0,0.0122229,"riginal while “All” means the whole testset. Table 2: Translation quality on the En-Ro and En-Ru benchmarks. “+” means incorporating PT and (Tagged) BT into NMT models. 4.2 All Src-Ori denotes the testing data originating in the source language, while Tgt-Ori denotes the data translating from the target language. 2903 Effects of Word Frequency Data augmentation is an effective way to improve the translation quality of low-frequency words (Sennrich et al., 2016b). Thus, we compare the performance of the models on translating different frequencies of words. Specifically, we employed compare-mt (Neubig et al., 2019) to calculate the f-measure of translating low- and high-frequency words (&lt;50 vs. ≥50). As shown in Table 4, PT improves more on translating low-frequency words (58.2 vs. 57.5 scores) while BT performs better on high-frequency words (67.3 vs. 66.7 scores). Furthermore, the combination of PT and tagged BT achieves the best performance on both low- and high-frequency words, leading to an overall improvement on the whole words. Similar phenomenons can be observed by combining self-training and BT (Ding et al., 2021b). Takeaway: 1) PT and BT complementary in terms of frequency of words; 2) Tagged"
2021.findings-emnlp.247,P02-1040,0,0.109581,"ir comparison, all the model architectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand th"
2021.findings-emnlp.247,W18-6319,0,0.0200641,"., 2020b). Setting To make a fair comparison, all the model architectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In th"
2021.findings-emnlp.247,2020.tacl-1.18,0,0.187012,"tiveness in improving the model performance of NMT, especially for those language pairs with smaller parallel corpora (Conneau and Lample, 2019). The first research line treats pre-trained models as external knowledge to guidance NMT to learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conduct"
2021.findings-emnlp.247,W16-2323,0,0.284038,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,P16-1009,0,0.470578,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,P16-1162,0,0.668047,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,2006.amta-papers.25,0,0.0314185,"hitectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences"
2021.findings-emnlp.247,D19-1149,0,0.0464751,"Missing"
2021.findings-emnlp.247,D19-1073,0,0.0181896,"n the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences between PT and BT sity and complexity in synthetic data, showing that adding symbols (e.g., noises and tags) to the back- on improving model performance. We design two probing tasks to study the research question: Which translated source can help NMT distinguish the data from various sources and learn better represen- module of NMT do PT and BT respectively play a tations (Fadaee and Monz, 2018; Wang et al., 2019; greater role in enhancing translation quality? Edunov et al., 2018; Caswell et al., 2019; Marie 3.1 Effects of PT on NMT et al., 2020). The claims and understandings from these works are chiefly at the data-level rather than Given a pre-trained model, it is common to use its the model-level. part or all parameters to initialize the downstream There also exists some works that combine tasks. We design four NMT models, which differ PT and BT to further boost the model perfor- from the NMT components (Encoder vs. Decoder) mance (Conneau et al., 2020; Song et al., 2019; with parameter initializa"
2021.findings-emnlp.247,2021.findings-acl.422,1,0.73879,"29.4 33.8 31.5 33.3 31.9 34.8 38.3 42.0 45.4 48.6 45.6 48.7 Tagged BT is to add a special token at the beginning of each back-translated source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and BT improves the model performance, while adding tags to BT data further improves Liu et al., 2021a; Wang et al., 2021).4 Generally speaking, the translation of Src-Ori is more important than that of Tgt-Ori for practical NMT systems (Graham et al., 2020), thus its performance should be taken seriously. As shown in Table 3, PT performs better on Src-Ori than BT (33.8 vs. 31.9 BLEU) while BT achieves higher scores on TgtOri than PT (45.6 vs. 42.0 BLEU). Besides, simply combining PT and BT can improve the translation quality on both Src-Ori and Tgt-Ori sentences, but the improvement of Src-Ori is lower than only using PT. By introducing tagged BT, the model can achieve better performance than the simple one, esp"
2021.findings-emnlp.247,2020.emnlp-main.208,0,0.143525,"al., 2019), which can ac2900 ∗ Work was done when Xuebo Liu and Liang Ding were interning at Tencent AI Lab. Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2900–2907 November 7–11, 2021. ©2021 Association for Computational Linguistics quire knowledge from unlabeled monolingual data, has shown its effectiveness in improving the model performance of NMT, especially for those language pairs with smaller parallel corpora (Conneau and Lample, 2019). The first research line treats pre-trained models as external knowledge to guidance NMT to learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation direct"
2021.findings-emnlp.247,D16-1160,0,0.0238903,"Missing"
2021.findings-emnlp.247,W19-5208,0,0.0274486,"e-art performances on the two benchmarks. Similar tendencies are observed in terms of the TER scores. The above results illustrate the better complementarity between PT and Tagged BT on improving translation quality for NMT models. Analysis We conducted extensive analyses to better understand the improvement of our approach. All results are reported on the En-Ro benchmark. Effects of Sentence Type Recent studies have shown that the evaluation of BT is sensitive to the sentences types, thus we report BLEU scores on the subsets of source-original (Src-Ori) and targetoriginal (Tgt-Ori) datasets (Zhang and Toral, 2019; Model Vanilla + PT + BT + BT + PT + Tagged BT + Tagged BT + PT Tgt 33.7 37.7 38.4 41.2 38.6 41.6 29.4 33.8 31.5 33.3 31.9 34.8 38.3 42.0 45.4 48.6 45.6 48.7 Tagged BT is to add a special token at the beginning of each back-translated source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and B"
C10-1112,N09-1003,0,0.290571,"; Yamaguchi 2008) for semantic class mining. This paper focuses on corpus-based approaches. As has been mentioned in the introduction part, primarily two types of methodologies are adopted: DS and PB. Syntactic context information is used in (Hindle, 1990; Ruge, 1992; Lin 1998; Lin and Pantel, 2001; Pantel and Lin, 2002) to compute term similarities. The construction of syntactic contexts requires sentences to be parsed by a dependency parser, which may be extremely time-consuming on large corpora. As an alternative, lexical context (such as text window) has been studied (Pantel et al., 2004; Agirre et al., 2009; Pantel et al., 2009). In the pattern-based category, a lot of work has been done to discover term relations by sentence lexical patterns (Hearst 1992; Pasca 2004), HTML tag patterns (Shinzato and Torisawa, 2005), or both (Shi et al., 2008; Zhang et al., 2009). In this paper, our focus is not one specific methodology, but the comparison and combination of them. A small amount of existing work is related to the comparison or combination of multiple methods. Pennacchiotti and Pantel (2009) proposed a feature combination framework (named ensemble semantic) to combine features generated by differ"
C10-1112,P97-1067,0,0.0136267,"NF values for the five seed sets are listed in Table 3, where we can see that proper nouns have the largest MNF values, followed by common nouns. In other words, the patterns in Table 2 capture the relations of more proper nouns than other term categories. Seed Categories Proper nouns Common nouns Verbs Adjectives Adverbs Terms MNF 40 40 40 40 40 0.2333 0.0716 0.0099 0.0126 0.0053 Table 3. MNF values of different seed categories As mentioned in the introduction, the PB and DS approaches we studied capture first-order and second-order term co-occurrences respectively. Some existing work (e.g., Edmonds, 1997) showed that second-order co-occurrence leads to better results for detecting synonymy. Considering that a high proportion of coordinate terms of verbs, adjectives, and adverbs are their synonyms and antonyms, it is reasonable that DS behaves better for these term types because it exploits second-order co-occurrence. For PB, different from the standard way of dealing with first-order cooccurrences where statistics are performed on all pairs of near terms, a subset of co-occurred terms are selected in PB by specific patterns. The patterns in Table-2 help detecting coordinate proper nouns, becau"
C10-1112,P90-1034,0,0.549811,"approaches are called contexts in Torisawa, 2005; Ohshima, et al., 2006; Zhang et the following part of this paper. Progress has been made and promising results al., 2009), where peer terms (or coordinate terms) have been reported in the past years for both DS are discovered from a corpus. and PB approaches. However, most previous reExisting approaches to semantic class mining search work (some exceptions are discussed in could roughly be divided into two categories: related work) involves solely one category of apdistributional similarity (DS), and pattern-based (PB). The first type of work (Hindle, 1990; Lin proach. And there is little work studying the 1998; Pantel and Lin 2002) is based on the distri- comparison of their performance for different butional hypothesis (Harris, 1985), saying that types of terms (we use “term” to represent a sinterms occurring in analogous (lexical or syntactic) gle word or a phrase). In this paper, we make an empirical study of contexts tend to be similar. DS approaches basithis problem, based on a large-scale, publicly cally exploit second-order co-occurrences to discover strongly associated concepts. In pattern- available dataset containing 500 million web"
C10-1112,I08-1047,0,0.0115616,"ntic classes are obtained by the offline processing of a corpus which can be unstructured (e.g., plain text) or semi-structured (e.g., web pages). Search-resultsbased approaches (Etzioni et al., 2004; Kozareva et al., 2008; Wang and Cohen, 2008) assume that multiple terms (or, less often, one term) in a semantic class have been provided as seeds. Other terms in the class are retrieved by sending queries 2 3 http://www.wikipedia.org/ http://needleseek.msra.cn/ 994 (constructed according to the seeds) to a web search engine and mining the search results. Query logs are exploited in (Pasca 2007; Komachi and Suzuki, 2008; Yamaguchi 2008) for semantic class mining. This paper focuses on corpus-based approaches. As has been mentioned in the introduction part, primarily two types of methodologies are adopted: DS and PB. Syntactic context information is used in (Hindle, 1990; Ruge, 1992; Lin 1998; Lin and Pantel, 2001; Pantel and Lin, 2002) to compute term similarities. The construction of syntactic contexts requires sentences to be parsed by a dependency parser, which may be extremely time-consuming on large corpora. As an alternative, lexical context (such as text window) has been studied (Pantel et al., 2004;"
C10-1112,P98-2127,0,0.93295,"for different types of terms. term or a term-pair, e.g., “(invent, subject-of)” for the term “Edison”, and “- starring -” for the 1 Introduction term-pair “(The Terminal, Tom Hanks)”. AltComputing the semantic relationship between hough “patterns” are utilized, we categorize them terms, which has wide applications in natural as DS approaches rather than PB, because they language processing and web search, has been a match the DS framework well. In this paper, PB hot topic nowadays. This paper focuses on cor- only refers to the approaches that utilize patterns pus-based semantic class mining (Lin 1998; Pan- to exploit first-order co-occurrences. And the tel and Lin 2002; Pasca 2004; Shinzato and patterns in DS approaches are called contexts in Torisawa, 2005; Ohshima, et al., 2006; Zhang et the following part of this paper. Progress has been made and promising results al., 2009), where peer terms (or coordinate terms) have been reported in the past years for both DS are discovered from a corpus. and PB approaches. However, most previous reExisting approaches to semantic class mining search work (some exceptions are discussed in could roughly be divided into two categories: related work) in"
C10-1112,P08-1003,0,0.017937,"Missing"
C10-1112,D09-1025,0,0.285544,"ming on large corpora. As an alternative, lexical context (such as text window) has been studied (Pantel et al., 2004; Agirre et al., 2009; Pantel et al., 2009). In the pattern-based category, a lot of work has been done to discover term relations by sentence lexical patterns (Hearst 1992; Pasca 2004), HTML tag patterns (Shinzato and Torisawa, 2005), or both (Shi et al., 2008; Zhang et al., 2009). In this paper, our focus is not one specific methodology, but the comparison and combination of them. A small amount of existing work is related to the comparison or combination of multiple methods. Pennacchiotti and Pantel (2009) proposed a feature combination framework (named ensemble semantic) to combine features generated by different extractors (distributional and “patternbased”) from various data sources. As has been discussed in the introduction, in our terminology, their “pattern-based” approaches are actually DS for term-pairs. In addition, their study is based on three semantic classes (actors, athletes, and musicians), all of which are proper nouns. Differently, we perform the comparison by classifying terms according to their lexical categories, based on which additional insights are obtained about the pros"
C10-1112,E06-1003,0,0.0166768,"emanpopular and high-quality pattern for extracting tic class mining include distributional peer terms (and also hyponyms). Besides the natsimilarity (DS) and pattern-based (PB). ural language patterns, some HTML tag tree patIn this paper, we perform an empirical terns (e.g., the drop down list) are also effective comparison of them, based on a publicly in semantic class mining. available dataset containing 500 million It is worth-noting that the word “pattern” also web pages, using various categories of appears in some DS approaches (Pasca et al., queries. We further propose a frequency2006; Tanev and Magnini, 2006; Pennacchiotti based rule to select appropriate approachand Pantel, 2009), to represent the context of a es for different types of terms. term or a term-pair, e.g., “(invent, subject-of)” for the term “Edison”, and “- starring -” for the 1 Introduction term-pair “(The Terminal, Tom Hanks)”. AltComputing the semantic relationship between hough “patterns” are utilized, we categorize them terms, which has wide applications in natural as DS approaches rather than PB, because they language processing and web search, has been a match the DS framework well. In this paper, PB hot topic nowadays. This"
C10-1112,P09-1052,1,0.814629,"ing that types of terms (we use “term” to represent a sinterms occurring in analogous (lexical or syntactic) gle word or a phrase). In this paper, we make an empirical study of contexts tend to be similar. DS approaches basithis problem, based on a large-scale, publicly cally exploit second-order co-occurrences to discover strongly associated concepts. In pattern- available dataset containing 500 million web based approaches (Hearst 1992; Pasca 2004; pages. For each approach P, we build a termShinzato and Torisawa, 2005; Ohshima, et al., similarity graph G(P), with vertices representing 2006; Zhang et al., 2009), patterns are applied to terms, and edges being the confidence that the two terms are peers. Approaches are compared * by the quality of their corresponding term graphs. Work done during an internship at Microsoft Abstract 1 993 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 993–1001, Beijing, August 2010 We measure the quality of a term graph by set expansion. Two query sets are adopted: One contains 49 semantic classes of named entities and 20220 trials (queries), collected by Pantel et al. (2009) from Wikipedia2; and the other contains 10"
C10-1112,C92-2082,0,\N,Missing
C10-1112,C94-1079,0,\N,Missing
C10-1112,P08-1119,0,\N,Missing
C10-1112,C98-2122,0,\N,Missing
C10-1112,D09-1098,0,\N,Missing
D12-1094,P08-1004,0,0.0613743,"Missing"
D12-1094,P11-1062,0,0.0127454,"Missing"
D12-1094,P04-1056,0,0.0133143,"Missing"
D12-1094,I05-2045,0,0.0163377,"Missing"
D12-1094,D11-1142,0,0.258339,"Missing"
D12-1094,P04-1053,1,0.931988,"Missing"
D12-1094,C92-2082,0,0.229379,"complexity. 4.1 Knowledge Sources Entity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and anot"
D12-1094,P08-1119,0,0.342981,"ings an open-ended set of relation types. To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types. The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance. However, most algorithms (Hasegawa et al., 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Recently, Kok and Domingos (2008) proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types. However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering. Empirically we found that it fails to group many relevant relation instances. These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice. In contrast, there exist sever"
D12-1094,N04-1041,0,0.0169342,"these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity:"
D12-1094,I05-1011,0,0.0506813,"Missing"
D12-1094,D09-1025,0,0.0418617,"Missing"
D12-1094,I05-5011,0,0.0637491,"Missing"
D12-1094,C10-1112,1,0.817003,"been proposed to resolve objects and relation synonyms (Resolver), extract semantic networks (SNE), and map extracted relations into an existing ontology (Soderland and Mandhani, 2007). Recent work shows that it is possible to construct semantic classes and sets of similar phrases automatically with data-driven approaches. For generating semantic classes, previous work applies distributional similarity (Pasca, 2007; Pantel et al., 2009), uses a few linguistic patterns (Pasca 2004; Sarmento et al., 2007), makes use of structure in webpages (Wang and Cohen 2007, 2009), or combines all of them (Shi et al., 2010). Pennacchiotti and Pantel (2009) combines several sources and features. To find similar phrases, there are 2 closely related tasks: paraphrase discovery and recognizing textual entailment. Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases. The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other. To efficiently cluster high-dimensional datasets, canopy c"
D12-1094,D08-1061,0,0.0252795,"ge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph"
D12-1094,P10-2068,0,0.0211762,"Missing"
D12-1094,N09-1033,0,0.0312473,"Missing"
D12-1094,P09-1050,0,0.0297532,"Missing"
D12-1094,P10-1013,0,0.166586,"Missing"
D12-1094,P03-1016,0,0.079683,"Missing"
D12-1094,D11-1135,0,0.228249,"Missing"
D12-1094,N07-1016,0,0.124788,"Missing"
D12-1094,P11-1116,1,0.779565,"ity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hypernyms such as city, location, and place. Therefore the similarity between the two cities is large according to the hypernymy graph, while their similarity in the DS graph and the PS graph may be very small. Following existing work (Hearst, 1992, Pantel & Ravichandran 2004; Snow et al., 2005; Talukdar et al., 2008; Zhang et al., 2011), we adopt a list of lexical patterns to extract hypernyms. The patterns include NP {,} (such as) {NP,}* {and|or} NP, NP (is|are|was|were|being) (a|an|the) NP, etc. The hypernymy graph is a bipartite graph with two types of nodes: entity nodes and label (hypernym) nodes. There is an edge (T, L) with weight w if L is a hypernym of entity T with probability w. There are about 8.2 million nodes and 42.4 million edges in the hypernymy graph. In this paper, we use the terms hypernym and label interchangeably. Relation phrase similarity: To generate the pairwise similarity graph for relation phrases"
D12-1094,P09-1052,1,0.836204,"tity similarity graph We build two similarity graphs for entities: a distributional similarity (DS) graph and a pattern-similarity (PS) graph. The DS graph is based on the distributional hypothesis (Harris, 1985), saying that terms sharing similar contexts tend to be similar. We use a text window of size 4 as the context of a term, use Pointwise Mutual Information (PMI) to weight context features, and use Jaccard similarity to measure the similarity of term vectors. The PS graph is generated by adopting both sentence lexical patterns and HTML tag patterns (Hearst, 1992; Kozareva et al., 2008; Zhang et al., 2009; Shi et al., 2010). Two terms (T) tend to be semantically similar if they cooccur in multiple patterns. One example of sentence lexical patterns is (such as |including) T{,T}* (and|,|.). HTML tag patterns include tables, dropdown boxes, etc. In these two graphs, nodes are entities and the edge weights indicate entity similarity. In all there are about 29.6 million nodes and 1.16 billion edges. 1030 Hypernymy graph Hypernymy relations are very useful for finding semantically similar term pairs. For example, we observed that a small city in UK and another small city in Germany share common hype"
D12-1094,N06-1039,0,\N,Missing
D12-1094,D09-1098,0,\N,Missing
D14-1087,C92-2082,0,0.213197,"ightly different from query template construction. First, some useful features such as query click-through is not available in category template construction. Second, categories should be valid natural language phrases, while queries need not. For example, “city Germany” is a query but not a valid category name. We discuss in more details in the related work section. Our major contributions are as follows. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”"
D14-1087,P08-1119,0,0.0413024,"Missing"
D14-1087,P10-1136,0,0.0325053,"m to group existing query templates by search intents of users. Compared to the open-domain unsupervised methods for query template construction, our approach improves on two aspects. First, we propose to incorporate multiple types of semantic knowledge (e.g., term peer similarity and term clusters) to improve template generation. Second, we propose a nonlinear template scoring function which is demonstrated to be more effective. Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final temp"
D14-1087,N09-1003,0,0.0596501,"Missing"
D14-1087,P11-1098,0,0.034426,"d ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Li et al. (2013) proposed an clustering algorithm to group existing query templates by search intents of users. Compared to the open-domain unsupervised methods for query template construction, our approach improves on two aspects. First, we propose to incorporate multiple types of semantic knowledge (e.g., term peer similarity and term clusters) to improve template generation. Second, we propose a nonlinear template scoring function which is demonstrated to be"
D14-1087,N04-1041,0,0.295994,"nt from query template construction. First, some useful features such as query click-through is not available in category template construction. Second, categories should be valid natural language phrases, while queries need not. For example, “city Germany” is a query but not a valid category name. We discuss in more details in the related work section. Our major contributions are as follows. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010)"
D14-1087,P11-1120,0,0.0148478,"d, we propose a nonlinear template scoring function which is demonstrated to be more effective. Query tagging/labeling: Some research work in recent years focuses on segmenting web search queries and assigning semantic tags to key segments. Li et al. (2009) and Li (2010) employed CRF (Conditional Random Field) or semi-CRF models for query tagging. A crowdsourcingassisted method was proposed by Han et al. (2013) for query structure interpretation. These supervised or semi-supervised approaches require much manual annotation effort. Unsupervised methods were proposed by Sarkas et al. (2010) and Reisinger and Pasca (2011). As been discussed in the introduction section, query tagging is only one of the two stages of template generation. The tagging results are for one query only, without aggregating the global information of all queries to generate the final templates. The goal of this paper is to construct a list of category templates from a collection of open-domain category names. Input: The input is a collection of category names, which can either be manually compiled (like Wikipedia categories) or be automatically extracted. The categories used in our experiments were automatically mined from the web, by f"
D14-1087,C10-1112,1,0.8823,"Missing"
D14-1087,P05-1047,0,0.0246128,"and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Li et al. (2013) proposed an clustering algorithm to group existing query templates by search intents of u"
D14-1087,W12-1511,0,0.0145865,"tions are as follows. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the patte"
D14-1087,P03-1044,0,0.0515478,"nior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to find lexical patterns in category names to define axioms (in medical domain). One example pattern mentioned in their papers is “[X] binding”. They need manual intervention to determine what X means. The main difference between the above work and ours is that we automatically assign semantic types to the pattern variables (or called arguments) while they do not. 800 Template mining for IE: Some research work in information extraction (IE) involves patterns. Yangarber (2003) and Stevenson and Greenwood (2005) proposed to learn patterns which were in the form of [subject, verb, object]. The category names and learned templates in our work are not in this form. Another difference between our work and their work is that, their methods need a supervised name classifer to generate the candidate patterns while our approach is unsupervised. Chambers and Jurafsky (2011) leverage templates to describe an event while the templates in our work are for understanding category names (a kind of short text). Li et al. (2013) proposed an clustering algorithm to group existing que"
D14-1087,P11-1116,1,0.786456,"atures such as query click-through is not available in category template construction. Second, categories should be valid natural language phrases, while queries need not. For example, “city Germany” is a query but not a valid category name. We discuss in more details in the related work section. Our major contributions are as follows. 2 Related work Several kinds of work are related to ours. Hypernymy relation extraction: Hypernymy relation extraction is an important task in text mining. There have been a lot of efforts (Hearst, 1992; Pantel and Ravichandran, 2004; Van Durme and Pasca, 2008; Zhang et al., 2011) in the literature to extract hypernymy (or is-a) relations from the web. Our target here is not hypernymy extraction, but discovering the semantic structure of hypernyms (or category names). Category name exploration: Category name patterns are explored and built in some existing research work. Third (2012) proposed to find axiom patterns among category names on an existing ontology. For example, infer axiom pattern “SubClassOf(AB, B)” from “SubClassOf(junior school school)” and “SubClassOf(domestic mammal mammal)”. Fernandez-Breis et al. (2010) and QuesadaMartınez et al. (2012) proposed to f"
D14-1087,D09-1098,0,\N,Missing
D15-1135,S07-1018,0,0.0121345,"e object. Liguda & Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars & Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee & Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to com"
D15-1135,W13-2322,0,0.0220911,"w semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first problem in Figure 1 (written below for convenience), One number is 16 more than another. If the smaller number is subtracted from 2/3 of the larger, the result is 1/4 of the sum of the two numbers. Find the numbers. To automatically solve this problem, th"
D15-1135,D13-1160,0,0.00887979,"ing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 A"
D15-1135,P14-1133,0,0.0147107,"mantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first problem in Figure 1 (w"
D15-1135,P13-1042,0,0.00942264,"much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math"
D15-1135,W04-2412,0,0.0108717,"on of the information gathered for one object. Liguda & Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars & Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee & Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. S"
D15-1135,J07-4004,0,0.0126457,"Missing"
D15-1135,P81-1022,0,0.747028,"classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury & Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possible parse trees. For syntactic parsing, a wellknown serious problem is ambiguity: the appearance of many syntactically correct but semantically unreasonable parse trees. Modern syntactic parsers reply on statistical information to reduce 4 VerbNet: http://verbs.colorado.edu/~mpalmer/projects/verbnet.html 1137 ambiguity. They are often based on probabilistic CFGs (PCFGs) or probabilistic lexicalized CFGs trained on hand-labeled TreeBanks. With the new set of D"
D15-1135,C92-2082,0,0.12496,"re manually built by referring to text books and online tu1136 torials. About 35 classes and 200 functions are obtained in this way. Additional instances of each element type are constructed in the ways below. Classes: Additional classes and grammar rules are obtained from two data sources: Freebase 3 types, and automatically extracted lexical semantic data. By treating Freebase types as DOL classes and the mapping from types to lexical names as grammar rules, we get the first version of grammar for classes. To improve coverage, we run a term peer similarity and hypernym extraction algorithm (Hearst, 1992; Shi et al., 2010; Zhang et al., 2011) on a web snapshot of 3 billion pages, and get a peer-similarity graph and a collection of is-a pairs. An is-a pair example is (Megan Fox, actress), where “Megan Fox” and “actress” are instance and type names respectively. In our peer similarity graph, “Megan Fox” and “Britney Spears” have a high similarity score. The peer similarity graph is used to clean the is-a data collection (with the idea that peer terms often share some common type names). Given the cleaned isa data, we sort the type names by weight and manually create classes for top-1000 type na"
D15-1135,kingsbury-palmer-2002-treebank,0,0.0365188,"$2) → {$2} {$1} mf.number.even → even mf.condition.if($1) → if {$1} mf.approximately → approximately |roughly education.university → university math.number → number math.integer → integer 3 Functions: Additional noun functions are automatically created from Freebase properties and attribute extraction results (Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury & Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possible parse trees. For syntactic parsing, a wellknown serious problem is ambiguity:"
D15-1135,P14-1026,0,0.352942,"hool-level math word problems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda & Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). ______________________________________ * Work done while this author was an intern at Microsoft Research Figure 1: Number word problem examples In this paper, we present a computer system called SigmaDolphin which automatically solves math word problems by semantic parsing and reasoning. We design a meaning representation language called DOL (abbreviation of dolphin language) as the structured semantic representation of NL text. A semantic parser is implemented to transform math problem text into DOL trees. A reasoning module is included to derive math expressions fro"
D15-1135,D13-1161,0,0.0108997,"cture of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first"
D15-1135,W04-0902,0,0.201286,"Missing"
D15-1135,J08-2001,0,0.0116413,"mation gathered for one object. Liguda & Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars & Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee & Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based"
D15-1135,P07-1121,0,0.0293539,"rsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representatio"
D15-1135,D07-1071,0,0.0230829,".wolframalpha.com Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the"
D15-1135,P11-1116,1,0.919889,"Missing"
D15-1135,C10-1112,1,\N,Missing
D15-1135,C08-1116,0,\N,Missing
D15-1135,D14-1058,0,\N,Missing
D15-1135,J14-1002,0,\N,Missing
D15-1135,J02-3001,0,\N,Missing
D17-1084,D14-1058,0,0.10866,"[NUM]” to (1 − n1 ) ∗ n2 = x. In this way, we can decompose the templates and learn math concepts in a finer grain. Furthermore, we observe that problems of the same template share some common properties. By aggregating problems of the same template and 806 struction. their pre-defined semantic language. However, these methods are only effective in their designated math problem categories and are not scalable to other categories. For example, the method used by Shi et al. (2015) works extremely well for solving number word problems but not others. In the statistical machine learning approach, Hosseini et al. (2014) solves addition and subtraction problems by extracting quantities as states and derive math concepts from verbs in the training data. Kushman et al. (2014) and Zhou et al. (2015) generalize equations attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. Upadhyay et al. (2016) follows their approach and leverage math word problems without equation annotation as external resources. Seo et al. (2015) solves a set of SAT geometry questions with text and diagram provided. KoncelKedziorski et al. (2015) and Roy and Roth ("
D17-1084,P16-1084,1,0.338646,"near equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. (Kushman et al., 2014; Shi et al., 2015)), but do not scale or perform well in datasets containing various math problem types as in Huang et al. (2016), as their designed features are becoming sparse. Their process of acquiring mathematical knowledge is either sparse or based on certain assumptions of specific problem types. To alleviate this problem, we introduce our template sketch construction and fine-grained expressions learning in the next section. 3 3.1 Definition Template: It is first introduced in Kushman et al. (2014). It is a unique form of an equation system. For example, given an equation system as follows: 2 · x1 + 4 · x2 = 34 x1 + 4 = x2 This equation system is a solution for a specific math word problem. We replace the number"
D17-1084,Q15-1042,0,0.684985,"Missing"
D17-1084,P14-1026,0,0.568474,"osseini et al., 2014; Roy and Roth, 2015; KoncelKedziorski et al., 2015) derive math concepts based on observations from their dataset of specific types of problems, e.g. problems with one single equation. For example, Hosseini et al. (2014) assumes verbs and only verbs embed math concepts and map them to addition/subtraction. Roy and Roth (2015); Koncel-Kedziorski et al. (2015) assume there is only one unknown variable in the problem and cannot derive math concepts involving constants or more than one unknown variables, such as “the product of two unknown numbers”. Template-based approaches (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016), on the other hand, leverage the built-in composition structure of equation system templates to formulate all types of math concepts seen in training data, such as (1 − n1 ) ∗ n2 = x in Figure 1. However, they suffer from two major shortcomings. First, the math concepts they learned, which is expressed as an entire template, fails to capture a lot of useful information with sparse training instances. We argue that it would be more expressive if the math concept is learned in a finer granularity. Second, their learning processes rely heavily on lexica"
D17-1084,P16-1202,0,0.450844,"ions attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. Upadhyay et al. (2016) follows their approach and leverage math word problems without equation annotation as external resources. Seo et al. (2015) solves a set of SAT geometry questions with text and diagram provided. KoncelKedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. (Kushman et al., 2014; Shi et al., 2015)), but do not scale or perform well in datasets containing various math problem types as in Huang et al. (2016), as their designed features are becoming sparse. Their process of acquiring mathematical knowledge is either sparse or based on certain"
D17-1084,D15-1202,0,0.413901,"et al. (2014) solves addition and subtraction problems by extracting quantities as states and derive math concepts from verbs in the training data. Kushman et al. (2014) and Zhou et al. (2015) generalize equations attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. Upadhyay et al. (2016) follows their approach and leverage math word problems without equation annotation as external resources. Seo et al. (2015) solves a set of SAT geometry questions with text and diagram provided. KoncelKedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. (Kushman et al., 2014; Shi et al., 2015)), but do not scale or perform well"
D17-1084,D15-1171,0,0.0211217,"works extremely well for solving number word problems but not others. In the statistical machine learning approach, Hosseini et al. (2014) solves addition and subtraction problems by extracting quantities as states and derive math concepts from verbs in the training data. Kushman et al. (2014) and Zhou et al. (2015) generalize equations attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. Upadhyay et al. (2016) follows their approach and leverage math word problems without equation annotation as external resources. Seo et al. (2015) solves a set of SAT geometry questions with text and diagram provided. KoncelKedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high"
D17-1084,D15-1135,1,0.530141,"ncelKedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mitra and Baral (2016) considers addition and subtraction problems in three basic problem types: “Change”, “Part Whole” and “Comparison”. They manually design different features for each type, which is difficult to expand to more types. In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. (Kushman et al., 2014; Shi et al., 2015)), but do not scale or perform well in datasets containing various math problem types as in Huang et al. (2016), as their designed features are becoming sparse. Their process of acquiring mathematical knowledge is either sparse or based on certain assumptions of specific problem types. To alleviate this problem, we introduce our template sketch construction and fine-grained expressions learning in the next section. 3 3.1 Definition Template: It is first introduced in Kushman et al. (2014). It is a unique form of an equation system. For example, given an equation system as follows: 2 · x1 + 4 ·"
D17-1084,D16-1029,0,0.685273,"KoncelKedziorski et al., 2015) derive math concepts based on observations from their dataset of specific types of problems, e.g. problems with one single equation. For example, Hosseini et al. (2014) assumes verbs and only verbs embed math concepts and map them to addition/subtraction. Roy and Roth (2015); Koncel-Kedziorski et al. (2015) assume there is only one unknown variable in the problem and cannot derive math concepts involving constants or more than one unknown variables, such as “the product of two unknown numbers”. Template-based approaches (Kushman et al., 2014; Zhou et al., 2015; Upadhyay et al., 2016), on the other hand, leverage the built-in composition structure of equation system templates to formulate all types of math concepts seen in training data, such as (1 − n1 ) ∗ n2 = x in Figure 1. However, they suffer from two major shortcomings. First, the math concepts they learned, which is expressed as an entire template, fails to capture a lot of useful information with sparse training instances. We argue that it would be more expressive if the math concept is learned in a finer granularity. Second, their learning processes rely heavily on lexical and syntactic features, such as the depen"
D17-1084,D15-1096,0,0.500547,"Missing"
D17-1088,D14-1058,0,0.213498,"erformance improvement. 3) A large dataset is constructed for facilitating the study of automatic math problem solving.1 The remaining part of this paper is organized as follows: After analyzing related work in Section 2, we formalize the problem and introduce our dataset in Section 3. We present our RNN-based seq2seq model in Section 4, and the hybrid model in Section 5. Then experimental results are shown and analyzed in Section 6. Finally we conclude the paper in Section 7. 2 2.1 Since 2014, statistical learning based approaches are proposed to solve the math word problems. Hosseini et al. Hosseini et al. (2014) deal with the open-domain aspect of algebraic word problems by learning verb categorization from training data. Kushman et al. Kushman et al. (2014) proposed a equation template system to solve a wide range of algebra word problems. Zhou et al. Zhou et al. (2015) further extends this method by adopting the max-margin objective, which results in higher accuracy and lower time cost. In addition, Roy and Roth Roy et al. (2015); Roy and Roth (2016) tries to handle arithmetic problems with multiple steps and operations without depending on additional annotations or predefined templates. Mitra et a"
D17-1088,P16-1084,1,0.620088,"solving. 1 Table 1: A math word problem syntactic parsing, and machine translation), it may be interesting to study whether DNN could also help math word problem solving. In this paper, we propose a recurrent neural network (RNN) model for automatic math word problem solving. It is a sequence to sequence (seq2seq) model that transforms natural language sentences in math word problems to mathematical equations. Experiments conducted on a large dataset show that the RNN model significantly outperforms state-of-the-art statistical learning approaches. Introduction Since it has been demonstrated Huang et al. (2016) that a simple similarity based method performs as well as more sophisticated statistical learning approaches on large datasets, we implement a similarity-based retrieval model and compare with our seq2seq model. We observe that although seq2seq performs better on average, the retrieval model is able to correctly solve many problems for which RNN generates wrong results. We also find that the accuracy of the retrieval model positively correlate with the maximal similarity score between the target problem and the problems in training data: the larger the similarity score, the higher the average"
D17-1088,Y14-1015,0,0.0506579,"etrieval model is able to correctly solve many problems for which RNN generates wrong results. We also find that the accuracy of the retrieval model positively correlate with the maximal similarity score between the target problem and the problems in training data: the larger the similarity score, the higher the average accuracy is. Developing computer models to automatically solve math word problems has been an interest of NLP researchers since 1963 Feigenbaum et al. (1963); Bobrow (1964); Briars and Larkin (1984); Fletcher (1985). Recently, machine learning techniques Kushman et al. (2014); Amnueypornsakul and Bhat (2014); Zhou et al. (2015); Mitra and Baral (2016) and semantic parsing methods Shi et al. (2015); Koncel-Kedziorski et al. (2015) are proposed to tackle this problem and promising results are reported on some datasets. Although progress has been made in this task, performance of state-of-the-art techniques is still quite low on large datasets having diverse problem types Huang et al. (2016). A typical math word problems are shown in Table 1. The reader is asked to infer how many pens Dan and Jessica have, based on constraints provided. Given the success of deep neural networks (DNN) on many NLP tas"
D17-1088,D16-1197,0,0.0342258,"Missing"
D17-1088,P16-1202,0,0.72984,"ms for which RNN generates wrong results. We also find that the accuracy of the retrieval model positively correlate with the maximal similarity score between the target problem and the problems in training data: the larger the similarity score, the higher the average accuracy is. Developing computer models to automatically solve math word problems has been an interest of NLP researchers since 1963 Feigenbaum et al. (1963); Bobrow (1964); Briars and Larkin (1984); Fletcher (1985). Recently, machine learning techniques Kushman et al. (2014); Amnueypornsakul and Bhat (2014); Zhou et al. (2015); Mitra and Baral (2016) and semantic parsing methods Shi et al. (2015); Koncel-Kedziorski et al. (2015) are proposed to tackle this problem and promising results are reported on some datasets. Although progress has been made in this task, performance of state-of-the-art techniques is still quite low on large datasets having diverse problem types Huang et al. (2016). A typical math word problems are shown in Table 1. The reader is asked to infer how many pens Dan and Jessica have, based on constraints provided. Given the success of deep neural networks (DNN) on many NLP tasks (like POS tagging, Inspired by these obse"
D17-1088,W14-4012,0,0.126015,"Missing"
D17-1088,Q15-1001,0,0.0860772,"n 6. Finally we conclude the paper in Section 7. 2 2.1 Since 2014, statistical learning based approaches are proposed to solve the math word problems. Hosseini et al. Hosseini et al. (2014) deal with the open-domain aspect of algebraic word problems by learning verb categorization from training data. Kushman et al. Kushman et al. (2014) proposed a equation template system to solve a wide range of algebra word problems. Zhou et al. Zhou et al. (2015) further extends this method by adopting the max-margin objective, which results in higher accuracy and lower time cost. In addition, Roy and Roth Roy et al. (2015); Roy and Roth (2016) tries to handle arithmetic problems with multiple steps and operations without depending on additional annotations or predefined templates. Mitra et al. Mitra and Baral (2016) presents a novel method to learn to use formulas to solve simple additionsubtraction arithmetic problems. As reported in 2016 Huang et al. (2016), stateof-the-art approaches have extremely low performance on a big and highly diverse data set (18,000+ problems). In contrast to these approaches, we study the feasibility of applying deep learning to the task of math word problem solving. Related work M"
D17-1088,P15-1152,0,0.053733,"Missing"
D17-1088,D15-1135,1,0.559026,"nd that the accuracy of the retrieval model positively correlate with the maximal similarity score between the target problem and the problems in training data: the larger the similarity score, the higher the average accuracy is. Developing computer models to automatically solve math word problems has been an interest of NLP researchers since 1963 Feigenbaum et al. (1963); Bobrow (1964); Briars and Larkin (1984); Fletcher (1985). Recently, machine learning techniques Kushman et al. (2014); Amnueypornsakul and Bhat (2014); Zhou et al. (2015); Mitra and Baral (2016) and semantic parsing methods Shi et al. (2015); Koncel-Kedziorski et al. (2015) are proposed to tackle this problem and promising results are reported on some datasets. Although progress has been made in this task, performance of state-of-the-art techniques is still quite low on large datasets having diverse problem types Huang et al. (2016). A typical math word problems are shown in Table 1. The reader is asked to infer how many pens Dan and Jessica have, based on constraints provided. Given the success of deep neural networks (DNN) on many NLP tasks (like POS tagging, Inspired by these observations, we design a hybrid model which combin"
D17-1088,E17-1047,0,0.613124,"Missing"
D17-1088,D16-1137,0,0.0215516,"Missing"
D17-1088,D15-1096,0,0.699974,"ly solve many problems for which RNN generates wrong results. We also find that the accuracy of the retrieval model positively correlate with the maximal similarity score between the target problem and the problems in training data: the larger the similarity score, the higher the average accuracy is. Developing computer models to automatically solve math word problems has been an interest of NLP researchers since 1963 Feigenbaum et al. (1963); Bobrow (1964); Briars and Larkin (1984); Fletcher (1985). Recently, machine learning techniques Kushman et al. (2014); Amnueypornsakul and Bhat (2014); Zhou et al. (2015); Mitra and Baral (2016) and semantic parsing methods Shi et al. (2015); Koncel-Kedziorski et al. (2015) are proposed to tackle this problem and promising results are reported on some datasets. Although progress has been made in this task, performance of state-of-the-art techniques is still quite low on large datasets having diverse problem types Huang et al. (2016). A typical math word problems are shown in Table 1. The reader is asked to infer how many pens Dan and Jessica have, based on constraints provided. Given the success of deep neural networks (DNN) on many NLP tasks (like POS tagging"
D17-1088,Q15-1042,0,\N,Missing
D18-1297,D16-1127,0,0.321596,"stical re-weighting method that assigns different weights for the multiple responses of the same query, and trains the standard neural generation model with the weights. Experimental results on a large Chinese dialogue corpus show that our method improves the acceptance rate of generated responses compared with several baseline models and significantly reduces the number of generated generic responses. 1 Introduction Many recent works have been proposed to use neural networks to generate responses for opendomain dialogue systems (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,c; Serban et al., 2017; Shen et al., 2017; Li et al., 2017; Yu et al., 2017; Xu et al., 2017). These methods are inspired by the sequence-tosequence (Seq2Seq) framework (Sutskever et al., 2014), which is originally applied for Neural Machine Translation (NMT). They aim at maximizing the probability of generating a response given an input query, and generally use the maximum likelihood estimation (MLE) as their objective function. However, various problems occur when Seq2Seq ∗ This work was done while Yahui Liu was with Tencent AI Lab. † Corresponding author models are used for dialogue gener"
D18-1297,D17-1230,0,0.0404807,"r the multiple responses of the same query, and trains the standard neural generation model with the weights. Experimental results on a large Chinese dialogue corpus show that our method improves the acceptance rate of generated responses compared with several baseline models and significantly reduces the number of generated generic responses. 1 Introduction Many recent works have been proposed to use neural networks to generate responses for opendomain dialogue systems (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,c; Serban et al., 2017; Shen et al., 2017; Li et al., 2017; Yu et al., 2017; Xu et al., 2017). These methods are inspired by the sequence-tosequence (Seq2Seq) framework (Sutskever et al., 2014), which is originally applied for Neural Machine Translation (NMT). They aim at maximizing the probability of generating a response given an input query, and generally use the maximum likelihood estimation (MLE) as their objective function. However, various problems occur when Seq2Seq ∗ This work was done while Yahui Liu was with Tencent AI Lab. † Corresponding author models are used for dialogue generation tasks. One of the most important problems is that such"
D18-1297,W17-5546,0,0.170061,"verse decoding approach (Li et al., 2016b), which modifies the beam search to re-rank meaningful responses into higher positions. Similar works explore different ways to encourage response diversity for picking less generic responses in the decoding search (Vijayakumar et al., 2016; Li and Jurafsky, 2016). In the reinforcement learning framework (Li et al., 2016c), the reward function used in the decoding considers the ease of answering, which is measured by a distance towards a set of 8 generic responses. Thus, it can also alleviate the problem of generating generic responses to some extent. Lison and Bibauw (2017) proposed to add a weighting model to learn the “quality” of the query and response pair, but it relies heavily on additional inputs. All these works tried to add extra optimized terms in the encoding or decoding modules in Seq2Seq, making the training or prediction more complicated. In this work, we consider the reason why Seq2Seq often generates generic responses by analyzing the MLE objective function directly. We notice that multiple responses are often associated with one single input query. As shown in Figure 1, the relationship between queries and responses is much looser in conversatio"
D18-1297,P02-1040,0,0.102419,"Missing"
D18-1297,D17-1065,0,0.0810922,"me query, and trains the standard neural generation model with the weights. Experimental results on a large Chinese dialogue corpus show that our method improves the acceptance rate of generated responses compared with several baseline models and significantly reduces the number of generated generic responses. 1 Introduction Many recent works have been proposed to use neural networks to generate responses for opendomain dialogue systems (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,c; Serban et al., 2017; Shen et al., 2017; Li et al., 2017; Yu et al., 2017; Xu et al., 2017). These methods are inspired by the sequence-tosequence (Seq2Seq) framework (Sutskever et al., 2014), which is originally applied for Neural Machine Translation (NMT). They aim at maximizing the probability of generating a response given an input query, and generally use the maximum likelihood estimation (MLE) as their objective function. However, various problems occur when Seq2Seq ∗ This work was done while Yahui Liu was with Tencent AI Lab. † Corresponding author models are used for dialogue generation tasks. One of the most important problems is that such models are inclined to generate ge"
D18-1297,P15-1152,0,0.0331117,"ric patterns. Inspired by this observation, we introduce a statistical re-weighting method that assigns different weights for the multiple responses of the same query, and trains the standard neural generation model with the weights. Experimental results on a large Chinese dialogue corpus show that our method improves the acceptance rate of generated responses compared with several baseline models and significantly reduces the number of generated generic responses. 1 Introduction Many recent works have been proposed to use neural networks to generate responses for opendomain dialogue systems (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,c; Serban et al., 2017; Shen et al., 2017; Li et al., 2017; Yu et al., 2017; Xu et al., 2017). These methods are inspired by the sequence-tosequence (Seq2Seq) framework (Sutskever et al., 2014), which is originally applied for Neural Machine Translation (NMT). They aim at maximizing the probability of generating a response given an input query, and generally use the maximum likelihood estimation (MLE) as their objective function. However, various problems occur when Seq2Seq ∗ This work was done while Yahui Liu was with Tencent AI L"
D18-1297,N15-1020,0,0.0517093,"ed by this observation, we introduce a statistical re-weighting method that assigns different weights for the multiple responses of the same query, and trains the standard neural generation model with the weights. Experimental results on a large Chinese dialogue corpus show that our method improves the acceptance rate of generated responses compared with several baseline models and significantly reduces the number of generated generic responses. 1 Introduction Many recent works have been proposed to use neural networks to generate responses for opendomain dialogue systems (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,c; Serban et al., 2017; Shen et al., 2017; Li et al., 2017; Yu et al., 2017; Xu et al., 2017). These methods are inspired by the sequence-tosequence (Seq2Seq) framework (Sutskever et al., 2014), which is originally applied for Neural Machine Translation (NMT). They aim at maximizing the probability of generating a response given an input query, and generally use the maximum likelihood estimation (MLE) as their objective function. However, various problems occur when Seq2Seq ∗ This work was done while Yahui Liu was with Tencent AI Lab. † Corresponding au"
D18-1420,K16-1002,0,0.108391,"Missing"
D18-1420,Q18-1031,0,0.0250233,"ms, producing monotonous language, and generating short common sentences (Li et al., 2017). To solve these problems, some researchers branch out into the way of post-editing (could be under some guidance, say sentiment polarity) a given message to generate text of better quality. For example, skeleton-based text generation first outlines a skeleton in the form of phrases/words, and then starts from the skeleton to generate text (Wang et al., 2017; Xiao et al., 2016). Another line of works conduct editing on an existing sentence and expect that the output will serve particular purposes better (Guu et al., 2018). Similarly in conversation, some systems post-edit the retrieval results to generate new sentences as the response (Song et al., 2016). The third type is to perform editing on the input under the guidance of specific style. For example, Shen et al. (2017) take a sentence with negative sentiment as input, and edit it to transfer its sentiment polarity into positive. In this paper, we generalize the third type of post-editing into a more general scenario, named Quantifiable Sequence Editing (QuaSE). Specifically, in the training stage, each input sentence is associated with a numeric outcome. F"
D18-1420,W17-4902,0,0.0179707,"lies in the content factors z and z 0 . Given that z and z 0 are not enforced to resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen"
D18-1420,D17-1230,0,0.030564,"lts are reported and discussed to elaborate the peculiarities of our framework. 1 ∗ The work described in this paper was done when Yi Liao was an intern at Tencent AI Lab. The work is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: CUHK413510) 1 Our code and data are available at https:// bitbucket.org/leoeaton/quase/src/master/ Introduction Typical neural text generation is observed suffering from the problems of repetitions in word ngrams, producing monotonous language, and generating short common sentences (Li et al., 2017). To solve these problems, some researchers branch out into the way of post-editing (could be under some guidance, say sentiment polarity) a given message to generate text of better quality. For example, skeleton-based text generation first outlines a skeleton in the form of phrases/words, and then starts from the skeleton to generate text (Wang et al., 2017; Xiao et al., 2016). Another line of works conduct editing on an existing sentence and expect that the output will serve particular purposes better (Guu et al., 2018). Similarly in conversation, some systems post-edit the retrieval results"
D18-1420,N18-1169,0,0.0943885,"Missing"
D18-1420,P14-5010,0,0.00247152,"ry of the primary data. The vocabulary size of the dataset is 9,625. In total, our dataset contains 599K sentences, and we randomly hold 50K for test, 10K for validation, and the remaining for training. For training, we need each input sentence being associated with a rating value, and for test, we need to measure the rating of a generated sentence to check if the generated sentence satisfies the specified outcome target. Therefore, an automatic method is needed for measuring the rating values of training sentences and generated sentences. We employ the sentiment analyzer in Stanford CoreNLP (Manning et al., 2014) to do so. Specifically, we first invoke CoreNLP to output the probability of each rating in {1, 2, 3, 4, 5} for a sentence, then we take the sum of the probabilitymultiplied ratings as the sentence rating. Some statistics of the data is given in Table 2. Hereafter, we use “rating” and “outcome” interchangeably. 3859 2 https://www.yelp.com/dataset/challenge Rating interval Sentence# [1, 2) 34273 [2, 3) 231740 [3, 4) 165159 [4, 5] 167803 4.2 Table 2: Numbers of sentences in each rating interval. One may think that would it be possible to use the original rating given by Yelp users as outcome fo"
D18-1420,Q18-1027,0,0.0238259,"Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prabhumoye et al., 2018) first learns the latent representation of th"
D18-1420,P18-1080,0,0.0520265,"resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prabhumoye et al., 2018) first learns the late"
D18-1420,D17-1298,0,0.0298791,"of the style. A transferred sentence is generated from a modified latent representation. Different from the aforementioned works based on latent representations, Li et al. (2018) propose a simpler method that achieves attribute transfer by changing a few attribute marker words or phrases in the sentence that are indicative of a particular attribute, while leaving the rest of the sentence largely unchanged. The simple method is able to generate better-quality sentences than the aforementioned works. Besides style transfer, sentence editing models can be developed for other tasks. For example, Schmaltz et al. (2017) propose neural sequence-labelling models for correcting the grammatical errors of sentences. 6 Conclusions We proposed a new task namely Quantifiable Sequence Editing (QuaSE), where a model needs to edit an input sentences towards the direction of a numerical outcome target. To tackle this task, we proposed a novel framework that simultaneously exploits the single sentences and pseudo-parallel sentence pairs. For evaluation, we prepared a dataset with Yelp sentences and their ratings. Experimental results show that our framework outperforms the compared methods under the measures of sentiment"
D18-1420,N18-1138,0,0.0434783,"en that z and z 0 are not enforced to resemble each other when Lsim is excluded from this tuning step, Lrec and Ld−rec cannot be minimized simultaneously. Moreover, when we minimize Lsim in the second step with the weights of Lrec and Lmse fixed, we observe that Ld−rec also decreases, which complies with the above analysis. 5 Related Works Inspired by the task of image style transfer (Gatys et al., 2016; Liu and Tuzel, 2016), researchers proposed the task of text style transfer and obtained some encouraging results (Fu et al., 2018; Hu et al., 2017; Jhamtani et al., 2017; Melnyk et al., 2017; Zhang et al., 2018; Li et al., 2018; Prabhumoye et al., 2018; Niu and Bansal, 2018). Existing studies on text style transfer mainly aim at transferring text from an original style into a target style, e.g., from negative to positive, from male to female, from rude/normal to polite; from modern text to Shakespeare style, etc. In contrast, our proposed task QuaSE assumes each sentence is associated with an outcome pertaining to continues values, and the editing is under the guidance of a specific target. To transfer the style of a sentence, the paradigm of most works (Shen et al., 2017; Mueller et al., 2017; Prab"
D18-1423,K16-1002,0,0.064903,"in a topic, especially when they are generated or extracted from an inventory (Wang et al., 2016c). On the other hand, Chinese poems are generally short in length, with every character carefully chosen to be concise and elegant. Yet, prior poem generation models with recurrent neural networks (RNN) are likely to generate highfrequency characters (Zhang et al., 2017a), and the resulted poems are trivial and boring. The reason is that RNN tends to be entrapped within local word co-occurrences, they normally fail to capture global characteristic such as topic or hierarchical semantic properties (Bowman et al., 2016). To address the aforementioned shortcomings, RNN is extended to autoencoder (Dai and Le, 2015) for improving sequence learning, which has *Corresponding author: Rui Yan (ruiyan@pku.edu.cn) †Work was partially done at Tencent AI Lab. 1 We use term and character interchangeably in this paper. 3890 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3890–3900 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics been proven to be appealing in explicitly modeling global properties such as syntactic, semantic, a"
D18-1423,D16-1126,0,0.031283,"s confirm the validity and effectiveness of our model, where its automatic and human evaluation scores outperform existing models. 1 Introduction In mastering concise, elegant wordings with aesthetic rhythms in fixed patterns, classical Chinese poem is a special cultural heritage to record personal emotions and political views, as well as document daily or historical events. Being a fascinating art, writing poems is an attractive task that researchers of artificial intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consist"
D18-1423,P17-4008,0,0.0532257,"Missing"
D18-1423,P17-1059,0,0.0269444,"e the reconstruction log-likelihood of the input x under the condition of c. Following the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is generated only conditioned on the title T"
D18-1423,D10-1051,0,0.142401,"ion with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular text genre, poetry. Automatic Poem Generation. According to methodology, previous approaches can be roughly classified into three categories: 1) rule and template based methods (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Zhou et al., 2010; Oliveira, 2012; Yan et al., 2013); 2) SMT approaches (Jiang and Zhou, 2008; Greene et al., 2010; He et al., 2012); 3) deep neural models (Zhang and Lapata, 2014; Wang et al., 2016b; Yan, 2016). Compared to rule-based and SMT models, neural models are able to learn more complicated representations and generate smooth poems. Most recent studies followed this paradigm. For example, Wang et al. (2016c) proposed a modified encoder-decoder model with keyword planning; Zhang et al. (2017a) adopted memory-augmented RNNs to dynamically choose each term from RNN output or a reserved inventory. To improve thematic consistency, Yang et al. (2017) combined CVAE and keywords planning. Compared to the"
D18-1423,P17-1016,0,0.0172345,"(Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is preferred that all lines concentrate on the same theme throughout a poem. Previous work mainly focused on using keywords (Wang et al., 2016c; Hopkins and Kiela, 2017) to plan a poem so as to generate each line with a specific keyword. Such strategy is risky for the reason that the keywords are not guaranteed consistent in a topic, especially when they are generated or extracted from an inventory (Wang et al., 2016c). On the other hand, Chinese poems are generally short in length, with every character carefully chosen to be concise and elegant. Yet, prior poem generation models with recurrent neural networks (RNN) are likely to generate highfrequency characters (Zhang et al., 2017a), and the resulted poems are trivial and boring. The reason is that RNN tend"
D18-1423,C08-1048,0,0.0401591,"2017) and text generation with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular text genre, poetry. Automatic Poem Generation. According to methodology, previous approaches can be roughly classified into three categories: 1) rule and template based methods (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Zhou et al., 2010; Oliveira, 2012; Yan et al., 2013); 2) SMT approaches (Jiang and Zhou, 2008; Greene et al., 2010; He et al., 2012); 3) deep neural models (Zhang and Lapata, 2014; Wang et al., 2016b; Yan, 2016). Compared to rule-based and SMT models, neural models are able to learn more complicated representations and generate smooth poems. Most recent studies followed this paradigm. For example, Wang et al. (2016c) proposed a modified encoder-decoder model with keyword planning; Zhang et al. (2017a) adopted memory-augmented RNNs to dynamically choose each term from RNN output or a reserved inventory. To improve thematic consistency, Yang et al. (2017) combined CVAE and keywords plan"
D18-1423,D16-1032,0,0.0321585,"e objective of CVAE is thus to maximize the reconstruction log-likelihood of the input x under the condition of c. Following the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is gen"
D18-1423,N16-1014,0,0.0818547,"is paper. Similarity: For thematic consistency, it is challenging to automatically evaluate different models. We adopt the embedding average metric to score sentence-level similarity as that was applied in Wieting et al. (2015). In this paper, we accumulate the embeddings of all characters from the generated poems and that from the given title, and use cosine to compute the similarity between the two accumulated embeddings. Distinctness: As an important characteristic, poems use novel and unique characters to maintain their elegance and delicacy. Similar to that proposed for dialogue systems (Li et al., 2016), this evaluation is employed to measure character diversity by calculating the proportion of distinctive [1,4]-grams12 in the generated poems, where final distinctness values are normalized to [0,100]. Human Evaluation: Since writing poems is a complicated task, there always exist incoordinations between automatic metrics and human experiences. Hence, we conduct human evaluation to 11 We tried different values for λ, varying from 0.001 to 1, which result in similar performance of the CVAE-D. 12 Defined as the number of distinctive n-grams divided by the total number of n-grams, shown as Dist-"
D18-1423,P15-1107,0,0.0609514,"Missing"
D18-1423,D17-1230,0,0.0259758,"s may deliver different mood from others. Since our model does not explicitly control such attributes, thus one potential solution to address this issue is to introduce other features to model such information, which requires a special design to adjust the current model. We also notice there exists a few extraordinary bad cases where their basic characteristics, such as wording, fluency, etc., are unacceptable. This phenomenon is randomly observed with no patterns, which could be explained by the complexity of the model and the fragile natural of adversarial training (Goodfellow et al., 2014; Li et al., 2017). Careful parameter setting and considerate module assemble could mitigate this problem, thus lead to potential future work of designing more robust frameworks. 6 Related Work Deep Generative Models. This work can be seen as an extension of research on deep generative models (Salakhutdinov and Hinton, 2009; Bengio et al., 2014), where most of the previous work, including VAE and CVAE, focused on image generation (Sohn et al., 2015; Yan et al., 2016b). Since GAN (Goodfellow et al., 2014) is also a successful generative model, there are studies tried to integrate VAE and GAN (Larsen et al., 2016"
D18-1423,P02-1040,0,0.102408,"2.16 2.14 2.58 2.29 2.08 2.53 2.35 2.34 2.96 Ovr. 1.74 1.79 2.13 2.23 2.13 2.14 2.18 2.56 Table 3: Results of automatic and human evaluations. BLEU-1 and BLEU-2 are BLEU scores on unigrams and bigrams (p < 0.01); Sim refer to the similarity score; Dist-n corresponds to the distinctness of n-gram, with n = 1 to 4; Con., Flu., Mea., Poe., Ovr. represent consistency, fluency, meaning, poeticness, and overall, respectively. 2017) to set the balancing parameter λ to 0.1.11 4.4 Evaluation Metrics To comprehensively evaluate the generated poems, we employ the following metrics: BLEU: The BLEU score (Papineni et al., 2002) is an effective metric, widely used in machine translation, for measuring word overlapping between ground truth and generated sentences. In poem generation, BLEU is also utilized as a metric in previous studies (Zhang and Lapata, 2014; Wang et al., 2016a; Yan, 2016; Wang et al., 2016b). We follow their settings in this paper. Similarity: For thematic consistency, it is challenging to automatically evaluate different models. We adopt the embedding average metric to score sentence-level similarity as that was applied in Wieting et al. (2015). In this paper, we accumulate the embeddings of all c"
D18-1423,C16-1100,0,0.272591,"that researchers of artificial intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consisten"
D18-1423,P17-1046,0,0.0143534,"aintaining the advantages of VAE. It is verified in supervised dialogue generation (Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017) that CVAE can generate better responses with given dialogue contexts. Given the above background and to align it with our expectations for poem generation, it is worth trying to apply CVAE to create poems. In the meantime, consider that modeling thematic consistency with adversarial training is proven to be promising in controlled text generation (Hu et al., 2017), models for semantic matching can be potentially improved with an explicit discriminator (Wu et al., 2017), so does poem generation. In this paper, we propose a novel poem generation model (CVAE-D) using CVAE to generate novel terms and a discriminator (D) to explicitly control thematic consistency with adversarial training. To the best of our knowledge, this is the first work of generating poems with the combination of CVAE and adversarial training. Experiments on a large classical Chinese poetry corpus confirm that, through encoding inputs with latent variables and explicit measurement of thematic information, the proposed model outperforms existing ones in various evaluations. Quantitative and"
D18-1423,P17-1125,0,0.200231,"intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is"
D18-1423,D14-1074,0,0.518039,"the operation for VAE, we have the corresponding variational lower bound of pθ (x|c) formulated as L(θ, φ; x, c) = − KL(qφ (z|x, c) k pθ (z|c)) + Eqφ (z|x,c) [log pθ (x|z, c)] (2) which is similar to Eq.1 except that all items are introduced with c, such as qφ (z|x, c) and pθ (z|c), 3891 referring to the conditioned approximate posterior and the conditioned prior, respectively. 2.2 Problem Formulation Following the text-to-text generation paradigm (Ranzato et al., 2015; Kiddon et al., 2016; Hu et al., 2017; Ghosh et al., 2017), our task has a similar problem setting with conventional studies (Zhang and Lapata, 2014; Wang et al., 2016c), where a poem is generated in a line-by-line manner that each line serves as the input for the next one, as illustrated in Figure 1. To formulate this task, we separate its input and output with necessary notations as follows. The I NPUT of the entire model is a title, T =(e1 ,e2 ,. . . ,eN ), functionalized as the theme of the target poem2 , where ei refers to i-the character’s embedding and N is the length of the title. The first line L1 is generated only conditioned on the title T , once this step is done, the model takes the input of the previous generated line as wel"
D18-1423,Q17-1036,0,0.166232,"intelligence are interested in (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2012; Yan et al., 2013, 2016a; Ghazvininejad et al., 2016, 2017; Singh et al., 2017; Xu et al., 2018), partially for the reason that poem generation and its related research could benefit other constrained natural language generation tasks. Conventionally, rule-based models (Zhou et al., 2010) and statistical machine translation (SMT) models (He et al., 2012) are proposed for this task. Recently, deep neural models are employed to generate fluent and natural poems (Wang et al., 2016a; Yan, 2016; Zhang et al., 2017a). Although these models look promising, they are limited in many aspects, e.g., previous studies generally fail to keep thematic consistency (Wang et al., 2016c; Yang et al., 2017) and improve term1 novelty (Zhang et al., 2017a), which are important characteristics of poems. In classical Chinese poem composing, thematic consistency and term novelty are usually mutually exclusive conditions to each other, i.e., consistent lines may bring duplicated terms while intriguing choices of characters could result in thematic diversities. On one hand, thematic consistency is essential for poems; it is"
D18-1423,P17-1061,0,0.472874,"obal properties such as syntactic, semantic, and discourse coherence (Li et al., 2015). Moreover, boosting autoencoder with variational inference (Kingma and Welling, 2014), known as variational autoencoder (VAE), can generate not only consistent but also novel and fluent term sequences (Bowman et al., 2016). To generalize VAE for versatile scenarios, conditional variational autoencoders (CVAE) are proposed to supervise a generation process with certain attributes while maintaining the advantages of VAE. It is verified in supervised dialogue generation (Serban et al., 2017; Shen et al., 2017; Zhao et al., 2017) that CVAE can generate better responses with given dialogue contexts. Given the above background and to align it with our expectations for poem generation, it is worth trying to apply CVAE to create poems. In the meantime, consider that modeling thematic consistency with adversarial training is proven to be promising in controlled text generation (Hu et al., 2017), models for semantic matching can be potentially improved with an explicit discriminator (Wu et al., 2017), so does poem generation. In this paper, we propose a novel poem generation model (CVAE-D) using CVAE to generate novel terms"
D18-1423,P16-1222,1,0.843427,"with no patterns, which could be explained by the complexity of the model and the fragile natural of adversarial training (Goodfellow et al., 2014; Li et al., 2017). Careful parameter setting and considerate module assemble could mitigate this problem, thus lead to potential future work of designing more robust frameworks. 6 Related Work Deep Generative Models. This work can be seen as an extension of research on deep generative models (Salakhutdinov and Hinton, 2009; Bengio et al., 2014), where most of the previous work, including VAE and CVAE, focused on image generation (Sohn et al., 2015; Yan et al., 2016b). Since GAN (Goodfellow et al., 2014) is also a successful generative model, there are studies tried to integrate VAE and GAN (Larsen et al., 2016). In natural language processing, many recent deep generative models are applied to dialogue systems Serban et al. (2017); Shen et al. (2017); Zhao et al. (2017) and text generation with (Hu et al., 2017; Yu et al., 2017; Lin et al., 2017; Zhang et al., 2017b; Guo et al., 2018). To the best of our knowledge, this work is the first one integrating CVAE and adversarial training with a discriminator for text generation, especially in a particular tex"
D18-1457,P18-1008,0,0.090299,"Missing"
D18-1457,P05-1066,0,0.0379572,"maximum length limited to 50, consisting of about 20.62 million sentence pairs. We used newsdev2017 as the development set and newstest2017 as the test set. For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs. We used newstest2013 as the development set and newstest2014 as the test set. Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. Models. We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017). We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced 1 We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are in opposite directions. In such case, the two vectors are in fact linearly dependent, while we aim at encouraging the vectors independent from each other. their reported"
D18-1457,P17-1106,0,0.0203105,"ks with advanced connecting strategies outperform their shallow counterparts. Due to its simplicity and effectiveness, skip connection becomes a standard component of state-of-the-art NMT models (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). In this work, we prove that deep representation exploitation can further improve performance over simply using skip connections. Representation Interpretation Several researchers have tried to visualize the representation of each layer to help better understand what information each layer captures (Zeiler and Fergus, 2014; Li et al., 2016; Ding et al., 2017). Concerning natural language processing tasks, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers. Anastasopoulos and Chiang (2018) show that higher level layers are more representative than lower level layers. Peters et al. (2018) demonstrate that higher-level layers capture context-dependent aspects of word meaning while lower-level layers model aspects of syntax. Inspired by these observations, we propose to expose all of these representations to better 4260 fuse information across la"
D18-1457,D18-1317,1,0.832724,"Missing"
D18-1457,N18-1008,0,0.0250029,"Luong et al., 2015). Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018). ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab. Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018). For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers. However, current NMT models only leverage the top layers of encoder and decoder in the subsequent process, which misses the opportunity to exploit useful information embedded in other layers. Recently, aggregating layers to better fuse semantic and spatial information has proven to be of profound value in computer vision tasks (Huang et al., 2017; Yu et al., 2018). In natural language processing community, Peters et al. (2"
D18-1457,D16-1025,0,0.0266353,"× × X X X Length of Source Sentence Figure 4: BLEU scores on the En⇒De test set with respect to various input sentence lengths. “Hier.” denotes hierarchical aggregation and “Div.” denotes diversity regularization. 4.3.1 Length Analysis Following Bahdanau et al. (2015) and Tu et al. (2016), we grouped sentences of similar lengths together and computed the BLEU score for each group, as shown in Figure 4. Generally, the performance of T RANSFORMER -BASE goes up with the increase of input sentence lengths, which is superior to the performance of RNN-based NMT models on long sentences reported by (Bentivogli et al., 2016). We attribute this to the strength of self-attention mechanism to model global dependencies without regard to their distance. Clearly, the proposed approaches outperform the baseline model in all length segments, while there are still considerable differences between the two variations. Hierarchical aggregation consistently outperforms the baseline model, and the improvement goes up on long sentences. One possible reason is that long sentences indeed require deep aggregation mechanisms. Introducing diversity regularization further improves performance on most sentences (e.g. ≤ 45), while the"
D18-1457,D15-1166,0,0.263339,"Missing"
D18-1457,P02-1040,0,0.100765,"the Zh⇒En task, we used all of the available parallel data with maximum length limited to 50, consisting of about 20.62 million sentence pairs. We used newsdev2017 as the development set and newstest2017 as the test set. For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs. We used newstest2013 as the development set and newstest2014 as the test set. Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. Models. We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017). We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced 1 We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are in opposite directions. In such case, the two vectors are in fact linearly dependent, while we aim at enco"
D18-1457,N18-1202,0,0.722334,"hdanau et al., 2015; Luong et al., 2015). Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018). ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab. Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018). For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers. However, current NMT models only leverage the top layers of encoder and decoder in the subsequent process, which misses the opportunity to exploit useful information embedded in other layers. Recently, aggregating layers to better fuse semantic and spatial information has proven to be of profound value in computer vision tasks (Huang et al., 2017; Yu et al., 2018). In natural language proc"
D18-1457,P16-1162,0,0.213758,"e conducted experiments on both Chinese⇒English (Zh⇒En) and English⇒German (En⇒De) translation tasks. For the Zh⇒En task, we used all of the available parallel data with maximum length limited to 50, consisting of about 20.62 million sentence pairs. We used newsdev2017 as the development set and newstest2017 as the test set. For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs. We used newstest2013 as the development set and newstest2014 as the test set. Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. Models. We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017). We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced 1 We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are"
D18-1457,N18-2074,0,0.0542048,"Missing"
D18-1457,1983.tc-1.13,0,0.597316,"Missing"
D18-1457,D18-1475,1,0.796356,"Missing"
D18-1457,D16-1159,0,0.234053,"ttention model (Bahdanau et al., 2015; Luong et al., 2015). Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018). ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab. Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018). For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers. However, current NMT models only leverage the top layers of encoder and decoder in the subsequent process, which misses the opportunity to exploit useful information embedded in other layers. Recently, aggregating layers to better fuse semantic and spatial information has proven to be of profound value in computer vision tasks (Huang et al., 2017; Yu et al., 2018). In"
D18-1457,Q17-1007,1,0.866873,"r with a residual connection: l l−1 H = Layer(H )+ l−1 X Hi . where {W1 , . . . , WL } are trainable matrices. While the strategy is similar in spirit to (Peters et al., 2018), there are two main differences: (1) they use normalized weights while we directly use parameters that could be either positive or negative numbers, which may benefit from more modeling flexibility. (2) they use a scalar that is shared by all elements in the layer states, while we use learnable matrices. The latter offers a more precise control of the combination by allowing the model to be more expressive than scalars (Tu et al., 2017). We also investigate strategies that iteratively and hierarchically merge layers by incorporating more depth and sharing, which have proven effective for computer vision tasks (Yu et al., 2018). Iterative Aggregation. As illustrated in Figure 1d, iterative aggregation follows the iterated stacking of the backbone architecture. Aggregation begins at the shallowest, smallest scale and then iteratively merges deeper, larger scales. The iterative deep aggregation function I for a series of layers Hl1 = {H1 , · · · , Hl } with increasingly deeper and semantic information is formulated as b l = I(H"
D18-1457,P17-4012,0,0.0732084,"set consisting of about 4.56 million sentence pairs. We used newstest2013 as the development set and newstest2014 as the test set. Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs. We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. Models. We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017). We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced 1 We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are in opposite directions. In such case, the two vectors are in fact linearly dependent, while we aim at encouraging the vectors independent from each other. their reported results on the En⇒De task. The parameters of the proposed models were initialized by the pre-trained model. We tried k = 2 and k = 3 for the multi-layer attention model, which allows to attend to the lower two or t"
D18-1457,P16-1008,1,0.868416,"h 30 Model BLEU 29 Ours Base BASE 28 O URS 27 Hier.+Div. Hier. Base 26 26.13 26.32 26.41 26.69 archical aggregation to different components on En⇒De validation set. 45 > ] (3 0, 45 ] 30 5, (1 5] ,1 (0 45 > ] 45 (3 0, 0] BLEU Table 3: Experimental results of applying hier25 Source Sentence Applied to Encoder Decoder N/A N/A X × × X X X Length of Source Sentence Figure 4: BLEU scores on the En⇒De test set with respect to various input sentence lengths. “Hier.” denotes hierarchical aggregation and “Div.” denotes diversity regularization. 4.3.1 Length Analysis Following Bahdanau et al. (2015) and Tu et al. (2016), we grouped sentences of similar lengths together and computed the BLEU score for each group, as shown in Figure 4. Generally, the performance of T RANSFORMER -BASE goes up with the increase of input sentence lengths, which is superior to the performance of RNN-based NMT models on long sentences reported by (Bentivogli et al., 2016). We attribute this to the strength of self-attention mechanism to model global dependencies without regard to their distance. Clearly, the proposed approaches outperform the baseline model in all length segments, while there are still considerable differences betw"
D18-1457,Q16-1027,0,0.347851,"ral machine translation (NMT) models have advanced the machine translation community in recent years (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). NMT models generally consist of two components: an encoder network to summarize the input sentence into sequential representations, based on which a decoder network generates target sentence word by word with an attention model (Bahdanau et al., 2015; Luong et al., 2015). Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018). ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab. Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018). For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at"
D19-1082,P02-1040,0,0.109143,"chine Translation Implementation Detail We conduct the experiments on the WMT14 English-to-German (En⇒De) and NIST Chinese-to-English (Zh⇒En) translation tasks. For En⇒De, the training dataset consists of 4.56M sentence pairs. We use the newstest2013 and newstest2014 as development set and test set respectively. For Zh⇒En, the training dataset consists of about 1.25M sentence pairs. We used NIST MT02 dataset as development set, and MT 03-06 datasets as test sets. Byte pair encoding (BPE) toolkit1 (Sennrich et al., 2016) is used with 32K merge operations. We used casesensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the relevant tags. We test both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and the number of attention heads (8 vs. 16). All models are trained on eight NVIDIA Tesla P40 GPUs where each is allocated with a batch size of 4096 tokens. We implement the proposed approaches on top of T RANS FORMER (Vaswani et al., 2017) – a state-of-theart S ANs-based model"
D19-1082,D17-1209,0,0.0192451,"., 2018), information aggregation (Li et al., 2019a), and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018). Our work demonstrates that multi-head attention also benefits from the integration of the phrase information. Related Works Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality. Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations. The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head S ANs-based encoder, which we believe is more challenging. Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et al., 2019b), prior attention bias (Yang et a"
D19-1082,N18-1202,0,0.107117,"Missing"
D19-1082,P05-1033,0,0.586135,"Missing"
D19-1082,P18-1198,0,0.045558,"lest phrase constituent that above each word, “POS”: Part-of-Speech tags for each words. The tasks for predicting larger labels require models to capture and record larger granularity of phrase information of sentences (Shi et al., 2016). We conduct these tasks to study whether the proposed M G -S A benefits the multi-granularity phrase modeling to produce more useful and informative representation. 4.2 Multi-Granularity Phrases Evaluation Data and Models We extracted the sentences from the Toronto Book Corpus (Zhu et al., 2015).We sample and pre-process 120k sentences for each task following Conneau et al. (2018). By instruction of Shi et al. (2016), we label these sentences for each task. The train/valid/test dataset ratios are set to 10/1/1. For pre-trained NMT encoders, we use the pretrained encoders of model variations in Table 3 followed by a MLP classifier, which are used to In this section, we conduct multi-granularity label prediction tasks to the proposed models in terms of whether the proposed model is effective as expected to capture different levels of granularity phrase information of sentences. We analyze the impact of multi-granularity self-attention based on 2 Since the attention weigh"
D19-1082,W18-5431,0,0.120452,"proach consistently improves performance. Targeted linguistic analysis reveals that M G -S A indeed captures useful phrase information at various levels of granularities. 1 Introduction Recently, T RANSFORMER (Vaswani et al., 2017), implemented as deep multi-head self-attention networks (S ANs), has become the state-of-the-art neural machine translation (NMT) model in recent years. The popularity of S ANs lies in its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. More recently, an in-depth study (Raganato and Tiedemann, 2018) reveals that S ANs generally focus on disperse words and ignore continuous phrase patterns, which have proven essential in both statistical machine translation (SMT, Koehn ∗ Work done when interning at Tencent AI Lab. 887 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 887–897, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics speeds marginally decrease. Analysis on multigranularity label prediction tasks reveals that M G S A indeed captu"
D19-1082,N19-1423,0,0.0237641,"syntactic phrase based models, we only apply syntactic boundary of phrases and do not use any tag supervision for fair comparison. carry out probing tasks. phrase interaction by using O N -L STM. We use same assignments of heads for multi-granularity phrases as machine translation task for all model variants. For models trained from scratch, each of our model consists of 3 encoding layers followed by a MLP classifier. For each encoding layer, we employ a multi-head self-attention block and a feed-forward block as in T RANSFORMER, which have shown significant performance on several NLP tasks (Devlin et al., 2019). The difference between the compared models merely lies in the self-attention mechanism: “BASE” denotes standard M H -S A, “N-Gram Phrase” and “Syntactic Phrase” are the proposed M G -S A under N-gram phrase and syntactic phrase partition, and “Syntactic Phrase + Interaction” denotes M G -S A with Results Analysis Table 5 lists the prediction accuracies of five syntactic labels on test. Several observations can be made here. 1). Comparing the two set of experiments, the experimental results from models trained from scratch consistently outperform the results from NMT encoder probing on all ta"
D19-1082,P16-1162,0,0.128307,"illion), “Speed” denotes the training speed (steps/second). Encoder Layers [1 − 6] [1 − 3] [1] Machine Translation Implementation Detail We conduct the experiments on the WMT14 English-to-German (En⇒De) and NIST Chinese-to-English (Zh⇒En) translation tasks. For En⇒De, the training dataset consists of 4.56M sentence pairs. We use the newstest2013 and newstest2014 as development set and test set respectively. For Zh⇒En, the training dataset consists of about 1.25M sentence pairs. We used NIST MT02 dataset as development set, and MT 03-06 datasets as test sets. Byte pair encoding (BPE) toolkit1 (Sennrich et al., 2016) is used with 32K merge operations. We used casesensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the relevant tags. We test both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and the number of attention heads (8 vs. 16). All models are trained on eight NVIDIA Tesla P40 GPUs where each is allocated with a batch size of 4096 tokens. We implement the proposed"
D19-1082,P16-1078,0,0.0161954,"regularization (Li et al., 2018; Tao et al., 2018), information aggregation (Li et al., 2019a), and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018). Our work demonstrates that multi-head attention also benefits from the integration of the phrase information. Related Works Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality. Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations. The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head S ANs-based encoder, which we believe is more challenging. Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et a"
D19-1082,D16-1159,0,0.470117,"arget phrases? Q3. Does M G -S A capture more phrase information at the various granularity levels? In Section 4.1, we demonstrate that integrating the proposed M G -S A into the T RANS FORMER consistently improves the translation quality on both WMT14 English⇒German and (11) 890 Phrase Modeling n/a M AX -P OOLING S ANs L STM NIST Chinese⇒English (Q1). Further analysis reveals that our approach has stronger ability of capturing the phrase information and promoting the generation of the target phrases (Q2). In Section 4.2, we conduct experiments on the multi-granularity label prediction tasks (Shi et al., 2016), and investigate the representations of NMT encoders trained on both translation data and the training data of the label prediction tasks. Experimental results show that the proposed M G -S A indeed captures useful phrase information at various levels of granularities in both scenarios (Q3). 4.1 Speed 1.28 1.27 1.26 1.14 BLEU 27.31 27.56 27.69 27.58 Table 1: Evaluation of various phrase composition strategies under N-gram phrase partition. “# Para” denotes the trainable parameter size of each model (M=million), “Speed” denotes the training speed (steps/second). Encoder Layers [1 − 6] [1 − 3]"
D19-1082,P19-1254,0,0.0154434,"so on monolingual tasks. 5 Multi-Head Attention Multi-head attention mechanism has shown its effectiveness in machine translation (Vaswani et al., 2017) and generative dialog (Tao et al., 2018) systems. Recent studies shows that the modeling ability of multi-head attention has not been completely developed. Several specific guidance cues of different heads without breaking the vanilla multi-head attention mechanism can further boost the performance, e.g., disagreement regularization (Li et al., 2018; Tao et al., 2018), information aggregation (Li et al., 2019a), and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018). Our work demonstrates that multi-head attention also benefits from the integration of the phrase information. Related Works Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality. Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations. T"
D19-1082,D18-1548,0,0.424859,"ssing, pages 887–897, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics speeds marginally decrease. Analysis on multigranularity label prediction tasks reveals that M G S A indeed captures and stores the information of different granularity phrases as expected. 2 hand, recent study (Vaswani et al., 2017) implicitly hint that attention heads are underutilized as increasing number of heads from 4 to 8 or even 16 can hardly improve the translation performance. Several attention heads can be further exploited under specific guidance to improve the performance (Strubell et al., 2018). We expect the inductive bias for multi-granularity phrase can further improve the performance of S ANs and meanwhile maintain its simplicity and flexibility. Background Multi-Head Self-attention Instead of performing a single attention, Multi-Head Self-attention Networks (M H -S A), which are the defaults setting in T RANSFORMER (Vaswani et al., 2017), project the queries, keys and values into multiple subspaces and performs attention on the projected queries, keys and values in each subspace. In the standard M H -S A, it jointly attends to information from different representation subspaces"
D19-1082,D19-1135,1,0.884992,"c phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence into 2-grams or 3grams. Then, we assign an attention head to attend over phrase fragments at each granularity. In this way, M G -S ANs provide a lightweight strategy to explicitly model phrase structures. Furthermore, we also model the interactions among phrases to enhance structure modeling, which is one commonly-cited weakness of S ANs (Tran et al., 2018; Hao et al., 2019b). We evaluate the proposed model on two widely-used translation tasks: WMT14 Englishto-German and NIST Chinese-to-English. Experimental results demonstrate that our approach consistently improves translation performance over strong T RANSFORMER baseline model (Vaswani et al., 2017) across language pairs, while Current state-of-the-art neural machine translation (NMT) uses a deep multi-head selfattention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases has produced subs"
D19-1082,N19-1122,1,0.804029,"Missing"
D19-1082,D18-1503,0,0.182789,"words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence into 2-grams or 3grams. Then, we assign an attention head to attend over phrase fragments at each granularity. In this way, M G -S ANs provide a lightweight strategy to explicitly model phrase structures. Furthermore, we also model the interactions among phrases to enhance structure modeling, which is one commonly-cited weakness of S ANs (Tran et al., 2018; Hao et al., 2019b). We evaluate the proposed model on two widely-used translation tasks: WMT14 Englishto-German and NIST Chinese-to-English. Experimental results demonstrate that our approach consistently improves translation performance over strong T RANSFORMER baseline model (Vaswani et al., 2017) across language pairs, while Current state-of-the-art neural machine translation (NMT) uses a deep multi-head selfattention network with no explicit phrase information. However, prior work on statistical machine translation has shown that extending the basic translation unit from words to phrases"
D19-1082,P03-1054,0,0.0349375,"s. For En⇒De, the training dataset consists of 4.56M sentence pairs. We use the newstest2013 and newstest2014 as development set and test set respectively. For Zh⇒En, the training dataset consists of about 1.25M sentence pairs. We used NIST MT02 dataset as development set, and MT 03-06 datasets as test sets. Byte pair encoding (BPE) toolkit1 (Sennrich et al., 2016) is used with 32K merge operations. We used casesensitive NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and bootstrap resampling (Koehn et al., 2003) for statistical significance test. We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the relevant tags. We test both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and the number of attention heads (8 vs. 16). All models are trained on eight NVIDIA Tesla P40 GPUs where each is allocated with a batch size of 4096 tokens. We implement the proposed approaches on top of T RANS FORMER (Vaswani et al., 2017) – a state-of-theart S ANs-based model on machine translation, and followed the setting in previous work (Vaswani et al., 2017) to train the models. We incorporate the proposed model into the encoder"
D19-1082,N03-1017,0,0.30611,"rases and meanwhile maintain their simplicity and flexibility. The starting point for our approach is an observation: the power of multiple heads in S ANs is not fully exploited. For example, Li et al. (2018) show that different attention heads generally attend to the same positions, and Voita et al. (2019) reveal that only specialized attention heads do the heavy lifting while the rest can be pruned. Accordingly, we spare several attention heads for modeling phrase patterns for S ANs. Specifically, we use two representative types of phrases that are widely-used in SMT models: n-gram phrases (Koehn et al., 2003) to use surface of adjacent words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence into 2-grams or 3grams. Then, we assign an attention head to attend over phrase fragments at each granularity. In this way, M G -S ANs provide a lightweight strategy to explicitly model phrase structures. Furthermore, we also model the interactions among phrases to enhance structure modeling, which is one common"
D19-1082,D18-1317,1,0.93476,"openg Tu Tencent AI Lab shumingshi@tencent.com Florida State University jinfeng@stat.fsu.edu Tencent AI Lab zptu@tencent.com Abstract et al., 2003; Chiang, 2005; Liu et al., 2006) and NMT (Eriguchi et al., 2016; Wang et al., 2017; Yang et al., 2018; Zhao et al., 2018). To alleviate this problem, in this work we propose multi-granularity self-attention (M G -S A), which offers S ANs the ability to model phrases and meanwhile maintain their simplicity and flexibility. The starting point for our approach is an observation: the power of multiple heads in S ANs is not fully exploited. For example, Li et al. (2018) show that different attention heads generally attend to the same positions, and Voita et al. (2019) reveal that only specialized attention heads do the heavy lifting while the rest can be pruned. Accordingly, we spare several attention heads for modeling phrase patterns for S ANs. Specifically, we use two representative types of phrases that are widely-used in SMT models: n-gram phrases (Koehn et al., 2003) to use surface of adjacent words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input senten"
D19-1082,P19-1580,0,0.0230715,"ent AI Lab zptu@tencent.com Abstract et al., 2003; Chiang, 2005; Liu et al., 2006) and NMT (Eriguchi et al., 2016; Wang et al., 2017; Yang et al., 2018; Zhao et al., 2018). To alleviate this problem, in this work we propose multi-granularity self-attention (M G -S A), which offers S ANs the ability to model phrases and meanwhile maintain their simplicity and flexibility. The starting point for our approach is an observation: the power of multiple heads in S ANs is not fully exploited. For example, Li et al. (2018) show that different attention heads generally attend to the same positions, and Voita et al. (2019) reveal that only specialized attention heads do the heavy lifting while the rest can be pruned. Accordingly, we spare several attention heads for modeling phrase patterns for S ANs. Specifically, we use two representative types of phrases that are widely-used in SMT models: n-gram phrases (Koehn et al., 2003) to use surface of adjacent words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence in"
D19-1082,N19-1359,1,0.833903,"-S A is not limited to machine translation, but also on monolingual tasks. 5 Multi-Head Attention Multi-head attention mechanism has shown its effectiveness in machine translation (Vaswani et al., 2017) and generative dialog (Tao et al., 2018) systems. Recent studies shows that the modeling ability of multi-head attention has not been completely developed. Several specific guidance cues of different heads without breaking the vanilla multi-head attention mechanism can further boost the performance, e.g., disagreement regularization (Li et al., 2018; Tao et al., 2018), information aggregation (Li et al., 2019a), and functional specialization (Fan et al., 2019) on attention heads, the combination of multi-head attention with multi-task learning (Strubell et al., 2018). Our work demonstrates that multi-head attention also benefits from the integration of the phrase information. Related Works Phrase Modeling for NMT Several works have proven that the introduction of phrase modeling in NMT can obtain promising improvement on translation quality. Tree-based encoders, which explicitly take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are pro"
D19-1082,D17-1149,1,0.772626,"take the constituent tree (Eriguchi et al., 2016) or dependency tree (Bastings et al., 2017) into consideration, are proposed to produce treebased phrase representations. The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head S ANs-based encoder, which we believe is more challenging. Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et al., 2019b), prior attention bias (Yang et al., 2018, 2019; Guo et al., 2019). Our work differs at that we explicitly model phrase patterns at different granularities, which is then attended by different attention heads. 6 Conclusion In this paper, we propose multi-granularity selfattention model, a novel attention mechanism to simultaneously attend different granularity phrase. We study effective phrase representation for Ngram phrase and syntactic phrase, and find that a syntactic phrase based mechanism obtains the best result due to effectively incorporating r"
D19-1082,D18-1408,0,0.0826882,"Missing"
D19-1082,P06-1077,0,0.069506,"oint for our approach is an observation: the power of multiple heads in S ANs is not fully exploited. For example, Li et al. (2018) show that different attention heads generally attend to the same positions, and Voita et al. (2019) reveal that only specialized attention heads do the heavy lifting while the rest can be pruned. Accordingly, we spare several attention heads for modeling phrase patterns for S ANs. Specifically, we use two representative types of phrases that are widely-used in SMT models: n-gram phrases (Koehn et al., 2003) to use surface of adjacent words, and syntactic phrases (Liu et al., 2006) induced from syntactic trees to represent well-formed structural information. We first partition the input sentence into phrase fragments at different levels of granularity. For example, we can split a sentence into 2-grams or 3grams. Then, we assign an attention head to attend over phrase fragments at each granularity. In this way, M G -S ANs provide a lightweight strategy to explicitly model phrase structures. Furthermore, we also model the interactions among phrases to enhance structure modeling, which is one commonly-cited weakness of S ANs (Tran et al., 2018; Hao et al., 2019b). We evalu"
D19-1082,D18-1475,1,0.737412,"l., 2017) into consideration, are proposed to produce treebased phrase representations. The difference of our work from these studies is that they adopt the RNN-based encoder to form the tree-based encoder while we explicitly introduce the phrase structure into the the state-of-the-art multi-layer multi-head S ANs-based encoder, which we believe is more challenging. Another thread of work is to implicitly promote the generation of phrase-aware representation, such as the integration of external phrase boundary (Wang et al., 2017; Nguyen and Joty, 2018; Li et al., 2019b), prior attention bias (Yang et al., 2018, 2019; Guo et al., 2019). Our work differs at that we explicitly model phrase patterns at different granularities, which is then attended by different attention heads. 6 Conclusion In this paper, we propose multi-granularity selfattention model, a novel attention mechanism to simultaneously attend different granularity phrase. We study effective phrase representation for Ngram phrase and syntactic phrase, and find that a syntactic phrase based mechanism obtains the best result due to effectively incorporating rich syntactic information. To evaluate the effectiveness of the proposed model, we"
D19-1082,N19-1407,1,0.86432,"Missing"
D19-1082,D16-1100,0,0.0286164,". Targeted multi-granularity phrases evaluation shows that our model indeed capture useful phrase information. As our approach is not limited to specific tasks, it is interesting to validate the proposed model in other tasks, such as reading comprehension, language inference, and sentence classification. Multi Granularity Representation Multigranularity representation, which is proposed to make full use of subunit composition at different levels of granularity, has been explored in various NLP tasks, such as paraphrase identification (Yin and Sch¨utze, 2015), Chinese word embedding learning (Yin et al., 2016), universal sentence encoding (Wu et al., 2018) and machine translation (Nguyen and Joty, 2018; Li et al., 2019b). The major difference between our work and Nguyen and Joty (2018); Li et al. (2019b) lies in that we successfully introduce syntactic information into our multi-granularity representation. Furthermore, it is not well measured how much phrase information are stored in multi-granularity Acknowledgments J.Z. was supported by the National Institute of General Medical Sciences of the National Institute of Health under award number R01GM126558. We thank the anonymous reviewers for their"
D19-1082,N15-1091,0,0.0712827,"Missing"
D19-1082,N19-1118,0,0.0190421,"phrase benefits to translation quality. In addition, incorporating tag loss (Row 4) in training stage can further boost the translation performance. This indicates the auxiliary syntax objective is necessary, which is consistent with the results in other NLP task (Strubell et al., 2018). We use syntactic phrase partition with tag supervision as the default setting for subsequent experiments unless otherwise stated. Main Results Table 4 lists the results on WMT14 En⇒De and NIST Zh⇒En translation tasks. Our baseline models, outperform the reported results on the same data (Vaswani et al., 2017; Zhang et al., 2019), which we believe make the evaluation convincing. As seen, in terms of BLEU score, the proposed M G -S A consistently improves translation performance across language pairs, which demonstrates the effectiveness and universality of the proposed approach. Phrasal Pattern Evaluation As aforementioned, the proposed M G -S A aims to simultaneously model different granularities of phrases with different heads in S ANs. To investigate whether the proposed M G -S A improves the generation of phrases in the output, we calculate the improvement of the proposed models over multiple N-grams, as shown in"
D19-1085,N18-1118,0,0.038634,"wouldn’t let you buy it? Non-Fixed Error 我 和 露西 只是 要 搬 到 对门。 我们 一 分手 (我) 就 搬 回去。 Once we broke up, I’ll move back. Once we broke up, she’ll move back. Once we broke up, we moved back. Once we broke up, we’ll move back. bilingual hidden representations at decoding steps of previous sentences. They also evaluated the above three models on different domains of data, showing that the hierarchical encoder performs comparable with the multi-attention model. More recently, some researchers began to investigate the effects of context-aware NMT on cross-lingual pronoun prediction (Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018). They mainly exploited general anaphora in non-pro-drop languages such as English⇒Russian. 6 Conclusion In this work, we proposed a unified model to learn jointly predict and translate ZPs by leveraging multi-task learning. We also employed hierarchical neural networks to exploit discourselevel information for better ZP prediction. Experimental results on both Chinese⇒English and Japanese⇒English data show that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform the existing ZP translat"
D19-1085,D13-1135,0,0.0234044,"e hidden states of both the encoder-decoder and the reconstructor to embed the ZPs in the source sentence. Although the calculation of labeling loss relies on explicitly annotated labels, it is only used in training to guide the parameters to learn ZP-enhanced representations. Benefiting from the implicit integration of ZP information, we release the reliance on external ZP prediction model in testing. P (zpt |hrec t ) t=1 T Y (5) 3.2 (4) Discourse-Aware ZP Prediction Discourse information have proven useful for predicting antecedents, which may occur in previous sentences (Zhao and Ng, 2007; Chen and Ng, 2013). Therefore, we further improve ZP prediction with discourse-level context, which is learned together with the joint model. gl (zpt , hrec t ) t=1 where gl (·) is softmax for the ZP labeler. As seen, we integrate the ZP generation component into the ZP translation model. There is no reliance on external ZP prediction models in decoding phase. Encoding Discourse-Level Context Hierarchical structure networks are usually used for modelling discourse context on various natural language processing tasks such query suggestion (Sordoni et al., 2015), dialogue modeling (Serban et al., 2016) and MT (Wa"
D19-1085,W12-4213,0,0.496166,"Missing"
D19-1085,D10-1062,0,0.0858334,"Missing"
D19-1085,P05-1066,0,0.33376,"Missing"
D19-1085,W17-5702,0,0.0150206,"that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform the existing ZP translation models in previous work, and achieve a new state-of-the-art on the widelyused subtitle corpus. Manual evaluation confirms that the performance improvement comes from the alleviation of translation errors, which are mainly caused by subjective, objective as well as discourse-aware ZPs. There are two potential extensions to our work. First, we will evaluate our method on other implication phenomena (or called unaligned words (Takeno et al., 2017)) such as tenses and article words for NMT. Second, we will investigate the impact of different contextaware models on ZP translation, including multiattention (Jean et al., 2017b) and context-aware Transformer(Voita et al., 2018). Table 7: Example translations where pronouns in brackets are dropped in original inputs (“I NP.”) but labeled by humans according to references (“R EF.”) and previous sentence (“P RE .”). We italicize some mistranslated errors and highlight the correct ones in bold. overcome the data-level gap, Wang et al. (2016) proposed an automatic approach of ZP annotation by ut"
D19-1085,tiedemann-2012-parallel,0,0.0363956,"Missing"
D19-1085,Q18-1029,1,0.853259,"nces (ˆ x, y) whose source-side sentences are auto-annotated with ZPs. The “+ Reconstruction” is the best model reported in Wang et al. (2018a), which employs two reconstructors to reconstruct the x ˆ from Experiments 4.1 Results on Chinese⇒English Task Setup We conducted translation experiments on both Chinese⇒English and Japanese⇒English translation tasks, since Chinese and Japanese are pro-drop languages while English is not. For Chinese⇒English translation task, we used the data of auto-annotated ZPs (Wang et al., 2018a).3 4 http://www.opensubtitles.org. We followed Wang et al. (2017) and Tu et al. (2018) to use 3 previous sentences as discourse context. 3 5 https://github.com/longyuewangdcu/ tvsub. 925 # Model 1 Baseline 2 3 4 5 Translation #Params BLEU Prediction P R F1 86.7M 31.80 n/a External ZP Prediction (Wang et al., 2018a) + ZP-Annotated Data +0M 32.67 0.67 + Reconstruction +73.8M 35.08 This Work: Joint ZP Prediction and Translation Joint Model +35.6M 36.04† 0.72 + Discourse-Level Context +56.6M 37.11† 0.76 n/a n/a 0.65 0.66 0.68 0.77 0.70 0.77 Table 2: Evaluation of ZP translation and prediction on the Chinese–English data. “#Params” represents the number of parameters used in differe"
D19-1085,W17-4806,0,0.120064,"buy one? Sure. Joey wouldn’t let you buy it? Non-Fixed Error 我 和 露西 只是 要 搬 到 对门。 我们 一 分手 (我) 就 搬 回去。 Once we broke up, I’ll move back. Once we broke up, she’ll move back. Once we broke up, we moved back. Once we broke up, we’ll move back. bilingual hidden representations at decoding steps of previous sentences. They also evaluated the above three models on different domains of data, showing that the hierarchical encoder performs comparable with the multi-attention model. More recently, some researchers began to investigate the effects of context-aware NMT on cross-lingual pronoun prediction (Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018). They mainly exploited general anaphora in non-pro-drop languages such as English⇒Russian. 6 Conclusion In this work, we proposed a unified model to learn jointly predict and translate ZPs by leveraging multi-task learning. We also employed hierarchical neural networks to exploit discourselevel information for better ZP prediction. Experimental results on both Chinese⇒English and Japanese⇒English data show that the two proposed approaches accumulatively improve both the translation performance and ZP prediction accuracy. Our models also outperform th"
D19-1085,P18-1117,0,0.418324,"yielded best performances on validation sets. For training the proposed models, the hidden layer sizes of hierarchical model and reconstruction model are 1,000 and 2,000, respectively. We modeled previous three sentences as discourse-level context.5 (6) After we can obtain all sentence-level representations HX = {h−K , . . . , h−1 }, we feed them into a sentence-level encoder to produce a vector that represents the discourse-level context: C = E NCODERsentence (HX ) (7) Here the summary C consists of not only the dependencies between words, but also the relations between sentences. Following Voita et al. (2018), we share the parameters of word-level encoder E NCODERword with the encoder component in standard NMT model. Note that, E NCODERword and E NCODERsentence can be implemented as arbitrary networks, such as recurrent networks (Cho et al., 2014), convolutional networks (Gehring et al., 2017), or self-attention networks (Vaswani et al., 2017). In this study, we used recurrent networks to implement our E NCODER. Integrating Discourse into ZP Prediction We directly feed the discourse-level context to the reconstructor to improve ZP prediction. Specifically, we combine the context vector and the rec"
D19-1085,D10-1086,0,0.182667,"Missing"
D19-1085,W10-1737,0,0.0613995,"Missing"
D19-1085,D17-1301,1,0.929721,"3). Therefore, we further improve ZP prediction with discourse-level context, which is learned together with the joint model. gl (zpt , hrec t ) t=1 where gl (·) is softmax for the ZP labeler. As seen, we integrate the ZP generation component into the ZP translation model. There is no reliance on external ZP prediction models in decoding phase. Encoding Discourse-Level Context Hierarchical structure networks are usually used for modelling discourse context on various natural language processing tasks such query suggestion (Sordoni et al., 2015), dialogue modeling (Serban et al., 2016) and MT (Wang et al., 2017). Therefore, we employ hierarchical encoder (Wang et al., 2017) to encoder discourseTraining and Testing The newly introduced prediction component is trained together with the 1 We introduce “ heosi” to cover the case that a pronoun is missing at the end of a sentence. 2 We employ the pronoun vocabulary used in Wang et al. (2016), which contains 30 distinct Chinese pronouns. 924 level context for NMT. More specifically, we use the previous K source sentences X = {x−K , . . . , x−1 } as the discourse information, which is summarized with a two-layer hierarchical encoder, as shown in Figure 2. F"
D19-1085,D18-1333,1,0.851182,"interactive attention models: enc α ˆ enc = ATTenc (xt−1 , hrec ) t−1 , h α ˆ dec = dec enc ˆt ) ATTdec (xt−1 , hrec ,c t−1 , h Figure 2: Architecture of hierarchical neural encoder. x−K , . . . , x−1 are K previous sentences before the current source sentence “你 烤 的 吗 ?” in a text. (2) (3) encoder-decoder-reconstructor:  J(θ, γ, ψ) = arg max log L(y|x; θ) {z } | θ,γ,ψ likelihood + log R(x|henc , hdec ; θ) | {z } reconstruction  rec + log P (zp|h ; θ, γ) {z } | ZP labeling The interaction between two attention models leads to a better exploitation of the encoder and decoder representations (Wang et al., 2018b). ZP Prediction as Sequence Labelling We cast ZP prediction as a sequence labelling task, where each word is labelled if there is a pronoun missing before it. Given the input x = {x1 , x2 , . . . , xT } with the last word xT being the end-of-sentence tag “ heosi”,1 the output to be labelled is a sequence of labels zp = {zp1 , zp2 , . . . , zpT } with zpt ∈ {N } ∪ Vzp . Among the label set, “N ” denotes no ZP, and Vzp is the vocabulary of pronouns.2 Taking Figure 1 as an example, the label sequence “N N N 它 N N” indicates that the pronoun “它” is missing before the fourth word “吗” in the sourc"
D19-1085,C10-1080,0,0.0468499,"Missing"
D19-1085,N16-1113,1,0.922694,"o components to interact with each other. 2. Our study demonstrates the effectiveness of discourse-level context for ZP prediction. 3. Based on our manually-annotated testset, we conduct extensive analyses to assess ZP prediction and translation. 2 2.1 等 我 搬进来，(我 我) 能 买 台 电视 吗？ Can I get a TV when I move in? When I move in to buy a TV. 这块 蛋糕 很 美味！你 烤 的 (它 它) 吗？ The cake is very tasty! Did you bake it? The cake is delicious! Are you baked? 2.2 Background Bridging Data Gap Between ZP Prediction and Translation Recent efforts have explored ways to bridge the gap of ZP prediction and translation (Wang et al., 2016, 2018a,b) by training both models on the homologous data. The pipeline involves two phases, as described below. Zero Pronoun In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to non-pro-drop languages such as English (Zhao and Ng, 2007). As seen in Table 1, the subject pronoun (“我”) and the object pronoun (“它”) are omitted in Chinese sentences (“Inp.”) while these pronouns are all compulsory Translation-Oriented ZP Prediction Its goal is to recall the ZPs in the source sentence (i.e. prodrop language) with the information of the target sentence (i.e."
D19-1085,P02-1040,0,0.103671,"Missing"
D19-1085,P13-1081,0,0.388518,"Missing"
D19-1085,Y15-1050,0,0.345278,"Missing"
D19-1085,C10-1135,0,0.0554474,"Missing"
D19-1085,N15-1052,0,0.216225,"Missing"
D19-1085,D17-1135,0,0.0842924,"Missing"
D19-1085,D07-1057,0,0.489977,"Can I get a TV when I move in? When I move in to buy a TV. 这块 蛋糕 很 美味！你 烤 的 (它 它) 吗？ The cake is very tasty! Did you bake it? The cake is delicious! Are you baked? 2.2 Background Bridging Data Gap Between ZP Prediction and Translation Recent efforts have explored ways to bridge the gap of ZP prediction and translation (Wang et al., 2016, 2018a,b) by training both models on the homologous data. The pipeline involves two phases, as described below. Zero Pronoun In pro-drop languages such as Chinese and Japanese, ZPs occur much more frequently compared to non-pro-drop languages such as English (Zhao and Ng, 2007). As seen in Table 1, the subject pronoun (“我”) and the object pronoun (“它”) are omitted in Chinese sentences (“Inp.”) while these pronouns are all compulsory Translation-Oriented ZP Prediction Its goal is to recall the ZPs in the source sentence (i.e. prodrop language) with the information of the target sentence (i.e. non-pro-drop language) in a paral922 lel corpus. Taking the second case (assuming that Inp. and Ref. are sentence pair in a parallel corpus) in Table 1 for instance, the ZP “它 (it)” is dropped in the Chinese side while its equivalent “it” exists in the English side. It is possib"
D19-1088,D17-1042,0,0.384726,"ral Machine Translation with Word Importance Shilin He1,2 Zhaopeng Tu3∗ Xing Wang3 Longyue Wang3 Michael R. Lyu1,2 Shuming Shi3 1 Department of Computer Science and Engineering, The Chinese University of Hong Kong 2 Shenzhen Research Institute, The Chinese University of Hong Kong 1,2 {slhe,lyu}@cse.cuhk.edu.hk 3 Tencent AI Lab 3 {zptu,brightxwang,vinnylywang,shumingshi}@tencent.com Abstract 2017) or hidden units (Bau et al., 2019; Ding et al., 2017). Another direction focuses on understanding the importance of input words by interpreting the input-output behavior of NMT models. Previous work (Alvarez-Melis and Jaakkola, 2017) treats NMT models as black-boxes and provides explanations that closely resemble the attention scores in NMT models. However, recent studies reveal that attention does not provide meaningful explanations since the relationship between attention scores and model output is unclear (Jain and Wallace, 2019). In this paper, we focus on the second thread and try to open the black-box by exploiting the gradients in NMT generation, which aims to estimate the word importance better. Specifically, we employ the integrated gradients method (Sundararajan et al., 2017) to attribute the output to the input"
D19-1088,W16-1601,0,0.0612708,"Missing"
D19-1088,P17-1080,0,0.0264362,"understanding NMT by identifying undertranslated words. • We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universa"
D19-1088,C08-1018,0,0.0267726,"n) as introduced in Section 3.2. In Section 4.1, to ensure that the translation performance decrease attributes to the selected words instead of the perturbation operations, we randomly select the same number of words to perturb (Random), which serves as a baseline. Since there is no ranking for content words, we randomly select a set of content words as important words. To avoid the potential bias introduced by randomness (i.e., Random and Con• Deletion perturbation removes the selected words from the input sentence, and it can be regarded as a specific instantiation of sentence compression (Cohn and Lapata, 2008). • Mask perturbation replaces embedding vectors of the selected words with all-zero vectors (Arras et al., 2016), which is similar to Deletion perturbation except that it retains the placeholder. • Grammatical Replacement perturbation replaces a word by another word of the same linguistic role (i.e., POS tags), yielding a sentence that is grammatically correct but semantically nonsensical (Chomsky and Lightfoot, 2002; Gulordava et al., 2018), such as “colorless green ideas sleep furiously”. Figure 2 illustrates the experimental results on Chinese⇒English translation with Transformer. It 956 D"
D19-1088,P17-1106,0,0.0239557,"rovide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universality of our findings. Concerning interpreting the input-out"
D19-1088,1997.mtsummit-papers.1,0,0.269192,"Missing"
D19-1088,D18-1548,0,0.0196634,"n, which we leave to the future work. 6 Acknowledgement Discussion and Conclusion We approach understanding NMT by investigating the word importance via a gradient-based method, which bridges the gap between word importance and translation performance. Empirical results show that the gradient-based method is superior to several black-box methods in estimating the word importance. Further analyses show that important words are of distinct syntactic categories on different language pairs, which might support the viewpoint that essential inductive bias should be introduced into the model design (Strubell et al., 2018). Our study also suggests the possibility of detecting the notorious under-translation problem via the gradient-based method. This paper is an initiating step towards the general understanding of NMT models, which may bring some potential improvements, such as Shilin He and Michael R. Lyu were supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14210717 of the General Research Fund), and Microsoft Research Asia (2018 Microsoft Research Asia Collaborative Research Award). We thank the anonymous reviewers for their insightful comments and sugg"
D19-1088,N18-1108,0,0.0186535,"Con• Deletion perturbation removes the selected words from the input sentence, and it can be regarded as a specific instantiation of sentence compression (Cohn and Lapata, 2008). • Mask perturbation replaces embedding vectors of the selected words with all-zero vectors (Arras et al., 2016), which is similar to Deletion perturbation except that it retains the placeholder. • Grammatical Replacement perturbation replaces a word by another word of the same linguistic role (i.e., POS tags), yielding a sentence that is grammatically correct but semantically nonsensical (Chomsky and Lightfoot, 2002; Gulordava et al., 2018), such as “colorless green ideas sleep furiously”. Figure 2 illustrates the experimental results on Chinese⇒English translation with Transformer. It 956 Deletion Mask Deletion Mask 2 2525 202020 202020 2020 151515 3 4 er of Operations 5 Random Random Random Frequency Frequency Frequency Content Content Content Attention Attention Attention Attribution Attribution Attribution 101010 0 0 0 1 11 2 22 3 33 4 4 4 5 55 Number Number ofOperations Operations Number of of Operations (a) Deletion 151515 BLEU BLEU 252525 BLEU BLEU BLEU 252525 BLEU BLEU BLEU m ncy t on tion Replacement (Same POS) Replacem"
D19-1088,P19-1354,1,0.774881,"dentifying undertranslated words. • We provide empirical support for the design principle of NMT architectures: essential inductive bias (e.g., language characteristics) should be considered for model design. 2 Related Work Interpreting Seq2Seq Models Interpretability of Seq2Seq models has recently been explored mainly from two perspectives: interpreting internal representations and understanding inputoutput behaviors. Most of the existing work focus on the former thread, which analyzes the linguistic information embeded in the learned representations (Shi et al., 2016; Belinkov et al., 2017; Yang et al., 2019) or the hidden units (Ding et al., 2017; Bau et al., 2019). Several researchers turn to expose systematic differences between human and NMT translations (L¨aubli et al., 2018; Schwarzenberg et al., 2019), indicating the linguistic properties worthy of investigating. However, the learned representations may depend on the model implementation, which potentially limit the applicability of these methods to a broader range of model architectures. Accordingly, we focus on understanding the input-output behaviors, and validate on different architectures to demonstrate the universality of our findings"
D19-1088,P17-1141,0,0.0261181,"comparing words of least importance and human-annotated under-translated words. As seen, our Attribution method consistently and significantly outperforms both Erasure and Attention approaches. By exploiting the word importance calculated by Attribution method, we can identify the under-translation errors automatically without the involvement of human interpreters. Although the accuracy is not high, it is worth noting that our under-translation method is very simple and straightforward. This is potentially useful for debugging NMT models, e.g., automatic post-editing with constraint decoding (Hokamp and Liu, 2017; Post and Vilar, 2018). In this section, we conduct analyses on two potential usages of word importance, which can help debug NMT models (Section 5.1) and design better architectures for specific languages (Section 5.2). Due to the space limitation, we only analyze the results of Chinese⇒English, English⇒French, and English⇒Japanese. We list the results on the reverse directions in Appendix, in which the general conclusions also hold. 5.1 Type Noun Verb Adj. Prep. Dete. Punc. Others ≥2 1 (0, 1) 0 Low Middle High Effect on Detecting Translation Errors In this experiment, we propose to use the"
D19-1088,N19-1357,0,0.368665,"encent AI Lab 3 {zptu,brightxwang,vinnylywang,shumingshi}@tencent.com Abstract 2017) or hidden units (Bau et al., 2019; Ding et al., 2017). Another direction focuses on understanding the importance of input words by interpreting the input-output behavior of NMT models. Previous work (Alvarez-Melis and Jaakkola, 2017) treats NMT models as black-boxes and provides explanations that closely resemble the attention scores in NMT models. However, recent studies reveal that attention does not provide meaningful explanations since the relationship between attention scores and model output is unclear (Jain and Wallace, 2019). In this paper, we focus on the second thread and try to open the black-box by exploiting the gradients in NMT generation, which aims to estimate the word importance better. Specifically, we employ the integrated gradients method (Sundararajan et al., 2017) to attribute the output to the input words with the integration of first-order derivatives. We justify the gradient-based approach via quantitative comparison with black-box methods on a couple of perturbation operations, several language pairs, and two representative model architectures, demonstrating its superiority on estimating word im"
D19-1088,D18-1512,0,0.0622219,"Missing"
D19-1088,W17-5706,0,0.0176538,"first choose two large-scale datasets that are publicly available, i.e., Chinese-English and EnglishFrench. Since English, French, and Chinese all belong to the subject-verb-object (SVO) family, we choose another very different subject-object-verb (SOV) language, Japanese, which might bring some interesting linguistic behaviors in EnglishJapanese translation. For Chinese-English task, we use WMT17 Chinese-English dataset that consists of 20.6M sentence pairs. For English-French task, we use WMT14 English-French dataset that comprises 35.5M sentence pairs. For English-Japanese task, we follow (Morishita et al., 2017) to use the first two sections of WAT17 English-Japanese dataset that consists of 1.9M sentence pairs. Following the standard NMT procedure, we adopt the standard byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all language pairs. We believe that these datasets are large enough to confirm the rationality and validity of our experimental analyses. Evaluation We evaluate the effectiveness of estimating word importance by the translation performance decrease. More specifically, unlike the usual way, we measure the decrease of translation performance when perturbing"
D19-1088,N18-1119,0,0.0171115,"st importance and human-annotated under-translated words. As seen, our Attribution method consistently and significantly outperforms both Erasure and Attention approaches. By exploiting the word importance calculated by Attribution method, we can identify the under-translation errors automatically without the involvement of human interpreters. Although the accuracy is not high, it is worth noting that our under-translation method is very simple and straightforward. This is potentially useful for debugging NMT models, e.g., automatic post-editing with constraint decoding (Hokamp and Liu, 2017; Post and Vilar, 2018). In this section, we conduct analyses on two potential usages of word importance, which can help debug NMT models (Section 5.1) and design better architectures for specific languages (Section 5.2). Due to the space limitation, we only analyze the results of Chinese⇒English, English⇒French, and English⇒Japanese. We list the results on the reverse directions in Appendix, in which the general conclusions also hold. 5.1 Type Noun Verb Adj. Prep. Dete. Punc. Others ≥2 1 (0, 1) 0 Low Middle High Effect on Detecting Translation Errors In this experiment, we propose to use the estimated word importan"
D19-1088,N19-4006,0,0.0510171,"Missing"
D19-1088,P16-1162,0,0.147475,"very different subject-object-verb (SOV) language, Japanese, which might bring some interesting linguistic behaviors in EnglishJapanese translation. For Chinese-English task, we use WMT17 Chinese-English dataset that consists of 20.6M sentence pairs. For English-French task, we use WMT14 English-French dataset that comprises 35.5M sentence pairs. For English-Japanese task, we follow (Morishita et al., 2017) to use the first two sections of WAT17 English-Japanese dataset that consists of 1.9M sentence pairs. Following the standard NMT procedure, we adopt the standard byte pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all language pairs. We believe that these datasets are large enough to confirm the rationality and validity of our experimental analyses. Evaluation We evaluate the effectiveness of estimating word importance by the translation performance decrease. More specifically, unlike the usual way, we measure the decrease of translation performance when perturbing a set of important words that are of top-most word importance in a sentence. The more translation performance degrades, the more important the word is. We use the standard BLEU score as the evaluation metric for"
D19-1088,D16-1159,0,0.194425,"-the-art results on a mass of language pairs with varying structural differences, such as English-French (Bahdanau et al., 2014; Vaswani et al., 2017) and Chinese-English (Hassan et al., 2018). However, so far not much is known about how and why NMT works, which pose great challenges for debugging NMT models and designing optimal architectures. The understanding of NMT models has been approached primarily from two complementary perspectives. The first thread of work aims to understand the importance of representations by analyzing the linguistic information embedded in representation vectors (Shi et al., 2016; Belinkov et al., ∗ Zhaopeng Tu is the corresponding author. Work was mainly done when Shilin He was interning at Tencent AI Lab. 953 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 953–962, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the feature importance. Starting from this observation, we exploit the intermediate gradients to better estimate word importance, which consistently outperforms its attention counterpart across model a"
D19-1135,P18-1008,0,0.310848,"e state of the art on a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Tan et al., 2018), and language representations (Devlin et al., 2018). However, a previous study empirically reveals that the hierarchical structure of the input sentence, which is essential for language understanding, is not well modeled by S ANs (Tran et al., 2018). Recently, hybrid models which combine the strengths of S ANs and recurrent neural networks (R NNs) have outperformed both individual architectures on a machine translation task (Chen et al., 2018). We attribute the improvement to that R NNs complement S ANs on the representation limitation of hi∗ erarchical structure, which is exactly the strength of R NNs (Tran et al., 2018). Starting with this intuition, we propose to further enhance the representational power of hybrid models with an advanced R NNs variant – Ordered Neurons L STM (O N -L STM, Shen et al., 2019). O N -L STM is better at modeling hierarchical structure by introducing a syntax-oriented inductive bias, which enables R NNs to perform tree-like composition by controlling the update frequency of neurons. Specifically, we s"
D19-1135,P18-1198,0,0.0353809,"ed with sequential context (Chen et al., 2018). Moreover, to dispel the doubt that whether the improvement of hybrid model comes from the increasement of parameters. We investigate the 8layers L STM and 10-layers S ANs encoders (Rows 3-4) which have more parameters compared with the proposed hybrid model. The results show that the hybrid model consistently outperforms these model variants with less parameters and the improvement should not be due to more parameters. 3.2 0.7 Targeted Linguistic Evaluation To gain linguistic insights into the learned representations, we conducted probing tasks (Conneau et al., 2018) to evaluate linguistics knowledge embedded in the final encoding representation learned by model, as shown in Table 3. We evaluated S ANs and proposed hybrid model with Short-Cut connection on these 10 targeted linguistic evaluation tasks. The tasks and model details are described in Appendix A.2. Experimental results are presented in Table 3. Several observations can be made here. The proposed hybrid model with short-cut produces more informative representation in most tasks (“Final” in “S” vs. in “Hybrid+Short-Cut”), indicating that the effectiveness of the model. The only exception are sur"
D19-1135,P19-1032,0,0.0251266,"e a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neural Networks in NLP Structure modeling in NLP has been studied for a long time as the na"
D19-1135,D18-1457,1,0.795963,"pdate frequencies. It also equals to the probability of each position’s value being 1 in the ideal master gate. Since this ideal master gate is binary, CU(·) is the expectation of the ideal master gate. Based on this activation function, the master gates are defined as f˜t = CUf (xt , ht−1 ), ˜it = 1 − CUi (xt , ht−1 ), (8) (9) where xt is the current input and ht−1 is the hidden state of previous step. CUf and CUi are two individual activation functions with their own trainable parameters. Short-Cut Connection Inspired by previous work on exploiting deep representations (Peters et al., 2018; Dou et al., 2018), we propose to simultaneously expose both types of signals by explicitly combining them with a simple short-cut connection (He et al., 2016). Similar to positional encoding injection in Transformer (Vaswani et al., 2017), we add the output of the O N -L STM encoder to the output of S ANs encoder: 1337 L b = HK H O N -L STM + HS ANs , (10) # 1 2 3 4 5 6 7 8 9 Encoder Architecture Base Model 6L S ANs 6L L STM 6L O N -L STM 6L L STM + 4L S ANs 6L O N -L STM + 4L S ANs 3L O N -L STM + 3L S ANs + Short-Cut Big Model 6L S ANs Hybrid Model + Short-Cut Para. BLEU 88M 97M 110M 104M 123M 99M 99M 27.31"
D19-1135,P15-1150,0,0.201741,"Missing"
D19-1135,D19-1082,1,0.811413,"Missing"
D19-1135,D18-1503,0,0.151578,"rence tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure. 1 Introduction Self-attention networks (S ANs, Lin et al., 2017) have advanced the state of the art on a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Tan et al., 2018), and language representations (Devlin et al., 2018). However, a previous study empirically reveals that the hierarchical structure of the input sentence, which is essential for language understanding, is not well modeled by S ANs (Tran et al., 2018). Recently, hybrid models which combine the strengths of S ANs and recurrent neural networks (R NNs) have outperformed both individual architectures on a machine translation task (Chen et al., 2018). We attribute the improvement to that R NNs complement S ANs on the representation limitation of hi∗ erarchical structure, which is exactly the strength of R NNs (Tran et al., 2018). Starting with this intuition, we propose to further enhance the representational power of hybrid models with an advanced R NNs variant – Ordered Neurons L STM (O N -L STM, Shen et al., 2019). O N -L STM is better at mo"
D19-1135,N19-1122,1,0.818568,"Missing"
D19-1135,P19-1624,1,0.822984,"odel. We also propose a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neural Networks in NLP Structure modeling in NLP has been studie"
D19-1135,C16-1229,0,0.0215293,"eling of hierarchical structure is an essential strength of hybrid models over the vanilla S ANs. • Our study proves that the idea of augmenting R NNs with ordered neurons (Shen et al., 2019) produces promising improvement on machine translation, which is one potential criticism of O N -L STM. 1336 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1336–1341, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Approach Partially motivated by Wang et al. (2016) and Chen et al. (2018), we stack a S ANs encoder on top of a R NNs encoder to form a cascaded encoder. In the cascaded encoder, hierarchical structure modeling is enhanced in the bottom R NNs encoder, based on which S ANs encoder is able to extract representations with richer hierarchical information. Let X = {x1 , . . . , xN } be the input sequence, the representation of the cascaded encoder is calculated by HK R NNs = E NC R NNs (X), (1) HL S ANs (2) = E NC S ANs (HK R NNs ), where E NC R NNs (·) is a K-layer R NNs encoder that reads the input sequence, and E NC S ANs (·) is a Llayer S ANs"
D19-1135,D18-1408,0,0.161685,"mechanism, as the R NNs counterpart to boost the hybrid model. We also propose a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neura"
D19-1135,D18-1475,1,0.736722,"structured gating mechanism, as the R NNs counterpart to boost the hybrid model. We also propose a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure M"
D19-1135,N18-1202,0,0.023472,"s denotes different update frequencies. It also equals to the probability of each position’s value being 1 in the ideal master gate. Since this ideal master gate is binary, CU(·) is the expectation of the ideal master gate. Based on this activation function, the master gates are defined as f˜t = CUf (xt , ht−1 ), ˜it = 1 − CUi (xt , ht−1 ), (8) (9) where xt is the current input and ht−1 is the hidden state of previous step. CUf and CUi are two individual activation functions with their own trainable parameters. Short-Cut Connection Inspired by previous work on exploiting deep representations (Peters et al., 2018; Dou et al., 2018), we propose to simultaneously expose both types of signals by explicitly combining them with a simple short-cut connection (He et al., 2016). Similar to positional encoding injection in Transformer (Vaswani et al., 2017), we add the output of the O N -L STM encoder to the output of S ANs encoder: 1337 L b = HK H O N -L STM + HS ANs , (10) # 1 2 3 4 5 6 7 8 9 Encoder Architecture Base Model 6L S ANs 6L L STM 6L O N -L STM 6L L STM + 4L S ANs 6L O N -L STM + 4L S ANs 3L O N -L STM + 3L S ANs + Short-Cut Big Model 6L S ANs Hybrid Model + Short-Cut Para. BLEU 88M 97M 110M 104M"
D19-1135,N19-1407,1,0.797897,"R NNs counterpart to boost the hybrid model. We also propose a modification of the cascaded encoder by explicitly combining the outputs of individual components, to enhance the ability of hierarchical structure modeling in a hybrid model. Experimental results on machine translation, targeted linguistic evaluation and logical inference tasks show that the proposed models achieve better performances by modeling hierarchical structure of sequence. Improved Self-Attention Networks Recently, there is a large body of work on improving S ANs in various NLP tasks (Yang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Guo et al., 2019; Wang et al., 2019a; Sukhbaatar et al., 2019), as well as image classification (Bello et al., 2019) and automatic speech recognition (Mohamed et al., 2019) tasks. In these works, several strategies are proposed to improve the utilize S ANs with the enhancement of local and global information. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neural Networks in NLP S"
D19-1135,P16-1162,0,0.0710345,"slation task. “↑ / ⇑”: significant over the conventional self-attention counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling. “6L S ANs” is the state-of-the-art Transformer model. “nL L STM + mL S ANs” denotes stacking n L STM layers and m S ANs layers subsequently. “Hybrid Model” denotes “3L O N -L STM + 3L S ANs”. 3 Encoder Architecture 3L O N -L STM → 3L S ANs 3L S ANs → 3L O N -L STM 8L L STM 10L S ANs Machine Translation For machine translation, we used the benchmark WMT14 English⇒German dataset. Sentences were encoded using byte-pair encoding (BPE) with 32K word-piece vocabulary (Sennrich et al., 2016). We implemented the proposed approaches on top of T RANSFORMER (Vaswani et al., 2017) – a state-of-the-art S ANs-based model on machine translation, and followed the setting in previous work (Vaswani et al., 2017) to train the models, and reproduced their reported results. We tested on both the Base and Big models which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and number of attention heads (8 vs. 16). All the model variants were implemented on the encoder. The implementation details are introduced in Appendix A.1. Table 1 lists the results. Baselines (Rows 1-3) Follow"
D19-1135,D13-1170,0,0.00435154,"ormation. In this work, we enhance the S ANs with the On-Lstm to form a hybrid model (Chen et al., 2018), and thoroughly evaluate the performance on machine translation, targeted linguistic evaluation, and logical inference tasks. Acknowledgments Structure Modeling for Neural Networks in NLP Structure modeling in NLP has been studied for a long time as the natural language sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). With the emergence of deep learning, tree-based models have been proposed to integrate syntactic tree structure into Recursive Neural Networks (Socher et al., 2013), L STMs (Tai et al., 2015), C NNs (Mou et al., 2016). As for S ANs, Hao et al. (2019a), Ma et al. (2019) and Wang et al. (2019b) enhance the S ANs with neural syntactic distance, multigranularity attention scope and structural position representations, which are generated from the syntactic tree structures. Closely related to our work, Hao et al. (2019b) find that the integration of the recurrence in S ANs encoder can provide more syntactic structure feaSamuel R. Bowman, Christopher D. Manning, and Christopher Potts. 2015. Tree-structured composition in neural networks without tree-structured"
D19-1145,W13-2322,0,0.0389766,"of relationship path between words, sequential PE measures the sequential distance between the words. As shown in Figure 1 (a), for each word, absolute sequential position represents the sequential distance to the beginning of the sentence, while relative sequential position measures the relative distance to the queried word (“talk” in the example). The latent structure can be interpreted in various ways, from syntactic tree structures, e.g., constituency tree (Collins, 2003) or dependency tree (K¨ubler et al., 2009), to semantic graph structures, e.g., abstract meaning representation graph (Banarescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the tree depth of the word in the dependency tree as its absolute structural position. Specifically, we treat the main verb (Tapanainen and Jarvi"
D19-1145,J03-4003,0,0.205527,"ctic relationships among input words. Figure 1 shows an example to illustrate the idea of the proposed approach. From the perspective of relationship path between words, sequential PE measures the sequential distance between the words. As shown in Figure 1 (a), for each word, absolute sequential position represents the sequential distance to the beginning of the sentence, while relative sequential position measures the relative distance to the queried word (“talk” in the example). The latent structure can be interpreted in various ways, from syntactic tree structures, e.g., constituency tree (Collins, 2003) or dependency tree (K¨ubler et al., 2009), to semantic graph structures, e.g., abstract meaning representation graph (Banarescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the"
D19-1145,P18-1198,0,0.197153,"we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to encode the distance of each word pair in the tree. We implement our structural encoding strategies on top of T RANSFORMER (Vaswani et al., 2017) and conduct experiments on both NIST Chinese⇒English and WMT14 English⇒German translation tasks. Experimental results show that exploiting structural position encoding strategies consistently boosts performance over both the absolute and relative sequential position representations across language pairs. Linguistic analyses (Conneau et al., 2018) reveal that the proposed structural position representation improves the translation performance with richer syntactic information. Our main contributions are: • Our study demonstrates the necessity and effectiveness of exploiting structural position encoding for S ANs, which benefits from modeling syntactic depth and distance under the latent structure of the sentence. • We propose structural position representations for S ANs to encode the latent structure of the input sentence, which are complementary to their sequential counterparts. 2 Background Self-Attention SANs produce representation"
D19-1145,N19-1423,0,0.0455592,"e to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese⇒English and WMT14 English⇒German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to"
D19-1145,D18-1457,1,0.850735,"and structure position representations1 : asb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utili"
D19-1145,N16-1024,0,0.0650603,"Missing"
D19-1145,P17-2012,0,0.0278999,"Missing"
D19-1145,D19-1082,1,0.844139,"Missing"
D19-1145,D19-1135,1,0.917158,", 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et al. (2019c,b) integrate the recurrence into the S ANs and empirically demonstrate that the hybrid models achieve better performances by modeling structure of sentences. Hao et al. (2019a) further make use of the multi-head attention to form the multi-granularity self-attention, to capture the different granularity phrases in source sentences. The difference is that we treat the position representation as a medium to transfer the structure information from the dependency tree into the S ANs. 1 We also use parameter-free element-wise addition method to combine two absolute position embedding and get 0.28"
D19-1145,P03-1054,0,0.105534,"ST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed position encoding strategies on T RANSFORMER (Vaswani et al., 2017) and implement them on top of THUMT (Zhang et al., 2017). We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the structural structural absolute and relative position as described in Section 3. When using relative structural position encoding, we use clipping distance r = 16. To make a fair comparison, we valid different position encoding strategies on the encoder and keep the T RANSFORMER decoder unchanged. Effect of Position Encoding We first remove the sequential encoding from the Transformer encoder (Model #1) and observe the translation performance degrades dramatically (28.33 − 44.31 = −15.98), which demonstrates the necessity of the position encoding strategie"
D19-1145,W04-3250,0,0.0760548,"t al. (2019c) Transformer-Big + Structural PE + Relative Sequential PE + Structural PE MT04 46.49 47.12↑ 47.01 47.37⇑ Zh⇒En MT05 45.21 45.84 45.65 46.20⇑ MT06 44.87 45.64⇑ 45.87⇑ 46.18⇑ Avg 45.47 46.06 46.00 46.40 En⇒De WMT14 28.98 28.58 28.88 28.90 29.19⇑ Table 2: Evaluation of translation performance on NIST Zh⇒En and WMT14 En⇒De test sets. Hao et al. (2019c) is a Transformer-Big model which adopted an additional recurrence encoder with the attentive recurrent network to model syntactic structure. “↑ / ⇑”: significant over the Transformer-Big (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). Model BASE + Rel. Seq. PE + Stru. PE SeLen 92.20 89.82 89.54 Surface WC Avg 63.00 77.60 63.17 76.50 62.90 76.22 TrDep 44.74 45.09 46.12 Syntactic ToCo BShif 79.02 71.24 78.45 71.40 79.12 72.36 Avg 65.00 64.98 65.87 Tense 89.24 88.74 89.30 SubN 84.69 87.00 85.47 Semantic ObjN SoMo 84.53 52.13 85.53 51.68 84.94 52.90 CoIn 62.47 62.21 62.99 Avg 74.61 75.03 75.12 Table 3: Performance on linguistic probing tasks. The probing tasks were conducted by evaluating linguistics embedded in the Transformer-Base encoder outputs. “Base”, “+ Rel. Seq. PE”, “+ Stru. PE” denote TransformerBase, Transformer-Ba"
D19-1145,P02-1040,0,0.103911,". The experimental results on the development set are shown in Table 1. Table 1: Impact of the position encoding components on Chinese⇒English NIST02 development dataset using Transformer-Base model. “Abs.” and “Rel.” denote absolute and relative position encoding, respectively. “Spd.” denotes the decoding speed (sentences/second) on a Tesla M40, the speed of structural position encoding strategies include the step of dependency parsing. 5 Model Variations Experiment We conduct experiments on the widely used NIST Chinese⇒English and WMT14 English⇒German data, and report the 4-gram BLEU score (Papineni et al., 2002). Chinese⇒English We use the training dataset consists of about 1.25 million sentence pairs. NIST 2002 (MT02) dataset is used as development set. NIST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed posit"
D19-1145,D16-1244,0,0.107899,"Missing"
D19-1145,N18-2074,0,0.405265,"n tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structur"
D19-1145,Q19-1002,0,0.0399602,"Missing"
D19-1145,D18-1548,0,0.0412553,", we use dependency tree to represent the grammatical structure of a sentence, and propose two strategies to encode the positional relationships among words in the dependency tree. Experimental results on NIST Chinese⇒English and WMT14 English⇒German translation tasks show that the proposed approach consistently boosts performance over both the absolute and relative sequential position representations. 1 Introduction In recent years, self-attention networks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this"
D19-1145,P19-1032,0,0.0226086,"ere fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et al. (2019c,b) integrate the"
D19-1145,N19-1122,1,0.884589,"Missing"
D19-1145,P15-1150,0,0.0469481,"ks (SANs, Parikh et al., 2016; Lin et al., 2017) have achieved the state-of-the-art results on a variety of NLP tasks (Vaswani et al., 2017; Strubell et al., 2018; Devlin et al., 2019). S ANs perform the attention operation under the position-unaware “bagof-words” assumption, in which positions of the input words are ignored. Therefore, absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018) are generally used to capture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structural depths and distances (Hewitt and Manning, 2019). Accordingly, we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to enc"
D19-1145,N19-1419,0,0.0324287,"pture the sequential order of words in the sentence. However, several researches reveal that the sequential structure may not be sufficient for NLP tasks (Tai et al., 2015; Kim et al., 2017; Shen et al., 2019), since sentences inherently have hierarchical structures (Chomsky, 1965; Bever, 1970). In response to this problem, we propose to augment S ANs with structural position representations to capture the hierarchical structure of the input sentence. The starting point for our approach is a recent finding: the latent structure of a sentence can be captured by structural depths and distances (Hewitt and Manning, 2019). Accordingly, we propose absolute structural position to encode the depth of each word in a parsing tree, and relative structural position to encode the distance of each word pair in the tree. We implement our structural encoding strategies on top of T RANSFORMER (Vaswani et al., 2017) and conduct experiments on both NIST Chinese⇒English and WMT14 English⇒German translation tasks. Experimental results show that exploiting structural position encoding strategies consistently boosts performance over both the absolute and relative sequential position representations across language pairs. Lingui"
D19-1145,A97-1011,0,0.169453,"arescu et al., 2013). In this work, dependency path, which is induced from the dependency tree, is adopted to provide a new perspective on modelling pairwise relationships. Figure 1 shows the difference between the sequential path and dependency path. The sequential distance between the two words “held” and “talk” is 2, while their structural distance is only 1 as word “talk” is the dependent of the head “held” (Nivre, 2005). 1404 Absolute Structural Position We exploit the tree depth of the word in the dependency tree as its absolute structural position. Specifically, we treat the main verb (Tapanainen and Jarvinen, 1997) of the sentence as the origin and use the distance of the dependency path from the target word to the origin as the absolute structural position absstru (xi ) = distancetree (xi , origin), (5) where xi is the target word, tree is the given dependency structure and the origin is the main verb of the tree. In the field of NMT, BPE sub-words and endof-sentence symbol should be carefully handled as they do not appear in the conventional dependency tree. In this work, we assign the BPE sub-words share the absolute structural position of the original word and set the the first larger integer than t"
D19-1145,C18-1255,0,0.0502484,"sb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structur"
D19-1145,P19-1624,1,0.835259,"uctural PE) achieves further improvement up to +0.40 BLEU points and outperforms the Transformer-Big by 0.93 BLEU points. For English⇒German, similar phenomenon is observed, which reveals that the proposed structural position encoding strategy can consistently boost translation performance over both the absolute and relative sequential position representations. 5.3 Linguistic Probing Evaluation We conduct probing tasks3 (Conneau et al., 2018) to evaluate structure knowledge embedded in the encoder output in the variations of the Base model that are trained on En⇒De translation task. We follow Wang et al. (2019) to set model configurations. The experimental results on probing tasks are shown in Table 3, and the BLEU scores of “Base”, “+ Rel. Seq. PE”, “+ Stru. PE” are 3 https://github.com/facebookresearch/ SentEval/tree/master/data/probing 27.31, 27.99 and 28.30. From the table, we can see 1) adding the relative sequential positional embedding achieves improvement over the baseline on semantic tasks (75.03 vs. 74.61). This may indicate the model benefits more from semantic modeling; 2) with the structural positional embedding, the model obtains improvement on syntactic tasks (65.87 v.s. 64.98), which"
D19-1145,D18-1408,0,0.0426954,"PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representation"
D19-1145,D18-1475,1,0.83168,"epresentations1 : asb(xi ) =fabs (A BS PE(absseq ), A BS PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree t"
D19-1145,N19-1407,1,0.85324,"S PE(absstru )) (7) where fabs is the nonlinear function. A BS PE(absseq ) and A BS PE(absstru ) are absolute sequential and structural position embedding in Eq.3 and Eq.5 respectively. For the relative position, we follow Shaw et al. (2018) to extend the self-attention computation to consider the pairwise relationships and project the relative structural position as described at Eq.(3) and Eq.(4) in Shaw et al. (2018)2 . 4 Related Work There has been growing interest in improving the representation power of S ANs (Dou et al., 2018, 2019; Yang et al., 2018; Wang et al., 2018; Wu et al., 2018; Yang et al., 2019a,b; Sukhbaatar et al., 2019). Among these approaches, a straightforward strategy is that augmenting the S ANs with position representations (Shaw et al., 2018; Ma et al., 2019; Bello et al., 2019; Yang et al., 2019c), as the position representations involves elementwise attention computation. In this work, we propose to augment S ANs with structural position representations to model the latent structure of the input sentence. Our work is also related to the structure modeling for S ANs, as the proposed model utilizes the dependency tree to generate structural representations. Recently, Hao et"
D19-1145,P17-4012,0,0.105233,"002 (MT02) dataset is used as development set. NIST 2003, 2004, 2005, 2006 datasets are used as test sets. We use byte-pair encoding (BPE) toolkit to alleviate the out-of-vocabulary problem with 32K merge operations. English⇒German We use the dataset consisting of about 4.5 million sentence pairs as the training set. The newstest2013 and newstest2014 are used as the development set and the test set. We also apply BPE with 32K merge operations to obtain subword unit. We evaluate the proposed position encoding strategies on T RANSFORMER (Vaswani et al., 2017) and implement them on top of THUMT (Zhang et al., 2017). We use the Stanford parser (Klein and Manning, 2003) to parse the sentences and obtain the structural structural absolute and relative position as described in Section 3. When using relative structural position encoding, we use clipping distance r = 16. To make a fair comparison, we valid different position encoding strategies on the encoder and keep the T RANSFORMER decoder unchanged. Effect of Position Encoding We first remove the sequential encoding from the Transformer encoder (Model #1) and observe the translation performance degrades dramatically (28.33 − 44.31 = −15.98), which demonst"
D19-1195,P19-1389,1,0.832148,"律的 What is irregular? 有啥不好意思的 Why you are shy? 我也不规律 I am irregular too. See a doctor, the easiest way is with Table 5: Example responses from different models, as well as a visualization of the skeleton extraction in our model. Darker color indicates bigger matching scores and the words being selected for skeleton are in red boxes. sons is that for most queries, the set of possible responses is considerably large and the query alone cannot specify an informative response. Various approaches (Li et al., 2016b; Xing et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018; Liu et al., 2018; Bi et al., 2019; Tian et al., 2019; Gao et al., 2019) have been proposed for this problem. Some previous studies have been about using the results of traditional retrieval systems for informative response generation. Song et al. (2016) introduced an extra encoder for the retrieved response. The encoder’s output, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes the lexical diff"
D19-1195,D18-1297,1,0.852868,"rregular too. 有啥不规律的 What is irregular? 有啥不好意思的 Why you are shy? 我也不规律 I am irregular too. See a doctor, the easiest way is with Table 5: Example responses from different models, as well as a visualization of the skeleton extraction in our model. Darker color indicates bigger matching scores and the words being selected for skeleton are in red boxes. sons is that for most queries, the set of possible responses is considerably large and the query alone cannot specify an informative response. Various approaches (Li et al., 2016b; Xing et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018; Liu et al., 2018; Bi et al., 2019; Tian et al., 2019; Gao et al., 2019) have been proposed for this problem. Some previous studies have been about using the results of traditional retrieval systems for informative response generation. Song et al. (2016) introduced an extra encoder for the retrieved response. The encoder’s output, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes"
D19-1195,D15-1166,0,0.0206424,"ators in unsure cases. A set of 300 different query samples are used for evaluation. We recruit five experienced annotators and take the average score among them. Besides, we also use dist-1/dist-2 (Li et al., 2016a) to examine a model’s ability for generating diverse responses. which is the number of distinct unigrams/bi-grams divided by the total number. 3.2 Compared Methods To show the effectiveness of our proposed methods, we compare it with the following methods. • Retrieval The underlying retrieval system used in our experiments. • Seq2Seq The basic Seq2Seq model (Bahdanau et al., 2014; Luong et al., 2015) that only takes the query as input. • Seq2Seq-MMI A variant of the basic Seq2Seq model that uses Maximum Mutual Information (MMI) for filtering out generic responses (Li et al., 2016a). Concretely, a response-toquery Seq2Seq model is trained and used to 4 Douban https://www.douban.com/ and Weibo https://www.weibo.com/ 5 https://ai.qq.com/product/nlpchat. shtml • EditVec The model proposed in Wu et al. (2019). In addition to the retrieved response, the lexical difference (insert words and delete words) between the query and the retrieved query is also encoded (in a so-called edit vector) to fe"
D19-1195,N18-1204,0,0.0168843,"context similarity, yet their work is done in close domain conversation. The idea of editing some prototype materials rather than generating from scratch has also been explored in other text generation tasks. For examples, Guu et al. (2018) proposed a prototypethen-edit model for unconditional text generation. Wiseman et al. (2017, 2018) used either fixed template or learned templates for data-to-text generation. Xu et al. (2018) conditioned the next sentence generation on a skeleton that is extracted from the source input and the already generated text in storytelling. Also for storytelling, Clark et al. (2018) proposed to extract the entities in sentences and use them as additional input. Gu et al. (2018) uses retrieved translation as a reference to the generative translation model. 5 Conclusion In this paper, we presented a novel framework, matching-to-generation, for retrieval-guided response generation. Our method uses an interpretable matching model for response skeleton extraction and a robust response generator for response completion. The two components are trained separately to allow more flexibility. Experiments show our method significantly outperforms several strong baselines. 1873 Refer"
D19-1195,C16-1316,0,0.0421746,"Missing"
D19-1195,Q18-1031,0,0.0661934,"decoder. Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes the lexical differences between the current query and the retrieved query. Besides, Pandey et al. (2018) proposed to weight different training instances by context similarity, yet their work is done in close domain conversation. The idea of editing some prototype materials rather than generating from scratch has also been explored in other text generation tasks. For examples, Guu et al. (2018) proposed a prototypethen-edit model for unconditional text generation. Wiseman et al. (2017, 2018) used either fixed template or learned templates for data-to-text generation. Xu et al. (2018) conditioned the next sentence generation on a skeleton that is extracted from the source input and the already generated text in storytelling. Also for storytelling, Clark et al. (2018) proposed to extract the entities in sentences and use them as additional input. Gu et al. (2018) uses retrieved translation as a reference to the generative translation model. 5 Conclusion In this paper, we presented a n"
D19-1195,P18-1123,0,0.153056,"l., 2016a). This problem is avoided in traditional retrieval systems (Ji et al., 2014; Hu et al., 2014) by preceding the selection of informative and engaging responses. It is of interest to benefit from both the generalization capacity of the seq2seq models and the information richness of the retrieved responses. Following the standard encoder-decoder framework, early attempts have either used an extra encoder for the retrieved response (Song et al., 2016; ∗ Vanilla seq2seq retrieve This work was mainly done when Deng Cai was an intern at Tencent AI Lab. Yan Wang is the corresponding author. Pandey et al., 2018; Wu et al., 2019) or a unified encoder for the concatenation of the query and the retrieved response (Weston et al., 2018). To prevent the inflow of erroneous information, Cai et al. (2019) proposed a general framework that first extracts a skeleton from the retrieved response and then generates the response based on the extracted skeleton. Despite their differences, a common issue is that the generation model easily learns to ignore the retrieved response entirely and collapses to a vanilla seq2seq model. As shown in Figure 1, this happens with improper training instances. Given the large sp"
D19-1195,P19-1372,1,0.886718,"Missing"
D19-1195,P15-1152,0,0.16558,"Missing"
D19-1195,N15-1020,0,0.209982,"neration is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs. 1 collapse Bad, I hate the weather. mismatch generate generate Response: Great, I get promotion today. Figure 1: The common problem for training a retrievalguided generation model in previous work. The model is forced to neglect the retrieved response even though it is a proper response, due to the mismatch between the retrieved response and the target response. Introduction Sequence-to-sequence (seq2seq) neural models (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a) have been popular for single-turn dialogue response generation. However, many of the generated responses (e.g., “I don’t know” and “I think so”) appear to be generic and dull (safe response problem) (Li et al., 2016a). This problem is avoided in traditional retrieval systems (Ji et al., 2014; Hu et al., 2014) by preceding the selection of informative and engaging responses. It is of interest to benefit from both the generalization capacity of the seq2seq models and the information richness of the retrieved responses. Following the standard encoder-decod"
D19-1195,N16-1014,0,0.69571,"ained generator. Extensive experiments demonstrate the effectiveness of our model designs. 1 collapse Bad, I hate the weather. mismatch generate generate Response: Great, I get promotion today. Figure 1: The common problem for training a retrievalguided generation model in previous work. The model is forced to neglect the retrieved response even though it is a proper response, due to the mismatch between the retrieved response and the target response. Introduction Sequence-to-sequence (seq2seq) neural models (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a) have been popular for single-turn dialogue response generation. However, many of the generated responses (e.g., “I don’t know” and “I think so”) appear to be generic and dull (safe response problem) (Li et al., 2016a). This problem is avoided in traditional retrieval systems (Ji et al., 2014; Hu et al., 2014) by preceding the selection of informative and engaging responses. It is of interest to benefit from both the generalization capacity of the seq2seq models and the information richness of the retrieved responses. Following the standard encoder-decoder framework, early attempts have eith"
D19-1195,P16-1094,0,0.0645175,"Missing"
D19-1195,P19-1371,1,0.839076,"lar? 有啥不好意思的 Why you are shy? 我也不规律 I am irregular too. See a doctor, the easiest way is with Table 5: Example responses from different models, as well as a visualization of the skeleton extraction in our model. Darker color indicates bigger matching scores and the words being selected for skeleton are in red boxes. sons is that for most queries, the set of possible responses is considerably large and the query alone cannot specify an informative response. Various approaches (Li et al., 2016b; Xing et al., 2017; Ghazvininejad et al., 2018; Zhou et al., 2018; Liu et al., 2018; Bi et al., 2019; Tian et al., 2019; Gao et al., 2019) have been proposed for this problem. Some previous studies have been about using the results of traditional retrieval systems for informative response generation. Song et al. (2016) introduced an extra encoder for the retrieved response. The encoder’s output, together with that of the query encoder, is utilized to feed the decoder. Weston et al. (2018) simply concatenated the original query and the retrieved response as the input to the encoder. Instead of solely using the retrieved response, Wu et al. (2019) further introduced to encodes the lexical differences between the"
D19-1195,W18-5713,0,0.28376,"election of informative and engaging responses. It is of interest to benefit from both the generalization capacity of the seq2seq models and the information richness of the retrieved responses. Following the standard encoder-decoder framework, early attempts have either used an extra encoder for the retrieved response (Song et al., 2016; ∗ Vanilla seq2seq retrieve This work was mainly done when Deng Cai was an intern at Tencent AI Lab. Yan Wang is the corresponding author. Pandey et al., 2018; Wu et al., 2019) or a unified encoder for the concatenation of the query and the retrieved response (Weston et al., 2018). To prevent the inflow of erroneous information, Cai et al. (2019) proposed a general framework that first extracts a skeleton from the retrieved response and then generates the response based on the extracted skeleton. Despite their differences, a common issue is that the generation model easily learns to ignore the retrieved response entirely and collapses to a vanilla seq2seq model. As shown in Figure 1, this happens with improper training instances. Given the large space of possible responses, it happens frequently that a retrieved response (extracted skeleton) is suitable for responding"
D19-1195,D18-1356,0,0.0916317,"Missing"
D19-1195,D18-1462,0,0.0231513,"ther introduced to encodes the lexical differences between the current query and the retrieved query. Besides, Pandey et al. (2018) proposed to weight different training instances by context similarity, yet their work is done in close domain conversation. The idea of editing some prototype materials rather than generating from scratch has also been explored in other text generation tasks. For examples, Guu et al. (2018) proposed a prototypethen-edit model for unconditional text generation. Wiseman et al. (2017, 2018) used either fixed template or learned templates for data-to-text generation. Xu et al. (2018) conditioned the next sentence generation on a skeleton that is extracted from the source input and the already generated text in storytelling. Also for storytelling, Clark et al. (2018) proposed to extract the entities in sentences and use them as additional input. Gu et al. (2018) uses retrieved translation as a reference to the generative translation model. 5 Conclusion In this paper, we presented a novel framework, matching-to-generation, for retrieval-guided response generation. Our method uses an interpretable matching model for response skeleton extraction and a robust response generato"
D19-1195,P18-1101,0,0.0560756,"Missing"
D19-1499,N16-1012,0,0.0398371,"space of different styles and design two constraints to train it. We also introduce two other simple but effective semisupervised methods to compare with. To evaluate the performance of the proposed methods, we build and release a novel style transfer dataset that alters sentences between the style of ancient Chinese poem and the modern Chinese. 1 Introduction Recently, the natural language generation (NLG) tasks have been attracting the growing attention of researchers, including response generation (Vinyals and Le, 2015), machine translation (Bahdanau et al., 2014), automatic summarization (Chopra et al., 2016), question generation (Gao et al., 2019), etc. Among these generation tasks, one interesting but challenging problem is text style transfer (Shen et al., 2017; Fu et al., 2018; Logeswaran et al., 2018). Given a sentence from one style domain, a style transfer system is required to convert it to another style domain as well as keeping its content meaning unchanged. As a fundamental attribute of text, style can have a broad and ambiguous scope, such as ancient poetry style v.s. modern language style and positive sentiment v.s. negative sentiment. ∗ This work was done when Mingyue Shang was an in"
D19-1499,D19-1306,0,0.0616094,"Missing"
D19-1499,D14-1181,0,0.00593167,"Missing"
D19-1499,J82-2005,0,0.727621,"Missing"
D19-1499,D18-1420,1,0.850292,"gn a strategy to disentangle the variables for content and style. Shen et al. (2017) first map the text corpora belonging to different styles to their own space respectively, and leverages the alignment of latent representations from different styles to perform style transfer. Chen et al. (2019) propose to extract and control the style of the image caption through domain layer normalization. Prabhumoye et al. (2018) and Zhang et al. (2018b) employ the back-translation mechanism to ensure that the input from the source style can be reconstructed from the transferred result in the target style. Liao et al. (2018) associate the style latent variable with a numeric discrete or continues numeric value and can generate sentences controlled by this value. Among them, many use the adversarial training mechanism (Goodfellow et al., 2014) to improve the performance of the basic models (Shen et al., 2017; Zhao et al., 2018). To sum up, most of the existing unsupervised frameworks on the text style transfer focus on getting the disentangled representations of style and content. However, Lample et al. (2019) illustrated that the disentanglement not adequate to learn the style-independent representations, thus th"
D19-1499,W02-0109,0,0.189984,"by Rao and Tetreault (2018) which contains texts of formal and informal style. With the released data, we randomly sample 5,000 sentence pairs from it as the parallel corpus with limited data volume. We then use the Yahoo Answers L6 corpus7 as the source which is in the same content domain as the parallel data to construct the large-scale nonparallel data. To divide nonparallel dataset into two styles, we train a CNN-based classifier (Kim, 2014) on the parallel data with annotation of styles and use it to classify the nonparallel data. sentence as 30. For the formality datasets, we use NLTK (Loper and Bird, 2002) to tokenize the texts and set the minimum length as 5 and the maximum length as 30 for both formal and informal styles. We adopt GloVE (Pennington et al., 2014) to pretrain the embeddings, and the dimensions of the embeddings are set to 300 for all the datasets. The hidden states are set to 500 for both encoders and decoders. We adopt SGD optimizer with the learning rate as 1 for DAE models and 0.1 for S2S models. The dropout rate is 0.4. In the inference stage, the beam size is set to 5. 7.2 8.1 Experimental Settings We perform different data preprocessing on different datasets. The Chinese"
D19-1499,P15-2097,0,0.0419915,"e supervised baseline and the three semi-supervised models. Fluency 0.3575 0.4425 0.5800 0.6325 0.3050 0.5475 0.5725 0.6200 9 Table 4: The human annotation results of the S2S model and CPLS model from three aspects. as the automatic evaluation metrics to measure the content preservation degree and the style changing degree. BLEU calculates the N-gram overlap between the generated sentence and the references, thus can be used to measure the preservation of text content. Considering that text style transfer is a monolingual text generation task, we also use GLEU, a generalized BLEU proposed by (Napoles et al., 2015). To evaluate the extent to which the sentences are transferred to the target style, we follow Shen et al. (2017); Hu et al. (2017) that build a CNN-based style classifier and use it to measure the style accuracy. 8.3 Human Evaluation We also adopt human evaluations to judge the quality of the transferred sentences from three aspects, namely content, style and fluency. These aspects evaluate how well the transferred text preserve the content of the input, the style strength and the fluency of the transferred text. Take the content relevance for example, the criterion is as follows: +2: The tra"
D19-1499,D18-1138,0,0.0162566,"e the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and design a strategy to disentangle the variables for content and style. Shen et al. (2017) first map the text corpora belonging to different styles to their own space respectively, and leverages the alignment of latent representations from different styles to perform style transfer. Chen et al. (2019) propose to extract and control the style of the image caption through domain layer normalization. Prabhumoye et al. (2018) and Zhang et al. (2018b) employ the back-translation mechanism to ensure that the input from the source style can be reconstructed from the transferred result in the target style. Liao et al. (2018) associate the style latent variable with a numeric discrete or continues numeric value and can generate sentences controlled by this value. Among them, many use the adversarial training mechanism (Goodfellow et al., 2014) to improve the performance of the basic models (Shen et al., 2017; Zhao et al., 2018). To sum up, most of the existing unsupervised frameworks on the text style transfer focus on getting the disentangl"
D19-1499,P02-1040,0,0.106223,"Missing"
D19-1499,D14-1162,0,0.0825285,"parallel corpus with limited data volume. We then use the Yahoo Answers L6 corpus7 as the source which is in the same content domain as the parallel data to construct the large-scale nonparallel data. To divide nonparallel dataset into two styles, we train a CNN-based classifier (Kim, 2014) on the parallel data with annotation of styles and use it to classify the nonparallel data. sentence as 30. For the formality datasets, we use NLTK (Loper and Bird, 2002) to tokenize the texts and set the minimum length as 5 and the maximum length as 30 for both formal and informal styles. We adopt GloVE (Pennington et al., 2014) to pretrain the embeddings, and the dimensions of the embeddings are set to 300 for all the datasets. The hidden states are set to 500 for both encoders and decoders. We adopt SGD optimizer with the learning rate as 1 for DAE models and 0.1 for S2S models. The dropout rate is 0.4. In the inference stage, the beam size is set to 5. 7.2 8.1 Experimental Settings We perform different data preprocessing on different datasets. The Chinese literary datasets are segmented by characters instead of word to alleviate the issue of unknown words. Our statistics show that the average length of ancient poe"
D19-1499,P18-1080,0,0.0387814,"ting the training mode between supervised and unsupervised. • We introduce another two semi-supervised methods that are simple but effective to leverage both the nonparallel and parallel data. • We build a small-scale parallel dataset that contains ancient Chinese poem style and modern Chinese style sentences. We also collect two large nonparallel datasets of these styles.1 2 Related Works Recently, text style transfer has stimulated great interests of researchers from the area of neural language processing and some encouraging results are obtained (Shen et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Hu et al., 2017; Jin et al., 2019). In the primary stage, due to the lacking of parallel corpus, most of the methods employ unsupervised learning paradigm to conduct the semantic modeling and transfer. 1 Download link: https://tinyurl.com/yyc8zkqg Unsupervised Learning Methods. Mueller et al. (2017) modify the latent variables of sentences in a certain direction guided by a classifier to generate the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and design a strategy to disenta"
D19-1499,N18-1012,0,0.358646,"el is flexible in alternating the training mode between supervised and unsupervised. • We introduce another two semi-supervised methods that are simple but effective to leverage both the nonparallel and parallel data. • We build a small-scale parallel dataset that contains ancient Chinese poem style and modern Chinese style sentences. We also collect two large nonparallel datasets of these styles.1 2 Related Works Recently, text style transfer has stimulated great interests of researchers from the area of neural language processing and some encouraging results are obtained (Shen et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Hu et al., 2017; Jin et al., 2019). In the primary stage, due to the lacking of parallel corpus, most of the methods employ unsupervised learning paradigm to conduct the semantic modeling and transfer. 1 Download link: https://tinyurl.com/yyc8zkqg Unsupervised Learning Methods. Mueller et al. (2017) modify the latent variables of sentences in a certain direction guided by a classifier to generate the sentence with classifier favored style. Hu et al. (2017) employ variational autoencoders (Kingma and Welling, 2013) to conduct the style latent variable learning and des"
D19-1499,P16-1009,0,0.0730157,"Missing"
N18-1125,P05-1033,0,0.188078,"dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation error due to the incorrect attention. (b) With the help of the target foresight information “VBZ”, TFA-NMT is likely to figure out the exact translation as the reference in (c). The light font denotes the target words to be translated in"
N18-1125,P11-2031,0,0.0255199,"Missing"
N18-1125,2016.amta-researchers.10,0,0.161257,"auxiliary information from a target foresight word into the attention model. However, there is a significant difference between our approach and their approaches. Our auxiliary information biases to the word to be translated at next timestep while theirs biases to the information available so far at the current timestep, and thereby our approach is orthogonal to theirs. The works mentioned above improve the attention models by access auxiliary information, and thus they modify the structure of attention models in both inference and learning. In contrast, Mi et al. (2016); Liu et al. (2016b); Chen et al. (2016) maintain the structure of the attention models in inference but utilize some external signals to supervise the outputs of attention models during the learning. They improve the generalization abilities of attention models by use of the external aligners as the signals, which typically yield alignment results accurate enough to guide the learning of attention. 6 Conclusion It has been argued that the traditional attention model in neural machine translation suf1387 System Model Dev MT05 MT06 MT08 Ave. (Liu et al., 2016b) Moses NMT-J – – 35.4 36.8 33.7 36.9 25.0 28.5 31.37 34.07 (Liu et al., 20"
N18-1125,C16-1290,0,0.0446104,"sh and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation er"
N18-1125,P07-2045,0,0.0110597,".1 1844.9 72.0 1666.1 70.1 1485.2 59.1 Performance BLEU FPA 38.65 – 38.57 – 38.83 69.03 39.26 69.95 40.63 71.91 Table 1: Speeds and performances of the proposed models. “Speed” is measured in words/second for both training and decoding, and performances are measured in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a"
N18-1125,N03-1017,0,0.0510404,"shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtains a translation error due to the incorrect attention. (b) With the help of the target foresight information “VBZ”, TFA-NMT is likely to figure out the exact translation as the reference in (c). The light font denotes the target words to be"
N18-1125,C16-1291,1,0.743925,"Missing"
N18-1125,N16-1046,1,0.904938,"tly the same as the original concept in alignment task (Peter et al., 2017). However, both of them share a common idea that foresight word should be at a later time step, and thus we respect the work in Peter et al. (2017) and maintain the same concept for easier understanding. 1380 Proceedings of NAACL-HLT 2018, pages 1380–1390 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics i.e. the first light color word in Figure 1(a), is not known but to be translated at the next time step. Therefore, this may lead to inadequate modeling for attention mechanism (Liu et al., 2016a; Peter et al., 2017). Regarding to this, Peter et al. (2017) explicitly feed this target word into the attention model, and demonstrate the significant improvements in alignment accuracy. Unfortunately, this approach relies on the premise that the target foresight word is available in advance in its alignment scenario, and thus it can not be used in the translation scenario. To address this issue, in this paper, we propose a target foresight based attention (TFA) model oriented to both alignment and translation tasks. Its basic idea includes two steps: it firstly designs an auxiliary mechani"
N18-1125,P05-1057,0,0.582372,"is issue, in this paper, we propose a target foresight based attention (TFA) model oriented to both alignment and translation tasks. Its basic idea includes two steps: it firstly designs an auxiliary mechanism to predict some information for the target foresight word which is helpful for alignment; and then it feeds the predicted result into the attention model for translation. For the sake of efficiency, instead of predicting the target foresight word with large vocabulary size, we only predict its partial information, i.e. partof-speech tag, which is proved to be helpful for word alignment (Liu et al., 2005). Figure 1(b) shows the main idea of TFA based on NMT. In order to remit the negative effects due to the prediction errors, we feed the distribution of the prediction result instead of the maximum a posteriori result into the attention model. In addition, since the target foresight words are available during the training, we jointly learn the prediction model for the target foresight words and the translation model in a supervised manner. This paper makes the following contributions: • It proposes a novel TFA-NMT for neural machine translation by using an auxiliary mechanism to predict the tar"
N18-1125,D15-1166,0,0.658725,"pirical experiments on chineseto-english and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a)"
N18-1125,C16-1205,0,0.0527507,"Missing"
N18-1125,D16-1249,0,0.0306058,"e propose approach, since we introduce auxiliary information from a target foresight word into the attention model. However, there is a significant difference between our approach and their approaches. Our auxiliary information biases to the word to be translated at next timestep while theirs biases to the information available so far at the current timestep, and thereby our approach is orthogonal to theirs. The works mentioned above improve the attention models by access auxiliary information, and thus they modify the structure of attention models in both inference and learning. In contrast, Mi et al. (2016); Liu et al. (2016b); Chen et al. (2016) maintain the structure of the attention models in inference but utilize some external signals to supervise the outputs of attention models during the learning. They improve the generalization abilities of attention models by use of the external aligners as the signals, which typically yield alignment results accurate enough to guide the learning of attention. 6 Conclusion It has been argued that the traditional attention model in neural machine translation suf1387 System Model Dev MT05 MT06 MT08 Ave. (Liu et al., 2016b) Moses NMT-J – – 35.4 36.8 33.7 36"
N18-1125,P03-1021,0,0.0158574,"ent of our model is not only from the POS tagging. In order to further validate the improvements of variant proposed models, we evaluate the foresight prediction accuracy (FPA) for three proposed prediction models. We found that the fine-grained Model3 achieves the best FPA, indicating a good estimated foresight is very important to obtain the gains in terms of BLEU. 4.2.2 Decode Exp Exp Map Analysis on Syntactic Categories In this experiment, we investigate which category of generated words benefit most from the proposed approach in terms of alignments measured by alignment error rate (AER) (Och, 2003). We carry out experiments on the evaluation dataset from (Liu and Sun, 2015), which contains 900 manually aligned ChineseEnglish sentence pairs. Following (Luong et al., 2015b), we force-decode both the bilingual sentences including source and reference sentences to obtain the attention matrices, and then we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. As shown in Table 2, the AER improvements are modest for content words such as Noun, Verb, and adjective (“Adj.”) words; but there are substantial improvements for func"
N18-1125,C04-1156,0,0.0908774,"Missing"
N18-1125,E17-3017,0,0.0271867,".91 Table 1: Speeds and performances of the proposed models. “Speed” is measured in words/second for both training and decoding, and performances are measured in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a batch. The dimension of word embedding is 620. The dimensions of both feed forward NN and RNN hidden layer are"
N18-1125,W16-2209,0,0.0362855,"epresentations in decoding: Exp for expectation and Map for maximum a posteriori. Table 2: Performances on syntactic categories. “Base” denotes “Nematus”, and Ours denotes the proposed model. icantly better than baseline, but Model2 is significantly better with p&lt;0.05 and Model3 is significantly better with p&lt;0.01. Given that simply introducing an additional layer (“+2Layer”) does not produce any improvement on this data, we believe the gain of our model is not only from the more introduced parameters. Besides, we augment the word embedding by concatenating the POS tag embedding, proposed by (Sennrich and Haddow, 2016), the BLEU is 38.96, which indicating the improvement of our model is not only from the POS tagging. In order to further validate the improvements of variant proposed models, we evaluate the foresight prediction accuracy (FPA) for three proposed prediction models. We found that the fine-grained Model3 achieves the best FPA, indicating a good estimated foresight is very important to obtain the gains in terms of BLEU. 4.2.2 Decode Exp Exp Map Analysis on Syntactic Categories In this experiment, we investigate which category of generated words benefit most from the proposed approach in terms of a"
N18-1125,N03-1033,0,0.0381169,"d in terms of BLEU scores (“BLEU”) and foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote better performance. erence, following (Goto et al., 2013; Liu et al., 2016b) for further comparison. Implementation We compare the proposed models with two strong baselines from SMT and NMT: • Moses (Koehn et al., 2007): an open source phrased based translation system with default configuration. • Nematus (Sennrich et al., 2017): generic attention based NMT. an We implement the proposed models on top of Nematus. We use Stanford Log-linear PartOf-Speech Tagger (Toutanova et al., 2003) to produce POS tags for the English side. For both Chinese-to-English and Japanese-toEnglish tasks, we limit the vocabularies to the most frequent 30K words for both sides. All the out-of-vocabulary words are mapped to a spacial token “UNK”. Only the sentences of length up to 50 words are used in training, with 80 sentences in a batch. The dimension of word embedding is 620. The dimensions of both feed forward NN and RNN hidden layer are 1000. The beam size for decoding is 12, and the cost function is optimized by Adadelta with hyper-parameters suggested by Zeiler (2012). Particularly for TFA"
N18-1125,P16-1008,1,0.922677,"n chineseto-english and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu. 1 French unemployment rate rises again &lt;/S> (a) Baseline fă guó shī yè rén shù zài dù huí shēng &lt;/S> 法国 失业 人数 再度 回升 &lt;/S> JJ NN VBZ RB EOS French unemployment rises again &lt;/S> (b) TFA-NMT French unemployment rises again &lt;/S> (c) Reference Introduction Since neural machine translation (NMT) was proposed (Bahdanau et al., 2014), it has been attracted increasing interests in machine translation community (Luong et al., 2015b; Tu et al., 2016; Feng et al., 2016; Cohn et al., 2016). NMT not only yields impressive translation performance in practice, but also has appealing model architecture in essence. Compared with traditional statistical machine translation (Koehn et al., 2003; Chiang, 2005), one of advantages in NMT is that its architecture combines language model, translation model and alignment between source and target words in a unified manner rather than a ∗ Work done when X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. Figure 1: A running example to motivate the proposed model. (a) The baseline obtai"
N18-1125,D17-1013,0,0.0215987,"n. βi is derived from Figure 3, zi is from Eq.(10-11), and other nodes are similar to ones in Figure 2. obtained from Eq.(10-11) and the prediction model as shown in Figure 3. Note that the proposed TFA-NMT models the target foresight word, which is a future word regarding to the current time step, to conduct attention calculation. In this sense, it employs the idea of modeling future and thus resembles to the work in (Zheng et al., 2017). The main difference is that TFA-NMT models the future from the target side whereas Zheng et al. (2017) models the future from the source side. In addition, Weng et al. (2017) imposes a regularization term by using future words during training. Unlike our approach, their approach does not use future words during the inference because these words are unavailable. Anyway, it is possible to put both their approach and our approach together for further improvements. 3.3 Suppose {⟨ k k a kset ⟩ of training data } is denoted by x , y , u |k = 1, · · · , K . Here xk , yk and uk denotes a source sentence, a target sentence and a POS tag sequence of yk , respectively. Then one can jointly train both the translation model for yk and the prediction model for uk by minimizing"
N18-1125,1983.tc-1.13,0,0.573571,"Missing"
N18-1125,Q18-1011,1,0.889688,"Missing"
N18-2028,P12-1015,0,0.0620188,"Missing"
N18-2028,N15-1184,0,0.0585757,"Missing"
N18-2028,P11-2008,0,0.0620614,"Missing"
N18-2028,J15-4004,0,0.0663978,"Missing"
N18-2028,D15-1242,0,0.0377637,"Missing"
N18-2028,P14-2131,0,0.0634583,"Missing"
N18-2028,N16-1030,0,0.0337005,"et al. (2015a), this task is performed in both news and social media data. For news data, we use Wall Street Journal (WSJ) proportion from the Penn Treebank (Marcus et al., 1993) and follow the standard split of 38,219/5,527/5,462 sentences for training, development, and test, respectively. The social media data is based on ARK dataset (Gimpel et al., 2011), which contains manual POS annotations on English tweets. The standard split of ARK contains 1,000/327/500 tweets as training/development/test set, respectively. POS prediction is conducted by a bidirectional LSTM-CRF (Huang et al., 2015; Lample et al., 2016) taking the produced embeddings as input. LSTM state size is setting to 200. For WSJ, we use the aforementioned embeddings trained from the Wiki corpus. For ARK, we prepare a Twitter corpus (TWT) to build embeddings. This data contains 100 million tweets collected through Twitter streaming API6 , followed by preprocessing using the toolkit described in Owoputi et al. (2013). The TWT embeddings are trained under the same procedure as the Wiki embeddings. Similar to word similarity task, we use CBOW, SG, CWin, SSG and SSSG as baselines in this task. Results are reported in Table 5. We observe th"
N18-2028,N15-1142,0,0.28912,"ional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings Yan Song, Shuming Shi, Jing Li, Haisong Zhang Tencent AI Lab {clksong,shumingshi,ameliajli,hansonzhang}@tencent.com Abstract resources, which are hard to obtain or annotate. To overcome such limitations, there are many approaches to further exploiting the characteristics of the running text, e.g., internal structure of the context. These approaches include enlarging the projection layer with consideration of word orders (Bansal et al., 2014; Ling et al., 2015a), learning context words with different weights (Ling et al., 2015b), etc. They are advantageous of learning word embeddings in an end-to-end unsupervised manner without requiring additional resources. Yet, they are also restricted in their implementation such as that they normally require a larger hidden layer or additional weights, which demand higher computation burden and could result in gradient explosion when embedding dimensions are enlarged. Another issue is that when considering word orders, they may suffer from data sparsity problem since n-gram coverage is much less than word, especially in the cold start scenario for a new domain where training d"
N18-2028,K17-1016,1,0.841233,"l with negative sampling (Mikolov et al., 2013a,c) is a popular choice for learning word embeddings and has had large impact in the community, for its efficient training and good performance in downstream applications. Although widely used for multiple tasks, SG model relies on word co-occurrence within local context in word prediction but ignores further detailed information such as word orders, positions. To improve original word embedding models, there are various studies leveraging external knowledge to update word embeddings with post processing (Faruqui et al., 2015; Kiela et al., 2015; Song et al., 2017) or supervised objectives (Yu and Dredze, 2014; Nguyen et al., 2016). However, these approaches are limited by reliable semantic 175 Proceedings of NAACL-HLT 2018, pages 175–180 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 2.1 Model SG SSG SSSG DSG Approach Skip-Gram Model The SG model (Mikolov et al., 2013b) is a popular choice to learn word embeddings by leveraging the relations between a word and its neighboring words. In detail, the SG model is to predict the context for each given word wt , and maximizes LSG |V | 1 X X log f (wt+i , wt ) = |"
N18-2028,D15-1161,0,0.310708,"ional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings Yan Song, Shuming Shi, Jing Li, Haisong Zhang Tencent AI Lab {clksong,shumingshi,ameliajli,hansonzhang}@tencent.com Abstract resources, which are hard to obtain or annotate. To overcome such limitations, there are many approaches to further exploiting the characteristics of the running text, e.g., internal structure of the context. These approaches include enlarging the projection layer with consideration of word orders (Bansal et al., 2014; Ling et al., 2015a), learning context words with different weights (Ling et al., 2015b), etc. They are advantageous of learning word embeddings in an end-to-end unsupervised manner without requiring additional resources. Yet, they are also restricted in their implementation such as that they normally require a larger hidden layer or additional weights, which demand higher computation burden and could result in gradient explosion when embedding dimensions are enlarged. Another issue is that when considering word orders, they may suffer from data sparsity problem since n-gram coverage is much less than word, especially in the cold start scenario for a new domain where training d"
N18-2028,P14-2089,0,0.0323907,"3a,c) is a popular choice for learning word embeddings and has had large impact in the community, for its efficient training and good performance in downstream applications. Although widely used for multiple tasks, SG model relies on word co-occurrence within local context in word prediction but ignores further detailed information such as word orders, positions. To improve original word embedding models, there are various studies leveraging external knowledge to update word embeddings with post processing (Faruqui et al., 2015; Kiela et al., 2015; Song et al., 2017) or supervised objectives (Yu and Dredze, 2014; Nguyen et al., 2016). However, these approaches are limited by reliable semantic 175 Proceedings of NAACL-HLT 2018, pages 175–180 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 2.1 Model SG SSG SSSG DSG Approach Skip-Gram Model The SG model (Mikolov et al., 2013b) is a popular choice to learn word embeddings by leveraging the relations between a word and its neighboring words. In detail, the SG model is to predict the context for each given word wt , and maximizes LSG |V | 1 X X log f (wt+i , wt ) = |V | Operations 2cC(n + 1)o 4c2 C(n + 1)o 4cC(n"
N18-2028,J93-2004,0,0.0613104,"n context, while not severely suffered from the data sparsity problem. Particularly among all SG models, DSG produces the best performance when trained on either the large or the small corpus. This fact further proves 3.3 Part-of-Speech Tagging Besides the intrinsic evaluation to test the embeddings semantically, we also evaluate different embeddings syntactically with an extrinsic evaluation: part-of-speech (POS) tagging. Following Ling et al. (2015a), this task is performed in both news and social media data. For news data, we use Wall Street Journal (WSJ) proportion from the Penn Treebank (Marcus et al., 1993) and follow the standard split of 38,219/5,527/5,462 sentences for training, development, and test, respectively. The social media data is based on ARK dataset (Gimpel et al., 2011), which contains manual POS annotations on English tweets. The standard split of ARK contains 1,000/327/500 tweets as training/development/test set, respectively. POS prediction is conducted by a bidirectional LSTM-CRF (Huang et al., 2015; Lample et al., 2016) taking the produced embeddings as input. LSTM state size is setting to 200. For WSJ, we use the aforementioned embeddings trained from the Wiki corpus. For AR"
N18-2028,N13-1090,0,0.705963,"s efficient as the original skip-gram model, when compared to other extensions of the skip-gram model. Experimental results show that our model outperforms others on different datasets in semantic (word similarity measurement) and syntactic (partof-speech tagging) evaluations, respectively. 1 Introduction Word embedding and its related techniques have shown to be vital for natural language processing (NLP) (Bengio et al., 2003; Collobert and Weston, 2008; Turney and Pantel, 2010; Collobert et al., 2011; Weston et al., 2015; Song and Lee, 2017). The skip-gram (SG) model with negative sampling (Mikolov et al., 2013a,c) is a popular choice for learning word embeddings and has had large impact in the community, for its efficient training and good performance in downstream applications. Although widely used for multiple tasks, SG model relies on word co-occurrence within local context in word prediction but ignores further detailed information such as word orders, positions. To improve original word embedding models, there are various studies leveraging external knowledge to update word embeddings with post processing (Faruqui et al., 2015; Kiela et al., 2015; Song et al., 2017) or supervised objectives (Y"
N18-2028,P16-2074,0,0.0174545,"oice for learning word embeddings and has had large impact in the community, for its efficient training and good performance in downstream applications. Although widely used for multiple tasks, SG model relies on word co-occurrence within local context in word prediction but ignores further detailed information such as word orders, positions. To improve original word embedding models, there are various studies leveraging external knowledge to update word embeddings with post processing (Faruqui et al., 2015; Kiela et al., 2015; Song et al., 2017) or supervised objectives (Yu and Dredze, 2014; Nguyen et al., 2016). However, these approaches are limited by reliable semantic 175 Proceedings of NAACL-HLT 2018, pages 175–180 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 2.1 Model SG SSG SSSG DSG Approach Skip-Gram Model The SG model (Mikolov et al., 2013b) is a popular choice to learn word embeddings by leveraging the relations between a word and its neighboring words. In detail, the SG model is to predict the context for each given word wt , and maximizes LSG |V | 1 X X log f (wt+i , wt ) = |V | Operations 2cC(n + 1)o 4c2 C(n + 1)o 4cC(n + 1)o 2cC(n + 2)o Tab"
N18-2028,N13-1039,0,0.0495604,"Missing"
N19-1046,D18-1036,0,0.0671355,"Missing"
N19-1046,W18-5451,0,0.0291084,"sistent improvements (up to +1.3 BLEU) compared to the state-of-the-art translation model. 1 apple … cola … dog … wolf … 4 0000 0001 0100 0111 3 000 of … in … 010 001 depth task 1100 1101 rice … cat … 4 run … 011 110 111 3 is … 2 00 01 10 11 2 Structural HR HR 1 x, y?< 0 1 1 root Figure 1: The structural hierarchical regularization framework. On the left is a 4-layer NMT decoder; on the right is a hierarchical clustering tree and the treeinduced relative tasks at every tree depth. to understand the hidden representations through the lens of a few linguistic tasks, while Ding et al. (2017) and Strobelt et al. (2018) propose appealing visualization approaches to understand NMT models including the representation of hidden layers. However, employing the analyses to motivate new methods for better translation, the ultimate goal of understanding NMT, is not achieved in these works. In our paper, we aim at understanding the hidden representation of NMT from an alternative viewpoint, and particularly we propose simple yet effective methods to improve the translation performance based on our understanding. We start from a fundamental question: what are the characteristics of the hidden representation for better"
N19-1046,P18-2104,0,0.0318354,"wledge about part-of-speech and semantic tags at different layers. Unlike those works that employ one or two linguistic tasks, we instead construct plenty of artificial tasks without any human annotations to analyze the hidden representations. This makes our approach more general and may potentially lead to less biased conclusions. Based on our understanding of the hidden representations, we further develop simple methods to improve NMT through representation regularization. Many works regularize NMT with lexical knowledge such as BOW (Weng et al., 2017) and morphology (Niehues and Cho, 2017; Zaremoodi et al., 2018), or syntactic knowledge (Kiperwasser and Ballesteros, 2018; Eriguchi et al., 2017). One significant difference is that we take into account the structure among plenty of artificial tasks and design a well motivated regularization term to encourage the structural consistency of tasks, which further improves NMT performance. In addition, our coarse-to-fine way to select tasks for regularization is also inspired by recent works using a coarse-to-fine mechanism for learning better word embeddings in NMT (Zhang et al., 2018) and predicting intermediate solutions for semantic parsing (Dong and Lapa"
N19-1124,E17-2029,0,0.0279951,"l techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re"
N19-1124,P18-1063,0,0.0252571,"latter took advantages of both sides. In a closed domain conversation setting, Pandey et al. (2018) further proposed to weight different training instances by context similarity. Our model differs from them in that we take an extra intermediate step for skeleton generation to filter the retrieval information before use, which shows the effectiveness in avoiding the erroneous copy in our experiments. Multi-step Language Generation Our work is also inspired by the recent success of decomposing an end-to-end language generation task into several sequential sub-tasks. For document summarization, Chen and Bansal (2018) first select salient sentences and then rewrite them in parallel. For sentiment-to-sentiment translation, Xu et al. (2018) first use a neutralization module to remove emotional words and then add sentiment to the neutralized content. Not only does their decomposition improve the overall performance, but also makes the whole generation process more interpretable. Our skeleton-to-response framework also sheds some light on the use of retrieval memories. 5 Experiments 5.1 Data We use the preprocessed data in (Wu et al., 2019) as our test bed. The total dataset consists of about 20 million single"
N19-1124,N16-1014,0,0.719468,"he IR-based models (Ji et al., 2014; Hu et al., 2014) directly copy an existing response from a training corpus when receiving a response request. Since the training corpus is usually collected from real-world conversations and possibly post-edited ∗ † Work done while DC was interning at Tencent AI Lab. Corresponding author. by a human, the retrieved responses are informative and grammatical. However, the performance of such systems drops when a given dialogue history is substantially different from those in the training corpus. The generative models (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), on the other hand, generate a new utterance from scratch. While those generative models have better generalization capacity in rare dialogue contexts, the generated responses tend to be universal and noninformative (e.g., “I don’t know”, “I think so” etc.) (Li et al., 2016a). It is partly due to the diversity of possible responses to a single query (i.e., the one-to-many problem). The dialogue query alone cannot decide a meaningful and specific response. Thus a well-trained model tends to generate the most frequent (safe but boring) responses instead. To summarize, IR-based models may give"
N19-1124,P18-1123,0,0.554103,"eters. The weight βw2 is obtained in a similar way with another set of parameters vD and WD . After acquiring the edit vector, we transform the prototype response r0 to a skeleton t by the follow( &lt; blank &gt; if m ˆ i = 0, φ(ri0 , hi , z) = , 0 ri else (3) where m ˆ i is the indicator and equals 0 if ri0 is replaced with a placeholder “&lt;blank&gt;” and 1 otherwise. The probability of m ˆ i = 1 is computed by P (m ˆ i = 1) = sigmoid(Wm [hi ⊕ z] + bm ). (4) 2.2 Response Generator The response generator can be implemented using most existing IR-augmented models (Song et al., 2016; Weston et al., 2018; Pandey et al., 2018), just by replacing the retrieved response input with the corresponding skeleton. We discuss our choices below. Encoders Two separate bidirectional LSTM (biLSTM) networks are used to obtain the distributed representations of the query memories and the skeleton memories, respectively. For biLSTM, 1221 the concatenation of the forward and the backward hidden states at each token position is considered a memory slot, producing two memory pools: Mq = {h1 , h2 , . . . , h|q |} for the input query, and Mt = {h01 , h02 , . . . , h0|t |} for the skeleton.1 Decoder During the generation process, our de"
N19-1124,P17-2079,0,0.0121808,"l., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re-rank the output from both models. However, the performance of such models is limited by the capacity of individual methods. Most related to our work, Song et al. (2016); Weston et al. (2018) and Wu et al. (2019) encoded the retrieved result into distributed representation and used it as the additional conditionals along with the standard query representation. While the former two only used the target side of the retrieved pairs, the latter took advantages of both sides. In a closed domain conversation setting, Pandey et al. (2018) further proposed to weight different training inst"
N19-1124,P15-1152,0,0.176413,"Missing"
N19-1124,N15-1020,0,0.0309983,"following objective is maximized: log D(r|q, rˆ, r, r) = log exp(hr T MD hq ) , P exp(hx T MD hq ) x∈{ˆ r ,r,r} (8) where hx is a vector representation of x, produced by a bidirectional LSTM (the last hidden state), and MD is a trainable matrix.2 4 Related Work Multi-source Dialogue Generation Chit-chat style dialogue system dates back to ELIZA (Weizenbaum, 1966). Early work uses handcrafted rules, while modern systems usually use data-driven approaches, e.g., information retrieval techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker"
N19-1124,P16-1094,0,0.0661705,"Missing"
N19-1124,D15-1166,0,0.129849,"cts a response skeleton. Lower: The response generator generates a response based on both the skeleton and the query. word embeddings to get the dense representations of I and D. The edit vector is computed as: X X z= αw1 Φ(w1 ) ⊕ βw2 Φ(w2 ), (1) w1 ∈I ing equations: 0 t = (φ(r10 , h1 , z), φ(r20 , h2 , z), · · · , φ(r|r 0 |, h|r 0 |, z)), w2 ∈D where ⊕ is the concatenation operation. Φ maps a word to its corresponding embedding vector, αw1 and βw2 are the weights of an insertion word w1 and a deletion word w2 respectively. The weights of different words are derived by an attention mechanism (Luong et al., 2015). Formally, the 0 ) is proretrieved response r0 = (r10 , r20 . . . , r|r 0| cessed by a bidirectional GRU network (biGRU). We denote the states of the biGRU (i.e. concatenation of forward and backward GRU states) as (h1 , h2 , . . . , h|r0 |). The weight αw1 is calculated by: exp(sw1 ) αw1 = P , w∈I exp(sw ) sw1 = vI&gt; tanh(WI [Φ(w1 ) ⊕ h|r0 |]), (2) where vI and WI are learnable parameters. The weight βw2 is obtained in a similar way with another set of parameters vD and WD . After acquiring the edit vector, we transform the prototype response r0 to a skeleton t by the follow( &lt; blank &gt; if m ˆ"
N19-1124,W18-5713,0,0.391089,"uery (i.e., the one-to-many problem). The dialogue query alone cannot decide a meaningful and specific response. Thus a well-trained model tends to generate the most frequent (safe but boring) responses instead. To summarize, IR-based models may give informative but inappropriate responses while generative models often do the opposite. It is desirable to combine both merits. Song et al. (2016) used an extra encoder for the retrieved response. The resulted dense representation, together with the original query, is used to feed the decoder in a standard S EQ 2S EQ model (Bahdanau et al., 2014). Weston et al. (2018) used a single encoder that takes the concatenation of the original query and the retrieved as input. Wu et al. (2019) noted that the retrieved information should be used in awareness of the context difference, and further proposed to construct an edit vector by explicitly encoding the lexical differences between the input query and the retrieved query. However, in our preliminary experiments, we found that the IR-guided models are inclined to degenerate into a copy mechanism, in which the generative models simply repeat the retrieved response without necessary modifications. Sharp performance"
N19-1124,P18-1101,0,0.0275646,"nformation retrieval techniques. Recently, end-to-end neural approaches (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et a"
N19-1124,P17-1061,0,0.0360971,"2015; Serban et al., 2016; Li et al., 2016a; Sordoni et al., 2015) have attracted increasing interest. For those generative models, a notorious problem is the “safe response” problem: the generated responses are dull and generic, which may attribute to the lack of sufficient input information. The query alone cannot specify an informative response. To mitigate the issue, many research efforts have been paid to introducing other information source, such as unsupervised latent variable (Serban et al., 2017; Zhao et al., 2018; Cao and Clark, 2017; Shen et al., 2017), discourse-level variations (Zhao et al., 2017), topic information (Xing et al., 2017), speaker personality (Li et al., 2016b) and knowl2 Note the classifier could be fine-tuned with the training of our generators, which falls into the adversarial learning setting (Goodfellow et al., 2014). edge base (Ghazvininejad et al., 2018; Zhou et al., 2018). Our work follows the similar motivation and uses the output of IR systems as the additional knowledge source. Combination of IR and Generative models To combine IR and generative models, early work (Qiu et al., 2017) tried to re-rank the output from both models. However, the performance of such"
N19-1164,D18-1439,0,0.0851511,"topic words as hashtags (Krestel et al., 2009; Ding et al., 2012; Godin et al., 2013; Gong et al., 2015; Zhang et al., 2016). However, these models are usually unable to produce phrase-level hashtags, which can be achieved by ours via generating hashtag word sequences with a decoder. Our work is also closely related to neural language generation, where the encoder-decoder framework (Sutskever et al., 2014) acts as a springboard for many sequence generation models. In particular, we are inspired by the keyphrase generation studies for scientific articles (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018, 2019), incorporating word extraction and generation using a seq2seq model with copy mechanism. However, our hashtag generation task is inherently different from theirs. As we can see from Table 4, it is suboptimal to directly apply keyphrase generation models on our data. The reason mostly lies in the informal language style of microblog users in writing both target posts and their hashtags. To adapt our model on microblog data, we explore the effects of conversation contexts on hashtag generation, which has never been studied in any prior work before. 6 Conclusion We have presented a novel"
N19-1164,C10-2028,0,0.141498,"Missing"
N19-1164,C12-2027,0,0.156152,"ation. Some prior work extracts phrases from target posts with sequence tagging models (Zhang et al., 2016, 2018). Another popular approach is to apply classifiers and select hashtags from a candidate list (Heymann et al., 2008; Weston et al., 2014; Sedhai and Sun, 2014; Gong and Zhang, 2016; 1631 Huang et al., 2016; Zhang et al., 2017). Unlike them, we generate hashtags with a language generation framework, where hashtags in neither the target posts nor the pre-defined candidate list can be created. Topic models are also widely applied to induce topic words as hashtags (Krestel et al., 2009; Ding et al., 2012; Godin et al., 2013; Gong et al., 2015; Zhang et al., 2016). However, these models are usually unable to produce phrase-level hashtags, which can be achieved by ours via generating hashtag word sequences with a decoder. Our work is also closely related to neural language generation, where the encoder-decoder framework (Sutskever et al., 2014) acts as a springboard for many sequence generation models. In particular, we are inspired by the keyphrase generation studies for scientific articles (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018, 2019), incorporating word extraction and gener"
N19-1164,D15-1046,0,0.254688,"on for Computational Linguistics defined list (Gong and Zhang, 2016; Huang et al., 2016; Zhang et al., 2017). However, hashtags usually appear in neither the target posts nor the given candidate list. The reasons are two folds. For one thing, microblogs allow large freedom for users to write whatever hashtags they like. For another, due to the wide range and rapid change of social media topics, a vast variety of hashtags can be daily created, making it impossible to be covered by a fixed candidate list. Prior research from another line employs topic models to generate topic words as hashtags (Gong et al., 2015; Zhang et al., 2016). These methods, ascribed to the limitation of most topic models, are nevertheless incapable of producing phrase-level hashtags. In this paper, we approach hashtag annotation from a novel sequence generation framework. In doing so, we enable phrase-level hashtags beyond the target posts or the given candidates to be created. Here, hashtags are first considered as a sequence of tokens (e.g., “#DeepLearning” as “deep learning”). Then, built upon the success of sequence to sequence (seq2seq) model on language generation (Sutskever et al., 2014), we present a neural seq2seq mo"
N19-1164,P16-1154,0,0.0396051,"are with the state-ofthe-art model based on classification (Gong and Zhang, 2016), where hashtags are selected from candidates seen in training data. Here two versions of their classifier are considered, one only taking a target post as input (henceforth C LASSIFIER (post only)) and the other taking the concatenation of a target post and its conversation as input (henceforth C LASSIFIER (post+conv)). • G ENERATOR: A seq2seq generator (henceforth S EQ 2S EQ) (Sutskever et al., 2014) is applied to generate hashtags given a target post. We also consider its variant augmented with copy mechanism (Gu et al., 2016) (henceforth S EQ 2S EQ COPY ), which has proven effective in keyphrase generation (Meng et al., 2017) and also takes the post as input. The proposed seq2seq with the biattention to encode both the post and its conversation is denoted as O UR MODEL for simplicity. Model Settings. We conduct model tunings on the development set based on grid search, where the hyper-parameters that give the lowest objective loss are selected. For the sequence generation models, the implementations are based on the OpenNMT framework (Klein et al., 2017). The word embeddings, with dimension set to 200, are randoml"
N19-1164,P17-1054,0,0.501272,"em. An attentive decoder generates the hashtag sequence. Input and Output. Formally, given a target post xp formulated as word sequence hxp1 , xp2 , ..., xp|xp |i and its conversation context xc formulated as word sequence hxc1 , xc2 , ..., xc|xc |i, where |xp |and |xc | denote the number of words in the input target post and its conversation, respectively, our goal is to output a hashtag y represented by a word sequence hy1 , y2 , ..., y|y |i. For training instances tagged with multiple gold-standard hashtags, we copy the instances multiple times, each with one goldstandard hashtag following Meng et al. (2017). All the input target posts, their conversations, and the hashtags share the same vocabulary V . Dual Encoder. To capture representations from both target posts and conversation contexts, we design a dual encoder, composed of a post encoder and a conversation encoder, each taking the xp and xc as input, respectively. For the post encoder, we use a bidirectional gated recurrent unit (Bi-GRU) (Cho et al., 2014) to encode the target post xp , where its embeddings e(xp ) are mapped into hidden states hp = − → ← − hhp1 , hp2 , ..., hp|xp |i. Specifically, hpi = [hpi ; hpi ] is − → the concatenatio"
N19-1164,C16-1090,0,0.0295674,"gs cannot reflect any content information from target posts and should be distinguished from topic hashtags in the future. 5 Related Work Our work mainly builds on two streams of previous work — microblog hashtag annotation and neural language generation. We are in the line of microblog hashtag annotation. Some prior work extracts phrases from target posts with sequence tagging models (Zhang et al., 2016, 2018). Another popular approach is to apply classifiers and select hashtags from a candidate list (Heymann et al., 2008; Weston et al., 2014; Sedhai and Sun, 2014; Gong and Zhang, 2016; 1631 Huang et al., 2016; Zhang et al., 2017). Unlike them, we generate hashtags with a language generation framework, where hashtags in neither the target posts nor the pre-defined candidate list can be created. Topic models are also widely applied to induce topic words as hashtags (Krestel et al., 2009; Ding et al., 2012; Godin et al., 2013; Gong et al., 2015; Zhang et al., 2016). However, these models are usually unable to produce phrase-level hashtags, which can be achieved by ours via generating hashtag word sequences with a decoder. Our work is also closely related to neural language generation, where the encod"
N19-1164,P17-4012,0,0.0400123,"st. We also consider its variant augmented with copy mechanism (Gu et al., 2016) (henceforth S EQ 2S EQ COPY ), which has proven effective in keyphrase generation (Meng et al., 2017) and also takes the post as input. The proposed seq2seq with the biattention to encode both the post and its conversation is denoted as O UR MODEL for simplicity. Model Settings. We conduct model tunings on the development set based on grid search, where the hyper-parameters that give the lowest objective loss are selected. For the sequence generation models, the implementations are based on the OpenNMT framework (Klein et al., 2017). The word embeddings, with dimension set to 200, are randomly initialized. For encoders, we employ two layers of Bi-GRU cells, and for decoders, one layer of GRU cell is used. The hidden size of all GRUs is set to 300. In learning, we use the Adam optimizer (Kingma and Ba, 2014) with the learning rate initialized to 0.001. We adopt the earlystop strategy: the learning rate decreases by a decay rate of 0.5 till either it is below 1e−6 or the validation loss stops decreasing. The norm of gradients is rescaled to 1 if the L2-norm &gt; 1 is observed. The dropout rate is 0.1 and the batch size is 64."
N19-1164,D15-1259,1,0.879278,"ven the short post, it is difficult to understand why it is tagged with “#AusOpen”, not to mention that neither “aus” nor “open” appear in the target post. In such a situation, how shall we generate hashtags for a post with limited words? To address the data sparsity challenge, we exploit conversations initiated by the target posts to enrich their contexts. Our approach is benefited from the nature that most messages in a conversation tend to focus on relevant topics. Content in conversations might hence provide contexts facilitating the understanding of the original post (Chang et al., 2013; Li et al., 2015). The effects of conversation contexts, useful on topic 2 For instance, the eligible length of a post on Twitter or Weibo is up to 140 characters. modeling (Li et al., 2016, 2018) and keyphrase extraction (Zhang et al., 2018), have never been explored on microblog hashtag generation. To show why conversation contexts are useful, we display in Table 1 a conversation snippet formed by some replies of the sample target post. As can be seen, key content words in the conversation (e.g., “Nadal”, “Tomic”, and “tennis”) are useful to reflect the relevance of the target post to the hashtag “#AusOpen”,"
N19-1164,P16-1199,1,0.884776,"Missing"
N19-1164,J18-4008,1,0.810296,"mean average precision (MAP) scores (Manning et al., 2008) are reported. Here, different K values are tested on F1@K and result in a similar trend, so only F1@1 and F1@5 are reported. MAP scores are also computed given the top 5 outputs. Besides, as we consider a hashtag as a sequence of words, ROUGE metrics for summarization evaluation (Lin, 2004) are also adopted. Here, we use ROUGE F1 for the top-ranked hashtag prediction computed by an open source toolkit pythonrouge,9 with Porter stemmer used for English tweets. For Weibo posts, scores calculated at the Chinese character level following Li et al. (2018). We report the average scores for multiple gold-standard hashtags on ROUGE evaluation. 4 Experimental Results In this section, we first report the main comparison results in Section 4.1, followed by an in-depth comparative study between classification and sequence generation models in Section 4.2. Further discussions are then presented to analyze our superiority and errors in Section 4.3. 4.1 Main Comparison Results Table 4 reports the main comparison results. For C LASSIFIER, their outputs are ranked according to the logits after a sof tmax layer. For E XTRAC TOR , it is unable to produce ra"
N19-1164,W04-1013,0,0.00885896,"and RG-4 refer to ROUGE-1 and ROUGE-SU4 respectively. The best results in each column are in bold. The “*” after numbers indicates significantly better results than all the other models (p < 0.05, paired t-test). Higher values indicate better performance. and mean average precision (MAP) scores (Manning et al., 2008) are reported. Here, different K values are tested on F1@K and result in a similar trend, so only F1@1 and F1@5 are reported. MAP scores are also computed given the top 5 outputs. Besides, as we consider a hashtag as a sequence of words, ROUGE metrics for summarization evaluation (Lin, 2004) are also adopted. Here, we use ROUGE F1 for the top-ranked hashtag prediction computed by an open source toolkit pythonrouge,9 with Porter stemmer used for English tweets. For Weibo posts, scores calculated at the Chinese character level following Li et al. (2018). We report the average scores for multiple gold-standard hashtags on ROUGE evaluation. 4 Experimental Results In this section, we first report the main comparison results in Section 4.1, followed by an in-depth comparative study between classification and sequence generation models in Section 4.2. Further discussions are then presen"
N19-1164,D14-1194,0,0.372336,"ndicating the messages forwarded from Facebook). Such non-topic hashtags cannot reflect any content information from target posts and should be distinguished from topic hashtags in the future. 5 Related Work Our work mainly builds on two streams of previous work — microblog hashtag annotation and neural language generation. We are in the line of microblog hashtag annotation. Some prior work extracts phrases from target posts with sequence tagging models (Zhang et al., 2016, 2018). Another popular approach is to apply classifiers and select hashtags from a candidate list (Heymann et al., 2008; Weston et al., 2014; Sedhai and Sun, 2014; Gong and Zhang, 2016; 1631 Huang et al., 2016; Zhang et al., 2017). Unlike them, we generate hashtags with a language generation framework, where hashtags in neither the target posts nor the pre-defined candidate list can be created. Topic models are also widely applied to induce topic words as hashtags (Krestel et al., 2009; Ding et al., 2012; Godin et al., 2013; Gong et al., 2015; Zhang et al., 2016). However, these models are usually unable to produce phrase-level hashtags, which can be achieved by ours via generating hashtag word sequences with a decoder. Our work i"
N19-1164,D18-1447,0,0.0483605,"y applied to induce topic words as hashtags (Krestel et al., 2009; Ding et al., 2012; Godin et al., 2013; Gong et al., 2015; Zhang et al., 2016). However, these models are usually unable to produce phrase-level hashtags, which can be achieved by ours via generating hashtag word sequences with a decoder. Our work is also closely related to neural language generation, where the encoder-decoder framework (Sutskever et al., 2014) acts as a springboard for many sequence generation models. In particular, we are inspired by the keyphrase generation studies for scientific articles (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018, 2019), incorporating word extraction and generation using a seq2seq model with copy mechanism. However, our hashtag generation task is inherently different from theirs. As we can see from Table 4, it is suboptimal to directly apply keyphrase generation models on our data. The reason mostly lies in the informal language style of microblog users in writing both target posts and their hashtags. To adapt our model on microblog data, we explore the effects of conversation contexts on hashtag generation, which has never been studied in any prior work before. 6 Conclusion We have"
N19-1164,D16-1080,0,0.472255,"l Linguistics defined list (Gong and Zhang, 2016; Huang et al., 2016; Zhang et al., 2017). However, hashtags usually appear in neither the target posts nor the given candidate list. The reasons are two folds. For one thing, microblogs allow large freedom for users to write whatever hashtags they like. For another, due to the wide range and rapid change of social media topics, a vast variety of hashtags can be daily created, making it impossible to be covered by a fixed candidate list. Prior research from another line employs topic models to generate topic words as hashtags (Gong et al., 2015; Zhang et al., 2016). These methods, ascribed to the limitation of most topic models, are nevertheless incapable of producing phrase-level hashtags. In this paper, we approach hashtag annotation from a novel sequence generation framework. In doing so, we enable phrase-level hashtags beyond the target posts or the given candidates to be created. Here, hashtags are first considered as a sequence of tokens (e.g., “#DeepLearning” as “deep learning”). Then, built upon the success of sequence to sequence (seq2seq) model on language generation (Sutskever et al., 2014), we present a neural seq2seq model to generate hasht"
P09-1052,D07-1109,0,0.126392,"Missing"
P09-1052,D07-1108,0,0.0610467,"Missing"
P09-1052,P90-1034,0,0.345715,"Missing"
P09-1052,P98-2127,0,0.0907353,"Missing"
P09-1052,N04-1010,0,0.213023,"Missing"
P09-1052,P08-1119,0,0.165764,"Missing"
P09-1052,S07-1053,0,\N,Missing
P09-1052,C98-2122,0,\N,Missing
P11-1116,N09-1003,0,0.275482,"to have many supporting sentences of different types. This is a big challenge for rare terms, due to their low frequency in sentences (and even lower frequency in supporting sentences because not all occurrences can be covered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propagation is motivated by the following two observations: (I) Similar entities or coordinate terms tend to share some common hypernyms. (II) Large term similarity graphs are able to be built efficiently with state-of-the-art techniques (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). With the graphs, we can obtain the similarity between two terms without their hypernyms being available. The first observation motivates us to “borrow” the supporting sentences from other terms as auxiliary evidence of the term. The second observation means that new information is brought with the state-of-the-art term similarity graphs (in addition to the term-label information discovered with the patterns of Table 1). Our evidence propagation algorithm contains two phases. In phase I, some pseudo supporting sentences are constructed for a term from t"
P11-1116,C92-2082,0,0.405589,"gies adopted in our semantic search and mining system NeedleSeek2. In the next section, we discuss major related efforts and how they differ from our work. Section 3 is a brief description of the baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.micro"
P11-1116,P08-1119,0,0.398193,"we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 The probabilistic evidence combination model that we exploit here was first proposed in (Shi et al., 2009), for combining the page in-link evidence in building a nonlinear static-rank computation algorith"
P11-1116,N04-1041,0,0.502779,"in our semantic search and mining system NeedleSeek2. In the next section, we discuss major related efforts and how they differ from our work. Section 3 is a brief description of the baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needl"
P11-1116,C10-2110,0,0.0331854,"s illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 The probabilistic evidence combination model that we exploit here was first proposed in (Shi et al., 2009), for combining the page in-link evidence in building a nonlinear static-rank computation algorithm. We applied it to the hyponymy extraction proble"
P11-1116,C10-1112,1,0.770237,"erent types. This is a big challenge for rare terms, due to their low frequency in sentences (and even lower frequency in supporting sentences because not all occurrences can be covered by patterns). With evidence propagation, we aim at discovering more supporting sentences for terms (especially rare terms). Evidence propagation is motivated by the following two observations: (I) Similar entities or coordinate terms tend to share some common hypernyms. (II) Large term similarity graphs are able to be built efficiently with state-of-the-art techniques (Agirre et al., 2009; Pantel et al., 2009; Shi et al., 2010). With the graphs, we can obtain the similarity between two terms without their hypernyms being available. The first observation motivates us to “borrow” the supporting sentences from other terms as auxiliary evidence of the term. The second observation means that new information is brought with the state-of-the-art term similarity graphs (in addition to the term-label information discovered with the patterns of Table 1). Our evidence propagation algorithm contains two phases. In phase I, some pseudo supporting sentences are constructed for a term from the supporting sentences of its neighbors"
P11-1116,N04-1010,0,0.023712,"baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1160 The probabilistic evidence combination model that we exploit here was first proposed in (Shi et al., 2009), for combining the page in-link e"
P11-1116,P06-1101,0,0.0385374,"eek2. In the next section, we discuss major related efforts and how they differ from our work. Section 3 is a brief description of the baseline approach. The probabilistic evidence combination model that we exploited is introduced in Section 4. Our main approach is illustrated in Section 5. Section 6 shows our experimental settings and results. Finally, Section 7 concludes this paper. 2 Related Work Existing efforts for hyponymy relation extraction have been conducted upon various types of data sources, including plain-text corpora (Hearst 1992; Pantel & Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Banko, et al., 2007; Durme & Pasca, 2008; Talukdar et al., 2008), semistructured web pages (Cafarella et al., 2008; Shinzato & Torisawa, 2004), web search results (Geraci et al., 2006; Kozareva et al., 2008; Wang & Cohen, 2009), and query logs (Pasca 2010). Our target for optimization in this paper is the approaches that use lexico-syntactic patterns to extract hyponymy relations from plain-text corpora. Our future work will study the application of the proposed algorithms on other types of approaches. 2 http://research.microsoft.com/en-us/projects/needleseek/ or http://needleseek.msra.cn/ 1"
P11-1116,P10-1149,0,0.0113897,"f instance-label pairs with a TF*IDF like method, by exploiting clusters of semantically related phrases. The core idea is to keep a term-label pair (T, L) only if the number of terms having the label L in the term T’s cluster is above a threshold and if L is not the label of too many clusters (otherwise the pair will be discarded). In contrast, we are able to add new (high-quality) labels for a term with our evidence propagation method. On the other hand, low quality labels get smaller score gains via propagation and are ranked lower. Label propagation is performed in (Talukdar et al., 2008; Talukdar & Pereira, 2010) based on multiple instance-label graphs. Term similarity information was not used in their approach. Most existing work tends to utilize small-scale or private corpora, whereas the corpus that we used is publicly available and much larger than most of the existing work. We published our term sets (refer to Section 6.1) and their corresponding user judgments so researchers working on similar topics can reproduce our results. Type Hearst-I Hearst-II Hearst-III IsA-I IsA-II IsA-III Pattern NPL {,} (such as) {NP,}* {and|or} NP NPL {,} (include(s) |including) {NP,}* {and|or} NP NPL {,} (e.g.|e.g)"
P11-1116,D08-1061,0,0.494774,"erformance improvement in terms of P@5, MAP and R-Precision. 1 Introduction 1 An important task in text mining is the automatic extraction of entities and their lexical relations; this has wide applications in natural language processing and web search. This paper focuses on mining the hyponymy (or is-a) relation from largescale, open-domain web documents. From the viewpoint of entity classification, the problem is to automatically assign fine-grained class labels to terms. There have been a number of approaches (Hearst 1992; Pantel & Ravichandran 2004; Snow et al., 2005; Durme & Pasca, 2008; Talukdar et al., 2008) to address the problem. These methods typically exploited manually-designed or automatical* This work was performed when Fan Zhang and Shuqi Sun were interns at Microsoft Research Asia ly-learned patterns (e.g., “NP such as NP”, “NP like NP”, “NP is a NP”). Although some degree of success has been achieved with these efforts, the results are still far from perfect, in terms of both recall and precision. As will be demonstrated in this paper, even by processing a large corpus of 500 million web pages with the most popular patterns, we are not able to extract correct labels for many (especially"
P11-1116,P09-1050,0,0.220088,"Missing"
P11-1116,P09-1052,1,0.869968,"approach, a term is represented by a feature vector, with each feature corresponding to a context in which the term appears. The similarity between two terms is computed as the similarity between their corresponding feature vectors. In PB approaches, a list of carefully-designed (or automatically learned) patterns is exploited and applied to a text collection, with the hypothesis that the terms extracted by applying each of the patterns to a specific piece of text tend to be similar. Two categories of patterns have been studied in the literature (Heast 1992; Pasca 2004; Kozareva et al., 2008; Zhang et al., 2009): sentence lexical patterns, and HTML tag patterns. An example of sentence lexical patterns is “T {, T}*{,} (and|or) T”. HTML tag patterns include HTML tables, drop-down lists, and other tag repeat patterns. In this paper, we generate the DS and PB graphs by adopting the best-performed methods studied in (Shi et al., 2010). We will compare, by experiments, the propagation performance of utilizing the two categories 1164 of graphs, and also investigate the performance of utilizing both graphs for evidence propagation. 6 6.1 Experiments Experimental setup Corpus We adopt a publicly available dat"
P11-1116,D09-1098,0,\N,Missing
P16-1084,D15-1202,0,0.572838,"s human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an intern at Microsoft Research. 887 Proceedings of the 54th Annual Meeting of the Association for Comp"
P16-1084,D15-1171,0,0.0327625,"structured answer text. We then conduct experiments to test the performance of some recent math problem solving systems on the dataset. We make the following main observations, Statistical machine learning methods have been proposed to solve math word problems since 2014. Hosseini et al. (2014) solve single step or multistep homogeneous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) and Zhou et al. (2015) solve a wide range of algebra word problems, given that systems of linear equations are attached to problems in the training set. Seo et al. (2015) focuses on SAT geometry questions with text and diagram provided. Koncel-Kedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. No empirical evaluation results are reported in most early publications on this topic. Although promising empirical results are reported in recent work, the datasets employed in their evaluation are small and lack diversity. For example, the Alg514 dataset used in Kushman et al. (2014) and Zhou et al. (2015) only contains 514 problems of 28 types. Please refer to Section 3.4 for more details about the d"
P16-1084,P08-1081,1,0.138135,"we need to develop algorithms which can utilize data more effectively. Our experiments indicate that the problem of automatic math word problem solving is still far from being solved. Good results obtained on small datasets may not be good indicators of high performance on larger and diverse datasets. For current methods, simply adding more training data is not an effective way to improve performance. New methodologies are required for this topic. 2 2.2 Our work on automatic answer and equation extraction is related to the recent CQA extraction work (Agichtein et al., 2008; Cong et al., 2008; Ding et al., 2008). Most of them aim to discover high-quality (question, answer text) pairs from CQA posts. We are different because we extract structured data (i.e., numbers and equation systems) inside the pieces of answer text. Related Work 2.1 Math Word Problem Solving Previous work on automatic math word problem solving falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a; Bobrow, 1964b; Charniak, 1968; Charniak, 1969; Bakman, 2007; Liguda and Pfeiffer, 2012; Shi et al., 2015), math problem sentences are transformed to certain structures (us"
P16-1084,D15-1135,1,0.692008,"the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an intern at Microsoft Research. 887 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 887–896, c Berlin,"
P16-1084,D14-1058,0,0.191765,"ers from the answer text provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an intern at Microsoft Research"
P16-1084,D15-1096,0,0.493986,"Missing"
P16-1084,N16-1136,0,0.386408,"diagram provided. Koncel-Kedziorski et al. (2015) and Roy and Roth (2015) target math problems that can be solved by one single linear equation. No empirical evaluation results are reported in most early publications on this topic. Although promising empirical results are reported in recent work, the datasets employed in their evaluation are small and lack diversity. For example, the Alg514 dataset used in Kushman et al. (2014) and Zhou et al. (2015) only contains 514 problems of 28 types. Please refer to Section 3.4 for more details about the datasets. Recently, a framework was presented in Koncel-Kedziorsk et al. (2016) for building an online repository of math word problems. The framework is initialized by including previous public available datasets. The largest dataset among them contains 1,155 problems. 1. All systems evaluated on the Dolphin18K dataset perform much worse than on their original small and less diverse datasets. 2. On the large dataset, a simple similaritybased method performs as well as more sophisticated statistical learning approaches. 3. System performance improves sub-linearly as more training data is used. This suggests that we need to develop algorithms which can utilize data more e"
P16-1084,Q15-1042,0,0.765136,"t provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an intern at Microsoft Research. 887 Proceedings of the 54th An"
P16-1084,P14-1026,0,0.149841,"y extract problem answers from the answer text provided by CQA users, which significantly reduces human annotation cost. Experiments conducted on the new dataset lead to interesting and surprising results. 1 Introduction Designing computer systems for automatically solving math word problems is a challenging research topic that dates back to the 1960s (Bobrow, 1964a; Briars and Larkin, 1984; Fletcher, 1985). As early proposals seldom report empirical evaluation results, it is unclear how well they perform. Recently, promising results have been reported on both statistical learning approaches (Kushman et al., 2014; Hosseini et al., 2014; Koncel-Kedziorski et al., 2015; Zhou et al., 2015; Roy and Roth, 2015) and semantic parsing methods (Shi et al., 2015). However, we observe two limitations on the datasets used by these previous works. First, the datasets are small. The most frequently used dataset (referred to as Alg514 hereafter) only contains 514 algebra problems. The Dolphin1878 1 According to our experience, the speed is about 10-15 problems per hour for a person with good math skills. 2 Available from http://research.microsoft.com/enus/projects/dolphin/. ∗ Work done while this author was an inter"
P18-1222,councill-etal-2008-parscit,0,0.104443,"Micro Macro Micro Model DeepWalk - 66.57 76.56 66.57 76.56 w2v (I) w2v (I+O) ×/× ×/× 19.77 15.97 47.32 45.66 59.80 50.77 72.90 70.08 d2v-nc d2v-cac h-d2v (I) h-d2v (I+O) /  /  /  /  61.54 65.23 58.59 66.64 73.73 75.93 69.79 75.19 69.37 70.43 66.99 68.96 78.22 78.75 75.63 76.61 Table 5: F1 on DBLP when newcomers are discarded. Datasets and Experimental Settings We use three datasets from the academic paper domain, i.e., NIPS4 , ACL anthology5 and DBLP6 , as shown in Table 3. They all contain full text of papers, and are of small, medium, and large size, respectively. We apply ParsCit7 (Councill et al., 2008) to parse the citations and bibliography sections. Each identiﬁed citation string referring to a paper in the same dataset, e.g., [1] or (Author et al., 2018), is replaced by a global paper id. Consecutive citations like [1, 2] are regarded as multiple ground truths occupying one position. Following He et al. (2010), we take 50 words before and after a citation as the citation context. ˇ uˇrek and Sojka, 2010) is used to Gensim (Reh˚ implement all w2v and d2v baselines as well as h-d2v. We use cbow for w2v and pv-dbow for d2v, unless otherwise noted. For all three baselines, we set the (half)"
P18-1222,D07-1074,0,0.128488,"biquitous World Wide Web has boosted research interests on hypertext documents, e.g., personal webpages (Lu and Getoor, 2003), Wikipedia pages (Gabrilovich and Markovitch, 2007), as well as academic papers (Sugiyama and Kan, 2010). Unlike independent plain documents, a hypertext document (hyper-doc for short) links to another hyper-doc by a hyperlink or citation mark in its textual content. Given this essential distinction, hyperlinks or citations are worth speciﬁc modeling in many tasks such as link-based classiﬁcation (Lu and Getoor, 2003), web retrieval (Page et al., 1999), entity linking (Cucerzan, 2007), and citation recommendation (He et al., 2010). To model hypertext documents, various efforts (Cohn and Hofmann, 2000; Kataria et al., 2010; Perozzi et al., 2014; Zwicklbauer et al., 2016; Wang et al., 2016) have been made to depict networks of hyper-docs as well as their content. Among potential techniques, distributed representation (Mikolov et al., 2013; Le and Mikolov, 2014) tends to be promising since its validity and effectiveness are proven for plain documents on many natural language processing (NLP) tasks. Conventional attempts on utilizing embedding techniques in hyper-doc-related t"
P18-1222,K16-1026,0,0.019312,"task. Huang et al. (2015b) propose Neural Probabilistic Model (NPM) to tackle this problem with embeddings. Their model outperforms non-embedding ones (Kataria et al., 2010; Tang and Zhang, 2009; Huang et al., 2012). Ebesu and Fang (2017) also exploit neural networks for citation recommendation, but require author information as additional input. Compared with h-d2v, these models are limited in a task-speciﬁc setting. Embedding-based entity linking is another topic that exploits embeddings to model certain hyperdocs, i.e., Wikipedia (Huang et al., 2015a; Yamada et al., 2016; Sun et al., 2015; Fang et al., 2016; He et al., 2013; Zwicklbauer et al., 2016), for entity linking (Shen et al., 2015). It resembles citation recommendation in the sense that linked entities highly depend on the contexts. Meanwhile, it requires extra steps like candidate generation, and can beneﬁt from sophisticated techniques such as collective linking (Cucerzan, 2007). 3 Preliminaries We introduce notations and deﬁnitions, then formally deﬁne the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness. 3.1 Notations and Deﬁnitions Let w ∈ W be a word fro"
P18-1222,N15-1184,0,0.0318277,"∈D exp(x d ) (3) To model contents’ impact on document vectors, we simply consider an additional objective function that is identical to pv-dm, i.e., enumerate words and contexts, and use the same input architecture as Figure 2 to predict the OUT vector of the current word. Such convenience owes to the fact that using two vectors makes the model parameters compatible with those of pv-dm. Note that combining the citation and content objectives leads to a joint learning framework. To facilitate easier and faster training, we adopt an alternative pre-training/ﬁne-tuning or retroﬁtting framework (Faruqui et al., 2015). We initialize with a predeﬁned number of pv-dm iterations, and then optimize Eq. 1 based on the initialization. Finally, similar to w2v (Mikolov et al., 2013) and d2v (Le and Mikolov, 2014), to make training efﬁcient, we adopt negative sampling: log σ(x dO t )+ n  Edi ∼PN (d) log σ(−x dO i ) i=1 (4) and use it to replace every log P (dt |ds , C). Following Huang et al. (2015b), we adopt a uniform distribution on D as the distribution PN (d). Unlike the other models in Table 1, h-d2v satisﬁes all four criteria. We refer to the example in Figure 2 to make the points clear. First, when optim"
P18-1222,P13-2006,0,0.0268736,"(2015b) propose Neural Probabilistic Model (NPM) to tackle this problem with embeddings. Their model outperforms non-embedding ones (Kataria et al., 2010; Tang and Zhang, 2009; Huang et al., 2012). Ebesu and Fang (2017) also exploit neural networks for citation recommendation, but require author information as additional input. Compared with h-d2v, these models are limited in a task-speciﬁc setting. Embedding-based entity linking is another topic that exploits embeddings to model certain hyperdocs, i.e., Wikipedia (Huang et al., 2015a; Yamada et al., 2016; Sun et al., 2015; Fang et al., 2016; He et al., 2013; Zwicklbauer et al., 2016), for entity linking (Shen et al., 2015). It resembles citation recommendation in the sense that linked entities highly depend on the contexts. Meanwhile, it requires extra steps like candidate generation, and can beneﬁt from sophisticated techniques such as collective linking (Cucerzan, 2007). 3 Preliminaries We introduce notations and deﬁnitions, then formally deﬁne the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness. 3.1 Notations and Deﬁnitions Let w ∈ W be a word from a vocabulary W"
P18-1222,P07-2045,0,0.0194057,"ally deﬁne the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness. 3.1 Notations and Deﬁnitions Let w ∈ W be a word from a vocabulary W , and d ∈ D be a document id (e.g., web page URLs and paper DOIs) from an id collection D. After ﬁltering out non-textual content, a hyper-document H is reorganized as a sequence of words and doc ids, 2385 Source doc ௦ (Zhao and Gildea, 2010) Context words ܥ … We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007) … Target (Papineni et al., 2002) doc ௧ (Koehn et al., 2007) … … Original (a) Hyper-documents. (Zhao and Gildea, 2010) …We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007)… … “Word” Vectors evaluate w2v BLEU … Citation as word (b) Citation as word. Word Vectors evaluate BLEU d2v … Doc Vectors (Zhao and (Koehn et al., 2007) Gildea, 2010) (Papineni et al., 2002) (Papineni et al., 2012) … …We also evaluate our model by computing the machine translation BLEU score using the Moses system … … machine translat"
P18-1222,P02-1040,0,0.119571,"e introduce notations and deﬁnitions, then formally deﬁne the embedding problem. We also propose four criteria for hyper-doc embedding models w.r.t their appropriateness and informativeness. 3.1 Notations and Deﬁnitions Let w ∈ W be a word from a vocabulary W , and d ∈ D be a document id (e.g., web page URLs and paper DOIs) from an id collection D. After ﬁltering out non-textual content, a hyper-document H is reorganized as a sequence of words and doc ids, 2385 Source doc ௦ (Zhao and Gildea, 2010) Context words ܥ … We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007) … Target (Papineni et al., 2002) doc ௧ (Koehn et al., 2007) … … Original (a) Hyper-documents. (Zhao and Gildea, 2010) …We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007)… … “Word” Vectors evaluate w2v BLEU … Citation as word (b) Citation as word. Word Vectors evaluate BLEU d2v … Doc Vectors (Zhao and (Koehn et al., 2007) Gildea, 2010) (Papineni et al., 2002) (Papineni et al., 2012) … …We also evaluate our model by computing the machine translation BLEU score"
P18-1222,K16-1025,0,0.0934266,") simply downcasts hyper-docs to plain documents and feeds them into word2vec (Mikolov et al., 2013) (w2v for short) or doc2vec (Le and Mikolov, 2014) (d2v for short). These approaches involve downgrading hyperlinks and inevitably omit certain information in hyper-docs. However, no previous work investigates the information loss, and how it affects the performance of such downcasting-based adaptations. The second type designs sophisticated embedding models to fulﬁll certain tasks, e.g., citation recommendation (Huang et al., 2015b), paper classiﬁcation (Wang et al., 2016), and entity linking (Yamada et al., 2016), etc. These models are limited to speciﬁc tasks, and it is yet unknown whether embeddings learned for those particular tasks can generalize to others. Based on the above facts, we are interested in two questions: • What information should hyper-doc embedding models preserve, and what nice property should they possess? • Is there a general approach to learning taskindependent embeddings of hyper-docs? To answer the two questions, we formalize the hyper-doc embedding task, and propose four criteria, i.e., content awareness, context awareness, newcomer friendliness, and context intent aware2384"
P18-1222,D10-1058,0,0.162956,"cilitate applications like hyper-doc classiﬁcation and citation recommendation. 3.3 are not referred to by any hyperlink in other hyper-docs. If such “newcomers” do not get embedded properly, downstream tasks involving them are infeasible or deteriorated. Criteria for Embedding Models A reasonable model should learn how contents and hyperlinks in hyper-docs impact both D and W. We propose the following criteria for models: • Content aware. Content words of a hyperdoc play the main role in describing it, so the document representation should depend on its own content. For example, the words in Zhao and Gildea (2010) should affect and contribute to its embedding. • Context aware. Hyperlink contexts usually provide a summary for the target document. Therefore, the target document’s vector should be impacted by words that others use to summarize it, e.g., paper Papineni et al. (2002) and the word “BLEU” in Figure 1(a). • Newcomer friendly. In a hyper-document network, it is inevitable that some documents We note that the ﬁrst three criteria are for hyperdocs, while the last one is desired for word vectors. 4 Representing Hypertext Documents In this section, we ﬁrst give the background of two prevailing tech"
P18-2025,W08-0312,0,0.0173426,"man correlation of BLEU-1, METEOR, and W-METEOR, showing that the BLEU-1 scores vary a lot given any fixed human score, appearing to be random noise, while the METEOR family exhibit strong consistency with human scores. Compared to W-METEOR, METEOR deviates from the regression line more frequently, esp. by assigning unexpectedly high scores to comments with low human grades. Notably, the best automatic metric, WMETEOR, achieves 0.59 Spearman and 0.57 Pearson, which is higher or comparable to automatic metrics in other generation tasks (Lowe et al., 2017; Liu et al., 2016; Sharma et al., 2017; Agarwal and Lavie, 2008), indicating a good supplement to human judgment for efficient evaluation and comparison. We use the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu"
P18-2025,P15-1034,0,0.0136403,"ments. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation o"
P18-2025,W05-0909,0,0.493806,"e articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M"
P18-2025,P00-1041,0,0.198504,"Missing"
P18-2025,W16-3605,0,0.288313,"explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small set of gold targets, human comments on an article live in"
P18-2025,J90-2002,0,0.751023,"ents. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small set of gold targets, human comments on an article live in much larger space by involving diverse topics and personal views, and critically, are of vary151 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 151–156 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Title: ˘úl¯iPhone 8 —⇤⇢ö(9 &gt;L (Apple’s iPhone 8 event is happening in Sept.) Score Criteria Content: ˘úl¯c✏⌘íS—⇤Ä˜ ˝ £⇤⌃é9 12ÂÏ ˘ú∞¡—⇤ ⇢ Âl¯⌃—⇤↵ „iPhone èKÙ∞ Ñÿ ˘úKh ˘úTV åiOSoˆ⇥Ÿ !—⇤⇢⌃&e &gt;∞iPhones⇢&OLED&gt; :O"
P18-2025,P15-2073,0,0.0206468,"ating comments, and develop a dataset that is orders-of-magnitude larger than previous related corpus. Instead of restricting to one or few specific aspects, we focus on the general comment quality aligned with human judgment, and provide over 27 gold references for each data instance to enable wide-coverage evaluation. Such setting also allows a large output space, and makes the task challenging and valuable for text generation research. Yao et al. (2017) explore defense approaches of spam or malicious reviews. We believe the proposed task and dataset can be potentially useful for the study. Galley et al. (2015) propose BLEU that weights multiple references for conversation generation evaluation. The quality weighted metrics developed in our work can be seen as a generalization of BLEU to many popular reference-based metrics (e.g., METEOR, ROUGE, and CIDEr). Our human survey demonstrates the effectiveness of the generalized metrics in the article commenting task. 3 Article Commenting Dataset The dataset is collected from Tencent News (news.qq.com), one of the most popular Chinese websites of news and opinion articles. Table 1 shows an example data instance in the dataset (For readability we also prov"
P18-2025,X98-1026,0,0.208952,"foster online communities. Besides, commenting on articles is one of the increasingly demanded skills of intelligent chatbot (Shum et al., 2018) to enable in-depth, content-rich conversations with humans. Article commenting poses new challenges for machines, as it involves multiple cognitive abil⇤ Work done while Lianhui interned at Tencent AI Lab The dataset is available on http://ai.tencent. com/upload/PapersUploads/article_ commenting.tgz 1 ities: understanding the given article, formulating opinions and arguments, and organizing natural language for expression. Compared to summarization (Hovy and Lin, 1998), a comment does not necessarily cover all salient ideas of the article; instead it is often desirable for a comment to carry additional information not explicitly presented in the articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annot"
P18-2025,D13-1020,0,0.0324806,"the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development Program of China (No. 2017Y"
P18-2025,W17-3002,0,0.139171,"nting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists of around 200K news articles and 4.5M human comments along with rich meta data for article categories and user votes of comments. Different from traditional text generation tasks such as machine translation (Brown et al., 1990) that has a relatively small"
P18-2025,W04-1013,0,0.0457846,"Missing"
P18-2025,D16-1230,0,0.116055,"ead articles using an encoder and generate comments using a decoder with or without attentions (Bahdanau et al., 2014), which are denoted as Seq2seq and Att if only article titles are read. We also set up an attentional sequence-tosequence model that reads full article title/content, and denote with Att-TC. Again, these approaches are mainly for demonstration purpose and for evaluating the metrics, and are far from solving the difficult commenting task. We discard comments with over 50 words and use a truncated vocabulary of size 30K. Results We follow previous setting (Papineni et al., 2002; Liu et al., 2016; Lowe et al., 2017) to evaluate the metrics, by conducting human evaluations and calculating the correlation between the scores assigned by humans and the metrics. Specifically, for each article in the test set, we obtained six comments, five of which come from IRT, IR-TC, Seq2seq, Att, and Att-TC, respectively, and one randomly drawn from real comments that are different from the reference comments. The comments were then graded by human annotators following the same procedure of test set scoring (sec.3). Meanwhile, we measure each comment with the vanilla and weighted automatic metrics base"
P18-2025,P17-1103,0,0.329605,"c be a generated comment to evaluate, R = {rj } the set of references, each of which has a quality score sj by human annotators. We assume properly normalized sj 2 [0, 1]. Due to space limitations, here we only present the enhanced METEOR, and defer the formulations of enhancing BLEU, ROUGE, and CIDEr to the supplements. Specifically, METEOR performs word matching through an alignment between the candidate and references. The weighted METEOR extends the Figure 1: Scatter plots showing the correlation between metrics and human judgments. Left: BLEU1; Middle: METEOR; Right: W-METEOR. Following (Lowe et al., 2017), we added Gaussian noise drawn from N (0, 0.05) to the integer human scores to better visualize the density of points. original metric by weighting references with sj : W-METEOR(c, R) = (1 BP ) maxj sj Fmean,j , (1) where Fmean,j is a harmonic mean of the precision and recall between c and rj , and BP is the penalty (Banerjee and Lavie, 2005). Note that the new metrics fall back to the respective original metrics by setting sj = 1. 5 Experiments We demonstrate the use of the dataset and metrics with simple retrieval and generation models, and show the enhanced metrics consistently improve cor"
P18-2025,P02-1040,0,0.101647,"tion not explicitly presented in the articles. Article commenting also differs from making product reviews (Tang et al., 2017; Li et al., 2017), as the latter takes structured data (e.g., product attributes) as input; while the input of article commenting is in plain text format, posing a much larger input space to explore. In this paper, we propose the new task of automatic article commenting, and release a largescale Chinese corpus with a human-annotated subset for scientific research and evaluation. We further develop a general approach of enhancing popular automatic metrics, such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), to better fit the characteristics of the new task. In recent years, enormous efforts have been made in different contexts that analyze one or more aspects of online comments. For example, Kolhatkar and Taboada (2017) identify constructive news comments; Barker et al. (2016) study human summaries of online comment conversations. The datasets used in these works are typically not directly applicable in the context of article commenting, and are small in scale that is unable to support the unique complexity of the new task. In contrast, our dataset consists"
P18-2025,D16-1264,0,0.0517994,"and comparison. We use the metrics to evaluate the above models in the supplements. 6 Conclusions and Future Work We have introduced the new task and dataset for automatic article commenting, as well as developed quality-weighted automatic metrics that leverage valuable human bias on comment quality. The dataset and the study of metrics establish a testbed for the article commenting task. We are excited to study solutions for the task in the future, by building advanced deep generative models (Goodfellow et al., 2016; Hu et al., 2018) that incorporate effective reading comprehension modules (Rajpurkar et al., 2016; Richardson et al., 2013) and rich external knowledge (Angeli et al., 2015; Hu et al., 2016). The large dataset is also potentially useful for a variety of other tasks, such as comment ranking (Hsu et al., 2009), upvotes prediction (Rizos et al., 2016), and article headline generation (Banko et al., 2000). We encourage the use of the dataset in these context. Acknowledgement. We would like to thank anonymous reviewers for their helpful suggestions and particularly the annotators for their contributions on the dataset. Hai Zhao was partially supported by National Key Research and Development P"
P18-2025,P16-2032,0,0.144391,"Work There is a surge of interest in natural language generation tasks, such as machine translation (Brown et al., 1990; Bahdanau et al., 2014), dialog (Williams and Young, 2007; Shum et al., 2018), text manipulation (Hu et al., 2017), visual description generation (Vinyals et al., 2015; Liang et al., 2017), and so forth. Automatic article commenting poses new challenges due to the large input and output spaces and the open-domain nature 152 of comments. Many efforts have been devoted to studying specific attributes of reader comments, such as constructiveness, persuasiveness, and sentiment (Wei et al., 2016; Kolhatkar and Taboada, 2017; Barker et al., 2016). We introduce the new task of generating comments, and develop a dataset that is orders-of-magnitude larger than previous related corpus. Instead of restricting to one or few specific aspects, we focus on the general comment quality aligned with human judgment, and provide over 27 gold references for each data instance to enable wide-coverage evaluation. Such setting also allows a large output space, and makes the task challenging and valuable for text generation research. Yao et al. (2017) explore defense approaches of spam or malicious revi"
P18-2025,D17-1239,0,0.0214441,"Table 2 provides the key data statistics. The dataset has a vocabulary size of 1,858,452. The average lengths of the article titles and content are 15 and 554 Chinese words (not characters), respectively. The average comment length is 17 words. Notably, the dataset contains an enormous volume of tokens, and is orders-of-magnitude larger than previous public data of article comment analysis (Wei et al., 2016; Barker et al., 2016). Moreover, each article in the dataset has on average over 27 human-written comments. Compared to other popular text generation tasks and datasets (Chen et al., 2015; Wiseman et al., 2017) which typically contain no more than 5 gold references, our dataset enables richer guidance for model training and wider coverage for evaluation, in order to fit the unique large output space of the commenting task. Each article is associated with one of 44 categories, whose distribution is shown in the supplements. The number of upvotes per comment ranges from 3.4 to 5.9 on average. Though the numbers look small, the distribution exhibits a long-tail pattern with popular comments having thousands of upvotes. Test Set Comment Quality Annotations Real human comments are of varying quality. Sel"
P19-1124,W18-6318,0,0.0624945,"mple, Bahdanau et al. (2014) is the first work to show word alignment examples by using attention in an NMT model. Tu et al. (2016) quantitatively evaluate word alignment captured by attention and find that its quality is much worse than statistical word aligners. Motivated by this finding, Chen et al. (2016), Mi et al. (2016) and Liu et al. (2016) improve attention with the supervision from silver alignment results obtained by statistical aligners, in the hope that the improved attention leads to better word alignment and translation quality consequently. More recently, there are also works (Alkhouli et al., 2018) that directly model the alignment and use it to sharpen the attention to bias translation. Despite the close relation between word alignment and attention, Koehn and Knowles (2017) and Ghader and Monz (2017) discuss the differences between word alignment and attention in NMT. Most of these works study word alignment for the same kind of NMT models with a single attention layer. One of our contribution is that we propose modelagnostic methods to study word alignment in a general way which deliver better word alignment quality than attention method. Moreover, for the first time, we further unde"
P19-1124,D17-1042,0,0.0515093,"Missing"
P19-1124,W16-1601,0,0.0667088,"Missing"
P19-1124,P17-1080,0,0.0234942,"re-trained translation models with their EAMs trained on the same FAST A LIGN annotated data. We find that a stronger (higher BLEU) translation model generally obtains better alignment (lower AER). As shown in Table 2, T RANSFORMER -L6 generates much better alignment than T RANSFORMER -L1, highly correlated with their translation performances. This suggests that supervision is not enough to obtain good alignment and the hidden units learned by a translation model indeed implicitly capture alignment knowledge by learning translation. In addition, EAM can be thought as a kind of agnostic probe (Belinkov et al., 2017; Hewitt and Manning, 2019) to investigate how much alignment are implicitly learned in the hidden representations. Table 1: AER of the proposed methods. Models AER BLEU * L1 54.50 36.51 T RANSFORMER L2 L3 L4 L5 47.94 40.47 38.40 38.80 44.83 45.63 47.19 46.35 L6 38.88 46.95 Results are measured on ZH⇒EN task. Table 2: EAM on translation models with different number of layer. Explicit Alignment Model (EAM) As shown in Table 1, EAM outperforms alignment induced from attention by a large margin. However, since EAM employs silver alignment annotations from FAST A LIGN for training the additional p"
P19-1124,I17-1004,0,0.384173,"d sl&lt;i according to different NMT models. As the dominant models, attentional NMT models define the context vector cli as a weighted sum  of h, where the weight l αli = g sl−1 i , s&lt;i , h is defined by a similarity function. Due to the space limitation, we refer readers to Bahdanau et al. (2014), Gehring et al. (2017) and Vaswani et al. (2017) for the details on the definitions of f and g. 2.2 Alignment by Attention Since the attention weight αli,j measures the similarity between sl−1 and hj , it has been widely i used to evaluate the word alignment between yi and xj (Bahdanau et al., 2014; Ghader and Monz, 2017). Once an attentional NMT model has been trained, one can easily extract word alignment A from the attention weight α according to the style of maximum a posterior strategy (MAP) as follows:  1 j = arg max αi,j 0 j0 Ai,j (α) = , (3) 0 o/w where Ai,j = 1 indicates yi aligns to xj . For NMT models with multiple attentional heads attentional layers as in Vaswani et al. (2017), we sum all attention weights with respect to all heads to a single α before MAP in equation 3. 3 Methods to Inducing Word Alignment Although attention might obtain some word alignment as described in previous section, it"
P19-1124,P09-5002,0,0.264299,"gnostic to specific NMT models. Experiments show that both methods induce much better word alignment than attention. This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics. 1 Introduction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment pro"
P19-1124,W17-3204,0,0.0274187,"by attention and find that its quality is much worse than statistical word aligners. Motivated by this finding, Chen et al. (2016), Mi et al. (2016) and Liu et al. (2016) improve attention with the supervision from silver alignment results obtained by statistical aligners, in the hope that the improved attention leads to better word alignment and translation quality consequently. More recently, there are also works (Alkhouli et al., 2018) that directly model the alignment and use it to sharpen the attention to bias translation. Despite the close relation between word alignment and attention, Koehn and Knowles (2017) and Ghader and Monz (2017) discuss the differences between word alignment and attention in NMT. Most of these works study word alignment for the same kind of NMT models with a single attention layer. One of our contribution is that we propose modelagnostic methods to study word alignment in a general way which deliver better word alignment quality than attention method. Moreover, for the first time, we further understand NMT through alignment and particularly quantify the effect of alignment errors on translation errors for NMT. The prediction difference method in this paper actually provides"
P19-1124,N03-1017,0,0.16742,"nt metrics. 1 Introduction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find"
P19-1124,D16-1011,0,0.0381201,"with a single attention layer. One of our contribution is that we propose modelagnostic methods to study word alignment in a general way which deliver better word alignment quality than attention method. Moreover, for the first time, we further understand NMT through alignment and particularly quantify the effect of alignment errors on translation errors for NMT. The prediction difference method in this paper actually provides an avenue to understand and interpret neural machine translation models. Therefore, it is closely related to many works on visualizing and interpreting neural networks (Lei et al., 2016; Bach et al., 2015; Zintgraf et al., 2017). Indeed, our method is inherited from (Zintgraf et al., 2017), and our advantage is that it is computationally efficient particularly for those tasks with a large vocabulary. In sequence-to-sequence tasks, Ding et al. (2017) focus on model interpretability by modeling how influence propagates across 5 It is interesting that SMT (MOSES) incorrectly translates this word into ‘and’ in our preliminary experiment. hidden units in networks, which is often too restrictive and challenging to achieve as argued by Alvarez-Melis and Jaakkola (2017). And instead"
P19-1124,J93-2003,0,0.151498,"ysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics. 1 Introduction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with"
P19-1124,N18-1125,1,0.817365,"ul to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as T RANSFORMER (Vaswani et al., 2017), as demonstrated in our experiments. In this paper, we propose two methods to induce word alignment from general NMT models and answer a fundamental question that how much word alignment NMT models can learn (§ 3). The first method explicitly builds a word alignment model between a pair of source and target word representations encoded by NMT models, and then it learns additional parameters for this word align"
P19-1124,2016.amta-researchers.10,0,0.100127,"t NMT captures the context of the surname ‘zheng’ by PD over target side besides the context of ‘h´e’ by PD over source side, thanks to its more powerful language model effect. 5 Related Work In NMT, there are many notable researches which mention word alignment captured by attention in some extent. For example, Bahdanau et al. (2014) is the first work to show word alignment examples by using attention in an NMT model. Tu et al. (2016) quantitatively evaluate word alignment captured by attention and find that its quality is much worse than statistical word aligners. Motivated by this finding, Chen et al. (2016), Mi et al. (2016) and Liu et al. (2016) improve attention with the supervision from silver alignment results obtained by statistical aligners, in the hope that the improved attention leads to better word alignment and translation quality consequently. More recently, there are also works (Alkhouli et al., 2018) that directly model the alignment and use it to sharpen the attention to bias translation. Despite the close relation between word alignment and attention, Koehn and Knowles (2017) and Ghader and Monz (2017) discuss the differences between word alignment and attention in NMT. Most of th"
P19-1124,C16-1291,1,0.855591,"is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as T RANSFORMER (Vaswani et al., 2017), as demonstrated in our experiments. In this paper, we propose two methods to induce word alignment from general NMT models and answer a fundamental question that how much word alignment NMT models can learn (§ 3). The first method explicitly builds a word alignment model between a pair of source and target word representations encoded by NMT models, and then it learns additional parameters f"
P19-1124,P05-1057,0,0.649273,"uction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may"
P19-1124,P17-1106,0,0.484147,"words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as T RANSFORMER (Vaswani et al., 2017), as demonstrated in our experiments. In this paper, we propose two methods to induce word align"
P19-1124,D16-1249,0,0.248338,"and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior researches ∗ Work done while X. Li interning at Tencent AI Lab. L. Liu is the corresponding author. observed that word alignment is captured by NMT through attention for recurrent neural network based NMT with a single attention layer (Bahdanau et al., 2014; Mi et al., 2016; Liu et al., 2016; Li et al., 2018). Unfortunately, we surprisingly find that attention may almost fail to capture word alignment for NMT models with multiple attentional layers such as T RANSFORMER (Vaswani et al., 2017), as demonstrated in our experiments. In this paper, we propose two methods to induce word alignment from general NMT models and answer a fundamental question that how much word alignment NMT models can learn (§ 3). The first method explicitly builds a word alignment model between a pair of source and target word representations encoded by NMT models, and then it learns addit"
P19-1124,A94-1016,0,0.0648596,"Missing"
P19-1124,N13-1073,0,0.6699,"air of column vectors of dimension d, and W is a matrix of dimension 2d × 2d. The explicit word alignment model is trained by maximizing the objective function with respect to the parameter matrix W : max W X log P (xj |yi ; W ) , (6) ∀j,i:Aref ij =1 where Aref ij is the reference alignment between xj and yi for a sentence pair x and y. As the number of elements in W is up to one million (i.e., (2 × 512)2 ), it is not feasible to train it using a small dataset with gold alignment. Therefore, following Mi et al. (2016) and Liu et al. (2016), we run statistical word aligner such as FAST A LIGN (Dyer et al., 2013) on a large corpus and then employ resulting word alignment as the silver alignment Aref for training. Note that our goal is to quantify word alignment learned by an NMT model, and thus we only treat W as the parameter to be learned, which differs from the joint training all parameters including those from NMT models as in Mi et al. (2016) and Liu et al. (2016). After training, one obtains the optimized W and then easily infers word alignment for a test sentence pair hx, yi via the MAP strategy as defined in equation 3 by setting αi,j 0 = P xj 0 |yi ; W . Note that if word embeddings and hidd"
P19-1124,W03-0301,0,0.311674,"air hx, yi, after collecting R(yi , xj ) one can easily infer word alignment via the MAP strategy as defined in equation 3 by setting αi,j 0 = R(yi , xj 0 ). Experiments In this section, we conduct extensive experiments on ZH⇒EN and DE⇒EN translation tasks to evaluate different methods for word alignment induced from the NMT model and compare them with a statistical alignment model FAST A LIGN (Dyer et al., 2013). Then, we use the induced word alignment to understand translation errors both qualitatively and quantitatively. The alignment performance is evaluated by alignment error rate (AER) (Mihalcea and Pedersen, 2003; Koehn, 2009). The proposed methods are implemented on top of T RANS FORMER (Vaswani et al., 2017) which is a state-ofthe-art NMT system. We report AER on NIST05 test set and RWTH data, whose reference alignment was manually annotated by experts (Liu et al., 2016; Ghader and Monz, 2017). More details on data and training these systems are described in Appendix A. 4.1 Inducing Word alignment from NMT Attention Since the bilingual corpus intrinsically includes word alignment in some extent, word alignment by attention should be better than the data intrinsic alignment if attention indeed captur"
P19-1124,J03-1002,0,0.104815,"This paper further visualizes the translation through the word alignment induced by NMT. In particular, it analyzes the effect of alignment errors on translation errors at word level and its quantitative analysis over many testing examples consistently demonstrate that alignment errors are likely to lead to translation errors measured by different metrics. 1 Introduction Machine translation aims at modeling the semantic equivalence between a pair of source and target sentences (Koehn, 2009), and word alignment tries to model the semantic equivalence between a pair of source and target words (Och and Ney, 2003). As a sentence consists of words, word alignment is conceptually related to machine translation and such a relation can be traced back to the birth of statistical machine translation (SMT) (Brown et al., 1993), where word alignment is the basis of SMT models and its accuracy is generally helpful to improve translation quality (Koehn et al., 2003; Liu et al., 2005). In neural machine translation (NMT), it is also important to study word alignment, because word alignment provides natural ways to understanding black-box NMT models and analyzing their translation errors (Ding et al., 2017). Prior"
P19-1124,N18-1202,0,0.0257554,"ured by attention on midRo (yi , yk ) = P (yi |y&lt;i , x)−P yi |y&lt;i (k,0) , x , dle layer(s) is reasonable, but that on low or high layer is obviously worse than PMI. The possible (10) reasons can be explained as follows. The possible where Ro indicates the relevance between two tarfunctionality of lower layers might be constructing get words yi and yk with k &lt; i, and P (yi | gradually better contextual representation of the y&lt;i (k,0) , x) is obtained by disabling the connecword at each position as suggested in recent contion between yk and the decoder network, simitextualized embedding works (Peters et al., 2018; larly to P yi |y&lt;i , x(j,0) . Unlike R(yi , xj ) capDevlin et al., 2018; Radford et al., 2019). So turing word alignment information, Ro (yi , yk ) is 2 able to capture word allocation in a target sentence More details in Appendix B. 1296 3 layer L1 Transformer L4 L3 73.29 52.66 45.22 51.19 86.92 92.13 4 5 6 0 71.68 46.72 91.02 85.98 67.09 50.07 67.05 93.16 88.41 72.11 56.68 53.98 82.40 93.97 1 (a) ZH⇒EN 52.95(PMI) 77.34 47.79 49.47 91.71 L5 L5 71.31 53.71 49.84 77.55 92.36 L6 64.88 49.32 52.81 90.11 2 45.37 67.20 65.66(PMI) 55.07 48.40 84.85 1 L2 100 48.55 L6 L1 47.80 77.38 Transformer L4 L"
P19-1124,H05-1010,0,0.154855,"ght obtain some word alignment as described in previous section, it is unknown whether NMT models contain more word alignment information than that obtained by attention. In addition, the method using attention is useful to induce word alignment for attentional 1294 NMT models, whereas it is useless for general NMT models. In this section, in order to induce word alignment from general NMT models, we propose two different methods, which are agnostic to specific NMT models. 3.1 Alignment by Explicit Alignment Model Given a source sentence x, a target sentence y, following Liu et al. (2005) and Taskar et al. (2005), we explicitly define a word alignment model as follows: exp (δ (xj , yi ; W ))  , P (xj |yi ; W ) = Pm j 0 =1 exp δ xj 0 , yi ; W (4) where δ (xj , yi ; W ) is a distance function parametrized by W . Ideally, δ is able to include arbitrary features such as IBM model 1 similar to Liu et al. (2005). However, as our goal is not to achieve the best word alignment but to focus on that captured by an NMT model, we only consider these features completely learned in NMT. Hence, we define the  δ (xj , yi ; W ) = (xj khj )> W yi ksL i , (5) where xj and yi are word embeddings of xj and yi learned i"
P19-1124,P16-1008,0,0.157983,"tgraf et al. (2017). Experiments on an advanced NMT model show that both methods achieve much better word alignment than the method by attention (§ 4.1). In addition, our experiments demonstrate that NMT captures good word alignment for those words mostly contributed from source (CFS), while their word alignment is much worse for those words mostly contributed from target (CFT). This finding offers a reason why advanced NMT models delivering excellent translation capture worse word alignment than statistical aligners in SMT, which was observed in prior researches yet without deep explanation (Tu et al., 2016; Liu et al., 2016). Furthermore, we understand and interpret NMT from the viewpoint of word alignment induced 1293 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1293–1303 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics from NMT (§ 4.2). Unlike existing researches on interpreting NMT by accessing few examples as case study (Ding et al., 2017; Alvarez-Melis and Jaakkola, 2017), we aim to provide quantitatively analysis for interpreting NMT by accessing many testing examples, which makes our findings mor"
P19-1124,P19-1580,0,0.0400862,"ZH⇒EN 52.95(PMI) 77.34 47.79 49.47 91.71 L5 L5 71.31 53.71 49.84 77.55 92.36 L6 64.88 49.32 52.81 90.11 2 45.37 67.20 65.66(PMI) 55.07 48.40 84.85 1 L2 100 48.55 L6 L1 47.80 77.38 Transformer L4 L3 L2 100 56.49 2 3 layer 4 5 6 0 (b) DE⇒EN Figure 1: AER of attention at each layer on T RANSFORMER with different number of layers. AER of PMI is shown as white. Blue and red means AER is better and worse than PMI respectively. the AERs become better while more unambiguous representations of the corresponding word are formed. However, for higher layers the representational redundancy is accumulated (Voita et al., 2019; Michel et al., 2019) for phrases or other larger meaning spans in the input, so attention is not capturing word-to-word align but more complicated semantic correspondence. Methods FAST A LIGN Attention mean Attention best EAM PD * Tasks ZH⇒EN DE⇒EN 36.57 26.58 56.44 74.59 45.22 53.98 38.88 39.25 41.77 42.81 Results are measured on T RANSFORMER -L6. AERs over different pre-trained translation models with their EAMs trained on the same FAST A LIGN annotated data. We find that a stronger (higher BLEU) translation model generally obtains better alignment (lower AER). As shown in Table 2, T RANSF"
P19-1240,S17-2126,0,0.0597462,"Missing"
P19-1240,D18-1439,0,0.437804,"09). For unsupervised methods, they are built on diverse algorithms, including graph ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008), document clustering (Liu et al., 2009, 2010), and statistical models like TF-IDF (Salton and McGill, 1986). Our work is especially in the line of social media keyphrase prediction, where extractive approaches are widely employed (Zhang et al., 2016, 2018). On the contrary, we predict keyphrases in a sequence generation manner, allowing the creation of absent keyphrases. Our work is inspired by seq2seq-based keyphrase generation models (Meng et al., 2017; Chen et al., 2018, 2019a,b), which are originally designed for scientific articles. However, their performance will be inevitably compromised when directly applied to social media language owing to the data sparsity problem. Recently, Wang et al. (2019) propose a microblog hashtag generation framework, which explicitly enriches context with user responses. Different from them, we propose to leverage corpus-level latent topic representations, which can be learned without requiring external data. Its potential usefulness on keyphrase generation has been ignored in previous research and will be extensively studie"
P19-1240,N19-1292,1,0.829005,"DF), TextRank algorighm (Mihalcea and Tarau, 2004) (henceforth T EXT R ANK), and KEA system (Witten et al., 1999) (henceforth KEA). We also compare with a neural state-of-the-art keyphrase extraction model based on sequence tagging (Zhang et al., 2016) (henceforth S EQ -TAG). In addition, we take the following state-of-the-art keyphrase generation models into consideration: seq2seq model with copy mechanism (Meng et al., 2017) (henceforth S EQ 2S EQ -C OPY) and its variation S EQ 2S EQ without copy mechanism, S EQ 2S EQ C ORR (Chen et al., 2018) exploiting keyphrase correlations, and TG-N ET (Chen et al., 2019b) jointly modeling of titles and descriptions (thereby only tested on StackExchange). 5 Experimental Results In the experiment, we first evaluate our performance on keyphrase prediction (§5.1). Then, we study whether jointly learning keyphrase generation can in turn help produce coherent topics (§5.2). At last, further discussions (§5.3) are presented with an ablation study, a case study, and an analysis for varying text genres. 5.1 Keyphrase Prediction Results In this section, we examine our performance in predicting keyphrases for social media. We first discuss the main comparison results,"
P19-1240,J18-4008,1,0.791055,"Missing"
P19-1240,D10-1036,0,0.199523,"Missing"
P19-1240,D09-1027,0,0.147834,"on and topic modeling. We introduce them in turn below. Keyphrase Prediction. Most previous efforts on this task adopt supervised or unsupervised approaches based on extraction — words or phrases selected from source documents to form keyphrases. Supervised methods are mostly based on sequence tagging (Zhang et al., 2016; Gollapalli et al., 2017) or binary classification using various features (Witten et al., 1999; Medelyan et al., 2009). For unsupervised methods, they are built on diverse algorithms, including graph ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008), document clustering (Liu et al., 2009, 2010), and statistical models like TF-IDF (Salton and McGill, 1986). Our work is especially in the line of social media keyphrase prediction, where extractive approaches are widely employed (Zhang et al., 2016, 2018). On the contrary, we predict keyphrases in a sequence generation manner, allowing the creation of absent keyphrases. Our work is inspired by seq2seq-based keyphrase generation models (Meng et al., 2017; Chen et al., 2018, 2019a,b), which are originally designed for scientific articles. However, their performance will be inevitably compromised when directly applied to social medi"
P19-1240,D09-1137,0,0.235967,"scientific articles shows that latent topics work better on text genres with informal language style. 2 Related Work Our work is mainly in the line of two areas: keyphrase prediction and topic modeling. We introduce them in turn below. Keyphrase Prediction. Most previous efforts on this task adopt supervised or unsupervised approaches based on extraction — words or phrases selected from source documents to form keyphrases. Supervised methods are mostly based on sequence tagging (Zhang et al., 2016; Gollapalli et al., 2017) or binary classification using various features (Witten et al., 1999; Medelyan et al., 2009). For unsupervised methods, they are built on diverse algorithms, including graph ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008), document clustering (Liu et al., 2009, 2010), and statistical models like TF-IDF (Salton and McGill, 1986). Our work is especially in the line of social media keyphrase prediction, where extractive approaches are widely employed (Zhang et al., 2016, 2018). On the contrary, we predict keyphrases in a sequence generation manner, allowing the creation of absent keyphrases. Our work is inspired by seq2seq-based keyphrase generation models (Meng et al., 2017; Che"
P19-1240,P17-1054,0,0.0666867,"udies, we approach social media keyphrase prediction with a sequence 2 Following common practice (Zhang et al., 2016, 2018), we consider author-annotated hashtags as tweets’ keyphrases. 2516 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2516–2526 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics generation framework, which is able to create absent keyphrases beyond source posts. Our work is built on the success of deep keyphrase generation models based on neural sequence-to-sequence (seq2seq) framework (Meng et al., 2017). However, existing models, though effective on well-edited documents (e.g., scientific articles), will inevitably encounter the data sparsity issue when adapted to social media. It is essentially due to the informal and colloquial nature of social media language, which results in limited features available in the noisy data. For instance, only given the words in S (Table 1), it is difficult to figure out why “super bowl” is its keyphrase. However, by looking at tweets T1 to T3 , we can see “yellow pants” is relevant to “steelers”, a super bowl team. As “yellow” and “pants” widely appear in tw"
P19-1240,W04-3252,0,0.730888,"Work Our work is mainly in the line of two areas: keyphrase prediction and topic modeling. We introduce them in turn below. Keyphrase Prediction. Most previous efforts on this task adopt supervised or unsupervised approaches based on extraction — words or phrases selected from source documents to form keyphrases. Supervised methods are mostly based on sequence tagging (Zhang et al., 2016; Gollapalli et al., 2017) or binary classification using various features (Witten et al., 1999; Medelyan et al., 2009). For unsupervised methods, they are built on diverse algorithms, including graph ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008), document clustering (Liu et al., 2009, 2010), and statistical models like TF-IDF (Salton and McGill, 1986). Our work is especially in the line of social media keyphrase prediction, where extractive approaches are widely employed (Zhang et al., 2016, 2018). On the contrary, we predict keyphrases in a sequence generation manner, allowing the creation of absent keyphrases. Our work is inspired by seq2seq-based keyphrase generation models (Meng et al., 2017; Chen et al., 2018, 2019a,b), which are originally designed for scientific articles. However, their performance will be"
P19-1240,P17-1099,0,0.0571988,"vocabulary according to: pgen = sof tmax(Wgen [sj ; cj ] + bgen ). P r(yj |y<j , M, θ), (4) where y<j = hy1 , y2 , ..., yj−1 i. And P r(yj |y<j , M, θ), denoted as pj , is a word distribution over vocabulary, reflecting how likely a word to fill in the j-th slot in target keyphrase. Below we describe the procedure to obtain pj . Our sequence decoder employs a unidirectional GRU layer. Apart from the general state update, the j-th hidden state sj is further designed to take input x’s topic mixture θ into consideration: sj = fGRU ([uj ; θ], sj−1 ), (5) (9) In addition, we adopt copy mechanism (See et al., 2017) following Meng et al. (2017), which allows keywords to be directly extracted from the source input. Specifically, we adopt a soft switcher λj ∈ [0, 1] to determine whether to copy a word from source as the j-th target word: λj = sigmoid(Wλ [uj ; sj ; cj ; θ] + bλ ), j=1 (6) where cj = Topic-Aware Sequence Decoder. In general, conditioned on the memory bank M and latent topic θ from NTM, we define the process to generate its keyphrase y with the following probability: P r(y |x) = exp(fα (hi , sj , θ)) αij = P|x| , 0 i0 =1 exp(fα (hi , sj , θ)) (2) ← − hi = fGRU (νi , hi+1 ). (3) → − ← − → − ←"
P19-1240,N19-1164,1,0.932824,"cGill, 1986). Our work is especially in the line of social media keyphrase prediction, where extractive approaches are widely employed (Zhang et al., 2016, 2018). On the contrary, we predict keyphrases in a sequence generation manner, allowing the creation of absent keyphrases. Our work is inspired by seq2seq-based keyphrase generation models (Meng et al., 2017; Chen et al., 2018, 2019a,b), which are originally designed for scientific articles. However, their performance will be inevitably compromised when directly applied to social media language owing to the data sparsity problem. Recently, Wang et al. (2019) propose a microblog hashtag generation framework, which explicitly enriches context with user responses. Different from them, we propose to leverage corpus-level latent topic representations, which can be learned without requiring external data. Its potential usefulness on keyphrase generation has been ignored in previous research and will be extensively studied here. Topic Modeling. Our work is closely related with topic models that discover latent topics from word co-occurrence in document level. They are commonly in the fashion of latent Dirichlet allocation (LDA) based on Bayesian graphic"
P19-1240,Q19-1017,1,0.806586,"are Decoder *./0 4/ 46 *+,47 45 ! σ 48 "" *3+,- Neural Topic Model Figure 1: Our topic-aware neural keyphrase generation framework (§3). (Blei et al., 2003). These models, however, rely on the expertise involvement to customize model inference algorithms. Our framework exploits the recently proposed neural topic models (Miao et al., 2017; Srivastava and Sutton, 2017) to infer latent topics, which facilitate end-to-end training with other neural models and do not require modelspecific derivation. It has proven useful for citation recommendation (Bai et al., 2018) and conversation understanding (Zeng et al., 2019). In particular, Zeng et al. (2018) propose to jointly train topic models and short text classification, which cannot fit our scenario due to the large diversity of the keyphrases (Wang et al., 2019). Different from them, our latent topics are learned together with language generation, whose effects on keyphrase generation have never been explored before in existing work. 3 Topic-Aware Neural Keyphrase Generation Model In this section, we describe our framework that leverages latent topics in neural keyphrase generation. Figure 1 shows our overall architecture consisting of two modules — a neu"
P19-1240,D18-1351,1,0.809749,"48 "" *3+,- Neural Topic Model Figure 1: Our topic-aware neural keyphrase generation framework (§3). (Blei et al., 2003). These models, however, rely on the expertise involvement to customize model inference algorithms. Our framework exploits the recently proposed neural topic models (Miao et al., 2017; Srivastava and Sutton, 2017) to infer latent topics, which facilitate end-to-end training with other neural models and do not require modelspecific derivation. It has proven useful for citation recommendation (Bai et al., 2018) and conversation understanding (Zeng et al., 2019). In particular, Zeng et al. (2018) propose to jointly train topic models and short text classification, which cannot fit our scenario due to the large diversity of the keyphrases (Wang et al., 2019). Different from them, our latent topics are learned together with language generation, whose effects on keyphrase generation have never been explored before in existing work. 3 Topic-Aware Neural Keyphrase Generation Model In this section, we describe our framework that leverages latent topics in neural keyphrase generation. Figure 1 shows our overall architecture consisting of two modules — a neural topic model for exploring laten"
P19-1240,D16-1080,0,0.69332,"phrase identification, most progress to date has focused on extracting words or phrases from source posts, thus failing to yield keyphrases containing absent words (i.e., words do not appear in the post). Such cases are indeed prominent on social media, mostly attributed to the informal writing styles of users therein. For example, Table 1 shows a tweet S tagged with keyphrase “super bowl” by its author, though neither “super” nor “bowl” appears in it.2 In our work, distinguishing from previous studies, we approach social media keyphrase prediction with a sequence 2 Following common practice (Zhang et al., 2016, 2018), we consider author-annotated hashtags as tweets’ keyphrases. 2516 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2516–2526 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics generation framework, which is able to create absent keyphrases beyond source posts. Our work is built on the success of deep keyphrase generation models based on neural sequence-to-sequence (seq2seq) framework (Meng et al., 2017). However, existing models, though effective on well-edited documents (e.g., scientific articles),"
P19-1240,N18-1151,1,0.827295,"r quantity of texts, language understanding has become a daunting task for human beings. Under this circumstance, there exists a pressing need for developing automatic systems capable of absorbing massive social media texts and figuring out what is important. * This work was partially done when Yue Wang was an intern at Tencent AI Lab. † Jing Li is the corresponding author. 1 Our data and code are publicly released in https:// github.com/yuewang-cuhk/TAKG In this work, we study the prediction of keyphrases, generally formed with words or phrases reflecting main topics conveyed in input texts (Zhang et al., 2018). Particularly, we focus on producing keyphrases for social media language, proven to be beneficial to a broad range of applications, such as instant detection of trending events (Weng and Lee, 2011), summarizing public opinions (Meng et al., 2012), analyzing social behavior (Ruths and Pfeffer, 2014), and so forth. In spite of the substantial efforts made in social media keyphrase identification, most progress to date has focused on extracting words or phrases from source posts, thus failing to yield keyphrases containing absent words (i.e., words do not appear in the post). Such cases are ind"
P19-1389,W17-4912,0,0.0629686,"Missing"
P19-1389,C14-1088,0,0.0229802,"a response following a relevant topic could make the user more engaged in continuing the conversation. The above studies, involving the control of emotions or topics, often affects a few words in the whole returned response, such as smiling for the happy emotion, moisturizing for the skincare topic. Different from them, sentence function adjusts the global structure of the entire response, including changing word order and word patterns (Ke et al., 2018). Modeling dialogue acts such as statement, question and backchannel, in conversation models has also attracted many researchers’ attention. Higashinaka et al. (2014) identify dialogue acts of utterances, which later contribute to the selection of appropriate responses. Zhao et al. (2017) utilize dialogue acts as the knowledge guided attributes in the CVAE for response generation. Sentence function is similar to dialogue act in that they both indicate the communicative purpose of a sentence in conversation. Moreover, our fine-grained sentence function types are in many ways inspired from the dialogue act tag set (Stolcke et al., 2000) designed for the Switchboard corpus (Godfrey and Holliman, 1997), which consists of human-human conversational telephone sp"
P19-1389,P18-1139,0,0.365778,"humans. Existing research has analyzed various factors indicating the conversational purpose such as emotions (Prendinger and Ishizuka, 2005; Zhou et al., 2018; Shi and Yu, 2018), topics (Xing et al., 2017; Wang et al., 2017), dialogue acts (Liscombe et al., 2005; Higashinaka et al., 2014; Zhao et al., 2017) and so on. This work describes an effort to understand conversations, especially shorttext conversations (Shang et al., 2015), in terms of sentence function. Sentence function is an important linguistic feature referring to a user’s purpose in uttering a specific sentence (Rozakis, 2003; Ke et al., 2018). There are four major sentence functions: Interrogative, Declarative, Imperative and Exclamatory (Rozakis, 2003). Sentences with different sentence functions generally have different structures of the entire text including word orders, syntactic patterns and other aspects (Akmajian, 1984; Yule, 2016). Some work has investigated the use of sentence function in conversation models. For example, Li et al. (2016) propose to output interrogative and imperative responses to avoid stalemates. Ke et al. (2018) incorporate a given sentence function as a controllable variable into the conditional varia"
P19-1389,D14-1181,0,0.00883019,"Missing"
P19-1389,D16-1230,0,0.0304203,"Missing"
P19-1389,P15-1152,0,0.205162,"Missing"
P19-1389,P18-1140,0,0.0660953,"Missing"
P19-1389,J00-3003,0,0.654659,"Missing"
P19-1389,D17-1228,0,0.029921,"Missing"
P19-1389,P17-1061,0,0.286349,"enerative models. Experimental results demonstrate that the use of sentence functions can help improve the quality of the returned responses. 1 Introduction The ability to model and detect the purpose of a user is essential when we build a dialogue system or chatbot that can have coherent conversations with humans. Existing research has analyzed various factors indicating the conversational purpose such as emotions (Prendinger and Ishizuka, 2005; Zhou et al., 2018; Shi and Yu, 2018), topics (Xing et al., 2017; Wang et al., 2017), dialogue acts (Liscombe et al., 2005; Higashinaka et al., 2014; Zhao et al., 2017) and so on. This work describes an effort to understand conversations, especially shorttext conversations (Shang et al., 2015), in terms of sentence function. Sentence function is an important linguistic feature referring to a user’s purpose in uttering a specific sentence (Rozakis, 2003; Ke et al., 2018). There are four major sentence functions: Interrogative, Declarative, Imperative and Exclamatory (Rozakis, 2003). Sentences with different sentence functions generally have different structures of the entire text including word orders, syntactic patterns and other aspects (Akmajian, 1984; Yul"
P19-1624,D18-1338,0,0.0718853,"+ D EEP (R NN) + D EEP (TAM) T RANSFORMER -B IG + D EEP (R NN) + D EEP (TAM) En⇒De 27.31 28.38⇑ 28.33⇑ 28.58 29.04↑ 29.19⇑ En⇒Fr 39.32 40.15⇑ 40.27⇑ 41.41 41.87 42.04⇑ Table 2: Case-sensitive BLEU scores on WMT14 En⇒De and En⇒Fr test sets. “↑ / ⇑”: significant over T RANSFORMER counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling. vs. 264.1M, not shown in the table). Furthermore, D EEP (TAM) consistently outperforms D EEP (RNN) in the T RANSFORMER -B IG configuration. One possible reason is that the big models benefit more from the improved gradient flow with the transparent attention (Bapna et al., 2018). 3.3 Linguistic Analysis To gain linguistic insights into the global and deep sentence representation, we conducted probing tasks1 (Conneau et al., 2018) to evaluate linguistics knowledge embedded in the encoder output and the sentence representation in the variations of the Base model that are trained on En⇒De translation task. The probing tasks are classification problems that focus on simple linguistic properties of sentences. The 10 probing tasks are categories into three groups: (1) Surface information. (2) Syntactic information. (3) Semantic information. For each task, we trained the cl"
P19-1624,P18-1198,0,0.194725,"of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer linguistic information. The contributions of this paper are: • Our study demonstrates the necessity and effectiveness of exploiting source-side sentential context for NMT, which benefits from fusing useful contextual information across encoder layers. • We propose several strategies to better capture useful sentential context for neural machine translation. Experimental results empirically show that the proposed approaches achieve improvement over the strong baseline model T RANSFORMER. 6197 Proceedings of"
P19-1624,D18-1457,1,0.59051,"ial context representation. 2.3 Deep Sentential Context Deep sentential context is a function of all encoder layers outputs {H1 , . . . , HL }: g = g(H1 , . . . , HL ) = D EEP(g1 , . . . , gL ), (8) where gl is the sentence representation of the l-th layer Hl , which is calculated by Equation 3. The motivation for this mechanism is that recent studies reveal that different encoder layers capture linguistic properties of the input sentence at different levels (Peters et al., 2018), and aggregating layers to better fuse semantic information has proven to be of profound value (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). In this work, we propose to fuse the global information across layers. Choices of D EEP(·) In this work, we investigate two representative functions to aggregate information across layers, which differ at whether the decoding information is taken into account. RNN Intuitively, we can treat G = {g1 , . . . , gL } as a sequence of representations, and recurring all the representations with an RNN: L X βi,l gl , (10) l=1 βi = ATTg (dli−1 , G), (11) where ATTg (·) is an attention model with its own parameters, that specifics which context representations is"
P19-1624,W07-0717,0,0.0324893,"ng the source sentence representation. The deep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the enco"
P19-1624,C18-1276,0,0.0898512,"SFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT. 1 Introduction Sentential context, which involves deep syntactic and semantic structure of the source and target languages (Nida, 1969), is crucial for machine translation. In statistical machine translation (SMT), the sentential context has proven beneficial for predicting local translations (Meng et al., 2015; Zhang et al., 2015). The exploitation of sentential context in neural machine translation (NMT, Bahdanau et al., 2015), however, is not well studied. Recently, Lin et al. (2018) showed that the translation at each time step should be conditioned on the whole target-side context. They introduced a deconvolution-based decoder to provide the global information from the target-side context for guidance of decoding. In this work, we propose simple yet effective approaches to exploiting source-side global sentence-level context for NMT models. We use encoder representations to represent the sourceside context, which are summarized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the w"
P19-1624,P15-1003,0,0.0729807,"context representation. Experimental results on the WMT14 English⇒German and English⇒French benchmarks show that our model consistently improves performance over the strong T RANSFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT. 1 Introduction Sentential context, which involves deep syntactic and semantic structure of the source and target languages (Nida, 1969), is crucial for machine translation. In statistical machine translation (SMT), the sentential context has proven beneficial for predicting local translations (Meng et al., 2015; Zhang et al., 2015). The exploitation of sentential context in neural machine translation (NMT, Bahdanau et al., 2015), however, is not well studied. Recently, Lin et al. (2018) showed that the translation at each time step should be conditioned on the whole target-side context. They introduced a deconvolution-based decoder to provide the global information from the target-side context for guidance of decoding. In this work, we propose simple yet effective approaches to exploiting source-side global sentence-level context for NMT models. We use encoder representations to represent the source"
P19-1624,N18-1202,0,0.408191,"ized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer ling"
P19-1624,W18-5431,0,0.211353,"l context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer linguistic information. The contrib"
P19-1624,P16-1162,0,0.0894149,"ee appealing strengths. First, TAM dynamically generates the weights βi based on the decoding information at every decoding step dli−1 , while R NN is unaware of the decoder states and the associated parameters are fixed after training. Second, TAM allows the model to adjust the gradient flow to different layers in the encoder depending on its training phase. 3 Experiment We conducted experiments on WMT14 En⇒De and En⇒Fr benchmarks, which contain 4.5M and 35.5M sentence pairs respectively. We reported experimental results with case-sensitive 4gram BLEU score. We used byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations to alleviate the out-of-vocabulary problem. We implemented the proposed approaches on top of T RANSFORMER model (Vaswani et al., 2017). We followed Vaswani et al. (2017) to set the model configurations, and reproduced their reported results. We tested both Base and Big models, which differ at the layer size (512 vs. 1024) and the number of attention heads (8 vs. 16). 3.1 Ablation Study (9) We first investigated the effect of components in the proposed approaches, as listed in Table 1. We use the last RNN state as the sentence representation: g = rL . As seen, the RNN"
P19-1624,N18-1117,0,0.0609503,"Missing"
P19-1624,D16-1159,0,0.0282864,", which are summarized into a sentential context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side context. Specifically, we propose two types of sentential context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that aggregates the sentence representations of all the encoder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different encoder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the stateof-the-art T RANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation performance across language pairs. Among the model variations, the deep strategies consistently outperform their shallow counterparts, which confirms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed"
P19-1624,Q18-1029,1,0.852228,"n which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specif"
P19-1624,2014.amta-researchers.11,0,0.0241312,"representation. The deep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations f"
P19-1624,P15-1162,0,0.0746289,"Missing"
P19-1624,D17-1301,1,0.612456,"eep sentential context which is induced from all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent d"
P19-1624,P14-1062,0,0.02126,"the choice of g(·), namely shallow sentential context (Figure 1b) and deep sentential context (Figure 1c), which differ at the encoder layers to be exploited. It should be pointed out that the new parameters introduced in the proposed approach are jointly updated with NMT model parameters in an endto-end manner. 2.2 Shallow Sentential Context Shallow sentential context is a function of the top encoder layer output HL : g = g(HL ) = G LOBAL(HL ), (3) where G LOBAL(·) is the composition function. Choices of G LOBAL(·) Two intuitive choices are mean pooling (Iyyer et al., 2015) and max pooling (Kalchbrenner et al., 2014): G LOBAL MEAN = M EAN(HL ), L G LOBAL MAX = M AX(H ). (4) (5) Recently, Lin et al. (2017) proposed a selfattention mechanism to form sentence representation, which is appealing for its flexibility on extracting implicit global features. Inspired by this, 6198 g3g3 r3 r3 g2g2 r2 r2 g1g1 r1 r1 di-1di-1 g3g3 βi,3βi,3 g2g2 βi,2βi,2 g1g1 r0 r0 (a) R NN gi gi βi,1βi,1 (b) TAM Figure 2: Illustration of the deep functions. “TAM” model dynamically aggregates sentence representations at each decoding step with state di−1 . aggregation repeatedly revises the sentence representations of the sequence with"
P19-1624,C18-1255,0,0.0348102,"entation. 2.3 Deep Sentential Context Deep sentential context is a function of all encoder layers outputs {H1 , . . . , HL }: g = g(H1 , . . . , HL ) = D EEP(g1 , . . . , gL ), (8) where gl is the sentence representation of the l-th layer Hl , which is calculated by Equation 3. The motivation for this mechanism is that recent studies reveal that different encoder layers capture linguistic properties of the input sentence at different levels (Peters et al., 2018), and aggregating layers to better fuse semantic information has proven to be of profound value (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). In this work, we propose to fuse the global information across layers. Choices of D EEP(·) In this work, we investigate two representative functions to aggregate information across layers, which differ at whether the decoding information is taken into account. RNN Intuitively, we can treat G = {g1 , . . . , gL } as a sequence of representations, and recurring all the representations with an RNN: L X βi,l gl , (10) l=1 βi = ATTg (dli−1 , G), (11) where ATTg (·) is an attention model with its own parameters, that specifics which context representations is relevant for each d"
P19-1624,W04-3250,0,0.0589519,"M +18.9M +18.9M +19.9M +26.8M +26.4M n/a R NN TAM Train 1.39 1.08 1.35 1.34 1.22 1.03 1.07 Decode 3.85 3.09 3.45 3.43 3.23 3.14 3.03 BLEU 27.31 27.81 27.58 27.81↑ 28.04⇑ 28.38⇑ 28.33⇑ Table 1: Impact of components on WMT14 En⇒De translation task. BLEU scores in the table are case sensitive. “Train” denotes the training speed (steps/second), and “Decode” denotes the decoding speed (sentences/second) on a Tesla P40. “TAM” denotes the transparent attention model to implement the function D EEP(·). “↑ / ⇑”: significant over T RANSFORMER counterpart (p &lt; 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). Model baseline Base model, validating the importance of sentential context in NMT. Among them, attentive mechanism (Row 5) obtains the best performance in terms of BLEU score, while maintains the training and decoding speeds. Therefore, we used the attentive mechanism to implement the function G LOBAL(·) as the default setting in the following experiments. Deep Sentential Context (Rows 6-7) As seen, both R NN and TAM consistently outperform their shallow counterparts, proving the effectiveness of deep sentential context. Introducing deep context significantly improves translation performance"
P19-1624,N19-1359,1,0.869305,"Missing"
P19-1624,P17-2089,0,0.0424251,"Missing"
P19-1624,P12-1079,0,0.0231744,"m all encoder layers can improve translation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generati"
P19-1624,D18-1475,1,0.81888,"a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specifically, the shallow and the deep strategies exploit the top encoder layer and all the encoder layers, respectively. Experimental results on W"
P19-1624,C16-1170,0,0.0197359,"ation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sentence abstraction from a specific aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al."
P19-1624,Q18-1011,1,0.817489,"t al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the encoder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder representations for the subsequent decoder. Concerning guiding the NMT generation with source-side context, Zheng et al. (2018) split the source content into translated and untranslated parts, while we focus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Specifically, the shallow and the deep strategies exploit the top encoder layer and all the encoder layers, respectively. Experimental results on WMT14 benchmarks show that exploiting sentential context improves performances over the state-of-theart T RANSFORMER model. Linguistic analyses reveal that the proposed approach indeed captures more linguistic information as exp"
Q18-1029,D12-1108,0,0.113404,"Missing"
Q18-1029,E14-1035,0,0.0675092,"Missing"
Q18-1029,P15-1001,0,0.0995313,"Missing"
Q18-1029,P14-1062,0,0.0258353,"Missing"
Q18-1029,P17-1137,0,0.0519107,"Missing"
Q18-1029,W17-3204,0,0.0627173,"Missing"
Q18-1029,P17-1064,1,0.884515,"Missing"
Q18-1029,D16-1147,0,0.0753802,"Missing"
Q18-1029,W04-3225,0,0.187657,"Missing"
Q18-1029,P02-1040,0,0.101239,"Missing"
Q18-1029,P16-1159,1,0.900068,"Missing"
Q18-1029,W10-2602,0,0.214099,"Missing"
Q18-1029,N16-1036,0,0.0609243,"Missing"
Q18-1029,P16-1008,1,0.898208,"Missing"
Q18-1029,Q17-1007,1,0.91196,"Missing"
Q18-1029,P16-1125,0,0.0781792,"Missing"
Q18-1029,D17-1301,1,0.617798,"Missing"
Q18-1029,D17-1149,1,0.87385,"Missing"
Q18-1029,2011.mtsummit-papers.13,0,0.403824,"Missing"
Q18-1029,P12-1079,0,0.0962214,"Missing"
Q18-1029,P17-2092,1,0.892768,"Missing"
