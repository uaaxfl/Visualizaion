2020.findings-emnlp.77,D19-1371,0,0.10446,"ing the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence classification. However, such methods have a critical problem: their performances on longer spans1 is not so good since they are designed to maximize the prediction of rhetorical roles for a small context. To tackle the problem, we propose a novel approach, neural sequential span classification, that directly gives the labels for the spans while considering all possible spans of vario"
2020.findings-emnlp.77,D19-1383,0,0.277241,"abels with a B(egin)/I(nside) tag set to each sentence while considering the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence classification. However, such methods have a critical problem: their performances on longer spans1 is not so good since they are designed to maximize the prediction of rhetorical roles for a small context. To tackle the problem, we propose a novel approach, neural sequential span classification, that directly gives"
2020.findings-emnlp.77,I17-2052,0,0.507361,"qKdgxjFBP83rNYwDJWkOFzPZzgHBexF2VQGVGSjVQlFmkG8CWUyQ9Xfo67&lt;/latexit&gt; F gure 1 Overv ew of he neura sequen a span c ass fica on In he examp e five rhe or ca abe s “Background” (B) “Ob ec ve” (O) “Me hods” (M) “Resu s” (R) and “Conc us ons” (C) can be ass gned o spans The abs rac ha cons s s of five sen ences s segmen ed n o four spans span(1 1) span(2 2) span(3 4) and span(5 5) and hese spans are abe ed as B M R and C respec ve y 2019) to handle spans of the different lengths To demonstrate the effectiveness of method we conduc ed exper men a eva ua ons on wo benchmark datasets PubMed 20k RCT (Dernoncourt and Lee 2017) and NICTA-PIBOSO (K m e a 2011) The results show that our method achieved the best micro sentence-F1 score of 93 1 and micro span-F1 score of 84 3 n he PubMed 20k RCT da ase and the best micro sentence-F1 score of 84 4 and micro span-F1 score of 58 7 in the NICTA-PIBOSO dataset 2 Proposed Method follows: −−−−→ ←−−−− f = LSTM(f −1 , s ), b = LSTM(b +1 , s ). (1) Here s represents the embedding of the i-th sentence To obtain s we utilize BERT which has been pre-trained with PubMed (Peng et al 2019) We insert [CLS] tokens at the beginning and [SEP] tokens at the end of sentences and then extract"
2020.findings-emnlp.77,E17-2110,0,0.0160424,"on-answering (Guo et al., 2013). Most previous methods in PubMed have regarded the task as a sequence labeling, namely sequential sentence classification, that assigns rhetorical labels with a B(egin)/I(nside) tag set to each sentence while considering the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence classification. However, such methods have a critical problem: their performances on longer spans1 is not so good since they are designed to m"
2020.findings-emnlp.77,N19-1423,0,0.00896325,"statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence classification. However, such methods have a critical problem: their performances on longer spans1 is not so good since they are designed to maximize the prediction of rhetorical roles for a small context. To tackle the problem, we propose a novel approach, neural sequential span classification, that directly gives the labels for the spans while considering all possible spans of various lengths in the abstract. That is, our method is"
2020.findings-emnlp.77,W19-5006,0,0.23808,"s of method we conduc ed exper men a eva ua ons on wo benchmark datasets PubMed 20k RCT (Dernoncourt and Lee 2017) and NICTA-PIBOSO (K m e a 2011) The results show that our method achieved the best micro sentence-F1 score of 93 1 and micro span-F1 score of 84 3 n he PubMed 20k RCT da ase and the best micro sentence-F1 score of 84 4 and micro span-F1 score of 58 7 in the NICTA-PIBOSO dataset 2 Proposed Method follows: −−−−→ ←−−−− f = LSTM(f −1 , s ), b = LSTM(b +1 , s ). (1) Here s represents the embedding of the i-th sentence To obtain s we utilize BERT which has been pre-trained with PubMed (Peng et al 2019) We insert [CLS] tokens at the beginning and [SEP] tokens at the end of sentences and then extract vectors corresponding to [CLS] tokens in the penultimate layer as sentence vectors Finally we represent a span from the i-th sentence to the j- h sen ence as a vec or vspan( j) wh ch s a concatenation of four vectors as follows: To perform sequen a span c ass fica on n an endto-end manner we need to represent spans as vecors and hand e a poss b e sequences w h var ous lengths in the abstract To this end we introduce BiLSTMs and Semi-Markov CRFs (SCRFs) Figure 1 shows an overview of our method The"
2020.findings-emnlp.77,P16-1111,0,0.022528,"sing (NLP). For example, abstracts in PubMed, a database of the biomedical literature, can be divided into rhetorical segments such as “Objective”, “Methods”, “Results”, and “Conclusions”. Abstracts segmented for each rhetorical role allows us to exploit advanced search. That is, researchers can easily find information by utilizing the structured queries such as “find abstracts that contain ‘Covid19’ in ‘Objective’ and ‘Remdesivir’ in ‘Methods’”. Furthermore, the technique can also be used for NLP applications such as academic writing support (Huang and Chen, 2017), scientific trend analysis (Prabhakaran et al., 2016), and question-answering (Guo et al., 2013). Most previous methods in PubMed have regarded the task as a sequence labeling, namely sequential sentence classification, that assigns rhetorical labels with a B(egin)/I(nside) tag set to each sentence while considering the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for"
2020.findings-emnlp.77,I08-1050,0,0.558478,"applications such as academic writing support (Huang and Chen, 2017), scientific trend analysis (Prabhakaran et al., 2016), and question-answering (Guo et al., 2013). Most previous methods in PubMed have regarded the task as a sequence labeling, namely sequential sentence classification, that assigns rhetorical labels with a B(egin)/I(nside) tag set to each sentence while considering the context in the abstract. To this end, some statistical methods with hand-engineered features have been proposed, including Hidden Markov Models (HMMs) (Lin et al., 2006) and Conditional Random Fields (CRFs) (Hirohata et al., 2008; Kim et al., 2011; Hassanzadeh et al., 2014). Recently, with the success of neural network models for NLP tasks, Dernoncourt et al. (2017) and Jin and Szolovits (2018) have employed BiLSTMs to obtain sentence embeddings based on word embeddings and CRFs for assigning labels to the sentences. Cohan et al. (2019) employed a pre-trained language model, S CIBERT (Beltagy et al., 2019), which is a variant of BERT (Devlin et al., 2019) trained with scientific papers, to improve the performance of classification without CRFs. Previous methods cast the segmentation with the labeling as a sentence cla"
2020.findings-emnlp.77,D18-1191,0,0.0278845,"anner we need to represent spans as vecors and hand e a poss b e sequences w h var ous lengths in the abstract To this end we introduce BiLSTMs and Semi-Markov CRFs (SCRFs) Figure 1 shows an overview of our method The BiLSTMs layer generates span vectors from sentence vectors incorporating the context in the abstract and the SCRFs layer learns the labeling of span sequences by cons der ng a poss b e sequences of various lengths The details are described below 21 Span Representation B LSTMs have been successfu y used o represen spans as vectors in many NLP tasks such as semantic role labeling (Ouchi et al 2018) syntactic parsing (Stern et al 2017) and coreference resolution (Lee et al 2017) BiLSTMs use a forward− −−− → LSTM func on LSTM and backward-LSTM func←−−−− tion LSTM where the forward and backward hidden states of the i-th sentence are represented as 872 vspan( 22 j) = [f −1 ; b ; fj ; bj+1 ]. (2) Neural Semi-Markov CRFs Neural SCRFs (Ye and Ling 2018; Kemos et al 2019) learn parameters the logPN to maximize ∗ likelihood function j=1 log P (yj |Xj ) where N is the number of training data yj∗ is the correctly labeled sequence of spans for the j-th abstract in the training data and X is the seq"
2021.naacl-main.127,P19-1061,0,0.0150032,"aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named data diversification: it diversifies the training data by using multiple forward and backward translation models. We can find some weak supervision approaches for other discourse representation formalisms such as (Badene et al., 2019). Teacher parsers Autoparsed trees Ptch=1 Ptch=k Training Unlabeled Documents CNN … … Gold data: RST-DT Autoparsed trees Silver data: Agreement subtrees Student parser Pstu Pre-training Gold data: RST-DT Pstu Fine-tuning Figure 1: Overview of proposed method. In the subtree extraction step, the teacher RST parsers first annotate trees to unlabeled documents, and then the proposed subtree extraction method constructs large silver data. In the training step, the student parser is trained through pre-training and fine-tuning. cannot predict nucleus and relation labels. Therefore, the predicted tr"
2021.naacl-main.127,E17-1028,0,0.0165477,". Kobayashi et al. (2020) proposed another top-down RST parsing method exploiting multiple granularity levels in a document and achieved the best Span and Nuclearity scores on the RST-DT, i.e., F1 of 87.0 and 74.6, respectively. Since the RST-DT, the largest treebank, contains only 385 documents, several studies have been conducted on overcoming the problem of a limited number of training data. Braud et al. (2016) leveraged multi-task learning not only with 13 related tasks as an auxiliary task but also for multiple views of discourse structures, such as Constituent, Nuclearity, and Relation. Braud et al. (2017) used multilingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotat"
2021.naacl-main.127,W01-1605,0,0.226951,"r ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classificatio"
2021.naacl-main.127,W11-0401,0,0.0365009,"d theories for representing the discourse structure of a text as a tree. RST trees are a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), i.e., clause-like units, and whose non-terminal nodes cover text spans consisting of either a sequence of EDUs or a single EDU. The label of a non-terminal node represents the attribution of a text span, i.e., nucleus (N) or satellite (S). A discourse relation is also assigned between two adjacent non-terminal nodes. 1 We can find some exceptions for other languages such In most cases, RST parsers have been devel- as Spanish (da Cunha et al., 2011) and German (Stede and oped on the basis of supervised learning algorithms Neumann, 2014). 1600 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1600–1612 June 6–11, 2021. ©2021 Association for Computational Linguistics (Sennrich et al., 2016) introduced a simple learning framework: first pre-train an NMT model with silver data, i.e., pseudo-parallel data generated by automatic back-translation, and then fine-tune it with gold data, i.e., real parallel data, to overcome the data sparseness prob"
2021.naacl-main.127,P14-1048,0,0.0273741,"cale data. We first pre-train the student parser by using the obtained silver data. We then fine-tune parameters of the parser on gold data, using the RST-DT. Experimental results on the RST-DT clearly indicate the effectiveness of our silver data. Our method obtained remarkable Nuclearity and Relation F1 scores of 75.0 and 63.2, respectively. 2 Related Work Early studies on RST parsing were based on traditional supervised learning methods with handcrafted features and the shift-reduce or CKY-like parsing algorithms (duVerle and Prendinger, 2009; Feng and Hirst, 2012; Joty et al., 2013, 2015; Feng and Hirst, 2014). Recently, Wang et al. (2017b) proposed a shift-reduce parser based on SVMs and achieved the current best results in classical statistical models on the RST-DT. The method first built nuclearity-labeled RST trees and then assigned relation labels between two adjacent spans consisting of a single or multiple EDUs. Inspired by the success of neural networks in many NLP tasks, several neural network-based models have been proposed for RST parsing (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017). Yu et al. (2018) proposed a shift-reduce parser based on neural networks and l"
2021.naacl-main.127,D19-1235,0,0.0143347,"n a document and achieved the best Span and Nuclearity scores on the RST-DT, i.e., F1 of 87.0 and 74.6, respectively. Since the RST-DT, the largest treebank, contains only 385 documents, several studies have been conducted on overcoming the problem of a limited number of training data. Braud et al. (2016) leveraged multi-task learning not only with 13 related tasks as an auxiliary task but also for multiple views of discourse structures, such as Constituent, Nuclearity, and Relation. Braud et al. (2017) used multilingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the"
2021.naacl-main.127,P14-1002,0,0.199435,"(3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes cal"
2021.naacl-main.127,C16-1245,0,0.0171105,"ingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the performance for infrequent relation labels. However, the method failed to improve the overall Relation score, while they did not aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named"
2021.naacl-main.127,P13-1048,0,0.0360339,"ubtrees to handle large-scale data. We first pre-train the student parser by using the obtained silver data. We then fine-tune parameters of the parser on gold data, using the RST-DT. Experimental results on the RST-DT clearly indicate the effectiveness of our silver data. Our method obtained remarkable Nuclearity and Relation F1 scores of 75.0 and 63.2, respectively. 2 Related Work Early studies on RST parsing were based on traditional supervised learning methods with handcrafted features and the shift-reduce or CKY-like parsing algorithms (duVerle and Prendinger, 2009; Feng and Hirst, 2012; Joty et al., 2013, 2015; Feng and Hirst, 2014). Recently, Wang et al. (2017b) proposed a shift-reduce parser based on SVMs and achieved the current best results in classical statistical models on the RST-DT. The method first built nuclearity-labeled RST trees and then assigned relation labels between two adjacent spans consisting of a single or multiple EDUs. Inspired by the success of neural networks in many NLP tasks, several neural network-based models have been proposed for RST parsing (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017). Yu et al. (2018) proposed a shift-reduce parser b"
2021.naacl-main.127,J15-3002,0,0.0619835,"Missing"
2021.naacl-main.127,W04-3250,0,0.109255,"ur model is statistically significantly better than underlined scores at p-level &lt; 0.01 in pairwise comparison.8 cannot handle, by acquiring training instances with the help of the teacher parser. 5.4 5.3 S 80 Comparison with state-of-the-art parsers Detailed Analysis of Relation Labeling Finally, we compare our SBP+AST with the enTo investigate the effectiveness of SBP+AST in semble to current state-of-the-art parsers. Table more detail, we show Relation F1 scores for re- 3 shows the micro-averaged F scores. We used 1 lation labels with SBP, SBP+AST, and the two- Paired Bootstrap Resampling (Koehn, 2004) for stage parser in Figure 4. The results of SBP and the significance test. We can see that our method SBP+AST were obtained from a five-model ensem- achieved the best scores except for Span. The gains ble. In most relation labels, since the two-stage against the previous best scores were 0.4, 3.0, and parser, the teacher parser, is comparable or supe- 2.7 points for Nuclearity, Relation, and Full, rerior to SBP, i.e., the student parser, the performance spectively. In particular, the gains for Relation and of SBP+AST can be improved. It finally outper- Full are remarkable. formed the two-sta"
2021.naacl-main.127,D14-1220,0,0.375346,"tion for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data."
2021.naacl-main.127,D16-1035,0,0.0452543,"Missing"
2021.naacl-main.127,P14-1043,0,0.253332,"tion for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data."
2021.naacl-main.127,P19-1410,0,0.06264,"y using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a hu"
2021.naacl-main.127,D17-1133,0,0.0881458,"ans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data. Among them, one of the studie"
2021.naacl-main.127,N06-1020,0,0.169545,"fferent parsers that rely on different parsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On t"
2021.naacl-main.127,D17-1136,0,0.0133782,"trees and evaluated system results with RST-DT; this setting is without any silver data. micro-averaged F1 scores of Span, Nuclearity, ReWith AST as the silver data, performance in all lation, and Full, based on RST-Parseval (Marcu, metrics improved against the baseline. In most met2000). Span, Nuclearity, Relation, and Full were rics, AST achieved the best scores. In particular, used to evaluate unlabeled, nuclearity-labeled, the gains in Relation and Full were impressive. DT relation-labeled, and fully-labeled tree structures, and ADT, which consist of document-level RST respectively. Since Morey et al. (2017) made a trees, also outperformed the baseline. However, the suggestion to use a standard parseEval toolkit for gains against the baseline were smaller than those evaluation, we also report the results using this in by AST. We believe this is related to the size and Appendix C. quality of the silver data. The number of trees and nodes in ADT is only 2,142 and 57,940, respec4.4 Compared Methods tively, while AST has 175,709 trees and 2,279,275 To demonstrate the effectiveness of our proposed nodes. Thus, a small number of silver data for method, we pre-trained the span-based neural top- pre-trai"
2021.naacl-main.127,2020.tacl-1.15,0,0.0140569,"le-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the performance for infrequent relation labels. However, the method failed to improve the overall Relation score, while they did not aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named data diversification: it diversifies the training data by using multiple forward and backward translation models. We can find some weak supervision approaches for other discourse representation formalisms such as (Badene et al., 2019). Teacher parsers Autoparsed trees Ptch=1 Ptch=k Training Unlabeled Documents CNN … … Gold data: RST-DT Autoparsed trees Silver data: Agreement subt"
2021.naacl-main.127,W14-6105,0,0.0180731,"arsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, recent neural models d"
2021.naacl-main.127,W05-1513,0,0.0873679,"Missing"
2021.naacl-main.127,P16-1009,0,0.215174,"e attribution of a text span, i.e., nucleus (N) or satellite (S). A discourse relation is also assigned between two adjacent non-terminal nodes. 1 We can find some exceptions for other languages such In most cases, RST parsers have been devel- as Spanish (da Cunha et al., 2011) and German (Stede and oped on the basis of supervised learning algorithms Neumann, 2014). 1600 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1600–1612 June 6–11, 2021. ©2021 Association for Computational Linguistics (Sennrich et al., 2016) introduced a simple learning framework: first pre-train an NMT model with silver data, i.e., pseudo-parallel data generated by automatic back-translation, and then fine-tune it with gold data, i.e., real parallel data, to overcome the data sparseness problem. Since the frameworks successfully improved the NMT systems, it has become a standard approach. Inspired by the above research, we propose a method for improving a student neural parser by exploiting large-scale silver data, thus generating RST trees using an automatic RST parser.2 Specifically, we improve the state-of-the-art neural RST"
2021.naacl-main.127,P17-2029,0,0.258284,"We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for de"
2021.naacl-main.127,D18-1116,0,0.0479865,"Missing"
2021.naacl-main.127,P15-1032,0,0.0260396,"that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, recent neural models do not necessarily ne"
2021.naacl-main.127,W15-2201,0,0.0218308,"ly on different parsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, re"
2021.naacl-main.127,C18-1047,0,0.215639,"le silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, i"
2021.naacl-main.127,2020.acl-main.569,0,0.0432201,"-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of docu"
2021.naacl-main.127,stede-neumann-2014-potsdam,0,0.0608475,"Missing"
2021.naacl-main.127,P17-2041,0,0.347521,"We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for de"
C02-1053,P01-1041,1,0.897899,"Missing"
C02-1053,W00-1303,1,0.435633,"t summarization. Aone et al. (1998) and Kupiec et al. (1995) employed Bayesian classiﬁers, Mani et al. (1998), Nomoto et al. (1997), Lin (1999), and Okumura et al. (1999) used decision tree learning. However, most machine learning methods overﬁt the training data when many features are given. Therefore, we need to select features carefully. Support Vector Machines (SVMs) (Vapnik, 1995) is robust even when the number of features is large. Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000). In this paper, we present an important sentence extraction technique based on SVMs. We veriﬁed the technique against the Text Summarization Challenge (TSC) (Fukushima and Okumura, 2001) corpus. 2 Important Sentence Extraction based on Support Vector Machines 2.1 Support Vector Machines (SVMs) SVM is a supervised learning algorithm for 2class problems. Training data is given by (x1 , y1 ), · · · , (xu , yu ), xj ∈ Rn , yj ∈ {+1, −1}. Here, xj is a feature vector of the j-th sample; yj is its class label, positive(+1) or negative(−1). SVM separates positive and negative examples by a hyperplan"
C02-1053,N01-1025,1,0.881428,"learning has attracted attention in the ﬁeld of automatic text summarization. Aone et al. (1998) and Kupiec et al. (1995) employed Bayesian classiﬁers, Mani et al. (1998), Nomoto et al. (1997), Lin (1999), and Okumura et al. (1999) used decision tree learning. However, most machine learning methods overﬁt the training data when many features are given. Therefore, we need to select features carefully. Support Vector Machines (SVMs) (Vapnik, 1995) is robust even when the number of features is large. Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000). In this paper, we present an important sentence extraction technique based on SVMs. We veriﬁed the technique against the Text Summarization Challenge (TSC) (Fukushima and Okumura, 2001) corpus. 2 Important Sentence Extraction based on Support Vector Machines 2.1 Support Vector Machines (SVMs) SVM is a supervised learning algorithm for 2class problems. Training data is given by (x1 , y1 ), · · · , (xu , yu ), xj ∈ Rn , yj ∈ {+1, −1}. Here, xj is a feature vector of the j-th sample; yj is its class label, positive(+1) or negative(−1"
C02-1053,C00-2167,0,0.0456631,"Missing"
C02-1053,C96-2166,0,0.0378212,"r Machines Important sentence extraction can be regarded as a two-class problem: important or unimportant. However, the proportion of important sentences in training data will diﬀer from that in the test data. The number of important sentences in a document is determined by a summarization rate that is given at run-time. A simple solution for this problem is to rank sentences in a document. We use g(x) the distance from the hyperplane to x to rank the sentences. 2.3 Features We deﬁne the boolean features discussed below that are associated with sentence Si by taking past studies into account (Zechner, 1996; Nobata et al., 2001; Hirao et al., 2001; Nomoto and Matsumoto, 1997). We use 410 boolean variables for each Si . Where x = (x[1], · · ·, x[410]). A real-valued feature normalized between 0 and 1 is represented by 10 boolean variables. Each variable corresponds to an internal [i/10,(i + 1)/10) where i = 0 to 9. For example, Posd = 0.75 is represented by “0000000100” because 0.75 belongs to [7/10,8/10). Position of sentences We deﬁne three feature functions for the position of Si . First, Lead is a boolean that corresponds to the output of the lead-based method described below1 . Second, Posd"
C02-1053,P98-1009,0,\N,Missing
C02-1053,C98-1009,0,\N,Missing
C04-1040,A00-2018,0,0.0187229,"th a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust. In Natural Language Processing, we use tens of thousands of words as features. Therefore, SVM often gives good performance. However, the accuracy of Yamada’s analyzer is lower than state-of-the-art phrase structure parsers such as Charniak’s Maximum-Entropy-Inspired Parser (MEIP) (Charniak, 2000) and Collins’ Model 3 parser. One reason is the lack of top-down information that is available in phrase structure parsers. In this paper, we show that the accuracy of the word dependency parser can be improved by adding a base-NP chunker, a Root-Node Finder, and a Prepositional Phrase (PP) Attachment Resolver. We introduce the base-NP chunker because base NPs are important components of a sentence and can be easily annotated. Since most words are contained in a base NP or are adjacent to a base NP, we expect that the introduction of base NPs will improve accuracy. We introduce the Root-Node F"
C04-1040,P02-1034,0,0.03485,"Missing"
C04-1040,P97-1003,0,0.0190629,"method based on an ‘IOB2’ chunking scheme. By the chunking, each word is tagged as – B: Beginning of a base NP, – I: Other elements of a base NP. – O: Otherwise. Please see Kudo’s paper for more details. • A Root-Node Finder (RNF): We will describe this later. • A Dependency Analyzer: It works just like Yamada’s Dependency Analyzer. • A PP-Attatchment Resolver (PPAR): This resolver improves the dependency accuracy of prepositions whose part-of-speech tags are IN or TO. The above procedures require a part-of-speech tagger. Here, we extract part-of-speech tags from the Collins parser’s output (Collins, 1997) for section 23 instead of reinventing a tagger. According to the document, it is the output of Ratnaparkhi’s tagger (Ratnaparkhi, 1996). Figure 2 shows the architecture of the system. PPAR’s output is used to rewrite the output of the Dependency Analyzer. 2.1 Finding root nodes When we use SVM, we regard root-node finding as a classification task: Root nodes are positive examples and other words are negative examples. For this classification, each word w i in a tagged sentence T = (w1 /p1 , . . . , wi /pi , . . . , wN /pN ) is characterized by a set of features. Since the given POS tags are s"
C04-1040,C96-1058,0,0.033018,"state-of-the-art phrase structure parsers because of the lack of top-down information given by phrase labels. This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver. Experimental results show that these modules based on Preference Learning give better scores than Collins’ Model 3 parser for these subproblems. We expect this method is also applicable to phrase structure parsers. 1 Introduction 1.1 Dependency Analysis Word dependency is important in parsing technology. Figure 1 shows a word dependency tree. Eisner (1996) proposed probabilistic models of dependency parsing. Collins (1999) used dependency analysis for phrase structure parsing. It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002). However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsu"
C04-1040,P02-1043,0,0.0136622,"and a Prepositional-Phrase Attachment Resolver. Experimental results show that these modules based on Preference Learning give better scores than Collins’ Model 3 parser for these subproblems. We expect this method is also applicable to phrase structure parsers. 1 Introduction 1.1 Dependency Analysis Word dependency is important in parsing technology. Figure 1 shows a word dependency tree. Eisner (1996) proposed probabilistic models of dependency parsing. Collins (1999) used dependency analysis for phrase structure parsing. It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002). However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown g"
C04-1040,C02-1054,1,0.892014,"that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown good performance in difHe girl with a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust. In Natural Language Processing, we use tens of thousands of words as features. Therefore, SVM often gives good performance. However, the accuracy of Yamada’s analyzer is lower than state-of-the-art phrase structure parsers such as Charniak’s Maximum-Entropy-Inspired Parser (MEIP) (Charniak, 2000) and Collins’ Model 3 parser. One reason is the lack of top-down information that is available in phrase structure parsers. In this paper, we show that the accurac"
C04-1040,N01-1025,0,0.173353,"udied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown good performance in difHe girl with a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust. In Natural Language Processing, we use tens of thousands of words as features. Therefore, SVM often gives good performance. However, the accuracy of Yamada’s analyzer is lower than state-of-the-art phrase structure parsers such as Charniak’s Maximum-Entropy-Inspired Parser (MEIP) (Charniak, 2000) and Collins’ Model 3 parser. One reason is the lack of top-down information that is available in phrase structure parsers. In this pape"
C04-1040,W02-2016,0,0.170951,"Missing"
C04-1040,J93-2004,0,0.0295554,"Missing"
C04-1040,W03-0402,0,0.0507802,"Missing"
C04-1040,P03-1029,0,0.0272678,"1 shows a word dependency tree. Eisner (1996) proposed probabilistic models of dependency parsing. Collins (1999) used dependency analysis for phrase structure parsing. It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002). However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown good performance in difHe girl with a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM is relatively robust"
C04-1040,P03-1005,1,0.81524,"g technology. Figure 1 shows a word dependency tree. Eisner (1996) proposed probabilistic models of dependency parsing. Collins (1999) used dependency analysis for phrase structure parsing. It is also studied by other researchers (Sleator and Temperley, 1991; Hockenmaier and Steedman, 2002). However, statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied. Recent studies show that Information Extraction (IE) and Question Answering (QA) benefit from word dependency analysis without phrase labels. (Suzuki et al., 2003; Sudo et al., 2003) Recently, Yamada and Matsumoto (2003) proposed a trainable English word dependency analyzer based on Support Vector Machines (SVM). They did not use phrase labels by considering annotation of documents in expert domains. SVM (Vapnik, 1995) has shown good performance in difHe girl with a telescope a He saw a girl with a telescope. Figure 1: A word dependency tree ferent tasks of Natural Language Processing (Kudo and Matsumoto, 2001; Isozaki and Kazawa, 2002). Most machine learning methods do not work well when the number of given features (dimensionality) is large, but SVM"
C04-1040,W96-0213,0,\N,Missing
C04-1040,J03-4003,0,\N,Missing
C04-1064,W99-0625,0,\N,Missing
C04-1064,J97-1003,0,\N,Missing
C04-1064,W02-2016,0,\N,Missing
C04-1064,W03-1004,0,\N,Missing
C04-1064,P02-1040,0,\N,Missing
C04-1064,W03-0507,0,\N,Missing
C04-1064,P03-1005,1,\N,Missing
C04-1077,C00-1072,0,0.108816,"Missing"
C04-1077,W03-0507,1,0.820221,"Missing"
C04-1077,P03-1048,0,0.0185685,"at we need other important techniques such as those for maintaining the consistency of words and phrases that refer to the same object, and for making the results more readable; however, they are not included here. fixed number, the situation was the same as TSC2. At DUC 2002, extracts (important sentences) were used, and this allowed us to evaluate sentence extraction. However, it is not possible to measure the effectiveness of redundant sentences reduction since the corpus was not annotated to show sentence with same content. In addition, this is the same even if we use the SummBank corpus (Radev et al., 2003). In any case, because many of the current summarization systems for multiple documents are based on sentence extraction, we believe these corpora to be unsuitable as sets of documents for evaluation. On this basis, in TSC3, we assumed that the process of multiple document summarization consists of the following three steps, and we produce a corpus for the evaluation of the system at each of the three steps4 . Step 1 Extract important sentences from a given set of documents Step 2 Minimize redundant sentences from the result of Step 1 Step 3 Rewrite the result of Step 2 to reduce the size of t"
C04-1077,W97-0710,0,0.0537484,"set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. (a) (b) (c) (d) Doc. x Mainichi articles abstract Doc. y Yomiuri articles Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natural to identify important segments in the document set and then produce summaries by combining and rephrasing such information than to select important sentences and revise them as summaries. Therefore, we believe that second type of extract is superior and thus we prepared the extracts in that way. However, as stated in the previous section, with multiple document summarization, there may be more than one sentence with the same content, and thus we may have more than one set of sentences in"
C04-1077,C96-2166,0,0.206176,"tomatic evaluation using a scoring program. We adopted an intrinsic evaluation by human judges for step 3, which is currently under evaluation. We provide details of the extracts prepared for steps 1 and 2 and their evaluation measures in the following sections. We do not report the overall evaluation results for TSC3. 2.2 Data Preparation for Sentence Extraction We begin with guidelines for annotating important sentences (extracts). We think that there are two kinds of extract. 1. A set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. (a) (b) (c) (d) Doc. x Mainichi articles abstract Doc. y Yomiuri articles Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natu"
C12-2087,W97-0703,0,0.258096,"the frequency distribution of content word occurrence. Obviously, some of words occur more than once in the document. The study of 2 http://lpsolve.sourceforge.net/ http://lr‐www.pi.titech.ac.jp/tsc/tsc3‐en.html http://mainichi.jp/ 5 P < 0.01 3 4 899 text coherence evaluation leverages this repetition to capture the coherence (Barzilay and Lapata, 2005); to make a text coherent, sometimes the same words are used in two successive sentences. In the context of automatic text summarization research, this repetition is referred to as Lexical Chain and can be leveraged to find important sentences (Barzilay and Elhadad, 1997). While MCM considers these repetitions as redundant information, RCKM can permit some redundancy in the summary. In view of this, redundancy parameter r can be estimated from the aspect of text coherence. Word 10000 1000 100 10 1 1 2 3 4 5 6 7 Frequency of occurrence 8 9 &gt;=10 Figure 1: Frequency distribution of content word occurrence in the references. The horizontal axis indicates the frequency of content word occurrence in one reference; the vertical axis indicates the number of words. For example, there are 2093 words that occur once in one reference; there are 10 words that occur more th"
C12-2087,P05-1018,0,0.0292204,"between KM and other methods are also significant. One reason for the success of the proposal is that the references usually contain some redundant information units. Interestingly, reference summaries contain two or more instances of the same word. In Figure 1, we show the frequency distribution of content word occurrence. Obviously, some of words occur more than once in the document. The study of 2 http://lpsolve.sourceforge.net/ http://lr‐www.pi.titech.ac.jp/tsc/tsc3‐en.html http://mainichi.jp/ 5 P < 0.01 3 4 899 text coherence evaluation leverages this repetition to capture the coherence (Barzilay and Lapata, 2005); to make a text coherent, sometimes the same words are used in two successive sentences. In the context of automatic text summarization research, this repetition is referred to as Lexical Chain and can be leveraged to find important sentences (Barzilay and Elhadad, 1997). While MCM considers these repetitions as redundant information, RCKM can permit some redundancy in the summary. In view of this, redundancy parameter r can be estimated from the aspect of text coherence. Word 10000 1000 100 10 1 1 2 3 4 5 6 7 Frequency of occurrence 8 9 &gt;=10 Figure 1: Frequency distribution of content word o"
C12-2087,D11-1003,0,0.0263095,"l is solved again by dynamic programming and the redundancy in the summary will be reduced (we detail our algorithm in the The redundancy‐constrained knapsack problem can also be solved in pseudo‐polynomial time. However its ∏ runtime is O , which is in effect exponential time. 1 896 next section). The Lagrange multipliers are calculated by solving the Lagrange dual problem of using the subgradient method. Constraint (9) is an min max inequality constraint, so an optimal solution on the model can’t be found unlike dependency parsing (Koo and Collins, 2010) and statistical machine translation (Chang and Collins, 2011), but an approximate solution can, however, be found by the decoding algorithm proposed below. 4 Decoding with Lagrange heuristic We propose the following algorithm to find an approximate solution on objective function (11) in Algorithm 1. We outline our decoding algorithm below. (1). Let all Lagrange multipliers λ be 0. (2). Iterate following steps T times. A) Find the optimal solution on objective function (11) by dynamic programming. B) If the solution by (A) satisfies all constraints, return the solution. If not, use the heuristic to find a feasible solution from the optimal solution by (A"
C12-2087,D07-1001,0,0.0281846,"as three reference summaries. Detailed information of the corpus is shown in (Hirao et al, 2004). 5.2 Parameter settings We set the three essential parameters as follows: y y y α: We set α as the inverse of the number of times that Lagrange multipliers have been updated. r: The allowed redundancy rj can be set for each information unit j. We set rj = tf where is the tfj is the number of information units, j, contained in the input document set and floor function. w: we simply set j as a content word, and weight wj based on tf-idf (Filatova and . N and dfj are the total Hatzivassiloglou, 2004; Clarke and Lapata, 2007), tf log number of documents and the number of documents containing word j in the corpus, respectively. They are calculated from the Mainichi Shimbun corpora4 2003 and 2004. α is used only by RCKM-LH. r is used by RCKM and RCKM-LH. w is used by all methods. Although r and w are can be estimated in a more sophisticated fashion such as the supervised approach, in this paper we simply estimate these parameters from just the input documents, i.e. the unsupervised approach. The use of the supervised approach is a future topic. 5.3 Results and Discussions We show the results of the ROUGE evaluation"
C12-2087,C04-1057,0,0.18345,"show that our proposals outperform a state-of-the-art text summarization model, the maximum coverage model, in finding the optimal solution. We also show that our decoding method quickly finds a good approximate solution comparable to the optimal solution of the maximum coverage model. KEYWORDS: Text summarization, Knapsack problem, Maximum coverage problem, Lagrange heuristics. Proceedings of COLING 2012: Posters, pages 893–902, COLING 2012, Mumbai, December 2012. 893 1 Introduction Many text summarization studies in recent years formulate text summarization as the maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Takamura and Okumura, 2009; Gillick and Favre, 2009; Nishikawa et al., 2010; Higashinaka et al., 2010). The maximum coverage model, based on the maximum coverage problem, generates a summary by selecting sentences to cover as many information units (such as unigrams and bigrams) as possible. Takamura and Okumura (2009) and Gillick and Favre (2009) demonstrated that the maximum coverage problem offers great performance as a text summarization model. Unfortunately, its potential is hindered by the fact that it is NP-hard (Khuller et al., 1999). There is little hope that a pol"
C12-2087,W09-1802,0,0.104881,"e maximum coverage model, in finding the optimal solution. We also show that our decoding method quickly finds a good approximate solution comparable to the optimal solution of the maximum coverage model. KEYWORDS: Text summarization, Knapsack problem, Maximum coverage problem, Lagrange heuristics. Proceedings of COLING 2012: Posters, pages 893–902, COLING 2012, Mumbai, December 2012. 893 1 Introduction Many text summarization studies in recent years formulate text summarization as the maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Takamura and Okumura, 2009; Gillick and Favre, 2009; Nishikawa et al., 2010; Higashinaka et al., 2010). The maximum coverage model, based on the maximum coverage problem, generates a summary by selecting sentences to cover as many information units (such as unigrams and bigrams) as possible. Takamura and Okumura (2009) and Gillick and Favre (2009) demonstrated that the maximum coverage problem offers great performance as a text summarization model. Unfortunately, its potential is hindered by the fact that it is NP-hard (Khuller et al., 1999). There is little hope that a polynomial time algorithm for the problem exists. Another theoretical fram"
C12-2087,C04-1077,1,0.765281,"proposed method. Find the optimal solution of Equation (11) using lp_solve2 solver. (2). Redundancy-constrained knapsack model with the Lagrange heuristic (RCKM-LH): Our proposed method. Find the approximate solution of Equation (11) by our proposed algorithm shown in Algorithm 1. We evaluate the proposed algorithm with 10 iterations (T = 10) and 100 (T = 100) iterations. (3). Maximum coverage model (MCM): Baseline. Find the optimal solution using lp_solve. (4). Knapsack model (KM): Baseline. Find the optimal solution using the algorithm shown in Algorithm 2. 5.1 Data We use the TSC-3 corpus (Hirao et al., 2004) for evaluation. It is an evaluation corpus for multidocument summarization and was used in Text Summarization Challenge 3 3 . It contains 30 Japanese news article sets, 352 articles and 3587 sentences. Each set has three reference summaries. Detailed information of the corpus is shown in (Hirao et al, 2004). 5.2 Parameter settings We set the three essential parameters as follows: y y y α: We set α as the inverse of the number of times that Lagrange multipliers have been updated. r: The allowed redundancy rj can be set for each information unit j. We set rj = tf where is the tfj is the number"
C12-2087,P10-1001,0,0.0351548,"its, j, contained in the summary will decrease when the model is solved again by dynamic programming and the redundancy in the summary will be reduced (we detail our algorithm in the The redundancy‐constrained knapsack problem can also be solved in pseudo‐polynomial time. However its ∏ runtime is O , which is in effect exponential time. 1 896 next section). The Lagrange multipliers are calculated by solving the Lagrange dual problem of using the subgradient method. Constraint (9) is an min max inequality constraint, so an optimal solution on the model can’t be found unlike dependency parsing (Koo and Collins, 2010) and statistical machine translation (Chang and Collins, 2011), but an approximate solution can, however, be found by the decoding algorithm proposed below. 4 Decoding with Lagrange heuristic We propose the following algorithm to find an approximate solution on objective function (11) in Algorithm 1. We outline our decoding algorithm below. (1). Let all Lagrange multipliers λ be 0. (2). Iterate following steps T times. A) Find the optimal solution on objective function (11) by dynamic programming. B) If the solution by (A) satisfies all constraints, return the solution. If not, use the heurist"
C12-2087,W04-1013,0,0.0430201,"s a feasible solution from the relaxed, infeasible solution induced by Lagrange relaxation. It is known to be effective in finding good approximate solutions for the set covering problem (Haddadi, 1997). We present the novelty and contribution of this paper as follows: y In this paper we define a novel objective function and decoding algorithm for multi-document summarization. The model and algorithm presented in this paper are new in the context of automatic summarization research. y Our proposal, the redundancy-constrained knapsack model, outperforms the maximum coverage model on the ROUGE (Lin, 2004) evaluation. y The approximate solution of our proposed model, found by our proposed decoding method, is comparable with the optimal solution of the maximum coverage model. We also show that this approximate solution is found far faster than the optimal solution of the maximum coverage model. This paper is organized as follows. In Section 2, we describe related work. In Section 3, we elaborate our proposed model. In Section 4, we explain the algorithm that finds a good approximate solution for our proposed model. In Section 5, we show results of experiments conducted to evaluate our proposal."
C12-2087,C10-2105,1,0.808584,"in finding the optimal solution. We also show that our decoding method quickly finds a good approximate solution comparable to the optimal solution of the maximum coverage model. KEYWORDS: Text summarization, Knapsack problem, Maximum coverage problem, Lagrange heuristics. Proceedings of COLING 2012: Posters, pages 893–902, COLING 2012, Mumbai, December 2012. 893 1 Introduction Many text summarization studies in recent years formulate text summarization as the maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Takamura and Okumura, 2009; Gillick and Favre, 2009; Nishikawa et al., 2010; Higashinaka et al., 2010). The maximum coverage model, based on the maximum coverage problem, generates a summary by selecting sentences to cover as many information units (such as unigrams and bigrams) as possible. Takamura and Okumura (2009) and Gillick and Favre (2009) demonstrated that the maximum coverage problem offers great performance as a text summarization model. Unfortunately, its potential is hindered by the fact that it is NP-hard (Khuller et al., 1999). There is little hope that a polynomial time algorithm for the problem exists. Another theoretical framework for text summariza"
C12-2087,E09-1089,0,0.0788671,"text summarization model, the maximum coverage model, in finding the optimal solution. We also show that our decoding method quickly finds a good approximate solution comparable to the optimal solution of the maximum coverage model. KEYWORDS: Text summarization, Knapsack problem, Maximum coverage problem, Lagrange heuristics. Proceedings of COLING 2012: Posters, pages 893–902, COLING 2012, Mumbai, December 2012. 893 1 Introduction Many text summarization studies in recent years formulate text summarization as the maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Takamura and Okumura, 2009; Gillick and Favre, 2009; Nishikawa et al., 2010; Higashinaka et al., 2010). The maximum coverage model, based on the maximum coverage problem, generates a summary by selecting sentences to cover as many information units (such as unigrams and bigrams) as possible. Takamura and Okumura (2009) and Gillick and Favre (2009) demonstrated that the maximum coverage problem offers great performance as a text summarization model. Unfortunately, its potential is hindered by the fact that it is NP-hard (Khuller et al., 1999). There is little hope that a polynomial time algorithm for the problem exists."
C12-2087,J08-1001,0,\N,Missing
C14-1156,P04-1051,0,0.032985,"ngth directly. This deficiency is problematic because in practical usage the maximum length of a summary is specified by the user; hence, the summarizer should be able to control output length. In contrast to their method, our approach naturally takes the maximum summary length into account when summarizing a document. 2.2 Coherence In the context of multi-document summarization, coherence has been studied widely. In multi-document summarization, sentences are selected from different documents, and hence some way of ordering the sentences is required. Sentence ordering (Barzilay et al., 2002; Althaus et al., 2004; Karamanis et al., 1 As an interesting related work, Clarke and Lapata (2007) compresses documents by making use of Centering Theory (Grosz et al., 1995). However, in their approach, the desired length of an output summary could not be specified and hence they said their method was compression rather than summarization. 1649 Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting of 10 symbols o1 ...o10 over time t1 ...t10 and transitions between states s1 ...s3 . Unlike the basic hidden Markov model, states can persist for a non-unit length. In this fi"
C14-1156,W97-0703,0,0.329492,"rence score is defined between two sentences. 2 As we explain later in Section 5, computation complexity of our algorithm is pseudo-polynomial, and hence the best solution of our model can be located quickly. This is also advantageous in the learning phase because to learn parameters using structured learning, the learner has to generate a summary to calculate the loss. Since our algorithm can quickly find the best solution and generate a summary, it can also contribute to shortening the time required for learning. 3 It is expected that this feature will also contribute to sentence selection. Barzilay and Elhadad (1997) showed that a closely related word-pair was a good indicator for sentence selection. This feature captures this property by learning. 1652 0.7 16000 0.69 14000 0.68 12000 0.67 ROUGE-2 The number of sentences 18000 10000 8000 6000 0.66 0.65 0.64 0.63 4000 0.62 2000 0 0.61 0 5 10 15 20 Levenshtein distance 25 30 0 Figure 2: Distribution of Levenshtein distance in the aligned sentences. Among the 36,413 sentences in the references, 16,643 were identical (Levenshtein distance is 0) to the aligned sentences in the input documents. 4 0.6 2000 4000 6000 8000 10000 The number of training samples Figu"
C14-1156,P05-1018,0,0.416293,"en providing summaries of articles in their websites to attract users, and Summly, which has been acquired by Yahoo!, provided the service of automatically summarizing articles on the Internet. Given the cost of manual summarization, we can greatly improve the information access of Internet users by creating an automatic summarizer that can approach the summarization quality of humans. To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown, 2011). Although coherence has been studied widely in a field of multi-document summarization (Karamanis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not been studied enough in the context of single-document summarization. In this paper, we revisit the problem of coherence and employ it to produce both informative and linguistically high-quality summaries. To obtain such summaries, we introduce a novel summarization method based on a hidden semiMarkov model. The method has the properties of both the popular single-document summarization model, the knapsack problem, which packs the sentences into the given length and the hidden Markov model, which takes summary coherence into accoun"
C14-1156,N04-1015,0,0.107683,"as large as possible but the total length is less than or equal to a given maximum summary length. Interestingly, a hidden semi-Markov model (Yu, 2010) can be regarded as a natural extension of the knapsack problem, we take advantage of this property for single-document summarization. We elaborate the relation between the knapsack problem and the hidden semi-Markov model in Section 3. To generate coherent summaries in single-document summarization, there are two types of approaches1 : tree-based approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) and sequence-based approaches (Barzilay and Lee, 2004; Shen et al., 2007). The former rely on the tree representation of a document based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Basically, the former approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) trim the tree representation of a document by making use of nucleus-satellite relations among sentences. The advantage of RST-based approaches is that they can take advantage of global information about the documents. However, a drawback is that they depend heavily on the RST parser that is used. Performance is remarkably sensitive to the accuracy of RST"
C14-1156,N13-1136,0,0.162436,"s to attract users, and Summly, which has been acquired by Yahoo!, provided the service of automatically summarizing articles on the Internet. Given the cost of manual summarization, we can greatly improve the information access of Internet users by creating an automatic summarizer that can approach the summarization quality of humans. To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown, 2011). Although coherence has been studied widely in a field of multi-document summarization (Karamanis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not been studied enough in the context of single-document summarization. In this paper, we revisit the problem of coherence and employ it to produce both informative and linguistically high-quality summaries. To obtain such summaries, we introduce a novel summarization method based on a hidden semiMarkov model. The method has the properties of both the popular single-document summarization model, the knapsack problem, which packs the sentences into the given length and the hidden Markov model, which takes summary coherence into account by determining sentence context when selecting se"
C14-1156,D07-1001,0,0.0249884,"he maximum length of a summary is specified by the user; hence, the summarizer should be able to control output length. In contrast to their method, our approach naturally takes the maximum summary length into account when summarizing a document. 2.2 Coherence In the context of multi-document summarization, coherence has been studied widely. In multi-document summarization, sentences are selected from different documents, and hence some way of ordering the sentences is required. Sentence ordering (Barzilay et al., 2002; Althaus et al., 2004; Karamanis et al., 1 As an interesting related work, Clarke and Lapata (2007) compresses documents by making use of Centering Theory (Grosz et al., 1995). However, in their approach, the desired length of an output summary could not be specified and hence they said their method was compression rather than summarization. 1649 Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting of 10 symbols o1 ...o10 over time t1 ...t10 and transitions between states s1 ...s3 . Unlike the basic hidden Markov model, states can persist for a non-unit length. In this figure, state s2 and state s3 persist for non-unit lengths. Hence, the system tr"
C14-1156,W02-1001,0,0.0366617,"lihood. Therefore, the hidden semi-Markov can naturally be used for single-document summarization. Suppose that the document to be summarized consists of 10 sentences and the length of each of them is measured by the number of words. In this case, the system transitions over 10 states corresponding to the 10 sentences until it cannot select any further sentence due to the given length requirement. Since each state persists for the length of the corresponding sentence, the remaining length decreases every time the system transitions to a new state. While an HMM is basically a generative model, Collins (2002) extended it to create a discriminative model. An HSMM can also be extended to become discriminative model (Sarawagi and Cohen, 2004). Our discriminative HSMM learns through the application of max-margin training. 3.3 Formulation We consider there are n input sentences s1 , s2 , ..., sn . These sentences have lengths ℓ1 , ℓ2 , ..., ℓn and weights w1 , w2 , ..., wn . We assume that a sentence that has a high weight should be present in the output summary. We also consider each sentence, si , has mi variants si,1 , si,2 , ..., si,m , each produced by some sort of sentence compression or paraphra"
C14-1156,P02-1057,0,0.378495,"Missing"
C14-1156,D13-1155,0,0.0427912,"Missing"
C14-1156,P98-1068,0,0.0868294,"in a Japanese newspaper company and the company sold these summaries for commercial purposes. 1654 We list the statistics of our corpus in Table 1. As shown, the task is to summarize the document in about a third of its original length in terms of the number of words. 6.2 Evaluation Criteria ROUGE; ROUGE is an automatic evaluation method for automatic summarization proposed by Lin (2004). We used ROUGE-1 and ROUGE-2 to evaluate the summaries. Since our document-reference pairs are written in Japanese, we segmented the sentences into words using the Japanese morphological analyzer developed by Fuchi and Takagi (1998). When calculating the ROUGE score, we used only content words (i.e. nouns, verbs and adjectives) and so excluded function words as stop words. Linguistic Quality: To evaluate the linguistic quality of the summaries generated by our method, we performed a manual evaluation according to quality questions proposed by the National Institute of Standards and Technology (NIST) (2007)5 . We randomly sampled 100 summaries from the outputs of each method described below and asked 7 subjects to evaluate the summaries according to the questions. All subjects were Japanese native and none were among the"
C14-1156,J95-2003,0,0.122007,"ould be able to control output length. In contrast to their method, our approach naturally takes the maximum summary length into account when summarizing a document. 2.2 Coherence In the context of multi-document summarization, coherence has been studied widely. In multi-document summarization, sentences are selected from different documents, and hence some way of ordering the sentences is required. Sentence ordering (Barzilay et al., 2002; Althaus et al., 2004; Karamanis et al., 1 As an interesting related work, Clarke and Lapata (2007) compresses documents by making use of Centering Theory (Grosz et al., 1995). However, in their approach, the desired length of an output summary could not be specified and hence they said their method was compression rather than summarization. 1649 Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting of 10 symbols o1 ...o10 over time t1 ...t10 and transitions between states s1 ...s3 . Unlike the basic hidden Markov model, states can persist for a non-unit length. In this figure, state s2 and state s3 persist for non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols. 2004; Okazaki et al., 20"
C14-1156,D13-1158,1,0.836255,"cks them into a summary so that the total value is as large as possible but the total length is less than or equal to a given maximum summary length. Interestingly, a hidden semi-Markov model (Yu, 2010) can be regarded as a natural extension of the knapsack problem, we take advantage of this property for single-document summarization. We elaborate the relation between the knapsack problem and the hidden semi-Markov model in Section 3. To generate coherent summaries in single-document summarization, there are two types of approaches1 : tree-based approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) and sequence-based approaches (Barzilay and Lee, 2004; Shen et al., 2007). The former rely on the tree representation of a document based on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). Basically, the former approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) trim the tree representation of a document by making use of nucleus-satellite relations among sentences. The advantage of RST-based approaches is that they can take advantage of global information about the documents. However, a drawback is that they depend heavily on the RST parser that is used. Perfo"
C14-1156,P07-2057,0,0.0241996,"es. 2. Shortening sentences by trimming their dependency trees. Basically this method follows the sentence trimmer proposed by Nomoto (2008). While using his method, we keep the predicate and its obligatory arguments in the sentences to keep the sentences grammatical. If a predicate is trimmed, its obligatory arguments are also trimmed and vice versa. Since there are an exponential number of subtrees in one tree, we use only n-best subtrees by ranking them according to n-gram language likelihood and dependency-based language likelihood. We used the dependency parser proposed by Imamura et al (Imamura et al., 2007) to acquire the dependency tree. 5 Decoding with Dynamic Programming To solve Equation 1 under the constraints of Equation 2, we use dynamic programming. Algorithm 1 shows the pseudo code of the decoding algorithm. Line 1 to Line 7 initializes the variables used in the algorithm. Vector x = ⟨x0 , ..., xn+1 ⟩ stores which sentence and which variants are included in the output summary. If x3 = 2, s3,2 is included in the summary. V , P and S are two-dimensional arrays, each of which is used as a dynamic programming table. They store the process of dynamic programming. 4 Alignment proceeds in two"
C14-1156,P04-1050,0,0.030019,"al Times and CNN have been providing summaries of articles in their websites to attract users, and Summly, which has been acquired by Yahoo!, provided the service of automatically summarizing articles on the Internet. Given the cost of manual summarization, we can greatly improve the information access of Internet users by creating an automatic summarizer that can approach the summarization quality of humans. To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown, 2011). Although coherence has been studied widely in a field of multi-document summarization (Karamanis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not been studied enough in the context of single-document summarization. In this paper, we revisit the problem of coherence and employ it to produce both informative and linguistically high-quality summaries. To obtain such summaries, we introduce a novel summarization method based on a hidden semiMarkov model. The method has the properties of both the popular single-document summarization model, the knapsack problem, which packs the sentences into the given length and the hidden Markov model, which takes su"
C14-1156,P03-1069,0,0.311782,"be specified and hence they said their method was compression rather than summarization. 1649 Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting of 10 symbols o1 ...o10 over time t1 ...t10 and transitions between states s1 ...s3 . Unlike the basic hidden Markov model, states can persist for a non-unit length. In this figure, state s2 and state s3 persist for non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols. 2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence (Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova, 2012). Many effective features have been found out to capture coherence and we utilize these features. Some work proposed a model that could jointly taking the content of the summary and its coherence into account (Nishikawa et al., 2010; Christensen et al., 2013). Since extracted sentences in multidocument summarization must be ordered, a task that is NP-hard, they relied on integer linear programming (Nishikawa et al., 2010) or a local search strategy (Christensen et al., 2013). The former can locate t"
C14-1156,W04-1013,0,0.231725,"let f = ⟨fw , fc ⟩ be a (dw + dc )dimensional feature function for the whole summary and let w = ⟨ww , wc ⟩ be a (dw + dc )-dimensional weight vector. The value that the objective function outputs for summary s is w · f (s). To optimize the parameter, we employ the Passive-Aggressive algorithm (Crammer, 2006), a widelyused structured learning method. Since the algorithm offers online learning, it can learn the parameter quickly and is easy to implement. To learn the parameter so that the output summary is optimized to the evaluation criteria popular in document summarization research, ROUGE (Lin, 2004), we introduce ROUGE as the loss function. The parameter is estimated by solving the following formula iteratively2 : 1 wnew = argmin ||w − wold ||2 2 w (5) s.t. w · f (r) − w · f (s) ≥ loss(s; r), where wnew is the parameter vector after update, wold is the parameter vector before update, r is a reference summary, and loss is the loss function. We define loss as 1 − ROUGE(s; r). Among the variants of ROUGE, we used ROUGE-1 for the loss function. 3.4.1 Sentence Feature The features introduced in this section are used to calculate the weights of sentences, wi,j . Term Frequency: Term frequency"
C14-1156,D12-1106,0,0.049593,"ion. 1649 Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting of 10 symbols o1 ...o10 over time t1 ...t10 and transitions between states s1 ...s3 . Unlike the basic hidden Markov model, states can persist for a non-unit length. In this figure, state s2 and state s3 persist for non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols. 2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence (Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova, 2012). Many effective features have been found out to capture coherence and we utilize these features. Some work proposed a model that could jointly taking the content of the summary and its coherence into account (Nishikawa et al., 2010; Christensen et al., 2013). Since extracted sentences in multidocument summarization must be ordered, a task that is NP-hard, they relied on integer linear programming (Nishikawa et al., 2010) or a local search strategy (Christensen et al., 2013). The former can locate the optimal solution at a heavy computation cost, while the latter runs quickly but there is no g"
C14-1156,P11-5003,0,0.544437,"much more attention as a key technology in providing better information access in a commercial context. The Financial Times and CNN have been providing summaries of articles in their websites to attract users, and Summly, which has been acquired by Yahoo!, provided the service of automatically summarizing articles on the Internet. Given the cost of manual summarization, we can greatly improve the information access of Internet users by creating an automatic summarizer that can approach the summarization quality of humans. To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown, 2011). Although coherence has been studied widely in a field of multi-document summarization (Karamanis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not been studied enough in the context of single-document summarization. In this paper, we revisit the problem of coherence and employ it to produce both informative and linguistically high-quality summaries. To obtain such summaries, we introduce a novel summarization method based on a hidden semiMarkov model. The method has the properties of both the popular single-document summarization model, th"
C14-1156,C10-2105,1,0.917827,"rticles in their websites to attract users, and Summly, which has been acquired by Yahoo!, provided the service of automatically summarizing articles on the Internet. Given the cost of manual summarization, we can greatly improve the information access of Internet users by creating an automatic summarizer that can approach the summarization quality of humans. To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown, 2011). Although coherence has been studied widely in a field of multi-document summarization (Karamanis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not been studied enough in the context of single-document summarization. In this paper, we revisit the problem of coherence and employ it to produce both informative and linguistically high-quality summaries. To obtain such summaries, we introduce a novel summarization method based on a hidden semiMarkov model. The method has the properties of both the popular single-document summarization model, the knapsack problem, which packs the sentences into the given length and the hidden Markov model, which takes summary coherence into account by determining sentenc"
C14-1156,P08-1035,0,0.0188223,"nd can be reproduced by simple operations. Few sentences exhibited paraphrasing or more sophisticated operations. We plot the distribution of Levenshtein distance in the aligned sentences in Figure 2. According to this observation, we produce the following types of variants by sentence compression: 1. Removing information in parentheses. Some sentences contain parentheses containing additional information for readers. The first type of variant deletes text in parentheses. 2. Shortening sentences by trimming their dependency trees. Basically this method follows the sentence trimmer proposed by Nomoto (2008). While using his method, we keep the predicate and its obligatory arguments in the sentences to keep the sentences grammatical. If a predicate is trimmed, its obligatory arguments are also trimmed and vice versa. Since there are an exponential number of subtrees in one tree, we use only n-best subtrees by ranking them according to n-gram language likelihood and dependency-based language likelihood. We used the dependency parser proposed by Imamura et al (Imamura et al., 2007) to acquire the dependency tree. 5 Decoding with Dynamic Programming To solve Equation 1 under the constraints of Equat"
C14-1156,C04-1108,0,0.0391022,"rosz et al., 1995). However, in their approach, the desired length of an output summary could not be specified and hence they said their method was compression rather than summarization. 1649 Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting of 10 symbols o1 ...o10 over time t1 ...t10 and transitions between states s1 ...s3 . Unlike the basic hidden Markov model, states can persist for a non-unit length. In this figure, state s2 and state s3 persist for non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols. 2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence (Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova, 2012). Many effective features have been found out to capture coherence and we utilize these features. Some work proposed a model that could jointly taking the content of the summary and its coherence into account (Nishikawa et al., 2010; Christensen et al., 2013). Since extracted sentences in multidocument summarization must be ordered, a task that is NP-hard, they relied on integer linear programming (Nishikawa et"
C14-1156,P10-1056,0,0.106857,"ather than summarization. 1649 Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting of 10 symbols o1 ...o10 over time t1 ...t10 and transitions between states s1 ...s3 . Unlike the basic hidden Markov model, states can persist for a non-unit length. In this figure, state s2 and state s3 persist for non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols. 2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence (Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova, 2012). Many effective features have been found out to capture coherence and we utilize these features. Some work proposed a model that could jointly taking the content of the summary and its coherence into account (Nishikawa et al., 2010; Christensen et al., 2013). Since extracted sentences in multidocument summarization must be ordered, a task that is NP-hard, they relied on integer linear programming (Nishikawa et al., 2010) or a local search strategy (Christensen et al., 2013). The former can locate the optimal solution at a heavy computation cost, while the latter runs"
C14-1156,C98-1065,0,\N,Missing
C14-1156,W01-0100,0,\N,Missing
C16-1021,J08-1001,0,0.801524,"nguages without much efforts. It also provides inspiration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies"
C16-1021,H01-1065,0,0.106884,"to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from permutation. This is confirmed by the fact that the above methods all reports limited improvement. A consideration of coherence during sentence selection leads to new methods, and these are mainly discourse driven models. Some of the summarization methods encode discourse analysis results in feature presentations together with other frequency based featu"
C16-1021,N10-1099,0,0.0837394,"s. It also provides inspiration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies in what entities it con"
C16-1021,N13-1136,0,0.275446,"ion. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013) formulated single document summarization as to extract a sub tree from the complete discourse tree and thus preserve the relations between extracted document units to form a readable text. Wang et al. (2015) extended it to multi-document summarization by regarding a document set as one document and developed a model which combined discourse parsing and summarization together. Christensen et al. (2013) proposed a graph-based model to bypass the tree constraints. They employed rich textual features to build a discourse relation graph for source documents with the aim of representing the relations between sentences (both inter and intra-document relations). Christensen et al. (2013) reported ROUGE scores lower than some baselines. This is because that, they claim, ROUGE is salience-focused and fails to notice the improvement in coherence. In a further human evaluation, they reported improvements in readability. These discourse-based methods without exception have discourse analysis as a prere"
C16-1021,P02-1057,0,0.0628032,"Missing"
C16-1021,W04-1017,0,0.029919,"of T (Barzilay and Lapata, 2008). To calculate the coherence score of T , Barzilay and Lapata (2008) used M (T ) as a feature vector. They calculated the transition probability for |{s(subj), o(bj), x(others), −(absent)}2 |= 16 transition patterns from M (T ) without distinguishing between entities, to form a vector f (T ) for T , and a weight vector w was then learnt from training data so that w ∗ f (T ) can be used as the coherence score for T . This kind of method has been adopted in many studies (Filippova and Strube, 2007; Barzilay and Lapata, 2008; Burstein et al., 2010). In particular, Filatova and Hatzivassiloglou (2004) extends entity grids to model semantical relations between entities, which provides a possible further improvement for our models. 3 Modeling Summarization The above model can only be used to measure coherence but summarization is much complex as it involves not only coherence bust also informativeness and redundancy. We design a much more sophisticated models leveraging entities. Two models are presented below. Both of them are based on entities and consider coherence as well as informativeness. The first one is based on global coherence and the second one local coherence. The global coheren"
C16-1021,W07-2321,0,0.0377804,"to the grammatical roles of Entity j in Sentence i. M (T ) is referred to as the Entity Grid of T (Barzilay and Lapata, 2008). To calculate the coherence score of T , Barzilay and Lapata (2008) used M (T ) as a feature vector. They calculated the transition probability for |{s(subj), o(bj), x(others), −(absent)}2 |= 16 transition patterns from M (T ) without distinguishing between entities, to form a vector f (T ) for T , and a weight vector w was then learnt from training data so that w ∗ f (T ) can be used as the coherence score for T . This kind of method has been adopted in many studies (Filippova and Strube, 2007; Barzilay and Lapata, 2008; Burstein et al., 2010). In particular, Filatova and Hatzivassiloglou (2004) extends entity grids to model semantical relations between entities, which provides a possible further improvement for our models. 3 Modeling Summarization The above model can only be used to measure coherence but summarization is much complex as it involves not only coherence bust also informativeness and redundancy. We design a much more sophisticated models leveraging entities. Two models are presented below. Both of them are based on entities and consider coherence as well as informativ"
C16-1021,P15-1056,0,0.0147264,"Hence the summarization systems need to identify important information and keep as much of it as possible. Most existing research follows such a guideline and takes salience as its sole focus. Salience-focused systems cannot guarantee the readability of the generated text as they fail to take coherence into consideration. Sentence reordering, as a post processing task has began to develop. Apparently, it cannot make up for the flaws of salience-focused systems because it is simply a reorganization of sentences. Besides, it also faces problems when dealing with temporal text (Yan et al., 2011; Ge et al., 2015). A better solution is to consider coherence when selecting sentences. Such comprehensive models have been proposed. Most of them are discourse driven and sacrifice informativeness for coherence. In this sense, our model is novel in dealing with coherence without discourse analysis. 219 5.1 Salience-Focused Method As stated, the summarization systems need to identify the important information and keep as much of it in the generated summaries as possible. One straightforward method is Maximum Marginal Relevance (Carbonell and Goldstein, 1998) (MMR). It is a greedy method, and is proposed to sel"
C16-1021,W09-1802,0,0.11699,"07; Wang et al., 2012)), which construct a graph between text units and use ranking algorithms to select top sentences to build summaries. Another kind is the optimization method. Our work is one of this kind. It formulates summarization as finding a subset that optimizes certain objective functions without violating certain constraints. To find such an optimal subset is a combinatorial optimization problem, which is an NP hard problem and hence cannot be solved in linear time (McDonald, 2007). Recently, maximum coverage methods have been proposed and yield good results (Gillick et al., 2009; Gillick and Favre, 2009; Takamura and Okumura, 2009). Maximum coverage methods formulate summarization as a maximum knapsack problem (MKMC). In MKMC methods, the meanings of sentences are believed to be made up by concepts, which usually refer to content words. And summarization involves extracting a subset of sentences that covers as many important concepts as possible without violating the length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a c"
C16-1021,J95-2003,0,0.89001,"ted improvements in readability. These discourse-based methods without exception have discourse analysis as a prerequisite. As we all know, discourse analysis is still under development thus preventing the expected improvement. Furthermore, languages other than English do not enjoy plenty of ready-to-use discourse analysis tools. This also limits the usage of these discourse-based methods. Is it possible to consider coherence in summarization without discourse analysis? Before answering this question, we need to find out what is the key to coherence in text. According to the centering theory (Grosz et al., 1995; Walker et al., 1998), the coherence of text is to a large extent maintained by entities and the relations between them. This indicates that discourse analysis is not a must to preserve coherence; we can directly take advantages of entities and their relations to generate coherence text. Based on this point, we design a novel graph-based model for multi-document summarization that eliminates the effort of conducting discourse relation analysis (inter or intra document) and generates informative and readable summaries. We formulate the document set as a graph whose nodes corresponds to sentenc"
C16-1021,D13-1158,1,0.937019,"uation (Lin, 2004)). 213 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 213–223, Osaka, Japan, December 11-17 2016. Existing work addresses coherence in summarization from different aspects. One kind of method employs reordering after selecting sentences, and the drawback is evident: coherence is considered after sentence selection. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013) formulated single document summarization as to extract a sub tree from the complete discourse tree and thus preserve the relations between extracted document units to form a readable text. Wang et al. (2015) extended it to multi-document summarization by regarding a document set as one document and developed a model which combined discourse parsing and summarization together. Christensen et al. (2013) proposed a graph-based model to bypass the tree constraints. They employed rich textual features to build a discourse relation graph for source documents with the aim of representing the relatio"
C16-1021,hong-etal-2014-repository,0,0.0620989,"multi-document summarization systems using ROUGE and human evaluation. The former aims to evaluate informativeness and the latter targets readability. ROUGE Evaluation MCKP is the maximum coverage methods proposed by Takamura and Okumura (2009). Lin is a model that uses a class of submodular functions (Lin and Bilmes, 2011). Christ is a graph based model proposed by Christensen et al. (2013). DPP is the determinantal point processes model Borodin (2009) and ICSI is another model based on maximum coverage Gillick et al. (2008). The results of DPP and ICSI comes from the repository presented in Hong et al. (2014). M1 is our model described in Section 3.1. M2 is the model described in Section 3.3, which is resolved using an ILP method. MEAD Radev et al. (2004a) is a baseline that employs ranking algorithms to generate multi-document summaries. The results are shown in Table 1. As we can see, our system (M1 and M2) produces comparable results to the state-of-the-art systems. With the MCKP method, all content words are used as concepts. But in 4 http://www-03.ibm.com/software/products/en/ibmilogcpleoptistud/ This method was first proposed by Yih et al. (2007) and then improved by Takamura and Okumura (20"
C16-1021,P14-1002,0,0.0177133,"s viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingly. Recently the neural network based discourse analysis (Li et al., 2014; Ji and Eisenstein, 2014) provides 220 us with an alternative way of conducting discourse analysis without traditional feature engineering. It can be used in our future work of modelling coherence using semantic relations. 6 Conclusion Previous summarization methods have usually focused on salience and neglected coherence. This work proposed a novel summarization system that combines coherence with salience. By taking entities and links between them into consideration, our weighted longest path model successfully improves the quality of summaries. The proposed model does not require discourse analysis and hence can be"
C16-1021,D14-1218,0,0.019942,"piration as regards other tasks that require computers to generate coherent text. The rest of the papers is organized as follows: Section 2 presents the centering theory and a coherence model based on entities. Section 3 presents our model. Section 4 describes the experiments and results. Section 5 presents some previous work and Section 6 concludes this paper. 2 Centering Theory and Coherence Modelling The centering theory (Grosz et al., 1995) as a popular theory on discourse analysis, serves as the basis of some coherence evaluation methods (Barzilay and Lapata, 2008; Burstein et al., 2010; Li and Hovy, 2014; Li and Jurafsky, 2016) and enables us to measure the coherence score of any given text without discourse parsing solely based on the reappearance of entities. Entities here refer to noun/pronoun 214 word/phrases 2 . According to the centering theory, we have the following assumptions: 1. Text that contains successive mentions of the same entities would be more coherent. 2. The main entities that are focused on tend to play an important grammatical role, such as the subject or object of the sentences. Therefore the key to the coherence of a text lies in what entities it contains and how their"
C16-1021,P13-2099,0,0.0150176,"d methods face some major challenges. One is informativeness, which means we need to maintain the important information of source documents in summaries. This is the focus of almost all research on summarization. Another challenge is presentation, namely that the extracted text should be well presented, i.e., it should contain little redundancy and be coherent so as to be readily understandable. Previous work has addressed the problem of redundancy, and some successful solutions like Maximum Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) have been proposed and widely adopted (e.g., (Li and Li, 2013)), but very few try to deal with coherence. Therefore the generated summaries generally suffer as regards readability and are very difficult to use for practical applications. In the report of the TAC 2011 summarization task (Owczarzak and Dang, 2011), it is stated that “in general, automatic summaries are better than baselines1 , except Readability.” Such a statement suggests, as for summarization, coherence should be treated with the same as salience and redundancy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons."
C16-1021,D14-1220,0,0.0209026,"et al. (2013) has viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingly. Recently the neural network based discourse analysis (Li et al., 2014; Ji and Eisenstein, 2014) provides 220 us with an alternative way of conducting discourse analysis without traditional feature engineering. It can be used in our future work of modelling coherence using semantic relations. 6 Conclusion Previous summarization methods have usually focused on salience and neglected coherence. This work proposed a novel summarization system that combines coherence with salience. By taking entities and links between them into consideration, our weighted longest path model successfully improves the quality of summaries. The proposed model does not require discourse"
C16-1021,P11-1052,0,0.0900013,"., 2009; Gillick and Favre, 2009; Takamura and Okumura, 2009). Maximum coverage methods formulate summarization as a maximum knapsack problem (MKMC). In MKMC methods, the meanings of sentences are believed to be made up by concepts, which usually refer to content words. And summarization involves extracting a subset of sentences that covers as many important concepts as possible without violating the length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a class of submodular functions for document summarization. The functions they use combine two parts, encouraging the summary to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coher"
C16-1021,W04-1013,0,0.0375736,". In the report of the TAC 2011 summarization task (Owczarzak and Dang, 2011), it is stated that “in general, automatic summaries are better than baselines1 , except Readability.” Such a statement suggests, as for summarization, coherence should be treated with the same as salience and redundancy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons.org/licenses/by/4.0/ 1 The baseline they used is the lead paragraph method and summaries are evaluated by human and ROUGE (Recall-Oriented Understudy for Gisting Evaluation (Lin, 2004)). 213 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 213–223, Osaka, Japan, December 11-17 2016. Existing work addresses coherence in summarization from different aspects. One kind of method employs reordering after selecting sentences, and the drawback is evident: coherence is considered after sentence selection. Another kind of widely adopted method takes discourse relations into consideration when selecting sentences, as discourse relations are believed to be essential for maintaining textual coherence. Hirao et al. (2013"
C16-1021,P14-5010,0,0.00489084,"Missing"
C16-1021,W98-1124,0,0.0172051,"s for sentence selection/compression. The problem is that these discourse based features usually play secondary roles, because the models all try to improve information coverage, which are evaluated by ROUGE. And ROUGE, as is commonly known, is not sensitive to coherence. Some others work directly on discourse analysis results, and they usually try to derive a passage from a given parse tree. The problem of summarization is regarded as finding a text T so that T = arg max F (T |T r) for a given tree T r. Here F is the objective function. Early representative work of this kind includes that of Marcu (1998) and that of Daum´e III and Marcu (2002). Recently, Hirao et al. (2013) has viewed summarization as a knapsack problem on trees, and uses an integer linear problem (ILP) to formulate it. A sub tree that maximizes some objective function and obeys some given constraints is extracted from the original parse tree as the summary. Discourse tree based methods cannot be extended to multi-document summarization. Christensen et al. (2013) propose a graph model that bypasses the tree constraints. They build a graph to represent discourse relations between sentences and then extract summaries accordingl"
C16-1021,C04-1108,0,0.024334,"ing diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from permutation. This is confirmed by the fact that the above methods all reports limited improvement. A consideration of coherence during sentence selection leads to new methods, and these are mainly discourse driven models. Some of the summarization methods encode discourse analysis results in feature presentations together with other frequency based features for sentence selection/compression. The pro"
C16-1021,radev-etal-2004-mead,0,0.346146,"y. ROUGE Evaluation MCKP is the maximum coverage methods proposed by Takamura and Okumura (2009). Lin is a model that uses a class of submodular functions (Lin and Bilmes, 2011). Christ is a graph based model proposed by Christensen et al. (2013). DPP is the determinantal point processes model Borodin (2009) and ICSI is another model based on maximum coverage Gillick et al. (2008). The results of DPP and ICSI comes from the repository presented in Hong et al. (2014). M1 is our model described in Section 3.1. M2 is the model described in Section 3.3, which is resolved using an ILP method. MEAD Radev et al. (2004a) is a baseline that employs ranking algorithms to generate multi-document summaries. The results are shown in Table 1. As we can see, our system (M1 and M2) produces comparable results to the state-of-the-art systems. With the MCKP method, all content words are used as concepts. But in 4 http://www-03.ibm.com/software/products/en/ibmilogcpleoptistud/ This method was first proposed by Yih et al. (2007) and then improved by Takamura and Okumura (2009). Here we follow the same steps with Takamura and Okumura (2009). 5 218 our systems, only nouns and pronouns are regarded as entities. There are"
C16-1021,C10-1111,0,0.0287562,"he length constraint. It is usually formulated as an integer linear problem. And some algorithms are proposed for obtaining approximated solutions (Takamura and Okumura, 2009; Gillick et al., 2009). Lin and Bilmes (2011) design a class of submodular functions for document summarization. The functions they use combine two parts, encouraging the summary to be representative of the corpus, and rewarding diversity separately. Other methods that have been applied to summarization include centroid-based methods (Radev et al., 2004b; Saggion and Gaizauskas, 2004), and minimum dominating set methods (Shen and Li, 2010). All these methods suffer in coherence. 5.2 Coherence-Focused Method Sentence reordering methods are developed to correct the salience-focused models. Sentence reordering tries to generate a more coherent text by reordering its contents. Rich semantic and syntactic features are used to find a better permutation for input sentences (Barzilay et al., 2001; Bollegala et al., 2010; Okazaki et al., 2004). The drawback to sentence reordering is obvious. The preceding sentence selection focuses solely on informativeness and totally neglects coherence. Thus it prevents the improvements expected from"
C16-1021,E09-1089,0,0.42513,"we assign a cost score, which is the number of words the corresponding sentence contains. To each path in the directed graph, we assign a gain score. The gain score is a comprehensive evaluation of the informativeness and coherence of the sequence of sentences represented by the path. The problem of extracting a good summary becomes the problem of extracting the best path. Note that it is an asymmetric graph. Gain scores for A → B → C and C → B → A are different. The direction determines the positions of corresponding sentences in the generated text. 2 In some previous work on summarization (Takamura and Okumura, 2009; Hirao et al., 2013), concepts are used to measure informativeness. Concepts can be used to refer any non functional words, including adjectives, adverbs. All the entities can be regarded as concepts, but some concept words (non-nominal words) are not entities. Entity is a subset of Concept. 215 One more thing to consider is the redundancy. Instead of formulating redundancy explicitly, we remove edges connecting similar sentences to turn the complete graph into an incomplete graph. This ensures that similar sentences do not occupy adjacent positions in the generated summaries and thus reduce"
C16-1021,P07-1070,0,0.0269097,"ant information and keep as much of it in the generated summaries as possible. One straightforward method is Maximum Marginal Relevance (Carbonell and Goldstein, 1998) (MMR). It is a greedy method, and is proposed to select sentences that are most relevant but not too similar to the already selected ones. It tries to keep a balance between relevance and redundancy. MMR is also widely employed to avoid redundancy in summarization systems. Among existing research, one popular kind is the ranking method (e.g., Textrank (Mihalcea and Tarau, 2004), Lexrank (Erkan and Radev, 2004) and its variants (Wan et al., 2007; Wang et al., 2012)), which construct a graph between text units and use ranking algorithms to select top sentences to build summaries. Another kind is the optimization method. Our work is one of this kind. It formulates summarization as finding a subset that optimizes certain objective functions without violating certain constraints. To find such an optimal subset is a combinatorial optimization problem, which is an NP hard problem and hence cannot be solved in linear time (McDonald, 2007). Recently, maximum coverage methods have been proposed and yield good results (Gillick et al., 2009; Gi"
C16-1021,W04-3252,0,\N,Missing
D10-1092,W05-0909,0,0.242163,"Missing"
D10-1092,W10-1749,0,0.0898134,"on of the square root to NKT imply that Chinese word order is close to that of English, and they have to measure subtle word order mistakes. 951 Table 3: NTCIR-7 √ meta-evaluation: Effects of square root (b(x) = 1 − 1 − x) √ NKT NKT b(NKT) Spearman w/ adequacy 0.940 0.940 0.922 Pearson w/ adequacy 0.922 0.817 0.941 Spearman w/ fluency 0.887 0.865 0.858 Pearson w/ fluency 0.931 0.917 0.833 In spite of these differences, the two groups independently recognized the usefulness of rank correlations for automatic evaluation of translation quality for distant language pairs. In their WMT-2010 paper (Birch and Osborne, 2010), they multiplied NKT with the brevity penalty and interpolated it with BLEU for the WMT-2010 shared task. This fact implies that incomprehensible or misleading word order mistakes are rare in translation among European languages. 6 Conclusions When Statistical Machine Translation is applied to distant language pairs such as Japanese and English, word order becomes an important problem. SMT systems often fail to find an appropriate translation because of a large search space. Therefore, they often output misleading or incomprehensible sentences such as “A because B” vs. “B because A.” To penal"
D10-1092,E06-1032,0,0.0197708,"a-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an auto"
D10-1092,W07-0718,0,0.0467588,"f sentence-level scores. We used default settings for conventional metrics, but we tuned GTM (Melamed et al., 2007) with -e option. This option controls preferences on longer word runs. We also used the paraphrase database TERp (http://www.umiacs.umd. 949 edu/˜snover/terp) for METEOR (Banerjee and Lavie, 2005). 3.2 Meta-evaluation with WMT-07 data We developed our metric mainly for automatic evaluation of translation quality for distant language pairs such as Japanese-English, but we also want to know how well the metric works for similar language pairs. Therefore, we also use the WMT07 data (Callison-Burch et al., 2007) that covers only European language pairs. Callison-Burch et al. (2007) tried different human evaluation methods and showed detailed evaluation scores. The Europarl test set has 2,000 sentences, and The News Commentary test set has 2,007 sentences. This data has different language pairs: Spanish, French, German ⇒ English. We exclude CzechEnglish because there were so few systems (See the footnote of p. 146 in their paper). 4 4.1 Results Meta-evaluation with NTCIR-7 data Table 1 shows the main results of this paper. The left part has corpus-level meta-evaluation with adequacy. Error metrics, WE"
D10-1092,I05-2014,0,0.0369692,"l by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers (http://chasenlegacy.sourceforge.jp/) following Fujii et al. (2008) In JE translation, most Statistical Machine Translation (SMT) systems translate the Japanese sentence (J0) kare wa sono hon wo yonda node sekaishi ni kyoumi ga atta which means 944 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944–952, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Associat"
D10-1092,2007.mtsummit-papers.21,0,0.0340785,"eni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundaries given by ChaSen, one of the standard morphological analyzers"
D10-1092,W10-1736,1,0.720649,") was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks in the reference with those in the hypothesis. There are two popular rank correlation coefficients: Spearman’s ρ and Kendall’s τ (Kendall, 1975). In Isozaki et al. (2010), we used Kendall’s τ to measure the effectiveness of our Head Finalization rule as a preprocessor for English-to-Japanese translation, but we measured the quality of translation by using conventional metrics. It is not clear how well τ works as an automatic evaluation metric of translation quality. Moreover, Spearman’s ρ might work better than Kendall’s τ . As we discuss later, τ considers only the direction of the rank change, whereas ρ considers the distance of the change. The first objective of this paper is to examine which is the better metric for distant language pairs. The second objec"
D10-1092,N03-1020,0,0.0120162,"tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and Araki, 2007) worked better. In these studies, Pearson’s correlation coefficient and Spearman’s rank correlation ρ with human evaluation scores are used to measure how closely an automatic evaluation method correlates with human evaluation. This evaluation of automatic evaluation methods is called meta-evaluation. In human evaluation, people judge the adequacy and the fluency of each translation. Denoual and Lepage (2005) pointed out that BLEU assumes word boundaries, which is ambiguous in Japanese and Chinese. Here, we assume the word boundar"
D10-1092,P02-1040,0,0.100666,"ese metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics. 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality machine translation systems because human evaluation is time consuming, expensive, and irreproducible. If we have a perfect automatic evaluation metric, we can tune our translation system for the metric. BLEU (Papineni et al., 2002b; Papineni et al., 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. However, Callison-Burch et al. (2006) argued that the MT community is overly reliant on BLEU by showing examples of poor performance. For Japanese-to-English (JE) translation, Echizen-ya et al. (2009) showed that the popular BLEU and NIST do not work well by using the system outputs of the NTCIR-7 PATMT (patent translation) JE task (Fujii et al., 2008). On the other hand, ROUGE-L (Lin and Hovy, 2003), Word Error Rate (WER), and IMPACT (Echizen-ya and A"
D10-1092,2006.amta-papers.25,0,0.103847,"evity Penalty (BP) min(1, exp(1 − r/h)), where r is the length of the reference and h is the length of the hypothesis. BLEU = BP × (p1 p2 p3 p4 )1/4 . Its range is [0, 1]. The BLEU score of (H0) with reference (R0) is 1.0×(11/11×9/10×6/9×4/8)1/4 = 0.740. Therefore, BLEU gives a very good score to this inadequate translation because it checks only ngrams and does not regard global word order. Since (R0) and (H0) look similar in terms of fluency, adequacy is more important than fluency in the translation between distant language pairs. Similarly, other popular scores such as NIST, PER, and TER (Snover et al., 2006) also give relatively good scores to this translation. NIST also considers only local word orders (n-grams). PER (Position-Independent Word Error Rate) was designed to disregard word order completely. TER (Snover et al., 2006) was designed to allow phrase movements without large penalties. Therefore, these standard metrics are not optimal for evaluating translation between distant language pairs. In this paper, we propose an alternative automatic evaluation metric appropriate for distant language pairs. Our method is based on rank correlation coefficients. We use them to compare the word ranks"
D10-1092,N03-2021,0,\N,Missing
D13-1158,W01-1605,0,0.882141,"Missing"
D13-1158,P02-1057,0,0.236537,"Missing"
D13-1158,P09-1075,0,0.0187614,"age model (MCP) and a lead method (LEAD). MCP is known to be a state-of-the-art method for multiple document summarization and we believe that MCP also performs well in terms of single document summarization. LEAD is also a widely used summarizer that simply takes the first K textual units of the document. Although this is a simple heuristic rule, it is known as a state-of-the-art summarizer (Nenkova and McKeown, 2011). For our method, we examined two types of DEP-DT. One was obtained from the gold RSTDT. The other was obtained from the RST-DT produced by a state-of-the-art RST parser, HILDA (duVerle and Prendinger, 2009; Hernault et al., 2010). For Marcu’s method, we examined both the gold RST-DT and HILDA’s RST-DT. We re-implemented HILDA and re-trained it on the RST-DT Corpus excluding the 30 documents used in the evaluation. The F-score of the parser was around 0.5. For KP, we exclude constraint (7) from the ILP formulation of TKP and set the depth of all EDUs in equations (3) and (5) at 1. For MCP, we use tf (equation (4)) as the word weight. We evaluated the summarization systems with ROUGE version 1.5.5 4 . Performance metrics were the recall (R) and F-score (F) of ROUGE -1,2. 4.2 Results and Discussio"
D13-1158,C04-1057,0,0.027106,"Missing"
D13-1158,W08-1105,0,0.0274936,"minals of an RST-DT are eliminated. However, we believe that these relations are no needed for the summarization that we are attempting to realize. 1517 We formulate the optimization problem in the previous section as a Tree Knapsack Problem, which is a kind of Precedence-Constrained Knapsack Problem (Samphaiboon and Yamada, 2000) and we can obtain the optimal rooted subtree by solving the following ILP problem2 : maximize x s.t. N ∑ W(ei ) xi Depth(ei ) i=1 N ∑ ℓi x i ≤ L (5) (6) i=1 2 ∀i : xparent(i) ≥ xi (7) ∀i : xi ∈ {0, 1}, (8) A similar approach has been applied to sentence compression (Filippova and Strube, 2008). TKP(G) TKP(H) Marcu(G) Marcu(H) MCP KP LEAD ROUGE -1 F R .310H,K,L .321G,H,K,L .281H .284H H .291 .272H .236 .219 .279 .295H .251 .266H .255 .240 ROUGE -2 F R .108 .112H .092 .093 .101 .093 .073 .068 .073 .077 .071 .075 .092 .086 Table 1: ROUGE scores of the RST discourse treebank dataset. In the table, G,H,K,L indicate a method statistically significant against Marcu(G), Marcu(H), KP, LEAD, respectively. where x is an N -dimensional binary vector that represents the summary, i.e. xi =1 denotes that the ith EDU is included in the summary. N is the number of EDUs in a document, ℓi is the leng"
D13-1158,W04-1013,0,0.0694284,"nce. This paper proposes a single document summarization method based on the trimming of a discourse tree. This is a two-fold process. First, we propose rules for transforming a rhetorical structure theorybased discourse tree into a dependency-based discourse tree, which allows us to take a treetrimming approach to summarization. Second, we formulate the problem of trimming a dependency-based discourse tree as a Tree Knapsack Problem, then solve it with integer linear programming (ILP). Evaluation results showed that our method improved ROUGE scores. These methods successfully improved ROUGE (Lin, 2004) scores, but they still have one critical shortcoming. Since these methods are based on subset selection, the summaries they generate cannot preserve the rhetorical structure of the textual units of a source document. Thus, the resulting summary may lack coherence and may not include significant textual units from a source document. 1 Introduction State-of-the-art extractive text summarization methods regard a document (or a document set) as a set of textual units (e.g. sentences, clauses, phrases) and formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of"
D13-1158,W98-1124,0,0.316243,"Introduction State-of-the-art extractive text summarization methods regard a document (or a document set) as a set of textual units (e.g. sentences, clauses, phrases) and formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of the set of textual units that maximizes an objective without violating a length constraint. For example, McDonald (2007) formulated text summarization as a Knapsack Problem, where he selects a set of textual One powerful and potential way to overcome the problem is to include discourse tree constraints in the summarization procedure. Marcu (1998) regarded a document as a Rhetorical Structure Theory (RST) (William Charles, Mann and Sandra Annear, Thompson, 1988)-based discourse tree (RSTDT) and selected textual units according to a preference ranking derived from the tree structure to make a summary. Daum´e et al. (2002) proposed a document compression method that directly models the probability of a summary given an RST-DT by using a noisy-channel model. These methods generate well-organized summaries, however, since they do not formulate summarizations as combinatorial op1515 Proceedings of the 2013 Conference on Empirical Methods in"
D13-1158,E09-1089,0,0.0923356,"Missing"
D13-1158,W01-0100,0,\N,Missing
D14-1196,P02-1057,0,0.242742,"Missing"
D14-1196,D13-1158,1,0.413854,"Missing"
D14-1196,P14-2052,1,0.463564,"an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT. 1 Introduction Discourse structures of documents are believed to be highly beneficial for generating informative and coherent summaries. Several discoursebased summarization methods have been developed, such as (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013; Kikuchi et al., 2014). Moreover, the current best ROUGE score for the summarization benchmark data of the RSTdiscourse Treebank (Carlson et al., 2002) has been provided by (Hirao et al., 2013), whose method also utilizes discourse trees. Thus, the discoursebased summarization approach is one promising way to obtain high-quality summaries. One possible weakness of discourse-based summarization techniques is that they rely greatly on the accuracy of the discourse parser they use. For example, the above discourse-based summarization methods utilize discourse trees based on the Rhetorical Structure Theory (RST) (Mann"
D14-1196,P14-1003,0,0.218582,"Nucleus, we obtain an incorrect DEP-DT in Figure 1(b) because the transformation algorithm uses the Nucleus-Satellite relationships in the RST-DT. The dependency relationships in Figure 1(b) are quite different from that of the correct DEP-DT in Figure 1(c). In this example, the parser failed to determine the most salient EDU e2 , that is the root EDU of the gold DEP-DT. Thus, the summary extracted from this DEP-DT will have a low ROUGE score. The results motivated us to design a new discourse parser fully trained on the DEP-DTs and 1 Li et al. also defined a similar transformation algorithm (Li et al., 2014). In this paper, we follow the transformation algorithm defined in (Hirao et al., 2013). 1835 Parser)Training)Phase Transforma;on)Algorithm Elabora7on$ N$ Root$ Elabora7on$ Root$ Example$ Elabora7on$ S$ N$ Elabora7on$ S$ S$ N$ Example$ Elabora7on$ S$ N$ Background$ Concession$ S$ S$ Example$An7thesis$ N$ N$ Elabora7on$ Elabora7on$ Background$ N$ N$ S$ N$ Elabora7on$ S$ S$ e1$ e2$ e1$ e1$ S$ e3$ e4$ e2$ e3$ e2$ e5$ N$ e4$ e5$ e3$ e4$ N$ N$ S$ Concession$ S$ Contrast$ Background$ N$ N$ S$ Elabora7on$ N$ Contrast$ N$ N$ N$ S$ Evidence$ N$ Contrast$N$ N$ S$ Evidence$ N$ RST=DTs Root$ Elabora7on$ R"
D14-1196,W98-1124,0,0.568676,"with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT. 1 Introduction Discourse structures of documents are believed to be highly beneficial for generating informative and coherent summaries. Several discoursebased summarization methods have been developed, such as (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013; Kikuchi et al., 2014). Moreover, the current best ROUGE score for the summarization benchmark data of the RSTdiscourse Treebank (Carlson et al., 2002) has been provided by (Hirao et al., 2013), whose method also utilizes discourse trees. Thus, the discoursebased summarization approach is one promising way to obtain high-quality summaries. One possible weakness of discourse-based summarization techniques is that they rely greatly on the accuracy of the discourse parser they use. For example, the above discourse-based summarization methods utiliz"
D14-1196,P05-1012,0,0.102477,"-DT. (b) Overview of (Hirao et al., 2013). In the parser training phase, the parser is trained on RST-DTs, and in the summarization phase, the document is parsed into the RST-DT, and then transformed into the DEP-DT. that could directly generate the DEP-DT. Figure 2(a) shows an overview of the TKP combined with our DEP-DT parser. In the parser training phase, we transform RST-DTs into DEP-DTs, and directly train our parser with the DEP-DTs. In the summarization phase, our method parses a raw document directly into a DEP-DT, and generates a summary with the TKP. fused Relaxed Algorithm (MIRA) (McDonald et al., 2005a; Crammer et al., 2006). We denote s(w, y) = wT fy as a score function given a weight vector w and a DEP-DT y. L(y, y? ) is a loss function, and we define it as the number of EDUs that have an incorrect parent EDU in a predicted DEP-DT y? = arg max s(w, y). Then, we y solve the following optimization problem: 3.2 Description of Discourse Dependency Parser min ||w − w(t) || w Our parser is based on the first-order Maximum Spanning Tree (MST) algorithm (McDonald et al., 2005b). Our parser extracts the features from the EDU ei and the EDU ej . We use almost the features as those shown in (Hernau"
D14-1196,H05-1066,0,0.071798,"-DT. (b) Overview of (Hirao et al., 2013). In the parser training phase, the parser is trained on RST-DTs, and in the summarization phase, the document is parsed into the RST-DT, and then transformed into the DEP-DT. that could directly generate the DEP-DT. Figure 2(a) shows an overview of the TKP combined with our DEP-DT parser. In the parser training phase, we transform RST-DTs into DEP-DTs, and directly train our parser with the DEP-DTs. In the summarization phase, our method parses a raw document directly into a DEP-DT, and generates a summary with the TKP. fused Relaxed Algorithm (MIRA) (McDonald et al., 2005a; Crammer et al., 2006). We denote s(w, y) = wT fy as a score function given a weight vector w and a DEP-DT y. L(y, y? ) is a loss function, and we define it as the number of EDUs that have an incorrect parent EDU in a predicted DEP-DT y? = arg max s(w, y). Then, we y solve the following optimization problem: 3.2 Description of Discourse Dependency Parser min ||w − w(t) || w Our parser is based on the first-order Maximum Spanning Tree (MST) algorithm (McDonald et al., 2005b). Our parser extracts the features from the EDU ei and the EDU ej . We use almost the features as those shown in (Hernau"
D14-1196,N03-1030,0,0.477924,"Missing"
D16-1112,W13-2322,0,0.0542076,"ted edges represent a relationship between nodes. Concepts &lt;s&gt; canada od m a1 a3 country na m e a2 nato prime op 1 name … summary AMR of the input sentence “canadian” yi+1 encAMR aj … announce … … a1 &lt;s&gt; E 0 yi C+1 canada … nato headline prime op 1 name a3 country od m na m e a2 E 0 yi ABS “canadian” AMR of the input sentence Figure 2: Model structure of our proposed attentionbased AMR encoder; it outputs a headline using ABS and encoded AMR with attention. consist of English words, PropBank event predicates, and special labels such as “person”. For edges, AMR has approximately 100 relations (Banarescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes (Hovy et al., 2006). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser (Wang et al., 2015). 3.2 Attention-Based AMR Encoder Figure 2 shows a brief sketch of the model structure of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in (Tai et al., 2015) to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG st"
D16-1112,N16-1012,0,0.30206,"e 3 supports this observation. For example, ABS+AMR successfully added the correct modifier ‘saudi’ to “crown prince” in the first example. Moreover, ABS+AMR generated a consistent subject in the third example. The comparison between ABS+AMR(w/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. 1058 5 Related Work Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do"
D16-1112,P16-1014,0,0.0597202,"ural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the AMR encoder into the baseline. We believe that our AMR encoder can possibly further improve the performance of their methods. We will test that hypothesis in future study. 6 Conclusion This paper mainly discussed the usefulness of incorporating structural syntactic and semantic information in"
D16-1112,N06-2015,0,0.0110087,"2 nato prime op 1 name … summary AMR of the input sentence “canadian” yi+1 encAMR aj … announce … … a1 &lt;s&gt; E 0 yi C+1 canada … nato headline prime op 1 name a3 country od m na m e a2 E 0 yi ABS “canadian” AMR of the input sentence Figure 2: Model structure of our proposed attentionbased AMR encoder; it outputs a headline using ABS and encoded AMR with attention. consist of English words, PropBank event predicates, and special labels such as “person”. For edges, AMR has approximately 100 relations (Banarescu et al., 2013) including semantic roles based on the PropBank annotations in OntoNotes (Hovy et al., 2006). To acquire AMRs for input sentences, we use the state-of-the-art transition-based AMR parser (Wang et al., 2015). 3.2 Attention-Based AMR Encoder Figure 2 shows a brief sketch of the model structure of our attention-based AMR encoder model. We utilize a variant of child-sum Tree-LSTM originally proposed in (Tai et al., 2015) to encode syntactic and semantic information obtained from output of the AMR parser into certain fixed-length embedding vectors. To simplify the computation, we transform a DAG structure of AMR parser output to a tree structure, which we refer to as “tree-converted AMR s"
D16-1112,W04-1013,0,0.222018,"esults if we observed statistical difference (p &lt; 0.05) between ABS (re-run) and ABS+AMR on the t-test. (R-1: ROUGE-1, R-2: ROUGE-2, R-L: ROUGE-L) For a fair comparison, we followed their evaluation setting. The training data was obtained from the first sentence and the headline of a document in the annotated Gigaword corpus (Napoles et al., 2012)4 . The development data is DUC-2003 data, and test data are both DUC-2004 (Over et al., 2007) and sentence-headline pairs obtained from the annotated Gigaword corpus as well as training data5 . All of the generated headlines were evaluated by ROUGE (Lin, 2004)6 . For evaluation on DUC2004, we removed strings after 75-characters for each generated headline as described in the DUC2004 evaluation. For evaluation on Gigaword, we forced the system outputs to be at most 8 words as in Rush et al. (2015) since the average length of headline in Gigaword is 8.3 words. For the preprocessing for all data, all letters were converted to lower case, all digits were replaced with ‘#’, and words appearing less than five times with ‘UNK’. Note that, for further evaluation, we prepared 2,000 sentence-headline pairs randomly sampled from the test data section of the G"
D16-1112,K16-1028,0,0.0442533,"/o attn) and ABS+AMR (with attention) suggests that the attention mechanism is necessary for AMR encoding. In other words, the encoder without the attention mechanism tends to be overfitting. 1058 5 Related Work Recently, the Recurrent Neural Network (RNN) and its variant have been applied successfully to various NLP tasks. For headline generation tasks, Chopra et al. (2016) exploited the RNN decoder (and its variant) with the attention mechanism instead of the method of Rush et al. (2015): the combination of the feed-forward neural network language model and attention-based sentence encoder. Nallapati et al. (2016) also adapted the RNN encoder-decoder with attention for headline generation tasks. Moreover, they made some efforts such as hierarchical attention to improve the performance. In addition to using a variant of RNN, Gulcehre et al. (2016) proposed a method to handle infrequent words in natural language generation. Note that these recent developments do not conflict with our method using the AMR encoder. This is because the AMR encoder can be straightforwardly incorporated into their methods as we have done in this paper, incorporating the AMR encoder into the baseline. We believe that our AMR e"
D16-1112,W12-3018,0,0.0606696,"Missing"
D16-1112,D15-1044,0,0.311302,"ral attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 1 Introduction Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014), image captioning (Vinyals et al., 2015), video description (Venugopalan et al., 2015), and headline generation (Rush et al., 2015). This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG tasks. However, to the best of our knowledge, t"
D16-1112,P15-1150,0,0.0400517,"ing the quality of NLG tasks. However, to the best of our knowledge, there is no clear evidence that syntactic and semantic information can enhance the recently developed encoder-decoder models in NLG tasks. To answer this research question, this paper proposes and evaluates a headline generation method based on an encoder-decoder architecture on Abstract Meaning Representation (AMR). The method is essentially an extension of attention-based summarization (ABS) (Rush et al., 2015). Our proposed method encodes results obtained from an AMR parser by using a modified version of TreeLSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR). 2 Attention-based summarization (ABS) ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dat"
D16-1112,N15-1173,0,0.0191451,"ormation additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared with the baseline neural attention-based model. 1 Introduction Neural network-based encoder-decoder models are cutting-edge methodologies for tackling natural language generation (NLG) tasks, i.e., machine translation (Cho et al., 2014), image captioning (Vinyals et al., 2015), video description (Venugopalan et al., 2015), and headline generation (Rush et al., 2015). This paper also shares a similar goal and motivation to previous work: improving the encoderdecoder models for natural language generation. There are several directions for enhancement. This paper respects the fact that NLP researchers have expended an enormous amount of effort to develop fundamental NLP techniques such as POS tagging, dependency parsing, named entity recognition, and semantic role labeling. Intuitively, this structural, syntactic, and semantic information underlying input text has the potential for improving the quality of NLG ta"
D16-1112,N15-1040,0,0.0546549,"ed method encodes results obtained from an AMR parser by using a modified version of TreeLSTM encoder (Tai et al., 2015) as additional information of the baseline ABS model. Conceptually, the reason for using AMR for headline generation is that information presented in AMR, such as predicate-argument structures and named entities, can be effective clues when producing shorter summaries (headlines) from original longer sentences. We expect that the quality of headlines will improve with this reasonable combination (ABS and AMR). 2 Attention-based summarization (ABS) ABS proposed in Rush et al. (2015) has achieved state-of-the-art performance on the benchmark data of headline generation including the DUC-2004 dataset (Over et al., 2007). Figure 1 illustrates the model structure of ABS. The model predicts a word sequence (summary) based on the combination of the neural network language model and an input sentence encoder. Let V be a vocabulary. xi is the i-th indicator vector corresponding to the i-th word in the input sentence. Suppose we have M words of an input sentence. X represents an input sentence, which 1054 Proceedings of the 2016 Conference on Empirical Methods in Natural Language"
D18-1450,hovy-etal-2006-automated,0,0.693296,"t consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations. 1 Introduction To develop high quality summarization systems, we need accurate automatic content evaluation. Although, various evaluation measures have been proposed, ROUGE-N (Lin, 2004), Basic Elements (BE) (Hovy et al., 2006) remain the de facto standard measures since they strongly correlate with various manual evaluations and are easy to use. However, the evaluation scores computed by these automatic measures are not so useful for improving system performance because they merely confirm if the summary contains small textual fragments and so they do not address semantic correctness. The pyramid method was proposed as a manual evaluation that well supports the improvement of summarization systems (Nenkova and Passonneau, 2004; Nenkova et al., 2007). First, the method identifies conceptual contents, Summary Content"
D18-1450,P15-1162,0,0.0742783,"Missing"
D18-1450,N16-1030,0,0.0177166,"d for multi document summarization tasks in DUC-2003 to 2007 and TAC-2008 to 2011. Table 1 and Table 2 show the statistics of the data sets. 4.2 EDU Segmenter We regard decomposing a sentence into EDUs as a sequential tagging problem and implement a neural EDU segmenter that classifies each word in a sentence as the boundary of EDU or not based on 3-layer bi-LSTM (Wang et al., 2015). The size of word embeddings and hidden layers of the LSTM were set to 100 and 256, respectively. To handle low-frequency words, all words are encoded to 40 dimension hidden state by using character-based bi-LSTM (Lample et al., 2016). To utilize entire words in a corpus, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate, 1.0. Moreover, to avoid overfitting the training data, dropout layer was adopted to the input of the LSTMs with the ratio 0.3. The segmenter was trained by utilizing the training data of RST Discourse Treebank corpus (Carlson et al., 2001). The macro-averaged Fmeasure of boundary detection on the test data of the corpus is 0.917. The source documents, system summaries and reference summaries utilized 4182 2003 Cov. ROUGE-2 .906/.821/.617 ROUGE-SU4 .782/.774/.600 BE .927/."
D18-1450,W16-3617,0,0.0220435,"e documents. In other words, we regard EDUs as alternatives to SCUs. To construct the pyramid, we transform human-made reference summaries into EDU-based extractive reference summaries and then weight every EDU by counting the number of the extractive reference summaries that contain the EDU. The rea4177 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4177–4186 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics son why we derive extractive reference summaries whose SCUs are EDUs is as follows. First, Li et al. (2016) reported that EDUs are very similar to SCUs. Second, the performance of EDU segmenter is sufficient to satisfy practical requirements (see Section 2). Third, we do not need measure any semantic similarity to identify EDUs common to the extractive reference summaries. We also examine two types of extractive reference summary. One is based on the alignment between EDUs in reference summary and source documents. The other is based on the extractive oracle summary (Hirao et al., 2017). We conducted experiments on the Document Understanding Conference (DUC) 2003 to 2007 data sets and Text Analysis"
D18-1450,W01-1605,0,0.107287,"STM (Wang et al., 2015). The size of word embeddings and hidden layers of the LSTM were set to 100 and 256, respectively. To handle low-frequency words, all words are encoded to 40 dimension hidden state by using character-based bi-LSTM (Lample et al., 2016). To utilize entire words in a corpus, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate, 1.0. Moreover, to avoid overfitting the training data, dropout layer was adopted to the input of the LSTMs with the ratio 0.3. The segmenter was trained by utilizing the training data of RST Discourse Treebank corpus (Carlson et al., 2001). The macro-averaged Fmeasure of boundary detection on the test data of the corpus is 0.917. The source documents, system summaries and reference summaries utilized 4182 2003 Cov. ROUGE-2 .906/.821/.617 ROUGE-SU4 .782/.774/.600 BE .927/.862/.617 − PEAK Prop(BE) .936/.909/.750 Prop(ROUGE) .908/.874/.750 Prop(AL) .831/.841/.633 2004 Cov. .909/.838/.691 .854/.772/.559 .936/.868/.721 − .929/.892/.750 .938/.814/.676 .904/.855/.735 2005 Resp. .932/.931/.792 .925/.893/.731 .897/.867/.714 − .845/.819/.657 .864/.809/.629 .821/.757/.567 2006 Resp. Pyr. .836/.767/.584 .905/.884/.740 .849/.790/.601 .885/."
D18-1450,W04-1013,0,0.0808668,"xtractive reference summaries that consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations. 1 Introduction To develop high quality summarization systems, we need accurate automatic content evaluation. Although, various evaluation measures have been proposed, ROUGE-N (Lin, 2004), Basic Elements (BE) (Hovy et al., 2006) remain the de facto standard measures since they strongly correlate with various manual evaluations and are easy to use. However, the evaluation scores computed by these automatic measures are not so useful for improving system performance because they merely confirm if the summary contains small textual fragments and so they do not address semantic correctness. The pyramid method was proposed as a manual evaluation that well supports the improvement of summarization systems (Nenkova and Passonneau, 2004; Nenkova et al., 2007). First, the method identi"
D18-1450,de-marneffe-etal-2006-generating,0,0.183189,"Missing"
D18-1450,P07-1062,0,0.0425633,"on Alignment between EDUs e50 e41 maximize W=3 φ(ej , mk )aj,k (1) j=1 k=1 W=2 e30 |E ||M| X X s.t. W=1 |E ||M| X X `(ej )aj,k ≤ Lmax (2) j=1 k=1 |E| X Figure 1: Overview of our pyramid construction. aj,k ≤ 1 ∀k (3) aj,k ≤ 1 ∀j (4) ∀j, k. (5) j=1 |M| (Pilehvar et al., 2013). As a result, the resultant pyramids have insufficient quality to be practical. Clearly, further improvement is necessary. While our method is required to decompose a document into EDUs accurately, the EDU segmenter offers accurate decomposition performance; existing EDU boundary detection methods have F-measures over 0.9 (Fisher and Roark, 2007; Feng and Hirst, 2014). Moreover, since extractive reference summaries are set of EDUs from the source documents, we do not need semantic similarity to identify EDUs that have the same meaning. Thus, we can easily construct a pyramid by simply counting the number of extractive reference summaries that contains each EDU. 3 Automatic Pyramid Evaluation First, we transform human-made reference summaries into extractive reference summaries; the EDUs in the source documents are used as the atomic units. Second, we construct a pyramid by weighting EDUs in the extractive reference summaries. EDU wei"
D18-1450,N04-1019,0,0.9025,"Although, various evaluation measures have been proposed, ROUGE-N (Lin, 2004), Basic Elements (BE) (Hovy et al., 2006) remain the de facto standard measures since they strongly correlate with various manual evaluations and are easy to use. However, the evaluation scores computed by these automatic measures are not so useful for improving system performance because they merely confirm if the summary contains small textual fragments and so they do not address semantic correctness. The pyramid method was proposed as a manual evaluation that well supports the improvement of summarization systems (Nenkova and Passonneau, 2004; Nenkova et al., 2007). First, the method identifies conceptual contents, Summary Content Units (SCUs), in reference summaries and then constructs a pyramid by collecting semantically equivalent SCUs. The weight of an SCU in the pyramid is defined as the number of reference summaries that contain the SCU. Thus, an SCU shared by many reference summaries is given higher weight. Second, a system summary is scored by the correspondences between SCUs in the summary and the pyramid. Its results are very useful for system improvement, i.e., we can know which important SCUs the system could or could"
D18-1450,P13-2026,0,0.102162,"hy a summary was given a good or bad score. During the past few years, studies have focused on the automatic scoring of summaries based on manually generated pyramids. Harnly et al. (2005) proposed a scoring method that matches SCUs in the pyramid with possible textual fragments in the summary. They enumerate all possible textual fragments within a sentence in the summary and compute similarity scores between the fragments and the SCUs in the pyramid based on unigram overlap. Then, they find the optimal correspondences between SCUs and the fragments that maximize the sum of similarity scores. Passonneau et al. (2013) extended the method by introducing distributional semantics to compute the similarity scores between SCUs and the fragments. Recently, Yang et al. (2016) proposed the first automatic pyramid method, Pyramid Evaluation via Automated Knowledge Extraction (PEAK). PEAK employs subject-predicate-object triples extracted by ClausIE (Del Corro and Gemulla, 2013) as SCUs, and constructs pyramids by cutting a graph whose vertices represent the triples and whose edges represent semantic similarity scores between the triples computed by Align, Disambiguate and Walk (ADW) (Pilehvar et al., 2013). When ev"
D18-1450,P13-1132,0,0.210488,"cores. Passonneau et al. (2013) extended the method by introducing distributional semantics to compute the similarity scores between SCUs and the fragments. Recently, Yang et al. (2016) proposed the first automatic pyramid method, Pyramid Evaluation via Automated Knowledge Extraction (PEAK). PEAK employs subject-predicate-object triples extracted by ClausIE (Del Corro and Gemulla, 2013) as SCUs, and constructs pyramids by cutting a graph whose vertices represent the triples and whose edges represent semantic similarity scores between the triples computed by Align, Disambiguate and Walk (ADW) (Pilehvar et al., 2013). When evaluating a summary, PEAK constructs a weighted bipartite graph whose vertices represent subject-predicate-object triples extracted from the pyramid and the summary, respectively; the edges represent the similarity scores between the triples as computed by ADW. It scores the summary by solving the Linear Assignment Problem which involves maximizing the sum of the similarity scores on the bipartite graph. The major difference between PEAK and our method is that the former regards the reference summary as a set of subject-predicate-object triples while the latter regards a reference summ"
D19-1587,D15-1263,0,0.141551,"similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be succe"
D19-1587,E17-1028,0,0.116463,"it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynamic programming that can build the optimal RST tree in terms of either a span splitting score or a span merging score. We regarded a document as a text span consisting of three different granularity levels and built trees at each level, a document tree, paragraph trees for each paragraph, and sentence trees for each sentence. Then, we"
D19-1587,W01-1605,0,0.0730738,"of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be successfully applied to RST parsing. In most cases, RST parsers have been developed on the basis of supervised learning algorithms, which require a high quality annotated corpus of sufficient size. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001). These supervised RST parsing methods might not be applied to languages with only a small-size corpus. This paper presents two types of language independent unsupervised RST parsing methods based on the CKY-like dynamic programming-based approach. One method builds the most likely parse tree in terms of a dissimilarity score defined for splitting a text span into two smaller ones. The other builds the optimal tree in terms of a similarity score defined for merging two adjacent text spans into a larger one. The similarity and dissimilarity scores between text spans are calculated on the basis"
D19-1587,W16-3616,1,0.857101,"T trees contain the multi-nuclear relations is larger. In fact, the ratio of the documents whose RST trees contain multi-nuclear relations in PCC is 0.712, while that in RST-DT is 0.464. We can obtain the information of sentences boundaries in most cases, however, sometimes we cannot obtain the information of paragraph boundaries. Thus, it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynam"
D19-1587,D13-1158,1,0.930181,"Missing"
D19-1587,P14-1002,0,0.17613,"he left span ` from i-th to k-th atomic text unit1 and the right span r from k+1 to j-th atomic text unit are given, we define the similarity score between them as follows: −→ −−→ 1 sim(`i:k , − rk+1:j )= 2 ( −→ ) −−→ `i:k · − rk+1:j −→ −−→ + 1 . r k k` kk− i:k −→ −−→ indicate the vector represenHere, `i:k and − rk+1:j tations of the left and right spans, which are defined as a concatenation of two vectors for the left most atomic unit and the right most atomic unit as follows: −→ − →], `i:k = [→ ui ; − u k (2) − − − → − − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score o"
D19-1587,P17-1092,0,0.0131247,"erging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be successfully applied to RST parsing. In most cases,"
D19-1587,W04-3250,0,0.0351274,"PCC contains two invalid RST trees, maz-12666 and maz-8838, we excluded them. Dataset Gran. Split Merge RB D2E .602 RST-DT D2S2E .755 D2P2S2E .793 .656 .788 .811 .545 .751 .803 D2E .656 D2S2E .757 D2P2S2E .787 .669 .760 .784 .626 .749 .789 PCC Table 2: Micro Span F1 scores for RST-DT and PCC. 3.2 Results Table 2 shows the evaluation results. Merge outperformed Split in most cases, and D2P2S2E achieved the best scores among our variants on both RST-DT and PCC. To clearly show the differences between our proposed method and RB, we performed significance tests, using paired bootstrap resampling (Koehn, 2004) at significance level=0.05. The results showed that there were significant differences between our method and RB at all the settings (D2E, D2S2E, D2P2S2E) for English, and at D2E and D2S2E for German, while there were no significant differences at D2P2S2E for German. The results imply that merging two adjacent spans and dividing with three granularity levels is suitable for unsupervised RST parsing. Comparing our methods with the baseline, right-branching (RB), we could find larger differences without considering the granularity levels of a document, whereas the differences became smaller by"
D19-1587,W98-1124,0,0.390101,"nes. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, synt"
D19-1587,D14-1162,0,0.0979854,"− − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score of frequent words and is defined as a = (1 − α)/(αZ), where α is a hyper parameter and Z is the total number of words. 2.3 Dynamic Programming-Based Approach for Building Optimal Trees We propose a dynamic programming-based approach to obtain the optimal tree in terms of either the total split or merge score from all the possible trees. We first illustrate the algorithm with a merge (similarity) score. We define V [b][e], which stores the maximum merge score for a span ub:e consisting from b-th unit to e-th unit, as fol"
D19-1587,N18-1202,0,0.0113315,"`i:k = [→ ui ; − u k (2) − − − → − − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score of frequent words and is defined as a = (1 − α)/(αZ), where α is a hyper parameter and Z is the total number of words. 2.3 Dynamic Programming-Based Approach for Building Optimal Trees We propose a dynamic programming-based approach to obtain the optimal tree in terms of either the total split or merge score from all the possible trees. We first illustrate the algorithm with a merge (similarity) score. We define V [b][e], which stores the maximum merge score for a span ub:e consisting"
D19-1587,stede-neumann-2014-potsdam,0,0.0280019,"e calculated on the basis of their distributional representations. Note that since our method is fully unsupervised, the parser predicts only the skeleton of an RST tree. Moreover, we exploit multiple granularity levels of a document: (1) we first independently build three types of trees, a tree whose leaves correspond to a paragraph, a tree whose leaves correspond to a sentence, and a tree whose leaves correspond to an EDU, and then (2) merge them to obtain the whole RST tree. We conducted experimental evaluation on English and German datasets, RST-DT and the Potsdam Commentary Corpus (PCC) (Stede and Neumann, 2014), respectively. The results demonstrated that our method with span merging outperformed our method with span splitting and ob5797 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5797–5802, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tained .811 and .784 span scores for RST-DT and PCC, respectively. The scores are close to the scores of early supervised RST parsers. 2 Unsupervised RST Parsing 2.1 Motivation Generally, RST trees with m"
D19-1587,P17-2029,0,0.160955,"RST trees contain multi-nuclear relations in PCC is 0.712, while that in RST-DT is 0.464. We can obtain the information of sentences boundaries in most cases, however, sometimes we cannot obtain the information of paragraph boundaries. Thus, it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynamic programming that can build the optimal RST tree in terms of either a span splitting score o"
D19-1674,D18-1443,0,0.0214465,"n search efficiency in finding word combinations that can form anagrams (Jordan and Monteiro, 2003). However, they place little consideration on the word orders that seem natural to humans. Fortunately, recent progress in NLP techniques now enables natural sentences to be generated in many ∗ This work was conducted while the author was at NTT Communication Science Laboratories. 1 NP-hardness of anagram generation can be proved by using the fact that it contains an exact cover problem, which has been proven to be NP-complete (Karp, 1972), as a special case. NLP tasks (J´ozefowicz et al., 2016; Gehrmann et al., 2018; Skadina and Pinnis, 2017). However, beam search, the de-facto decoding algorithm used in many language generation methods, cannot be used as-is to generate anagrams since it cannot ensure that the generated sentences are anagrams. Although beam search variants that can generate sentences that satisfy some constraints have been proposed (Anderson et al., 2017; Hokamp and Liu, 2017), applying them to anagram generation tasks soon becomes intractable. To summarize, generating natural anagrams is a challenging task that cannot be solved by simply applying existing methods. In this paper, we prop"
D19-1674,P17-1141,0,0.16024,"-hardness of anagram generation can be proved by using the fact that it contains an exact cover problem, which has been proven to be NP-complete (Karp, 1972), as a special case. NLP tasks (J´ozefowicz et al., 2016; Gehrmann et al., 2018; Skadina and Pinnis, 2017). However, beam search, the de-facto decoding algorithm used in many language generation methods, cannot be used as-is to generate anagrams since it cannot ensure that the generated sentences are anagrams. Although beam search variants that can generate sentences that satisfy some constraints have been proposed (Anderson et al., 2017; Hokamp and Liu, 2017), applying them to anagram generation tasks soon becomes intractable. To summarize, generating natural anagrams is a challenging task that cannot be solved by simply applying existing methods. In this paper, we propose an anagram generation algorithm. The proposed algorithm is very simple; we run depth-first search to enumerate anagrams. As to the generation procedure, we use a neural language model and character frequency to suppress unnecessary search operations. Due to the power of the currently available pre-trained neural language models, this simple approach works surprisingly well. In e"
D19-1674,D17-1098,0,0.127442,"ence Laboratories. 1 NP-hardness of anagram generation can be proved by using the fact that it contains an exact cover problem, which has been proven to be NP-complete (Karp, 1972), as a special case. NLP tasks (J´ozefowicz et al., 2016; Gehrmann et al., 2018; Skadina and Pinnis, 2017). However, beam search, the de-facto decoding algorithm used in many language generation methods, cannot be used as-is to generate anagrams since it cannot ensure that the generated sentences are anagrams. Although beam search variants that can generate sentences that satisfy some constraints have been proposed (Anderson et al., 2017; Hokamp and Liu, 2017), applying them to anagram generation tasks soon becomes intractable. To summarize, generating natural anagrams is a challenging task that cannot be solved by simply applying existing methods. In this paper, we propose an anagram generation algorithm. The proposed algorithm is very simple; we run depth-first search to enumerate anagrams. As to the generation procedure, we use a neural language model and character frequency to suppress unnecessary search operations. Due to the power of the currently available pre-trained neural language models, this simple approach works"
D19-1674,N18-1202,0,0.0210559,"s) for s = “christmas”. The anagram generation problem is the problem of finding sentence t given input sentence s, where t is an anagram of s. We use a language model when generating anagrams. Let p(s) be the probability that sentence s = (w1 , w2 , . . . , w|s |) appears. A language model is a stochastic model that gives conditional probability p(wi+1 |w1 , . . . , wi ). A language model derives probability p(s) by |s|−1 p(s) = p(w1 ) Y p(wi+1 |w1 , . . . , wi ). (1) i=1 While our anagram generation algorithm can use any language model, our experiments use the neural language model of ELMo (Peters et al., 2018). 4 Anagram Generation Our anagram generation algorithm uses simple DFS. Given input string s, we start the search with the initial string t =“hSi” and then try to append a word w ∈ V to the tail of t for each step to search for anagrams of s. If we find that t yields no solution by appending any words to it, we pop back the last word from t and continue the search by appending a different word to the tail of t. Since there are |V |N possible sentences consisting of N words, naive DFS is intractable. Thus we need to reduce the search steps. For this we introduce the following two criteria. The"
D19-1674,I17-1038,0,0.0191609,"finding word combinations that can form anagrams (Jordan and Monteiro, 2003). However, they place little consideration on the word orders that seem natural to humans. Fortunately, recent progress in NLP techniques now enables natural sentences to be generated in many ∗ This work was conducted while the author was at NTT Communication Science Laboratories. 1 NP-hardness of anagram generation can be proved by using the fact that it contains an exact cover problem, which has been proven to be NP-complete (Karp, 1972), as a special case. NLP tasks (J´ozefowicz et al., 2016; Gehrmann et al., 2018; Skadina and Pinnis, 2017). However, beam search, the de-facto decoding algorithm used in many language generation methods, cannot be used as-is to generate anagrams since it cannot ensure that the generated sentences are anagrams. Although beam search variants that can generate sentences that satisfy some constraints have been proposed (Anderson et al., 2017; Hokamp and Liu, 2017), applying them to anagram generation tasks soon becomes intractable. To summarize, generating natural anagrams is a challenging task that cannot be solved by simply applying existing methods. In this paper, we propose an anagram generation a"
E17-1037,C02-1053,1,0.520056,"mploys another summarization approach, the extractive summarization paradigm is worthwhile to leverage research resources. As another benefit, identifying an oracle summary for a set of reference summaries allows us to utilize yet another evaluation measure. Since both oracle and extractive summaries are sets of sentences, it is easy to check whether a system summary contains sentences in the oracle summary. As a result, F-measures, which are available to evaluate a system summary, are useful for evaluating classification-based extractive summarization (Mani and Bloedorn, 1998; Osborne, 2002; Hirao et al., 2002). Since ROUGEn evaluation does not identify which sentence is important, an Fmeasure conveys useful information in terms of “important sentence extraction.” Thus, combining ROUGEn and an F-measure allows us to scrutinize the failure analysis of systems. Note that more than one oracle summary might exist for a set of reference summaries because ROUGEn scores are based on the unweighted counting of n-grams. As a result, an F-measure might not be identical among multiple oracle summaries. Thus, we need to enumerate the oracle summaries for a set of reference summaries and compute the F-measures b"
E17-1037,P13-1020,0,0.0615489,"maries to exploit F-measures that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceeding"
E17-1037,E14-1075,0,0.0408382,"measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 386–396, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics summary from a set of reference summaries and a source document(s). To the best of our knowledge, this is the firs"
E17-1037,hong-etal-2014-repository,0,0.410501,"Missing"
E17-1037,P15-1153,0,0.0190065,"that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computationa"
E17-1037,D15-1011,0,0.027651,"Missing"
E17-1037,N10-1133,0,0.0286592,"here the ROUGEn scores of the oracle summaries are significantly higher than those of the state-ofthe-art summarization systems. 3 Definition of Extractive Oracle Summaries We first briefly describe ROUGEn . Given set of reference summaries R and system summary S, ROUGEn is defined as follows: ROUGEn (R, S) = |R ||U (Rk )| X X k=1 min{N (gjn , Rk ), N (gjn , S)} j=1 |R ||U (Rk )| X X k=1 . Related Work Lin and Hovy (2003) utilized a naive exhaustive search method to obtain oracle summaries in terms of ROUGEn and exploited them to understand the limitations of extractive summarization systems. Ceylan et al. (2010) proposed another naive exhaustive search method to derive a probability density function from the ROUGEn scores of oracle summaries for the domains to which source documents belong. The computational complexity of naive exhaustive methods is exponential to the size of the sentence set. Thus, it may be possible to apply them to single document summarization tasks involving a dozen sentences, but it is infeasible to apply them to multiple document summarization tasks that involve several hundred sentences. To describe the difference between the ROUGEn scores of oracle and system summaries in mu"
E17-1037,P11-1052,0,0.204221,"nt Lmax (line 11). When we do not have room to add w, we update U by adding the score obtained by multiplying the density of w by the remaining length, Lmax (line 13), and exit the while loop. 5.3 Initial Score for Search 1. ROUGEn (R, V ) ≥ τ ; 2. ROUGEn (R, V ) < τ , R OUGE n (R, V ) < τ ; 3. ROUGEn (R, V ) < τ , R OUGE n (R, V ) ≥ τ . Since the branch and bound technique prunes the search by comparing the best solution found so far with the upper bounds, obtaining a good solution in the early stage is critical for raising search efficiency. Since ROUGEn is a monotone submodular function (Lin and Bilmes, 2011), we can obtain a good approximate solution by a greedy algorithm (Khuller et al., 1999). It is guaranteed that the score of the obtained approximate solution is larger than 12 (1 − 1e )OPT, where OPT is the score of the optimal solution. We employ the solution as the initial ROUGEn score of the candidate oracle summary. Algorithm 2 shows the greedy algorithm. In it, S denotes a summary and D denotes a set of sentences. The algorithm iteratively adds sentence s∗ that yields the largest gain in the ROUGEn score to current summary S, provided the length of the summary does not violate length con"
E17-1037,W13-3108,0,0.06166,"Missing"
E17-1037,W03-0510,0,0.049582,"inatorial optimization problem, and no polynomial time algorithms exist that can attain an optimal solution. 1. Room still exists for the further improvement of extractive summarization, i.e., where the ROUGEn scores of the oracle summaries are significantly higher than those of the state-ofthe-art summarization systems. 3 Definition of Extractive Oracle Summaries We first briefly describe ROUGEn . Given set of reference summaries R and system summary S, ROUGEn is defined as follows: ROUGEn (R, S) = |R ||U (Rk )| X X k=1 min{N (gjn , Rk ), N (gjn , S)} j=1 |R ||U (Rk )| X X k=1 . Related Work Lin and Hovy (2003) utilized a naive exhaustive search method to obtain oracle summaries in terms of ROUGEn and exploited them to understand the limitations of extractive summarization systems. Ceylan et al. (2010) proposed another naive exhaustive search method to derive a probability density function from the ROUGEn scores of oracle summaries for the domains to which source documents belong. The computational complexity of naive exhaustive methods is exponential to the size of the sentence set. Thus, it may be possible to apply them to single document summarization tasks involving a dozen sentences, but it is"
E17-1037,W09-1802,0,0.38159,"Missing"
E17-1037,W02-0401,0,0.114143,"a system that employs another summarization approach, the extractive summarization paradigm is worthwhile to leverage research resources. As another benefit, identifying an oracle summary for a set of reference summaries allows us to utilize yet another evaluation measure. Since both oracle and extractive summaries are sets of sentences, it is easy to check whether a system summary contains sentences in the oracle summary. As a result, F-measures, which are available to evaluate a system summary, are useful for evaluating classification-based extractive summarization (Mani and Bloedorn, 1998; Osborne, 2002; Hirao et al., 2002). Since ROUGEn evaluation does not identify which sentence is important, an Fmeasure conveys useful information in terms of “important sentence extraction.” Thus, combining ROUGEn and an F-measure allows us to scrutinize the failure analysis of systems. Note that more than one oracle summary might exist for a set of reference summaries because ROUGEn scores are based on the unweighted counting of n-grams. As a result, an F-measure might not be identical among multiple oracle summaries. Thus, we need to enumerate the oracle summaries for a set of reference summaries and com"
E17-1037,W12-2601,0,0.0227224,"sentence from V and return to the top of the recurrence. 6 6.1 Experiments Experimental Setting We conducted experiments on the corpora developed for a multiple document summarization task in DUC 2001 to 2007. Table 1 show the statistics of the data. In particular, the DUC-2005 to -2007 data sets not only have very large numbers of sentences and words but also a long target length (the reference summary length) of 250 words. All the words in the documents were stemmed by Porter’s stemmer (Porter, 1980). We computed ROUGE1 scores, excluding stopwords, and computed ROUGE2 scores, keeping them. Owczarzak et al. (2012) suggested using ROUGE1 and keeping stopwords. However, as Takamura et al. argued (Takamura and Okumura, 2009), the summaries optimized with non-content words failed to consider the actual quality. Thus, we excluded stopwords for computing the ROUGE1 scores. We enumerated the following two types of oracle summaries: those for a set of references for a given topic and those for each reference in the set of references. 6.2 6.2.1 Results and Discussion Impact of Oracle ROUGEn scores Table 2 shows the average ROUGE1,2 scores of the oracle summaries obtained from both a set of references and each r"
E17-1037,D15-1226,0,0.0258891,"tly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 386–396, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics summary from a set of reference summaries and a source document(s). To the best of our knowledge, this is the first ILP formulation that extracts oracle summaries. Second, since it"
E17-1037,P16-1172,0,0.0151764,"denotes the multiple set of n-grams that appear in system-generated summary S (a set of sentences). N (gjn , Rk ) and N (gjn , S) return the number of occurrences of n-gram gjn in the k-th reference and system summaries, respectively. Function U (·) transforms a multiple set into a normal set. ROUGEn takes values in the range of [0, 1], and when the n-gram occurrences of the system summary agree with those of the reference summary, the value is 1. In this paper, we focus on extractive summarization, employ ROUGEn as an evaluation measure, 387 with oracle summaries found by a greedy algorithm. Peyrard and Eckle-Kohler (2016) proposed a method to find a summary that approximates a ROUGE score based on the ROUGE scores of individual sentences and exploited the framework to train their summarizer. As mentioned above, such summaries do not always agree with the oracle summaries defined in our paper. Thus, the quality of the training data is suspect. Moreover, since these studies fail to consider that a set of reference summaries has multiple oracle summaries, the score of the loss function defined between their oracle and system summaries is not appropriate in most cases. As mentioned above, no known efficient algori"
E17-1037,D13-1156,0,0.0742105,"that evaluate which system summaries contain how many sentences that are extracted as an oracle summary. Our experimental results obtained from Document Understanding Conference (DUC) corpora demonstrated the following: (1) room still exists to improve the performance of extractive summarization; (2) the F-measures derived from the enumerated oracle summaries have significantly stronger correlations with human judgment than those derived from single oracle summaries. 1 Introduction Recently, compressive and abstractive summarization are attracting attention (e.g., Almeida and Martins (2013), Qian and Liu (2013), Yao et al. (2015), Banerjee et al. (2015), Bing et al. (2015)). However, extractive summarization remains a primary research topic because the linguistic quality of the resultant summaries is guaranteed, at least at the sentence level, which is a key requirement for practical use (e.g., Hong and Nenkova (2014), Hong et al. (2015), Yogatama et al. (2015), Parveen et al. (2015)). The summarization research community is experiencing a paradigm shift from extractive to compressive or abstractive summarization. Currently our question is: “Is extractive summariza386 Proceedings of the 15th Confere"
E17-1037,E12-1023,0,0.0998676,"widely used to solve NP-hard combinatorial optimization problems, the solutions are not always optimal. Thus, the summary does not always have a maximum ROUGEn score for the set of reference summaries. Both works called the summary found by their methods the oracle, but it differs from the definition in our paper. Since summarization systems cannot reproduce human-made reference summaries in most cases, oracle summaries, which can be reproduced by summarization systems, have been used as training data to tune the parameters of summarization systems. For example, Kulesza and Tasker (2011) and Sipos et al. (2012) trained their summarizers 2. The F-measures derived from multiple oracle summaries obtain significantly stronger correlations with human judgment than those derived from single oracle summaries. 2 (2) (1) N (gjn , Rk ) j=1 Rk denotes the multiple set of n-grams that occur in k-th reference summary Rk , and S denotes the multiple set of n-grams that appear in system-generated summary S (a set of sentences). N (gjn , Rk ) and N (gjn , S) return the number of occurrences of n-gram gjn in the k-th reference and system summaries, respectively. Function U (·) transforms a multiple set into a normal"
E17-1037,E09-1089,0,0.140371,"ted experiments on the corpora developed for a multiple document summarization task in DUC 2001 to 2007. Table 1 show the statistics of the data. In particular, the DUC-2005 to -2007 data sets not only have very large numbers of sentences and words but also a long target length (the reference summary length) of 250 words. All the words in the documents were stemmed by Porter’s stemmer (Porter, 1980). We computed ROUGE1 scores, excluding stopwords, and computed ROUGE2 scores, keeping them. Owczarzak et al. (2012) suggested using ROUGE1 and keeping stopwords. However, as Takamura et al. argued (Takamura and Okumura, 2009), the summaries optimized with non-content words failed to consider the actual quality. Thus, we excluded stopwords for computing the ROUGE1 scores. We enumerated the following two types of oracle summaries: those for a set of references for a given topic and those for each reference in the set of references. 6.2 6.2.1 Results and Discussion Impact of Oracle ROUGEn scores Table 2 shows the average ROUGE1,2 scores of the oracle summaries obtained from both a set of references and each reference in the set (“multi” and “single”), those of the best conventional system (Peer), and those obtained f"
E17-1037,D15-1228,0,0.0364183,"Missing"
H05-1019,W04-1003,0,0.0188577,"or topics 7-12, 13-18 and 25-30 on Ó  , Ó , and Ó .  respectively. Note that each human subject, A to E, was a retired professional journalist; that is, they shared a common background. Table 3 shows the Pearson’s correlation coefficient ( ) and Spearman’s rank correlation coefficient Ö for the human subjects. The results show that every pair has a high correlation. Therefore, changing the human subject has little influence as regards creating references and evaluating system summaries. The evaluation by human subjects is stable. This result agrees with DUC’s additional evaluation results (Harman and Over, 2004). However, the behavior of the correlations between humans with different backgrounds is uncertain. The correlation might be fragile if we introduce a human subject whose background is different from the others. 4.3 Compared Automatic Evaluation Methods We compared our method with ROUGE-N and ROUGE-L described below. We used only content words to calculate the ROUGE scores because the correlation coefficient decreased if we did not remove functional words. WSK-based method We use WSK instead of ESK in equation (6)-(8).   ;=&gt;Z1ADC ;=&gt;Z1A (16)  Here, function SU returns the number of"
H05-1019,C04-1077,1,0.811115,"luations require a huge effort and the cost is considerable. Moreover, we cannot automatically evaluate a new system even if we use the corpora built for these workshops, and we cannot conduct reevaluation experiments. To cope with this situation, there is a particular need to establish a high quality automatic evaluation method. Once this is done, we can expect great progress to be made on natural language generation. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more r"
H05-1019,C04-1064,1,0.847492,"luations require a huge effort and the cost is considerable. Moreover, we cannot automatically evaluate a new system even if we use the corpora built for these workshops, and we cannot conduct reevaluation experiments. To cope with this situation, there is a particular need to establish a high quality automatic evaluation method. Once this is done, we can expect great progress to be made on natural language generation. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more r"
H05-1019,N03-1020,0,0.306123,"eneration. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natu"
H05-1019,P04-1077,0,0.382558,"hod for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 145–152, Vancouver, Octo"
H05-1019,W04-1013,0,0.385578,"aper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Languag"
H05-1019,P02-1040,0,0.0948615,"eam 1 0 a-DREAM cosmonaut-my great is my Becoming-DREAM Becoming-SPACEMAN Becoming-a Becoming-ambition 2 Becoming-an Becoming-astronaut Becoming-cosmonaut Becoming-dream Becoming-great 1 1 1 0 1 1     1 0 0 0 0  1   0  0  0 a-SPACEMAN 2 a-cosmonaut a-dream a-great a-is a-my an-DREAM an-SPACEMAN an-ambition an-astronaut an-is an-my 2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004). Hori et. al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy. They reported that their method is superior to BLEU (Papineni et al., 2002) in terms of the correlation between human assessment and automatic evaluation. Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. They applied ROUGE-L to the evaluation of summarization and machine translation. The results showed that the LCS-based measure is comparable to Ngram-based automatic evaluation methods. However, these methods tend to be strongly influenced by word order. Various N-gram-based methods have been proposed since BLEU, which is now widely used for the evaluation of machine translation. Lin et al. (2003) proposed a"
H05-1019,P04-1078,0,0.0189371,"AN-dream   0 cosmonaut-DREAM   0 an 0 1 SPACEMAN-great cosmonaut-dream  0   0 1 astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great  0  cosmonaut 1 0 SPACEMAN-my cosmonaut-is 1 0    0  0 dream 1 0 a-DREAM cosmonaut-my great is my Becoming-DREAM Becoming-SPACEMAN Becoming-a Becoming-ambition 2 Becoming-an Becoming-astronaut Becoming-cosmonaut Becoming-dream Becoming-great 1 1 1 0 1 1     1 0 0 0 0  1   0  0  0 a-SPACEMAN 2 a-cosmonaut a-dream a-great a-is a-my an-DREAM an-SPACEMAN an-ambition an-astronaut an-is an-my 2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004). Hori et. al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy. They reported that their method is superior to BLEU (Papineni et al., 2002) in terms of the correlation between human assessment and automatic evaluation. Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. They applied ROUGE-L to the evaluation of summarization and machine translation. The results showed that the LCS-based measure is comparable to Ngram-based automatic evaluation methods. However, these methods tend"
I17-2002,2016.amta-researchers.10,0,0.0160326,"end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal and input words, the attention mechanism has"
I17-2002,P97-1003,0,0.247328,"nd Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq2Seq+right Seq2S"
I17-2002,D16-1001,0,0.0389053,"data into three parts: The Wall Street Journal (WSJ) sections 02-21 for training, section 22 for development and section 23 for testing. In our models, the dimensions of the input word embeddings, the fed label embeddings, the hidden layers, and an attention vector were respectively set to 150, 30, 200, and 200. The LSTM depth was set to 3. Label set Lcon had a size of 61. The input vocabulary size of PTB was set to 42393. Supervised attention rate λ was set to 1.0. To use entire words as a vocabulary, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate 0.8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997),"
I17-2002,P15-1030,0,0.0325395,"rd LSTM to encode previously predicted label yt−1 into hidden state st . For each time t, with a 2-layer feed-forward neural network r, encoder and decoder hidden lay→ ers h and − s t are used to calculate the attention weight: → exp(r(hi , − s t )) . αti = ∑n → exp(r(h ′ , − s )) i i′ =1 − t=1 −λ × logP (yt |yt−1 , ..., y0 , x) n ∑ m ∑ i=1 t=1 ait × logαti , to jointly learn the attention and output distributions. All our alignments are represented by oneto-many links between input words x and nonterminals y. 3 Design of our Alignments In the traditional parsing framework (Hall et al., 2014; Durrett and Klein, 2015), lexical features have been proven to be useful in improving parsing performance. Inspired by previous work, we enhance the attention mechanism utilizing the linguistically-motivated annotations between surface words and nonterminals by supervised attention. In this paper, we define four types of alignments for supervised attention. The first three methods use the monolexical properties of heads without incurring any inferential costs of lexicalized annotations. Although the last needs manually constructed annotation schemes, it can capture bilexical relationships along dependency arcs. The f"
I17-2002,C96-1058,0,0.0611496,".8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq"
I17-2002,P14-1022,0,0.0132644,"layer stacked forward LSTM to encode previously predicted label yt−1 into hidden state st . For each time t, with a 2-layer feed-forward neural network r, encoder and decoder hidden lay→ ers h and − s t are used to calculate the attention weight: → exp(r(hi , − s t )) . αti = ∑n → exp(r(h ′ , − s )) i i′ =1 − t=1 −λ × logP (yt |yt−1 , ..., y0 , x) n ∑ m ∑ i=1 t=1 ait × logαti , to jointly learn the attention and output distributions. All our alignments are represented by oneto-many links between input words x and nonterminals y. 3 Design of our Alignments In the traditional parsing framework (Hall et al., 2014; Durrett and Klein, 2015), lexical features have been proven to be useful in improving parsing performance. Inspired by previous work, we enhance the attention mechanism utilizing the linguistically-motivated annotations between surface words and nonterminals by supervised attention. In this paper, we define four types of alignments for supervised attention. The first three methods use the monolexical properties of heads without incurring any inferential costs of lexicalized annotations. Although the last needs manually constructed annotation schemes, it can capture bilexical relationships al"
I17-2002,P15-1162,0,0.0359884,"Missing"
I17-2002,C16-1291,0,0.145898,"and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal and input words, the"
I17-2002,D15-1166,0,0.106565,"odel and five-model ensembles. We used the products of the output probabilities for the ensemble. • Last word: All output tokens were linked to the end of the sentence tokens in the input sentence. We evaluated the compared methods using bracketing Precision, Recall and F-measure. We used evalb2 as a parsing evaluation. We also evalAll models were written in C++ on Dynet (Neubig et al., 2017). We compared Seq2Seq models with and with2 10 http://nlp.cs.nyu.edu/evalb/ uated the learned attention using alignment error rate (AER) (Och and Ney, 2003) on their alignments. Following a previous work (Luong et al., 2015), attention evaluation was conducted on gold output. on the English Penn Treebank against the baseline methods. These results emphasize, the effectiveness of our alignments in parsing performances. Acknowledgement We thank the anonymous reviewers for their careful reading and useful comments. 4.2 Results Table 1 shows the results. All our lexical head, left word, right word and span word alignments improved bracket F-measure of baseline on every setting. From the +random, +first, and +last results, only supervised attention itself did not improve the parsing performances. Furthermore, each AER"
I17-2002,D16-1249,0,0.0316216,"utput tokens. &lt;s&gt; and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal an"
I17-2002,J03-1002,0,0.00594689,"size was set to ten. The decoding was performed on both a single model and five-model ensembles. We used the products of the output probabilities for the ensemble. • Last word: All output tokens were linked to the end of the sentence tokens in the input sentence. We evaluated the compared methods using bracketing Precision, Recall and F-measure. We used evalb2 as a parsing evaluation. We also evalAll models were written in C++ on Dynet (Neubig et al., 2017). We compared Seq2Seq models with and with2 10 http://nlp.cs.nyu.edu/evalb/ uated the learned attention using alignment error rate (AER) (Och and Ney, 2003) on their alignments. Following a previous work (Luong et al., 2015), attention evaluation was conducted on gold output. on the English Penn Treebank against the baseline methods. These results emphasize, the effectiveness of our alignments in parsing performances. Acknowledgement We thank the anonymous reviewers for their careful reading and useful comments. 4.2 Results Table 1 shows the results. All our lexical head, left word, right word and span word alignments improved bracket F-measure of baseline on every setting. From the +random, +first, and +last results, only supervised attention it"
I17-2002,D15-1044,0,0.0712984,"rpus showed that the bracketing F-measure was improved by supervised attention. 1 (NP )S )NP (VP XX XX XX the chef cooks )VP (NP )NP XX XX the soup Figure 1: S-expression format for Vinyals et al. (2015)’s Seq2seq constituency parser. The Seq2seq model employs “&lt;s&gt; (S (NP XX XX )NP (VP XX (NP XX XX )NP )VP )S &lt;/s&gt;” as output tokens. &lt;s&gt; and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be a"
I17-2002,W03-3023,0,0.135784,"produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq2Seq+right Seq2Seq+span 89.2 89.6 89.2 89.3 88.9 89.4 88.9 89.1 89.1 89.5 89.0 89.2 6.9 1.8 4.7 1.6 89.2 89.4 89.5 89.2 88.1 88.7 88.6 88.4 88.6 89.0 89.1 88.8 6.9 1.7 4.7 1.6 Vinyals et al. (2015) w att† Vinyals et al. (2015) w/o att† Seq2Seq+beam"
K15-2015,W05-0305,0,0.0675271,"ased argument extractors. One extracts both Arg1 and Arg2 from the same sentence (SS). The other extracts Arg1 and Arg2 from adjacent sentences respectively (PS). 3.1 Argument Position Identification 4.1 SS Cases In the argument identification step, following Ghosh et al. (2011), the identifier examines all adjacent sentence pairs within each paragraph. For each pair of sentences (Si , Si+1 ), we identify the existence of a discourse relation. To identify the existence of the relation (binary classification), we used SVM with the following features. 4.1.1 Subordinating Conjunctions We adopted Dinesh et al. (2005)’s tree subtraction method for subordinating conjunctions. This method takes a constituent parse tree as an input and detects argument spans as follows: (1) set a node variable x to the last word of the target connective, • First unigram, last unigram, and first trigram of Si and Si+1 . (2) set x to the parent node of x and repeat until x has label SBAR or S and set a node variable Arg2 to the node of x, • Si (or Si+1 ) contains modality words or not. 96 Con. Arg1 Arg2 Arg1&Arg2 Overall P .924 .658 .768 .566 .348 Dev R .857 .549 .640 .471 .290 F .889 .599 .698 .514 .316 P .918 .719 .587 .488 ."
K15-2015,P14-1003,0,0.0557319,"ural Language Learning: Shared Task, pages 95–99, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics Type Context Parse Tree Features Cs , POSs , {Wordu }, {POSu } Path(Cs ,root), Parent(Cs ), depth(Cs ), RightSib(Cs ), LeftSib(Cs ) u = s − 5, . . . , s − 1, s + 1, . . . , s + 5 • Word pairs (wi , wi+1 ) ∈ Si × Si+1 • Brown cluster pairs feature defined in Rutherford and Xue (2014) • Sentence-to-sentence discourse dependency tree features including existence of dependency edges and rhetorical relation labels. Discourse dependency trees are defined in Li et al. (Li et al., 2014). Table 1: Features used in connective classifier discourse connective into “same sentence” (SS) or “previous sentence” (PS). SS indicates both Arg1 and Arg2 are located in the same sentence that contains the discourse connective. PS indicates Arg1 is located in the sentence previous to that containing both the discourse connective and Arg2. We utilized context features in Table 1 and the position of the connective Cs : start, middle, or end. We also trained the classifier by using SVM with second-order polynomial kernel. If the identifier identifies that a pair of sentences (Si , Si+1 ) has t"
K15-2015,E14-1068,0,0.182146,"on By following (Ziheng et al., 2014), we implemented an argument position classifier that classifies the location of the arguments of arbitrary 95 Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 95–99, c Beijing, China, July 26-31, 2015. 2014 Association for Computational Linguistics Type Context Parse Tree Features Cs , POSs , {Wordu }, {POSu } Path(Cs ,root), Parent(Cs ), depth(Cs ), RightSib(Cs ), LeftSib(Cs ) u = s − 5, . . . , s − 1, s + 1, . . . , s + 5 • Word pairs (wi , wi+1 ) ∈ Si × Si+1 • Brown cluster pairs feature defined in Rutherford and Xue (2014) • Sentence-to-sentence discourse dependency tree features including existence of dependency edges and rhetorical relation labels. Discourse dependency trees are defined in Li et al. (Li et al., 2014). Table 1: Features used in connective classifier discourse connective into “same sentence” (SS) or “previous sentence” (PS). SS indicates both Arg1 and Arg2 are located in the same sentence that contains the discourse connective. PS indicates Arg1 is located in the sentence previous to that containing both the discourse connective and Arg2. We utilized context features in Table 1 and the position"
K15-2015,N15-1081,0,0.0230907,"ces (Si , Si+1 ) has the discourse relation, we heuristically regard Si as Arg1 and Si+1 as Arg2. 3.2 Sense Classification In the sense classification step, we classify the discourse relation between a pair of sentences (Si , Si+1 ) into five senses: “Expansion”, “Contingency”, “Temporal”, “Comparison”, and “EntRel”. To classify the sense of a pair of sentences, we used multi-class SVM. We used the same features described in the argument position identification step. To increase the number of training data, we used the (inter-sentential) explicit training data as the additional training data (Rutherford and Xue, 2015). We removed a connective from each instance in the explicit training data and treated them as implicit training data. The accuracy of classification into five senses is still low because the distribution of the senses is imbalanced. Following Rutherford and Xue (2014), we resampled the instances in the training data of sense classification to balance the distribution of the senses. 2.3 Sense Classification We assign majority sense ℓ∗ for each discourse connective Cs as follows: ℓ∗ = arg maxℓ∈L freq(Cs , ℓ). (1) L is a set of sense labels used in training data and freq returns the frequency of"
N15-1049,W01-1605,0,0.014309,"Missing"
N15-1049,P14-1085,0,0.0239079,"ent, we can give theoretical upper bounds when we represent the set of all subtrees of an input tree. ZDD uses O(N log N ) nodes to represent the set of all subtrees of an N node input tree. Hence the DP algorithm runs in O(N L log N ) time. The main virtues of the proposed algorithm are that (1) it can always find an exact solution, (2) its running time is theoretically guaranteed, and (3) it can solve the three known tree trimming problems. Furthermore, our algorithm is fast enough to be practical and scalable. Since text summarization methods are often applied to large scale inputs (e.g., (Christensen et al., 2014; Nakao, 2000)), scalability is important. We compare it to state-of-the-art ILP solvers and confirm that the proposed algorithm can be hundreds of times faster. Since our method assumes known formuations for text summarization, the summary created by our algorithm is exactly the same as that obtained by applying previous methods. However, we believe that algorithmic improvements in computational cost is as important as improvements in accuracy in order to make better practical systems. 2 Tree Trimming Problems We briefly review the three tree trimming formulations used in text summarization a"
N15-1049,C04-1057,0,0.0263461,"exact solutions, and our algorithm is applicable to different tree trimming problems. Moreover, experiments show that our algorithm is faster than state-of-the-art ILP solvers, and that it scales well to handle large summarization problems. 1 Introduction Extractive text summarization and sentence compression are tasks that basically select a subset of the input set of textual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such a"
N15-1049,D13-1155,0,0.0955036,"asks that basically select a subset of the input set of textual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset is made by forming a subtree by trimming the input tree. Since the optimal trimmed subtree preserves the relationships between textual units, it is a concise representation of the original set that preserves linguistic quality. A shortcoming of tree trimming-based meth"
N15-1049,W08-1105,0,0.156311,"d sentence compression are tasks that basically select a subset of the input set of textual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset is made by forming a subtree by trimming the input tree. Since the optimal trimmed subtree preserves the relationships between textual units, it is a concise representation of the original set that preserves linguistic quality. A shortcoming"
N15-1049,D13-1158,1,0.928735,"tual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset is made by forming a subtree by trimming the input tree. Since the optimal trimmed subtree preserves the relationships between textual units, it is a concise representation of the original set that preserves linguistic quality. A shortcoming of tree trimming-based methods is that they are formulated as integer linear pr"
N15-1049,P14-2052,1,0.861463,"sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset is made by forming a subtree by trimming the input tree. Since the optimal trimmed subtree preserves the relationships between textual units, it is a concise representation of the original set that preserves linguistic quality. A shortcoming of tree trimming-based methods is that they are formulated as integer linear programming (ILP) problems and so an ILP solver is needed to solve them. Although modern ILP solvers can"
N15-1049,N10-1134,0,0.0631445,"ifferent tree trimming problems. Moreover, experiments show that our algorithm is faster than state-of-the-art ILP solvers, and that it scales well to handle large summarization problems. 1 Introduction Extractive text summarization and sentence compression are tasks that basically select a subset of the input set of textual units that is appropriate as a summary or a compressed sentence. Current text summarization and sentence compression methods regard the problem of extracting such a subset as a combinatorial optimization problem (e.g., (Filatova and Hatzivassiloglou, 2004; McDonald, 2007; Lin and Bilmes, 2010)). Tree trimming, the problem of finding an optimal subtree of an input tree, is one kind of these combinatorial optimization problems, and it is used in three classes of text summarizations: sentence compression (Filippova and Strube, 2008; Filippova and Altun, 2013), single-document summarization (Hirao et al., 2013), and the combination of sentence compression and single-document summarization (Kikuchi et al., 2014). In these tasks, the set of input textual units is represented as a rooted tree whose nodes correspond to the minimum textual units such as sentences and words. Next, a subset i"
N15-1049,P11-1052,0,0.0395879,"properties required. However, their complexity makes it difficult 469 to solve optimization problems efficiently. These problems can be solved by using ILP solvers, however, they may fail to find optimal solutions and they have no guarantee on the running time. Since the proposed method is a DP algorithm and it has a theoretical guarantee, it always find an optimal solution in time proportional to the size of the input tree. Our method also can be seen as a kind of fast text summarization algorithm. Previous fast algorithms are approximate algorithms (Qian and Liu, 2013; Lin and Bilmes, 2010; Lin and Bilmes, 2011; Davis et al., 2012), while our algorithm is an exact algorithm. Of course, there is a difference in task hardness since previous methods were designed for multi-document summarization and ours for single document summarization. Those works suggest the label order used in this paper to give a theoretical bound that only depends on the size of an input tree. Nevertheless, the bound attained with this extension is worse than that shown in this paper. Solution time (ms) 2500 ZDD Gurobi 2000 1500 1000 10 500 0 0 5000 10000 15000 20000 25000 Number of tree nodes Figure 7: Solution time of our algo"
N15-1049,W09-1801,0,0.0186519,"00 1000 1500 Number of tree nodes 2000 (c) Extraction & compression Figure 6: ZDD sizes with number of input tree nodes trimming problem since it is the most complex problem. We make a large artificial nested tree by concatenating outer-trees of the nested trees of 30 RSTDT datasets. The results are shown in Fig. 7, and it shows that out method scales well with large inputs comparing with Gurobi. 9 Related Work Recently proposed text summarization and sentence compression methods solve a task by formulating it as a combinatorial optimization problem (McDonald, 2007; Woodsend and Lapata, 2010; Martins and Smith, 2009; Clarke and Lapata, 2008). These combinatorial optimization-based formulations enable flexible models that can reflect the properties required. However, their complexity makes it difficult 469 to solve optimization problems efficiently. These problems can be solved by using ILP solvers, however, they may fail to find optimal solutions and they have no guarantee on the running time. Since the proposed method is a DP algorithm and it has a theoretical guarantee, it always find an optimal solution in time proportional to the size of the input tree. Our method also can be seen as a kind of fast t"
N15-1049,P00-1039,0,0.0991454,"al upper bounds when we represent the set of all subtrees of an input tree. ZDD uses O(N log N ) nodes to represent the set of all subtrees of an N node input tree. Hence the DP algorithm runs in O(N L log N ) time. The main virtues of the proposed algorithm are that (1) it can always find an exact solution, (2) its running time is theoretically guaranteed, and (3) it can solve the three known tree trimming problems. Furthermore, our algorithm is fast enough to be practical and scalable. Since text summarization methods are often applied to large scale inputs (e.g., (Christensen et al., 2014; Nakao, 2000)), scalability is important. We compare it to state-of-the-art ILP solvers and confirm that the proposed algorithm can be hundreds of times faster. Since our method assumes known formuations for text summarization, the summary created by our algorithm is exactly the same as that obtained by applying previous methods. However, we believe that algorithmic improvements in computational cost is as important as improvements in accuracy in order to make better practical systems. 2 Tree Trimming Problems We briefly review the three tree trimming formulations used in text summarization and sentence co"
N15-1049,D13-1156,0,0.0203098,"able flexible models that can reflect the properties required. However, their complexity makes it difficult 469 to solve optimization problems efficiently. These problems can be solved by using ILP solvers, however, they may fail to find optimal solutions and they have no guarantee on the running time. Since the proposed method is a DP algorithm and it has a theoretical guarantee, it always find an optimal solution in time proportional to the size of the input tree. Our method also can be seen as a kind of fast text summarization algorithm. Previous fast algorithms are approximate algorithms (Qian and Liu, 2013; Lin and Bilmes, 2010; Lin and Bilmes, 2011; Davis et al., 2012), while our algorithm is an exact algorithm. Of course, there is a difference in task hardness since previous methods were designed for multi-document summarization and ours for single document summarization. Those works suggest the label order used in this paper to give a theoretical bound that only depends on the size of an input tree. Nevertheless, the bound attained with this extension is worse than that shown in this paper. Solution time (ms) 2500 ZDD Gurobi 2000 1500 1000 10 500 0 0 5000 10000 15000 20000 25000 Number of tr"
N15-1049,P10-1058,0,0.0272796,"compression 150 2000 0 0 500 1000 1500 Number of tree nodes 2000 (c) Extraction & compression Figure 6: ZDD sizes with number of input tree nodes trimming problem since it is the most complex problem. We make a large artificial nested tree by concatenating outer-trees of the nested trees of 30 RSTDT datasets. The results are shown in Fig. 7, and it shows that out method scales well with large inputs comparing with Gurobi. 9 Related Work Recently proposed text summarization and sentence compression methods solve a task by formulating it as a combinatorial optimization problem (McDonald, 2007; Woodsend and Lapata, 2010; Martins and Smith, 2009; Clarke and Lapata, 2008). These combinatorial optimization-based formulations enable flexible models that can reflect the properties required. However, their complexity makes it difficult 469 to solve optimization problems efficiently. These problems can be solved by using ILP solvers, however, they may fail to find optimal solutions and they have no guarantee on the running time. Since the proposed method is a DP algorithm and it has a theoretical guarantee, it always find an optimal solution in time proportional to the size of the input tree. Our method also can be"
N18-1155,P17-2021,0,0.0129318,"ed methods for sentence compression use syntactic features. Filippova et al. (2015) employs the features obtained from automatic parse trees in the LSTM-based encoder-decoder in a pipeline manner. Wang et al. (2017) trims dependency trees based on the scores predicted by an LSTM-based tagger. Although these methods can consider dependency relationships between words, the pipeline approach and the 1st-order dependency relationship fail to compress longer than average sentences. Several recent machine translation studies also utilize syntactic features in Seq2Seq models. Eriguchi et al. (2017); Aharoni and Goldberg (2017) incorporate syntactic features of the target language in the decoder part of Seq2Seq. Both methods outperformed Seq2Seq without syntactic features in terms of translation quality. However, both methods fail to provide an entire parse tree until the decoding phase is finished. Thus, these methods cannot track all possible parents for each word within the decoding process. Similar to HiSAN, Hashimoto and Tsuruoka (2017) use dependency features as attention distributions, but different from HiSAN, they use pre-trained dependency relations, and do not take into account the chains of dependencies."
N18-1155,D17-1209,0,0.011857,"of the target language in the decoder part of Seq2Seq. Both methods outperformed Seq2Seq without syntactic features in terms of translation quality. However, both methods fail to provide an entire parse tree until the decoding phase is finished. Thus, these methods cannot track all possible parents for each word within the decoding process. Similar to HiSAN, Hashimoto and Tsuruoka (2017) use dependency features as attention distributions, but different from HiSAN, they use pre-trained dependency relations, and do not take into account the chains of dependencies. Marcheggiani and Titov (2017); Bastings et al. (2017) consider higherorder dependency relationships in Seq2Seq by incorporating a graph convolution technique (Kipf and Welling, 2016) into the encoder. However, the dependency information of the graph convolution technique is still given in pipeline manner. Unlike the above methods, HiSAN can capture higher-order dependency features using d-length dependency chains without relying on pipeline processing. 7 Conclusion In this paper, we incorporated higher-order dependency features into Seq2Seq to compress sentences of all lengths. Experiments on the Google sentence compression test data showed that"
N18-1155,D17-1012,0,0.114686,"Missing"
N18-1155,P11-1049,0,0.0302536,"eadability and informativeness. 80 F1 85 75 70 65 60 16-20 21-25 26-30 31-35 36-40 Sentence Length 41-45 46-50 51-55 Figure 1: F1 scores of LSTM-based sentence compression method for each sentence length.1 Average Depth 12 11 10 9 8 7 6 5 4 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 51-55 Sentence Length Figure 2: Average tree depths for each sentence length.2 Introduction Sentence compression is the task of compressing long sentences into short and concise ones by deleting words. To generate compressed sentences that are grammatical, many researchers (Jing, 2000; Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova and Altun, 2013) have adopted tree trimming methods. Even though Filippova and Altun (2013) reported the best results on this task, automatic parse errors greatly degrade the performances of these tree trimming methods. 1 11-15 We used an LSTM-based sentence compression method (Filippova et al., 2015) in the evaluation setting as described in Section 4.1. Recently, Filippova et al. (2015) proposed an LSTM sequence-to-sequence (Seq2Seq) based sentence compression method that can generate fluent sentences without utilizing any syntactic features. Therefore, Seq2Seq based sentence com"
N18-1155,W14-4012,0,0.0424489,"Missing"
N18-1155,P17-2012,0,0.0150649,"veral neural network based methods for sentence compression use syntactic features. Filippova et al. (2015) employs the features obtained from automatic parse trees in the LSTM-based encoder-decoder in a pipeline manner. Wang et al. (2017) trims dependency trees based on the scores predicted by an LSTM-based tagger. Although these methods can consider dependency relationships between words, the pipeline approach and the 1st-order dependency relationship fail to compress longer than average sentences. Several recent machine translation studies also utilize syntactic features in Seq2Seq models. Eriguchi et al. (2017); Aharoni and Goldberg (2017) incorporate syntactic features of the target language in the decoder part of Seq2Seq. Both methods outperformed Seq2Seq without syntactic features in terms of translation quality. However, both methods fail to provide an entire parse tree until the decoding phase is finished. Thus, these methods cannot track all possible parents for each word within the decoding process. Similar to HiSAN, Hashimoto and Tsuruoka (2017) use dependency features as attention distributions, but different from HiSAN, they use pre-trained dependency relations, and do not take into accoun"
N18-1155,D15-1042,0,0.183986,"Missing"
N18-1155,D13-1155,0,0.704104,"80 F1 85 75 70 65 60 16-20 21-25 26-30 31-35 36-40 Sentence Length 41-45 46-50 51-55 Figure 1: F1 scores of LSTM-based sentence compression method for each sentence length.1 Average Depth 12 11 10 9 8 7 6 5 4 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 51-55 Sentence Length Figure 2: Average tree depths for each sentence length.2 Introduction Sentence compression is the task of compressing long sentences into short and concise ones by deleting words. To generate compressed sentences that are grammatical, many researchers (Jing, 2000; Knight and Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova and Altun, 2013) have adopted tree trimming methods. Even though Filippova and Altun (2013) reported the best results on this task, automatic parse errors greatly degrade the performances of these tree trimming methods. 1 11-15 We used an LSTM-based sentence compression method (Filippova et al., 2015) in the evaluation setting as described in Section 4.1. Recently, Filippova et al. (2015) proposed an LSTM sequence-to-sequence (Seq2Seq) based sentence compression method that can generate fluent sentences without utilizing any syntactic features. Therefore, Seq2Seq based sentence compression is a promising alte"
N18-1155,I17-2002,1,0.704068,"β0,t indicates the weight when the method does not use the dependency features. Context vector ct is calculated as ct = ← − − → → [ h 0, h n, − s t ] using the current decoder hidden → state − s t. The calculated Ωt is concatenated and input to the output layer. In detail, dt in Eq. (5) is replaced → s t ]; furtherby concatenated vector d′t = [ht , Ωt , − ′ more, instead of dt , dt is also fed to the input of the decoder LSTM at t + 1. 3.3 Objective Function To alleviate the influence of parse errors, we jointly update the 1st-order attention distribution α1,t,k and label probability P (y|x) (Kamigaito et al., 2017). The 1st-order attention distribution is learned by dependency parse trees. If at,j = 1 is an edge between parent word wj and child wt on a dependency tree (at,j = 0 denotes that wj is not a parent of wt .), the objective function of our method can be defined as: −logP (y|x) − λ · n ∑ n ∑ j=1 t=1 at,j · logα1,t,j , (14) where λ is a hyper-parameter that controls the importance of the output labels and parse trees in the training dataset. 4 Evaluations 4.1 Evaluation Settings 4.1.1 Dataset This evaluation used the Google sentence compression dataset (Filippova and Altun, 2013)5 . This dataset"
N18-1155,N16-1179,0,0.252118,"82.7, respectively. In particular, HiSAN attained remarkable compression performance with long sentences. In human evaluations, HiSAN also outperformed the baseline methods. 2 Baseline Sequence-to-Sequence Method Sentence compression can be regarded as a tagging task, where given a sequence of input tokens x = (x0 , ..., xn ), a system assigns output label yt , which is one of three types of specific labels (“keep,”“delete,” or“end of sentence”) to each input token xt (1 ≤ t ≤ n). The LSTM-based approaches for sentence compression are mostly based on the bi-LSTM based tagging method (Tagger) (Klerke et al., 2016; Wang et al., 2017; Chen and Pan, 2017) or Seq2Seq (Filippova et al., 2015; Tran et al., 2016). Tagger independently predicts labels in a point estimation manner, whereas Seq2Seq predicts labels by considering previously predicted labels. Since Seq2Seq is more expressive than Tagger, we built HiSAN on the baseline Seq2Seq model. Our baseline Seq2Seq is a version of Filippova et al. (2015) extended through the addition of bi-LSTM, an input feeding approach (Vinyals et al., 2015; Luong et al., 2015), and a monotonic hard attention method (Yao and Zweig, 2015; Tran et al., 2016). As described in"
N18-1155,W17-3204,0,0.0215914,"Missing"
N18-1155,P04-1077,0,0.0109078,"ach training epoch. The clipping threshold of the gradient was set to 5.0. We selected trained models with early stopping based on maximizing per-sentence accuracy (i.e., how many compressions could be fully reproduced) of the development data set. To obtain a compressed sentence, we used greedy decoding, rather than beam decoding, as the latter attained no gain in the development dataset. All methods were written in C++ on Dynet (Neubig et al., 2017). 4.2 Automatic Evaluation In the automatic evaluation, we used token level F1 -measure (F1 ) as well as recall of ROUGE-1, ROUGE-2 and ROUGE-L (Lin and Och, 2004)9 as evaluation measures. We used ∆C = system compression ratio − gold compression ratio to evaluate how close the compression ratio of system outputs was to that of gold compressed sentences. The average compression ratio of the gold compression for input sentence was 39.8. We used micro-average for F1 -measure and compression ratio10 , and macroaverage for ROUGE scores, respectively. To verify the benefits of our methods on long sentences, we additionally report scores on sentences longer than the average sentence length (= 28) in the test set. The average compression ratio of the gold compr"
N18-1155,D15-1166,0,0.539389,"proaches for sentence compression are mostly based on the bi-LSTM based tagging method (Tagger) (Klerke et al., 2016; Wang et al., 2017; Chen and Pan, 2017) or Seq2Seq (Filippova et al., 2015; Tran et al., 2016). Tagger independently predicts labels in a point estimation manner, whereas Seq2Seq predicts labels by considering previously predicted labels. Since Seq2Seq is more expressive than Tagger, we built HiSAN on the baseline Seq2Seq model. Our baseline Seq2Seq is a version of Filippova et al. (2015) extended through the addition of bi-LSTM, an input feeding approach (Vinyals et al., 2015; Luong et al., 2015), and a monotonic hard attention method (Yao and Zweig, 2015; Tran et al., 2016). As described in the evaluations section, this baseline achieved comparable or even better scores than the state-of-the-art scores reported in Filippova et al. (2015). The baseline Seq2Seq model consists of embedding, encoder, decoder, and output layers. In the embedding layer, the input tokens x are converted to the embeddings e. As reported in Wang et al. (2017), syntactic features are important for learning a generalizable embedding for sentence compression. Following their results, we also introduce syntactic"
N18-1155,D17-1159,0,0.0343335,"incorporate syntactic features of the target language in the decoder part of Seq2Seq. Both methods outperformed Seq2Seq without syntactic features in terms of translation quality. However, both methods fail to provide an entire parse tree until the decoding phase is finished. Thus, these methods cannot track all possible parents for each word within the decoding process. Similar to HiSAN, Hashimoto and Tsuruoka (2017) use dependency features as attention distributions, but different from HiSAN, they use pre-trained dependency relations, and do not take into account the chains of dependencies. Marcheggiani and Titov (2017); Bastings et al. (2017) consider higherorder dependency relationships in Seq2Seq by incorporating a graph convolution technique (Kipf and Welling, 2016) into the encoder. However, the dependency information of the graph convolution technique is still given in pipeline manner. Unlike the above methods, HiSAN can capture higher-order dependency features using d-length dependency chains without relying on pipeline processing. 7 Conclusion In this paper, we incorporated higher-order dependency features into Seq2Seq to compress sentences of all lengths. Experiments on the Google sentence compressi"
N18-1155,E17-1063,0,0.0383264,"This is because the output length is the same as the input length, and each xt can be assigned to each yt in a one-to-one correspondence. 1718 1. Parent Attention module calculates Pparent (xj |xt , x), the probability of xj being the parent of xt , by using hj and ht . This probability is calculated for all pairs of xj , xt . The arc in Figure 5 shows the most probable dependency parent for each child token. y1 y6 y7 sof tmax sof tmax sof tmax h 1 Ω1 h 7 Ω7 h 6 Ω6 − → s1 − → s6 y0 x 1 y5 x 6 ← − h0 − → s7 y6 Recursive Attention (d = {1, 2, 3}) d=3 γ3,7 Σ γ2,7 Σ 3.2.1 Parent Attention Module Zhang et al. (2017) formalized dependency parsing as the problem of independently selecting the parent of each word in a sentence. They produced a distribution over possible parents for each child word by using the attention layer on bi-LSTM hidden layers. In a dependency tree, a parent has more than one child. Under this constraint, dependency parsing is represented as follows. Given sentence S = (x0 , x1 , ..., xn ), the parent of xj is selected from S  xi for each token S  x0 . Note that x0 denotes the root node. The probability of token xj being the parent of token xt in sentence x is calculated as follows"
N18-1155,W14-4009,0,0.0338832,"Missing"
N18-1157,P11-1049,0,0.211654,"ften used, which is advantageous in that each compressed sentence preserves its original dependency relations. Specifically, given a set of dependency trees constructed for sentences in the original documents, a summary is obtained by extracting some rooted subtrees; each subtree corresponds to a compressed sentence. Different from the extractive summarization, the dependency relations in each sentence must be taken into account, and hence the aforementioned extractive methods cannot be applied to compressive summarization. A number of methods have been proposed for compressive summarization (Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Morita et al., 2013; Kikuchi et al., 2014; Hirao et al., 2017a). These methods formulate summarization as a type of combinatorial optimization problem with a tree constraint, and they obtain summaries by solving the problem. Unfortunately, the existing methods have two drawbacks: (1) The class of objective functions to which they are applicable is limited; for example, they work only with the linear function or coverage function. As a result, the performance of these methods cannot be improved by elaborating the objective functions. (2) They contain costly procedur"
N18-1157,P16-1046,0,0.0711965,"Missing"
N18-1157,N16-2014,0,0.106628,"ed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016); examples of such functions include the coverage function, diversity reward function, and ROUGE. Therefore, the method can deliver high performance by using monotone submodular objective functions that are suitable for the given tasks. (2) The efficient greedy algorithm is effective for the submodular maximization problem, which provides fast summarization systems. (3) Theoretical performance guarantees of the greedy algorithm can be proved; for example, a 12 (1 − e−1 )approximation guarantee can be obtained. Although the above extractive methods successfully obtain summaries with high ROUGE"
N18-1157,de-marneffe-etal-2006-generating,0,0.245993,"Missing"
N18-1157,C04-1057,0,0.134823,"Missing"
N18-1157,D13-1155,0,0.142678,"objective functions well-suited for document summarization have submodularity and monotonicity; examples of such functions include the coverage function, diversity reward function, and ROUGE, to name a few. 3 Problem Statements We formulate the summarization task as the following subtree extraction problem called STKP hereafter. In what follows, we let [M ] := {1, . . . , M } for any positive integer M . We attempt to summarize document data consisting of N sentences. Each sentence forms a dependency tree, which can be constructed by using existing methods (e.g., (Filippova and Strube, 2008; Filippova and Altun, 2013)). For convenience, we call the dependency tree of a sentence the sentence tree. The i-th sentence (i ∈ [N ]) yields sentence tree Ti = (Vi , Ei ) rooted at ri ∈ Vi , where Vi is a set of textual units (e.g., words or chunks) contained in the i-th sentence, and edges in Ei represent their dependency relations. We define a document tree with a dummy root vertex r as T := ({r} ∪ V, E), where V and E are vertex and edge sets, respectively, defined as follows: V := [ i∈[N ] Vi , E := [ i∈[N ] {Ei ∪ {(r, ri )}}. Namely, V is the set of all textual units contained in the document data, and edges in"
N18-1157,W08-1105,0,0.446485,"me sentences often includes many redundant parts. As a result, if the limitation placed on summary length is tight, the extractive approach cannot yield an informative summary. Compressive summarization is known to be effective in overcoming this problem. With this approach, a summary is constructed with some 1737 Proceedings of NAACL-HLT 2018, pages 1737–1746 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics compressed sentences, and thus we can obtain a concise and informative summary. To make compressed sentences, the dependency-tree-based approach (Filippova and Strube, 2008) is often used, which is advantageous in that each compressed sentence preserves its original dependency relations. Specifically, given a set of dependency trees constructed for sentences in the original documents, a summary is obtained by extracting some rooted subtrees; each subtree corresponds to a compressed sentence. Different from the extractive summarization, the dependency relations in each sentence must be taken into account, and hence the aforementioned extractive methods cannot be applied to compressive summarization. A number of methods have been proposed for compressive summarizat"
N18-1157,P17-2043,1,0.86869,"69; Cheng and Lapata, 2016; Peyrard and Eckle-Kohler, 2017). Owing to the recent advances in data collection, the size of document data to be summarized has been exploding, which has been bringing a drastic increase in the demand for fast summarization systems. Extractive summarization is a widely used approach to designing fast summarization systems. With this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016)"
N18-1157,E17-1037,1,0.939011,"69; Cheng and Lapata, 2016; Peyrard and Eckle-Kohler, 2017). Owing to the recent advances in data collection, the size of document data to be summarized has been exploding, which has been bringing a drastic increase in the demand for fast summarization systems. Extractive summarization is a widely used approach to designing fast summarization systems. With this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016)"
N18-1157,P14-2052,1,0.842528,"original dependency relations. Specifically, given a set of dependency trees constructed for sentences in the original documents, a summary is obtained by extracting some rooted subtrees; each subtree corresponds to a compressed sentence. Different from the extractive summarization, the dependency relations in each sentence must be taken into account, and hence the aforementioned extractive methods cannot be applied to compressive summarization. A number of methods have been proposed for compressive summarization (Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Morita et al., 2013; Kikuchi et al., 2014; Hirao et al., 2017a). These methods formulate summarization as a type of combinatorial optimization problem with a tree constraint, and they obtain summaries by solving the problem. Unfortunately, the existing methods have two drawbacks: (1) The class of objective functions to which they are applicable is limited; for example, they work only with the linear function or coverage function. As a result, the performance of these methods cannot be improved by elaborating the objective functions. (2) They contain costly procedures as their building blocks: integer-linear-programming (ILP) solvers,"
N18-1157,W04-1013,0,0.12033,"trieval (Luhn, 1958; Edmundson, 1969; Cheng and Lapata, 2016; Peyrard and Eckle-Kohler, 2017). Owing to the recent advances in data collection, the size of document data to be summarized has been exploding, which has been bringing a drastic increase in the demand for fast summarization systems. Extractive summarization is a widely used approach to designing fast summarization systems. With this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin an"
N18-1157,N10-1134,0,0.464084,"ith this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016); examples of such functions include the coverage function, diversity reward function, and ROUGE. Therefore, the method can deliver high performance by using monotone submodular objective functions that are suitable for the given tasks. (2) The efficient greedy algorithm is effective for the submodular maximization problem, which provides fast summarizatio"
N18-1157,P11-1052,0,0.106124,"Missing"
N18-1157,P13-1101,0,0.0389869,"Missing"
N18-1157,P17-1100,0,0.025609,"Missing"
N18-1157,E09-1089,0,0.324943,"fast summarization systems. Extractive summarization is a widely used approach to designing fast summarization systems. With this approach, we construct a summary by extracting some sentences from the original document(s). The extractive approach is not only fast but also has the potential to achieve state-of-theart ROUGE scores (Lin, 2004), which was revealed by Hirao et al. (2017b). In many existing methods, sentences are extracted by solving various subset selection problems: for example, the knapsack problem (McDonald, 2007), maximum coverage problem (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a), budgeted median problem (Takamura and Okumura, 2009b), and submodular maximization problem (Lin and Bilmes, 2010). Of particular interest, the method based on submodular maximization has three advantages: (1) Many objective functions used for document summarization are known to be monotone and submodular (Lin and Bilmes, 2011; J Kurisinkel et al., 2016); examples of such functions include the coverage function, diversity reward function, and ROUGE. Therefore, the method can deliver high performance by using monotone submodular objective functions that are suitable for the given tasks. (2)"
N18-2104,hovy-etal-2006-automated,0,0.0728621,"ally overlapped basic elements based on word similarity. Even though it is simple, pBE outperforms ROUGE in DUC datasets in most cases and achieves the highest rank correlation coefficient in TAC 2011 AESOP task. 1 Introduction Automatic evaluation measures have a significant impact on the research on summarization. Since there is no other practical way to quickly evaluate the quality of system summaries, summarization studies work on raising the scores that are given by automatic evaluation measures. Among the automatic evaluation measures, the most popular ones are ROUGE (Lin, 2004) and BE (Hovy et al., 2006). ROUGE/BE counts the number of ngrams/basic elements1 that match those in manual reference summaries. ROUGE normally employs unigrams or bigrams while BE uses dependency triples (head|modifier|relation) as their units. It is known that both ROUGE and BE are well correlated with human judgment. Their evaluation approach, however, is quite different from humans’ in two ways: they score low-information units higher and ignore the semantic overlap of units. The first problem is 2 Related Work ROUGE-WE (Ng and Abrecht, 2015) and BEwTE (Tratz and Hovy, 2008) are closely related to our method in tha"
N18-2104,W04-1013,0,0.0108104,") reducing semantically overlapped basic elements based on word similarity. Even though it is simple, pBE outperforms ROUGE in DUC datasets in most cases and achieves the highest rank correlation coefficient in TAC 2011 AESOP task. 1 Introduction Automatic evaluation measures have a significant impact on the research on summarization. Since there is no other practical way to quickly evaluate the quality of system summaries, summarization studies work on raising the scores that are given by automatic evaluation measures. Among the automatic evaluation measures, the most popular ones are ROUGE (Lin, 2004) and BE (Hovy et al., 2006). ROUGE/BE counts the number of ngrams/basic elements1 that match those in manual reference summaries. ROUGE normally employs unigrams or bigrams while BE uses dependency triples (head|modifier|relation) as their units. It is known that both ROUGE and BE are well correlated with human judgment. Their evaluation approach, however, is quite different from humans’ in two ways: they score low-information units higher and ignore the semantic overlap of units. The first problem is 2 Related Work ROUGE-WE (Ng and Abrecht, 2015) and BEwTE (Tratz and Hovy, 2008) are closely r"
N18-2104,P14-5010,0,0.00395057,"s of the datasets. “Evaluation” represents manual evaluation methods and “Limit” represents word limits of summarization. Combined with step 1, fully pruned BE is defined as follows: pBE−cnt+cls (R, S) = PK PMk Evaluation coverage coverage responsiveness responsiveness pyramid responsiveness pyramid pyramid method ROUGE-WE. We chose the latest AESOP dataset, TAC 2011, for which ROUGE-WE achieved the highest Spearman coefficient (Ng and Abrecht, 2015). The details of our experimental setup are given in Table 3 and below. Parser: We used the neural-network dependency parser of Stanford CoreNLP (Manning et al., 2014). Dependencies were set to enhanced++ Universal Dependencies (Schuster and Manning, 2016). Clustering: We employed hierarchical clustering, maximum distance method. The number of clusters, N , was set to 0.975 ∗ Q. Word Embeddings: A set of pre-trained Google-News word embeddings10 . It contains 3 million words, each of which has a word embedding of 300 dimensions. (3) Experimental Setup To assess the effectiveness of pBE, we computed the correlation coefficient between pBE scores and human judgments, as well as between the scores of other automatic evaluation measures and manual scores for co"
N18-2104,D15-1222,0,0.0850817,"tic evaluation measures, the most popular ones are ROUGE (Lin, 2004) and BE (Hovy et al., 2006). ROUGE/BE counts the number of ngrams/basic elements1 that match those in manual reference summaries. ROUGE normally employs unigrams or bigrams while BE uses dependency triples (head|modifier|relation) as their units. It is known that both ROUGE and BE are well correlated with human judgment. Their evaluation approach, however, is quite different from humans’ in two ways: they score low-information units higher and ignore the semantic overlap of units. The first problem is 2 Related Work ROUGE-WE (Ng and Abrecht, 2015) and BEwTE (Tratz and Hovy, 2008) are closely related to our method in that they aim to improve unit matching. ROUGE-WE exploits word embeddings to softly match ngrams based on their cosine similarities. Although this also takes semantic correspondence into consideration, it is different from pBE because it does not judge word similarity within one summary, but only between a target sum1 We use “BE” to represent the evaluation method Basic Elements, “basic element(s)” to represent the fragments of Basic Elements and “unit” as a general term of ngrams and basic elements. 661 Proceedings of NAAC"
N18-2104,L16-1376,0,0.0603922,"Missing"
P03-1005,C02-1054,0,\N,Missing
P03-1005,W02-2016,0,\N,Missing
P09-1093,P06-1048,0,0.241824,"del (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determined by the tree structures. This approach is more suitable fo"
P09-1093,W04-3239,0,0.0304391,"s output by a state-of-the-art Japanese dependency parser contain at least one error (Kudo and Matsumoto, 2005). Even more, it is well known that if we parse a sentence whose source is different from the training data of the parser, the performance could be much worse. This critically degrades the overall performance of sentence compression. Moreover, summarization systems often have to process megabytes of documents. Parsers are still slow and users of on2 Generally, a dependency relation is defined between bunsetsu. Therefore, in order to identify word dependencies, we followed Kudo’s rule (Kudo and Matsumoto, 2004) 827 3.2.1 Intra-sentence Positional Term Weighting (IPTW) IDF is a global term weighting scheme in that it measures the significance score of a word in a text corpus, which could be extremely large. By contrast, this paper proposes another type of term weighting; it measures the positional significance score of a word within its sentence. Here, we assume the following hypothesis: demand summarization systems are not prepared to wait for parsing to finish. 3 A Syntax Free Sequence-oriented Sentence Compression Method As an alternative to syntactic parsing, we propose two novel features, intra-"
P09-1093,E06-1038,0,0.127006,"thods trim dependency trees by the discriminative model (Takeuchi and Matsumoto, 2001; Nomoto, 2008) through the use of simple linear combined features (Oguro et al., 2002). The tree trimming approach guarantees that the compressed sentence is grammatical if the source sentence does not trigger parsing error. However, as we mentioned in Section 2, the tree trimming approach is not suitable for Japanese sentence compression because in many cases it cannot reproduce human-produced compressions. As an alternative to these tree trimming approaches, sequence-oriented approaches have been proposed (McDonald, 2006; Nomoto, 2007; Hori and Furui, 2003; Clarke and Lapata, 2006). Nomoto (2007) and McDonald (2006) employed the random field based approach. Hori et al. (2003) and Clarke et al. (2006) employed the linear model with simple combined features. They simply regard a sentence as a word sequence and structural information, such as full parse tree or dependency trees, are encoded in the sequence as features. The advantage of these methods over the tree trimming approach is that they have the potential to drop arbitrary words from the original sentence without the need to consider the boundaries determ"
P09-1093,P08-1035,0,0.413878,"Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Chunk 4 ta Chunk 2 が fukutake た nitsuite 福武 fukutake が ga Chunk 7 枝問 部分 の Compression Chunk 5 公表 し て い kouhyou shi te i センタ試験枝問 center shiken edamon edamon bub"
P09-1093,P02-1040,0,0.0783887,"Corpus and Evaluation Measures We randomly selected 1,000 lead sentences (a lead sentence is the first sentence of an article excluding the headline.) whose length (number of words) was greater than 30 words from the Mainichi Newspaper from 1994 to 2002. There were five different ideal compressions (reference compressions produced by human) for each sentence; all had a 0.6 compression rate. The average length of the input sentences was about 42 words and that of the reference compressions was about 24 words. For MCE learning, we selected the reference compression that maximize the BLEU score (Papineni et al., 2002) (= argmaxr∈R BLEU(r, R)) from the set of reference compressions and used it as correct data for training. Note that r is a reference compression and R is the set of reference compressions. We employed both automatic evaluation and human subjective evaluation. For automatic evaluation, we employed BLEU (Papineni et al., 2002) by following (Unno et al., 2006). We utilized 5fold cross validation, i.e., we broke the whole data set into five blocks and used four of them for training and the remainder for testing and repeated the evaluation on the test data five times changing the test block each"
P09-1093,P05-1036,0,0.566966,"show that our method outperforms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Ch"
P09-1093,P06-2109,0,0.213731,"forms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method. 1 Introduction In order to compress a sentence while retaining its original meaning, the subject-predicate relationship of the original sentence should be preserved after compression. In accordance with this idea, conventional sentence compression methods employ syntactic parsers. English sentences are usually analyzed by a full parser to make parse trees, and the trees are then trimmed (Knight and Marcu, 2002; Turner and Charniak, 2005; Unno et al., 2006). For Japanese, dependency trees are trimmed instead of full parse trees (Takeuchi and Matsumoto, 2001; Oguro et al., 2002; Nomoto, 2008)1 This parsing approach is reasonable because the compressed output is grammatical if the 1 Hereafter, we refer these compression processes as “tree trimming.” 826 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826–833, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Chunk 1 推定 し suitei shi Chunk 2 推定 し た suitei shi ta Chunk 3 配点 について haiten Chunk 1 nitsuite 福武 Chunk 3 配点 について haiten ga Chunk 4 ta Chunk 2 が f"
P09-1093,P05-1012,0,\N,Missing
P12-2068,J10-3005,0,0.0355848,"p), and SRL-based F1 (F1-SRL). In driven”. In addition, “say” is the main verb in this our experiment, we have three models. McDonald sentence and hard to be deleted due to the syntactic is a re-implementation of McDonald (2006). Clarke significance. and Lapata (2008) also re-implemented McDonald’s The second example in Table 4 requires to idenmodel with an ILP solver and experimented it on the tify a coreference relation between artificial lake and WNC Corpus. 9 MLN with SRL and MLN w/o Roadford Reservour. We consider that discourse SRL are our Markov Logic models with and with- constraints (Clarke and Lapata, 2010) help our model out SR Constraints, respectively. handle these cases. Discourse and coreference inforNote our three models have no constraint for the mation enable our model to select important argulength of compression. Therefore, we think the com- ments and their predicates. pression rate of the better system should get closer to 5 Conclusion that of human compression. In comparison between In this paper, we proposed new semantic conMLN models and McDonald, the former models outstraints for sentence compression. Our model with perform the latter model on both F1-Dep and F1global constraints"
P12-2068,C08-1018,0,0.168832,"tion. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of predicates, arguments and their relapus (http://j"
P12-2068,N10-1131,0,0.343879,"features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of predicates, arguments and their relapus (http://jamesclarke.net/research/resources)."
P12-2068,W08-2123,0,0.0528298,"Missing"
P12-2068,C10-1081,0,0.021202,"dependency relation between Harari and became directly. SRs allow us to model the relations between a predicate and its arguments in a direct fashion. SR constraints are also advantageous in that we can compress sentences with semantic information. In Figure 1, became has three arguments, Harari as A1, businessman as A2, and shortly afterward as AM-TMP. As shown in this example, shortly afterword can be omitted (shaded boxes). In general, modifier arguments like AM-TMP or AM-LOC are more likely to be reduced than complement cases like A0-A4. We can implement such properties by SR constraints. Liu and Gildea (2010) suggests that SR features contribute to generating more readable sentence in machine translation. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based"
P12-2068,H05-1066,0,0.0795211,"Missing"
P12-2068,E06-1038,0,0.77525,"more readable sentence in machine translation. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of p"
P12-2068,N03-1026,0,0.0363577,"d) by role(i, j, +r). 4 Experiment and Result 4.1 Experimental Setup Our experimental setting follows previous work (Clarke and Lapata, 2008). As stated in Section 2, we employed the WNC Corpus. For preprocessing, we performed POS tagging by stanford-tagger. 5 and dependency parsing by MST-parser (McDonald et al., 2005). In addition, LTH 6 was exploited to perform both dependency parsing and SR labeling. We implemented our model by Markov Thebeast with Gurobi optimizer. 7 Our evaluation consists of two types of automatic evaluations. The first evaluation is dependency based evaluation same as Riezler et al. (2003). We performed dependency parsing on gold data and system outputs by RASP. 8 Then we calculated precision, recall, and F1 for the set of label(head, modi f ier). In order to demonstrate how well our SR constraints keep correct predicate-argument structures in compression, we propose SRL based evaluation. We performed SR labeling on gold data pos(i, +p1 ) ∧ pos(i + 1, +p2 ) ⇒ inComp(i). (9) POS features are often more reasonable than word 5 http://nlp.stanford.edu/software/tagger.shtml 6 form features to combine with the other properties. http://nlp.cs.lth.se/software/semantic_ parsing:_propban"
P13-2038,P08-1088,0,0.238888,"e proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods. 1 Introduction Unsupervised object matching is a method for finding one-to-one correspondences between objects across different domains without knowledge about the relation between the domains. Kernelized sorting (Novi et al., 2010) and canonical correlation analysis based methods (Haghighi et al., 2008; Tripathi et al., 2010) are two such examples of unsupervised object matching, which have been shown to be quite useful for cross-language natural language processing (NLP) tasks. One of the most important properties of the unsupervised object matching is that it does not require any linguistic resources which connects between the languages. This distinguishes it from other crosslanguage NLP methods such as machine transla212 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 212–216, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computat"
P13-2038,D10-1025,0,0.0534617,"Missing"
P13-2038,W05-0802,0,\N,Missing
P14-2052,P13-1020,0,0.132407,"Missing"
P14-2052,P13-1101,1,0.60525,"Missing"
P14-2052,P11-1049,0,0.200251,"Missing"
P14-2052,C10-2105,0,0.017709,"n based on RST use EDUs as extraction textual units. We converted the rhetorical relations 12 between EDUs to the relations between sentences to build the nested tree structure. We could 10 thus take into account both relations between sentences and relations between words. 8 Figure 2: Example of one sentence. Each line corresponds to one EDU. Number of selected sentences from source document tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese st"
P14-2052,W01-1605,0,0.409306,"Missing"
P14-2052,D13-1156,0,0.0298168,"ta to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and St"
P14-2052,N13-1136,0,0.00537175,"as extraction textual units. We converted the rhetorical relations 12 between EDUs to the relations between sentences to build the nested tree structure. We could 10 thus take into account both relations between sentences and relations between words. 8 Figure 2: Example of one sentence. Each line corresponds to one EDU. Number of selected sentences from source document tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted r"
P14-2052,P02-1057,0,0.0824756,"Missing"
P14-2052,P09-1075,0,0.015587,"Missing"
P14-2052,C04-1057,0,0.0689065,"rtant content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts. 1 Introduction Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; 315 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315–320, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source document John was running on a track in the park. He looks very tired. Mike said he is training for a race. The race is held next month. John was running on a track in the park. ＊ He looks ve"
P14-2052,W08-1105,0,0.109308,"an and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and Strube (2008) allows the to EDUs and whose nonterminal nodes indicate model to extract non-rooted subtrees in sentence the relations. Hirao et al. converted RST-DTs compression tasks that compress a single sentence into dependency-based discourse trees (DEP-DTs) with a given compression ratio. However, it is not whose nodes corresponded to EDUs and whose trivial to apply their method to text summarizaedges corresponded to the head modifier relationtion because no compression ratio is given to senships of EDUs. See Hirao et al. for details (Hirao tences. None of these methods use the discourse et al., 2013)"
P14-2052,W09-1802,0,0.0293277,"structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and Strube (2008) allows the to EDUs and whose nonterminal nodes indicate model to extract non-rooted subtr"
P14-2052,D13-1158,1,0.609684,"Missing"
P14-2052,W04-1013,0,0.03652,"Missing"
P14-2052,W98-1124,0,0.257936,"ct Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transformed RST trees into dependency trees and used them for single document summarization (Hirao et al., 2013). They formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees. We propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency parser. We have explained our method with an example in Figure 1. First, we represent a document as a"
P17-2043,P11-1049,0,0.0707887,"pper bound performance of the paradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming the dependency trees obtained"
P17-2043,P13-1020,0,0.0175913,"aradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming the dependency trees obtained from the document(s) (Marti"
P17-2043,de-marneffe-etal-2006-generating,0,0.30686,"Missing"
P17-2043,D13-1155,0,0.0615266,"ch topic to leverage our resources. Table 1: ROUGE scores and the number of sentences of extractive and compressive oracle summaries and those obtained from state-of-the-art summarization systems, RegSum and ICSISumm. n= 1 corresponds to ROUGE1 , n=2 corresponds to ROUGE2 , n=1+2 corresponds to ROUGE-SU0. “Sent.” indicates the average number of sentences in the summaries. sentences in the dataset to obtain dependency relations between words, and then we transformed them into trees that represent the dependency relations between chunks by applying Filippova’s rules (Filippova and Strube, 2008; Filippova and Altun, 2013). To solve the ILP problem, we utilized CPLEX version 12.5.1.0. We obtained and evaluated oracle summaries based on three variants of ROUGE, ROUGE1 , ROUGE2 and ROUGE -SU0, with the following conditions3 : (1) ROUGE1 , utilizing unigrams excluding stopwords (2) ROUGE2 , utilizing bigrams with stopwords, and (3) ROUGE -SU0, which is an extension of ROUGEn , utilizing unigram and bigram (excluding skip-bigram) statistics. 4.2 n=1 n=2 n=1 n=2 Results and Discussion Table 1 shows ROUGE scores of compressive and extractive oracle summaries and those of RegSum (Hong and Nenkova, 2014) that achieved"
P17-2043,W08-1105,0,0.12461,"rization is important research topic to leverage our resources. Table 1: ROUGE scores and the number of sentences of extractive and compressive oracle summaries and those obtained from state-of-the-art summarization systems, RegSum and ICSISumm. n= 1 corresponds to ROUGE1 , n=2 corresponds to ROUGE2 , n=1+2 corresponds to ROUGE-SU0. “Sent.” indicates the average number of sentences in the summaries. sentences in the dataset to obtain dependency relations between words, and then we transformed them into trees that represent the dependency relations between chunks by applying Filippova’s rules (Filippova and Strube, 2008; Filippova and Altun, 2013). To solve the ILP problem, we utilized CPLEX version 12.5.1.0. We obtained and evaluated oracle summaries based on three variants of ROUGE, ROUGE1 , ROUGE2 and ROUGE -SU0, with the following conditions3 : (1) ROUGE1 , utilizing unigrams excluding stopwords (2) ROUGE2 , utilizing bigrams with stopwords, and (3) ROUGE -SU0, which is an extension of ROUGEn , utilizing unigram and bigram (excluding skip-bigram) statistics. 4.2 n=1 n=2 n=1 n=2 Results and Discussion Table 1 shows ROUGE scores of compressive and extractive oracle summaries and those of RegSum (Hong and N"
P17-2043,W09-1802,0,0.0385364,"utilized CPLEX version 12.5.1.0. We obtained and evaluated oracle summaries based on three variants of ROUGE, ROUGE1 , ROUGE2 and ROUGE -SU0, with the following conditions3 : (1) ROUGE1 , utilizing unigrams excluding stopwords (2) ROUGE2 , utilizing bigrams with stopwords, and (3) ROUGE -SU0, which is an extension of ROUGEn , utilizing unigram and bigram (excluding skip-bigram) statistics. 4.2 n=1 n=2 n=1 n=2 Results and Discussion Table 1 shows ROUGE scores of compressive and extractive oracle summaries and those of RegSum (Hong and Nenkova, 2014) that achieved the best ROUGE1 and ICSISumm (Gillick and Favre, 2009; Gillick et al., 2009) that achieved the best ROUGE2 on the DUC-2004 dataset, respectively. We compare ROUGE scores of compressive oracle summaries with extractive oracle summaries. The best scores are obtained when we use the same ROUGE variant for both computation and evaluation (see bolded scores in Table 1). There are large differences between the best scores of ex4.3 Readability evaluation We conducted human evaluation to compare readability of extractive oracle summaries to that of compressive oracle summaries. We presented the oracle summaries to five human subjects and asked them to r"
P17-2043,E17-1037,1,0.865656,". As a result, researchers cannot know how much room for further improvement is left. Thus, it is beneficial to reveal the upper bound summary that achieves the maximum ROUGE score and can be produced by the systems. The upper bound summary is known as the oracle summary. To obtain the oracle summary on extractive summarization paradigms, several approaches have been proposed. Sipos et al. (2012) utilized a greedy algorithm, and Kubina et al. (2013) utilized exhaustive search based on heuristics. However, their oracle summaries do not always retain the optimal (maximum) ROUGE score. Recently, Hirao et al. (2017) derived an Integer Linear Programming (ILP) formulation to obtain the optimal oracle summary. Their oracle summary can help researchers to comprehend the strict limitation of the extractive summarization paradigm. However, their method cannot be applied to obtain compressive oracle summaries. To reveal the ultimate limitation of the compressive summarization paradigm, we propose an ILP formulation to obtain a compressive oracle summary that maximizes the ROUGE score. We conThis paper derives an Integer Linear Programming (ILP) formulation to obtain an oracle summary of the compressive summari"
P17-2043,hong-etal-2014-repository,0,0.0155404,"there is no dependency relationship between c3,3 and c3,5 . After solving the ILP problem, we can obtain compressive oracle summaries by collecting chunks according to bi,u =1. 4 Experiments To investigate the potential limitation of the compressive summarization paradigm, we compare ROUGE scores of compressive oracle summaries with those of extractive oracle summaries and those obtained from state-of-the-art summarization systems. Extractive oracle summaries are obtained by solving the ILP formulation proposed by (Hirao et al., 2017). System summaries are extracted from a public repository2 (Hong et al., 2014). We give an example to show how chunks and word sequences are related. When we pack a bigram “live in” in an oracle summary, there are four candidates in the source document (Fig. 1). Word subsequences, w1,6 ,w2,5 ,w2,6 and w3.9 match “live in”. Thus, T (live in) = {(1, 6), (2, 5), (2, 6), (3, 9)}. Here, when we want to pack w2,6 into the oracle summary, we have to pack both chunks c2,2 and c2,4 (b2,2 = b2,4 = 1) because U2 (w2,6 ) = {2, 4}. Then, we have to drop chunk c2,3 (b2,3 = 0) because c2,3 is within the gap between chunks c2,2 and c2,4 (V2 (w2,6 ) = 3). Similarly, when we pack w3,9 in"
P17-2043,E14-1075,0,0.013269,"rube, 2008; Filippova and Altun, 2013). To solve the ILP problem, we utilized CPLEX version 12.5.1.0. We obtained and evaluated oracle summaries based on three variants of ROUGE, ROUGE1 , ROUGE2 and ROUGE -SU0, with the following conditions3 : (1) ROUGE1 , utilizing unigrams excluding stopwords (2) ROUGE2 , utilizing bigrams with stopwords, and (3) ROUGE -SU0, which is an extension of ROUGEn , utilizing unigram and bigram (excluding skip-bigram) statistics. 4.2 n=1 n=2 n=1 n=2 Results and Discussion Table 1 shows ROUGE scores of compressive and extractive oracle summaries and those of RegSum (Hong and Nenkova, 2014) that achieved the best ROUGE1 and ICSISumm (Gillick and Favre, 2009; Gillick et al., 2009) that achieved the best ROUGE2 on the DUC-2004 dataset, respectively. We compare ROUGE scores of compressive oracle summaries with extractive oracle summaries. The best scores are obtained when we use the same ROUGE variant for both computation and evaluation (see bolded scores in Table 1). There are large differences between the best scores of ex4.3 Readability evaluation We conducted human evaluation to compare readability of extractive oracle summaries to that of compressive oracle summaries. We prese"
P17-2043,P14-2052,1,0.885796,"t showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming the dependency trees obtained from the document(s) (Martins and Smith, 2009; Almeida and Martins, 2"
P17-2043,W04-1013,0,0.139099,"Missing"
P17-2043,W09-1801,0,0.0314274,"essential to reveal the upper bound performance of the paradigm. Experimental results on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming"
P17-2043,D13-1156,0,0.0213451,"ts on the DUC dataset showed that ROUGE scores of compressive oracles are significantly higher than those of extractive oracles and state-of-the-art summarization systems. These results reveal that compressive summarization is a promising paradigm and encourage us to continue with the research to produce informative summaries. 1 Introduction Compressive summarization, a joint model integrating sentence extraction and sentence compression within a unified framework, has been attracting attention in recent years (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Qian and Liu, 2013; Kikuchi et al., 2014; Yao et al., 2015). Since compressive summarization methods can use a sub-sentence as an atomic unit, they can pack more information into summaries than extractive methods, which employ sentences as atomic units. Thus, compressive summarization is essential when we want to produce summaries under tight length constraints. There are two approaches to compress entire document(s) to be grammatical; one is trimming the phrase structure trees (Berg-Kirkpatrick et al., 2011) and the other is trimming the dependency trees obtained from the document(s) (Martins and Smith, 2009;"
P17-2043,E12-1023,0,0.0179702,"system summaries cannot achieve ROUGE=1 since summarization systems cannot reproduce reference summaries in most cases. In other words, the maximum ROUGE score that can be achieved by compressive summarization is unclear. As a result, researchers cannot know how much room for further improvement is left. Thus, it is beneficial to reveal the upper bound summary that achieves the maximum ROUGE score and can be produced by the systems. The upper bound summary is known as the oracle summary. To obtain the oracle summary on extractive summarization paradigms, several approaches have been proposed. Sipos et al. (2012) utilized a greedy algorithm, and Kubina et al. (2013) utilized exhaustive search based on heuristics. However, their oracle summaries do not always retain the optimal (maximum) ROUGE score. Recently, Hirao et al. (2017) derived an Integer Linear Programming (ILP) formulation to obtain the optimal oracle summary. Their oracle summary can help researchers to comprehend the strict limitation of the extractive summarization paradigm. However, their method cannot be applied to obtain compressive oracle summaries. To reveal the ultimate limitation of the compressive summarization paradigm, we propo"
W03-1024,P95-1017,0,0.0735895,"automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic r"
W03-1024,J95-2003,0,0.190148,"oun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic rules that take various features of antecedents and anaphors into account. We have to take even more factors into account, but it is difficult to maintain such heuristic rules. Th"
W03-1024,C02-1053,1,0.876791,"Missing"
W03-1024,W03-2604,0,0.217699,"independent, removing a heavily weighted feature does not necessarily degrade the system’s performance. Hence, feature elimination is more reliable for reducing the number of features. However, feature elimination takes a long time. On the other hand, feature weights can give rough guidance. According to the table, our new features (Parallel, Unfinished, and Intra) obtained relatively large weights. This implies their importance. When we eliminated these three features, vrads+svm2’s score for editorial dropped by 4 points. Therefore, combinations of these three features are useful. Recently, Iida et al. (2003a) proposed an SVMbased tournament model that compares two candidates and selects the better one. We would like to compare or combine their method with our method. For further improvement, we have to make the morphological analyzer and the dependency analyzer more reliable because they make many mistakes when they process complex sentences. SVM has often been criticized as being too slow. However, the above data were small enough for the state-of-the-art SVM programs. The number of examples in each set of training data was about 5,000– 6,100, and each training phase took only 5–18 seconds on a"
W03-1024,C02-1054,1,0.879479,"Missing"
W03-1024,P86-1031,0,0.512921,"ill increase. From this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles. In Japanese, anaphors are often omitted and these omissions are called zero pronouns. Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kamey"
W03-1024,N01-1025,0,0.0875759,"Missing"
W03-1024,C96-2147,0,0.635965,"t al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not sufficient for resolving zeros accurately. Murata and Nagao (1997) proposed complicated heuristic rules that take various features of antecedents and anaphors into account. We have to take even more factors into account, but it is difficult to maintain such heuristic rules. Therefore, recent studies employ machine learning approaches. However, it is also difficult to prepare a sufficient number of annotated corpora. In this paper, we propose a method that combines these two approac"
W03-1024,C02-1078,0,0.617086,"a / Bobu ni / hon wo / okutta. Tom=subj Bob=object2 book=object sent (Tom sent a book to Bob.) Bunsetsu dependency is represented by a list of bunsetsu (modifier, modified). For instance,    pairs      ""!  # indicates that there are four bunsetsus in this sentence and that the first bunsetsu modifies the fourth bunsetsu and so on. The last bunsetsu modifies no bunsetsu, which is in!  dicated by . It takes a long time to construct high-quality annotated data, and we want to compare our results with conventional methods. Therefore, we obtained Seki’s data (Seki et al., 2002a; Seki et al., 2002b), which are based on the Kyoto University Corpus 2 2.0. These data are divided into two groups: general and editorial. General contains 30 general news articles, and editorial contains 30 editorial articles. According to his experiments, editorial is harder than general. Perhaps this is caused by the difference in rhetorical styles and the lengths of articles. The average number of sentences in an editorial article is 28.7, while that in a general article is 13.9. However, we found problems in his data. For instance, the data contained ambiguous antecedents like dou-shi ("
W03-1024,J94-2003,0,0.624289,"om this motivation, we are developing our system toward the ability to resolve anaphors in full-text newspaper articles. In Japanese, anaphors are often omitted and these omissions are called zero pronouns. Since they do not give any hints (e.g., number or gender) about antecedents, automatic zero pronoun resolution is difficult. In this paper, we focus on resolving the zero pronoun, which is shortened for simplicity to ‘zero.’ Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa and Ikehara, 1996; Ehara and Kim, 1996) or organization names in newspaper articles (Aone and Bennett, 1995). There are two approaches to the problem: the heuristic approach and the machine learning ap1 http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized"
W03-1024,C02-1051,0,0.011681,"in other cases of the verb. When a verb has two or more zeros, we resolve ga first, and its best candidate is excluded from the candidates of wo or ni. 2.2 Ranking rules Various heuristics have been reported in past literature. Here, we use the following heuristics. 1. Forward center ranking (Walker et al., 1994): (topic , empathy , subject , object2 , object , others). 2. Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedent’s sentence. If a zero is an object, its antecedent is perhaps an object. 3. Semantic constraints (Yamura-Takei et al., 2002; Yoshino, 2001): If a zero is the subject of ‘eat,’ its antecedent is probably a person or an animal, and so on. We use Nihongo Goi Taikei (Ikehara et al., 1997), which has 14,730 English-to-Japanese translation patterns for 6,103 verbs, to check the acceptability of a candidate. Goi Taikei also has 300,000 words in about 3,000 semantic categories. (See Appendix A for details.) 4. Demotion of candidates in a relative clause (rentai shuushoku setsu): Usually, Japanese zeros do not refer to noun phrases in relative clauses (Ehara and Kim, 1996). (See Appendix B for details.) Since sentences in"
W03-1024,A92-1028,0,\N,Missing
W04-1014,J02-4006,0,0.0287356,"by automatic speech recognition instead of manually transcribed speech (Hori and Furui, 2000a). This summarization approach is word extraction (sentence compaction) that attempts to extract significant information, exclude acoustically and linguistically unreliable words, and maintain the meanings of the original speech. The summarization approaches that have been mainly researched so far are extracting sentences or words from original text or transcribed speech. There has also been research on generating an “abstract” like the much higher level summarization composed freely by human experts (Jing, 2002). This approach includes not only extracting sentences but also combining sentences to generate new sentences, replacing words, reconstructing syntactic structure, and so on. Evaluation Measures for Summarization Metrics that can be used to accurately evaluate the various appropriateness to summarization are needed.The simplest and probably the ideal way of evaluating automatic summarization is to have human subjects read the summaries and evaluate them in terms of the appropriateness of summarization. However, this type of evaluation is too expensive for comparing the efficiencies of many dif"
W04-1014,N03-1020,0,0.0264945,"n sentences or words have meanings, so some concatenations of sentences or words in the automatic summaries sometimes generate meanings different from the original. The evaluation metrics for summarization should thus consider each concatenation between components in the automatic results. To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al., 2002) for machine translation. Evaluation metrics based on word accuracy, summarization accuracy (SumACCY), using a word network made by merging manual summaries has been proposed (Hori and Furui, 2001). In addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (WSumACCY) in which SumACCY is weighted by the majority of the humans’ selections, has been proposed (Hori and Furui, 2003a). In contrast, summarization through sentence extraction has been"
W04-1014,P02-1040,0,0.0908383,"oncatenations of sentences or words in the automatic summaries sometimes generate meanings different from the original. The evaluation metrics for summarization should thus consider each concatenation between components in the automatic results. To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al., 2002) for machine translation. Evaluation metrics based on word accuracy, summarization accuracy (SumACCY), using a word network made by merging manual summaries has been proposed (Hori and Furui, 2001). In addition, to solve the problems for the coverage of correct answers and the reliability of manual summaries as correct answers, weighted summarization accuracy (WSumACCY) in which SumACCY is weighted by the majority of the humans’ selections, has been proposed (Hori and Furui, 2003a). In contrast, summarization through sentence extraction has been evaluated using only single sentence precision."
W10-1762,J93-2003,0,0.011344,"ated studies on reordering. Section 3 describes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007)"
W10-1762,J07-2003,0,0.366382,"et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse trees. Xia and Our"
W10-1762,P05-1066,0,0.134623,"Missing"
W10-1762,P98-1070,0,0.140627,"ur method can be seen as a variant of tree-to-string translation that focuses only on the clause structure in parse trees and independently translates the clauses. Although previous syntax-based methods can theoretically model this kind of derivation, it is practically difﬁcult to decode long multi-clause sentences as described above. Our approach is also related to sentence simpliﬁcation and is intended to obtain simple and short source sentences for better translation. Kim and Ehara (1994) proposed a rule-based method for splitting long Japanese sentences for Japaneseto-English translation; Furuse et al. (1998) used a syntactic structure to split ill-formed inputs in speech translation. Their splitting approach splits a sentence sequentially to obtain short segments, and does not undertake their reordering. Another related ﬁeld is clause identiﬁcation (Tjong et al., 2001). The proposed method is not limited to a speciﬁc clause identiﬁcation method and any method can be employed, if their clause deﬁnition matches the proposed method where clauses are independently translated. 3 Bilingual source Corpus (Training) target parse & clause segmentation Source Sentences (clause-segmented) word alignment Wor"
W10-1762,P02-1040,0,0.0837809,"est sentences are multi-clause sentences. Training Corpus Type Parallel (no-clause-seg.) Parallel (auto-aligned) (oracle-aligned) Dictionary Development Corpus Type Parallel (oracle-aligned) Test Corpus Type Parallel (clause-seg.) E J E J J E J #words 690,536 942,913 135,698 183,043 183,147 263,175 291,455 #words E 34,417 J 46,480 E J #words 34,433 45,975 decoders employed two language models: a word 5-gram language model from the Japanese sentences in the parallel corpus and a word 4-gram language model from the Japanese entries in the dictionary. The feature weights were optimized for BLEU (Papineni et al., 2002) by MERT, using the development sentences. 4.4 Results Table 3 shows the results in BLEU, Translation Edit Rate (TER) (Snover et al., 2006), and Position-independent Word-error Rate (PER) (Och et al., 2001), obtained with Moses and our hierarchical phrase-based SMT, respectively. Bold face results indicate the best scores obtained with the compared methods (excluding oracles). The proposed method consistently outperformed the baseline. The BLEU improvements with the proposed method over the baseline and comparison methods were statistically signiﬁcant according to the bootstrap sampling test ("
W10-1762,N04-1035,0,0.0349554,"many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reorderi"
W10-1762,2006.amta-papers.25,0,0.0712603,"Missing"
W10-1762,N04-1014,0,0.0158515,"typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed s"
W10-1762,N04-4026,0,0.0472133,"ur thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically"
W10-1762,W01-0708,0,0.0411713,"Missing"
W10-1762,D09-1105,0,0.0200396,"nguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between languages with large syntactic differences, the rules are usually unsuitable for other language groups. On the other hand, statistical methods can be applied to any language pai"
W10-1762,J97-3002,0,0.192686,"sed SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-side parse"
W10-1762,N03-1017,0,0.0275575,"ribes the proposed method in detail. Section 4 presents and discusses our experimental results. Finally, we conclude this paper with our thoughts on future studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be ext"
W10-1762,C04-1073,0,0.0479303,"Missing"
W10-1762,2005.iwslt-1.8,0,0.0209809,"uture studies. 2 Related Work Reordering in SMT can be roughly classiﬁed into two approaches, namely a search in SMT decoding and preprocessing. The former approach is a straightforward way that models reordering in noisy channel translation, and has been studied from the early period of SMT research. Distance-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering ove"
W10-1762,N09-1028,0,0.052652,"Computational Linguistics McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem over source words without a linguistically syntactic structure. These preprocessing methods reorder source words close to the target-side order by employing languagedependent rules or statistical reordering models based on automatic word alignment. Although the use of language-dependent rules is a natural and promising way of bridging gaps between l"
W10-1762,P01-1067,0,0.0503465,"ce-based reordering is a typical approach used in many previous studies related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previou"
W10-1762,P07-2045,0,0.00356861,"inal symbol s0 with the second clause and obtain the Japanese sentence: watashi wa tom ga kino susume ta zasshi o kat ta . 4 Experiment We conducted the following experiments on the English-to-Japanese translation of research paper abstracts in the medical domain. Such technical documents are logically and formally written, and sentences are often so long and syntactically complex that their translation needs long distance reordering. We believe that the medical domain is suitable as regards evaluating the proposed method. 4.2 Model and Decoder We used two decoders in the experiments, Moses9 (Koehn et al., 2007) and our inhouse hierarchical phrase-based SMT (almost equivalent to Hiero (Chiang, 2007)). Moses used a phrase table with a maximum phrase length of 7, a lexicalized reordering model with msd-bidirectional-fe, and a distortion limit of 1210 . Our hierarchical phrase-based SMT used a phrase table with a maximum rule length of 7 and a window size (Hiero’s Λ) of 12 11 . Both 4.1 Resources Our bilingual resources were taken from the medical domain. The parallel corpus consisted of research paper abstracts in English taken from PubMed4 and the corresponding Japanese translations. The training port"
W10-1762,zhang-etal-2004-interpreting,0,0.0423294,"Missing"
W10-1762,P07-1091,0,0.327459,"ally follows the Penn Treebank II scheme but also includes SINV, SQ, SBAR. See http://www-tsujii.is.s.u-tokyo.ac.jp/enju/enju-manual/enjuoutput-spec.html#correspondence for details. 418 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 418–427, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics McCord (2004) extracted reordering rules automatically from bilingual corpora for English-toFrench translation; Collins et al. (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al. (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-toEnglish translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al. (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. Tromble and Eisner (2009) proposed another reordering approach based on a linear ordering problem ove"
W10-1762,P06-1077,0,0.0402884,"related to word-based SMT (Brown et al., 1993) and phrase-based SMT (Koehn et al., 2003). Along with the advances in phrase-based SMT, lexicalized reordering with a block orientation model was proposed (Tillmann, 2004; Koehn et al., 2005). This kind of reordering is suitable and commonly used in phrase-based SMT. On the other hand, a syntax-based SMT naturally includes reordering in its translation model. A lot of research work undertaken in this decade has used syntactic parsing for linguistically-motivated translation. (Yamada and Knight, 2001; Graehl and Knight, 2004; Galley et al., 2004; Liu et al., 2006). Wu (1997) and Chiang (2007) focus on formal structures that can be extracted from parallel corpora, instead of a syntactic parser trained using treebanks. These syntactic approaches can theoretically model reordering over an arbitrary length, however, long distance reordering still faces the difﬁculty of searching over an extremely large search space. The preprocessing approach employs deterministic reordering so that the following translation process requires only short distance reordering (or even a monotone). Several previous studies have proposed syntax-driven reordering based on source-"
W10-1762,P06-1004,0,0.0185365,"in our corpora. 421 John lost the book that was borrowed ... clause(1) clause(2) p(that |kara) + p(was |kara ) + ... p(John |john ) + p(lost |john ) + ... john John k|fm ) between each Japanese word fm and English clause k. Theoretically, we can simply output the clause id k ′ for each fm by ﬁnding k ′ = arg maxk t(lm = k|fm ). In practice, this may sometimes lead to Japanese clauses that have too many gaps, so we employ a two-stage procedure to extract clauses that are more contiguous. First, we segment the Japanese sentence into K clauses based on a dynamic programming algorithm proposed by Malioutov and Barzilay (2006). We deﬁne an M × M similarity matrix S = [sij ] with sij = exp(−||li −lj ||) where li is (K + i)-th row vector in the label matrix L. sij represents the similarity between the i-th and j-th Japanese words with respect to their clause alignment score distributions; if the score distributions are similar then sij is large. The details of this algorithm can be found in (Malioutov and Barzilay, 2006). The clause segmentation gives us contiguous Japanese clauses f˜1 , f˜2 , ..., f˜K , thus minimizing inter-segment similarity and maximizing intra-segment similarity. Second, we determine the clause"
W10-1762,J08-1002,0,0.0605287,"Missing"
W10-1762,W01-1408,0,0.038533,"Missing"
W10-1762,C98-1067,0,\N,Missing
W10-1762,J08-3004,0,\N,Missing
W14-3335,D13-1139,0,0.132672,"ears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Japanese will not understand which word order is acceptable. 2.1"
W14-3335,D10-1092,1,0.901921,"t al., 2013). Table 2 shows the result of metaevaluation at NICTR-9/10 PatentMT. The table shows that RIBES is more reliable than BLEU and NIST. Current RIBES has the following improvements. Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × P α • BLEU’s Brevity Penalty (BP) was introduced 287 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287–292, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics jon ga Table 2: Meta-evaluation at NTCIR-9/10 PatentMT (Spearman’s ρ, Goto et al. 2011, 2013) BLEU NIST RIBES NTCIR-9 JE −0.042 −0.114 0.632 NTCIR-9 EJ −0.029 −0.074 0.716 NTCIR-10 JE"
W14-3335,W05-0909,0,0.0892898,"between RIBES and human-judged adequacy. 1 BLEU METEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s τ ) is defined by (τ + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word order. P is precision of unigrams. RIBES is defined for each test sentence and averaged RIBES is used for evaluating the entire test corpus. Table 1 is a table in an IWSLT-2012 invited talk (http://hltc.cs.ust.hk/iwslt/slides/ Isozaki2012 slides.pdf). METEOR was proposed by Banerjee and Lavie (2005). ROUGE-L was proposed by Lin and Och (2004). According to this table, RIBES with α = 0.2 has a very strong correlation (Spearman’s ρ = 0.947) with human-judged adequacy. For each sentence, we use the average of adequacy scores of three judges. Here, we call this average “Adequacy”. We focus on Adequacy because current SMT systems tend to output inadequate sentences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standar"
W14-3335,W02-2016,0,0.0829415,"We use only single reference translations provided by the NTCIR organizers. • postOrder: We generate all permutations of the given reference sentence generated by post-order traversals of its dependency tree. This can be achieved by the following two steps. First, we enumerate all permutations of child nodes at each node. Then, we combine these permutations. This is implemented by cartesian products of the permutation sets. 3 Results We applied the above four methods to the reference sentences of human-judged 100 sentences of NTCIR-7 Patent MT EJ task. (Fujii et al., 2008) We applied CaboCha (Kudo and Matsumoto, 2002) to the reference sentences, and manually corrected the dependency trees because Japanese dependency parsers are not satisfactory in terms of sentence accuracy (Tamura et al., 2007). To support this manual correction, CaboCha’s XML output was automatically converted to dependency tree pictures by using cabochatrees package for LATEX. http:// • caseMarkers: We reorder only “case marker (kaku joshi) phrases”. Here, a “case marker phrase” is post-order traversal of a subtree rooted at a case marker bunsetsu. For instance, the root of the following sentence S3 has a non-case marker child “kaburi ,"
W14-3335,W12-4207,0,0.0203146,"ne sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Japanese will not understand which word order is acceptable. 2.1 Scrambling as Post-"
W14-3335,P04-1077,0,0.056877,"TEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s τ ) is defined by (τ + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word order. P is precision of unigrams. RIBES is defined for each test sentence and averaged RIBES is used for evaluating the entire test corpus. Table 1 is a table in an IWSLT-2012 invited talk (http://hltc.cs.ust.hk/iwslt/slides/ Isozaki2012 slides.pdf). METEOR was proposed by Banerjee and Lavie (2005). ROUGE-L was proposed by Lin and Och (2004). According to this table, RIBES with α = 0.2 has a very strong correlation (Spearman’s ρ = 0.947) with human-judged adequacy. For each sentence, we use the average of adequacy scores of three judges. Here, we call this average “Adequacy”. We focus on Adequacy because current SMT systems tend to output inadequate sentences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standard quality measure in recent NTCIR PatentMT t"
W14-3335,2007.mtsummit-papers.21,0,0.665733,"CIR PatentMT tasks (Goto et al., 2011; Goto et al., 2013). Table 2 shows the result of metaevaluation at NICTR-9/10 PatentMT. The table shows that RIBES is more reliable than BLEU and NIST. Current RIBES has the following improvements. Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × P α • BLEU’s Brevity Penalty (BP) was introduced 287 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287–292, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics jon ga Table 2: Meta-evaluation at NTCIR-9/10 PatentMT (Spearman’s ρ, Goto et al. 2011, 2013) BLEU NIST RIBES NTCIR-9 JE −0.042 −0.114 0.632 NTC"
W14-3335,D12-1077,0,0.0142759,"or disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Japanese will not under"
W14-3335,P02-1040,0,0.100355,"ences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standard quality measure in recent NTCIR PatentMT tasks (Goto et al., 2011; Goto et al., 2013). Table 2 shows the result of metaevaluation at NICTR-9/10 PatentMT. The table shows that RIBES is more reliable than BLEU and NIST. Current RIBES has the following improvements. Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × P α • BLEU’s Brevity Penalty (BP) was introduced 287 Proceedings of the Ninth Workshop on Statistical"
W14-3335,D07-1063,0,0.0249916,"f its dependency tree. This can be achieved by the following two steps. First, we enumerate all permutations of child nodes at each node. Then, we combine these permutations. This is implemented by cartesian products of the permutation sets. 3 Results We applied the above four methods to the reference sentences of human-judged 100 sentences of NTCIR-7 Patent MT EJ task. (Fujii et al., 2008) We applied CaboCha (Kudo and Matsumoto, 2002) to the reference sentences, and manually corrected the dependency trees because Japanese dependency parsers are not satisfactory in terms of sentence accuracy (Tamura et al., 2007). To support this manual correction, CaboCha’s XML output was automatically converted to dependency tree pictures by using cabochatrees package for LATEX. http:// • caseMarkers: We reorder only “case marker (kaku joshi) phrases”. Here, a “case marker phrase” is post-order traversal of a subtree rooted at a case marker bunsetsu. For instance, the root of the following sentence S3 has a non-case marker child “kaburi ,” (wear) between case marker children, “jon ga” and “zubon wo” (Trousers are the object). Figure 3 shows its dependency tree. jon ga shiroi boushi wo kaburi , kuroi zubon wo hai te"
W14-3335,P12-2061,0,0.0237623,"n the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Japanese will not understand which word or"
W14-3335,P12-3022,0,0.0153841,"ed only bigrams for disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. The term “scrambling” stands for these acceptable permutations. These case markers explicitly show grammatical cases and reordering of them does not hurt interpretation of these sentences. Almost all other permutations of words are not acceptable (∗). RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 o-sushi wo tabe-ta in order to penalize too short sentences. α sushi-ya de ∗ jon ga de sushi-ya o-sushi tabe-ta wo . ∗ jon de sushi-ya ga o-sushi wo tabe-ta . ∗ jon tabe-ta ga o-sushi wo sushi-ya de . ∗ sushi-ya ga jon tabe-ta de o-sushi wo . Most readers unfamiliar with Ja"
W16-3616,W08-1301,0,0.159623,"Missing"
W16-3616,egg-redeker-2010-complex,0,0.0153872,"e the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is called the Discourse Graphbank. The graph represents crossed dependency and multiple parentship discourse phenomena, which cannot be represented by tree structures, but whose graph structures become very complex (Egg and Redeker, 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is a large-scale corpus of annotated discourse connectives and their arguments. Its connective-argument structure can also represent complex discourse phenomena like multiple parentship, but its objective is to annotate the discourse relations between individual discourse units, not full discourse structures. Unfortunately, to the best of our knowledge, neither the Discourse Graphbank nor the PDTB has been used for any specific NLP applications. Related Work Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of"
W16-3616,P99-1059,0,0.080509,"he performance with MST’s DDTs closely approached that of the gold DDTs. These results imply that the auto parse trees obtained from Hirao13 have broad and shallow hierarchies because important EDUs, which must be included in a summary, can be easily extracted by TKP. Thus, the DDTs converted by the Hirao13 rule have better tree structures for a single document summarization even though the structures are complex and difficult to parse. This is a significant advantage over Li’s conversion rule. ing since it offers cubic-time dynamic programming algorithms for dependency parsing (Eisner, 1996; Eisner and Satta, 1999; G´omez-Rodrıguez et al., 2008). A higher gap degree means that the dependency trees have more complex nonprojective structures. Both the Hirao13 and MHirao13 methods produce many non-projective dependency edges, but most of the DDTs have at most 1 gap degree and all are well-nested. The well-nested dependency structures of the low gap degree also allow efficient dynamic programming solutions with polynominal time complexity to dependency parsing (G´omez-Rodrıguez et al., 2009). 5.2 Impact on Automatic Parsing Accuracy The conversion methods introduce different complexities in DDTs. This sect"
W16-3616,afantenos-etal-2012-empirical,0,0.0544238,"Missing"
W16-3616,P12-1007,0,0.271197,"corpora for the automatic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003). 128 Proceedings of the SIGDIAL 2016 Conference, pages 128–136, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics are more suitable to text summarization? We show from experimental results that even though the Hirao13 DDT format reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization. While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is called the Discourse Graphbank. The graph repr"
W16-3616,W08-1105,0,0.0168151,"s more complex dependency structures. 1 Introduction Recent years have seen an increase in the use of dependency representations throughout various NLP applications. For the discourse analysis of texts, dependency graph representations have also been studied by many researchers (Prasad et al., 2008; Muller et al., 2012; Hirao et al., 2013; Li et al., 2014). In particular, Hirao et al. (2013) proposed a current state-of-the-art text summarization method based on trimming discourse dependency trees (DDTs). Dependency tree representation is the key to the formulation of the tree trimming method (Filippova and Strube, 2008), and dependency-based discourse syntax has further potential to improve the modeling of a wide range of text-based applications. 1 Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents discourse as a (constituent-style) tree structure. RST was developed as the basis of annotated corpora for the automatic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003). 128 Proceedings of the SIGDIAL 2016 Conference, pages 128–136, c Los Angeles, USA, 13-15 September 2016. 201"
W16-3616,P08-1110,0,0.037315,"Missing"
W16-3616,P98-1044,0,0.664343,"Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents a discourse structure as a constituent tree. The RST Discourse Treebank (RST-DTB) (Carlson et al., 2003) has played a critical role in automatic discourse analysis (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013), mainly because trees are both easy to formalize and computationally tractable. RST discourse trees (RST-DTs) are also used for modeling many text-based applications, such as text summarization (Marcu, 2000) and anaphora resolution (Cristea et al., 1998). Hirao et al. (2013) and Li et al. (2014) introduced dependency conversion methods from RSTDTs into DDTs in which a full discourse structure is represented by head-dependent binary relations between elementary discourse units. Hirao et al. (2013) also showed that a text summarization method, based on trimming DDTs, achieves significant improvements against Marcu (2000)’s method using RST-DTs. On the other hand, some researchers argue that trees are inadequate to account for a full discourse structure (Wolf and Gibson, 2005; Lee et al., 2006; Danlos and others, 2008; Venant et al., 2013). Segm"
W16-3616,E09-1034,0,0.0424113,"Missing"
W16-3616,prasad-etal-2008-penn,0,0.781536,"formats from a dependency graph theoretic point of view? (2) Which formats are analyzed more accurately by automatic parsers? (3) Which are more suitable for text summarization task? Experimental results showed that Hirao’s conversion rule produces DDTs that are more useful for text summarization, even though it derives more complex dependency structures. 1 Introduction Recent years have seen an increase in the use of dependency representations throughout various NLP applications. For the discourse analysis of texts, dependency graph representations have also been studied by many researchers (Prasad et al., 2008; Muller et al., 2012; Hirao et al., 2013; Li et al., 2014). In particular, Hirao et al. (2013) proposed a current state-of-the-art text summarization method based on trimming discourse dependency trees (DDTs). Dependency tree representation is the key to the formulation of the tree trimming method (Filippova and Strube, 2008), and dependency-based discourse syntax has further potential to improve the modeling of a wide range of text-based applications. 1 Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents"
W16-3616,D13-1158,1,0.798445,"Missing"
W16-3616,W09-3813,0,0.576112,"roduced by Li’s method for RST discourse tree in Figure 1: “Elabo.” is short for “Elaboration”. Li et al. (2014)’s dependency conversion method is based on the idea of assigning each discourse unit in an RST-DT a unique head selected among the unit’s children. Traversing each nonterminal node in a bottom-up manner, the headassignment procedure determines the head from its children in the following manner: the head of the leftmost child node with the Nucleus is the head; if no child node is the Nucleus, the head of the leftmost child node is the head. The procedure was originally introduced by Sagae (2009), and its core idea is identical as the head-assignment rules for Penn Treebankstyle constituent trees (Magerman, 1994; Collins, 1999). Li’s conversion method uses the procedure to assign a head to each non-terminal node of a right-branching binarized RST-DT (Hernault et al., 2010) and transforms the head-annotated binary tree into a DDT. Algorithms 1-3 show the dependency conversion method. For brevity, we describe it in a different form from Li’s original conversion process2 cited above. In Algorithm 1, the main routine iteratively processes every EDU in given RST-DT t to directly find its s"
W16-3616,N03-1030,0,0.452687,"ure. RST was developed as the basis of annotated corpora for the automatic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003). 128 Proceedings of the SIGDIAL 2016 Conference, pages 128–136, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics are more suitable to text summarization? We show from experimental results that even though the Hirao13 DDT format reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization. While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is c"
W16-3616,W07-2416,0,0.0368233,"then 8: C←P 9: end if 10: Return C Conversions from RST-DTs to DDTs Next, this paper discusses text-level dependency syntax, which represents grammatical structure by head-dependent binary relations between EDUs. This section introduces two existing automatic conversion methods from RST-DTs to DDTs: the methods of Li et al. (2014) and Hirao et al. (2013). Additionally, this paper presents a simple postediting method to reduce the complexity of DDTs. The heart of these conversions closely resembles that of constituent-to-dependency conversions for English sentences (Yamada and Matsumoto, 2003; Johansson and Nugues, 2007; De Marneffe and Manning, 2008), since RST-DTs can be regarded as Penn Treebank-style constituent trees because EDUs and discourse units respectively correspond to terminal and non-terminal nodes, and a rhetorical relation, like a CFG-rule, forms an edge in the tree. 4.1 Li et al. (2014)’s Method 130 Algorithm 3 find-Head-EDU(P) node in t to which current processed EDU e- j must be assigned as the head in Sagae’s lexicalization manner. Parent(P) and LeftmostNucleusChild(P) are respectively operations that return the parent node of node P and the leftmost child node with the Nucleus of node P3"
W16-3616,P13-1048,0,0.336485,"atic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003). 128 Proceedings of the SIGDIAL 2016 Conference, pages 128–136, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics are more suitable to text summarization? We show from experimental results that even though the Hirao13 DDT format reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization. While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is called the Discourse Graphbank. The graph represents crossed depe"
W16-3616,W13-4002,0,0.0593735,"ion (Cristea et al., 1998). Hirao et al. (2013) and Li et al. (2014) introduced dependency conversion methods from RSTDTs into DDTs in which a full discourse structure is represented by head-dependent binary relations between elementary discourse units. Hirao et al. (2013) also showed that a text summarization method, based on trimming DDTs, achieves significant improvements against Marcu (2000)’s method using RST-DTs. On the other hand, some researchers argue that trees are inadequate to account for a full discourse structure (Wolf and Gibson, 2005; Lee et al., 2006; Danlos and others, 2008; Venant et al., 2013). Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003) represents discourse structures as logical form, and relations function like logical operators on the meaning of their arguments. The annotation in the ANNODIS corpus was conducted based on SDRT (Afantenos et al., 2012). For automatic discourse analysis using the corpus, Muller et al. (2012) adopted dependency tree representation to 3 RST Discourse Tree RST represents a discourse as a tree structure. The leaves of an RST discourse tree (RST-DT) correspond to Elementary Discourse Units (EDUs). Adjacent EDUs are link"
W16-3616,J05-2005,0,0.445795,"reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization. While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications. 2 simplify discourse parsing. They also presented a method to automatically derive DDTs from SDR structures. Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal. The annotated corpus is called the Discourse Graphbank. The graph represents crossed dependency and multiple parentship discourse phenomena, which cannot be represented by tree structures, but whose graph structures become very complex (Egg and Redeker, 2010). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is a large-scale corpus of annotated discourse connectives and their arguments. Its connective-argument structure can also represent complex discou"
W16-3616,P06-2066,0,0.468614,"ly because the authors described only abstracts of their conversion methods. To clarify their algorithmic differences, this paper provides pseudocodes where the two different methods can be described in a unified form, showing that they analyze multinuclear relations differently on RST-DTs. As we show by example in Section 4, such a slight difference can derive significantly different DDTs. The main purpose of this paper is to experimentally reveal the differences between dependency formats. By investigating the complexity of their structures from the dependency graph theoretic point of view (Kuhlmann and Nivre, 2006), we prove that the Hirao13 method, which keeps the semantic equivalence of multinuclear discourse units in the dependency structures, introduces much more complex DDTs than Li14, while a simple post-editing method greatly reduces the complexity of DDTs. This paper also compares the methods with both intrinsic and extrinsic evaluations: (1) Which dependency structures are analyzed more accurately by automatic parsers? and (2) Which structures Two heuristic rules that transform Rhetorical Structure Theory discourse trees into discourse dependency trees (DDTs) have recently been proposed (Hirao"
W16-3616,W03-3023,0,0.0829739,"hile 7: if isRoot(P) = TRUE then 8: C←P 9: end if 10: Return C Conversions from RST-DTs to DDTs Next, this paper discusses text-level dependency syntax, which represents grammatical structure by head-dependent binary relations between EDUs. This section introduces two existing automatic conversion methods from RST-DTs to DDTs: the methods of Li et al. (2014) and Hirao et al. (2013). Additionally, this paper presents a simple postediting method to reduce the complexity of DDTs. The heart of these conversions closely resembles that of constituent-to-dependency conversions for English sentences (Yamada and Matsumoto, 2003; Johansson and Nugues, 2007; De Marneffe and Manning, 2008), since RST-DTs can be regarded as Penn Treebank-style constituent trees because EDUs and discourse units respectively correspond to terminal and non-terminal nodes, and a rhetorical relation, like a CFG-rule, forms an edge in the tree. 4.1 Li et al. (2014)’s Method 130 Algorithm 3 find-Head-EDU(P) node in t to which current processed EDU e- j must be assigned as the head in Sagae’s lexicalization manner. Parent(P) and LeftmostNucleusChild(P) are respectively operations that return the parent node of node P and the leftmost child node"
W16-3616,D14-1196,1,0.788655,"ee and all are well-nested. The well-nested dependency structures of the low gap degree also allow efficient dynamic programming solutions with polynominal time complexity to dependency parsing (G´omez-Rodrıguez et al., 2009). 5.2 Impact on Automatic Parsing Accuracy The conversion methods introduce different complexities in DDTs. This section investigates which formats are more accurately analyzed by automatic discourse parsers. For evaluation, we implemented a maximum spanning tree algorithm for discourse dependency parsing, which was recently proposed (Muller et al., 2012; Li et al., 2014; Yoshida et al., 2014). To compare discourse dependency parsing with standard RST parsing, we also implemented the HILDA RST parser (Hernault et al., 2010), which achieved 82.6/66.6/54.2 points for a standard set of RST-style evaluation measures, i.e., Span, Nuclearity and Relation (Marcu, 2000). We used a standard split of DDTs automatically converted from RST-DTB: 347 DDTs as the training set and 38 as the test set. Table 3 shows the evaluation results of dependency parsing. The lower the complexity of the DDT format, the higher is the dependency unlabeled attachment score. Post-editing the Hirao13 DDTs improves"
W16-3616,P14-1003,0,0.567173,"t the Hirao13 method, which keeps the semantic equivalence of multinuclear discourse units in the dependency structures, introduces much more complex DDTs than Li14, while a simple post-editing method greatly reduces the complexity of DDTs. This paper also compares the methods with both intrinsic and extrinsic evaluations: (1) Which dependency structures are analyzed more accurately by automatic parsers? and (2) Which structures Two heuristic rules that transform Rhetorical Structure Theory discourse trees into discourse dependency trees (DDTs) have recently been proposed (Hirao et al., 2013; Li et al., 2014), but these rules derive significantly different DDTs because their conversion schemes on multinuclear relations are not identical. This paper reveals the difference among DDT formats with respect to the following questions: (1) How complex are the formats from a dependency graph theoretic point of view? (2) Which formats are analyzed more accurately by automatic parsers? (3) Which are more suitable for text summarization task? Experimental results showed that Hirao’s conversion rule produces DDTs that are more useful for text summarization, even though it derives more complex dependency struc"
W16-3616,C12-1115,0,0.402053,"ency graph theoretic point of view? (2) Which formats are analyzed more accurately by automatic parsers? (3) Which are more suitable for text summarization task? Experimental results showed that Hirao’s conversion rule produces DDTs that are more useful for text summarization, even though it derives more complex dependency structures. 1 Introduction Recent years have seen an increase in the use of dependency representations throughout various NLP applications. For the discourse analysis of texts, dependency graph representations have also been studied by many researchers (Prasad et al., 2008; Muller et al., 2012; Hirao et al., 2013; Li et al., 2014). In particular, Hirao et al. (2013) proposed a current state-of-the-art text summarization method based on trimming discourse dependency trees (DDTs). Dependency tree representation is the key to the formulation of the tree trimming method (Filippova and Strube, 2008), and dependency-based discourse syntax has further potential to improve the modeling of a wide range of text-based applications. 1 Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents discourse as a (const"
W16-3616,W01-1605,0,\N,Missing
W16-3616,C96-1058,0,\N,Missing
W16-3616,C98-1044,0,\N,Missing
W19-5365,P18-1007,0,0.0158784,"the Japanese side and not translated into English. We fixed these errors by the script provided by Yamamoto and Takahashi (2016)1 . We use different preprocessing steps for each translation direction. This is because we need to submit tokenized output for En-Ja translation, thus it seems to be better to tokenize the Japanese side in the same way as the submission in the preprocessing steps, whereas we use a relatively simple method for Ja-En direction. For Ja-En, we tokenized the raw text into subwords by simply applying sentencepiece with the vocabulary size of 32,000 for each language side (Kudo, 2018; Kudo and Richardson, 2018). For En-Ja, we tokenized the text by KyTea (Neubig et al., 2011) and the Moses tokenizer (Koehn et al., 2007) for Japanese and English, respectively. We also truecased the English words by the script provided with Moses toolkits2 . Then we further tokenized the words into subwords using joint Byte-Pair-Encoding (BPE) with 16,000 merge operations3 (Sennrich et al., 2016b). provided monolingual corpus, and noisy data filtering for its data. (Section 2.3). • Placeholder mechanism to handle tokens that should be copied from a source-side sentence (Section 2.4). NMT Mod"
W19-5365,D18-2012,0,0.0176228,"side and not translated into English. We fixed these errors by the script provided by Yamamoto and Takahashi (2016)1 . We use different preprocessing steps for each translation direction. This is because we need to submit tokenized output for En-Ja translation, thus it seems to be better to tokenize the Japanese side in the same way as the submission in the preprocessing steps, whereas we use a relatively simple method for Ja-En direction. For Ja-En, we tokenized the raw text into subwords by simply applying sentencepiece with the vocabulary size of 32,000 for each language side (Kudo, 2018; Kudo and Richardson, 2018). For En-Ja, we tokenized the text by KyTea (Neubig et al., 2011) and the Moses tokenizer (Koehn et al., 2007) for Japanese and English, respectively. We also truecased the English words by the script provided with Moses toolkits2 . Then we further tokenized the words into subwords using joint Byte-Pair-Encoding (BPE) with 16,000 merge operations3 (Sennrich et al., 2016b). provided monolingual corpus, and noisy data filtering for its data. (Section 2.3). • Placeholder mechanism to handle tokens that should be copied from a source-side sentence (Section 2.4). NMT Model Neural Machine Translatio"
W19-5365,2012.eamt-1.60,0,0.0291062,"of sentences and words contained in the provided monolingual corpus. As NMT can be trained with only parallel data, utilizing a monolingual corpus for NMT is a key Data Preprocessing For an in-domain corpus, the organizers provided the MTNT (Machine Translation of Noisy Text) parallel corpus (Michel and Neubig, 2018), which is a collection of Reddit discussions and their manual translations. They also provided relatively large out-of-domain parallel corpora, namely KFTT (Kyoto Free Translation Task) (Neubig, 2011), JESC (Japanese-English Subtitle Corpus) (Pryzant et al., 2017), and TED talks (Cettolo et al., 2012). Table 2 shows the number of sentences and words on the English side contained in the provided parallel corpora. 1 https://github.com/kanjirz50/mt_ ialp2016/blob/master/script/ja_prepro.pl 2 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ recaser/truecase.perl 3 Normally, Japanese and English do not share any words, thus using joint BPE does not seem effective. However, for this dataset, we found that Japanese sentences often include English words (e.g., named entities), so we use joint BPE even for this language pair. 545 TRG monolingual TRG → SRC Model (2) data cleaning & tr"
W19-5365,W19-5303,0,0.0852541,"created a synthetic corpus from a target-side monolingual corpus with a target-to-source translation model. Lastly, we fine-tuned our translation model with the synthetic and in-domain parallel corpora for domain adaptation. The paper is organized as follows. In Section 2, we present a detailed overview of our systems. Section 3 shows experimental settings and main results, and Section 4 provides an analysis of our systems. Finally, Section 5 draws a brief conclusion of our work for the WMT19 robustness task. Introduction This paper describes NTT’s submission to the WMT 2019 robustness task (Li et al., 2019). This year, we participated in English-to-Japanese (EnJa) and Japanese-to-English (Ja-En) translation tasks with a constrained setting, i.e., we used only the parallel and monolingual corpora provided by the organizers. The task focuses on the robustness of Machine Translation (MT) to noisy text that can be found on social media (e.g., Reddit, Twitter). The task is more challenging than a typical machine translation task like the news translation tasks (Bojar et al., 2018) due to the characteristics of noisy text and the lack of a publicly available parallel corpus (Michel and Neubig, 2018)."
W19-5365,D15-1166,0,0.201069,"Missing"
W19-5365,D18-1050,0,0.150521,"ess task (Li et al., 2019). This year, we participated in English-to-Japanese (EnJa) and Japanese-to-English (Ja-En) translation tasks with a constrained setting, i.e., we used only the parallel and monolingual corpora provided by the organizers. The task focuses on the robustness of Machine Translation (MT) to noisy text that can be found on social media (e.g., Reddit, Twitter). The task is more challenging than a typical machine translation task like the news translation tasks (Bojar et al., 2018) due to the characteristics of noisy text and the lack of a publicly available parallel corpus (Michel and Neubig, 2018). Table 1 shows example comments from Reddit, a discussion website. Text on social media usually contains various noise such as (1) abbreviations, (2) grammatical errors, (3) misspellings, (4) emojis, and (5) emoticons. In addition, most provided parallel corpora are not related to our target domain, ⇤ 2 System Details In this section, we describe the overview and features of our systems: • Data preprocessing techniques for the provided parallel corpora (Section 2.2). • Synthetic corpus, back-translated from the Equal contribution. 544 Proceedings of the Fourth Conference on Machine Translatio"
W19-5365,W12-3131,0,0.0267844,"n using the spaCy4 toolkit. After that, we discarded the sentences that are either longer than 80 tokens or equal to 1 token. Since a synthetic corpus might contain noisy sentence pairs, previous work shows that an additional filtering technique helps to improve accuracy (Morishita et al., 2018). We also apply a filtering technique to the synthetic corpus as illustrated in (3) in Figure 1. For this task, we use the qe-clean5 toolkit, which filtered out the noisy sentences on the basis of a word alignment and language models by estimating how correctly translated and natural the sentences are (Denkowski et al., 2012). We train the word alignment and language models by using KFTT, TED, and MTNT corpora6 . We use fast_align for word alignment and KenLM for language modeling (Dyer et al., 2013; Heafield, 2011). 2.4 Placeholder Noisy text on social media often contains tokens that do not require translation such as emojis, “ , , ”, and emoticons, “m(_ _)m, ( ` · ! · ´ ), 4 https://spacy.io https://github.com/cmu-mtlab/qe-clean 6 Note that the JESC corpus is relatively noisy, thus we decided not to use it for cleaning. 5 546 (ˆoˆ)/”. However, to preserve the meaning of the input sentence that contains emojis"
W19-5365,N13-1073,0,0.0374731,"rs, previous work shows that an additional filtering technique helps to improve accuracy (Morishita et al., 2018). We also apply a filtering technique to the synthetic corpus as illustrated in (3) in Figure 1. For this task, we use the qe-clean5 toolkit, which filtered out the noisy sentences on the basis of a word alignment and language models by estimating how correctly translated and natural the sentences are (Denkowski et al., 2012). We train the word alignment and language models by using KFTT, TED, and MTNT corpora6 . We use fast_align for word alignment and KenLM for language modeling (Dyer et al., 2013; Heafield, 2011). 2.4 Placeholder Noisy text on social media often contains tokens that do not require translation such as emojis, “ , , ”, and emoticons, “m(_ _)m, ( ` · ! · ´ ), 4 https://spacy.io https://github.com/cmu-mtlab/qe-clean 6 Note that the JESC corpus is relatively noisy, thus we decided not to use it for cleaning. 5 546 (ˆoˆ)/”. However, to preserve the meaning of the input sentence that contains emojis or emoticons, such tokens need to be output to the target language side. Therefore, we simply copy the emojis and emoticons from a source language to a target language with a pl"
W19-5365,W18-6421,1,0.776039,"for the sentence i. Finally, if i is higher than a specific threshold, we assume that the sentence i contains an ASCII art and discard it from the monolingual data. We set the threshold to 6.0. Moreover, since the provided monolingual data includes lines with more than one sentence, we first performed the sentence tokenization using the spaCy4 toolkit. After that, we discarded the sentences that are either longer than 80 tokens or equal to 1 token. Since a synthetic corpus might contain noisy sentence pairs, previous work shows that an additional filtering technique helps to improve accuracy (Morishita et al., 2018). We also apply a filtering technique to the synthetic corpus as illustrated in (3) in Figure 1. For this task, we use the qe-clean5 toolkit, which filtered out the noisy sentences on the basis of a word alignment and language models by estimating how correctly translated and natural the sentences are (Denkowski et al., 2012). We train the word alignment and language models by using KFTT, TED, and MTNT corpora6 . We use fast_align for word alignment and KenLM for language modeling (Dyer et al., 2013; Heafield, 2011). 2.4 Placeholder Noisy text on social media often contains tokens that do not"
W19-5365,W11-2123,0,0.0125137,"hows that an additional filtering technique helps to improve accuracy (Morishita et al., 2018). We also apply a filtering technique to the synthetic corpus as illustrated in (3) in Figure 1. For this task, we use the qe-clean5 toolkit, which filtered out the noisy sentences on the basis of a word alignment and language models by estimating how correctly translated and natural the sentences are (Denkowski et al., 2012). We train the word alignment and language models by using KFTT, TED, and MTNT corpora6 . We use fast_align for word alignment and KenLM for language modeling (Dyer et al., 2013; Heafield, 2011). 2.4 Placeholder Noisy text on social media often contains tokens that do not require translation such as emojis, “ , , ”, and emoticons, “m(_ _)m, ( ` · ! · ´ ), 4 https://spacy.io https://github.com/cmu-mtlab/qe-clean 6 Note that the JESC corpus is relatively noisy, thus we decided not to use it for cleaning. 5 546 (ˆoˆ)/”. However, to preserve the meaning of the input sentence that contains emojis or emoticons, such tokens need to be output to the target language side. Therefore, we simply copy the emojis and emoticons from a source language to a target language with a placeholder mechani"
W19-5365,P11-2093,0,0.0184736,"ipt provided by Yamamoto and Takahashi (2016)1 . We use different preprocessing steps for each translation direction. This is because we need to submit tokenized output for En-Ja translation, thus it seems to be better to tokenize the Japanese side in the same way as the submission in the preprocessing steps, whereas we use a relatively simple method for Ja-En direction. For Ja-En, we tokenized the raw text into subwords by simply applying sentencepiece with the vocabulary size of 32,000 for each language side (Kudo, 2018; Kudo and Richardson, 2018). For En-Ja, we tokenized the text by KyTea (Neubig et al., 2011) and the Moses tokenizer (Koehn et al., 2007) for Japanese and English, respectively. We also truecased the English words by the script provided with Moses toolkits2 . Then we further tokenized the words into subwords using joint Byte-Pair-Encoding (BPE) with 16,000 merge operations3 (Sennrich et al., 2016b). provided monolingual corpus, and noisy data filtering for its data. (Section 2.3). • Placeholder mechanism to handle tokens that should be copied from a source-side sentence (Section 2.4). NMT Model Neural Machine Translation (NMT) has been making remarkable progress in the field of MT (B"
W19-5365,N19-4009,0,0.0192537,"that makes use of GPUs more efficiently for faster training (Micikevicius et al., 2018). Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update (Ott et al., 2018). We trained the model for 20,000 iterations, saved the model parameters each 200 iterations, and took an average of the last eight models9 . Training took about 1.5 days to converge with four NVIDIA V100 GPUs. We compute case-sensitive BLEU scores (Papineni et al., 2002) for evaluating translation quality10 . All our implementations are based on the fairseq11 toolkit (Ott et al., 2019). After training the model with the whole provided parallel corpora, we fine-tuned it with indomain data. During fine-tuning, we used almost the same settings as the initial training setup except we changed the model save interval to every three iterations and continued the learning rate decay schedule. For fine-tuning, we trained the model for 50 iterations, which took less than 10 minutes with four GPUs. When decoding, we used a beam search with the size of six and a length normalization technique with ↵ = 2.0 and = 0.0 (Wu et al., 2016). For the submission, we used an ensemble of three (En-"
W19-5365,W18-6301,0,0.0618061,"layer by three-way-weight-tying (Press and Wolf, 2017). Each layer is connected with a dropout probability of 0.3 (Srivastava et al., 2014). For an optimizer, we used Adam (Kingma and Ba, 2015) with a learning rate of 0.001, 1 = 0.9, 2 = 0.98. We use a root-square decay learning rate schedule with a linear warmup of 4000 steps (Vaswani et al., 2017). We applied mixed precision training that makes use of GPUs more efficiently for faster training (Micikevicius et al., 2018). Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update (Ott et al., 2018). We trained the model for 20,000 iterations, saved the model parameters each 200 iterations, and took an average of the last eight models9 . Training took about 1.5 days to converge with four NVIDIA V100 GPUs. We compute case-sensitive BLEU scores (Papineni et al., 2002) for evaluating translation quality10 . All our implementations are based on the fairseq11 toolkit (Ott et al., 2019). After training the model with the whole provided parallel corpora, we fine-tuned it with indomain data. During fine-tuning, we used almost the same settings as the initial training setup except we changed the"
W19-5365,P02-1040,0,0.107578,"cay learning rate schedule with a linear warmup of 4000 steps (Vaswani et al., 2017). We applied mixed precision training that makes use of GPUs more efficiently for faster training (Micikevicius et al., 2018). Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update (Ott et al., 2018). We trained the model for 20,000 iterations, saved the model parameters each 200 iterations, and took an average of the last eight models9 . Training took about 1.5 days to converge with four NVIDIA V100 GPUs. We compute case-sensitive BLEU scores (Papineni et al., 2002) for evaluating translation quality10 . All our implementations are based on the fairseq11 toolkit (Ott et al., 2019). After training the model with the whole provided parallel corpora, we fine-tuned it with indomain data. During fine-tuning, we used almost the same settings as the initial training setup except we changed the model save interval to every three iterations and continued the learning rate decay schedule. For fine-tuning, we trained the model for 50 iterations, which took less than 10 minutes with four GPUs. When decoding, we used a beam search with the size of six and a length no"
W19-5365,E17-2025,0,0.0142778,"etected on the basis of Unicode Emoji Charts7 . We detect emoticons included in both the source- and the target-side sentences with the nagisa8 toolkit, which is a Japanese morphological analyzer that can also be used as an emoticon detector for Japanese and English text. Moreover, we also replace “&gt;” tokens at the beginning of the sentence with the placeholders because “&gt;” is commonly used as a quotation mark in social media posts and emails and does not require translation. 2.5 the parameter of the encoder/decoder word embedding layers and the decoder output layer by three-way-weight-tying (Press and Wolf, 2017). Each layer is connected with a dropout probability of 0.3 (Srivastava et al., 2014). For an optimizer, we used Adam (Kingma and Ba, 2015) with a learning rate of 0.001, 1 = 0.9, 2 = 0.98. We use a root-square decay learning rate schedule with a linear warmup of 4000 steps (Vaswani et al., 2017). We applied mixed precision training that makes use of GPUs more efficiently for faster training (Micikevicius et al., 2018). Each minibatch contains about 8000 tokens (subwords), and we accumulated the gradients of 128 mini-batches for an update (Ott et al., 2018). We trained the model for 20,000 ite"
W19-5365,P16-1009,0,0.14271,"eps, whereas we use a relatively simple method for Ja-En direction. For Ja-En, we tokenized the raw text into subwords by simply applying sentencepiece with the vocabulary size of 32,000 for each language side (Kudo, 2018; Kudo and Richardson, 2018). For En-Ja, we tokenized the text by KyTea (Neubig et al., 2011) and the Moses tokenizer (Koehn et al., 2007) for Japanese and English, respectively. We also truecased the English words by the script provided with Moses toolkits2 . Then we further tokenized the words into subwords using joint Byte-Pair-Encoding (BPE) with 16,000 merge operations3 (Sennrich et al., 2016b). provided monolingual corpus, and noisy data filtering for its data. (Section 2.3). • Placeholder mechanism to handle tokens that should be copied from a source-side sentence (Section 2.4). NMT Model Neural Machine Translation (NMT) has been making remarkable progress in the field of MT (Bahdanau et al., 2015; Luong et al., 2015). However, most existing MT systems still struggle with noisy text and easily make mistranslations (Belinkov and Bisk, 2018), though the Transformer has achieved the state-of-the-art performance in several MT tasks (Vaswani et al., 2017). In our submission system, w"
W19-5365,P16-1162,0,0.229085,"Missing"
W19-5365,W17-4811,0,0.0250148,"anslation quality. In future work, we will explore ways to use monolingual data more effectively, introduce contextual information, and deal with a variety of noisy tokens such as abbreviations, ASCII-arts, and grammar errors. Use of Contextual Information Some sentences need contextual information for them to be precisely translated. The MTNT corpus provides comment IDs as the contextual information to group sentences from the same original comment. We did not use the contextual information in our systems, but we consider that it would help to improve translation quality as in previous work (Tiedemann and Scherrer, 2017; Bawden et al., 2018). For example, in the following two sentences, “Airborne school isn’t a hard school.” and “Get in there with some confidence!”, which can be found in the MTNT corpus and have the same comment ID, we consider that leveraging their contextual information would help to clarify what “there” means in the latter and to translate it more accurately. 5 Acknowledgments We thank two anonymous reviewers for their careful reading and insightful comments and suggestions. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning"
W19-5365,W18-6401,0,\N,Missing
W19-5365,W18-6408,0,\N,Missing
W19-5365,W18-6429,0,\N,Missing
W19-5365,N18-1118,0,\N,Missing
