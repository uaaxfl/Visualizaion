2011.eamt-1.42,N10-1028,0,0.321955,"d anywhere in the training corpus. Naturally, the number of rules explodes when we need to consider all the possible phrasal translations that could have occurred somewhere in the training corpus as individual rules in our grammar. Our solution to this problem is to start with a token based grammar, and then extract larger biterminals that occur in the parse forest of the simpler grammar. This allows us to avoid enumerating phrasal biterminals that would likely be weeded out by EM anyway, and thus saves considerable amounts of space (Section 5). Another approach to this problem is reported in Blunsom and Cohn (2010), involving sampling. Up until the point that a rule is actually sampled, it can be implicitly represented with the probability distribution it is to be drawn from. 3 Linear Transduction Grammars Linear transduction grammars constitute the natural bilingualization of linear grammars (LGs). This class of grammar has received relatively little attention, mostly because there is no obvious use for it. LGs lie between finite-state grammars and context-free grammars in computational complexity, but all you get is palindromes. Another way to look at LGs is that they relate the beginning of a string"
2011.eamt-1.42,J93-2003,0,0.0407826,"Missing"
2011.eamt-1.42,P09-1104,0,0.176076,"from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for biparsing in O(n4 ) time. By approximating the search, linear time complexity is attainable. Saers (2011) later introduced the more general class of linear transduction grammars (LTG), which are equivalent to LITG s in terms of generative capacity. Searching for linear transductions rather than inversion transductions or full syntax-directed transductions makes the grammar induction tractable. It does, however, als"
2011.eamt-1.42,2005.iwslt-1.8,0,0.0280376,"Missing"
2011.eamt-1.42,P07-2045,0,0.00420266,"g data to sentence pairs where none of the two sentences were longer than 20 tokens (see Table 1). Our train–tune–test pipeline includes: 1. Preprocessing tools: tokenizer, corpus cleaner and case folder (supplied by the organizers). 2. Language model: 5-gram model using SRILM (Stolcke, 2002). 3. Translation model: Phrase-based model extracted and scored with with the Moses 318 Corpus French–English (train) French–English (tune) French–English (test) English (LM) Size 381,780 sent. pairs 2,000 sent. pairs 2,000 sent. pairs 1,412,546 sentences Table 1: Corpora used in the experiments. toolkit (Koehn et al., 2007) using the grow-diag-final-and heuristic (Koehn et al., 2005) on bidirectional word alignments obtained through IBM-models (Brown et al., 1993) and HMM-alignment (Vogel et al., 1996) using GIZA ++ (Och and Ney, 2000). 4. Tuning: Minimum error-rate training (Och and Ney, 2002). 5. Decoder: Moses (Koehn et al., 2007). 6. Postprocessing: recaser (trained with the Moses toolkit) and detokenization (supplied by the organizers). 7. Evaluation: NIST (Doddington, 2002) and BLEU (Papineni et al., 2002). This constitutes the baseline. Our system replaces the translation model with a phrasal bilexicon fr"
2011.eamt-1.42,P00-1056,0,0.212615,"Missing"
2011.eamt-1.42,P02-1038,0,0.0719414,"(Stolcke, 2002). 3. Translation model: Phrase-based model extracted and scored with with the Moses 318 Corpus French–English (train) French–English (tune) French–English (test) English (LM) Size 381,780 sent. pairs 2,000 sent. pairs 2,000 sent. pairs 1,412,546 sentences Table 1: Corpora used in the experiments. toolkit (Koehn et al., 2007) using the grow-diag-final-and heuristic (Koehn et al., 2005) on bidirectional word alignments obtained through IBM-models (Brown et al., 1993) and HMM-alignment (Vogel et al., 1996) using GIZA ++ (Och and Ney, 2000). 4. Tuning: Minimum error-rate training (Och and Ney, 2002). 5. Decoder: Moses (Koehn et al., 2007). 6. Postprocessing: recaser (trained with the Moses toolkit) and detokenization (supplied by the organizers). 7. Evaluation: NIST (Doddington, 2002) and BLEU (Papineni et al., 2002). This constitutes the baseline. Our system replaces the translation model with a phrasal bilexicon from PLITG induction. To isolate the effect of the bilexicon (which is the main focus of this paper), we refrained from using the more advanced reordering model that the Moses toolkit can build from alignments. This is a concept that is completely orthogonal to the bilexicon, a"
2011.eamt-1.42,P02-1040,0,0.0835989,"000 sent. pairs 2,000 sent. pairs 1,412,546 sentences Table 1: Corpora used in the experiments. toolkit (Koehn et al., 2007) using the grow-diag-final-and heuristic (Koehn et al., 2005) on bidirectional word alignments obtained through IBM-models (Brown et al., 1993) and HMM-alignment (Vogel et al., 1996) using GIZA ++ (Och and Ney, 2000). 4. Tuning: Minimum error-rate training (Och and Ney, 2002). 5. Decoder: Moses (Koehn et al., 2007). 6. Postprocessing: recaser (trained with the Moses toolkit) and detokenization (supplied by the organizers). 7. Evaluation: NIST (Doddington, 2002) and BLEU (Papineni et al., 2002). This constitutes the baseline. Our system replaces the translation model with a phrasal bilexicon from PLITG induction. To isolate the effect of the bilexicon (which is the main focus of this paper), we refrained from using the more advanced reordering model that the Moses toolkit can build from alignments. This is a concept that is completely orthogonal to the bilexicon, and rather than simulating it when converting the PLITG to a bilexicon, we chose to leave it out. The PLITG bilexicon was induced by combining existing biterminals three times, with five iterations of expectation maximizati"
2011.eamt-1.42,W09-3804,1,0.816584,"e, which makes induction from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for biparsing in O(n4 ) time. By approximating the search, linear time complexity is attainable. Saers (2011) later introduced the more general class of linear transduction grammars (LTG), which are equivalent to LITG s in terms of generative capacity. Searching for linear transductions rather than inversion transductions or full syntax-directed transductions makes the grammar induction tract"
2011.eamt-1.42,W10-3802,1,0.653028,"rminals directly from the parallel corpus using expectationmaximization (EM). When it has stabilized, we collect all biterminal pairs that could form larger biterminals and incorporate them into the grammar to produce a multi-token transduction grammar. The process is then repeated until large enough units are learned. Since maximum likelihood learning methods such as EM tends to favor longer chunks over shorter, we introduce a length penalty for multi-token terminals. Since induction of transduction grammars is very time consuming, we opt to view the parallel corpus as a linear transduction (Saers et al., 2010b; Saers, 2011). This assumption allows us to use something that is equivalent to linear transduction grammars (LTGs), which can approximate the search for a parse forest given a sentence pair in linear time. LTGs do not, however, have an explicit biterminal concept, making it non-trivial to map the grammar to a probabilistic bilexicon. To fix this, we introduce preterminalized linear inversion transduction grammars (PLITGs), which will allow the desired parameterization. Learning a stochastic bracketing PLITG from a parallel corpus is equivalent to building a probabilistic bilexicon based on"
2011.eamt-1.42,N10-1050,1,0.875306,"rminals directly from the parallel corpus using expectationmaximization (EM). When it has stabilized, we collect all biterminal pairs that could form larger biterminals and incorporate them into the grammar to produce a multi-token transduction grammar. The process is then repeated until large enough units are learned. Since maximum likelihood learning methods such as EM tends to favor longer chunks over shorter, we introduce a length penalty for multi-token terminals. Since induction of transduction grammars is very time consuming, we opt to view the parallel corpus as a linear transduction (Saers et al., 2010b; Saers, 2011). This assumption allows us to use something that is equivalent to linear transduction grammars (LTGs), which can approximate the search for a parse forest given a sentence pair in linear time. LTGs do not, however, have an explicit biterminal concept, making it non-trivial to map the grammar to a probabilistic bilexicon. To fix this, we introduce preterminalized linear inversion transduction grammars (PLITGs), which will allow the desired parameterization. Learning a stochastic bracketing PLITG from a parallel corpus is equivalent to building a probabilistic bilexicon based on"
2011.eamt-1.42,C96-2141,0,0.465776,"Missing"
2011.eamt-1.42,P95-1033,1,0.579108,"represent an NP-complete search problem that needs to be heavily pruned to be practically useful. The problem with SDTGs is that they are very time consuming to learn from parallel corpora. Whereas all context-free grammars can be reduced to an equivalent grammar in a two-normal form, SDTGs cannot. With an arbitrary-rank SDTG, parsing a sentence pair requires O(n2n+2 ) time, which makes induction from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for bipa"
2011.eamt-1.42,W95-0106,1,0.735405,"represent an NP-complete search problem that needs to be heavily pruned to be practically useful. The problem with SDTGs is that they are very time consuming to learn from parallel corpora. Whereas all context-free grammars can be reduced to an equivalent grammar in a two-normal form, SDTGs cannot. With an arbitrary-rank SDTG, parsing a sentence pair requires O(n2n+2 ) time, which makes induction from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for bipa"
2011.eamt-1.42,P96-1021,1,0.689329,"Missing"
2011.eamt-1.42,J97-3002,1,0.682699,"languages to each other, with the caveat that two of them are also related to each other. Definition 1. An LTG over languages L1 and L2 is a tuple G = hN, Σ, ∆, S, Ri where N is a finite, nonempty set of nonterminal symbols, Σ is a finite, nonempty set of L1 symbols, ∆ is a finite, nonempty set of L2 symbols, S ∈ N is the designated start symbol, and R is a finite, nonempty set of linear transduction rules on the forms: A → a/x B b/y, A → a/x where A, B ∈ N , a, b ∈ Σ∗ and x, y ∈ ∆∗ . Linear inversion transduction grammars (LITGs), on the other hand, are inversion transduction grammars, ITGs (Wu, 1997) that have been subjected to a linearity constraint. An ITG is a transduction grammar restricted to have only context-free rules, and only monotonic permutations (specifically identity or inversion permutation). By subjecting an ITG in normal form to a linearity constraint, we replace one of the nonterminal symbols with biterminal symbols, thereby significantly reducing the expressive power of the grammar, for a significant efficiency boost. LITGs were introduced in Saers et al. (2010b), and subsequently compared to full ITGs in Saers et al. (2010a). The rules in an LITG take the following for"
2011.eamt-1.42,P08-1012,0,0.299944,"equires O(n2n+2 ) time, which makes induction from parallel corpora intractable. One way to make induction tractable is to restrict the search to a subset of the syntaxdirected transductions. This approach was pioneered in a series of papers by Wu (1995a; 1995b; 1996; 1997), where inversion transductions and the corresponding inversion transduction grammars (ITGs) were introduced. This restricted set of grammars can parse a sentence pair in O(n6 ) time, making it tractable but not practical to induce grammars from data. Several attempts have been made to approximate biparsing, see for example Zhang et al. (2008), Saers et al. (2009) and Haghighi et al. (2009). An even more aggressive restriction to SDTGs was proposed in Saers et al. (2010b). Where the original ITGs allow branching, the introduced linear ITGs (LITGs) do not, allowing for biparsing in O(n4 ) time. By approximating the search, linear time complexity is attainable. Saers (2011) later introduced the more general class of linear transduction grammars (LTG), which are equivalent to LITG s in terms of generative capacity. Searching for linear transductions rather than inversion transductions or full syntax-directed transductions makes the gr"
2011.mtsummit-papers.49,J93-2003,0,0.061902,"Missing"
2011.mtsummit-papers.49,J09-4009,0,0.183874,"lignments that can be generated are perfectly straight diagonals. Given a grammar that can generate inﬁnitely long sentence pairs, the set of these token alignments (one for every possible sentence length) is also inﬁnitely large, which means that we cannot compare the absolute sizes of the sets. Instead we will observe how the number of token alignment grows as a function of their length, and for easy comparison to following grammars, we will express it as a recurrence formula: aF1 = 1, aFn = aFn−1 + 1 Moving on to inversion transduction grammars (ITGs), we know from previous work (Wu, 1997; Huang et al., 2009) that the number of token alignments up to and including length n is equal to the nth large Schr¨oder numbers (Schr¨oder, 1870), which can be expressed as: aI1 = 1, aI2 = 2, aIn = 6n−9 n aIn−1 − n−3 n aIn−2 Finally, we have the arbitrary rank syntax-directed transduction grammars (SDTGs), which are capable of generating any permutation (Lewis and Stearns, 433 1968; Aho and Ullman, 1972). The number of permutations are n!, which we can also formulate as a recurrence formula: aT1 = 1, aTn = naTn−1 It should be clear that these series grow at different paces, and that: aFn &lt; aIn &lt; aTn 4 Weak alig"
2011.mtsummit-papers.49,N03-1017,0,0.0699369,"Missing"
2011.mtsummit-papers.49,W02-1018,0,0.0540128,"Missing"
2011.mtsummit-papers.49,2011.eamt-1.42,1,0.872194,"ments that are allowed under a transduction as its weak alignment capacity. This aspect of expressivity is quantiﬁed for linear transductions using preterminalized linear inversion transduction grammars, and compared to the expressivity of ﬁnite-state transductions, inversion transductions and syntax-directed transductions. 1 Introduction We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars (Saers, 2011, LTGs), linear inversion transduction grammars (Saers et al., 2010, LITGs) and preterminalized LITGs (Saers and Wu, 2011, PLITGs). While empirical results such as those in previous work are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transduction classes for all ranks above 3, while ranks 2 an"
2011.mtsummit-papers.49,N10-1050,1,0.938221,"s generated. We refer to the number of different alignments that are allowed under a transduction as its weak alignment capacity. This aspect of expressivity is quantiﬁed for linear transductions using preterminalized linear inversion transduction grammars, and compared to the expressivity of ﬁnite-state transductions, inversion transductions and syntax-directed transductions. 1 Introduction We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars (Saers, 2011, LTGs), linear inversion transduction grammars (Saers et al., 2010, LITGs) and preterminalized LITGs (Saers and Wu, 2011, PLITGs). While empirical results such as those in previous work are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transd"
2011.mtsummit-papers.49,2010.eamt-1.5,0,0.0218833,"is that a PLITG is unable to generate the inside-out alignments, which is ex3 This is only valid for permutation vectors of length four, since we know that the underspeciﬁed part, being of length three, constitutes a valid permutation vector. pected since linear transductions are a proper subset of inversion transductions. The other two that cannot be generated are [1, 0, 3, 2] (which we call serial inversion) and [2, 3, 0, 1] (which we call constituent swapping). Whereas there are some evidence that the inside-out alignments are irrelevant to natural language translation (Huang et al., 2009; Søgaard, 2010), no such results exist for serial inversion and constituent swapping. On the contrary, we intuitively expect these phenomena to be frequent between natural languages. We consider this to be a serious problem with linear transductions, but empirical studies will have give the ﬁnal say on how much it hurts performance. 5 Conclusions In this paper we have presented an analysis of the weak reordering capacity of linear transductions, and compared it to that of ﬁnite-state transduction grammars, inversion transduction grammars and syntax-directed transduction grammars. We have showed that it is po"
2011.mtsummit-papers.49,C96-2141,0,0.262768,"ence and Engineering One Microsoft Way, Redmond Hong Kong University of Science and Technology Washington, USA {masaers|dekai}@cs.ust.hk chrisq@microsoft.com Abstract equally important to understand the formal theoretical properties of any such new representation. In recent years, there has been a shift away from surface-based translation method such as phrasebased statistical machine translation (phrase-based SMT) (Marcu and Wong, 2002; Koehn et al., 2003) in favor of grammar-based SMT. Although most of the grammar based methods still rely on surfacebased word alignments (Brown et al., 1993; Vogel et al., 1996) and language speciﬁc parsers, the grammar-based models themselves restrict reordering to a much higher degree than the surface-based methods, which typically allow any permutation of any segmentation of the input, and relies on heuristic search methods such as beam search, to restrict the exponential time to a tractable polynomial. We investigate the formal expressivity properties of linear transductions, the class of transductions generated by linear transduction grammars, linear inversion transduction grammars and preterminalized linear inversion transduction grammars. While empirical resul"
2011.mtsummit-papers.49,J97-3002,1,0.895993,"ork are of course an ultimate test of modeling adequacy for machine translation applications, it is 431 Although not always given the attention deserved, the expressivity of different grammar-based methods varies quite a lot. Unlike monolingual grammars, where all context-free grammars, for example, fall into one class of languages, the bilingual case is not as well-behaved. Syntax-directed transduction grammars (Lewis and Stearns, 1968; Aho and Ullman, 1972) form distinct transduction classes for all ranks above 3, while ranks 2 and 3 form a class of their own termed inversion transductions (Wu, 1997), and rank 1 forms the class termed linear transductions (Saers, 2011). In this paper we will take a closer look at the expressive powers of transduction grammars in general, noting that the concept of generative capacity fails to capture all the relevant details. Instead, we will propose a division of the expressivity into a strong and weak transductive capacity and alignCapacity Weak Transductive Strong Weak Alignment Strong Required equality Sentence pairs Biparse trees Token alignments Compositional alignments Table 1: Capacities of transduction grammars. ment capacity. The argument for th"
2012.eamt-1.64,W07-0718,0,0.2635,"Missing"
2012.eamt-1.64,2011.eamt-1.3,0,0.0287249,"Missing"
2012.eamt-1.64,W08-0309,0,0.234232,"Missing"
2012.eamt-1.64,W07-0738,0,0.354497,"Missing"
2012.eamt-1.64,W08-0332,0,0.246741,"Missing"
2012.eamt-1.64,J09-4009,0,0.0233322,"Huang et al., 2009). The linear time skeleton algorithm builds canonical binarization trees by reducing greedily but such an approach would not work for a LTG. For example, the permutation [3, 2, 0, 1] which can be parsed by an LTG reduces to 2-3, 0-1 on the stack which cannot be further reduced. 296 We propose an algorithm that makes use of a technique similar to top-down parsing of bisentences using linear transduction grammars. The algorithm is as shown in the procedure parsable. In order to prove the correctness of the algorithm, we use the deﬁnition of permuted sequence from Huang et al. (2009) but we redeﬁne proper split in the context of BLTGs. The proof is as follows: Deﬁnition 1. A permuted sequence is a permutation of consecutive integers. If a permuted sequence of sequence a can be split into the concatenation of a permuted sequence b and a single element of permutation α such that a = (b; α) or a = (α; b), then the corresponding split is called the proper split of a. The deﬁnition of a proper split implicitly imposes the constraints of a linear transduction grammar. Restricting one of the elements in a split to be a single element in the permutation is equivalent to allowing"
2012.eamt-1.64,P11-1023,1,0.855519,"Missing"
2012.eamt-1.64,J05-1004,0,0.0113465,"labels, we noticed that there were some inconsistencies in the annotation. The sentence pairs were manually annotated with the frame sets deﬁned for Chinese and English Zh/En alt. patterns [arg0:action:arg1] [arg0:action] [action:arg1] [arg1:action] [action:arg2] [arg0:action:arg2] [action:arg4] [arg1:action:arg2] [arg1:action:arg4] Sum [arg0:action:arg1] 88 0 0 0 0 3 0 3 3 97 [arg0:action 0 11 3 12 1 0 1 0 0 28 [action:arg1] 0 0 39 6 5 0 1 0 0 51 [arg1:action] 0 0 1 3 0 0 0 0 0 4 Sum 88 11 43 21 6 3 2 3 3 Table 1: Frequency of source and target alternation pattern occurrence in the Propbank (Palmer et al., 2005). We argue that it is due to the limitation of frame set deﬁnitions as they were deﬁned to be consistent within one language but not across languages. For example, in the frame set deﬁnition of 死于 (died of), the arg0 is the entity who dies, while in the frame set deﬁnition of its translation die, the deceased is arg1 and there is no arg0 deﬁned. Similar observations could be made for most of the sentence pairs which diﬀered in source and target alternation labels. As our initial analysis of cross-lingual verb frame alternation patterns suggests that patterns in one language align with only a r"
2012.eamt-1.64,2011.mtsummit-papers.49,1,0.897227,"Missing"
2012.eamt-1.64,N09-2004,1,0.901535,"Missing"
2012.eamt-1.64,J97-3002,1,0.916868,"nd Linear Transduction Grammars (Saers, 2011). As a part of our evaluation, we discuss the reordering of semantic roles within a frame and across frames within a sentence. We also present a novel algorithm to determine whether there exists a canonical parse for an alignment under Linear Transduction Grammars. © 2012 European Association for Machine Translation. 295 To fulﬁll the above requirements, we evaluate two well known syntax-based machine translation formalisms: Inversion Transduction Grammars or ITGs (Wu, 1997) and Linear Transduction Grammars or LTGs (Saers, 2011). As discussed in Wu (1997), ITGs allow nearly all possible reorderings (22 out of 24) given up to four semantic role labels within a semantic frame. Further, various forms of empirical conﬁrmation for the eﬀectiveness of ITG expressivity constraints (Zens and Ney, 2003; Zhang and Gildea, 2005, 2004) motivate us to choose it as a likely candidate. Though ITGs are far more constraining than other higher order syntax directed transduction grammars and IBM models, it would be interesting to see how far an even more constrained model is able to handle reorderings of semantic role ﬁllers. For this purpose, we choose LTGs whi"
2012.eamt-1.64,P03-1019,0,0.745834,"Missing"
2012.eamt-1.64,C04-1060,0,0.0688712,"Missing"
2012.eamt-1.64,P05-1059,0,0.658455,"Missing"
2012.eamt-1.64,H93-1040,0,\N,Missing
2012.eamt-1.64,1993.mtsummit-1.24,0,\N,Missing
2012.eamt-1.64,C00-2108,0,\N,Missing
2012.eamt-1.64,2009.eamt-1.30,1,\N,Missing
2012.eamt-1.64,W09-2300,0,\N,Missing
2012.eamt-1.64,N04-1030,0,\N,Missing
2013.iwslt-papers.15,W13-0806,1,0.512843,"of the hypotheses while (d) uses them to further generate longer transduction rules. For convenience, our implementation breaks this into two stages: one that generates a large set of short transduction rule hypotheses, and another that iteratively segments long transduction rules (initialized from the sentence pairs in the training data) by trying to reuse a minimal subset of the hypotheses while chipping away at the long sentence pair rules until the conditional description length is minimized. 2. Background Description length has been used before to drive iterative segmenting ITG learning [1]. We will use their algorithm as our baseline, but the simple mixture model we used then works poorly with our ITG with categories. Instead, we propose a tighter incorporation, where the rule segmenting learning is biased towards rules that are present in the categorized ITG. We refer to this objective as minimizing conditional description length, since technically, the length of the ITG being segmented is conditioned on the categorized ITG. Conditional description length (CDL) is detailed in Section 3. The minimum CDL (MCDL) objective differs from the simple mixture model in that it separates"
2013.iwslt-papers.15,W07-0403,0,0.0703722,"Missing"
2013.iwslt-papers.15,P08-1012,0,0.0217528,"p into the vast engineering efforts that have gone into perfecting existing decoders, it also prevents you from surpassing them in the long run. The motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing exte"
2013.iwslt-papers.15,P09-1088,0,0.0161427,"e motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by a"
2013.iwslt-papers.15,P09-1104,0,0.0524446,"Missing"
2013.iwslt-papers.15,W09-2304,1,0.905197,"Missing"
2013.iwslt-papers.15,N10-1028,0,0.0131358,"e motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by a"
2013.iwslt-papers.15,N10-1015,0,0.0263712,"Missing"
2013.iwslt-papers.15,P10-1017,0,0.0438788,"Missing"
2013.iwslt-papers.15,N10-1050,1,0.865646,"Missing"
2013.iwslt-papers.15,2011.eamt-1.42,1,0.870705,"Missing"
2013.iwslt-papers.15,P11-1064,0,0.0126129,"e motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by a"
2013.iwslt-papers.15,P12-1018,0,0.0131275,"e motivation for our present series of experiments is that, as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. When the structure of an ITG is induced without supervision, it is possible to get an effect that resembles MDL. [3] impose a sparsity prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by a"
2013.iwslt-papers.15,P06-1121,0,0.0416765,"y prior over the rule probabilities to prevent the search from having to consider all the rules found in the Viterbi biparses. [4, 5, 8, 13, 14] use Gibbs sampling to learn ITGs with priors over the rule structures that serve a similar purpose to the model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints [15]. Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by adding external constraints that are bound to match the translation model poorly. 3. Conditional description length Conditional description length (CDL) is a general method for evaluating a model and a dataset given a preexisting model. This makes it ideal for augmenting an existing model with a variant model of the same family. In this paper we will apply this to augment an existing inversion transduction grammar (ITG) with rules that are found with a diff"
2013.iwslt-papers.15,C12-1142,1,0.888267,"( ) DL D|Φ′ , Ψ − DL (D|Φ, Ψ) ( ) = −lg P D|Φ′ , Ψ − −lg P (D|Φ, Ψ) = −lg P (D|Φ′ , Ψ) P (D|Φ, Ψ) 4. Generating rule hypotheses In the first stage of our learning approach, we generate a large set of possible rules, from which the second stage will choose a small subset to keep. The goal of this stage is to keep the recall high with respect to a theoretical “optimal ITG”, precision is achieved in the second stage. We rely on chunking and category splitting to generate this large set of rule hypotheses. To generate these high-recall ITGs, we will follow the bootstrapping approach presented in [19], and start with a finite-state transduction grammar (FSTG), do the chunking and category splitting within the FSTG framework before transferring the resulting grammar to a corresponding ITG. This is likely to produce an ITG that performs poorly on its own, but may be informative in the second stage. 5. Segmenting rules In the second stage of our learning approach, we segment rules explicitly representing the entire training data, into smaller—more general—rules, reusing rules from the first stage whenever we can. By driving the segmentation-based learning with a minimum description length obj"
2013.iwslt-papers.15,W09-3804,1,0.854975,"ys of using these two stages to arrive at a final ITG, and how we intend to evaluate the quality of those ITGs. For the first stage, we will use the technique described in [19] to start with a finite-state transduction grammar (FSTG) and perform chunking before splitting the nonterminal categories and moving the FSTG into ITG form. We perform one round of chunking, and two rounds of category splitting (resulting in 4 nonterminals and 4 preterminals, which becomes 8 nonterminals in the ITG form). At each stage, we run a few iterations of expectation maximization using the algorithm detailed in [20] for biparsing. For comparison we also bootstrap a comparable ITG that has not had the categories split. Before using either of the bootstrapped ITGs, we eliminate all rules that do not have a probability above a threshold that we fixed to 10−50 . This eliminates the highly unlikely rules from the ITG. For the second stage, we use the iterative rule segmentation learning algorithm driven by minimum conditional description length that we introduced in Section 5. We will try three different variants on this algorithm: one without an ITG to condition on, one conditioned on the chunked ITG, and on"
2013.iwslt-papers.15,J07-2003,0,0.0344983,"learned ITG. All the above outlined ITGs are trained using the IWSLT07 Chinese–English data set [21], which contains 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more “word like” sequences [22]. To test the learned ITGs, we use them as translation systems with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm [23–25] and cube pruning [26] to integrate the language model scores. For language model, we use a trigram language model trained with the SRILM toolkit [27] on the English side of the training corpus. To evaluate the resulting translations, we use BLEU [28] and NIST [29]. 7. Results In this section we present the empirical results: bilingual categories help translation quality under the experimental conditions detailed in the previous section. The results are summarized in Table 1. As predicted the base chunked only ITG fares poorly, while the categories help a great deal in the chunked w/categories only ITG—though the s"
2013.iwslt-papers.15,P02-1040,0,0.0878297,"test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more “word like” sequences [22]. To test the learned ITGs, we use them as translation systems with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm [23–25] and cube pruning [26] to integrate the language model scores. For language model, we use a trigram language model trained with the SRILM toolkit [27] on the English side of the training corpus. To evaluate the resulting translations, we use BLEU [28] and NIST [29]. 7. Results In this section we present the empirical results: bilingual categories help translation quality under the experimental conditions detailed in the previous section. The results are summarized in Table 1. As predicted the base chunked only ITG fares poorly, while the categories help a great deal in the chunked w/categories only ITG—though the scores are not very reliable when in this low range. The trade-off between model and data size during segmentation conditioned on the ITG with categories is illustrated in Figure 1. It starts out with most of the total description"
2013.mtsummit-papers.14,C12-1142,1,0.568939,"ces involving alignments which are not purely monotonic. We trade-off some of the initial fluency that segmental phrase-based models offer for the flexibility offered by the token based models. We induce the bracketing ITG on the corpus generated from the previous stage to identify the word associations between rhyming lines. Expectation maximization (Dempster et al., 1977) is used to estimate the model parameters for the bracketing grammar. As the corpora are fairly large, beam pruning is used to make the training faster. Further details of the transduction grammar induction can be found in (Saers et al., 2012; Saers and Wu, 2011). 3.3 Decoding: Challenge-response algorithm We translate the challenge into responses using our in-house ITG decoder Wu (1996); Wu and Wong (1998) for the task of decoding. The decoder builds the parse forest using a CKY-style parsing algorithm which is represented in an efficient hypergraph structure. The translation hypotheses are scored using the transduction grammar and the language model efficiently using cube pruning (Chiang, 2007) In our decoding algorithm, we allow only straight rules as we want to produce responses with the same rhyming order as the challenge. In"
2013.mtsummit-papers.14,W09-2006,0,0.240592,"ng responses to hip hop challenge lyrics. An SMT system was used in conjunction with stress patterns and rhymes found in a pronunciation dictionary to produce translations of poems by Genzel et al. (2010). However, it was challenging to produce translations of full verses that satisfied all the constraints enforced by classical poetry. For example, the translations could not comply with the desired meter of the line although the rhyming constraints were satisfied. These results indicate the difficulty of producing quality output via automatic methods even for very structured domains. A et al. (2009) proposed a model for automatically generating Tamil lyrics given a melody. The lyrics were represented as a sequence of labels using the KNM system where K, N and M represented the long vowels, short vowels and consonants respectively. They solved the sequence labeling problem of generating the lyrics given a melody using conditional random fields. Others have attempted to identify word-to-word relationships, stress patterns (Greene et al., 2010) and rhyming words (Reddy and Knight, 2011), mostly in the domain of poetry. Greene et al. (2010) used an FST to assign stress patterns to words give"
2013.mtsummit-papers.14,J07-2003,0,0.155234,"rly large, beam pruning is used to make the training faster. Further details of the transduction grammar induction can be found in (Saers et al., 2012; Saers and Wu, 2011). 3.3 Decoding: Challenge-response algorithm We translate the challenge into responses using our in-house ITG decoder Wu (1996); Wu and Wong (1998) for the task of decoding. The decoder builds the parse forest using a CKY-style parsing algorithm which is represented in an efficient hypergraph structure. The translation hypotheses are scored using the transduction grammar and the language model efficiently using cube pruning (Chiang, 2007) In our decoding algorithm, we allow only straight rules as we want to produce responses with the same rhyming order as the challenge. Interleaved rhyming order is harder to evaluate without the larger context of the song and we do not address that problem in our current model. Singleton rules are penalized, as successive lines in a stanza are typically of similar length. Lastly, we add a penalty to reflexive translation rules that map the same surface form to itself such as A → ”yo”/”yo”. We observed that such rules have a high probability in our learned grammar due to presence of sentence pa"
2013.mtsummit-papers.14,D10-1051,0,0.731321,"re discussed in Section 2. Sections 3 and 4 contain our system description and experimental setup respectively. Results and conclusions are presented in Sections 5 and 6. 2 Related work Although a handful of previous approaches applied SMT models and other statistical learning methods have been to unconventional domains in the past, ours is the first known work on the domain of hip hop lyrics. Most of the past work in this vein can be classified into two categories. In the first category some form of prior linguistic knowledge about the domain, such as pronunciation dictionaries Genzel et al. (2010) or phonological or morphological information is used to bootstrap the learning. While the second category uses unsupervised learning methods to identify word association probabilities, appropriate bias is provided by the inherent constraints in the domain such as a set number of words in a line (in Chinese couplets), or a set meter (in classical poetry). In this work, we present a completely unsupervised model on a domain that inherently has very few such constraints. As previously mentioned, hip hop lyrics unlike poems (especially in classical poetry where, for example, an octave has exactly"
2013.mtsummit-papers.14,P09-1104,0,0.0357094,"T model. Each selected pair generates two training instances: a challenge-response and a responsechallenge pair as the source and target languages are identical. 3.2 Unsupervised learning: Stochastic transduction grammar induction We choose to induce a token based inversion transduction grammar (ITG) model (Wu, 1997, 1995a,b) because of its expressiveness and the empirical evidence for its representational capacity across a wide spectrum of natural language tasks including textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003; Haghighi et al., 2009). We restrict the ITG to a bracketing ITG (BITG) and use it as the translation model for our SMT system as the focus of our model is to learn the token level correspondences in order to identify potential rhyming candidates. We chose an ITG as opposed to a monotonic finitestate transduction grammar model in order to be able to learn token level correspondences involving alignments which are not purely monotonic. We trade-off some of the initial fluency that segmental phrase-based models offer for the flexibility offered by the token based models. We induce the bracketing ITG on the corpus gene"
2013.mtsummit-papers.14,C08-1048,0,0.407246,". In this work, we present a completely unsupervised model on a domain that inherently has very few such constraints. As previously mentioned, hip hop lyrics unlike poems (especially in classical poetry where, for example, an octave has exactly 10 syllables per line and 8 lines per stanza) do not require a set number of syllables in a line. Also, not all words in the lyrics are required to be a part of the lexicon. Finally, rhyming is frequently achieved via intonation and assonance making it hard to apply prior phonological constraints. A brief summary of the related work is presented below. Jiang and Zhou (2008) trained a phrase-based SMT system to translate the first line of of a Chinese couplet or duilian into the second. Linguistic constraints were applied to the n best output of the SMT system to select the most suitable next line. 110 However in contrast to Chinese couplets, which adhere to strict rules requiring, for example, an identical number of characters in each line and one-toone correspondence in their metrical length, the domain of hip hop lyrics is far more unstructured and there exists no clear constraint that would ensure fluent and rhyming responses to hip hop challenge lyrics. An S"
2013.mtsummit-papers.14,P07-2045,0,0.00374336,"on lines of hip hop lyrics. A subset of 85 lines was randomly chosen as a test set to provide the hip hop challenge lyrics to the systems. In order to train the rhyme scheme detector, we extracted the end-of-line words and words before all the commas2 from each verse. We obtained a corpus containing 4.2 million tokens corresponding to potential rhyming candidates (with around 153,000 unique token types). 4.2 Phrase-based SMT baseline In order to evaluate the performance of standard SMT alignment and search strategies on this novel “translation” task, we also trained a standard Moses baseline (Koehn et al., 2007) and compared its performance to our transduction grammar based SMT system. A 4-gram language model which was trained on the entire training corpus using SRILM (Stolcke, 2002) was used in decoding both the baseline and our model. Both the baseline and our bracketing ITG model were used to decode a held out test set with a slightly higher language model weight which was empirically chosen using a small development set to produce fluent outputs. The best translation produced by both these SMT systems was used to evaluate their performance at the task of improvising fluent and rhyming responses g"
2013.mtsummit-papers.14,W11-1008,1,0.194579,"ents which are not purely monotonic. We trade-off some of the initial fluency that segmental phrase-based models offer for the flexibility offered by the token based models. We induce the bracketing ITG on the corpus generated from the previous stage to identify the word associations between rhyming lines. Expectation maximization (Dempster et al., 1977) is used to estimate the model parameters for the bracketing grammar. As the corpora are fairly large, beam pruning is used to make the training faster. Further details of the transduction grammar induction can be found in (Saers et al., 2012; Saers and Wu, 2011). 3.3 Decoding: Challenge-response algorithm We translate the challenge into responses using our in-house ITG decoder Wu (1996); Wu and Wong (1998) for the task of decoding. The decoder builds the parse forest using a CKY-style parsing algorithm which is represented in an efficient hypergraph structure. The translation hypotheses are scored using the transduction grammar and the language model efficiently using cube pruning (Chiang, 2007) In our decoding algorithm, we allow only straight rules as we want to produce responses with the same rhyming order as the challenge. Interleaved rhyming ord"
2013.mtsummit-papers.14,P95-1033,1,0.688319,"Missing"
2013.mtsummit-papers.14,J97-3002,1,0.321446,"e then segment each verse into stanzas with their respective rhyme schemes according to the Viterbi parse of the model. We then select the lines from each stanza that rhyme with each other according to its rhyme scheme and add them as training instances for our transduction grammar based SMT model. Each selected pair generates two training instances: a challenge-response and a responsechallenge pair as the source and target languages are identical. 3.2 Unsupervised learning: Stochastic transduction grammar induction We choose to induce a token based inversion transduction grammar (ITG) model (Wu, 1997, 1995a,b) because of its expressiveness and the empirical evidence for its representational capacity across a wide spectrum of natural language tasks including textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003; Haghighi et al., 2009). We restrict the ITG to a bracketing ITG (BITG) and use it as the translation model for our SMT system as the focus of our model is to learn the token level correspondences in order to identify potential rhyming candidates. We chose an ITG as opposed to a monotonic finitestate transduction gra"
2013.mtsummit-papers.14,I05-1023,1,0.779954,"them as training instances for our transduction grammar based SMT model. Each selected pair generates two training instances: a challenge-response and a responsechallenge pair as the source and target languages are identical. 3.2 Unsupervised learning: Stochastic transduction grammar induction We choose to induce a token based inversion transduction grammar (ITG) model (Wu, 1997, 1995a,b) because of its expressiveness and the empirical evidence for its representational capacity across a wide spectrum of natural language tasks including textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003; Haghighi et al., 2009). We restrict the ITG to a bracketing ITG (BITG) and use it as the translation model for our SMT system as the focus of our model is to learn the token level correspondences in order to identify potential rhyming candidates. We chose an ITG as opposed to a monotonic finitestate transduction grammar model in order to be able to learn token level correspondences involving alignments which are not purely monotonic. We trade-off some of the initial fluency that segmental phrase-based models offer for the flexibility offered by the"
2013.mtsummit-papers.14,P98-2230,1,0.651793,"y the token based models. We induce the bracketing ITG on the corpus generated from the previous stage to identify the word associations between rhyming lines. Expectation maximization (Dempster et al., 1977) is used to estimate the model parameters for the bracketing grammar. As the corpora are fairly large, beam pruning is used to make the training faster. Further details of the transduction grammar induction can be found in (Saers et al., 2012; Saers and Wu, 2011). 3.3 Decoding: Challenge-response algorithm We translate the challenge into responses using our in-house ITG decoder Wu (1996); Wu and Wong (1998) for the task of decoding. The decoder builds the parse forest using a CKY-style parsing algorithm which is represented in an efficient hypergraph structure. The translation hypotheses are scored using the transduction grammar and the language model efficiently using cube pruning (Chiang, 2007) In our decoding algorithm, we allow only straight rules as we want to produce responses with the same rhyming order as the challenge. Interleaved rhyming order is harder to evaluate without the larger context of the song and we do not address that problem in our current model. Singleton rules are penali"
2015.mtsummit-papers.26,2012.eamt-1.64,1,0.794263,"Missing"
2015.mtsummit-papers.26,W05-0909,0,0.303895,"ectively. The MEANT score for ITG based systems is considerably higher than the MEANT score for GIZA++ aligned model. We also observe that MEANT score for ITG with SRL constraints is better than the conventional ITG model. We believe that a better SRL-parser would yield a better system still. Tables 4, 5 and 6 give an upper bound on the results for the ten runs, we also see here that new semantically biased ITG model outperforms the baseline and ITG based alignment. In addition, we evaluated the performance of our model using surface-based metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006), and observed that both the unbiased ITG and semantically biased ITG based systems yield comparable results that are high in comparison to conventional GIZA++ alignment. Figure 1 shows an interesting example extracted from the test data. The Chinese input sentence has been pre-segmented into eight word-like units, and a word-for-word gloss reads approximately ”this one in Japan yet haven’t sell .” The translator who produce the reference translation took some liberties and introduced the actor they, who no d"
2015.mtsummit-papers.26,2014.iwslt-evaluation.4,1,0.88414,"ivated by the fact that including semantic role labeling in the SMT pipeline in a different way has already been shown to increase translation quality. The semantic frame based evaluation metric MEANT, which was shown to correlate better with human adequacy judgment than conventional surface based evaluation metrics (Lo et al., 2012), can be used as an objective function for tuning SMT. Tuning to MEANT, which attempts to optimize the degree to which a sentence’s semantic frames can be preserved across translation, was shown to improve translation quality across many metrics (Lo et al., 2013b; Beloucif et al., 2014). We show in this paper that including soft constraints based on semantic role labeling into the alignment training step yields both higher adequacy-oriented MEANT and, while still producing comparable scores on surface based and edit distance metrics. 2 2.1 Related work Alignment For most recent automatic machine translation systems, learning a good word alignment is paramount for producing meaningful translation. Unfortunately, conventional alignment algorithms such as IBM models (Brown et al., 1990) and the HMM-alignment model (Vogel et al., 1996) are flat and directed, meaning that (a) the"
2015.mtsummit-papers.26,P09-1088,0,0.212889,"complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good trans"
2015.mtsummit-papers.26,J90-2002,0,0.818289,"at tuning towards preserving the shallow semantic structure across translations, robustly improves translation performance. Our approach brings the same intuition into the training phase. We show that our new alignment outperforms both conventional Moses and BITG alignment baselines in terms of the adequacy-oriented MEANT scores, while still producing comparable results in terms of edit distance metrics. 1 Introduction The quality of machine translation output relies heavily on word alignment. However, the most widespread approach to word alignment is the ad hoc method of training IBM models (Brown et al., 1990) in both directions and combining their results using various heuristics. Word alignments based on inversion transduction grammars or ITGs (Wu, 1997), on the other hand, provide a more structured model leading to efficient and optimal bidirectional alignments. In this paper we introduce an improved word aligner based on applying soft semantic role label constraints to ITG alignment. We show that both translation adequacy and fluency can be improved by replacing the conventional GIZA++ based alignment (Och and Ney, 2000) with more semantically motivated alignments obtained through training Proc"
2015.mtsummit-papers.26,W07-0403,0,0.20279,"r cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the prin"
2015.mtsummit-papers.26,J07-2003,0,0.22586,"omes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully c"
2015.mtsummit-papers.26,P09-1104,0,0.371113,"Missing"
2015.mtsummit-papers.26,P07-2045,0,0.0060811,"rd SRL constraints did not lead to any alignment at all: the constraints were too harsh and did not permit any biparses. Soft SRL constrained ITGs, on the other hand, outperformed the unbiased BITG model in term of both adequacyoriented MEANT scores. We noticed that λ0 = 1 and λ1 = 0.5 correspond to the best combination. The SRL constraints were only used during training of the probabilities of the ITG, and not when extracting the Viterbi parses and the corresponding word alignments. 4.3 SMT pipeline To test the different alignments described in this paper, we used the standard Moses toolkit (Koehn et al., 2007), with a 6-gram language model learned with the SRI language model toolkit (Stolcke, 2002), to train our baselines. We tested our approach using Moses hierarchical models. For tuning, we used ZMERT (Zaidan, 2009), a standard implementation of minimum error rate training or MERT (Och, 2003), we run each tuning task ten times for each system, then we decoded with both DEV and TEST set, we then chose the results according to what performed the best on the know developProceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 339 ment data. We also present the highes"
2015.mtsummit-papers.26,E06-1031,0,0.23652,"based systems is considerably higher than the MEANT score for GIZA++ aligned model. We also observe that MEANT score for ITG with SRL constraints is better than the conventional ITG model. We believe that a better SRL-parser would yield a better system still. Tables 4, 5 and 6 give an upper bound on the results for the ten runs, we also see here that new semantically biased ITG model outperforms the baseline and ITG based alignment. In addition, we evaluated the performance of our model using surface-based metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006), and observed that both the unbiased ITG and semantically biased ITG based systems yield comparable results that are high in comparison to conventional GIZA++ alignment. Figure 1 shows an interesting example extracted from the test data. The Chinese input sentence has been pre-segmented into eight word-like units, and a word-for-word gloss reads approximately ”this one in Japan yet haven’t sell .” The translator who produce the reference translation took some liberties and introduced the actor they, who no doubt was present in the cont"
2015.mtsummit-papers.26,P11-1023,1,0.898338,"biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good translation is one where a human can successfully understand the main meaning of the output sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). The MEANT family of metrics are semantic evaluation MT evaluation metrics that correlate with human adequacy judgements more closely than most commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). MEANT compares the MT output sentence against provided reference translations, and produce a score measuring the degree of similarity between their semantic frame structures. Our new approach is encouraged by the fact that many previous studies have empirically shown that integrating semantic role labeling i"
2015.mtsummit-papers.26,W12-4206,1,0.913979,"Missing"
2015.mtsummit-papers.26,2013.mtsummit-papers.12,1,0.882039,"Missing"
2015.mtsummit-papers.26,W13-2254,1,0.858506,"onventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good translation is one where a human can successfully understand the main meaning of the output sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). The MEANT family of metrics are semantic evaluation MT evaluation metrics that correlate with human adequacy judgements more closely than most commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). MEANT compares the MT output sentence against provided reference translations, and produce a score measuring the degree of similarity between their semantic frame structures. Our new approach is encouraged by the fact that many previous studies have empirically shown that integrating semantic role labeling into the training pipeline by tuning against MEANT improves the translation adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014). We show here, that soft incorporation of SRL constraints much earlier in the pipeline, at the word"
2015.mtsummit-papers.26,W12-3129,1,0.921193,"Missing"
2015.mtsummit-papers.26,P13-2067,1,0.910847,"ach is further motivated by the fact that including semantic role labeling in the SMT pipeline in a different way has already been shown to increase translation quality. The semantic frame based evaluation metric MEANT, which was shown to correlate better with human adequacy judgment than conventional surface based evaluation metrics (Lo et al., 2012), can be used as an objective function for tuning SMT. Tuning to MEANT, which attempts to optimize the degree to which a sentence’s semantic frames can be preserved across translation, was shown to improve translation quality across many metrics (Lo et al., 2013b; Beloucif et al., 2014). We show in this paper that including soft constraints based on semantic role labeling into the alignment training step yields both higher adequacy-oriented MEANT and, while still producing comparable scores on surface based and edit distance metrics. 2 2.1 Related work Alignment For most recent automatic machine translation systems, learning a good word alignment is paramount for producing meaningful translation. Unfortunately, conventional alignment algorithms such as IBM models (Brown et al., 1990) and the HMM-alignment model (Vogel et al., 1996) are flat and direc"
2015.mtsummit-papers.26,2013.iwslt-evaluation.5,1,0.888572,"ach is further motivated by the fact that including semantic role labeling in the SMT pipeline in a different way has already been shown to increase translation quality. The semantic frame based evaluation metric MEANT, which was shown to correlate better with human adequacy judgment than conventional surface based evaluation metrics (Lo et al., 2012), can be used as an objective function for tuning SMT. Tuning to MEANT, which attempts to optimize the degree to which a sentence’s semantic frames can be preserved across translation, was shown to improve translation quality across many metrics (Lo et al., 2013b; Beloucif et al., 2014). We show in this paper that including soft constraints based on semantic role labeling into the alignment training step yields both higher adequacy-oriented MEANT and, while still producing comparable scores on surface based and edit distance metrics. 2 2.1 Related work Alignment For most recent automatic machine translation systems, learning a good word alignment is paramount for producing meaningful translation. Unfortunately, conventional alignment algorithms such as IBM models (Brown et al., 1990) and the HMM-alignment model (Vogel et al., 1996) are flat and direc"
2015.mtsummit-papers.26,W13-2202,0,0.110404,"tics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good translation is one where a human can successfully understand the main meaning of the output sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). The MEANT family of metrics are semantic evaluation MT evaluation metrics that correlate with human adequacy judgements more closely than most commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). MEANT compares the MT output sentence against provided reference translations, and produce a score measuring the degree of similarity between their semantic frame structures. Our new approach is encouraged by the fact that many previous studies have empirically shown that integrating semantic role labeling into the training pipeline by tuning against MEANT improves the translation adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014). We show here, that soft incorporation of SRL constraints much earlier in the pipeline, at the word alignment stage of SMT trai"
2015.mtsummit-papers.26,P11-1064,0,0.385195,"Missing"
2015.mtsummit-papers.26,niessen-etal-2000-evaluation,0,0.362777,"bly higher than the MEANT score for GIZA++ aligned model. We also observe that MEANT score for ITG with SRL constraints is better than the conventional ITG model. We believe that a better SRL-parser would yield a better system still. Tables 4, 5 and 6 give an upper bound on the results for the ten runs, we also see here that new semantically biased ITG model outperforms the baseline and ITG based alignment. In addition, we evaluated the performance of our model using surface-based metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006), and observed that both the unbiased ITG and semantically biased ITG based systems yield comparable results that are high in comparison to conventional GIZA++ alignment. Figure 1 shows an interesting example extracted from the test data. The Chinese input sentence has been pre-segmented into eight word-like units, and a word-for-word gloss reads approximately ”this one in Japan yet haven’t sell .” The translator who produce the reference translation took some liberties and introduced the actor they, who no doubt was present in the context, but is not needed when"
2015.mtsummit-papers.26,P00-1056,0,0.595762,"Missing"
2015.mtsummit-papers.26,P03-1021,0,0.0267747,"o the best combination. The SRL constraints were only used during training of the probabilities of the ITG, and not when extracting the Viterbi parses and the corresponding word alignments. 4.3 SMT pipeline To test the different alignments described in this paper, we used the standard Moses toolkit (Koehn et al., 2007), with a 6-gram language model learned with the SRI language model toolkit (Stolcke, 2002), to train our baselines. We tested our approach using Moses hierarchical models. For tuning, we used ZMERT (Zaidan, 2009), a standard implementation of minimum error rate training or MERT (Och, 2003), we run each tuning task ten times for each system, then we decoded with both DEV and TEST set, we then chose the results according to what performed the best on the know developProceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 339 ment data. We also present the highest score achieved by each system among all runs (an upper bound on the score that can be achieved). We compared an edit-distance based metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), and a semantic evaluation metric MEANT (Lo et al., 2012) as the tuning objective. 5 Res"
2015.mtsummit-papers.26,P02-1040,0,0.0952017,"s hierarchical models. For tuning, we used ZMERT (Zaidan, 2009), a standard implementation of minimum error rate training or MERT (Och, 2003), we run each tuning task ten times for each system, then we decoded with both DEV and TEST set, we then chose the results according to what performed the best on the know developProceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 339 ment data. We also present the highest score achieved by each system among all runs (an upper bound on the score that can be achieved). We compared an edit-distance based metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), and a semantic evaluation metric MEANT (Lo et al., 2012) as the tuning objective. 5 Results We compared the the performance of our soft SRL-constrained ITG alignment to (a) the GIZA++ baseline and (b) the unbiased BITG, for both BLEU, TER and MEANT tuned systems. We evaluated our MT output using the semantic metric MEANT (Lo et al., 2012). Tables 1, 2 and 3 show the improvment in terms of MEANT scores for the SRL ITG aligned system in comparison to conventional ITG alignment and GIZA++ alignment for BLEU, TER and MEANT tuned systems respectively. The MEANT score"
2015.mtsummit-papers.26,N04-1030,0,0.769282,"troduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment method is fully compatible with the principle that a good translation is one where a human can successfully understand the main meaning of the output sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). The MEANT family of metrics are semantic evaluation MT evaluation metrics that correlate with human adequacy judgements more closely than most commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). MEANT compares the MT output sentence against provided reference translations, and produce a score measuring the degree of similarity between their semantic frame structures. Our new approach is encouraged by the fact that many previous studies have empirically shown that integrating semanti"
2015.mtsummit-papers.26,W09-2304,1,0.915134,"optimal. Transduction grammars, on the other hand, do provide a model that (a) is inherently structurally compositional, and (b) can provide optimal bidirectional alignments. Although this structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses"
2015.mtsummit-papers.26,W09-3804,1,0.965484,"on grammars, on the other hand, do provide a model that (a) is inherently structurally compositional, and (b) can provide optimal bidirectional alignments. Although this structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or"
2015.mtsummit-papers.26,N10-1050,1,0.917202,"ectional alignment that the translation system is trained on to be optimal. Transduction grammars, on the other hand, do provide a model that (a) is inherently structurally compositional, and (b) can provide optimal bidirectional alignments. Although this structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased"
2015.mtsummit-papers.26,2006.amta-papers.25,0,0.525027,"g, we used ZMERT (Zaidan, 2009), a standard implementation of minimum error rate training or MERT (Och, 2003), we run each tuning task ten times for each system, then we decoded with both DEV and TEST set, we then chose the results according to what performed the best on the know developProceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 339 ment data. We also present the highest score achieved by each system among all runs (an upper bound on the score that can be achieved). We compared an edit-distance based metrics, BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), and a semantic evaluation metric MEANT (Lo et al., 2012) as the tuning objective. 5 Results We compared the the performance of our soft SRL-constrained ITG alignment to (a) the GIZA++ baseline and (b) the unbiased BITG, for both BLEU, TER and MEANT tuned systems. We evaluated our MT output using the semantic metric MEANT (Lo et al., 2012). Tables 1, 2 and 3 show the improvment in terms of MEANT scores for the SRL ITG aligned system in comparison to conventional ITG alignment and GIZA++ alignment for BLEU, TER and MEANT tuned systems respectively. The MEANT score for ITG based systems is cons"
2015.mtsummit-papers.26,C96-2141,0,0.562813,"y across many metrics (Lo et al., 2013b; Beloucif et al., 2014). We show in this paper that including soft constraints based on semantic role labeling into the alignment training step yields both higher adequacy-oriented MEANT and, while still producing comparable scores on surface based and edit distance metrics. 2 2.1 Related work Alignment For most recent automatic machine translation systems, learning a good word alignment is paramount for producing meaningful translation. Unfortunately, conventional alignment algorithms such as IBM models (Brown et al., 1990) and the HMM-alignment model (Vogel et al., 1996) are flat and directed, meaning that (a) they allow unstructured movement of words leading to weak word alignment, (b) translations in one direction are considered in isolation, and (c) two separate alignments are needed to form a single bidirectional alignment. The harmonization of two directed alignments is typically done heuristically, which means that there is no model that considers the final bidirectional alignment that the translation system is trained on to be optimal. Transduction grammars, on the other hand, do provide a model that (a) is inherently structurally compositional, and (b"
2015.mtsummit-papers.26,W95-0106,1,0.736259,"lthough this structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeli"
2015.mtsummit-papers.26,J97-3002,1,0.787959,"ion into the training phase. We show that our new alignment outperforms both conventional Moses and BITG alignment baselines in terms of the adequacy-oriented MEANT scores, while still producing comparable results in terms of edit distance metrics. 1 Introduction The quality of machine translation output relies heavily on word alignment. However, the most widespread approach to word alignment is the ad hoc method of training IBM models (Brown et al., 1990) in both directions and combining their results using various heuristics. Word alignments based on inversion transduction grammars or ITGs (Wu, 1997), on the other hand, provide a more structured model leading to efficient and optimal bidirectional alignments. In this paper we introduce an improved word aligner based on applying soft semantic role label constraints to ITG alignment. We show that both translation adequacy and fluency can be improved by replacing the conventional GIZA++ based alignment (Och and Ney, 2000) with more semantically motivated alignments obtained through training Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 333 ITGs (Saers and Wu, 2009) under soft SRL constraints. The n"
2015.mtsummit-papers.26,P05-1059,0,0.384966,"s structured optimality comes at a higher cost in terms of time complexity, it allows for preexisting structured information to be incorporated into the model, and for models to be compared in a meaningful way. There are different classes of transduction grammars, ranging from finite-state transduction grammar, via linear transduction grammar (Saers et al., 2010) and inversion transduction grammar (Wu, 1997; Saers and Wu, 2009; Saers et al., 2009), to syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972) and many ways to formulate the model over them: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 334 (2009); Saers et al. (2010); Neubig et al. (2011). In this paper, we introduce a semantically biased version of inversion transduction grammars (Wu, 1997) that is biased towards constituents that conform to monolingual semantic parses on the input and/or output languages, and compare their performance against (a) ITGs without such a bias, and (b) the conventional heuristics. 2.2 Semantic role labeling in MT Our alignment me"
C12-1142,P05-1033,0,0.111335,"hat a transduction grammar (or synchronous grammar) is learned without using any external resources such as parses, POS tags, or dictionaries. In contrast, many, if not most, tree-based statistical MT approaches rely on monolingually parsed, chunked, and/or tagged parallel corpora (e.g., Galley et al. (2006)), from which a transduction grammar is extracted. Such approaches must compensate for monolingual analyses that often are not designed optimally for expressing the relationship between, say, English and Chinese. Exceptions include the hierarchical phrase-based SMT method of learning ITGs (Chiang, 2005), which does not rely on external resources. However, unlike Chiang (2005) where huge numbers of (linguistically questionable) phrase translations are essentially memorized, our present work aims at inducing syntactic categories at an early stage in the learning, as occurs in child language acquisition. While only time will tell whether our more cognitively motivated approach to the induction of structural and lexical relationships between two languages will lead to better machine translation models, it is our belief that ultimately, learning the correct categories as early as humans do will p"
C12-1142,P06-1121,0,0.227971,"ach to bilingual grammar induction that is cognitively oriented toward early category formation and phrasal chunking in the bootstrapping process up the expressiveness hierarchy from finite-state to linear to inversion transduction grammars. In the context of bilingual grammar induction, “unsupervised” means that a transduction grammar (or synchronous grammar) is learned without using any external resources such as parses, POS tags, or dictionaries. In contrast, many, if not most, tree-based statistical MT approaches rely on monolingually parsed, chunked, and/or tagged parallel corpora (e.g., Galley et al. (2006)), from which a transduction grammar is extracted. Such approaches must compensate for monolingual analyses that often are not designed optimally for expressing the relationship between, say, English and Chinese. Exceptions include the hierarchical phrase-based SMT method of learning ITGs (Chiang, 2005), which does not rely on external resources. However, unlike Chiang (2005) where huge numbers of (linguistically questionable) phrase translations are essentially memorized, our present work aims at inducing syntactic categories at an early stage in the learning, as occurs in child language acqu"
C12-1142,P02-1040,0,0.092651,"ments on using our trained models for the task of SMT. Table 3 shows the BLEU scores obtained using only token based models (without any sort of chunking) on the IWSLT07_CE test set. These scores gives a gist of how translation quality changes with the grammar formalism, but a token-based model will naturally perform poorly compared to the state of the art. We used an in-house decoder for the purpose of decoding using our models. We used a trigram LM trained using SRILM (Stolcke, 2002) on the IWSLT dataset and a part of the gigaword data set. 2337 We can observe significant gains in the BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) scores after training on LTGs and a further improvement after training on ITGs. Further, these scores seem to be reflecting our estimates about the goodness of our models using crossentropy. Due to time constraints, we could not perform extensive experiments on the quality of translations produced with other models discussed in the rest of the paper. We also want to emphasize that an exhaustive evaluation of the performance of these models on translation tasks would result in a combinatoric explosion of models where chunking and splitting could be applied at variou"
C12-1142,W09-3804,1,0.734216,"g promoted. In this round of experiments, β was uniform over all nonterminals. Naturally, we want to keep the new ITG in normal form, so we will actually not insert the unary rules, but rather anything the nonterminal can expand into. This gives us the following new probability function (p′ ) over the new and old rules: p′ (x → φ; π) = αp(x → φ; π) + (1 − α) ∑ y∈N  β y p( y → φ; π) Where x is the preterminal being promoted, N is the set of nonterminals, φ is a sequence of nonterminals and biterminals, and π is a permutation over φ. To train at the ITG stage, we use the algorithm presented in Saers et al. (2009), which we generalize to handle multiple nonterminals rather than being restricted to bracketing ITGs. It is clear from Table 1 that the ITG models explain the data better than any other kind of model, and that more induction steps are better (the best model is the most heavily processed one, with two chunking steps and three splitting steps before moving on to ITGs). In the cases where we move to ITGs via PLITGs, we also see some improvements, which is encouraging. 2335 7 Qualitative analysis and translation assessments In this section, we present a qualitative analysis of an example to illus"
C12-1142,N10-1050,1,0.774451,"iform probability, and the lexical rules will have a portion of the probability mass relative to their cooccurrence in the training corpus. Specifically, we will initialize a stochastic bracketing FSTG such that: p (S → A) = 1, = 0.5, p (A → ε/ε)  p Q → e/ f = 0.5, = ∑ p (A → QA) where c e/ f {ε/ε}.  c e/ f e′ ∈Σ, f ′ ∈∆ c  e′ / f ′  is the cooccurrence count for the biterminal e/ f ∈ ((Σ ∪ {ε}) × (∆ ∪ {ε})) − To calculate the expectations for the expectation maximization, we will parse the training corpus with the grammar we have, using an adaptation of the parsing algorithm described in Saers et al. (2010). The adaptation consists of allowing multiple categories (not simply bracketing grammars), and to handle preterminals. Although this algorithm was designed for the linear family of transduction grammars, the PFSTGs are merely a restricted form of PLITGs, which means that the algorithm applies to them as well. Training this grammar on IWSLT07 ChineseEnglish data (Fordyce, 2007) gave us a sentencelevel cross-entropy of 110.2. The results of all training runs can be found in Table 1, where the baseline model is designated fstg. 2329 3 Chunking helps We can induce new lexical rules by allowing le"
C12-1142,2011.eamt-1.42,1,0.74365,"orithm applies to them as well. Training this grammar on IWSLT07 ChineseEnglish data (Fordyce, 2007) gave us a sentencelevel cross-entropy of 110.2. The results of all training runs can be found in Table 1, where the baseline model is designated fstg. 2329 3 Chunking helps We can induce new lexical rules by allowing lexical entities to combine in order to form larger lexical entities. The end results are similar to phrase-based machine translation, but the method of arriving at the segments (chunks or “phrases”) is very different. To apply chunking to our PFSTG, we use the method described in Saers and Wu (2011). The method was originally developed for PLITGs, which is a superset of PFSTGs, so it can be applied as is. The gist of the method is to allow two bitokens that are observed next to each other to combine into a new, larger bitoken. The process is thus limited to produce bitokens of twice the length of existing bitokens. There is, however, nothing stopping us from applying the method several times, getting larger and larger chunks. The maximum bitoken length of the baseline grammar is 2 (one input token and one output token), which we can double by chunking. By applying chunking to the baselin"
C12-1142,2011.mtsummit-papers.49,1,0.814142,"grammar that can possibly be afforded. PLITGs are wedged between PFSTGs and ITGs in terms of expressiveness. They do allow for some structured reordering. Like PFSTGs, the parse trees are chains rather than trees, but unlike PFSTGs, the links in these chains do not have to be physically adjacent. Allowing the first input token to be lexically associated with the last output token is an example of the kind of reordering that PLITGs allow, which make them more expressive than PFSTGs. The fact that these reorderings are limited to lexical units only is what makes them less expressive than ITGs (Saers et al., 2011) . The rest of the paper is structured so that we begin by describing the initial finite-state transduction grammar that we will start our bootstrapping sequence (Section 2). We then describe how the lexical chunking (Section 3) and category splitting (Section 4) is carried out before moving on to expressivity expansion: first from finite-state to linear transduction grammars (Section 5), and then from linear to inversion transduction grammar (Section 6). We then move on to an illustrative example of what the bootstrapping process actually entails (Section 7), along with some tentative decodin"
C12-1142,J97-3002,1,0.680696,"shing the foundations for understanding category formation in bottom-up bilingual grammar induction, it is more important to compare more directly how well the relationships between the two languages are learned by the very many possible various combinations of bootstrapping between transduction expressivity levels, phrasal chunking, and category formation. Therefore, we focus more on the cross entropy of the induced bilingual grammars—which is exactly the bilingual form of the standard way to evaluate how well monolingual grammar induction captures monolingual data. Note that, as observed in Wu (1997), this way of analyzing bilingual grammar induction can also be viewed as the problem of bilingual language modeling—modeling two languages simultaneously. We attack this problem of bilingual language modeling by extending a surprisingly effective finite-state baseline in two different ways: adding reordering capabilities and adding context information in the form of categories. The ultimate aim of this research direction is to have a grammar-based end-to-end statistical machine translation system, which would enable the usage of the same model in training and decoding instead of relying on a"
C12-1142,2007.iwslt-1.1,0,\N,Missing
D07-1097,W07-2416,0,0.0143741,"es, extrapolating from the optimal parameters settings for each language. When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score. 1 Introduction In the multilingual track of the CoNLL 2007 shared task on dependency parsing, a single parser must be trained to handle data from ten different languages: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Catalan, (Mart´ı et al., 2007), Chinese (Chen et al., 2003), Czech (B¨ohmov´a et al., 2003), English (Marcus et al., 1993; Johansson and Nugues, 2007), Greek (Prokopidis et al., 2005), Hungarian (Csendes et al., 2005), Italian (Montemagni et al., 2003), and Turkish (Oflazer et al., 2003).1 Our contribution is a study in multilingual parser optimization using the freely available MaltParser system, which performs 1 For more information about the task and the data sets, see Nivre et al. (2007). deterministic, classifier-based parsing with historybased feature models and discriminative learning, and which was one of the top performing systems in the CoNLL 2006 shared task (Nivre et al., 2006). In order to maximize parsing accuracy, optimizatio"
D07-1097,J93-2004,0,0.0418663,"rent parsing strategies, extrapolating from the optimal parameters settings for each language. When evaluated on the official test sets, the ensemble system significantly outperforms the single-parser system and achieves the highest average labeled attachment score. 1 Introduction In the multilingual track of the CoNLL 2007 shared task on dependency parsing, a single parser must be trained to handle data from ten different languages: Arabic (Hajiˇc et al., 2004), Basque (Aduriz et al., 2003), Catalan, (Mart´ı et al., 2007), Chinese (Chen et al., 2003), Czech (B¨ohmov´a et al., 2003), English (Marcus et al., 1993; Johansson and Nugues, 2007), Greek (Prokopidis et al., 2005), Hungarian (Csendes et al., 2005), Italian (Montemagni et al., 2003), and Turkish (Oflazer et al., 2003).1 Our contribution is a study in multilingual parser optimization using the freely available MaltParser system, which performs 1 For more information about the task and the data sets, see Nivre et al. (2007). deterministic, classifier-based parsing with historybased feature models and discriminative learning, and which was one of the top performing systems in the CoNLL 2006 shared task (Nivre et al., 2006). In order to maximize"
D07-1097,N06-2033,0,0.410516,"the defining feature for the split (instead of POSTAG). With respect to the SVM parameters (γ, r, C, and ), Arabic, Basque, Catalan, Greek and Hungarian retain the baseline settings, while the other languages have slightly different values for some parameters. The cumulative improvement after optimization of feature model and learning algorithm parameters was 1.71 percentage points on average over all ten languages, with a minimum of 0.69 (Turkish) and a maximum of 3.25 (Chinese) (cf. table 1). 3 The Blended Parser The Blended parser is an ensemble system based on the methodology proposed by Sagae and Lavie (2006). Given the output dependency graphs Gi (1 ≤ i ≤ m) of m different parsers for an input sentence x, we construct a new graph containing all the labeled dependency arcs proposed by some parser and weight each arc a by a score s(a) reflecting its popularity among the m parsers. The output of the ensemble system for x is the maximum spanning tree of this graph (rooted at the node 0), which can be extracted using the Chu-Liu-Edmonds algorithm, as shown by McDonald et al. (2005). Following 936 c Sagae and Lavie (2006), we let s(a) = m i=1 wi ai , where wic is the average labeled attachment score of"
D07-1097,D07-1013,1,0.840676,"Missing"
D07-1097,W03-3023,0,0.235532,"Missing"
D07-1097,H05-1066,0,0.562194,"Missing"
D07-1097,P05-1013,1,0.292258,"Missing"
D07-1097,W06-2933,1,0.445728,"Missing"
D07-1097,N07-1050,1,0.691603,"ai , where wic is the average labeled attachment score of parser i for the word class c8 of the dependent of a, and ai is 1 if a ∈ Gi and 0 otherwise. The Blended parser uses six component parsers, with three different parsing algorithms, each of which is used to construct one left-to-right parser and one right-to-left parser. The parsing algorithms used are the arc-eager baseline algorithm, the arcstandard variant of the baseline algorithm, and the incremental, non-projective parsing algorithm first described by Covington (2001) and recently used for deterministic classifier-based parsing by Nivre (2007), all of which are available in MaltParser. Thus, the six component parsers for each language were instances of the following: P 1. Arc-eager projective left-to-right 2. Arc-eager projective right-to-left 3. Arc-standard projective left-to-right 4. Arc-standard projective right-to-left 5. Covington non-projective left-to-right 6. Covington non-projective right-to-left 8 We use CPOSTAG to determine the part of speech. root 1 2 3–6 7+ Parser R P R P R P R P R P Single Malt 87.01 80.36 95.08 94.87 86.28 86.67 77.97 80.23 68.98 71.06 Blended 92.09 74.20 95.71 94.92 87.55 88.12 78.66 83.02 65.29 78"
D07-1097,W07-2201,0,\N,Missing
D07-1097,W02-2016,0,\N,Missing
D07-1097,W06-2920,0,\N,Missing
D07-1097,H92-1026,0,\N,Missing
D07-1097,W07-2207,0,\N,Missing
D07-1097,W07-2202,0,\N,Missing
D07-1097,W05-1518,0,\N,Missing
D07-1097,P95-1037,0,\N,Missing
D07-1097,P06-2041,1,\N,Missing
D07-1097,D07-1096,1,\N,Missing
D07-1097,W03-3017,1,\N,Missing
D13-1011,W09-2006,0,0.195947,"Chinese couplets, which adhere to strict rules requiring, for example, an identical number of characters in each line and one-to-one correspondence in their metrical length, the domain of hip hop lyrics is far more unstructured and there exists no clear constraint that would ensure fluent and rhyming responses to hip hop challenge lyrics. Barbieri et al. (2012) use controlled Markov processes to semi-automatically generate lyrics that satisfy the structural constraints of rhyme and meter. 104 Tamil lyrics were automatically generated given a melody using conditional random fields by A. et al. (2009). The lyrics were represented as a sequence of labels using the KNM system where K, N and M represented the long vowels, short vowels and consonants respectively. Genzel et al. (2010) used SMT in conjunction with stress patterns and rhymes found in a pronunciation dictionary to produce translations of poems. Although many constraints were applied in translating full verses of poems, it was challenging to satisfy all the constraints. Stress patterns were assigned to words given the meter of a line in Shakespeare’s sonnets by Greene et al. (2010), which were then combined with a language model t"
D13-1011,J07-2003,0,0.187334,"Missing"
D13-1011,D10-1016,0,0.0135278,"gth, the domain of hip hop lyrics is far more unstructured and there exists no clear constraint that would ensure fluent and rhyming responses to hip hop challenge lyrics. Barbieri et al. (2012) use controlled Markov processes to semi-automatically generate lyrics that satisfy the structural constraints of rhyme and meter. 104 Tamil lyrics were automatically generated given a melody using conditional random fields by A. et al. (2009). The lyrics were represented as a sequence of labels using the KNM system where K, N and M represented the long vowels, short vowels and consonants respectively. Genzel et al. (2010) used SMT in conjunction with stress patterns and rhymes found in a pronunciation dictionary to produce translations of poems. Although many constraints were applied in translating full verses of poems, it was challenging to satisfy all the constraints. Stress patterns were assigned to words given the meter of a line in Shakespeare’s sonnets by Greene et al. (2010), which were then combined with a language model to generate poems. Sonderegger (2011) attempted to infer the pronunciation of words in old English by identifying the rhyming patterns using graph theory. However, their heuristic of c"
D13-1011,D10-1051,0,0.196029,"ted given a melody using conditional random fields by A. et al. (2009). The lyrics were represented as a sequence of labels using the KNM system where K, N and M represented the long vowels, short vowels and consonants respectively. Genzel et al. (2010) used SMT in conjunction with stress patterns and rhymes found in a pronunciation dictionary to produce translations of poems. Although many constraints were applied in translating full verses of poems, it was challenging to satisfy all the constraints. Stress patterns were assigned to words given the meter of a line in Shakespeare’s sonnets by Greene et al. (2010), which were then combined with a language model to generate poems. Sonderegger (2011) attempted to infer the pronunciation of words in old English by identifying the rhyming patterns using graph theory. However, their heuristic of clustering words with similar IPA endings resulted in large clusters of false positives such as bloom and numb. A language-independent generative model for stanzas in poetry was proposed by Reddy and Knight (2011) via which they could discover rhyme schemes in French and English poetry. 3 Experimental conditions Before introducing our Freestyle models, we first deta"
D13-1011,C08-1048,0,0.644324,"e structured domains such as poetry. However, in hip hop lyrics it is hard to make any linguistic or structural assumptions. For example, words such as sho, flo, holla which frequently appear in the lyrics are not part of any standard lexicon and hip hop does not require a set number of syllables in a line, unlike poems. Also, surprising and unlikely rhymes in hip hop are frequently achieved via intonation and assonance, making it hard to apply prior phonological constraints. A phrase based SMT system was trained to “translate” the first line of a Chinese couplet or duilian into the second by Jiang and Zhou (2008). The most suitable next line was selected by applying linguistic constraints to the n best output of the SMT system. However in contrast to Chinese couplets, which adhere to strict rules requiring, for example, an identical number of characters in each line and one-to-one correspondence in their metrical length, the domain of hip hop lyrics is far more unstructured and there exists no clear constraint that would ensure fluent and rhyming responses to hip hop challenge lyrics. Barbieri et al. (2012) use controlled Markov processes to semi-automatically generate lyrics that satisfy the structur"
D13-1011,P07-2045,0,0.00599273,"op listeners for manual evaluation. They were asked to evaluate the system outputs according to fluency and the degree of rhyming. They were free to choose the tune to make the lyrics rhyme as the beats of the song were not used in the training data. Each evaluator was asked to score the response of each system on the criterion of fluency and rhyming as being good, acceptable or bad. 3.3 Phrase-based SMT baseline In order to evaluate the performance of an out-ofthe-box phrase-based SMT (PBSMT) system toward this novel task of generating rhyming and fluent responses, a standard Moses baseline (Koehn et al., 2007) was also trained in order to compare its performance with our transduction grammar induction model. A 4-gram language model which was trained on the entire training corpus using SRILM (Stolcke, 2002) was used to generate responses in conjunction with the phrase-based translation model. As no automatic quality evaluation metrics exist for hip hop responses analogous to BLEU for SMT, the model weights cannot be tuned in conventional ways such as MERT (Och, 2003). Instead, a slightly higher than typical language model weight was empirically chosen using a small development set to produce fluent"
D13-1011,P03-1021,0,0.00591131,"phrase-based SMT (PBSMT) system toward this novel task of generating rhyming and fluent responses, a standard Moses baseline (Koehn et al., 2007) was also trained in order to compare its performance with our transduction grammar induction model. A 4-gram language model which was trained on the entire training corpus using SRILM (Stolcke, 2002) was used to generate responses in conjunction with the phrase-based translation model. As no automatic quality evaluation metrics exist for hip hop responses analogous to BLEU for SMT, the model weights cannot be tuned in conventional ways such as MERT (Och, 2003). Instead, a slightly higher than typical language model weight was empirically chosen using a small development set to produce fluent outputs. 4 Interpolated segmenting model vs. token based model We compare the performance of transduction grammars induced via interpolated token based and rule segmenting (ISTG) versus token based transduction grammars (TG) on the task of generating a rhyming and fluent response to hip hop challenges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model"
D13-1011,P02-1040,0,0.0904368,"h rule in a segmental ITG grammar can contain more than one token in both input and output languages. In machine translation applications, segmental models produce translations that are more fluent as they can capture lexical knowledge at a phrasal level. However, only a handful of purely unsupervised algorithms exist for learning segmental ITGs under matched training and testing assumptions. Most other approaches in SMT use a variety of ad hoc heuristics for extracting segments from token alignments, justified purely by short term improvements in automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) which cannot be transferred to our current task. Instead, we use a completely unsupervised learning algorithm for segmental ITGs that stays strictly within the transduction grammar optimization framework for both training and testing as proposed in Saers et al. (2013). Saers et al. (2013) induce a phrasal inversion transduction grammar via interpolating the bottomup rule chunking approach proposed in Saers et al. (2012) with a top-down rule segmenting approach driven by a minimum description length objective function (Solomonoff, 1959; Rissanen, 1983) that trades off the maximum likelihood ag"
D13-1011,P11-2014,0,0.514818,"ses of poems, it was challenging to satisfy all the constraints. Stress patterns were assigned to words given the meter of a line in Shakespeare’s sonnets by Greene et al. (2010), which were then combined with a language model to generate poems. Sonderegger (2011) attempted to infer the pronunciation of words in old English by identifying the rhyming patterns using graph theory. However, their heuristic of clustering words with similar IPA endings resulted in large clusters of false positives such as bloom and numb. A language-independent generative model for stanzas in poetry was proposed by Reddy and Knight (2011) via which they could discover rhyme schemes in French and English poetry. 3 Experimental conditions Before introducing our Freestyle models, we first detail our experimental assumptions and the evaluation scheme under which the responses generated by different models are compared against one another. We describe our training data as well as a phrasebased SMT (PBSMT) contrastive baseline. We also define the evaluation scheme used to compare the responses of different systems on criteria of fluency and rhyming. 3.1 Training data We used freely available user generated hip hop lyrics on the Inte"
D13-1011,C12-1142,1,0.908351,"ges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framework for our modeling needs. Examples of lexical transduction rules can be seen in Tables 3 and 5. In addition, the grammar also includes structural transduction rules for the straight case A → [A A] and also the inverted case A → &lt;A A>. 4.1 Token based vs. segmental ITGs The degenerate case of ITGs are token based ITGs wherein each translation rule contains at most one token in input and output languages. Efficient induction algorithms with polynomial run time exist for token"
D13-1011,W13-0806,1,0.83837,"rely unsupervised algorithms exist for learning segmental ITGs under matched training and testing assumptions. Most other approaches in SMT use a variety of ad hoc heuristics for extracting segments from token alignments, justified purely by short term improvements in automatic MT evaluation metrics such as BLEU (Papineni et al., 2002) which cannot be transferred to our current task. Instead, we use a completely unsupervised learning algorithm for segmental ITGs that stays strictly within the transduction grammar optimization framework for both training and testing as proposed in Saers et al. (2013). Saers et al. (2013) induce a phrasal inversion transduction grammar via interpolating the bottomup rule chunking approach proposed in Saers et al. (2012) with a top-down rule segmenting approach driven by a minimum description length objective function (Solomonoff, 1959; Rissanen, 1983) that trades off the maximum likelihood against model size. Saers et al. (2013) report improvements in BLEU score (Papineni et al., 2002) on their translation task. In our current approach instead of using a bottom-up rule chunking approach we use a simpler token based grammar instead. Given two grammars (Ga a"
D13-1011,W11-1008,1,0.818598,"ework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framework for our modeling needs. Examples of lexical transduction rules can be seen in Tables 3 and 5. In addition, the grammar also includes structural transduction rules for the straight case A → [A A] and also the inverted case A → &lt;A A>. 4.1 Token based vs. segmental ITGs The degenerate case of ITGs are token based ITGs wherein each translation rule contains at most one token in input and output languages. Efficient induction algorithms with polynomial run time exist for token based ITGs and the e"
D13-1011,P96-1021,1,0.501234,"t address that problem in our current model. We also penalize singleton rules to produce responses of similar length as successive lines in a stanza are typically of similar length. Finally, we add a penalty to reflexive translation rules that map the same surface form to itself such as A → yo/yo. We obtain these rules with a high probability due to the presence of sentence pairs where both the input and output are identical strings as many stanzas in our data contain repeated chorus lines. Decoding heuristics We use our in-house ITG decoder implemented according to the algorithm mentioned in Wu (1996) for the generating responses to challenges by decoding with the trained transduction grammars. The decoder uses a CKY-style parsing algorithm (Cocke, 106 4.3 Results: Rule segmentation improves responses Results in Table 1 indicate that the ISTG outperforms the TG model towards the task of generating fluent and rhyming responses. On the criterion of fluency, Table 1: Percentage of ≥good and ≥acceptable (i.e., either good or acceptable) responses on fluency and rhyming criteria. PBSMT, TG and ISTG models trained using corpus generated from all adjacent lines in a verse. PBSMT+RS, TG+RS, ISTG+R"
D13-1011,J97-3002,1,0.198613,"ional ways such as MERT (Och, 2003). Instead, a slightly higher than typical language model weight was empirically chosen using a small development set to produce fluent outputs. 4 Interpolated segmenting model vs. token based model We compare the performance of transduction grammars induced via interpolated token based and rule segmenting (ISTG) versus token based transduction grammars (TG) on the task of generating a rhyming and fluent response to hip hop challenges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framewor"
D13-1011,2013.mtsummit-papers.14,1,0.814366,"Missing"
D13-1011,I05-1023,1,0.782388,"rsus token based transduction grammars (TG) on the task of generating a rhyming and fluent response to hip hop challenges. We use the framework of stochastic transduction grammars, specifically bracketing ITGs (inversion transduction 105 grammars) (Wu, 1997), as our translation model for “transducing” any given challenge into a rhyming and fluent response. Our choice is motivated by the significant amount of empirical evidence for the representational capacity of transduction grammars across a spectrum of natural language tasks such as textual entailment (Wu, 2006), mining parallel sentences (Wu and Fung, 2005) and machine translation (Zens and Ney, 2003). Further, existence of efficient learning algorithms (Saers et al., 2012; Saers and Wu, 2011) that make no language specific assumptions, make inversion transduction grammars a suitable framework for our modeling needs. Examples of lexical transduction rules can be seen in Tables 3 and 5. In addition, the grammar also includes structural transduction rules for the straight case A → [A A] and also the inverted case A → &lt;A A>. 4.1 Token based vs. segmental ITGs The degenerate case of ITGs are token based ITGs wherein each translation rule contains at"
D13-1011,P03-1019,0,\N,Missing
I13-1165,N10-1028,0,0.0806238,"on for this line of inquiry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by the many unrelated variables in the subsequent process"
I13-1165,P09-1088,0,0.239881,"Missing"
I13-1165,N10-1015,0,0.0136058,"ry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by the many unrelated variables in the subsequent processing pipeline. Our own p"
I13-1165,P95-1031,0,0.0367051,", any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. The motivation for our present series of experiments is that as a field we are well served by tackling the fundamental questions as well, and not exclusively focusing on engineering short term incremental BLEU score boosts where the quality of an induced ITG itself is obscured because it is embedded within many other heuristic algorithms. Bayesian approaches to grammar induction have a long history in computation linguistics. Starting with monolingual grammar induction (Chen, 1995; Stolcke and Omohundro, 1994), and moving on to transduction grammar induction (Blunsom et al., 2008, 2009; Blunsom and Cohn, 2010; Neubig et al., 2011, 2012). So far, the induced transduction grammar have only been used to derive Viterbi-style word alignments to feed into existing translation system, and there has been no evaluation of the grammars actually learned. In contrast, we directly evaluate the grammars that we induce. Our algorithm for learning the structure of an ITG relies on segmenting known bilingual segments, starting with the sentence pairs of the training data, and continuin"
I13-1165,W07-0403,0,0.021434,"lt into the formalism (although we will not make use of this feature in this work). A word as to the bigger-picture motivation for this line of inquiry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the"
I13-1165,P05-1033,0,0.0344124,"ent estimation of the model prior and the likelihood of the data given a limited change in the model—it is, in other words, possible to generate a set of possible rule segmentations and compare them using the model posteriors or description length. Our new approach differs from most other SMT approaches to unsupervised learning of phrasal translations, which (a) require enormous amounts of run-time memory, (b) contain a high degree of redundancy, and (c) do not provide an obvious basis for generalization to abstract translation schemas. The current state-of-the-art in SMT (Koehn et al., 2003; Chiang, 2005) relies on long pipelines of mismatched learning models and heuristics. There is no way for latter stages of the pipeline to recover a mistake of ommission made in an earlier stage, which forces the individual steps to massively overgenerate hypotheses. This typically manifests as massive redundancy in the phrasal lexicon, which causes significant overhead at run-time. The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates these deficiencies well. By s"
I13-1165,J07-2003,0,0.133658,"007), which contains 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool which tries to clump characters together into more “wordlike” sequences (Wu, 1999). After each induction iteration there is a fullyfunctional grammar that we can test as a translation system. For this, we use our in-house ITG decoder, which uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) with cube pruning (Chiang, 2007) to integrate the language model scores. We use SRILM (Stolcke, 2002) to train a trigram language model on the English side of the training data. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002), and NIST (Doddington, 2002), and compare the results against our bottom-up oriented chunking ITG induction approach (Saers et al., 2012). 1164 50 40 30 20 10 0 0 1 2 3 4 Iterations 5 6 7 (a) Rule count Probability in log domain (Mbit) Number of rules (thousands) 60 Table 1: Translation results of the baseline, the initial model and the model after n interations. 14 12 10 Sys"
I13-1165,P06-1121,0,0.0143135,"inuing with the segments learned in this way. This is similar to the Recursive Alignment Model, or MAR (Vilar, 2005; Vilar and Vidal, 2005). Our method is, however, learning a full ITG, where MAR only learns a translation lexicon; furthermore, MAR is a discriminative model, whereas ours is a generative. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints—taking it from the realm of unsupervised induction into the realm of supervised induction. This approach was pioneered by Galley et al. (2006) with numerous variants in subsequent research, usually referred to as tree-to-tree, tree-to-string and string-to-tree, depending on where the analyses are found in the training data. Our view on this line of research is that it complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. The work presented in this paper is related to our preliminary"
I13-1165,P09-1104,0,0.0190238,"his work). A word as to the bigger-picture motivation for this line of inquiry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by th"
I13-1165,D07-1103,0,0.0150731,"rent state-of-the-art in SMT (Koehn et al., 2003; Chiang, 2005) relies on long pipelines of mismatched learning models and heuristics. There is no way for latter stages of the pipeline to recover a mistake of ommission made in an earlier stage, which forces the individual steps to massively overgenerate hypotheses. This typically manifests as massive redundancy in the phrasal lexicon, which causes significant overhead at run-time. The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates these deficiencies well. By staying within a single framework throughout training and testing, we do not have to overgenerate hypotheses—instead, we are able to evaluate their effect on the posterior model probability at the time they are proposed during learning. This cuts down the size of the phrasal lexicon significantly, and consequently saves the decoder a lot of run-time resources. The fact that we learn a phrasal inversion transduction grammar, or ITG (Wu, 1997) also means that the power to generalize and abstract over categories is built into the formalism (although we wil"
I13-1165,N03-1017,0,0.0348063,"Missing"
I13-1165,P12-1018,0,0.0227958,"Missing"
I13-1165,P11-1064,0,0.0257227,"Missing"
I13-1165,P02-1040,0,0.0887694,"nese is written without whitespace, we use a tool which tries to clump characters together into more “wordlike” sequences (Wu, 1999). After each induction iteration there is a fullyfunctional grammar that we can test as a translation system. For this, we use our in-house ITG decoder, which uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) with cube pruning (Chiang, 2007) to integrate the language model scores. We use SRILM (Stolcke, 2002) to train a trigram language model on the English side of the training data. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002), and NIST (Doddington, 2002), and compare the results against our bottom-up oriented chunking ITG induction approach (Saers et al., 2012). 1164 50 40 30 20 10 0 0 1 2 3 4 Iterations 5 6 7 (a) Rule count Probability in log domain (Mbit) Number of rules (thousands) 60 Table 1: Translation results of the baseline, the initial model and the model after n interations. 14 12 10 System baseline initial iteration 1 iteration 2 iteration 3 iteration 4 iteration 5 iteration 6 iteration 7 8 6 4 2 0 0 1 2 3 4 Iterations 5 6 7 (b) Impact of model structure Figure 1: Number of rules (a), and the impact of"
I13-1165,P10-1017,0,0.0181855,"insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by the many unrelated variables in the subsequent processing pipeline. Our own past work has also taken"
I13-1165,C12-1142,1,0.852343,"ach induction iteration there is a fullyfunctional grammar that we can test as a translation system. For this, we use our in-house ITG decoder, which uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) with cube pruning (Chiang, 2007) to integrate the language model scores. We use SRILM (Stolcke, 2002) to train a trigram language model on the English side of the training data. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002), and NIST (Doddington, 2002), and compare the results against our bottom-up oriented chunking ITG induction approach (Saers et al., 2012). 1164 50 40 30 20 10 0 0 1 2 3 4 Iterations 5 6 7 (a) Rule count Probability in log domain (Mbit) Number of rules (thousands) 60 Table 1: Translation results of the baseline, the initial model and the model after n interations. 14 12 10 System baseline initial iteration 1 iteration 2 iteration 3 iteration 4 iteration 5 iteration 6 iteration 7 8 6 4 2 0 0 1 2 3 4 Iterations 5 6 7 (b) Impact of model structure Figure 1: Number of rules (a), and the impact of changes in the model structure (b) during the structure induction phase. The change in model structure is broken down into the model prior"
I13-1165,W13-0806,1,0.631774,"lly referred to as tree-to-tree, tree-to-string and string-to-tree, depending on where the analyses are found in the training data. Our view on this line of research is that it complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. The work presented in this paper is related to our preliminary work with description length as learning objective (Saers et al., 2013b). A key difference lies in the added parameter training, which facilitates a completely Bayesian interpretation. The present paper aims to be self-contained, by explaining the relationships throughout. 2 Background In this section we briefly survey essential foundations for inversion transduction grammars and description length together with its Bayesian interpretation—in other words, what we search for, and how. 2.1 Inversion transduction grammars Inversion transduction grammars, or ITGs (Wu, 1997), are an expressive yet efficient way to model translation. Much like context-free grammars (C"
I13-1165,W13-2810,1,0.317916,"Missing"
I13-1165,N10-1050,1,0.833847,"ental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Although this allows us to tap into the vast engineering efforts that have gone into tweaking existing decoders, it also prevents us from understanding the quality of the learned transduction grammar, whose characteristics become obscured by the many unrelated variables in the subsequent processing pipeline. Our own past work has also taken similar approaches, b"
I13-1165,2011.eamt-1.42,1,0.868685,"Missing"
I13-1165,W05-0815,0,0.0175858,"08, 2009; Blunsom and Cohn, 2010; Neubig et al., 2011, 2012). So far, the induced transduction grammar have only been used to derive Viterbi-style word alignments to feed into existing translation system, and there has been no evaluation of the grammars actually learned. In contrast, we directly evaluate the grammars that we induce. Our algorithm for learning the structure of an ITG relies on segmenting known bilingual segments, starting with the sentence pairs of the training data, and continuing with the segments learned in this way. This is similar to the Recursive Alignment Model, or MAR (Vilar, 2005; Vilar and Vidal, 2005). Our method is, however, learning a full ITG, where MAR only learns a translation lexicon; furthermore, MAR is a discriminative model, whereas ours is a generative. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints—taking it from the realm of unsupervised induction into the realm of supervised induction. This approach was pioneered by Galley et al. (2006) with numerous variants in subsequent research, usually referred to as tree-to-tree, tree-to-st"
I13-1165,W05-0835,0,0.0312758,"nsom and Cohn, 2010; Neubig et al., 2011, 2012). So far, the induced transduction grammar have only been used to derive Viterbi-style word alignments to feed into existing translation system, and there has been no evaluation of the grammars actually learned. In contrast, we directly evaluate the grammars that we induce. Our algorithm for learning the structure of an ITG relies on segmenting known bilingual segments, starting with the sentence pairs of the training data, and continuing with the segments learned in this way. This is similar to the Recursive Alignment Model, or MAR (Vilar, 2005; Vilar and Vidal, 2005). Our method is, however, learning a full ITG, where MAR only learns a translation lexicon; furthermore, MAR is a discriminative model, whereas ours is a generative. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints—taking it from the realm of unsupervised induction into the realm of supervised induction. This approach was pioneered by Galley et al. (2006) with numerous variants in subsequent research, usually referred to as tree-to-tree, tree-to-string and string-to-tree,"
I13-1165,W95-0106,1,0.598849,"complete model P (D|ΦG , ΦS , θΦ )). The latter adjusts the model parameters θΦ to optimize the posterior (thus affecting the prior over the parameters P (θΦ |ΦS , ΦG ) and again P (D|ΦG , ΦS , θΦ )), assuming the model structure ΦS to be fixed, as well as ΦG which remains fixed as bracketing inversion transduction grammars. The prior is a symmetric Dirichlet distribution over rule right-hand sides given rule left-hand sides. To get the conditional, we have to biparse the training data, and to maximize it, we perform expectation maximization (Dempster et al., 1977), as specified for ITGs by (Wu, 1995) with the caveat that we increase all the fractional counts by one before normalizing. The biparsing is done with our in-house implementation of the cubic time biparsing algorithm described in Saers et al. (2009), with a beam width of 100. 5 Experimental setup To test the viability of the idea of starting with a very specific ITG consisting of long rules, and iteratively segmenting the rules to induce a more general ITG under a MAP or MDL objective, we have implemented the steps detailed in Sections 3 and 4; in this section we will describe in greater detail the exact experimental conditions o"
I13-1165,J97-3002,1,0.821419,"a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates these deficiencies well. By staying within a single framework throughout training and testing, we do not have to overgenerate hypotheses—instead, we are able to evaluate their effect on the posterior model probability at the time they are proposed during learning. This cuts down the size of the phrasal lexicon significantly, and consequently saves the decoder a lot of run-time resources. The fact that we learn a phrasal inversion transduction grammar, or ITG (Wu, 1997) also means that the power to generalize and abstract over categories is built into the formalism (although we will not make use of this feature in this work). A word as to the bigger-picture motivation for this line of inquiry may be necessary. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we accept forfeiting the short term boost 1158 International Joint Conference on Natural Language Processing, pages 1158–1166, Nagoya, Japan, 14-18 October 2013. in BLEU that is typically seen when embedding a learned ITG in the midst of the"
I13-1165,W09-3804,1,\N,Missing
I13-1165,2007.iwslt-1.1,0,\N,Missing
I13-1165,W09-2304,1,\N,Missing
I13-1165,P08-1012,0,\N,Missing
N10-1050,J93-2003,0,0.072497,"e a word alignment over a parallel corpus. We show that alignment via Stochastic Bracketing LITGs is considerably faster than Stochastic Bracketing ITGs, while still yielding alignments superior to the widelyused heuristic of intersecting bidirectional IBM alignments. Performance is measured as the translation quality of a phrase-based machine translation system built upon the word alignments, and an improvement of 2.85 BLEU points over baseline is noted for French– English. 1 Introduction Machine translation relies heavily on word alignments, which are usually produced by training IBMmodels (Brown et al., 1993) in both directions and combining the resulting alignments via some heuristic. Automatically training an Inversion Transduction Grammar ( ITG) has been suggested as a viable way of producing superior alignments (Saers and Wu, 2009). The main problem of using Bracketing ITGs for alignment is that exhaustive biparsing runs in O(n6 ) time. Several ways to lower the complexity of ITGs has been suggested, but in this paper, a different approach is taken. Instead of using full ITGs, we explore the possibility of subjecting the grammar to a linear constraint, making exhaustive biparsing of a sentence"
N10-1050,P09-1104,0,0.256675,"Missing"
N10-1050,P07-2045,0,0.0192853,"m in a bucket can be analyzed in 8 possible ways, requiring O(1) time. In summary, we have: O(n) × O(n3 ) × O(1) = O(n4 ) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3 ) to O(b), where b is the beam width. This gives time complexity O(n) × O(b) × O(1) = O(bn). 343 We used the guidelines of the shared task of WMT ’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA ++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA ++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT ’08 shared task, but limited the training set to sentence pairs where both sentences were of length 2"
N10-1050,J03-1002,0,0.0176392,"aking the total number of items in a bucket O(n3 ). Each item in a bucket can be analyzed in 8 possible ways, requiring O(1) time. In summary, we have: O(n) × O(n3 ) × O(1) = O(n4 ) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3 ) to O(b), where b is the beam width. This gives time complexity O(n) × O(b) × O(1) = O(bn). 343 We used the guidelines of the shared task of WMT ’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA ++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA ++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT ’08 shared task, but limited the trainin"
N10-1050,P03-1021,0,0.0182863,"ime. In summary, we have: O(n) × O(n3 ) × O(1) = O(n4 ) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3 ) to O(b), where b is the beam width. This gives time complexity O(n) × O(b) × O(1) = O(bn). 343 We used the guidelines of the shared task of WMT ’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA ++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA ++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT ’08 shared task, but limited the training set to sentence pairs where both sentences were of length 20 or less. This was necessary in order to carry out exhau"
N10-1050,P02-1040,0,0.0775543,"Missing"
N10-1050,W09-2304,1,0.850266,"secting bidirectional IBM alignments. Performance is measured as the translation quality of a phrase-based machine translation system built upon the word alignments, and an improvement of 2.85 BLEU points over baseline is noted for French– English. 1 Introduction Machine translation relies heavily on word alignments, which are usually produced by training IBMmodels (Brown et al., 1993) in both directions and combining the resulting alignments via some heuristic. Automatically training an Inversion Transduction Grammar ( ITG) has been suggested as a viable way of producing superior alignments (Saers and Wu, 2009). The main problem of using Bracketing ITGs for alignment is that exhaustive biparsing runs in O(n6 ) time. Several ways to lower the complexity of ITGs has been suggested, but in this paper, a different approach is taken. Instead of using full ITGs, we explore the possibility of subjecting the grammar to a linear constraint, making exhaustive biparsing of a sentence pair in O(n4 ) time possible. This can be further improved by applying pruning. A transduction is the bilingual version of a language. A language (Ll ) can be formally viewed as a set of sentences, sequences of tokens taken from a"
N10-1050,W09-3804,1,0.78707,"Missing"
N10-1050,P95-1033,1,0.573616,"one as transduction, that is: a sentence in one language is rewritten into the other language. In NLP, interest has shifted away from hand-crafted grammars, towards stochastic grammars induced from corpora. To induce a stochastic grammar from a parallel corpus, expectations of all possible parses over a sentence pair are typically needed. S TGs can biparse sentence pairs in polynomial time, but are unable to account for the complexities typically found in natural languages. S DTGs do account for the complexities in natural languages, but are intractable for biparsing. Inversion transductions (Wu, 1995; Wu, 1997) are a special case of transductions that are not monotone, but where permutations are severely limited. By limiting the possible permutations, biparsing becomes tractable. This in turn means that ITGs can be induced from parallel corpora in polynomial time, 341 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341–344, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics as well as account for most of the reorderings found between natural languages. An Inversion transduction is limited so that it"
N10-1050,J97-3002,1,0.906411,"nsduction, that is: a sentence in one language is rewritten into the other language. In NLP, interest has shifted away from hand-crafted grammars, towards stochastic grammars induced from corpora. To induce a stochastic grammar from a parallel corpus, expectations of all possible parses over a sentence pair are typically needed. S TGs can biparse sentence pairs in polynomial time, but are unable to account for the complexities typically found in natural languages. S DTGs do account for the complexities in natural languages, but are intractable for biparsing. Inversion transductions (Wu, 1995; Wu, 1997) are a special case of transductions that are not monotone, but where permutations are severely limited. By limiting the possible permutations, biparsing becomes tractable. This in turn means that ITGs can be induced from parallel corpora in polynomial time, 341 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341–344, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics as well as account for most of the reorderings found between natural languages. An Inversion transduction is limited so that it must be exp"
N10-1050,P08-1012,0,0.537157,"Missing"
P13-2067,2006.iwslt-evaluation.11,0,0.0718925,"c metrics below to justify our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorpo"
P13-2067,W05-0909,0,0.275982,"MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) does not sufficiently drive SMT into making decisions to produce adequate translations that correctly preserve ”who did what to whom, In contrast to TINE, MEANT (Lo et al., 2012), which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. This makes it more suitable for tuning SMT systems to produce much adequate translations. Although TINE (Rios et al., 2011) is an recalloriented automatic evaluation metric which aims to preserve the basic event structure, no work has be"
P13-2067,E06-1032,0,0.770131,"ein automatic semantic parsing might be expected to fare worse. These results strongly indicate that using a semantic frame based objective function for tuning would drive development of MT towards direction of higher utility. Glaring errors caused by semantic role confusion that plague the state-of-the-art MT systems are a consequence of using fast and cheap lexical n-gram based objective functions like BLEU to drive their development. Despite enforcing fluency it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). We argue that instead of BLEU, a metric that focuses on getting the meaning right should be used as an objective function for tuning SMT so as to drive continuing progress towards higher utility. MEANT (Lo et al., 2012), is an automatic semantic MT evaluation metric that measures similarity between the MT output and the reference translation via semantic frames. It correlates better with human adequacy judgment than other automatic MT evaluation metrics. Since a high MEANT score is contingent on correct lexical choices as well as syntactic and semantic structures, we b"
P13-2067,E06-1031,0,0.691061,"of our system with that of two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SM"
P13-2067,W07-0718,0,0.470422,"Missing"
P13-2067,C10-1081,0,0.0750956,"ncorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Bu"
P13-2067,W08-0309,0,0.411594,"Missing"
P13-2067,niessen-etal-2000-evaluation,0,0.790958,"two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SMT system’s decisions to pro"
P13-2067,P02-1040,0,0.107244,"àrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) does not sufficiently drive SMT into making decisions to produce adequate translations that correctly preserve ”who did what to whom, In contrast to TINE, MEANT (Lo et al., 2012), which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. This makes it more suitable for tuning SMT systems to produce much adequate translations. Although TINE (Rios et al., 2011) is an recalloriented automatic evaluation metric whic"
P13-2067,W07-0738,0,0.382895,"he model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations"
P13-2067,W08-0332,0,0.421386,"ystem. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et a"
P13-2067,2006.amta-papers.25,0,0.252688,"against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SMT system’s decisions to produce adequate translations. Alt"
P13-2067,N09-2004,1,0.891593,"ranslations. We argue that an SMT system tuned against an adequacy-oriented metric that correlates well with human adequacy judgement produces more adequate translations. For this purpose, we choose MEANT, an automatic semantic MT evaluation metric that focuses on getting the meaning right by comparing the semantic structures of the MT output and the reference. We briefly describe some of the alternative semantic metrics below to justify our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train"
P13-2067,I11-1004,0,0.0218854,"our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semanti"
P13-2067,P12-1095,0,0.0228147,"t reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008)"
P13-2067,W06-3114,0,\N,Missing
P13-2067,W11-2112,0,\N,Missing
P13-2067,W12-3129,1,\N,Missing
P13-2067,W11-2136,0,\N,Missing
P13-2067,N04-1030,0,\N,Missing
P14-2124,2011.mtsummit-papers.52,0,0.078029,"Missing"
P14-2124,W12-3108,0,0.0431379,"an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that"
P14-2124,W06-3114,0,0.0405831,"T assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it pe"
P14-2124,W05-0909,0,0.0845738,"s), pages 765–771, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency s"
P14-2124,E06-1031,0,0.689315,"ces and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human"
P14-2124,W05-0904,0,0.0689774,"R (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive"
P14-2124,C04-1046,0,0.0699212,"equate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et"
P14-2124,P11-1023,1,0.950242,"echnology Center Department of Computer Science and Engineering Hong Kong University of Science and Technology {jackielo|mbeloucif|masaers|dekai}@cs.ust.hk Abstract than that of the reference translation, and on the other hand, the BITG constraints the word alignment more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metri"
P14-2124,E06-1032,0,0.0512033,"ing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic ev"
P14-2124,W07-0718,0,0.0464491,"fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: M"
P14-2124,W12-4206,1,0.863115,"the MT out766 Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic shallow semantic parser. The reference and MT output are parsed by an English automatic shallow semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate. put. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two diff"
P14-2124,W08-0309,0,0.0729217,"Missing"
P14-2124,2013.mtsummit-papers.12,1,0.927794,"that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of in"
P14-2124,W13-2254,1,0.745244,"that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of in"
P14-2124,W12-3103,0,0.0176841,"event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: Monolingual MEANT algorithm. or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle. 2.2 The MEANT family of metrics MEANT (Lo et al., 2012), which is the weighted fscore over the matched semantic role labels of the automatically aligned semantic"
P14-2124,W12-3129,1,0.91538,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,2013.iwslt-evaluation.5,1,0.868047,"anslations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences. We therefore propose XMEANT, a cross-lingual MT evaluation metric, that modifies MEANT using (1) simple translation probabilities (in our experiments, We introduce XMEANT—a new cross-lingual version of the semantic frame based MT evaluation metric MEANT—which can correl"
P14-2124,W13-2202,0,0.0143653,"one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences. We th"
P14-2124,W12-3122,0,0.0699178,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,niessen-etal-2000-evaluation,0,0.245663,"14. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005),"
P14-2124,W07-0411,0,0.159197,"Missing"
P14-2124,P02-1040,0,0.103731,"he Association for Computational Linguistics (Short Papers), pages 765–771, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG"
P14-2124,N04-1030,0,0.0990837,"nt more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is"
P14-2124,quirk-2004-training,0,0.0654216,"two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the pr"
P14-2124,W11-2112,0,0.0746507,"ce. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the referen"
P14-2124,W09-2304,1,0.953515,"ully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more closely with human translation adequacy judgm"
P14-2124,W09-3804,1,0.872735,"dy demonstrate XMEANT’s potential to drive research progress toward semantic SMT. max(|ei,j |,|fi,j |) where G is a bracketing ITG, whose only nonterminal is A, and where R is a set of transduction rules where e ∈ W 0 ∪ {ϵ} is an output token (or the null token), and f ∈ W 1 ∪ {ϵ} is an input token (or the null token). The rule probability function p is defined using fixed probabilities for the structural rules, and a translation table t trained using IBM model 1 in both directions. To calculate (the inside probability of a pair of seg) ∗ ments, P A ⇒ e/f|G , we use the algorithm described in Saers et al. (2009). si,pred and si,j are the length normalized BITG parsing probabilities of the predicates and role fillers of the arguments of type j between the input and the MT output. 4 Kendall 0.53 0.51 0.48 0.46 0.46 0.29 0.20 0.12 0.10 Results Table 1 shows that for human adequacy judgments at the sentence level, the f-score based XMEANT (1) correlates significantly more closely than other commonly used monolingual automatic MT evaluation metrics, and (2) even correlates nearly as well as monolingual MEANT. This suggests that the semantic structure of the MT output is indeed closer to that of the input"
P14-2124,2013.mtsummit-papers.21,0,0.0112627,"m of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more rec"
P14-2124,2006.amta-papers.25,0,0.117577,"ational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher corr"
P14-2124,2011.eamt-1.12,0,0.024364,"E system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et"
P14-2124,Y12-1062,1,0.912362,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,H05-1096,0,0.0325066,"s UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small anno"
P14-2124,W12-3107,0,0.0199386,"correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: Monolingual MEANT algorithm. or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle. 2.2"
P14-2124,J97-3002,1,0.352751,"that MEANT is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more clos"
P14-2124,P10-1062,0,0.0203401,"rast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of"
P14-2124,P03-1019,0,0.0342442,"T is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more closely with human trans"
P14-2124,J93-2003,0,\N,Missing
P14-2124,H93-1040,0,\N,Missing
P14-2124,1993.mtsummit-1.24,0,\N,Missing
P14-2124,W07-0738,0,\N,Missing
P14-2124,W08-0332,0,\N,Missing
P14-2124,P11-1124,0,\N,Missing
P14-2124,P13-2067,1,\N,Missing
P14-2124,2012.eamt-1.64,1,\N,Missing
R11-1092,W10-3802,1,0.894696,"Missing"
R11-1092,N10-1050,1,0.873507,"Missing"
R11-1092,J97-3002,1,\N,Missing
R13-1077,N10-1028,0,0.0128659,"time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard."
R13-1077,P09-1088,0,0.276814,"Missing"
R13-1077,P09-1104,0,0.0203367,"actually rely on generating one output token at a time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-"
R13-1077,J93-2003,0,0.0336153,"Missing"
R13-1077,N10-1015,0,0.0130487,"input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct transla"
R13-1077,N03-1017,0,0.0239069,"ernal to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct translation systems rely heavily on the language model to compensate for the mistakes they make, as well as relying on a fine-tuned log-linear combination of several features to choose which lexical units to use. Pinning down exactly wh"
R13-1077,W07-0403,0,0.0208788,"3; Vogel et al., 1996), but since no translation systems in use today actually rely on generating one output token at a time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned"
R13-1077,P11-1064,0,0.0260965,"Missing"
R13-1077,N09-1025,0,0.013534,"o inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct translation systems rely heavily on the language model to compensate for the mistakes they make, as well as relying on a fine-tuned log-linear combination of several features to choose which lexical units to use. Pinning down exactly where and why an error occurred in this setup is very hard. The transduction grammar based approach is better in this respect, but the state-of-the-art typically relies on massive amounts, tens of thousands (Chiang et al., 2009), of features. As a community, we still have no clear idea of why these features help translation, only that they do when the whole system pipeline is treated as a black box, but treating the system as a black box prevents effective error analysis. The state-of-the-art systems also relies on long and complicated learning pipelines that form ad-hoc models of how translation happens. These adhoc models differ significantly from the models of how translation happens that are used during actual translation, which violates the basic machine learning assumption that the same model should When parse"
R13-1077,P12-1018,0,0.0252622,"Missing"
R13-1077,P05-1033,0,0.0604327,"testing. In contrast, the only difference between biparsing with ITGs (training) and decoding (testing) is that both sentences are given during biparsing, but only the input sentence during decoding—the model itself does not change, only the way it is used. The space of possible ITG structures is intractably large, and there have been many attempts to introduce external constraints to guide the search. We do completely unsupervised search without introducing such constraints, which limits the scope of error analysis to the search strategy. Popular external constraints include word alignments (Chiang, 2005) and parse trees. Word alignments are typically learned as a many-to-one function from one language into the other language (Brown et al., 1993; Vogel et al., 1996), but since no translation systems in use today actually rely on generating one output token at a time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are"
R13-1077,P02-1040,0,0.0998753,"89 Chinese–English translation task from IWSLT07 (Fordyce, 2007) as training and test data. In contains 46,867 sentence pairs of training data, and 489 sentence pairs of test data with 6 reference translations each. To decode with the learned model, we use our in-house ITG decoder with a trigram language model learned on the English part of the training data. The decoder uses CKY-style parsing (Cocke, 1969; Kasami, 1965; Younger, 1967) with cube pruning to integrate the language model (Chiang, 2007). The language model is trained with SRILM (Stolcke, 2002). To evaluate the output we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). Table 1: The results of decoding. ITG model BLEU NIST Rules Baseline 17.44 4.3909 47,298 Initial only 15.71 4.1267 251,947 Auxiliary only 16.11 3.9334 60,133 Augmented 19.32 4.4243 301,293 still need to calculate the change in the length of the data, which is: ( ) P (D|Φ′ , Ψ) DL D|Φ′ , Ψ −DL (D|Φ, Ψ) = −lg P (D|Φ, Ψ) For the sake of convenience in efficiently calculating this probability, we make the simplifying assumption that: 8 Results P (D|Φ, Ψ) ≈ P (D|Φ) = P (D|θ) The results (Table 1) show the baseline ITG and the proposed augmented ITG, as well as test sco"
R13-1077,P10-1017,0,0.0150374,"sing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct translation systems rely heavi"
R13-1077,C96-2141,0,0.316229,"only the input sentence during decoding—the model itself does not change, only the way it is used. The space of possible ITG structures is intractably large, and there have been many attempts to introduce external constraints to guide the search. We do completely unsupervised search without introducing such constraints, which limits the scope of error analysis to the search strategy. Popular external constraints include word alignments (Chiang, 2005) and parse trees. Word alignments are typically learned as a many-to-one function from one language into the other language (Brown et al., 1993; Vogel et al., 1996), but since no translation systems in use today actually rely on generating one output token at a time from zero or more input tokens, two opposing such functions are typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007;"
R13-1077,2011.eamt-1.42,1,0.830351,"Missing"
R13-1077,P08-1012,0,0.310361,"Missing"
R13-1077,W09-3804,1,0.938401,"Missing"
R13-1077,N10-1050,1,0.878032,"typically combined heuristically to form a many-to-many function between the input and output tokens. This is problematic, as it turns the alignments into hard constraints that are external to any model learned from them. Ironically, whenever transduction grammars are used to learn alignments these alignments are also treated as hard external constraints to the translation models that are learned from them (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2008, 2009; Haghighi et al., 2009; Saers and Wu, 2009, 2011; Blunsom and Cohn, 2010; Burkett et al., 2010; Riesa and Marcu, 2010; Saers et al., 2010; Neubig et al., 2011, 2012). We choose to work with the well-defined and theoretically sound formalism of ITGs rather than over-engineered direct translation models (Koehn et al., 2003) or feature-heavy transduction grammars (Chiang, 2005). The reason for this is twofold: (a) they allow for manual inspection, and (b) the assumptions stay the same through learning and testing. Being able to inspect the learned model is crucial for error analysis, but inspecting a typical state-of-the-art translation system is prohibitively hard. Phrasal direct translation systems rely heavily on the language m"
R13-1077,C12-1142,1,0.84265,"δsum ← δsum + δ ′ 18: end if 19: end for 20: until δsum ≥ 0 21: return Φ Algorithm 2 Iterative rule segmenting learning driven by minimum conditional description length. 1: Φ, Ψ ▷ The auxiliary and initial ITG 2: repeat 3: δsum ← 0 4: bs ← collect_biaffixes(Φ) 5: bδ ← [] 6: for all b ∈ bs do 7: δ ← eval_cdl(b, Ψ, Φ) 8: if δ &lt; 0 then 9: bδ ← [bδ, ⟨b, δ⟩] 10: end if 11: end for 12: sort_by_delta(bδ) 13: for all ⟨b, δ⟩ ∈ bδ do 14: δ ′ ← eval_cdl(b, Ψ, Φ) 15: if δ ′ &lt; 0 then 16: Φ ← make_segmentations(b, Φ) 17: δsum ← δsum + δ ′ 18: end if 19: end for 20: until δsum ≥ 0 21: return Φ Saers et al. (2012). That is: we start by initializing a token-based bracketing finite-state transduction grammar, or FSTG, parameterized with relative frequencies from the training corpus. We then tune the parameters to the training corpus, and then change the structure of the grammar to include lexical rules that can be formed by chunking adjacent preterminals. The tune–chunk step is repeated twice, before transforming the FSTG into a bracketing linear inversion transduction grammar, or LITG (Saers et al., 2010), whose parameters are also tuned to the training corpus. The LITG is then transformed into a full I"
R13-1077,2007.iwslt-1.1,0,\N,Missing
R13-1077,W09-2304,1,\N,Missing
R13-1077,J97-3002,1,\N,Missing
R13-1077,P06-1121,0,\N,Missing
R13-1077,J07-2003,0,\N,Missing
W08-2136,J02-3001,0,0.153068,"between speed and accuracy. Performance consistently deteriorated when splitting into smaller bins. The final system contained two variants, one with more bins based on a combination of PoS-tags and lemma frequency information, and one with fewer bins 249 based only on PoS-tag information. The three learning tasks used different splits. In general, the argument identification step was the most difficult and therefore required a larger number of bins. 3.3 Features We implemented a large number of features (over 50)1 for the SRL system. Many of them can be found in the literature, starting from Gildea and Jurafsky (2002) and onward. All features, except bag-of-words, take nominal values, which are binarized for the vectors used as input to the SVM classifier. Low-frequency feature values (except for Voice, Initial Letter, Number of Words, Relative Position, and the Distance features), below a threshold of 20 occurrences, were given a default value. We distinguish between single node and node pair features. The following single node features were used for all three learning tasks and for both the predicate and argument node:2 • Lemma, PoS, and Dependency relation (DepRel) for the node itself, the parent, and t"
W08-2136,D07-1097,1,0.889154,"Missing"
W08-2136,W05-0625,0,0.0315274,"belling, different syntactic structures risk being assigned to the same sentence, depending on which predicate is currently processed. This means that several, possibly different, parses have to be combined into one. In this experiment, the head and the dependency label were concatenated, and the most frequent one was used. In case of a tie, the first one to appear was used. The likelihood of the chosen labelling was also used as a confidence measure for the syntactic blender. 3.5 Blending and Post-Processing Combining the output from several different systems has been shown to be beneficial (Koomen et al., 2005). For the final submission, we combined the output of two variants of the pipelined SRL system, each using different data splits, with 3 The version of the joint system used in the submission was based on an early predicate prediction. More accurate predicates would give a major improvement for the results. 250 Test set WSJ Brown Pred PoS All NN* VB* All NN* VB* Labelled F1 82.90 81.12 85.52 67.48 58.34 73.24 Unlabelled F1 90.90 86.39 96.49 85.49 75.35 91.97 Syn + Sem Syn Sem Table 2: Semantic predicate results on the test sets. the SRL output of the joint system. A simple uniform weight major"
W08-2136,P07-1122,0,0.0461056,"Missing"
W08-2136,W04-0308,0,0.0872979,"Missing"
W08-2136,W08-2121,0,0.138033,"Missing"
W09-2304,P06-1002,0,0.0269673,"Missing"
W09-2304,2007.mtsummit-papers.14,0,0.0627909,"Missing"
W09-2304,W07-0403,0,0.379146,"Missing"
W09-2304,P05-1033,0,0.216217,"Missing"
W09-2304,2005.mtsummit-papers.11,0,0.0331301,"Missing"
W09-2304,W07-0734,0,0.0644471,"Missing"
W09-2304,P00-1056,0,0.476316,"Missing"
W09-2304,P03-1021,0,0.174322,"Missing"
W09-2304,W06-3117,0,0.030922,"Missing"
W09-2304,P95-1033,1,0.85149,"Missing"
W09-2304,W95-0106,1,0.804478,"Missing"
W09-2304,P96-1021,1,0.761648,"Missing"
W09-2304,J97-3002,1,0.872351,"Missing"
W09-2304,P98-2230,1,0.799746,"Missing"
W09-2304,P05-1059,0,0.213271,"Missing"
W09-2304,W03-0303,0,0.0431593,"Missing"
W09-2304,J93-2003,0,\N,Missing
W09-2304,C96-2141,0,\N,Missing
W09-2304,P02-1040,0,\N,Missing
W09-2304,P07-2045,0,\N,Missing
W09-2304,N03-1017,0,\N,Missing
W09-2304,J03-1002,0,\N,Missing
W09-2304,C98-2225,1,\N,Missing
W09-2304,N06-1033,0,\N,Missing
W09-3804,P07-2045,0,0.0103169,"re made with different pruning parameters. The EM process was halted when a relative improvement in log-likelihood of 10−3 was no longer achieved over the previous iteration. Evaluation We evaluate the parser on a translation task (WMT’08 shared task3 ). In order to evaluate on a translation task, a translation system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers & Wu (2009). We will evaluate the resulting translations with two automatic 3 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guid"
W09-3804,2005.mtsummit-papers.11,0,0.0102206,"s, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guidelines for building a baseline system for the WMT’08 shared task. The translation tasks we applied the above procedure to are all taken from the Europarl corpus (Koehn, 2005). We selected the tasks German-English, French-English and SpanishEnglish. Furthermore, we restricted the training sentence pairs so that none of the sentences exceeded length 10. This was necessary to be able to carry out exhaustive search. The total amount of training data was roughly 100,000 sentence pairs in each language pair, which is a relatively small corpus, but by no means a toy example. Analysis 5 Empirical results http://www.statmt.org/wmt08/ 31 Metric Baseline (GIZA++) ∞ BLEU NIST time 0.2597 6.6352 0.2663 6.7407 03:20:00 BLEU NIST time 0.2059 5.8668 0.2113 5.9380 03:40:00 BLEU NI"
W09-3804,J03-1002,0,0.0233874,"he item being extended. This fact can be exploited to limit the number of possible siblings explored, but has no impact on time complexity. 3.3 metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 In this section we describe the experimental setup as well as the outcomes. Viterbi parsing When doing Viterbi parsing, all derivations but the most probable are discarded. This gives an unambiguous parse, which dictates exactly one alignment between e and f . The alignment of the Viterbi parse can be used to substitute that of other word aligners (Saers and Wu, 2009) such as GIZA++ (Och and Ney, 2003). 4 6.1 Looking at the algorithm, it is clear that there will be a total of T + V = O(n) agendas, each containing items of a certain length. The items in an agenda can start anywhere in the alignment space: O(n2 ) possible starting points, but once the end point in one language is set, the end point in the other follows from that, adding a factor O(n). This means that each agenda contains O(n3 ) active items. Each active item has to go through all possible siblings in the recursive step. Since the start point of the sibling is determined by the item itself (it has to be adjacent), only the O(n"
W09-3804,P03-1021,0,0.00690123,"pus to substitute the alignments of GIZA++. This is the same approach as taken in Saers & Wu (2009). We will evaluate the resulting translations with two automatic 3 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guidelines for building a baseline system for the WMT’08 shared task. The translation tasks we applied the above procedure to are all taken from the Europarl corpus (Koehn, 2005). We selected the tasks German-English, French-English and SpanishEnglish. Furthermore, we restricted the training sentence pairs so that none of the sentences exceeded length 10. This was necessary to be able to carry"
W09-3804,P02-1040,0,0.106966,"ld a set of extensions E(i) for all active items i. All items in E(i) are then activated by placing them on their corresponding agenda (i ∈ A|i |). E(Xstuv ) = {XStU v |0 ≤ S ≤ s, 0 ≤ U ≤ u, XSsU u ∈ C} ∪ {XsSuU |t ≤ S ≤ T, v ≤ U ≤ V, XtSvU ∈ C} ∪ {XsSU v |t ≤ S ≤ T, 0 ≤ U ≤ u, XtSU u ∈ C} ∪ {XStuU |0 ≤ S ≤ s, v ≤ U ≤ V, XSsvU ∈ C} 30 Since we are processing the agendas in order, any item in the chart will be as long as or shorter than the item being extended. This fact can be exploited to limit the number of possible siblings explored, but has no impact on time complexity. 3.3 metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 In this section we describe the experimental setup as well as the outcomes. Viterbi parsing When doing Viterbi parsing, all derivations but the most probable are discarded. This gives an unambiguous parse, which dictates exactly one alignment between e and f . The alignment of the Viterbi parse can be used to substitute that of other word aligners (Saers and Wu, 2009) such as GIZA++ (Och and Ney, 2003). 4 6.1 Looking at the algorithm, it is clear that there will be a total of T + V = O(n) agendas, each containing items of a certain length. The items in an agenda"
W09-3804,W09-2304,1,0.789481,"will be as long as or shorter than the item being extended. This fact can be exploited to limit the number of possible siblings explored, but has no impact on time complexity. 3.3 metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 In this section we describe the experimental setup as well as the outcomes. Viterbi parsing When doing Viterbi parsing, all derivations but the most probable are discarded. This gives an unambiguous parse, which dictates exactly one alignment between e and f . The alignment of the Viterbi parse can be used to substitute that of other word aligners (Saers and Wu, 2009) such as GIZA++ (Och and Ney, 2003). 4 6.1 Looking at the algorithm, it is clear that there will be a total of T + V = O(n) agendas, each containing items of a certain length. The items in an agenda can start anywhere in the alignment space: O(n2 ) possible starting points, but once the end point in one language is set, the end point in the other follows from that, adding a factor O(n). This means that each agenda contains O(n3 ) active items. Each active item has to go through all possible siblings in the recursive step. Since the start point of the sibling is determined by the item itself (i"
W09-3804,W95-0106,1,0.822411,"as they must be completely made up of terminals. For notational convenience, the orientation of the rule is written as surrounding the production, like so: X → γ, X → [γ] and X → hγi. The vocabularies of the languages may both include the empty token , allowing for deletions and insertions. The empty biterminal, / is not allowed. 2.1 Stochastic ITGs In a Stochastic ITG (SITG), each rule is also associated with a probability, such that X P r(X → γ) = 1 γ for all X ∈ N . The probability of a deriva∗ tion S ⇒ e/f is defined as the production of the probabilities of all rules used. As shown by Wu (1995), it is possible to fit the parameters of a SITG to a parallel corpus via EM (expectationmaximization) estimation. 2.2 3.1 Bracketing ITGs In the initial step, the set of lexical items L is built. All lexical items i ∈ L are then activated by placing them on their corresponding agenda A|i |.   0 ≤ s ≤ t ≤ T,   L = Xstuv 0 ≤ u ≤ v ≤ V,  X → es..t /fu..v ∈ ∆  An ITG where there is only one nonterminal (other than the start symbol) is called a bracketing ITG (BITG). Since the one nonterminal is devoid of information, it can only be used to group its children together, imposing a bracketing"
W09-3804,J97-3002,1,\N,Missing
W10-1724,P02-1040,0,0.0791218,"→eC A→Be A→eC A→Be A→ A→f C A→Bf A→Cf A→f B A→ 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus German–English Europarl German–English news commentary English news commentary German–English news commentary German–English news commentary Type out of domain in-domain in-domain in-domain tuning data in-domain test data Size 1,219,343 sentence pairs 86,941 sentence pairs 48,653,884 sentences 2,051 sentence pairs 2,489 sentence pairs Table 1: Corpora available for the German–English translation task after baseline cleaning. System GIZA++ SBLITG SBLITG (only Europarl) SBLITG (only news) GIZA++ and SBLITG GIZA++ and SBLITG (only Europarl) GIZA++ and SBLI"
W10-1724,W09-2304,1,0.799204,"d one L2 ) synchronized CFG rules: ITG L1 ,L2 CFG L1 φ While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. CFG L2 A→[BC] A→BC A→BC A→hBCi A→BC A→CB A → e/f A→e A→f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6 ). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual g"
W10-1724,W09-3804,1,0.712078,"zed CFG rules: ITG L1 ,L2 CFG L1 φ While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. CFG L2 A→[BC] A→BC A→BC A→hBCi A→BC A→CB A → e/f A→e A→f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6 ). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear g"
W10-1724,N10-1050,1,0.31957,"1 ,L2 CFG L1 φ While training a Stochastic Bracketing ITG (SBITG) or LITG (SBLITG) with EM, expectations of probabilities over the biparse-forest are calculated. These expectations approach the true probabilities, and can be used as approximations. The probabilities over the biparse-forest can be used to select the one-best parse-tree, which in turn forces an alignment over the sentence pair. The alignments given by SBITGs and SBLITGs has been shown to give better translation quality than bidirectional IBM-models, when applied to short sentence corpora (Saers and Wu, 2009; Saers et al., 2009; Saers et al., 2010). In this paper we explore whether this hold for SBLITGs on standard sentence corpora. CFG L2 A→[BC] A→BC A→BC A→hBCi A→BC A→CB A → e/f A→e A→f Inducing an ITG from a parallel corpus is still slow, as the time complexity is O(Gn6 ). Several ways to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITG L"
W10-1724,J93-2003,0,0.0146871,"to get around this has been proposed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITG L1 ,L2 A → [ e/f C A → [ B e/f A → h e/f C A → h B e/f A → / ] ] i i LG L1 LGL2 A→eC A→Be A→eC A→Be A→ A→f C A→Bf A→Cf A→f B A→ 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus German–English Europarl German–English news commentary English news commentary German–English news commentary German–En"
W10-1724,P09-1104,0,0.0989571,"G), the grammar constant (G) can be eliminated, but O(n6 ) is still prohibitive for large data sets. There has been some work on approximate inference of ITGs. Zhang et al. (2008) present a method for evaluating spans in the sentence pair to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3 ). Saers, Nivre & Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3 ). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasal ITGs for translationdriven segmentation introduced in Wu (1997), although they only allow for one-to-many alignments, rather than many-to-many alignments. A more extreme approach is taken in Saers, Nivre & Wu (2010). Not only is the search severely pruned, but the grammar itself is limited to a lin167 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167–171, c Uppsala, Sweden, 15-16 July 201"
W10-1724,P07-1065,0,0.0244674,"German–English translation task after baseline cleaning. System GIZA++ SBLITG SBLITG (only Europarl) SBLITG (only news) GIZA++ and SBLITG GIZA++ and SBLITG (only Europarl) GIZA++ and SBLITG (only news) BLEU 17.88 17.61 17.46 15.49 17.66 17.58 17.48 NIST 5.9748 5.8846 5.8491 5.4987 5.9650 5.9819 5.9693 Table 2: Results for the German–English translation task. with both. We also combined all three SBLITG systems with the baseline system to see whether the additional translation paths would help. The system we submitted corresponds to the “GIZA ++ and SBLITG (only news)” system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. This was because we lacked the necessary RAM resources to calculate the full SRILM model before the system submission deadline. We chose to focus on the German–English translation task. The corpora resources available for that task is summarized in Table 1. We used the entire news commentary monolingual data concatenated with the English side of the Europarl bilingual data to train the language model. In retrospect, this was probably a bad choice, as others seem to prefer the use of two language models instead. We contrasted the baseline system with pure S"
W10-1724,C96-2141,0,0.186002,"ed (Zhang et al., 2008; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Taking a closer look at the linear ITGs (Saers et al., 2010), there are five rules in normal form. Decomposing these five rule types into monolingual rule types reveals that the monolingual grammars are linear grammars (LGs): LITG L1 ,L2 A → [ e/f C A → [ B e/f A → h e/f C A → h B e/f A → / ] ] i i LG L1 LGL2 A→eC A→Be A→eC A→Be A→ A→f C A→Bf A→Cf A→f B A→ 3 Setup The baseline system for the shared task was a phrase based translation model based on bidirectional IBM- (Brown et al., 1993) and HMMmodels (Vogel et al., 1996) combined with the grow-diag-final-and heuristic. This is computed with the GIZA++ tool (Och and Ney, 2003) and the Moses toolkit (Koehn et al., 2007). The language model was a 5-gram SRILM (Stolcke, 2002). Parameters in the final translation system were determined with Minimum Error-Rate Training (Och, 2003), and translation quality was assessed with the automatic measures BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 168 Corpus German–English Europarl German–English news commentary English news commentary German–English news commentary German–English news commentary Type out of d"
W10-1724,J97-3002,1,0.335182,"air to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3 ). Saers, Nivre & Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3 ). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasal ITGs for translationdriven segmentation introduced in Wu (1997), although they only allow for one-to-many alignments, rather than many-to-many alignments. A more extreme approach is taken in Saers, Nivre & Wu (2010). Not only is the search severely pruned, but the grammar itself is limited to a lin167 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 167–171, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics This means that LITGs are transduction grammars that transduce between linear languages. There is also a nice parallel in search time complexities between CFGs and ITGs on"
W10-1724,P08-1012,0,0.0607247,"TGs nor SDTGs are intuitively useful in translating natural languages, since STGs have no way to model reordering, and SDTGs require exponential time to be induced from examples (parallel corpora). Since Lately, there has been some interest in using Inversion Transduction Grammars (ITGs) for alignment purposes. The main problem with ITGs is the time complexity, O(Gn6 ) doesn’t scale well. By limiting the grammar to a bracketing ITG (BITG), the grammar constant (G) can be eliminated, but O(n6 ) is still prohibitive for large data sets. There has been some work on approximate inference of ITGs. Zhang et al. (2008) present a method for evaluating spans in the sentence pair to determine whether they should be excluded or not. The algorithm has a best case time complexity of O(n3 ). Saers, Nivre & Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3 ). They also show that severe pruning is possible without significant deterioration in alignment quality (as measured by downstream translation quality). Haghighi et al. (2009) use a simpler aligner as guidance for pruning, which reduces the time complexity by two orders of magnitude. Their work also partially implements the phrasa"
W10-1724,P07-2045,0,\N,Missing
W10-1724,J03-1002,0,\N,Missing
W10-1724,P03-1021,0,\N,Missing
W10-3802,J93-2003,0,0.053371,"parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, Background Any form of automatic translation that relies on generalizations of observed translations needs to align these translations on a sub-sentential level. The standard way of doing this is by aligning words, which works well for languages that use white space separators between words. The standard method is a combination of the family of IBM-models (Brown et al., 1993) and Hidden Markov Models (Vogel et al., 1996). These methods all arrive at a function (A) from language 1 (F ) to language 2 (E). By running the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statisti"
W10-3802,P81-1022,0,0.735882,"refer to that pair of terminals as a biterminal, which will be written as e/f . Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F , which is binarizable (since it is a CFG ), and can therefor be computed in polynomial time (O(n3 )). Once there is a parse tree for F , the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across a sentence pair) is needed. The time complexity for biparsing with SDTGs is O(n2n+2 ), which is clearly intractable. Inversion Transduction Grammars or ITGs (Wu, 1997) are transduction grammars that have a two-normal form, thus guara"
W10-3802,P09-1104,0,0.0427671,"directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars ( SDTGs). To differentiate identical nonterminal symbols, indices were used (the b"
W10-3802,J09-4009,0,0.0165327,"terminals are pair up with the empty string (ǫ). A → X x B X a B ; 0, 1, 2, 3 X a → a, ǫ X x → ǫ, x Lexical rules involving the empty string are referred to as singletons. Whenever a preterminal is used to pair up two terminal symbols, we refer to that pair of terminals as a biterminal, which will be written as e/f . Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F , which is binarizable (since it is a CFG ), and can therefor be computed in polynomial time (O(n3 )). Once there is a parse tree for F , the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across"
W10-3802,P07-2045,0,0.00937191,"03:10 35 17:00 1:49 6.7312 6.7101 6.6657 6.6637 6.6464 6.6464 Training times 38:00 1:20:00 3:40 7:33 Table 2: Results for the Spanish–English translation task. out (see table 1). The GIZA++ system was built according to the instructions for creating a baseline system for the Fifth Workshop on Statistical Machine Translation (WMT’10),1 but the above corpora were used instead of those supplied by the workshop. This includes word alignment with GIZA++, a 5-gram language model built with SRILM (Stolcke, 2002) and parameter tuning with MERT (Och, 2003). To carry out the actual translations, Moses (Koehn et al., 2007) was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1 http://www.statmt.org/wmt10/ 15 for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of EM, also with beam size 25. All systems are evaluated with BLEU ("
W10-3802,2005.mtsummit-papers.11,0,0.0193828,"study the impact of pruning on efficiency and translation quality. Initial grammars will be estimated by counting cooccurrences in the training corpus, after which expectation-maximization (EM) will be used to refine the initial estimate. At the last iteration, the one-best parse of each sentence will be considered as the word alignment of that sentence. In order to keep the experiments comparable, relatively small corpora will be used. If larger corpora were used, it would not be possible to get any results for unpruned SBITGs because of the prohibitive time complexity. The Europarl corpus (Koehn, 2005) was used as a starting point, and then all sentence pairs where one of the sentences were longer than 10 tokens were filtered Figure 1: Trade-offs between translation quality (as measured by BLEU) and biparsing time (in seconds plotted on a logarithmic scale) for SBLTGs, SBITGs and the combination. System 1 10 Beam size 50 25 75 100 ∞ 0.2661 0.2625 0.2597 0.2671 0.2633 0.2597 0.2663 0.2628 0.2597 6.7329 6.6714 6.6464 6.7445 6.6863 6.6464 6.6793 6.6765 6.6464 2:00:00 9:44 2:40:00 12:13 3:20:00 11:59 BLEU SBITG SBLTG GIZA++ 0.1234 0.2574 0.2597 0.2608 0.2645 0.2597 0.2655 0.2631 0.2597 0.2653 0"
W10-3802,J03-1002,0,0.0148984,"Missing"
W10-3802,P03-1021,0,0.00600772,"++ SBITG SBLTG 3.9705 6.6023 6.6464 6.6439 6.6800 6.6464 03:10 35 17:00 1:49 6.7312 6.7101 6.6657 6.6637 6.6464 6.6464 Training times 38:00 1:20:00 3:40 7:33 Table 2: Results for the Spanish–English translation task. out (see table 1). The GIZA++ system was built according to the instructions for creating a baseline system for the Fifth Workshop on Statistical Machine Translation (WMT’10),1 but the above corpora were used instead of those supplied by the workshop. This includes word alignment with GIZA++, a 5-gram language model built with SRILM (Stolcke, 2002) and parameter tuning with MERT (Och, 2003). To carry out the actual translations, Moses (Koehn et al., 2007) was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1 http://www.statmt.org/wmt10/ 15 for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of"
W10-3802,P02-1040,0,0.0841085,"was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1 http://www.statmt.org/wmt10/ 15 for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of EM, also with beam size 25. All systems are evaluated with BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 7 Results The results for the three different translation tasks are presented in Tables 2, 3 and 4. It is interesting to note that the trend they portray is quite similar. When the beam is very narrow, GIZA++ is better, but already at beam size 10, both transduction grammars are superior. ConSystem 1 10 Beam size 50 25 75 100 ∞ 0.2668 0.2672 0.2603 0.2655 0.2662 0.2603 0.2663 0.2649 0.2603 6.8068 6.8020 6.6907 6.8088 6.7925 6.6907 6.8151 6.7784 6.6907 2:10:00 9:35 2:45:00 13:56 3:10:00 10:52 BLEU SBITG SBLTG GIZA++ 0.1268 0.2600 0.2603 0.2632 0.2638 0.2603 0.2654"
W10-3802,W09-2304,1,0.787343,"the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars ( SDTGs). To differentiate identical nonterminal symbols, i"
W10-3802,W09-3804,1,0.888955,"ns can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars ( SDTGs). To differentiate identical nonterminal symbols, indices were used (the bag of nonterminals f"
W10-3802,N10-1050,1,0.576259,"that Linear Transduction Grammars (LTGs) generate the same transductions as Linear Inversion Transduction Grammars, and present a scheme for arriving at LTGs by bilingualizing Linear Grammars. We also present a method for obtaining Inversion Transduction Grammars from Linear (Inversion) Transduction Grammars, which can speed up grammar induction from parallel corpora dramatically. 2 1 Introduction In this paper we introduce Linear Transduction Grammars ( LTGs), which are the bilingual case of Linear Grammars ( LGs). We also show that LTG s are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, Background Any form of automatic translation that relies on generalizations of observed translations needs to align these translations on a sub-sentential level. The standard way of doing this is by aligning words, which works well for languages that use white space separators between words. The stan"
W10-3802,C96-2141,0,0.410167,"rses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, Background Any form of automatic translation that relies on generalizations of observed translations needs to align these translations on a sub-sentential level. The standard way of doing this is by aligning words, which works well for languages that use white space separators between words. The standard method is a combination of the family of IBM-models (Brown et al., 1993) and Hidden Markov Models (Vogel et al., 1996). These methods all arrive at a function (A) from language 1 (F ) to language 2 (E). By running the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Bei"
W10-3802,J97-3002,1,0.885768,"rom Linear (Inversion) Transduction Grammars, which can speed up grammar induction from parallel corpora dramatically. 2 1 Introduction In this paper we introduce Linear Transduction Grammars ( LTGs), which are the bilingual case of Linear Grammars ( LGs). We also show that LTG s are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, Background Any form of automatic translation that relies on generalizations of observed translations needs to align these translations on a sub-sentential level. The standard way of doing this is by aligning words, which works well for languages that use white space separators between words. The standard method is a combination of the family of IBM-models (Brown et al., 1993) and Hidden Markov Models (Vogel et al., 1996). These methods all arrive at a function (A) from language 1 (F ) to language 2 (E). By running the process in both directions, two f"
W10-3802,P08-1012,0,0.0612347,"e 2 (E). By running the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars ( SDTGs). To differentiate identical no"
W11-1008,P89-1018,0,0.436958,"anslation, pages 70–78, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics on the one side, and the flexibility of grammar formalism and parsing algorithm development afforded by semiring (bi-) parsing. It is, however, possible to have both, which we will show in Section 5. An integral part of this unification is the concept of contextual probability. Finally, we will offer some conclusions in Section 6. 2 Background A common view on probabilistic parsing—be it bilingual or monolingual—is that it involves the construction of a weighted hypergraph (Billot and Lang, 1989; Manning and Klein, 2001; Huang, 2008). This is an appealing conceptualization, as it separates the construction of the parse forest (the actual hypergraph) from the probabilistic calculations that need to be carried out. The calculations are, in fact, given by the hypergraph itself. To get the probability of the sentence (pair) being parsed, one simply have to query the hypergraph for the value of the goal node. It is furthermore possible to abstract away the calculations themselves, by defining the hypergraph over an arbitrary semiring. When the Boolean semiring is used, the value of the go"
W11-1008,P02-1001,0,0.0341339,"parses were introduced in Goodman (1999). In this paper we will reify the grammar rules by moving them from the meta level to the object level—effectively making them first-class citizens of the parse trees, which are no longer weighted hypergraphs, but mul/add-graphs. This move allows us to calculate rule expectations for expectation maximization (Dempster et al., 1977) as part of the parsing process, which significantly shortens turn-over time for experimenting with different grammar for71 malisms. Another approach which achieve a similar goal is to use a expectation semiring (Eisner, 2001; Eisner, 2002; Li and Eisner, 2009). In this semiring, all values are pairs of probabilities and expectations. The inside-outside algorithm with the expectation semiring requires the usual inside and outside calculations over the probability part of the semiring values, followed by a third traversal over the parse forest to populate the expectation part of the semiring values. The approach taken in this paper also requires the usual inside and outside calculations, but o third traversal of the parse forest. Instead, the proposed approach requires two passes over the rules of the grammar per EM iteration. T"
W11-1008,J99-4004,0,0.48061,"rammar, and false otherwise. When the probabilistic semiring is used, the probability of the sentence (pair) is attained, and with the tropical semiring, the probability of the most likely tree is attained. To further generalize the building of the hypergraph—the parsing algorithm—a deductive system can be used. By defining a hand-full of deductive rules that describe how items can be constructed, the full complexities of a parsing algorithm can be very succinctly summarized. Deductive systems to represent parsers and semirings to calculate the desired values for the parses were introduced in Goodman (1999). In this paper we will reify the grammar rules by moving them from the meta level to the object level—effectively making them first-class citizens of the parse trees, which are no longer weighted hypergraphs, but mul/add-graphs. This move allows us to calculate rule expectations for expectation maximization (Dempster et al., 1977) as part of the parsing process, which significantly shortens turn-over time for experimenting with different grammar for71 malisms. Another approach which achieve a similar goal is to use a expectation semiring (Eisner, 2001; Eisner, 2002; Li and Eisner, 2009). In t"
W11-1008,D09-1005,0,0.0218808,"troduced in Goodman (1999). In this paper we will reify the grammar rules by moving them from the meta level to the object level—effectively making them first-class citizens of the parse trees, which are no longer weighted hypergraphs, but mul/add-graphs. This move allows us to calculate rule expectations for expectation maximization (Dempster et al., 1977) as part of the parsing process, which significantly shortens turn-over time for experimenting with different grammar for71 malisms. Another approach which achieve a similar goal is to use a expectation semiring (Eisner, 2001; Eisner, 2002; Li and Eisner, 2009). In this semiring, all values are pairs of probabilities and expectations. The inside-outside algorithm with the expectation semiring requires the usual inside and outside calculations over the probability part of the semiring values, followed by a third traversal over the parse forest to populate the expectation part of the semiring values. The approach taken in this paper also requires the usual inside and outside calculations, but o third traversal of the parse forest. Instead, the proposed approach requires two passes over the rules of the grammar per EM iteration. The asymptotic time com"
W11-1008,W01-1812,0,0.0458272,"c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics on the one side, and the flexibility of grammar formalism and parsing algorithm development afforded by semiring (bi-) parsing. It is, however, possible to have both, which we will show in Section 5. An integral part of this unification is the concept of contextual probability. Finally, we will offer some conclusions in Section 6. 2 Background A common view on probabilistic parsing—be it bilingual or monolingual—is that it involves the construction of a weighted hypergraph (Billot and Lang, 1989; Manning and Klein, 2001; Huang, 2008). This is an appealing conceptualization, as it separates the construction of the parse forest (the actual hypergraph) from the probabilistic calculations that need to be carried out. The calculations are, in fact, given by the hypergraph itself. To get the probability of the sentence (pair) being parsed, one simply have to query the hypergraph for the value of the goal node. It is furthermore possible to abstract away the calculations themselves, by defining the hypergraph over an arbitrary semiring. When the Boolean semiring is used, the value of the goal node will be true if t"
W11-1008,2011.eamt-1.42,1,0.819373,"ly, the contextual probability is: The contextual probability of something is the sum of the probabilities of all contexts where it was used. This opens up an interesting line of inquiry into what this quantity might represent. In this paper we show that the contextual probabilities of the rules contain precisely the new information needed in order to calculate the expectations needed to reestimate the rule probabilities. This line of inquiry was discovered while working on a preterminalized version of linear inversion transduction grammars (LITGs), so we will use these preterminalized LITGs (Saers and Wu, 2011) as an example throughout this paper. We will start by examining semiring parsing (parsing as deductive systems over semirings, Section 3), followed by a section on how this relates to weighted hypergraphs, a common representation of parse forests (Section 4). This reveals a disparity between weighted hypergraphs and semiring parsing. It seems like we are forced to choose between the inside-outside algorithm for context-free grammars 70 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 70–78, c ACL HLT 2011, Portland, Oregon, USA, June 2"
W11-2144,S10-1021,0,0.0238005,"ds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentence anaphora is left to chance. We therefore added a word-dependency model (Hardmeier and Federico, 2010) to our system to handle anaphora explicitly. Our processing of anaphoric pronouns follows the procedure outlined by Hardmeier and Federico (2010). We use the open-source coreference resolution system BART (Broscheit et al., 2010) to link pronouns to their antecedents in the text. Coreference links are handled differently depending on whether or not they cross sentence boundaries. If a coreference link points to a previous sentence, we process the sentence containing the antecedent with the SMT system and look up the translation of the antecedent in the translated output. If the coreference link is sentence-internal, the translation lookup is done dynamically by the decoder during search. In either case, the word-dependency model adds a feature function to the decoder score representing the probability of a particular"
W11-2144,de-marneffe-etal-2006-generating,0,0.0128364,"Missing"
W11-2144,D08-1089,0,0.301274,"ench Our submission to the English-French task was a phrase-based Statistical Machine Translation based on the Moses decoder (Koehn et al., 2007). Phrase tables were separately trained on Europarl, news commentary and UN data and then linearly interpolated with uniform weights. For language modelling, we used 5-gram models trained with the IRSTLM toolkit (Federico et al., 2008) on the monolingual News corpus and parts of the English-French 109 corpus. More unusual features of our system included a special component to handle pronominal anaphora and the hierarchical lexical reordering model by Galley and Manning (2008). Selected features of our system will be discussed in depth in the following sections. 1.1 Handling pronominal anaphora Pronominal anaphora is the use of pronominal expressions to refer to “something previously mentioned in the discourse” (Strube, 2006). It is a very common phenomenon found in almost all kinds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentenc"
W11-2144,P07-2053,0,0.0307365,"Missing"
W11-2144,2010.iwslt-papers.10,1,0.918745,"ing sections. 1.1 Handling pronominal anaphora Pronominal anaphora is the use of pronominal expressions to refer to “something previously mentioned in the discourse” (Strube, 2006). It is a very common phenomenon found in almost all kinds of texts. Anaphora can be local to a sentence, or it can cross sentence boundaries. Standard SMT methods do not handle this phenomenon in a satisfactory way at present: For sentence-internal anaphora, they depend on the n-gram language model with its limited history, while cross-sentence anaphora is left to chance. We therefore added a word-dependency model (Hardmeier and Federico, 2010) to our system to handle anaphora explicitly. Our processing of anaphoric pronouns follows the procedure outlined by Hardmeier and Federico (2010). We use the open-source coreference resolution system BART (Broscheit et al., 2010) to link pronouns to their antecedents in the text. Coreference links are handled differently depending on whether or not they cross sentence boundaries. If a coreference link points to a previous sentence, we process the sentence containing the antecedent with the SMT system and look up the translation of the antecedent in the translated output. If the coreference li"
W11-2144,P06-1063,0,0.0313791,"Missing"
W11-2144,2005.iwslt-1.8,0,0.214486,"Missing"
W11-2144,P07-2045,1,0.0116502,"at WMT 2011. We created two largely independent systems for English-to-French and Haitian Creole-toEnglish translation to evaluate different features and components from our ongoing research on these language pairs. Key features of our systems include anaphora resolution, hierarchical lexical reordering, data selection for language modelling, linear transduction grammars for word alignment and syntaxbased decoding with monolingual dependency information. 1 English to French Our submission to the English-French task was a phrase-based Statistical Machine Translation based on the Moses decoder (Koehn et al., 2007). Phrase tables were separately trained on Europarl, news commentary and UN data and then linearly interpolated with uniform weights. For language modelling, we used 5-gram models trained with the IRSTLM toolkit (Federico et al., 2008) on the monolingual News corpus and parts of the English-French 109 corpus. More unusual features of our system included a special component to handle pronominal anaphora and the hierarchical lexical reordering model by Galley and Manning (2008). Selected features of our system will be discussed in depth in the following sections. 1.1 Handling pronominal anaphora"
W11-2144,J93-2004,0,0.0368938,"he overall best scores in both translation directions. The fact that both alignments lead to complementary information can be seen in the size of the phrase tables extracted (see table 3). 2.2 Syntax-based SMT We used Moses and its syntax-mode for our experiments with hierarchical phrase-based and syntaxaugmented models. Our main interest was to investigate the influence of monolingual parsing on the translation performance. In particular, we tried to integrate English dependency parses created by MaltParser (Nivre et al., 2007) trained on the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) extended with about 4000 questions 3 We actually swapped the development set and the test set by mistake. But, of course, we never mixed development and test data in any result reported. null from the Question Bank (Judge et al., 2006). The conversion to dependency trees was done using the Stanford Parser (de Marneffe et al., 2006). Again, we ran both translation directions to test our settings in more than just one task. Interesting here is also the question whether there are significant differences when integrating monolingual parses on the source or on the target side. The motivation for a"
W11-2144,J03-1002,0,0.00544101,"MT 2011 to build a large scale-background language model. The English data from the Haitian Creole task were used as a separate domain-specific language model. For the other translation direction we only used the in-domain data provided. We used standard 5-gram models with Witten-Bell discounting and backoff interpolation for all language models. For the translation model we applied standard techniques and settings for phrase extraction and score estimations. However, we applied two different systems for word alignment: One is the standard GIZA++ toolbox implementing the IBM alignment models (Och and Ney, 2003) and extensions and the other is based on transduction grammars which will briefly be introduced in the next section. 2.1.1 Alignment with PLITGs By making the assumption that the parallel corpus constitutes a linear transduction (Saers, 2011)2 we can induce a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be ext"
W11-2144,2011.eamt-1.42,1,0.822224,"e a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be extracted and used instead of other word alignment approaches such as GIZA++. There are several grammar types that generate linear transductions, and in this work, stochastic bracketing preterminalized linear inversion transduction grammars (PLITG) were used (Saers and Wu, 2011). Since we were mainly interested in the word alignments, we did not induce phrasal grammars. Although alignments from PLITGs may not reach the same level of translation quality as GIZA++, they make different mistakes, so both complement 2A transduction is a set of pairs of strings, and thus represents a relation between two languages. 375 each other. By duplicating the training corpus and aligning each copy of the corpus with a different alignment tool, the phrase extractor seems to be able to pick the best of both worlds, producing a phrase table that is superior to one produced with either"
W11-2144,N10-1050,1,0.845828,"dard GIZA++ toolbox implementing the IBM alignment models (Och and Ney, 2003) and extensions and the other is based on transduction grammars which will briefly be introduced in the next section. 2.1.1 Alignment with PLITGs By making the assumption that the parallel corpus constitutes a linear transduction (Saers, 2011)2 we can induce a grammar that is the most likely to have generated the observed corpus. The grammar induced will generate a parse forest for each sentence pair in the corpus, and each parse tree in that forest will correspond to an alignment between the two sentences. Following Saers et al. (2010), the alignment corresponding to the best parse can be extracted and used instead of other word alignment approaches such as GIZA++. There are several grammar types that generate linear transductions, and in this work, stochastic bracketing preterminalized linear inversion transduction grammars (PLITG) were used (Saers and Wu, 2011). Since we were mainly interested in the word alignments, we did not induce phrasal grammars. Although alignments from PLITGs may not reach the same level of translation quality as GIZA++, they make different mistakes, so both complement 2A transduction is a set of"
W11-2144,C08-1144,0,0.0330283,"lish test set with (=malt) or without (=hiero) English parse trees and various parse relaxation strategies. The final system submitted to WMT11 is malt(target)-samt2. rule extraction is based on tree manipulation and relaxed extraction algorithms. Moses implements several algorithms that have been proposed in the literature. Tree binarisation is one of them. This can be done in a left-branching and in a right-branching mode. We used a combination of both in the settings denoted as binarised. The other relaxation algorithms are based on methods proposed for syntaxaugmented machine translation (Zollmann et al., 2008). We used two of them: samt1 combines pairs of neighbouring children nodes into combined complex nodes and creates additional complex nodes of all children nodes except the first child and similar complex nodes for all but the last child. samt2 combines any pair of neighbouring nodes even if they are not children of the same parent. All of these relaxation algorithms lead to increased rule sets (table 4). In terms of translation performance there seems to 377 be a strong correlation between rule table size and translation quality as measured by BLEU. None of the dependency-based models beats t"
W13-0806,P09-1088,0,0.727578,"es are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will"
W13-0806,J93-2003,0,0.0286628,"l—this used to be a given in machine learning and statistical prediction, but has been largely ignored in the statistical machine translation (SMT) community, where most current SMT approaches to learning phrase translations that (a) require enormous amounts of run-time memory, and (b) contain a high degree of redundancy. In particular, phrase-based SMT models such as Koehn et al. (2003) and Chiang (2007) often search for candidate translation segments and transduction rules by committing to a word alignment that is completely alien to the grammar, as it is learned with very different models (Brown et al. (1993), Vogel et al. (1996)), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations (Och 48 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates the above points well. Transduction grammars"
W13-0806,W07-0403,0,0.486174,"arch since, usually referred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic struct"
W13-0806,P06-1121,0,0.276218,"on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates the above points well. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints. This approach was pioneered by Galley et al. (2006), and there has been a lot of research since, usually referred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through trans"
W13-0806,P09-1104,0,0.163408,"ding on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to"
W13-0806,D07-1103,0,0.0264717,"the grammar, as it is learned with very different models (Brown et al. (1993), Vogel et al. (1996)), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations (Och 48 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates the above points well. Transduction grammars can also be induced from treebanks instead of unannotated corpora, which cuts down the vast search space by enforcing additional, external constraints. This approach was pioneered by Galley et al. (2006), and there has been a lot of research since, usually referred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not b"
W13-0806,N03-1017,0,0.051497,"mbination of them is superior in terms of translation accuracy to either of them in isolation. The transduction grammar approach has the advantage that induction, tuning and testing are optimized on the exact same underlying model—this used to be a given in machine learning and statistical prediction, but has been largely ignored in the statistical machine translation (SMT) community, where most current SMT approaches to learning phrase translations that (a) require enormous amounts of run-time memory, and (b) contain a high degree of redundancy. In particular, phrase-based SMT models such as Koehn et al. (2003) and Chiang (2007) often search for candidate translation segments and transduction rules by committing to a word alignment that is completely alien to the grammar, as it is learned with very different models (Brown et al. (1993), Vogel et al. (1996)), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations (Och 48 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that"
W13-0806,P11-1064,0,0.840672,"l constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will provide a way to escape the maximum-likelihood-ofthe-data-given-the-model optimum that w"
W13-0806,P12-1018,0,0.745634,"bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will provide a way to escape the maximum-likelihood-ofthe-data-given-the-model optimum that we start out with. Howe"
W13-0806,J03-1002,0,0.0225479,"Missing"
W13-0806,P03-1021,0,0.0478082,"ctation maximization (Dempster et al., 1977) and parse forests acquired with the above mentioned biparser, again with a beam width of 100. To do the actual decoding, we use our in-house ITG decoder. The decoder uses a CKYstyle parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. The decoder builds an efficient hypergraph structure which is then scored using both the induced grammar and the language model. The weights for the language model and the grammar, are tuned towards BLEU (Papineni et al., 2002) using MERT (Och, 2003). We use the ZMERT (Zaidan, 2009) implementation of MERT as it is a robust and flexible implementation of MERT, while being loosely coupled with the decoder. We use SRILM (Stolcke, 2002) for training a trigram language model on the English side of the training data. To evaluate the quality of the resulting translations, we use BLEU, and NIST (Doddington, 2002). 7 Experimental results The results from running the experiments detailed in the previous section can be summarized in four graphs. Figures 1 and 2 show the size of our new, segmenting model during induction, in terms of description leng"
W13-0806,P02-1040,0,0.0867275,"ers to the training data using expectation maximization (Dempster et al., 1977) and parse forests acquired with the above mentioned biparser, again with a beam width of 100. To do the actual decoding, we use our in-house ITG decoder. The decoder uses a CKYstyle parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. The decoder builds an efficient hypergraph structure which is then scored using both the induced grammar and the language model. The weights for the language model and the grammar, are tuned towards BLEU (Papineni et al., 2002) using MERT (Och, 2003). We use the ZMERT (Zaidan, 2009) implementation of MERT as it is a robust and flexible implementation of MERT, while being loosely coupled with the decoder. We use SRILM (Stolcke, 2002) for training a trigram language model on the English side of the training data. To evaluate the quality of the resulting translations, we use BLEU, and NIST (Doddington, 2002). 7 Experimental results The results from running the experiments detailed in the previous section can be summarized in four graphs. Figures 1 and 2 show the size of our new, segmenting model during induction, in te"
W13-0806,W09-2304,1,0.894294,"string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective t"
W13-0806,2011.eamt-1.42,1,0.862873,"ess by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will provide a way to escape the maximum-likelihood-ofthe-data-given-th"
W13-0806,W09-3804,1,0.889494,"proach in isolation. We have outlined how this can be done in practice, and we now substantiate that claim empirically. We will initialize a stochastic bracketing inversion transduction grammar (BITG) to rewrite it’s one nonterminal symbol directly into all the sentence pairs of the training data (iteration 0). We will then segment the grammar iteratively a total of seven times (iterations 1–7). For each iteration we will record the change in description length and test the grammar. Each iteration requires us to biparse the training data, which we do with the cubic time algorithm described in Saers et al. (2009), with a beam width of 100. As training data, we use the IWSLT07 Chinese– English data set (Fordyce, 2007), which contains 46,867 sentence pairs of training data, 506 Chinese sentences of development data with 16 English reference translations, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more “word like” sequences (Wu, 1999). As the bottom-up grammar, we will reuse the grammar learned in Saers e"
W13-0806,C96-2141,0,0.400564,"iven in machine learning and statistical prediction, but has been largely ignored in the statistical machine translation (SMT) community, where most current SMT approaches to learning phrase translations that (a) require enormous amounts of run-time memory, and (b) contain a high degree of redundancy. In particular, phrase-based SMT models such as Koehn et al. (2003) and Chiang (2007) often search for candidate translation segments and transduction rules by committing to a word alignment that is completely alien to the grammar, as it is learned with very different models (Brown et al. (1993), Vogel et al. (1996)), whose output is then combined heuristically to form the alignment actually used to extract lexical segment translations (Och 48 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 48–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics and Ney, 2003). The fact that it is even possible to improve the performance of a phrase-based direct translation system by tossing away most of the learned segmental translations (Johnson et al., 2007) illustrates the above points well. Transduction grammars can also be induced"
W13-0806,N10-1050,1,0.878678,"ining data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The minimum description length (MDL) objective that we will be using to drive the learning will provide a way to esc"
W13-0806,C12-1142,1,0.601597,"of Science and Technology {masaers|vskaddanki|dekai}@cs.ust.hk Abstract We show that combining both bottom-up rule chunking and top-down rule segmentation search strategies in purely unsupervised learning of phrasal inversion transduction grammars yields significantly better translation accuracy than either strategy alone. Previous approaches have relied on incrementally building larger rules by chunking smaller rules bottomup; we introduce a complementary top-down model that incrementally builds shorter rules by segmenting larger rules. Specifically, we combine iteratively chunked rules from Saers et al. (2012) with our new iteratively segmented rules. These integrate seamlessly because both stay strictly within a pure transduction grammar framework inducing under matching models during both training and testing—instead of decoding under a completely different model architecture than what is assumed during the training phases, which violates an elementary principle of machine learning and statistics. To be able to drive induction top-down, we introduce a minimum description length objective that trades off maximum likelihood against model size. We show empirically that combining the more liberal rul"
W13-0806,P08-1012,0,0.870395,"erred to as tree-to-tree, treeto-string and string-to-tree, depending on where the analyses are found in the training data. This complicates the learning process by adding external constraints that are bound to match the translation model poorly; grammarians of English should not be expected to care about its relationship to Chinese. It does, however, constitute a way to borrow nonterminal categories that help the translation model. It is also possible for the word alignments leading to phrase-based SMT models to be learned through transduction grammars (see for example Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Saers and Wu (2009), Haghighi et al. (2009), Blunsom et al. (2009), Saers et al. (2010), Blunsom and Cohn (2010), Saers and Wu (2011), Neubig et al. (2011), Neubig et al. (2012)). Even when the SMT model is hierarchical, most of the information encoded in the grammar is tossed away, when the learned model is reduced to a word alignment. A word alignment can only encode the lexical relationships that exist between a sentence pair according to a single parse tree, which means that the rest of the model: the alternative parses and the syntactic structure, is ignored. The"
W13-0806,W05-0835,0,0.366784,"Missing"
W13-0806,2007.iwslt-1.1,0,\N,Missing
W13-0806,J97-3002,1,\N,Missing
W13-0806,J07-2003,0,\N,Missing
W13-0806,N10-1028,0,\N,Missing
W13-2810,N10-1028,0,0.296831,"n BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theo"
W13-2810,P09-1088,0,0.389373,"Missing"
W13-2810,D07-1103,0,0.12647,"Missing"
W13-2810,J93-2003,0,0.0792232,"Missing"
W13-2810,N10-1015,0,0.121115,"tifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded"
W13-2810,W07-0403,0,0.059124,"s. Instead of embedding our learned ITG in the midst of many other heuristic components for the sake of a short term boost in BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised appr"
W13-2810,P11-1064,0,0.177299,"Missing"
W13-2810,P05-1033,0,0.124966,"onious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded minimum description length (MDL) objective to explicitly drive two opposing, extreme ITGs towards one minimal ITG. This represents a new attack on the problem suffered by most current SMT approaches of learning phrase translations that require enormous amounts of run-time memory, contain a high degree of redundancy, and fails to provide an obvious basis for generalization to abstract translation schemas. In particular, phrasal SMT models such as Koehn et al. (2003) and Chiang (2005) often search for candidate translation segments and transduction rules by committing 67 Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 67–73, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics When the structure of an ITG is induced without supervision, it has so far been assumed that smaller rules get clumped together into larger rules. This is a natural way to search, since maximum likelihood (ML) tends to improve with longer rules, which is typically balanced with Bayesian priors (Zhang et al., 2008). Bayesian priors are also us"
W13-2810,P12-1018,0,0.107404,"Missing"
W13-2810,J07-2003,0,0.208877,"le probability function p′ is identical to p, except that: 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written without whitespace, we use a tool that tries to clump characters together into more “word like” sequences (Wu, 1999). After each iteration, we use the long ITG to translate the held out test set with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. The decoder builds an efficient hypergraph structure which is scored using both the induced grammar and a language model. We use SRILM (Stolcke, 2002) for training a trigram language model on the English side of the training corpus. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). We also perform a combination experiment, where the grammar at different stages of the learning process (iterations) are interpolated with each other. This is a straight-forward linear interpolation, where the probabiliti"
W13-2810,J03-1002,0,0.0241249,"Missing"
W13-2810,P02-1040,0,0.0965798,"into more “word like” sequences (Wu, 1999). After each iteration, we use the long ITG to translate the held out test set with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. The decoder builds an efficient hypergraph structure which is scored using both the induced grammar and a language model. We use SRILM (Stolcke, 2002) for training a trigram language model on the English side of the training corpus. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). We also perform a combination experiment, where the grammar at different stages of the learning process (iterations) are interpolated with each other. This is a straight-forward linear interpolation, where the probabilities of the rules are added up and the grammar is renormalized. Although it makes little sense from an MDL point of view to increase the size of the grammar so indiscriminately, it does make sense from an engineering point of view, since more rules typically means better coverage, which in turn typically means better translations of unknown data. p′"
W13-2810,P10-1017,0,0.0597358,"the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded minimum description leng"
W13-2810,P06-1121,0,0.121311,"rs, making it hard to evaluate how accurate the learned models themselves were. In this work we take a radically different approach, and start with the longest rules possible and attempt to segment them into shorter rules iteratively. This makes ML useless, since our initial model maximizes it. Instead, we balance the ML objective with a minimum description length (MDL) objective, which let us escape the initial ML optimum by rewarding model parsimony. Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints (Galley et al., 2006). This complicates the learning process by adding external constraints that are bound to match the translation model poorly. It does, however, constitute a way to borrow nonterminal categories that help the translation model. MDL has been used before in monolingual grammar induction (Grünwald, 1996; Stolcke and Omohundro, 1994), as well as to interpret visual scenes (Si et al., 2011). Our work is markedly different in that we (a) induce an ITG rather than a monolingual grammar, and (b) focus on learning the terminal segments rather than the nonterminal categories. Iterative segmentation has al"
W13-2810,C96-2141,0,0.521476,"Missing"
W13-2810,2011.eamt-1.42,1,0.912018,"Missing"
W13-2810,W09-3804,1,0.961398,"y other heuristic components for the sake of a short term boost in BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean ph"
W13-2810,N10-1050,1,0.86223,"L-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded minimum description length (MDL) objective to"
W13-2810,P08-1012,0,0.129418,"ITG in the midst of many other heuristic components for the sake of a short term boost in BLEU, we focus on scientifically understanding the behavior of pure MDL-based search for phrasal translations, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing pars"
W13-2810,C12-1142,1,0.930882,"ions, divorced from the effect of other variables, even though BLEU is naturally much lower this way. The common practice of plugging some aspect of a learned ITG into either (a) a long pipeline of training heuristics and/or (b) an existing decoder that has been patched up to compensate for earlier modeling mistakes, as we and others have done before—see for example Cherry and Lin (2007); Zhang et al. (2008); Blunsom et al. (2008, 2009); Haghighi et al. (2009); Saers and Wu (2009, 2011); Blunsom and Cohn (2010); Burkett et al. (2010); Riesa and Marcu (2010); Saers et al. (2010); Neubig et al. (2011, 2012)—obscures the specific traits of the induced grammar. Instead, we directly use our learned ITG in translation mode (any transduction grammar also represents a decoder when parsing with the input sentence as a hard constraint) which allows us to see exactly which aspects of correct translation the transduction rules have captured. Introduction We introduce an unsupervised approach to inducing parsimonious, relatively clean phrasal inversion transduction grammars or ITGs (Wu, 1997) that employs a theoretically well-founded minimum description length (MDL) objective to explicitly drive two opposi"
W13-2810,W05-0835,0,0.399325,"Missing"
W13-2810,2007.iwslt-1.1,0,\N,Missing
W13-2810,W09-2304,1,\N,Missing
W13-2810,N03-1017,0,\N,Missing
W13-2810,J97-3002,1,\N,Missing
W13-2810,P09-1104,0,\N,Missing
W13-5703,N10-1028,0,0.0803563,"g different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into th"
W13-5703,P09-1088,0,0.0748141,"e word alignment). We will show that we can counter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard con"
W13-5703,N10-1015,0,0.0156261,"the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering effo"
W13-5703,W07-0403,0,0.0205269,"ossible segmental translations (that do not violate the prerequisite word alignment). We will show that we can counter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for"
W13-5703,J07-2003,0,0.0228713,"w/categories only Segmented ITG only Segmented ITG mixed with chunked ITG Segmented ITG mixed with chunked ITG w/categories Segmented ITG conditioned on chunked ITG Segmented ITG conditioned on chunked ITG w/categories ... with iterations combined ... and improved search parameters 0 Figure 2: Rule count versus BLEU scores for the bootstrapped ITG, the pruned bootstrapped ITG and the segmented ITG conditioned on the pruned bootstrapped ITG. lation systems with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. For language model, we use a trigram language model trained with the SRILM toolkit (Stolcke, 2002) on the English side of the training corpus. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). to have when translating are not explicitly in the grammar. This is potentially a source of translation mistakes, and to investigate this, we create a mixture model from iterations of the segmenting learning process leading up to the learned ITG. All the above outlined ITGs are trained using the IWSLT07 Chines"
W13-5703,W09-2304,1,0.835944,"nter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it"
W13-5703,2011.eamt-1.42,1,0.822178,"significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into perfecting existing decoders, it also prev"
W13-5703,P06-1121,0,0.0173463,"e model length component of description length. All of the above evaluate their models by biparsing the training data and feeding the imposed word alignment into an existing, mismatched SMT learning pipeline. DL (Ψ, Φ, D) = DL (D|Φ, Ψ) + DL (Φ|Ψ) + DL (Ψ) In minimizing CDL, we fix DL (Ψ) instead of allowing Ψ to vary as we would in full MCDL; to be precise, we seek: argmin DL (Ψ, Φ, D) Φ = argmin DL (D|Φ, Ψ) + DL (Φ|Ψ) + DL (Ψ) Φ = argmin DL (Φ, D|Ψ) Φ Transduction grammars can also be induced with supervision from treebanks, which cuts down the search space by enforcing external constraints (Galley et al., 2006). Although this constitutes a way to borrow nonterminal categories that help the translation model, it complicates the learning process by adding external constraints that are bound to match the translation model poorly. = argmin DL (D|Φ, Ψ) + DL (Φ|Ψ) Φ To measure the CDL of the data, we turn to information theory to count the number of bits needed to encode the data given the two models under an optimal encoding (Shannon, 1948), which gives: DL (D|Φ, Ψ) = −lg P (D|Φ, Ψ) 28 To measure the CDL of the model, we borrow the encoding scheme for description length presented in Saers et al. (2013),"
W13-5703,W09-3804,1,0.850213,"chnique described in Saers et al. (2012) to start with a finite-state transduction grammar (FSTG) and perform chunking before splitting the nonterminal categories and moving the FSTG into ITG form. We will perform one round of chunking, and two rounds of category splitting (resulting in 4 nonterminals and 4 preterminals, which becomes 8 nonterminals in the ITG form). Splitting all categories is guarnteed to at least double the size of the grammar, which makes is impractical to repeat more times. At each stage, we run a few iterations of expectation maximization using the algorithm detailed in Saers et al. (2009) for biparsing. For comparison we also bootstrap a comparable ITG that has not had the categories split. Before using either of the bootstrapped ITGs, we eliminate all rules that do not have a probability above a threshold that we fixed to 10−50 . This eliminates the highly unlikely rules from the ITG. For the second stage, we use the iterative rule segmentation learning algorithm driven by minimum conditional description length that we introduced in Section 5. We will try three different variants on this algorithm: one without an ITG to condition on, one conditioned on the chunked ITG, and on"
W13-5703,N10-1050,1,0.830417,"ammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into perfecting existing d"
W13-5703,P09-1104,0,0.0191262,"ill show that we can counter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in thi"
W13-5703,C12-1142,1,0.85922,"Missing"
W13-5703,P11-1064,0,0.0586452,"LEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into perfecting existing decoders, it also prevents you from surpassi"
W13-5703,W13-0806,1,0.776941,"thin the context of relevant related research (Section 2 ), we define the proposed conditional description length approach, which represents the ideal model search (Section 3 ). We then detail the two stages of our proposed learning algorithm, which represents our approximation of the search problem (Sections 4 and 5 ). After the theory we detail the particular experiments we conducted (Section 6 ) and the results from those experiments (Section 7). Finally, we offer some conclusions (Section 8 ). 2 Background Description length has been used before to drive iterative segmenting ITG learning (Saers et al., 2013). We will use their algorithm as our baseline, but the simple mixture model we used then works poorly with our ITG with categories. Instead, we propose a tighter incorporation, where the rule segmenting learning is biased towards rules that are present in the categorized ITG. We refer to this objective as minimizing conditional description length, since technically, the length of the ITG being segmented is conditioned on the categorized ITG. Conditional description length (CDL) is detailed in Section 3. The minimum CDL (MCDL) objective differs from the simple mixture model in that it separates"
W13-5703,P12-1018,0,0.044226,"n the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into perfecting existing decoders, it also prevents you from surpassing them in the long run. T"
W13-5703,P02-1040,0,0.0895003,".. and improved search parameters 0 Figure 2: Rule count versus BLEU scores for the bootstrapped ITG, the pruned bootstrapped ITG and the segmented ITG conditioned on the pruned bootstrapped ITG. lation systems with our in-house ITG decoder. The decoder uses a CKY-style parsing algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) and cube pruning (Chiang, 2007) to integrate the language model scores. For language model, we use a trigram language model trained with the SRILM toolkit (Stolcke, 2002) on the English side of the training corpus. To evaluate the resulting translations, we use BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). to have when translating are not explicitly in the grammar. This is potentially a source of translation mistakes, and to investigate this, we create a mixture model from iterations of the segmenting learning process leading up to the learned ITG. All the above outlined ITGs are trained using the IWSLT07 Chinese–English data set (Fordyce, 2007), which contains 46,867 sentence pairs of training data, and 489 Chinese sentences with 6 English reference translations each as test data; all the sentences are taken from the traveling domain. Since the Chinese is written w"
W13-5703,P10-1017,0,0.0185273,"process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly parsing with the input sentence as a hard constraint, as we do in this paper. Although it allows you to tap into the vast engineering efforts that have gone into"
W13-5703,P08-1012,0,0.0881242,"lations (that do not violate the prerequisite word alignment). We will show that we can counter this by combining different iterations of the same segmentation process into a single grammar, which gives a significant bump in BLEU scores. By insisting on the fundamental machine learning principle of matching the training model to the testing model, we do forfeit the short term boost in BLEU that is typically seen when embedding a learned ITG 27 3 Conditional description length in the midst of the common heuristics employed in statistical machine translation. For example, Cherry and Lin (2007), Zhang et al. (2008), Blunsom et al. (2008), Blunsom et al. (2009), Haghighi et al. (2009), Saers and Wu (2009), Blunsom and Cohn (2010), Burkett et al. (2010), Riesa and Marcu (2010), Saers et al. (2010), Saers and Wu (2011), Neubig et al. (2011), and Neubig et al. (2012) all plug some aspect of the ITGs they learn into training pipelines for existing, mismatched decoders, typically in the form of the word alignment that an ITG imposes on a parallel corpus as it is biparsed. Our own past work has also taken similar approaches, but it is not necessary to do so—instead, any ITG can be used for decoding by directly"
W14-4003,W12-3103,0,0.116296,"porates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and employ expensive linguistic resources such as WordNet or paraphrase tables. The expensive training, tuning and/or running time renders these metrics difficult to use in the SMT training cycle. 3 replace humans with automatic SRL and alignment algorithms. MEANT typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human adequacy judgment, and is relatively easy to port to other languages, requiri"
W14-4003,W14-3348,0,0.0416962,"ditdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Ri"
W14-4003,W07-0738,0,0.459718,"adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity"
W14-4003,2012.eamt-1.64,1,0.919302,"ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference and machine translations, but subject to obeying the semantic frames in both. Resulting improvements support the presumption that ITGs, which constrain the allowable permutations between compositional"
W14-4003,W08-0332,0,0.114202,"high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and employ expensive linguistic"
W14-4003,W06-3114,0,0.111485,"T systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG"
W14-4003,W05-0909,0,0.276417,"who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and"
W14-4003,E06-1032,0,0.101147,"e, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency"
W14-4003,2003.mtsummit-papers.32,0,0.0492008,"sible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is found in previous MT evaluation work. One exception is invWER, proposed by Leusch et al. (2003) and Leusch and Ney (2008). The invWER metric interprets weighted BITGs as a generalization of the Levenshtein edit distance, in which entire segments (blocks) can be inverted, as long as this is done strictly compositionally so as not to violate legal ITG biparse tree structures. The input and output languages are considered to be those of the reference and machine translations, and thus are over the same vocabulary (say,English). At the sentence level, correlation of invWER with human adequacy judgments was found to be among the best. Our current approach differs in several key respects from"
W14-4003,W07-0718,0,0.496708,"Missing"
W14-4003,E06-1031,0,0.492343,")—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER"
W14-4003,W05-0904,0,0.146685,"acy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to mat"
W14-4003,W13-2202,0,0.0494942,"reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-e"
W14-4003,P11-1023,1,0.944104,"on grammars, is able to exploit bracketing ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated non terminal category (Wu, 1995a), so as to produce even higher correlation with human adequacy judgments than any automatic MEANT variants, or other common automatic metrics. We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), our proposed IMEANT metric measures the degree to which the basic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEAN"
W14-4003,1996.amta-1.13,0,0.120255,"n be seen in Figure 3, which shows the result on the same example sentence as in Figure 1. Disregarding the semantic parsing errors arising from the current limitations of automatic SRL tools, the ITG tends to provide clean, sparse alignments for role fillers like the ARG1 of the resumed PRED, preferring to leave tokens like complete and range unaligned instead of aligning them anyway as MEANT’s maximal alignment algorithm tends to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) which also generally produces sparser alignments does not work as well in MEANT, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. For contrast, Figure 4 shows a case where IMEANT appropriately accepts dense alignments. Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. IMEANT always yields top correlations, and is more consistent than either MEANT or its recent cross-lingual XMEANT quality estimation variant. For reference, the human HMEANT upper bound is 0.53 for"
W14-4003,W12-4206,1,0.915736,"Missing"
W14-4003,niessen-etal-2000-evaluation,0,0.583405,"ranslation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and"
W14-4003,2013.mtsummit-papers.12,1,0.803851,"mprove the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and"
W14-4003,W13-2254,1,0.727687,"mprove the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and"
W14-4003,P02-1040,0,0.0984325,"ic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papin"
W14-4003,P13-2067,1,0.947369,"rs (Wu, 1997) to improve the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related"
W14-4003,2013.iwslt-evaluation.5,1,0.888254,"rs (Wu, 1997) to improve the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related"
W14-4003,W11-2112,0,0.235694,"Missing"
W14-4003,W09-3804,1,0.903027,"rminal is A, and R is a set of transduction rules with e ∈ W 0 ∪{ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined using MEANT’s context vector model based lexical similarity measure. To calculate the inside probability (or more(accurately, inside score) of a ) ∗ pair of segments, P A ⇒ e/f|G , we use the algorithm described in Saers et al. (2009). Given this, si,pred and si,j now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations. 4 Experiments In this section we discuss experiments indicating that IMEANT further improves upon MEANT’s 26 ARG0 PRED ARG1 [MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency . [REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work . ARG0 ARG0 pred The level of reduction is conducive to raising the inspection and s"
W14-4003,2006.amta-papers.25,0,0.129607,"essfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do no"
W14-4003,Y12-1062,1,0.928956,"ly two months of SK - II line of products . Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since there is no predicate in the MT output. ∑ prece,f e∈e = ∑ rece,f = si,pred = si,j = max s(e, f ) |e| f ∈f max e∈e precei,pred ,fi,pred + recei,pred ,fi,pred si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been sho"
W14-4003,W12-3107,0,0.556805,"na for nearly two months of SK - II line of products . Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since there is no predicate in the MT output. ∑ prece,f e∈e = ∑ rece,f = si,pred = si,j = max s(e, f ) |e| f ∈f max e∈e precei,pred ,fi,pred + recei,pred ,fi,pred si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fill"
W14-4003,P95-1033,1,0.718211,"eorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference"
W14-4003,W95-0106,1,0.710667,"eorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference"
W14-4003,J97-3002,1,0.672318,"Missing"
W14-4003,P03-1019,0,0.242835,"ANT still aims to evaluate MT output in terms of the degree to which the translation has preserved the essential “who did what to whom,for whom, when, where, how and why” of the foreign input sentence. Unlike MEANT, however, IMEANT aligns and scores under ITG assumptions. MEANT uses a maximum alignment algorithm to align the tokens in the role fillers between the reference and machine translations, and then scores by aggregating the lexical similarities into a phrasal similarity using an f-measure. In contrast, IMEANT aligns and scores by utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). To be precise in this regard, we can see IMEANT as differing from the foregoing description of MEANT in the definition of si,pred and si,j , as follows. The IMEANT metric Although MEANT was previously shown to produce higher correlation with human adequacy judgments compared to other automatic metrics, our error analyses suggest that it still suffers from a common weakness among metrics employing lexical similarity, namely that word/token alignments between the reference and machine translations are severely under constrained. No bijectivity or perm"
W14-4003,W07-0411,0,\N,Missing
W14-4003,W12-3129,1,\N,Missing
W14-4010,P09-1088,0,0.0224651,"be able to induce the desired ITG from the above corpus. The paper is structured so that we start by giving a definition of the grammar formalism we use: ITGs (Section 2). We then describe the notion of description length that we use (Section 3), and how ternary segmentation differs from and complements binary segmentation (Section 4). We then present our induction algorithm (Section 5) and give an example of a run through (Section 6). Finally we offer some concluding remarks (Section 7). There is also the option of implicitly defining all possible grammars, and sample from that distribution. Blunsom et al. (2009) do exactly that; they induce with collapsed Gibbs sampling which keeps one derivation for each training sentence that is altered and then resampled. The operations to change the derivations are split, join, delete and insert. The split-operator corresponds to binary segmentation, the join-operator corresponds to chunking; the delete-operator removes an internal node, resulting in its parent having three children, and the insert-operator allows a parent with three children to be normalized to have only two. The existence of ternary nodes in the derivation means that the learned grammar contain"
W14-4010,J07-2003,0,0.0256093,"h in Top-down Induction of Segmental ITGs Markus Saers Dekai Wu HKUST Human Language Technology Center Department of Computer Science and Engineering Hong Kong University of Science and Technology {masaers|dekai}@cs.ust.hk Abstract of a sequence of segments that can be individually translated and reordered to form the translation. This is counter-intuitive, as the who-did-what-towhoms of a sentence tends to be translated and reordered as units, rather than have their components mixed together. Transduction grammars (Aho and Ullman, 1972; Wu, 1997), also called hierarchical translation models (Chiang, 2007) or synchronous grammars, address this through a mechanism similar to context-free grammars. Inducing a segmental transduction grammar is hard, so the standard practice is to use a similar method as the surfacebased models use to learn the chunks, which is problematic, since that method mostly relies on memorizing the relationships that the mechanics of a compositional model is designed to generalize. A compositional translation model would be able to translate lexical chunks, as well as generalize different kinds of compositions; a segmental transduction grammar captures this by having segmen"
W14-4010,W99-0604,0,0.354448,"Missing"
W14-4010,I13-1165,1,0.734083,"of three sentence pairs with identical structure: Another way to arrive at a segmental ITG is to start with the degenerate chunking case: each sentence pair as a lexical item, and segment the existing lexical rules into shorter rules. Since the start point is the degenerate case when optimizing for data likelihood, this approach requires a different objective function to optimize against. Saers et al. (2013c) proposes to use description length of the model and the data given the model, which is subsequently expressed in a Bayesian form with the addition of a prior over the rule probabilities (Saers and Wu, 2013). The way they generate hypotheses is restricted to segmenting an existing lexical item into two parts, which is problematic, because embedded lexical items are potentially overlooked. he has a red book / han har en röd bok she has a biology book / hon har en biologibok it has begun / det har börjat The main difference is that Swedish concatenates rather than juxtaposes compounds such as biologibok instead of biology book. A bilingual person looking at this corpus would produce bilingual parse trees like those in Figure 1. Inducing this relatively simple segmental ITG from the data is, however"
W14-4010,W09-3804,1,0.898619,"Missing"
W14-4010,P08-1012,0,0.0283478,"chunk. Existing surface-based models (Och et al., 1999) have high recall in capturing the chunks, but tend to over-generate, which leads to big models and low precision. Surface-based models have no concept of hierarchical composition, instead they make the assumption that a sentence consists One natural way would be to start with a tokenbased grammar and chunk adjacent tokens to form segments. The main problem with chunking is that the data becomes more and more likely as the segments get larger, with the degenerate end point of all sentence pairs being memorized lexical items. Zhang et al. (2008) combat this tendency by introducing a sparsity prior over the rule probabilities, and variational Bayes to maximize the posterior probability of the data subject to this symmetric Dirichlet prior. To hypothesize possible chunks, they examine the Viterbi biparse of the existing model. Saers et al. (2012) use the entire parse forest to generate the hypotheses. They also bootstrap the ITG from linear and finite-state transduction grammars (LTGs, Saers (2011), and FSTGs), 86 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86–93, c Octobe"
W14-4010,C12-1142,1,0.856627,"l way would be to start with a tokenbased grammar and chunk adjacent tokens to form segments. The main problem with chunking is that the data becomes more and more likely as the segments get larger, with the degenerate end point of all sentence pairs being memorized lexical items. Zhang et al. (2008) combat this tendency by introducing a sparsity prior over the rule probabilities, and variational Bayes to maximize the posterior probability of the data subject to this symmetric Dirichlet prior. To hypothesize possible chunks, they examine the Viterbi biparse of the existing model. Saers et al. (2012) use the entire parse forest to generate the hypotheses. They also bootstrap the ITG from linear and finite-state transduction grammars (LTGs, Saers (2011), and FSTGs), 86 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86–93, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics rather than initialize the lexical probabilities from IBM models. identical, they already illustrate the common problem of rare embedded correspondences. Imagine a really simple corpus of three sentence pairs with identical structure"
W14-4010,W13-0806,1,0.907575,"nverted order respectively. With straight order, both the L0 and the L1 productions are generated left-to-right, but with inverted order, the L1 production is generated right-to-left. The brackets are frequently left out when there is only one element on the righthand side, which means that S → [A] is shortened to S → A. Like CFGs, ITGs also have a 2-normal form, analogous to the Chomsky normal form for CFGs, where the rules are further restricted to only the following four forms: 3 Description length We follow the definition of description length from Saers et al. (2013b,c,d,a); Saers and Wu (2013), that is: the size of the model is determined by counting the number of symbols needed to encode the rules, and the size of the data given the model is determined by biparsing the data with the model. Formally, given a grammar Φ its description length DL (Φ) is the sum of the length of the symbols needed to serialize the rule set. For convenience later on, the symbols are assumed to be uniformly distributed with a length of −lg N1 bits each (where N is the number of different symbols). The description length of the data D given the model is defined as DL (D|Φ) = −lgP (D|Φ). S → A, A → [BC] ,"
W14-4010,W13-2810,1,0.89492,"nverted order respectively. With straight order, both the L0 and the L1 productions are generated left-to-right, but with inverted order, the L1 production is generated right-to-left. The brackets are frequently left out when there is only one element on the righthand side, which means that S → [A] is shortened to S → A. Like CFGs, ITGs also have a 2-normal form, analogous to the Chomsky normal form for CFGs, where the rules are further restricted to only the following four forms: 3 Description length We follow the definition of description length from Saers et al. (2013b,c,d,a); Saers and Wu (2013), that is: the size of the model is determined by counting the number of symbols needed to encode the rules, and the size of the data given the model is determined by biparsing the data with the model. Formally, given a grammar Φ its description length DL (Φ) is the sum of the length of the symbols needed to serialize the rule set. For convenience later on, the symbols are assumed to be uniformly distributed with a length of −lg N1 bits each (where N is the number of different symbols). The description length of the data D given the model is defined as DL (D|Φ) = −lgP (D|Φ). S → A, A → [BC] ,"
W14-4010,J97-3002,1,0.67298,"Missing"
W14-4010,R13-1077,1,\N,Missing
W14-4719,2012.eamt-1.64,1,0.930826,"Gs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent"
W14-4719,W05-0909,0,0.0608269,"valuation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring"
W14-4719,E06-1032,0,0.0515052,"oo many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a"
W14-4719,W08-0309,0,0.0215813,"e j between the reference and machine translations. 4 Experiments In this section we discuss experiments comparing the four alternative lexical access preference and constraint strategies. 4.1 Experimental setup We compared using the DARPA GALE P2.5 Chinese-English translation test set, as used in Lo and Wu (2011a). The corpus includes the Chinese input sentences, each accompanied by an English reference translation and three participating state-of-the-art MT systems’ output. We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; Macháček and Bojar, 2013), which use Kendall’s τ correlation coefficient, to evaluate the correlation of evaluation metrics against human judgment on ranking the translation adequacy of the three systems’ output. A higher value for Kendall’s τ indicates more similarity to the human adequacy rankings by the evaluation metrics. The range of possible values of Kendall’s τ correlation coefficient is [-1, 1], where 1 means the 148 Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. For reference, the human HMEANT upper bo"
W14-4719,W10-1703,0,0.0746439,"Missing"
W14-4719,W12-3102,0,0.0524454,"Missing"
W14-4719,W06-3114,0,0.0383454,"plicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety"
W14-4719,E06-1031,0,0.030895,"eal-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the bas"
W14-4719,P11-1023,1,0.865963,"e main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST"
W14-4719,W12-4206,1,0.780835,"MT and REF, respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j , all of which employ a standard context vector model of the individual words/tokens in the multiword expressio"
W14-4719,2013.mtsummit-papers.12,1,0.794283,"surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee"
W14-4719,W13-2254,1,0.767435,"surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee"
W14-4719,W12-3129,1,0.780658,"Missing"
W14-4719,P13-2067,1,0.835635,"Missing"
W14-4719,2013.iwslt-evaluation.5,1,0.781022,"Missing"
W14-4719,W13-2202,0,0.0555413,"and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: url{http://creativecommons.org/licenses/by/4.0/ 144 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153, Dublin, Ireland, August 23, 2014. Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using auto"
W14-4719,1996.amta-1.13,0,0.0738935,", 2009). Secondly, the permutation and bijectivity constraints enforced by the ITG provide better leverage to reject token alignments when they are not appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. 5 Conclusion We have compared four alternative lexical access strategies for aggregation, preferences, and constraints in scoring multiword expression associations that are far too numerous to be explicitly enumerated in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words, 149 Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized"
W14-4719,niessen-etal-2000-evaluation,0,0.0616973,"sed performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is prese"
W14-4719,P02-1040,0,0.0917038,"rd expression associations within automatic semantic MT evaluation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation"
W14-4719,N04-1030,0,0.320691,"the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furth"
W14-4719,W09-2304,1,0.930201,"d expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,j as follows. ( ( ))  ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |)  si,pred = si,j = where G R ≡ ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p ([AA] |A) p (e/f |A) = = p (⟨AA⟩|A) = 1 s(e, f ) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e ∈ W 0 ∪ {ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation (or the null token)"
W14-4719,W09-3804,1,0.924813,"general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for e"
W14-4719,2006.amta-papers.25,0,0.030085,"adequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who d"
W14-4719,Y12-1062,1,0.864908,"bel in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j , all of which employ a standard context vector model of the individual words/tokens in the multiword expression arguments between the reference and machine translations, as descibed by Lo et al. (2012) and Tumuluru et al. (2012). 3.1 Bag of words (geometric mean) The original MEANT approaches employed standard a bag-of-words strategy for lexical association. This baseline approach applies no alignment constraints on multiword expressions: ∑ si,pred si,j = = e e ∑ ∑ e∈ei,pred f ∈fi,pred lg(s(e,f )) |ei,pred |·|fi,pred | e∈ei,j ∑ f ∈fi,j lg(s(e,f )) |ei,j |·|fi,j | 146 3.2 Maximum alignment (precision-recall average) In the first maximum alignment based approach we will consider, the definitions of si,pred and si,j are inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length."
W14-4719,P95-1033,1,0.486517,"derings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic f"
W14-4719,W95-0106,1,0.54073,"derings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic f"
W14-4719,J97-3002,1,0.760126,"f precision and recall with a proper f-score. Although this is less consistent with the previous literature, such as Mihalcea et al. (2006), it seems more consistent with the overall f-score based approach of MEANT, and thus we include it in our comparison as a variant of the maximum alignment strategy. 3.4 si,pred = si,j = 2 · precei,pred ,fi,pred · recei,pred ,fi,pred precei,pred ,fi,pred + recei,pred ,fi,pred 2 · precei,j ,fi,j · recei,j ,fi,j precei,j ,fi,j + recei,j ,fi,j Inversion transduction grammar based There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve the accuracy of MT evaluation metrics—despite (1) long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints, and (2) the observation that most current state-of-the-art SMT systems employ ITG decoders. Especially when considering semantic MT metrics, ITGs would seem to be a natural strategy for multiword expression association for several cognitively motivated reasons, having to do with language universal properties of cross-linguistic semantic frame structure. To begin with, it is quite natural to think of s"
W14-4719,P03-1019,0,0.0475249,"s can cause multiword expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,j as follows. ( ( ))  ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |)  si,pred = si,j = where G R ≡ ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p ([AA] |A) p (e/f |A) = = p (⟨AA⟩|A) = 1 s(e, f ) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e ∈ W 0 ∪ {ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation"
W14-4719,W11-2103,0,\N,Missing
W16-1207,J93-2003,0,0.119052,"forms: S → A, A → [BC] , A → ⟨BC⟩, A → e/f, A → e/ϵ, A → ϵ/f where S ∈ N is the start symbol, A, B, C ∈ N are nonterminals, e ∈ W0 is an L0 token, f ∈ W1 is an L1 token, and ϵ is the empty token. A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit. 2 Related work The use of structure as input to the training of a statistical model in machine translation was pioneered by Yamada and Knight (2001), where they extend the IBM model 1 (Brown et al., 1993) to incorporate syntactic features derived from a parse tree on the output language (the input to the noisy channel, but the output of the decoder). The generative story of the model is that an English parse tree has the children of its nodes reordered, gets the option to insert a foreign token to the left or right of any node, and finally have all the English leaf nodes translated. Reading the leaf nodes of the tree in order yields the generated foreign sentence. Being a generative model, it is straight forward to train using EM, which they do. Manual evaluation shows that the word alignments"
W16-1207,D12-1079,0,0.0167389,"sa and Marcu (2010) formulate a discriminative k-best alignment model with hundreds of features that can be used to choose the best alignment that matches a given tree. The method as presented require some analysis on both input and output language for the feature model, as well as a set of hand aligned sentences for training; neither of which are readily available for low resource languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein (2012) take the opposite approach and alter the trees to fit the alignments, as presented, their approach requires parallel treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than building possible output tree"
W16-1207,N10-1015,0,0.0231144,"ments and parse trees. Riesa and Marcu (2010) formulate a discriminative k-best alignment model with hundreds of features that can be used to choose the best alignment that matches a given tree. The method as presented require some analysis on both input and output language for the feature model, as well as a set of hand aligned sentences for training; neither of which are readily available for low resource languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein (2012) take the opposite approach and alter the trees to fit the alignments, as presented, their approach requires parallel treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than"
W16-1207,P07-1003,0,0.0320137,"s the sentences, within a generative model that can be optimized globally across the entire training data. It is possible to alleviate the mismatch between given alignments and parse trees. Riesa and Marcu (2010) formulate a discriminative k-best alignment model with hundreds of features that can be used to choose the best alignment that matches a given tree. The method as presented require some analysis on both input and output language for the feature model, as well as a set of hand aligned sentences for training; neither of which are readily available for low resource languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein (2012) take the opposite approach and alter the trees to fit the alignments, as presented, their approach requires parallel treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn tra"
W16-1207,N04-1035,0,0.0950343,"t of the evaluated 50 sentence pairs perfectly aligned, whereas IBM model 5 got none. There are two key differences between Yamada and Knight (2001) and out model: (a) Their model describes how a foreign sentence is generated from an English parse tree, our model describes how sentence pairs are jointly generated. And (b) their model requires committing to a single English parse tree, our model jointly parses and aligns the sentences, effectively integrating out all parse trees that our grammar allows for the English sentence. Perhaps the most prolific translation model that involves trees is Galley et al. (2004), which learns very complex rules such as (ne VB pas) → (VP (AUX does) (RB not) x2 ) where x2 is a variable binding to the second element in the left-hand 57 side. The method takes a parallel sentence pair where one of the sentences has been parsed, and a word alignment, and produces, for each observed word aligned sentence pair, the minimal set of rules to explain it. This method allows complicated rules to be extracted, and different feature scores to be calculated for the extracted rules. This does, however, come at the cost of not being able to optimize the model in any meaningful way. Ins"
W16-1207,2006.amta-papers.8,0,0.0439611,"urce languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein (2012) take the opposite approach and alter the trees to fit the alignments, as presented, their approach requires parallel treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than building possible output trees. The model has also been generalized so that rules can be extracted from input forests rather than single trees (Mi and Huang, 2008), and the decoder has been extended to accept forests as input (Mi et al., 2008). The rule extraction process still requires a word alignment to be provided. Since our model is based on a grammar rather than parse trees, it essentially integ"
W16-1207,J93-2004,0,0.0531507,"). We implement skipping as implicit, low probability rules on the following forms: A → [A ϵ/f ] , A → [ϵ/f A] , A → [A e/ϵ] , A → [e/ϵ A] This allows the parser to maintain the category, but consume a foreign or English token adjacent to a known constituent. The low probability makes the parser avoid skipping if possible. 4 Experimental Setup To test the induction algorithm, we empirically compare the results of our proposed system to a bracketing ITG induced from the same parallel corpus, but without any prior knowledge of English. To extract a CFG over English, we use the Penn treebank 60 (Marcus et al., 1993), with relative frequencies of the productions as the rule probabilities. As translation dictionary, we use the Chinese–English Translation Lexicon(Huang and Graff, 2002). When transforming the CFG into an ITG (Section 3.1) we divide the probability mass of the CFG-rules uniformly among the ITG rules they spawn. As the small parallel data set we use the IWSLT07 Chinese–English data set (Fordyce, 2007), which contains 46,867 sentence pairs. Chinese sentence are typically written without spaces, so we use a tool (Wu, 1999) to segment it into more “word like” units. We allow the wildcard to match"
W16-1207,D08-1022,0,0.0235409,"el treebanks to train on, which cannot be expected to be available when translating to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than building possible output trees. The model has also been generalized so that rules can be extracted from input forests rather than single trees (Mi and Huang, 2008), and the decoder has been extended to accept forests as input (Mi et al., 2008). The rule extraction process still requires a word alignment to be provided. Since our model is based on a grammar rather than parse trees, it essentially integrates out all trees as well as the alignments needed to perform the rule extraction. There is a difference in that we have an output language grammar, but we still implicitly build input language forests. Our model is related to the grammatical channel model of Wu and Wong (1998) in that the way we set up our initial grammar is similar, but where they then"
W16-1207,P08-1023,0,0.0317867,"to or from a low resource language. It is also possible to learn translation rules focusing on the syntax of the input language rather than output language. Huang et al. (2006) turn the tables, and learn translation rules with the input language parsed. Their approach forces the decoder to commit to a single input tree and then build possible output strings rather than building possible output trees. The model has also been generalized so that rules can be extracted from input forests rather than single trees (Mi and Huang, 2008), and the decoder has been extended to accept forests as input (Mi et al., 2008). The rule extraction process still requires a word alignment to be provided. Since our model is based on a grammar rather than parse trees, it essentially integrates out all trees as well as the alignments needed to perform the rule extraction. There is a difference in that we have an output language grammar, but we still implicitly build input language forests. Our model is related to the grammatical channel model of Wu and Wong (1998) in that the way we set up our initial grammar is similar, but where they then proceed to translate directly with it, induction has just started with our model"
W16-1207,P10-1017,0,0.023479,"scores to be calculated for the extracted rules. This does, however, come at the cost of not being able to optimize the model in any meaningful way. Instead, one has to resort to tuning the feature weights used by the decoder. Any mistakes made by the parser or the automatic word aligner are incorporated into the model without any recourse. In contrast, our model jointly parses the output language and aligns the sentences, within a generative model that can be optimized globally across the entire training data. It is possible to alleviate the mismatch between given alignments and parse trees. Riesa and Marcu (2010) formulate a discriminative k-best alignment model with hundreds of features that can be used to choose the best alignment that matches a given tree. The method as presented require some analysis on both input and output language for the feature model, as well as a set of hand aligned sentences for training; neither of which are readily available for low resource languages. Similarly, DeNero and Klein (2007) describe a model to tailor word alignments to existing syntactic trees; in our model, we instead consider all possible trees allowed by a CFG. Burkett et al. (2010) and Burkett and Klein ("
W16-1207,W09-3805,1,0.778881,"it relates two formal languages to each other. Inversion transductions are generated by Inversion Transduction Grammars, or ITGs (Wu, 1997), which share several traits with CFGs in that the transduction rules have single non-terminals on the left-hand side, and in that there is always a 2-normal form equivalence for every ITG. The latter is quite rare for transduction grammars, and limits the structural differences that can be generated between the languages. These limits in structural differences have been empirically shown to include most of the differences found between natural languages (Søgaard and Wu, 2009), and make efficient processing possible. Formally, an ITG is a tuple ⟨N , W0 , W1 , R, S⟩, where N is a finite nonempty set of nonterminals, W0 is a finite set of terminals in the output language L0 , W1 is a finite set of terminals in the input language L1 , R is a finite nonempty set of inversion transduction rules and S ∈ N is a designated start symbol. An inversion transduction rule is restricted to take one of the following forms: S → [A] , A → [φ+ ] , A → ⟨φ+ ⟩ where S ∈ N is the start symbol, A ∈ N is a nonterminal, and φ+ is a nonempty sequence of nonterminals and biterminals. A biter"
W16-1207,P98-2230,1,0.605963,"une 17, 2016. 2016 Association for Computational Linguistics the input language, and large amounts of resources in the output language (English). The job of a translation system is to produce fluent output that adequately represents the meaning of the input, so a translation system should be biased towards the output language. We do this by basing our ITG model on an English treebank, which allows us to (a) extract a binarized context-free grammar, CFG, and (b) estimate initial probabilities for the structural rules. The stochastic CFG can then be mirrored to form a grammatical channel model (Wu and Wong, 1998). Conventional statistical machine translation, or SMT, systems such as phrase-based SMT rely on large amounts of parallel data to collect statistics over how large chunks translate between two languages. These models are highly specific, and may have two different rules for example, for a long and complicated noun phrase with the determiner and for the very same noun phrase without it. Needless to say this kind of modeling is too wasteful to be of much use when there is very small amounts of parallel data available. Tree-based models, models that allow for chunks containing general categories"
W16-1207,P01-1067,0,0.249215,"where the rules are further restricted to only the following forms: S → A, A → [BC] , A → ⟨BC⟩, A → e/f, A → e/ϵ, A → ϵ/f where S ∈ N is the start symbol, A, B, C ∈ N are nonterminals, e ∈ W0 is an L0 token, f ∈ W1 is an L1 token, and ϵ is the empty token. A bracketing ITG, or BITG, has only one nonterminal symbol (other than the dedicated start symbol), which means that the nonterminals carry no information at all other than the fact that their yields are discrete unit. 2 Related work The use of structure as input to the training of a statistical model in machine translation was pioneered by Yamada and Knight (2001), where they extend the IBM model 1 (Brown et al., 1993) to incorporate syntactic features derived from a parse tree on the output language (the input to the noisy channel, but the output of the decoder). The generative story of the model is that an English parse tree has the children of its nodes reordered, gets the option to insert a foreign token to the left or right of any node, and finally have all the English leaf nodes translated. Reading the leaf nodes of the tree in order yields the generated foreign sentence. Being a generative model, it is straight forward to train using EM, which t"
W16-1207,P08-1012,0,0.0788354,"Missing"
W16-4507,2012.eamt-1.64,1,0.908136,"Missing"
W16-4507,W05-0909,0,0.470778,"penalty on the Chinese-English translation set. cased/uncased BLEU METEOR TER WER PER CDER 16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating m"
W16-4507,2014.iwslt-evaluation.4,1,0.80785,"60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more semantically correct bilingual correlations for low resource languages. 54 Weight 0 0.01 0.1 0.5 0.6 0.9 3 3.1 Table 3: Tuning the error penalty on the Hausa-English translation set. cased/uncased BLEU METEOR TER WER PER CDER 16.60/17.14 44.8/47.8 70.63/69.69 73."
W16-4507,2015.mtsummit-papers.26,1,0.11633,"59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more semantically correct bilingual correlations for low resource languages. 54 Weight 0 0.01 0.1 0.5 0.6 0.9 3 3.1 Table 3: Tuning the error penalty on the Hausa-English translation set. cased/uncased BLEU METEOR TER WER PER CDER 16.60/17.14 44.8/47.8 70.63/69.69 73.16/72.46 58.24/56.77 69.59/68.71 16.83/17.37 43.9/46.7 71.06/70.08 73.62/72."
W16-4507,P09-1088,0,0.0245513,"ted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a bet"
W16-4507,J90-2002,0,0.776613,"hey emphasize issues of efficient generalization as opposed to mere memorization from big data collections. We report results and examples showing that this way for inducing ITGs gives better translation quality compared to the conventional ITG (Saers and Wu, 2009) and GIZA++ (Och and Ney, 2000) alignments. 2 2.1 Related work Alignment Word alignment is considered to be an important step in training machine translation systems, since it helps to learn the correlations between the input and the output languages. Unfortunately, conventional alignments are generally based on training IBM models (Brown et al., 1990), which are known to produce weak word alignment since they allow unstructured movement of words. Then use heuristics to combine alignments of both directions to produce the final alignment. A hidden Markov model (HMM) based alignment was proposed (Vogel et al., 1996), but similarly to IBM models, the objective function uses surface based alignment rather than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit imple"
W16-4507,J93-2003,0,0.0456634,"orm A → [BC] and inverted rules use inverted brackets and take the form A → ⟨BC⟩. Straight transduction rules generate transductions with the same order in L0 and L1 , inverted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a g"
W16-4507,N12-1047,0,0.0504114,"Missing"
W16-4507,W07-0403,0,0.0218618,"er in L0 and L1 , inverted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). Th"
W16-4507,J07-2003,0,0.0868354,"th the same order in L0 and L1 , inverted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 201"
W16-4507,P11-1042,0,0.0212706,"they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of ITG where rule probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Some loglinear models have been proposed to incorporate those features (Dyer et al., 2011). The problem with those approaches is that they require language specific knowledge and that they work better on more morphologically rich languages. Few studies that approximately integrate semantic knowledge in computing word alignment are proposed by Ma et al. (2011) and Songyot and Chiang (2014). However, the former needs to have a prior word alignment learned on lexical words. The authors in the latter model proposed a semantic oriented word alignment. However, the problem is, they need to extract word similarity from the monolingual data for both languages, which is problematic in low r"
W16-4507,J02-3001,0,0.0993818,"the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the conventional syntax-based parsing. Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a g"
W16-4507,P09-1104,0,0.0207323,"hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding"
W16-4507,P07-2045,0,0.00400548,"ts used for our experimental setup. We tried to vary the data size and the language family for tuning the error weight and testing our proposed model to show that our approach is not language dependent and can easily be generalized across languages. We adopted the DARPA LORELEI program approach by using a relatively small Chinese corpus, a medium Hausa corpus and a slightly larger Uzbek corpus, we show that our approach is able to learn from small to medium datasets and does not rely on heavy memorization. We tested the different alignments described above by using the standard MOSES toolkit (Koehn et al., 2007), and a 4-gram language model learned with the SRI language model toolkit (Stolcke, 2002) trained on the training data of each language respectively. To tune the loglinear mixture weights, we use k-best MIRA (Cherry and Foster, 2012), a version of margin-based classification algorithm or MIRA (Chiang, 2012). 56 4 Results We compared the performance of the semantic frame based BITG alignments against both the conventional token based BITG alignments and the traditional GIZA++ alignments. We evaluated our MT output using the surface based evaluation metrics BLEU (Papineni et al., 2002), METEOR ("
W16-4507,E06-1031,0,0.0949572,"ranslation set. cased/uncased BLEU METEOR TER WER PER CDER 16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic informat"
W16-4507,P11-1023,1,0.929401,"2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the conventional syntax-based parsing. Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation met"
W16-4507,W12-4206,1,0.897757,"Missing"
W16-4507,2013.mtsummit-papers.12,1,0.921253,". Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure defined in (Pradhan et al., 2004). Recent works have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than common evaluation metrics (Lo and Wu, 2011, 2012; 53 Figure 1: Token based BITG induction algorithm. Weight 0 0.01 0.1 0."
W16-4507,W13-2254,1,0.915683,". Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure defined in (Pradhan et al., 2004). Recent works have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than common evaluation metrics (Lo and Wu, 2011, 2012; 53 Figure 1: Token based BITG induction algorithm. Weight 0 0.01 0.1 0."
W16-4507,W12-3129,1,0.891634,"Missing"
W16-4507,P13-2067,1,0.832594,"8/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more semantically correct bilingual correlations for low resource languages. 54 Weight 0 0.01 0.1 0.5 0.6 0.9 3 3.1 Table 3: Tuning the error penalty on the Hausa-English translation set. cased/uncased BLEU"
W16-4507,2013.iwslt-evaluation.5,1,0.846151,"8/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more semantically correct bilingual correlations for low resource languages. 54 Weight 0 0.01 0.1 0.5 0.6 0.9 3 3.1 Table 3: Tuning the error penalty on the Hausa-English translation set. cased/uncased BLEU"
W16-4507,2011.mtsummit-papers.41,0,0.0371905,"le probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Some loglinear models have been proposed to incorporate those features (Dyer et al., 2011). The problem with those approaches is that they require language specific knowledge and that they work better on more morphologically rich languages. Few studies that approximately integrate semantic knowledge in computing word alignment are proposed by Ma et al. (2011) and Songyot and Chiang (2014). However, the former needs to have a prior word alignment learned on lexical words. The authors in the latter model proposed a semantic oriented word alignment. However, the problem is, they need to extract word similarity from the monolingual data for both languages, which is problematic in low resource conditions, then produce alignments using word similarities. 2.2 Inversion transduction grammars Inversion transduction grammars, or ITGs, (Wu, 1997) are by definition a subset of syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972"
W16-4507,W13-2202,0,0.0131989,"s in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can successfully understand the general meaning of the input sentence as captured by the basic event structure defined in (Pradhan et al., 2004). Recent works have shown that the semantic frame based metric, MEANT, correlates better with human adequacy judgment than common evaluation metrics (Lo and Wu, 2011, 2012; 53 Figure 1: Token based BITG induction algorithm. Weight 0 0.01 0.1 0.5 0.6 0.9 Table 2: Tuning th"
W16-4507,P11-1064,0,0.0187818,"der. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the c"
W16-4507,niessen-etal-2000-evaluation,0,0.255367,"ed BLEU METEOR TER WER PER CDER 16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT syst"
W16-4507,P00-1056,0,0.3775,"ge amount of parallel corpora. We believe that low resource conditions are 51 Proceedings of the Sixth Workshop on Hybrid Approaches to Translation, pages 51–60, Osaka, Japan, December 11, 2016. more interesting than high resource conditions because they are both scientifically and socioeconomically more interesting as they emphasize issues of efficient generalization as opposed to mere memorization from big data collections. We report results and examples showing that this way for inducing ITGs gives better translation quality compared to the conventional ITG (Saers and Wu, 2009) and GIZA++ (Och and Ney, 2000) alignments. 2 2.1 Related work Alignment Word alignment is considered to be an important step in training machine translation systems, since it helps to learn the correlations between the input and the output languages. Unfortunately, conventional alignments are generally based on training IBM models (Brown et al., 1990), which are known to produce weak word alignment since they allow unstructured movement of words. Then use heuristics to combine alignments of both directions to produce the final alignment. A hidden Markov model (HMM) based alignment was proposed (Vogel et al., 1996), but sim"
W16-4507,J05-1004,0,0.00819179,"(SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the conventional syntax-based parsing. Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semantic evaluation metrics that correlate more closely with human adequacy judgements than the commonly used surface based metrics (Lo and Wu, 2011, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Unlike n-gram or edit-distance based metrics, the MEANT family of metrics (Lo and Wu, 2011, 2012; Lo et al., 2012) adopt the principle that a good translation is one in which humans can"
W16-4507,P02-1040,0,0.0969543,". Weight 0 0.01 0.1 0.5 0.6 0.9 Table 2: Tuning the error penalty on the Chinese-English translation set. cased/uncased BLEU METEOR TER WER PER CDER 16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the"
W16-4507,N04-1030,0,0.0713533,"5); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a given sentence than the conventional syntax-based parsing. Recent approaches in semantic role labeling use unsupervised machine learning techniques to automatically find the semantic roles. They generally use FrameNet (Gildea and Jurafsky, 2002) or Proposition Bank (Palmer et al., 2005) notation to specify what a predicate is and what the other arguments are. The most recent research that include SRL in the SMT pipeline was done for MT evaluation. The MEANT family of metrics are semant"
W16-4507,W09-2304,1,0.807729,"ons without memorizing from a huge amount of parallel corpora. We believe that low resource conditions are 51 Proceedings of the Sixth Workshop on Hybrid Approaches to Translation, pages 51–60, Osaka, Japan, December 11, 2016. more interesting than high resource conditions because they are both scientifically and socioeconomically more interesting as they emphasize issues of efficient generalization as opposed to mere memorization from big data collections. We report results and examples showing that this way for inducing ITGs gives better translation quality compared to the conventional ITG (Saers and Wu, 2009) and GIZA++ (Och and Ney, 2000) alignments. 2 2.1 Related work Alignment Word alignment is considered to be an important step in training machine translation systems, since it helps to learn the correlations between the input and the output languages. Unfortunately, conventional alignments are generally based on training IBM models (Brown et al., 1990), which are known to produce weak word alignment since they allow unstructured movement of words. Then use heuristics to combine alignments of both directions to produce the final alignment. A hidden Markov model (HMM) based alignment was propose"
W16-4507,W09-3804,1,0.916681,"e based alignment rather than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit implements both IBM and HMM models described above. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 ). A more efficient version that runs in O(n3 ) was proposed later (Saers et al., 2009). Zens and Ney (2003) show that ITG constraints allow a higher flexibility in word ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of ITG where rule probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work"
W16-4507,N10-1050,1,0.827079,"ion in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pradhan et al., 2004; Lo and Wu, 2011, 2012; Lo et al., 2012). This approach gives a better way of understanding the meaning of a giv"
W16-4507,2006.amta-papers.25,0,0.298353,"16.29/16.63 36.9/38.9 69.09/68.69 71.34/71.03 60.78/60.22 67.89/67.44 15.93/16.34 36.4/38.6 69.14/68.77 71.80/71.42 60.99/60.43 68.29/67.87 15.77/15.99 37.0/38.9 69.30/68.90 71.85/71.48 60.46/59.90 68.18/67.76 16.90/17.19 37.9/40.1 68.85/68.53 71.53/71.26 60.14/59.61 67.44/67.18 17.06/17.38 38.0/40.1 68.69/68.32 71.48/71.16 59.87/59.34 67.47/67.12 16.34/16.60 37.4/39.3 69.80/69.33 72.33/71.96 60.75/60.19 68.58/68.18 Lo et al., 2012) such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). It has been shown that including semantic role labeling in the training pipeline by tuning against a semantic frame objective function such as the semantic evaluation metric MEANT (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b; Beloucif et al., 2014) significantly improves the quality of the MT output. Beloucif et al. (2015) showed that injecting a crosslingual objective function into the training pipeline helps to improve the quality of the word alignment. We argue in this paper that incorporating monolingual semantic information while training SMT systems can help to learn more sema"
W16-4507,D14-1197,0,0.0136424,"lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Some loglinear models have been proposed to incorporate those features (Dyer et al., 2011). The problem with those approaches is that they require language specific knowledge and that they work better on more morphologically rich languages. Few studies that approximately integrate semantic knowledge in computing word alignment are proposed by Ma et al. (2011) and Songyot and Chiang (2014). However, the former needs to have a prior word alignment learned on lexical words. The authors in the latter model proposed a semantic oriented word alignment. However, the problem is, they need to extract word similarity from the monolingual data for both languages, which is problematic in low resource conditions, then produce alignments using word similarities. 2.2 Inversion transduction grammars Inversion transduction grammars, or ITGs, (Wu, 1997) are by definition a subset of syntax-directed transduction grammar (Lewis and Stearns, 1968; Aho and Ullman, 1972). A transduction is a set of"
W16-4507,C96-2141,0,0.514981,"d GIZA++ (Och and Ney, 2000) alignments. 2 2.1 Related work Alignment Word alignment is considered to be an important step in training machine translation systems, since it helps to learn the correlations between the input and the output languages. Unfortunately, conventional alignments are generally based on training IBM models (Brown et al., 1990), which are known to produce weak word alignment since they allow unstructured movement of words. Then use heuristics to combine alignments of both directions to produce the final alignment. A hidden Markov model (HMM) based alignment was proposed (Vogel et al., 1996), but similarly to IBM models, the objective function uses surface based alignment rather than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit implements both IBM and HMM models described above. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 )."
W16-4507,W95-0106,1,0.607333,"tion rules generate transductions with the same order in L0 and L1 , inverted rules on the other hand, generate transduction in an inverted order. This means that, in the parse tree, the children instantiated by straight rules are read in the same order and children instantiated in an inverted order are read in an inverted order in L1 . The rule probability function p is initialized using uniform probabilities for the structural rules, and a translation table t that is trained using IBM model 1 (Brown et al., 1993) in both directions. There are also many ways to formulate the model over ITGs: Wu (1995); Zhang and Gildea (2005); Chiang (2007); Cherry and Lin (2007); Blunsom et al. (2009); Haghighi et al. (2009); Saers et al. (2010); Neubig et al. (2011). In this work, we use BITGs or bracketing transduction grammars (Saers et al., 2009) which only use one single nonterminal category and surprisingly achieve good results. 2.3 Semantic frames in the MT training pipeline Semantic role labeling (SRL) is an important task in natural language processing since it helps to define the basic event structure in a given sentence: who did what to whom, for whom, when, where, how and why as defined in (Pr"
W16-4507,J97-3002,1,0.754102,"duce the final alignment. A hidden Markov model (HMM) based alignment was proposed (Vogel et al., 1996), but similarly to IBM models, the objective function uses surface based alignment rather than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit implements both IBM and HMM models described above. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 ). A more efficient version that runs in O(n3 ) was proposed later (Saers et al., 2009). Zens and Ney (2003) show that ITG constraints allow a higher flexibility in word ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of IT"
W16-4507,P03-1019,0,0.0446594,"her than a more structure based alignment. No constraints are used while training, allowing any random word-to-word permutations. Such an alignment generally hurts the translation accuracy. The traditional GIZA++ (Och and Ney, 2000) toolkit implements both IBM and HMM models described above. Saers and Wu (2009) proposed a better method of producing word alignment by training inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 ). A more efficient version that runs in O(n3 ) was proposed later (Saers et al., 2009). Zens and Ney (2003) show that ITG constraints allow a higher flexibility in word ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of ITG where rule probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment use"
W16-4507,P05-1059,0,0.114782,"aining inversion transduction grammars (Wu, 1997). One problem encountered with such a model was the exhaustive biparsing that runs in O(n6 ). A more efficient version that runs in O(n3 ) was proposed later (Saers et al., 2009). Zens and Ney (2003) show that ITG constraints allow a higher flexibility in word ordering for longer sentences than the conventional IBM model. Furthermore, they demonstrate that applying ITG constraints for word alignment leads to learning a significantly better alignment than the constraints used in conventional IBM models for both German-English and French-English. Zhang and Gildea (2005) presented a version of ITG where rule probabilities are lexicalized throughout the synchronous parse tree for efficient training which helped to align sentences up to 15 words. Some of the previous work on word alignment used morphological and syntactic features (De Gispert et al., 2006). Some loglinear models have been proposed to incorporate those features (Dyer et al., 2011). The problem with those approaches is that they require language specific knowledge and that they work better on more morphologically rich languages. Few studies that approximately integrate semantic knowledge in compu"
