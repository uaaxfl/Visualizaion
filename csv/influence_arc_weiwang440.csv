2003.mtsummit-papers.23,C00-2090,0,0.121507,"y example, they have the opportunity to be combined to one chunk. We express the procedure of getting ~ s &apos;1K &apos; → ~ s1K |E) . Pr(s1K |~ s &apos;1K &apos; , E) with function F (~ Consider the data sparseness problem, submit to losing some of linguistic constraints, we get a back off function L(~s &apos;1K&apos; → ~s1K |E) which is based on a weighted length to substitute the function F. We have: (2) example based chunk combination In base chunking, we follow the SL monolingual chunking model that is introduced by Wang et al. (2002), in which 13 chunk types were used for English, which is the same with Erik et al. (2000). ~ t1 K and its example set e with equation (3). The ~ s K and the best example set best translation tˆ K of ~ 1 1 ê for the translation should gain the highest Pr(e, ~ t1 K |~ s1K , e’). Pr(e, ~ t1 K |~ s1K , e &apos; ) = ∏ Pr(e~sk |~ sk , e &apos; ) ⋅ Pr(~ tk |~ s , e~sk ) 424 4 3 14 42k44 3 k 14 exampleselection model translation model (3) There are an example selection model and a phrase-based translation model in the example-based translation model. The distortion model is not adopted because our model is for a TMS and only partial translation will be provided to users. Example selection model Giv"
2003.mtsummit-papers.23,2002.tmi-tutorials.1,0,0.0310936,"Missing"
2011.jeptalnrecital-court.8,P08-1004,0,0.0388798,"Missing"
2011.jeptalnrecital-court.8,P04-1053,0,0.105174,"Missing"
2011.jeptalnrecital-court.8,P06-2094,0,0.0457351,"Missing"
2011.jeptalnrecital-court.8,N06-1039,0,0.059062,"Missing"
2012.amta-papers.18,2010.amta-papers.16,0,0.292091,"em to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of domain-specific data obtain"
2012.amta-papers.18,D08-1064,0,0.129598,"optimization problem, this objective does not explicitly take genre into account. However, the weights of different domains, {λdi , d = 1 . . . D, i = 1 . . . Id }, are decoupled (by the runtime feature name re-labeling (Section 3)) in the n-best lists, rather than being shared, thus the weight optimization for one domain can concentrate on the domain itself, without being constrained too much by any other domain. Since BLEU is not decomposable at the sentence level, this objective generally can not guarantee maximized BLEUs on respective domain partitions, but rather an optimal overall BLEU (Chiang et al., 2008). Another optimization objective is the max BLEU sum that maximizes the sum of BLEUs of the individual genre partitions: ! D X BLEU(Sd ) (6) max {λdi ,d=1...D,i=1...Id } d=1 When none of the features is shared in the n-best lists across different domains (which is our case), this objective is equivalent to the summation of the individually maximized BLEUs: ! D X max BLEU(Sd ) (7) d=1 {λdi ,i=1...Id } There could be other genre-aware tuning objectives that mix (or “nest”) the above two.1 But in our paper, we are mainly interested in the experimental comparison between the max joint BLEU and the"
2012.amta-papers.18,P11-2080,0,0.0600089,"is no explicit treatment on genre, but there is a subtlety: Features are decoupled in n-best lists across domains and this gives MERT the freedom to adjust weights for one domain without much constraints from any other domain. In other words, MERT is made implicitly genre-aware. Actually, we can further tailor the MERT tuning objectives to be explicitly aware of genre. We explain alternative genre-aware tuning objectives that the classical MERT can be altered to adopt in a later section and compare their effects experimentally. Daume III (2007) does domain adaptation by augumenting features. Chiang et al. (2011) improve lexical smoothing also by augmenting features refined by genres. In our approach, the n-best lists for tuning can be viewed as containing augmented features as well. But we augment features in order to decouple them across domains, rather than providing refined domain/genre bias in the model. The language model feature deserves further explanation. Even though we do not have domain specific features in the translation model, we have domain specific language models. In our approach, the generic language model is used by different domains, but a domain language model is turned on only i"
2012.amta-papers.18,P07-1033,0,0.262823,"Missing"
2012.amta-papers.18,W07-0717,0,0.271504,"n system that is capable of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach unifies automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability. 1 Introduction Research in domain adaption for machine translation (MT) has been mostly focusing on one domain. Various methods have been proposed to make a system work best on a resource-scarce domain when most of the training data is from another open, resource-rich domain, e.g., (Foster et al., 2010; Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Decent improvements have been made on domain translation accuracy, but often, accuracy improvements for one domain are obtained at the expense of accuracy losses in another (e.g., the background) domain. With these methods, it remains unaddressed how they can be generalized to work equally well with more than one domains at the same time. One could trivially build one system/model per domain, but that does not scale and will require manual domain detection if the incoming texts belong to heterogeneous domains. So far, there has been little work on better infrastru"
2012.amta-papers.18,D10-1044,0,0.18498,"em to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of domain-specific data obtain"
2012.amta-papers.18,W07-0733,0,0.499117,"e of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach unifies automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability. 1 Introduction Research in domain adaption for machine translation (MT) has been mostly focusing on one domain. Various methods have been proposed to make a system work best on a resource-scarce domain when most of the training data is from another open, resource-rich domain, e.g., (Foster et al., 2010; Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Decent improvements have been made on domain translation accuracy, but often, accuracy improvements for one domain are obtained at the expense of accuracy losses in another (e.g., the background) domain. With these methods, it remains unaddressed how they can be generalized to work equally well with more than one domains at the same time. One could trivially build one system/model per domain, but that does not scale and will require manual domain detection if the incoming texts belong to heterogeneous domains. So far, there has been little work on better infrastructure for building and deplo"
2012.amta-papers.18,N03-1017,1,0.0230989,"Missing"
2012.amta-papers.18,W04-3250,0,0.165773,"Missing"
2012.amta-papers.18,D08-1076,1,0.891121,"Missing"
2012.amta-papers.18,P03-1021,1,0.022694,") d(f ) eˆ = arg max λi · hi (f, e) (2) e i=1 Generalizing decoding for genre awareness in turn makes tuning genre-aware. Our tuning development set consists of sentences from D domains. And we do not need to know the domain for each sentence beforehand. The genre-aware decoding on the entire set automatically classifies it into D partitions. The runtime feature re-labeling makes each partition, Sd , d = 0, . . . , D − 1, have its own set of features that are decoupled from other domains. Therefore the system has D sets of features in total. We then can use Minimum Error Rate Training (MERT) (Och, 2003) out of the box to learn the weights for all these features in a single MERT run which maximizes the overall BLEU of the entire genre-mixed development set: max {λdi ,d=1...D,i=1...Id } BLEU(∪D−1 d=0 Sd ) (3) In this formula, there is no explicit treatment on genre, but there is a subtlety: Features are decoupled in n-best lists across domains and this gives MERT the freedom to adjust weights for one domain without much constraints from any other domain. In other words, MERT is made implicitly genre-aware. Actually, we can further tailor the MERT tuning objectives to be explicitly aware of gen"
2012.amta-papers.18,2007.mtsummit-papers.68,0,0.458729,"in detector, to generalize an MT system to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of"
2012.amta-papers.18,N09-1028,1,0.895385,"Missing"
2020.aacl-main.45,D18-2029,1,0.861593,"Missing"
2020.aacl-main.45,W19-5435,0,0.0151623,"from the internet. Much of the gains in the downstream evaluation are achieved in the first iteration of the method, but later iterations keep improving the D-E models. Despite being self-supervised, our methods show competitive performance when compared against a de-noising method that uses supervision. 2 Related Work One line of the research that directly relates to our work is corpus filtering for training NMT models. Below we classify the related work into two categories depending on the amount of supervision needed (e.g. high quality parallel texts). erate embeddings and score the data (Chaudhary et al., 2019). Wang et al. (2018b) use two NMT models taken from two training epochs to decide which data to use in order to improve the training efficiency and to show a de-noising effect. Our methods here try to take advantages of all of these approaches. Koehn et al. (2018) and Koehn et al. (2019) summarize findings of the WMT corpus filtering efforts, though our work here primarily examines a self-supervised method in the context of de-noising, rather than on a targeted filtering effort. Our methods are unsupervised. We use dualencoder models, rather than an encoder-decoder architecture, to model pairw"
2020.aacl-main.45,K16-1031,0,0.0254653,"the context of de-noising, rather than on a targeted filtering effort. Our methods are unsupervised. We use dualencoder models, rather than an encoder-decoder architecture, to model pairwise data and let the model self-supervise itself or, further, be co-trained with an NMT model to refine the training data. (Semi-)Supervised Methods Some data denoising methods simply use filtering rules or heuristics such as language identification of both the source and target texts, vocabulary checks, language model (syntactic) verification, and so on. In contrast to rule-based approaches, approaches like Chen and Huang (2016) and Wang et al. (2018c) train classifiers to distinguish in-domain vs. outof-domain (or clean vs. noisy) data with a small parallel corpus, while other approaches build reference models on larger amounts of high-quality data (Junczys-Dowmunt, 2018; Defauw et al., 2019). There are approaches that combine rules and heuristics with probabilistic models to determine the amount of noise in each sentence pair. In some cases these systems are designed as targeted efforts to denoise a particular dataset. Bicleaner (S´anchez-Cartagena et al., 2018), in relationship to the ParaCrawl (Espl`a et al., 201"
2020.aacl-main.45,W19-6721,0,0.0465895,"Missing"
2020.aacl-main.45,K19-1049,0,0.0205622,"ethods, unsupervised methods do not require good-quality data to be available. Recent work (Zhang et al., 2020) leverages pre-trained language models and synthetic data (Vyas et al., 2018), in place of true supervision. Some efforts focus on using monolingual corpora and align them through bootstrapping in order to generate sentence pairs (Tran et al., 2020; Ruiter et al., 2020), while others train a model with noisy data directly to gen436 3 Dual-Encoder Model Dual-encoder (D-E) models have demonstrated to be an effective learning framework applied to both supervised (Henderson et al., 2017; Gillick et al., 2019) and unsupervised tasks (Cer et al., 2018; Chidambaram et al., 2018). A multi-task D-E model consists of two encoders and a combination function for each of the tasks. In the context of the D-E framework, the selection of bilingual text can be interpreted as a ranking problem where, with yi as the true target of source sentence xi , P (yi |xi ) is ranked above all the other target candidates in Y. P (yi |xi ) can be expressed as a log-linear model but, for practical reasons, we approximate the full set of target candidates Y with a sample (Henderson et al., 2017). When training in a batch, P ("
2020.aacl-main.45,W18-6317,1,0.895447,"Missing"
2020.aacl-main.45,W18-6478,0,0.0174558,", be co-trained with an NMT model to refine the training data. (Semi-)Supervised Methods Some data denoising methods simply use filtering rules or heuristics such as language identification of both the source and target texts, vocabulary checks, language model (syntactic) verification, and so on. In contrast to rule-based approaches, approaches like Chen and Huang (2016) and Wang et al. (2018c) train classifiers to distinguish in-domain vs. outof-domain (or clean vs. noisy) data with a small parallel corpus, while other approaches build reference models on larger amounts of high-quality data (Junczys-Dowmunt, 2018; Defauw et al., 2019). There are approaches that combine rules and heuristics with probabilistic models to determine the amount of noise in each sentence pair. In some cases these systems are designed as targeted efforts to denoise a particular dataset. Bicleaner (S´anchez-Cartagena et al., 2018), in relationship to the ParaCrawl (Espl`a et al., 2019) data, is an example of that approach. Unsupervised Methods In contrast to the supervised methods, unsupervised methods do not require good-quality data to be available. Recent work (Zhang et al., 2020) leverages pre-trained language models and s"
2020.aacl-main.45,W19-5404,0,0.0148946,"s supervision. 2 Related Work One line of the research that directly relates to our work is corpus filtering for training NMT models. Below we classify the related work into two categories depending on the amount of supervision needed (e.g. high quality parallel texts). erate embeddings and score the data (Chaudhary et al., 2019). Wang et al. (2018b) use two NMT models taken from two training epochs to decide which data to use in order to improve the training efficiency and to show a de-noising effect. Our methods here try to take advantages of all of these approaches. Koehn et al. (2018) and Koehn et al. (2019) summarize findings of the WMT corpus filtering efforts, though our work here primarily examines a self-supervised method in the context of de-noising, rather than on a targeted filtering effort. Our methods are unsupervised. We use dualencoder models, rather than an encoder-decoder architecture, to model pairwise data and let the model self-supervise itself or, further, be co-trained with an NMT model to refine the training data. (Semi-)Supervised Methods Some data denoising methods simply use filtering rules or heuristics such as language identification of both the source and target texts, v"
2020.aacl-main.45,W18-6453,0,0.0212851,"-noising method that uses supervision. 2 Related Work One line of the research that directly relates to our work is corpus filtering for training NMT models. Below we classify the related work into two categories depending on the amount of supervision needed (e.g. high quality parallel texts). erate embeddings and score the data (Chaudhary et al., 2019). Wang et al. (2018b) use two NMT models taken from two training epochs to decide which data to use in order to improve the training efficiency and to show a de-noising effect. Our methods here try to take advantages of all of these approaches. Koehn et al. (2018) and Koehn et al. (2019) summarize findings of the WMT corpus filtering efforts, though our work here primarily examines a self-supervised method in the context of de-noising, rather than on a targeted filtering effort. Our methods are unsupervised. We use dualencoder models, rather than an encoder-decoder architecture, to model pairwise data and let the model self-supervise itself or, further, be co-trained with an NMT model to refine the training data. (Semi-)Supervised Methods Some data denoising methods simply use filtering rules or heuristics such as language identification of both the so"
2020.aacl-main.45,D18-2012,0,0.0221895,"ay to generate training data. One can anticipate that the models are prone to mimic the training data, including the noise. Just as in our first method, we break the cycle by adding a selection step based on the D-E scores and using only the top-ranking data to train the next NMT model. 5 Experimental Setup Machine Translation Model To assess if we can recover useful subsets from noisy data, we train Transformer-Big (Vaswani et al., 2017) NMT models using data refined with our methods. To train the models, we split the source and target texts into pieces using bilingual sentence piece models (Kudo and Richardson, 2018) that were trained with the ParaCrawl v1.0 data only. We train for a maximum of 200k steps using (Shazeer and Stern, 2018) and pick the best checkpoint according to the performance on a validation set. The models are trained on Google’s Cloud TPU v3 with batch size 3072. In all our experiments, the configuration of the NMT models is kept the same with the only difference being the training data. All sentence pairs Pre-filtered 70th percentile (for NMT) 80th percentile (for D-E) 1: 3: 4: 5: 6: 7: 8: 9: 10: τ ← selection threshold NMT = TrainNMT(data) while D-E improves or NMT improves do transl"
2020.aacl-main.45,P02-1040,0,0.112483,"nclude in table 2 the AUCPR and F1 from embeddings generated with the public “universal-sentence-encodermultilingual-large” v2 (Yang et al., 2019b) from TFHub1 to show the performance of a D-E model trained on multiple large and non-public industry datasets. As expected, training on this kind of data is far better than de-noising, but the evaluation shows that our methods do a good job refining data, especially considering how much noise there is in the ParaCrawl datasets to start with. 6.2 Translation Evaluation To illustrate the end-performance of our methods, table 3 shows the BLEU scores (Papineni et al., 2002) of NMT models trained with data subsets selected with our methods. The D-E models used to score the data in each iteration correspond to the same models reported in table 2. As baseline we use an NMT model trained with all the sentence pairs just after pre-filtering, i.e. selection is not used yet. For both our methods the NMT models show considerable improvement over the baseline. It is interesting that the initial NMT (IF0 in table 3), shows good improvement in spite of using a D-E whose only difference over baseline is the use of hard negatives. There is also noticeable improvement between"
2020.aacl-main.45,P99-1068,0,0.106828,"tures the relationship in two modalities, is used to train deep learning models such as Neural Machine Translation (NMT) (Wu et al., 2016), Question Answering (Wang et al., 2007), Image Captioning (Sharma et al., 2018), etc. To train this kind of models, large-scale data can often be obtained from weak signals like text cooccurrence (Yang et al., 2018) or dictionary n-gram matching (Uszkoreit et al., 2010). For example, in the machine translation community, the large amount of multilingual text available on the internet has naturally led to the idea of using internet data to train NMT models (Resnik, 1999). This approach has proven advantageous but it has the drawback that data mined this way is intrinsically noisy (Resnik and Smith, 2003). Despite the poor quality, usually this kind of data contains a helpful subset that can be recovered through a process of data cleaning or refinement. Data cleaning could be implemented with linguistic knowledge such as its script, vocabulary, syntax, etc. Alternatively, a model can be trained on “clean” or “trusted” pairs that are verified through manual annotation. Both options can be highly effective, but the former is limited in scope and error-prone, whi"
2020.aacl-main.45,J03-3002,0,0.0223938,"t al., 2016), Question Answering (Wang et al., 2007), Image Captioning (Sharma et al., 2018), etc. To train this kind of models, large-scale data can often be obtained from weak signals like text cooccurrence (Yang et al., 2018) or dictionary n-gram matching (Uszkoreit et al., 2010). For example, in the machine translation community, the large amount of multilingual text available on the internet has naturally led to the idea of using internet data to train NMT models (Resnik, 1999). This approach has proven advantageous but it has the drawback that data mined this way is intrinsically noisy (Resnik and Smith, 2003). Despite the poor quality, usually this kind of data contains a helpful subset that can be recovered through a process of data cleaning or refinement. Data cleaning could be implemented with linguistic knowledge such as its script, vocabulary, syntax, etc. Alternatively, a model can be trained on “clean” or “trusted” pairs that are verified through manual annotation. Both options can be highly effective, but the former is limited in scope and error-prone, while the latter can be costly due to the number of required annotated examples. In this paper we introduce two self-supervised methods to"
2020.aacl-main.45,2020.emnlp-main.202,0,0.0699929,"Missing"
2020.aacl-main.45,D07-1003,0,0.00991227,"ly adopted and have demonstrated their usefulness in many areas and applications. Despite their diversity, one common characteristic of these models is the large number of parameters that need to be adjusted during training (some recent models that have billions of parameters include T5 (Raffel et al., 2019) and GPT2 (Radford et al., 2019)). This leads to the need of collecting large amounts of training examples. Pairwise data, that captures the relationship in two modalities, is used to train deep learning models such as Neural Machine Translation (NMT) (Wu et al., 2016), Question Answering (Wang et al., 2007), Image Captioning (Sharma et al., 2018), etc. To train this kind of models, large-scale data can often be obtained from weak signals like text cooccurrence (Yang et al., 2018) or dictionary n-gram matching (Uszkoreit et al., 2010). For example, in the machine translation community, the large amount of multilingual text available on the internet has naturally led to the idea of using internet data to train NMT models (Resnik, 1999). This approach has proven advantageous but it has the drawback that data mined this way is intrinsically noisy (Resnik and Smith, 2003). Despite the poor quality, u"
2020.aacl-main.45,W18-6488,0,0.0463484,"Missing"
2020.aacl-main.45,P18-2048,0,0.112707,"of the gains in the downstream evaluation are achieved in the first iteration of the method, but later iterations keep improving the D-E models. Despite being self-supervised, our methods show competitive performance when compared against a de-noising method that uses supervision. 2 Related Work One line of the research that directly relates to our work is corpus filtering for training NMT models. Below we classify the related work into two categories depending on the amount of supervision needed (e.g. high quality parallel texts). erate embeddings and score the data (Chaudhary et al., 2019). Wang et al. (2018b) use two NMT models taken from two training epochs to decide which data to use in order to improve the training efficiency and to show a de-noising effect. Our methods here try to take advantages of all of these approaches. Koehn et al. (2018) and Koehn et al. (2019) summarize findings of the WMT corpus filtering efforts, though our work here primarily examines a self-supervised method in the context of de-noising, rather than on a targeted filtering effort. Our methods are unsupervised. We use dualencoder models, rather than an encoder-decoder architecture, to model pairwise data and let th"
2020.aacl-main.45,P18-1238,0,0.0413801,"Missing"
2020.aacl-main.45,W18-6314,1,0.928625,"of the gains in the downstream evaluation are achieved in the first iteration of the method, but later iterations keep improving the D-E models. Despite being self-supervised, our methods show competitive performance when compared against a de-noising method that uses supervision. 2 Related Work One line of the research that directly relates to our work is corpus filtering for training NMT models. Below we classify the related work into two categories depending on the amount of supervision needed (e.g. high quality parallel texts). erate embeddings and score the data (Chaudhary et al., 2019). Wang et al. (2018b) use two NMT models taken from two training epochs to decide which data to use in order to improve the training efficiency and to show a de-noising effect. Our methods here try to take advantages of all of these approaches. Koehn et al. (2018) and Koehn et al. (2019) summarize findings of the WMT corpus filtering efforts, though our work here primarily examines a self-supervised method in the context of de-noising, rather than on a targeted filtering effort. Our methods are unsupervised. We use dualencoder models, rather than an encoder-decoder architecture, to model pairwise data and let th"
2020.aacl-main.45,C10-1124,0,0.030557,"ome recent models that have billions of parameters include T5 (Raffel et al., 2019) and GPT2 (Radford et al., 2019)). This leads to the need of collecting large amounts of training examples. Pairwise data, that captures the relationship in two modalities, is used to train deep learning models such as Neural Machine Translation (NMT) (Wu et al., 2016), Question Answering (Wang et al., 2007), Image Captioning (Sharma et al., 2018), etc. To train this kind of models, large-scale data can often be obtained from weak signals like text cooccurrence (Yang et al., 2018) or dictionary n-gram matching (Uszkoreit et al., 2010). For example, in the machine translation community, the large amount of multilingual text available on the internet has naturally led to the idea of using internet data to train NMT models (Resnik, 1999). This approach has proven advantageous but it has the drawback that data mined this way is intrinsically noisy (Resnik and Smith, 2003). Despite the poor quality, usually this kind of data contains a helpful subset that can be recovered through a process of data cleaning or refinement. Data cleaning could be implemented with linguistic knowledge such as its script, vocabulary, syntax, etc. Al"
2020.aacl-main.45,N18-1136,0,0.0608034,"Missing"
2020.aacl-main.45,W18-3022,1,0.832847,"meters that need to be adjusted during training (some recent models that have billions of parameters include T5 (Raffel et al., 2019) and GPT2 (Radford et al., 2019)). This leads to the need of collecting large amounts of training examples. Pairwise data, that captures the relationship in two modalities, is used to train deep learning models such as Neural Machine Translation (NMT) (Wu et al., 2016), Question Answering (Wang et al., 2007), Image Captioning (Sharma et al., 2018), etc. To train this kind of models, large-scale data can often be obtained from weak signals like text cooccurrence (Yang et al., 2018) or dictionary n-gram matching (Uszkoreit et al., 2010). For example, in the machine translation community, the large amount of multilingual text available on the internet has naturally led to the idea of using internet data to train NMT models (Resnik, 1999). This approach has proven advantageous but it has the drawback that data mined this way is intrinsically noisy (Resnik and Smith, 2003). Despite the poor quality, usually this kind of data contains a helpful subset that can be recovered through a process of data cleaning or refinement. Data cleaning could be implemented with linguistic kn"
2020.aacl-main.45,2020.acl-main.756,0,0.039094,"Missing"
2020.aacl-main.45,D18-1030,0,0.0434022,"Missing"
2020.acl-main.689,D11-1033,0,0.602,"and the use of curriculum are crucial for balancing and improving all domains, including out-ofdomain. In large-scale experiments, the multidomain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training. 1 Multi domain noise domain noise Static Y Y Y N Dynamic Y Y N (Our Work) Table 1: Data selection and data mixing research in NMT. ‘Y’: There is previous research that studies this case. ‘N’: No previous research has studied this case. Introduction In machine translation (MT), data selection, e.g., (Moore and Lewis, 2010; Axelrod et al., 2011), has remained as a fundamental and important research topic. It has played a crucial role in domain adaptation by selecting domain-matching training examples, or data cleaning (aka denoising) by selecting high-quality examples. So far, the most extensively studied scenario assumes a single domain to improve. It becomes both technically challenging and practically appealing to build a large-scale multidomain neural machine translation (NMT) model that performs simultaneously well on multiple domains at once. This requires addressing research challenges such as catastrophic forgetting (Goodfell"
2020.acl-main.689,W17-4712,0,0.065461,"rms simultaneously well on multiple domains at once. This requires addressing research challenges such as catastrophic forgetting (Goodfellow et al., 2014) at scale and data balancing. Such a model can easily find potential use cases, i.e., as a solid general service, for downstream transfer learning, for better deployment efficiency, or for transfer learning across datasets. Unfortunately, existing single-domain dataselection methods do not work well for multiple domains. For example, improving the translation accuracy of one domain will often hurt that of another (van der Wees et al., 2017; Britz et al., 2017), and improving model generalization across all domains by clean-data selection (Koehn et al., 2018) may not promise optimization of a particular domain. Multiple aspects need to be considered for training a multi-domain model. This paper presents a dynamic data selection method to multi-domain NMT. Things we do differently from previous work in mixing data are the choice of instance-level features and the employment of a multi-domain curriculum that is additionally able to denoise. These are crucial for mixing and improving all domains, including outof-domain. We experiment with large dataset"
2020.acl-main.689,W17-3205,0,0.0676695,"009) has been used as a formulation for dynamic data selection. Domain curricula (van der Wees et al., 2017; Zhang et al., 2019) are used for domain adaptation. Model stacking (Sajjad et al., 2017; Freitag and Al-Onaizan, 2016) is a practical idea to build domain models. CL is also used for denoising (Kumar et al., 2019; Wang et al., 2018a,b), and for faster convergence and improved general quality (Zhang et al., 2018; Platanios et al., 2019). Wang et al. (2018a) introduce a curriculum for training efficiency. In addition to data sorting/curriculum, instance/loss weighting (Wang et al., 2017; Chen et al., 2017; Wang et al., 2019b) has been used as an alternative. CL for NMT represents the SOTA data-selection method, but most existing works target at a single “domain”, be it a specific domain or the “denoising domain”. Static data mixing for multiple domains. When mixing data from multiple domains, a fundamental challenge is to address catastrophic forgetting (Goodfellow et al., 2014)–training an NMT model to focus on one domain can likely hurt another (van der Wees et al., 2017; Britz et al., 1 We treat denoising as a domain in the paper, inspired by previous works that treat data noise using domai"
2020.acl-main.689,K16-1031,0,0.0235508,"by two aspects and shows where our work stands. These two aspects are: 1. Is the method concerned with a single domain or multiple domains? 2. Does the method use data statically or dynamically? 7711 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7711–7723 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Static data selection for a single domain. Moore and Lewis (2010) select in-domain data for n-gram language model (LM) training. It is later generalized by Axelrod et al. (2011) to select parallel data for training MT models. Chen and Huang (2016); Chen et al. (2016) use classifiers to select domain data. Clean-data selection (Koehn et al., 2019, 2018; Junczys-Dowmunt, 2018) reduces harmful data noise to improve translation quality across domains. All these works select a data subset for a single “domain”1 and treat the selected data as a static/flat distribution. Dynamic data selection for a single domain. Static selection has two shortcomings: it discards data and it treats all examples equally after selection. When data is scarce, any data could be helpful, even if it is out of domain or noisy2 . Dynamic data selection is introduced"
2020.acl-main.689,2016.amta-researchers.8,0,0.0301354,"ws where our work stands. These two aspects are: 1. Is the method concerned with a single domain or multiple domains? 2. Does the method use data statically or dynamically? 7711 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7711–7723 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Static data selection for a single domain. Moore and Lewis (2010) select in-domain data for n-gram language model (LM) training. It is later generalized by Axelrod et al. (2011) to select parallel data for training MT models. Chen and Huang (2016); Chen et al. (2016) use classifiers to select domain data. Clean-data selection (Koehn et al., 2019, 2018; Junczys-Dowmunt, 2018) reduces harmful data noise to improve translation quality across domains. All these works select a data subset for a single “domain”1 and treat the selected data as a static/flat distribution. Dynamic data selection for a single domain. Static selection has two shortcomings: it discards data and it treats all examples equally after selection. When data is scarce, any data could be helpful, even if it is out of domain or noisy2 . Dynamic data selection is introduced to “sort” data from"
2020.acl-main.689,W17-4713,0,0.0168955,"ion methods, e.g., (Junczys-Dowmunt, 2018). 2 We refer to data regularization (using more data) and to transfer learning (fine-tuning) to exploit both data quantity and quality, the idea behind dynamic data selection. See Appendix C. 2017). Britz et al. (2017) learn domain-discerning (or -invariant) network representation with a domain discriminator network for NMT. The methods, however, require that domain labels are available in data. Tars and Fishel (2018) cluster data and tag each cluster as multi-domain NMT training data, but the method treats data in each cluster as a flat distribution. Farajian et al. (2017) implement multi-domain NMT by on-the-fly data retrieval and adaptation per sentence, at increased inference cost. Most existing methods (or experiment setups) have the following problems: (i) They mix data statically. (ii) They don’t consider the impact of data noise, which is a source of catastrophic forgetting. (iii) Experiments are carried out with small datasets, without separate examination on the data regularization effect. (iv) They do not examine out-of-domain performamce. Automatic data balancing for multi-domains. (Wang et al., 2020) automatically learn to weight (flat) data streams"
2020.acl-main.689,W18-6478,0,0.0606979,"le domains? 2. Does the method use data statically or dynamically? 7711 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7711–7723 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Static data selection for a single domain. Moore and Lewis (2010) select in-domain data for n-gram language model (LM) training. It is later generalized by Axelrod et al. (2011) to select parallel data for training MT models. Chen and Huang (2016); Chen et al. (2016) use classifiers to select domain data. Clean-data selection (Koehn et al., 2019, 2018; Junczys-Dowmunt, 2018) reduces harmful data noise to improve translation quality across domains. All these works select a data subset for a single “domain”1 and treat the selected data as a static/flat distribution. Dynamic data selection for a single domain. Static selection has two shortcomings: it discards data and it treats all examples equally after selection. When data is scarce, any data could be helpful, even if it is out of domain or noisy2 . Dynamic data selection is introduced to “sort” data from least in-domain to most in-domain. Training NMT models on data sorted this way effectively takes advantage of"
2020.acl-main.689,W18-2705,0,0.02574,"culum for an embedding learning task, through Bayesian Optimization. A similar idea (Ruder and Plank, 2017) is used to improve other NLP tasks. Here, we use the idea for NMT to construct a multi-domain data selection scheme with various selection scores at our disposal. The problem we study is connected to the more general multiobjective optimization problem. Duh (2018) uses Bandit learning to tune hyper-parameters such as the number of network layers for NMT. More related work. Previously, catastrophic forgetting has mostly been studied in the continued-training setup (Saunders et al., 2019; Khayrallah et al., 2018), to refer to the degrading performance on the out-of-domain task when a model is fine-tuned on in-domain data. This setup is a popular topic in general machine learning research (Aljundi et al., 2019). Thompson et al. (2018) study domain adaptation by freezing subnetworks. Our work instead addresses forgetting in the data-balancing scenario for multi-domains. We use curriculum to generalize fine-tuning. 7712 S1 S3 S2 S2 (1) S2 (2) S1 S1 (4) S3 →W2 → W3   W1  1.0 1/2 1/3 1/3 1/2   0.0   0.0 0.0 1/3   (1) S3 S1 S3 S3 (3) S2 S1 (5) S2 Table 2: Figure 1: Data order in single-dom"
2020.acl-main.689,kocmi-bojar-2017-curriculum,0,0.0358627,"in our paper, Wt (x, y) uses uniform weights over selected examples and assigns zero weights for filtered examples, similar to a mask. Curriculum examples characterized by reweighting, Wt (x, y), over three steps, to stochastically order data to benefit a final domain. Strikethrough discards examples. (1) corresponds to data order Figure 1 (2). (2) corresponds to data order Figure 1 (5). In NMT, CL is used to implement dynamic data selection. First, a scoring function (Section 4.3) is employed to measure the usefulness of an example to a domain and sort data. Then mini-batch sampling, e.g., (Kocmi and Bojar, 2017), is designed to realize the weighting Wt , to dynamically evolve the training criteria Qt towards in-domain. Figure 1 (1)-(4) illustrates the basic idea of the curriculum we use. (1) shows three sentence pairs, S1 , S2 , S3 , each having three scores, respectively representing usefulness to three domains. A greydomain training curriculum, for example, relies on the data order in (2), gradually discards least useful examples according to Wt (x, y) (Eq. 1) in Table 2 (1): At step 1, the learner uniformly samples from all examples (W1 ), producing model m1 . In step 2, the least-in-domain S3 is"
2020.acl-main.689,W19-5404,0,0.013332,"a single domain or multiple domains? 2. Does the method use data statically or dynamically? 7711 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7711–7723 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Static data selection for a single domain. Moore and Lewis (2010) select in-domain data for n-gram language model (LM) training. It is later generalized by Axelrod et al. (2011) to select parallel data for training MT models. Chen and Huang (2016); Chen et al. (2016) use classifiers to select domain data. Clean-data selection (Koehn et al., 2019, 2018; Junczys-Dowmunt, 2018) reduces harmful data noise to improve translation quality across domains. All these works select a data subset for a single “domain”1 and treat the selected data as a static/flat distribution. Dynamic data selection for a single domain. Static selection has two shortcomings: it discards data and it treats all examples equally after selection. When data is scarce, any data could be helpful, even if it is out of domain or noisy2 . Dynamic data selection is introduced to “sort” data from least in-domain to most in-domain. Training NMT models on data sorted this way"
2020.acl-main.689,W18-6453,0,0.0210311,"ch as catastrophic forgetting (Goodfellow et al., 2014) at scale and data balancing. Such a model can easily find potential use cases, i.e., as a solid general service, for downstream transfer learning, for better deployment efficiency, or for transfer learning across datasets. Unfortunately, existing single-domain dataselection methods do not work well for multiple domains. For example, improving the translation accuracy of one domain will often hurt that of another (van der Wees et al., 2017; Britz et al., 2017), and improving model generalization across all domains by clean-data selection (Koehn et al., 2018) may not promise optimization of a particular domain. Multiple aspects need to be considered for training a multi-domain model. This paper presents a dynamic data selection method to multi-domain NMT. Things we do differently from previous work in mixing data are the choice of instance-level features and the employment of a multi-domain curriculum that is additionally able to denoise. These are crucial for mixing and improving all domains, including outof-domain. We experiment with large datasets at different noise levels and show that the resulting models meet our requirements. 2 Related Work"
2020.acl-main.689,P18-1007,0,0.0290962,"elects higher f (x, y)scoring examples. In implementation, we integrate χfρ (x, y; t) in the data feeder to pass only selected examples to the downstream model trainer; we also normalize f (x, y) offline to directly compare to ρ(t) online to decide filtering. As an example, the Wt (x, y) for the multi-domain curriculum order in Figure 1 (5) can look like Table 2 (2). 5 5.1 Experiments Setup Data and domains. We experiment with two English→French training datasets: the noisy ParaCrawl data9 (290 million sentence pairs) and the WMT14 training data (38 million pairs). We use SentencePiece model (Kudo, 2018) for subword segmentation with a source-target shared vocabulary of 32,000 subword units. We evaluate our method with three “domains”: two specific domains, news and TED subtitles, and outof-domain. News domain uses the WMT14 news 7 E.g.,https://github.com/tobegit3hub/ advisor 8 When the training data is small, we can, in practice, let a model warm up before applying the schedule. 9 https://paracrawl.eu testset (N14) for testing, and WMT12-13 for validation in early stopping (Prechelt, 1997). The TED domain uses the IWSLT15 testset (T15) for testing, and the IWSLT14 testset for validation. Out"
2020.acl-main.689,N19-1208,0,0.12369,"Missing"
2020.acl-main.689,P10-2041,0,0.628336,"the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-ofdomain. In large-scale experiments, the multidomain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training. 1 Multi domain noise domain noise Static Y Y Y N Dynamic Y Y N (Our Work) Table 1: Data selection and data mixing research in NMT. ‘Y’: There is previous research that studies this case. ‘N’: No previous research has studied this case. Introduction In machine translation (MT), data selection, e.g., (Moore and Lewis, 2010; Axelrod et al., 2011), has remained as a fundamental and important research topic. It has played a crucial role in domain adaptation by selecting domain-matching training examples, or data cleaning (aka denoising) by selecting high-quality examples. So far, the most extensively studied scenario assumes a single domain to improve. It becomes both technically challenging and practically appealing to build a large-scale multidomain neural machine translation (NMT) model that performs simultaneously well on multiple domains at once. This requires addressing research challenges such as catastroph"
2020.acl-main.689,N19-1119,0,0.060229,"from least in-domain to most in-domain. Training NMT models on data sorted this way effectively takes advantage of transfer learning. Curriculum learning (CL) (Bengio et al., 2009) has been used as a formulation for dynamic data selection. Domain curricula (van der Wees et al., 2017; Zhang et al., 2019) are used for domain adaptation. Model stacking (Sajjad et al., 2017; Freitag and Al-Onaizan, 2016) is a practical idea to build domain models. CL is also used for denoising (Kumar et al., 2019; Wang et al., 2018a,b), and for faster convergence and improved general quality (Zhang et al., 2018; Platanios et al., 2019). Wang et al. (2018a) introduce a curriculum for training efficiency. In addition to data sorting/curriculum, instance/loss weighting (Wang et al., 2017; Chen et al., 2017; Wang et al., 2019b) has been used as an alternative. CL for NMT represents the SOTA data-selection method, but most existing works target at a single “domain”, be it a specific domain or the “denoising domain”. Static data mixing for multiple domains. When mixing data from multiple domains, a fundamental challenge is to address catastrophic forgetting (Goodfellow et al., 2014)–training an NMT model to focus on one domain ca"
2020.acl-main.689,W18-6319,0,0.027887,"Missing"
2020.acl-main.689,D17-1038,0,0.0353609,"They do not examine out-of-domain performamce. Automatic data balancing for multi-domains. (Wang et al., 2020) automatically learn to weight (flat) data streams of multi-languages (or &quot;domains&quot;). We perform dynamic data selection and regularization through a mulit-domain curriculum. Automatic curriculum learning. Our work falls under automatic curriculum construction (Graves et al., 2017) and is directly inspired by Tsvetkov et al. (2016), who learn to weight and combine instance-level features to form a curriculum for an embedding learning task, through Bayesian Optimization. A similar idea (Ruder and Plank, 2017) is used to improve other NLP tasks. Here, we use the idea for NMT to construct a multi-domain data selection scheme with various selection scores at our disposal. The problem we study is connected to the more general multiobjective optimization problem. Duh (2018) uses Bandit learning to tune hyper-parameters such as the number of network layers for NMT. More related work. Previously, catastrophic forgetting has mostly been studied in the continued-training setup (Saunders et al., 2019; Khayrallah et al., 2018), to refer to the degrading performance on the out-of-domain task when a model is f"
2020.acl-main.689,E17-2045,0,0.0415458,"tatic selection has two shortcomings: it discards data and it treats all examples equally after selection. When data is scarce, any data could be helpful, even if it is out of domain or noisy2 . Dynamic data selection is introduced to “sort” data from least in-domain to most in-domain. Training NMT models on data sorted this way effectively takes advantage of transfer learning. Curriculum learning (CL) (Bengio et al., 2009) has been used as a formulation for dynamic data selection. Domain curricula (van der Wees et al., 2017; Zhang et al., 2019) are used for domain adaptation. Model stacking (Sajjad et al., 2017; Freitag and Al-Onaizan, 2016) is a practical idea to build domain models. CL is also used for denoising (Kumar et al., 2019; Wang et al., 2018a,b), and for faster convergence and improved general quality (Zhang et al., 2018; Platanios et al., 2019). Wang et al. (2018a) introduce a curriculum for training efficiency. In addition to data sorting/curriculum, instance/loss weighting (Wang et al., 2017; Chen et al., 2017; Wang et al., 2019b) has been used as an alternative. CL for NMT represents the SOTA data-selection method, but most existing works target at a single “domain”, be it a specific"
2020.acl-main.689,P19-1022,0,0.0354822,"Missing"
2020.acl-main.689,P16-1009,0,0.0593101,"a 10-nearest neighbor retrieval in the embedding space, excluding the true translation. We pick the nearest neighbor to form a hard negative pair with the English sentence, and a random neighbor to form another negative pair. We sample 600k positive pairs and produce 1.8M pairs in total. Model. We use LSTM NMT (Wu et al., 2016) as our models, but with the Adam optimizer (Kingma and Ba, 2015). The batch size is 10k averaged over 8 length-buckets (with synchronous training). NLM/NMT features uses 512 dimensions by 3 layers–NLM shares the same architecture as NMT by using dummy source sentences (Sennrich et al., 2016). The final models are of 1024 dimensions by 8 layers, trained for 55k max steps. Training on WMT data uses a dropout probability of 0.2. Transformer results are in Appendix B. Curriculum optimization. In Eq. 5 (Section 4.5), we launch 30 trials (candidate curricula). BayesOpt spends 25 trials in exploration 10 Randomly sampled from www.epo.org Signature: BLEU+case.mixed+numrefs.1+ smooth.exp+tok.13a+version.1.4.2 7716 11 Curriculum N14 P1: B 33.4 P2: Cb6-feats 37.0 W1: B 38.039 .2 (Wu et al., 2016) 39.2 W2: Cb6-feats 39.3 T15 35.7 38.1 37.9 – 38.8 PA 29.8 48.3 45.6 – 46.1 D15 30.4 35.7 34.5 –"
2020.acl-main.689,W18-6313,0,0.0187118,"h various selection scores at our disposal. The problem we study is connected to the more general multiobjective optimization problem. Duh (2018) uses Bandit learning to tune hyper-parameters such as the number of network layers for NMT. More related work. Previously, catastrophic forgetting has mostly been studied in the continued-training setup (Saunders et al., 2019; Khayrallah et al., 2018), to refer to the degrading performance on the out-of-domain task when a model is fine-tuned on in-domain data. This setup is a popular topic in general machine learning research (Aljundi et al., 2019). Thompson et al. (2018) study domain adaptation by freezing subnetworks. Our work instead addresses forgetting in the data-balancing scenario for multi-domains. We use curriculum to generalize fine-tuning. 7712 S1 S3 S2 S2 (1) S2 (2) S1 S1 (4) S3 →W2 → W3   W1  1.0 1/2 1/3 1/3 1/2   0.0   0.0 0.0 1/3   (1) S3 S1 S3 S3 (3) S2 S1 (5) S2 Table 2: Figure 1: Data order in single-domain curricula and a potential multi-domain curriculum. (1) A toy training dataset of 3 examples. Each example has three scores, representing relevance to three domains, grey/dark/white domains, respectively. The higher the ba"
2020.acl-main.689,P16-1013,0,0.0834102,"ta noise, which is a source of catastrophic forgetting. (iii) Experiments are carried out with small datasets, without separate examination on the data regularization effect. (iv) They do not examine out-of-domain performamce. Automatic data balancing for multi-domains. (Wang et al., 2020) automatically learn to weight (flat) data streams of multi-languages (or &quot;domains&quot;). We perform dynamic data selection and regularization through a mulit-domain curriculum. Automatic curriculum learning. Our work falls under automatic curriculum construction (Graves et al., 2017) and is directly inspired by Tsvetkov et al. (2016), who learn to weight and combine instance-level features to form a curriculum for an embedding learning task, through Bayesian Optimization. A similar idea (Ruder and Plank, 2017) is used to improve other NLP tasks. Here, we use the idea for NMT to construct a multi-domain data selection scheme with various selection scores at our disposal. The problem we study is connected to the more general multiobjective optimization problem. Duh (2018) uses Bandit learning to tune hyper-parameters such as the number of network layers for NMT. More related work. Previously, catastrophic forgetting has mos"
2020.acl-main.689,D17-1155,0,0.0991459,") (Bengio et al., 2009) has been used as a formulation for dynamic data selection. Domain curricula (van der Wees et al., 2017; Zhang et al., 2019) are used for domain adaptation. Model stacking (Sajjad et al., 2017; Freitag and Al-Onaizan, 2016) is a practical idea to build domain models. CL is also used for denoising (Kumar et al., 2019; Wang et al., 2018a,b), and for faster convergence and improved general quality (Zhang et al., 2018; Platanios et al., 2019). Wang et al. (2018a) introduce a curriculum for training efficiency. In addition to data sorting/curriculum, instance/loss weighting (Wang et al., 2017; Chen et al., 2017; Wang et al., 2019b) has been used as an alternative. CL for NMT represents the SOTA data-selection method, but most existing works target at a single “domain”, be it a specific domain or the “denoising domain”. Static data mixing for multiple domains. When mixing data from multiple domains, a fundamental challenge is to address catastrophic forgetting (Goodfellow et al., 2014)–training an NMT model to focus on one domain can likely hurt another (van der Wees et al., 2017; Britz et al., 1 We treat denoising as a domain in the paper, inspired by previous works that treat dat"
2020.acl-main.689,P18-2048,0,0.104134,"helpful, even if it is out of domain or noisy2 . Dynamic data selection is introduced to “sort” data from least in-domain to most in-domain. Training NMT models on data sorted this way effectively takes advantage of transfer learning. Curriculum learning (CL) (Bengio et al., 2009) has been used as a formulation for dynamic data selection. Domain curricula (van der Wees et al., 2017; Zhang et al., 2019) are used for domain adaptation. Model stacking (Sajjad et al., 2017; Freitag and Al-Onaizan, 2016) is a practical idea to build domain models. CL is also used for denoising (Kumar et al., 2019; Wang et al., 2018a,b), and for faster convergence and improved general quality (Zhang et al., 2018; Platanios et al., 2019). Wang et al. (2018a) introduce a curriculum for training efficiency. In addition to data sorting/curriculum, instance/loss weighting (Wang et al., 2017; Chen et al., 2017; Wang et al., 2019b) has been used as an alternative. CL for NMT represents the SOTA data-selection method, but most existing works target at a single “domain”, be it a specific domain or the “denoising domain”. Static data mixing for multiple domains. When mixing data from multiple domains, a fundamental challenge is to"
2020.acl-main.689,P19-1123,1,0.558117,"Missing"
2020.acl-main.689,W18-6314,1,0.906247,"helpful, even if it is out of domain or noisy2 . Dynamic data selection is introduced to “sort” data from least in-domain to most in-domain. Training NMT models on data sorted this way effectively takes advantage of transfer learning. Curriculum learning (CL) (Bengio et al., 2009) has been used as a formulation for dynamic data selection. Domain curricula (van der Wees et al., 2017; Zhang et al., 2019) are used for domain adaptation. Model stacking (Sajjad et al., 2017; Freitag and Al-Onaizan, 2016) is a practical idea to build domain models. CL is also used for denoising (Kumar et al., 2019; Wang et al., 2018a,b), and for faster convergence and improved general quality (Zhang et al., 2018; Platanios et al., 2019). Wang et al. (2018a) introduce a curriculum for training efficiency. In addition to data sorting/curriculum, instance/loss weighting (Wang et al., 2017; Chen et al., 2017; Wang et al., 2019b) has been used as an alternative. CL for NMT represents the SOTA data-selection method, but most existing works target at a single “domain”, be it a specific domain or the “denoising domain”. Static data mixing for multiple domains. When mixing data from multiple domains, a fundamental challenge is to"
2020.acl-main.689,2020.acl-main.754,0,0.060603,"Missing"
2020.acl-main.689,D17-1147,0,0.0340391,"Missing"
2020.acl-main.689,N19-1189,0,0.471729,"ach training example and instantiate them in Experiments (Section 5). NMT domain features (qZ ) compute, for a pair (x, y), the cross-entropy difference between two NMT models: qZ (x, y)= log P (y|x; θZ )−log P (y|x; θbase ) (6) |y| P (y|x; θbase ) is a baseline model with parameters θbase trained on the background parallel corpus, P (y|x; θZ ) is a Z-domain model with θZ by finetuning θbase on a small, Z-domain parallel corpus b Z with trusted quality and |y |is the length of y. D qZ discerns both noise and domain Z (Wang et al., bZ . 2019a). Each domain Z has its own D Importantly, Grangier (2019) shows that, under the Taylor approximation (Abramowitz and Stegun, 1964), qZ approximates the dot product between gradient, g(x, y; θbase ), of training examb Z , θbase ), of seed data ple (x, y) and gradient, g(D 5 b DZ . Thus an example with positive qZ likely 5 That is, according to Grangier (2019): qZ (x, y) × |y| log P (y|x; θZ ) − log P (y|x; θbase ) b Z , θbase ) λ g(x, y; θbase )&gt; g(D 4 But N does not necessarily equal K because we can introduce multiple features for one domain or a single feature for multiple domains. (4) = ≈ (7) when θbase and θZ are close, which is the case for fin"
2020.acl-main.75,S17-2079,0,0.0502423,"Missing"
2020.acl-main.75,N16-1079,0,0.0243694,"word may have different meanings regarding its contexts. Especially, an infrequent meaning of the word might be utilized for creating a pun. Therefore, static word embeddings are insufficient to represent words. In addition, some puns are created by replacing a word with another word with the same or similar pronunciation as examples shown in Table 1. Therefore, to recognize puns, it is essential to model the association between words in the sentence and the pronunciation of words. Despite existing approaches attempt to leverage phonological structures to understand puns (Doogan et al., 2017; Jaech et al., 2016), there is a lack of a general framework to model these two types of signals in a whole. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to jointly model the contextualized word embeddings and phonological word representations for pun recognition. To capture the phonological structures of words, we break each word into a sequence of phonemes as its pronunciation so that homophones can have similar phoneme sets. For instance, the phonemes of the word pun are {P, AH, N}. In PCPR, we construct a pronunciation attentive module to identify important phonemes"
2020.acl-main.75,P19-1394,0,0.01637,"ts only on Saturday and Sunday because Monday to Friday are weak (week) days. Table 1: Examples of homographic and heterographic puns. Introduction During the last decades, social media has promoted the creation of a vast amount of humorous web contents (Nijholt et al., 2017). Automatic recognition of humor has become an important task in the area of figurative language processing, which can benefit various downstream NLP applications such as dialogue systems, sentiment analysis, and machine translation (Melby and Warner, 1995; Augello et al., 2008; Ghosh et al., 2015; Bertero and Fung, 2016; Blinov et al., 2019). However, humor is one of the most complicated behaviors in natural language semantics and sometimes it is even difficult for humans to interpret. In most cases, understanding humor requires adequate background knowledge and a rich context. Puns are a form of humorous approaches using the different meanings of identical words or words with similar pronunciations to explain texts or utterances. There are two main types of puns. Homographic puns rely on multiple interpretations of the same word. As shown in Table 1, the phrase all right means good condition or opposite to left; the word reactio"
2020.acl-main.75,P18-2087,0,0.0514278,"g word senses of pun words (Oele and Evang, 2017). However, these methods cannot tackle heterographic puns with distinct word 813 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 813–822 c July 5 - 10, 2020. 2020 Association for Computational Linguistics spellings and knowledge bases that only contain a limited vocabulary. To resolve the issues of sparseness and heterographics, the word embedding techniques (Mikolov et al., 2013; Pennington et al., 2014) provide flexible representations to model puns (Hurtado et al., 2017; Indurthi and Oota, 2017; Cai et al., 2018). However, a word may have different meanings regarding its contexts. Especially, an infrequent meaning of the word might be utilized for creating a pun. Therefore, static word embeddings are insufficient to represent words. In addition, some puns are created by replacing a word with another word with the same or similar pronunciation as examples shown in Table 1. Therefore, to recognize puns, it is essential to model the association between words in the sentence and the pronunciation of words. Despite existing approaches attempt to leverage phonological structures to understand puns (Doogan e"
2020.acl-main.75,W17-5009,0,0.0433476,"Missing"
2020.acl-main.75,N18-2018,0,0.0361787,"Missing"
2020.acl-main.75,S17-2011,0,0.138384,"., 2018). However, a word may have different meanings regarding its contexts. Especially, an infrequent meaning of the word might be utilized for creating a pun. Therefore, static word embeddings are insufficient to represent words. In addition, some puns are created by replacing a word with another word with the same or similar pronunciation as examples shown in Table 1. Therefore, to recognize puns, it is essential to model the association between words in the sentence and the pronunciation of words. Despite existing approaches attempt to leverage phonological structures to understand puns (Doogan et al., 2017; Jaech et al., 2016), there is a lack of a general framework to model these two types of signals in a whole. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to jointly model the contextualized word embeddings and phonological word representations for pun recognition. To capture the phonological structures of words, we break each word into a sequence of phonemes as its pronunciation so that homophones can have similar phoneme sets. For instance, the phonemes of the word pun are {P, AH, N}. In PCPR, we construct a pronunciation attentive module to identif"
2020.acl-main.75,S15-2080,0,0.0203562,"had its best sail (sale) ever. I lift weights only on Saturday and Sunday because Monday to Friday are weak (week) days. Table 1: Examples of homographic and heterographic puns. Introduction During the last decades, social media has promoted the creation of a vast amount of humorous web contents (Nijholt et al., 2017). Automatic recognition of humor has become an important task in the area of figurative language processing, which can benefit various downstream NLP applications such as dialogue systems, sentiment analysis, and machine translation (Melby and Warner, 1995; Augello et al., 2008; Ghosh et al., 2015; Bertero and Fung, 2016; Blinov et al., 2019). However, humor is one of the most complicated behaviors in natural language semantics and sometimes it is even difficult for humans to interpret. In most cases, understanding humor requires adequate background knowledge and a rich context. Puns are a form of humorous approaches using the different meanings of identical words or words with similar pronunciations to explain texts or utterances. There are two main types of puns. Homographic puns rely on multiple interpretations of the same word. As shown in Table 1, the phrase all right means good c"
2020.acl-main.75,N19-1172,0,0.0196297,"short sentences as word embeddings do not have much context information. Besides, Zou and Lu (2019) jointly detect and locate the pun from a sequence labeling perspective by employing a new tagging schema. Diao et al. (2018) expand word embeddings using WordNet to settle the polysemy of homographic puns, following by a neural attention mechanism to extract the collocation to detect the homographic pun. However, all these methods only make use of limited context information. Other than the pun recognition, Yu et al. (2018) generate homographic puns without requiring any pun data for training. He et al. (2019) improve the homographic pun generation based on the “local-global surprisal principle” which posits that the pun word and the alternative word have a strong association with the distant and immediate context respectively. Pronunciation Embeddings Word embeddings assign each word with a vector so that words with similar semantic meanings are close in the embedding space. Most word embedding models only make use of text information and omitting the rich information contained in the pronunciation. How814 ever, the pronunciation is also an important part of the language (Zhu et al., 2018). Prior"
2020.acl-main.75,H05-1067,0,0.108396,"chtomova, 2017). While most of them extract complicated linguistic features to train rule based and machine learning based classifiers. In addition to task participants, Sense (Cai et al., 2018) incorporates word sense representations into RNNs to tackle the homographic pun location task. The CRF (Zou and Lu, 2019) captures linguistic features such as POS tags, n-grams, and word suffix to model puns. Moreover, the Joint (Zou and Lu, 2019) jointly models two tasks with RNNs and a CRF tagger. For the PTD dataset, four baseline methods with reported performance are selected for comparisons. MCL (Mihalcea and Strapparava, 2005) exploits word representations with multiple stylistic features while HAE (Yang et al., 2015) applies a random forest model with Word2Vec and humancentric features. PAL (Chen and Lee, 2017) trains a convolutional neural network (CNN) to learn essential feature automatically. Based on existing CNN models, HUR (Chen and Soo, 2018) improves the performance by adjusting the filter size and adding a highway layer. 4.2 Experimental Results Pun Detection. Table 3 presents the pun detection performance of methods for both homographic and heterographic puns on the SemEval dataset while Table 4 shows th"
2020.acl-main.75,S17-2072,0,0.0244467,"ngs. To tune the hyperparameters, we search the phoneme embedding size dP and the attention size dA from {8, 16, 32, 64, 128, 256, 512} as shown in Figure 2. For the SemEval dataset, the best setting is (dP = 64, dA = 256) for the homographic puns while heterographic puns favor (dP = 64, dA = 32). For the PTD dataset, (dP = 64, dA = 32) can reach the best performance. Baseline Methods. We compare PCPR with several baseline methods. For the SemEval dataset, nine baseline methods are compared in the experiments, including Duluth (Pedersen, 2017), JU CES NLP (Pramanick and Das, 2017), PunFields (Mikhalkova and Karyakin, 2017), UWAV (Vadehra, 2017), Fermi (Indurthi and Oota, 2017), and 3 https://dumps.wikimedia.org/enwiki/ latest/ UWaterloo (Vechtomova, 2017). While most of them extract complicated linguistic features to train rule based and machine learning based classifiers. In addition to task participants, Sense (Cai et al., 2018) incorporates word sense representations into RNNs to tackle the homographic pun location task. The CRF (Zou and Lu, 2019) captures linguistic features such as POS tags, n-grams, and word suffix to model puns. Moreover, the Joint (Zou and Lu, 2019) jointly models two tasks with RNNs an"
2020.acl-main.75,L18-1008,0,0.0204808,"each word are derived by the CMU Pronouncing Dictionary2 . We initialize the phoneme embeddings by using the fastText 817 1 http://alt.qcri.org/semeval2017/ task7/ 2 http://svn.code.sf.net/p/cmusphinx/ code/trunk/cmudict/ F1 score 0.90 homographic heterographic 0.88 4 8 16 32 64 128 Phoneme embedding size (dP) (a) Phoneme emb. size dP F1 score 0.92 0.92 0.90 homographic heterographic 0.88 0.86 16 32 64 128 256 512 Attention size (dA) (b) Attention size dA Figure 2: Pun location performance over different phoneme embedding sizes dP and attention sizes dA on the SemEval dataset. word embedding (Mikolov et al., 2018) trained on Wikipedia articles3 crawled in December, 2017. The PCPR is implemented in PyTorch while the fused Adam optimizer (Kingma and Ba, 2014) optimizes the parameters with an initial learning rate of 5 × 10−5 . The dropout and batch size are set as 10−1 and 32. We follow BERT (BASE) (Devlin et al., 2018) to use 12 Transformer layers and self-attention heads. To clarify, in PCPR, tokens and phonemes are independently processed, so the tokens processed with WordPiece tokenizer (Wu et al., 2016) in BERT are not required to line up with phonemes for computations. To deal with the out-of-vocab"
2020.acl-main.75,P02-1019,0,0.216766,"ve word have a strong association with the distant and immediate context respectively. Pronunciation Embeddings Word embeddings assign each word with a vector so that words with similar semantic meanings are close in the embedding space. Most word embedding models only make use of text information and omitting the rich information contained in the pronunciation. How814 ever, the pronunciation is also an important part of the language (Zhu et al., 2018). Prior studies have demonstrated that the phonetic information can be used in speech recognition (Bengio and Heigold, 2014), spell correction (Toutanova and Moore, 2002) and speech synthesis (Miller, 1998a). By projecting to the embedding space, words sound alike are nearby to each other (Bengio and Heigold, 2014). Furthermore, Kamper et al. (2016) make use of word pairs information to improve the acoustic word embedding. Zhu et al. (2018) show that combining the pronunciation with the writing texts can help to improve the performance of word embeddings. However, these pronunciation embeddings are word-level features, while in our approach, we make use of syllabic pronunciations which is phoneme-level and could help with the out-of-vocabulary (OOV) situation."
2020.acl-main.75,S17-2077,0,0.0214071,"earch the phoneme embedding size dP and the attention size dA from {8, 16, 32, 64, 128, 256, 512} as shown in Figure 2. For the SemEval dataset, the best setting is (dP = 64, dA = 256) for the homographic puns while heterographic puns favor (dP = 64, dA = 32). For the PTD dataset, (dP = 64, dA = 32) can reach the best performance. Baseline Methods. We compare PCPR with several baseline methods. For the SemEval dataset, nine baseline methods are compared in the experiments, including Duluth (Pedersen, 2017), JU CES NLP (Pramanick and Das, 2017), PunFields (Mikhalkova and Karyakin, 2017), UWAV (Vadehra, 2017), Fermi (Indurthi and Oota, 2017), and 3 https://dumps.wikimedia.org/enwiki/ latest/ UWaterloo (Vechtomova, 2017). While most of them extract complicated linguistic features to train rule based and machine learning based classifiers. In addition to task participants, Sense (Cai et al., 2018) incorporates word sense representations into RNNs to tackle the homographic pun location task. The CRF (Zou and Lu, 2019) captures linguistic features such as POS tags, n-grams, and word suffix to model puns. Moreover, the Joint (Zou and Lu, 2019) jointly models two tasks with RNNs and a CRF tagger. For th"
2020.acl-main.75,S17-2005,0,0.0142755,"model contextualized word embeddings and pronunciation embeddings to recognize puns. Both contexts and phonological properties are beneficial to pun recognition. • Extensive experiments are conducted on two benchmark datasets. PCPR significantly outperforms existing methods in both pun detection and pun location. In-depth analyses also verify the effectiveness and robustness of PCPR. • We release our implementations and pre-trained phoneme embeddings at https://github.com/ joey1993/pun-recognition to facilitate future research. 2 Related Work Pun Recognition and Generation To recognize puns, Miller et al. (2017) summarize several systems for the SemEval 2017 tasks. To detect the pun, Pedersen (2017) supposes that if there is one pun in the sentence, when adopting different Word Sense Disambiguation (WSD) methods, the sense assigned to the sentence will be different. To locate the pun, based on the WSD results for pun detection, they choose the last word which changes the senses between different WSD runs. Even though this method can tackle both homographic and heterographic pun detection, it does not use any pretrained embedding model. Xiu et al. (2017) detect the pun in the sentence using similarity"
2020.acl-main.75,S17-2076,0,0.0255175,"weak and week in Table 1 have the same or similar pronunciations. The sentences are funny because both words fit the same context. Understanding puns is a big fish to fry for deep comprehension of complex semantics. These two forms of puns have been studied in literature from different angles. To recognize puns in a sentence, word sense disambiguation techniques (WSD) (Navigli, 2009) have been employed to identify the equitable intention of words in utterances (Pedersen, 2017). External knowledge bases such as WordNet (Miller, 1998b) have been applied in determining word senses of pun words (Oele and Evang, 2017). However, these methods cannot tackle heterographic puns with distinct word 813 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 813–822 c July 5 - 10, 2020. 2020 Association for Computational Linguistics spellings and knowledge bases that only contain a limited vocabulary. To resolve the issues of sparseness and heterographics, the word embedding techniques (Mikolov et al., 2013; Pennington et al., 2014) provide flexible representations to model puns (Hurtado et al., 2017; Indurthi and Oota, 2017; Cai et al., 2018). However, a word may have diffe"
2020.acl-main.75,S17-2070,0,0.175978,"the other hand, heterographic puns take advantage of phonologically same or similar words. For example, the word pairs sale and sail, weak and week in Table 1 have the same or similar pronunciations. The sentences are funny because both words fit the same context. Understanding puns is a big fish to fry for deep comprehension of complex semantics. These two forms of puns have been studied in literature from different angles. To recognize puns in a sentence, word sense disambiguation techniques (WSD) (Navigli, 2009) have been employed to identify the equitable intention of words in utterances (Pedersen, 2017). External knowledge bases such as WordNet (Miller, 1998b) have been applied in determining word senses of pun words (Oele and Evang, 2017). However, these methods cannot tackle heterographic puns with distinct word 813 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 813–822 c July 5 - 10, 2020. 2020 Association for Computational Linguistics spellings and knowledge bases that only contain a limited vocabulary. To resolve the issues of sparseness and heterographics, the word embedding techniques (Mikolov et al., 2013; Pennington et al., 2014) provi"
2020.acl-main.75,D14-1162,0,0.087366,"in utterances (Pedersen, 2017). External knowledge bases such as WordNet (Miller, 1998b) have been applied in determining word senses of pun words (Oele and Evang, 2017). However, these methods cannot tackle heterographic puns with distinct word 813 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 813–822 c July 5 - 10, 2020. 2020 Association for Computational Linguistics spellings and knowledge bases that only contain a limited vocabulary. To resolve the issues of sparseness and heterographics, the word embedding techniques (Mikolov et al., 2013; Pennington et al., 2014) provide flexible representations to model puns (Hurtado et al., 2017; Indurthi and Oota, 2017; Cai et al., 2018). However, a word may have different meanings regarding its contexts. Especially, an infrequent meaning of the word might be utilized for creating a pun. Therefore, static word embeddings are insufficient to represent words. In addition, some puns are created by replacing a word with another word with the same or similar pronunciation as examples shown in Table 1. Therefore, to recognize puns, it is essential to model the association between words in the sentence and the pronunciati"
2020.acl-main.75,N18-1202,0,0.0150098,"the problem and then introduce the proposed method, PCPR. 3.1 Problem Statement Suppose the input text consists of a sequence of N words {w1 , w2 , · · · , wN }. For each word wi with Mi phonemes in its pronunciation, the phonemes are denoted as R(wi ) = {ri,1 , ri,2 , · · · , ri,Mi }, Framework Overview Contextualized Word Embeddings The context is essential for interpreting a word in the text. Hence, we propose to apply contextualized word embeddings to derive word representations. In the framework of PCPR, any contextualized word embedding method, such as BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), and XLNet (Yang et al., 2019), can be utilized. Here, we choose BERT to derive contextualized word embeddings without loss of generality. 815 yˆ1L yˆD Pun Detection Prediction yˆ2L L yˆN Pun Location Predictions Self-attentive Encoder J T[CLS] Contextualized Word Embeddings C T[CLS] T1J T1C T2C ··· E[CLS] Input Words E1 w1 E2 w2 TNJ ··· Joint Embeddings T1P T2P TNP u1,1 · · · u1,M1 u2,1 · · · u2,M2 uN,1 · · · uN,M r1,1 r2,1 Pronunciation Embeddings TNC Contextualized Word Encoder Input Embeddings T2J ··· EN ··· wN Phonological Attention · · · r1,M1 · · · r2,M2 ··· rN,1 · · · rN,M N Phoneme E"
2020.acl-main.75,P17-1161,0,0.0239255,"c pronunciations which is phoneme-level and could help with the out-of-vocabulary (OOV) situation. Luo et al. (2019) also propose an adversarial generative network for pun generation, which does not require any pun corpus. Contextualized Word Embeddings Traditional word embeddings assign a fixed vector to one word even if the word has multiple meanings under different contexts (e.g., “the river bank” v.s. “the commercial bank”). McCann et al. (2017) combine the pivot word embeddings as well as the contextual embeddings generated by an encoder from a supervised neural machine translation task. Peters et al. (2017) enrich the word embeddings by the contextual information extracted from a bidirectional language model. (Devlin et al., 2018) learn the language embedding by stacking multiple transformer layers with masked language model objective which advances the state-of-the-art for many NLP tasks. Yang et al. (2019) enable learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and solve the problem of pretrain-finetune discrepancy. where ri,j is the j-th phoneme in the pronunciation of wi . These phonemes are given by a dictionary. In this"
2020.acl-main.75,S17-2073,0,0.0244674,"ffectiveness of pronunciation embeddings. To tune the hyperparameters, we search the phoneme embedding size dP and the attention size dA from {8, 16, 32, 64, 128, 256, 512} as shown in Figure 2. For the SemEval dataset, the best setting is (dP = 64, dA = 256) for the homographic puns while heterographic puns favor (dP = 64, dA = 32). For the PTD dataset, (dP = 64, dA = 32) can reach the best performance. Baseline Methods. We compare PCPR with several baseline methods. For the SemEval dataset, nine baseline methods are compared in the experiments, including Duluth (Pedersen, 2017), JU CES NLP (Pramanick and Das, 2017), PunFields (Mikhalkova and Karyakin, 2017), UWAV (Vadehra, 2017), Fermi (Indurthi and Oota, 2017), and 3 https://dumps.wikimedia.org/enwiki/ latest/ UWaterloo (Vechtomova, 2017). While most of them extract complicated linguistic features to train rule based and machine learning based classifiers. In addition to task participants, Sense (Cai et al., 2018) incorporates word sense representations into RNNs to tackle the homographic pun location task. The CRF (Zou and Lu, 2019) captures linguistic features such as POS tags, n-grams, and word suffix to model puns. Moreover, the Joint (Zou and Lu,"
2020.acl-main.75,S17-2071,0,0.0583232,"Missing"
2020.acl-main.75,S17-2078,0,0.0250511,"ecognition and Generation To recognize puns, Miller et al. (2017) summarize several systems for the SemEval 2017 tasks. To detect the pun, Pedersen (2017) supposes that if there is one pun in the sentence, when adopting different Word Sense Disambiguation (WSD) methods, the sense assigned to the sentence will be different. To locate the pun, based on the WSD results for pun detection, they choose the last word which changes the senses between different WSD runs. Even though this method can tackle both homographic and heterographic pun detection, it does not use any pretrained embedding model. Xiu et al. (2017) detect the pun in the sentence using similarity features which are calculated on sense vectors or cluster center vectors. To locate the pun, they use an unsupervised system by scoring each word in the sentence and choosing the word with the smallest score. However, this model exclusively relies on semantics to detect the heterographic puns but ignores the rich information embedded in the pronunciations. Doogan et al. (2017) leverage word embeddings as well as the phonetic information by concatenating pronunciation strings, but the concatenation has limited expression ability. They also mentio"
2020.acl-main.75,D15-1284,0,0.179607,"c and heterographic puns. Pun detection employs all of the examples in the two datasets while pun location only exploits the examples with puns in SemEval due to the limitation of annotations.  C  J J . T[CLS] = T[CLS] ; T[ATT] Moreover, each word wi is benefited from the selfattentive encoder and is represented by a joint embedding: J Ti,[ATT] = αiS · TiJ . SemEval Homo Hetero 1,607 1,271 643 509 2,250 1,780 4.1 Experiment settings Experimental Datasets. We conducted experiments on the SemEval 2017 shared task 7 dataset1 (SemEval) (Miller et al., 2017) and the Pun of The Day dataset (PTD) (Yang et al., 2015). For pun detection, the SemEval dataset consists of 4, 030 and 2, 878 examples for pun detection and location while each example with a pun can be a homographic or heterographic pun. In contrast, the PTD dataset contains 4, 826 examples without labels of pun types. Table 2 further shows the data statistics. The two experimental datasets are the largest publicly available benchmarks that are used in the existing studies. SemEval-2017 dataset contains punning and non-punning jokes, aphorisms, and other short texts composed by professional humorists and online collections. Hence, we assume the g"
2020.acl-main.75,P18-1153,0,0.0179353,"tenation has limited expression ability. They also mention that their systems suffer for short sentences as word embeddings do not have much context information. Besides, Zou and Lu (2019) jointly detect and locate the pun from a sequence labeling perspective by employing a new tagging schema. Diao et al. (2018) expand word embeddings using WordNet to settle the polysemy of homographic puns, following by a neural attention mechanism to extract the collocation to detect the homographic pun. However, all these methods only make use of limited context information. Other than the pun recognition, Yu et al. (2018) generate homographic puns without requiring any pun data for training. He et al. (2019) improve the homographic pun generation based on the “local-global surprisal principle” which posits that the pun word and the alternative word have a strong association with the distant and immediate context respectively. Pronunciation Embeddings Word embeddings assign each word with a vector so that words with similar semantic meanings are close in the embedding space. Most word embedding models only make use of text information and omitting the rich information contained in the pronunciation. How814 ever"
2020.acl-main.75,N19-1217,0,0.0457919,"vectors. To locate the pun, they use an unsupervised system by scoring each word in the sentence and choosing the word with the smallest score. However, this model exclusively relies on semantics to detect the heterographic puns but ignores the rich information embedded in the pronunciations. Doogan et al. (2017) leverage word embeddings as well as the phonetic information by concatenating pronunciation strings, but the concatenation has limited expression ability. They also mention that their systems suffer for short sentences as word embeddings do not have much context information. Besides, Zou and Lu (2019) jointly detect and locate the pun from a sequence labeling perspective by employing a new tagging schema. Diao et al. (2018) expand word embeddings using WordNet to settle the polysemy of homographic puns, following by a neural attention mechanism to extract the collocation to detect the homographic pun. However, all these methods only make use of limited context information. Other than the pun recognition, Yu et al. (2018) generate homographic puns without requiring any pun data for training. He et al. (2019) improve the homographic pun generation based on the “local-global surprisal princip"
2020.acl-main.75,S17-2075,0,\N,Missing
2020.acl-main.75,D18-1272,0,\N,Missing
2020.acl-main.75,D19-1336,0,\N,Missing
2020.emnlp-main.444,D19-1219,0,0.0184127,"to obtain from the text. However, if we link the sentences to their visual scenes, the contradiction is much clearer because the two scenes cannot happen in the same visual context. We think it is necessary to incorporate other modalities for the unsupervised natural language inference. The idea of adapting multimodal in SSL is not new. According to (Su et al., 2020), we brieﬂy divide previous multimodal SSL approaches into two categories based on their encoder infrastructures. As shown in Fig. 1a, the ﬁrst category uses one joint encoder to represent the multimodal inputs (Sun et al., 2019; Alberti et al., 2019; Li et al., 2019, 2020; Su et al., 2020). Obviously, if the downstream task is only for plain text, we cannot extract the representation of text separately from the joint encoder. So the ﬁrst category is infeasible for the natural language inference. The second category (Lu et al., 2019; Tan and Bansal, 2019; Sun et al., 2019) ﬁrst encodes the text and the image separately by two encoders. Then it represents the multimodal information via a joint encoder over the lower layer encoders. This is shown in Fig. 1b. Although the textual representation can be extracted from the text encoder in the l"
2020.emnlp-main.444,N18-1038,0,0.0283872,"et al., 2019) learn the sentence level contextual information (i.e. by next sentence prediction task) and the word level contextual information (i.e. by masked language model task). Besides linguistic contexts, humans also link other modalities (e.g. visions, voices) to novel inputs (Bar, 2009). Even if the goal is to reason about 5511 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5511–5520, c November 16–20, 2020. 2020 Association for Computational Linguistics plain texts, other modalities still help (although they are not provided as inputs) (Kiela et al., 2018). For example, if only textual information is used, it is difﬁcult to entail the contradiction between a and . b We need the commonsense that a man  only has two arms, which cannot play the piano and clarinet simultaneously. This commonsense is hard to obtain from the text. However, if we link the sentences to their visual scenes, the contradiction is much clearer because the two scenes cannot happen in the same visual context. We think it is necessary to incorporate other modalities for the unsupervised natural language inference. The idea of adapting multimodal in SSL is not new. According"
2020.emnlp-main.444,S17-2001,0,0.0252745,"x), and τ  is the temperature. By combing the lifelong learning regularization, we obtain the ﬁnal loss for SSL: θˆx , θˆy , θˆα = argmax γLNCE global (X, Y ) θf ,θg + βLNCE local (X, Y ) + (1 − γ − β)Lanchor (X) (15) 4.1 Test 1379 754 9824 3207 Table 1: Statistics of datasets. (14) − (1 − )cosine(f (x), f  (x))] 4 #Text Dev 47675 202654 1500 875 9842 3321 Experiments Setup All the experiments run over a computer with 4 Nvdia Tesla V100 GPUs. Datasets We use Flickr30k (Young et al., 2014) and COCO (Lin et al., 2014) as the text2image dataset Dt2i for self-supervised learning. We use STS-B (Cer et al., 2017) and SNLI (Bowman et al.) as the downstream NLI tasks for evaluation. STSB is a collection of sentence pairs, each of which has a human-annotated similarity score from 1 to 5. The task is to predict these scores. We follow GLUE (Wang et al., 2018) and use Pearson and 4.2 Model Details Encoder details We use BERT-base as the text encoder fglobal . The local information fword (x(i) ) is the feature vector of the i-th word through BERT. We use Resnet-50 as the image encoder gglobal . We use the encoding before the ﬁnal pooling layer as the representations of M 2 patches gpatch (y (i) ). To guaran"
2020.emnlp-main.444,2021.ccl-1.108,0,0.132219,"Missing"
2020.emnlp-main.444,N18-1202,0,0.0192204,"ity reaches a certain threshold. More speciﬁcally, if the similarity >= ψ1 , we predict “entailment”. If the similarity &lt; ψ2 , we predict “contradiction”. Otherwise we predict “neutral”. Competitors We compare MACD with the single-modal pre-training model BERT, and multimodal pre-training model LXMERT (Tan and Bansal, 2019) and VilBert (Lu et al., 2019). Both LXMERT and VilBert use the network architec5516 ture as in Fig. 1b. We extract the lower layer text encoder for unsupervised representation and ﬁne-tuning. We also compare MACD with classical NLP models, including BiLSTM and BiLSTM+ELMO (Peters et al., 2018). Hyper-parameters We list the hyperparameters below. For ψ1 and ψ2 , we use the best set of values chosen in the grid search from range {−1, −0.95, −0.9, · · · , 1}. For τσ and τc , we use the best set of values chosen in the grid search from range {0.01, 0.1, 1}. For τ  , , γ and β, we follow their settings in DistilBert (Sanh et al., 2019). τσ 0.1 Batch Size 64 τc 1 lr 1e-4 τ 2 Epochs 10  5/6 Grad Acc 8 γ 1/3 ψ1 0.80 β 1/3 ψ2 0.55 Table 2: Hyper-parameters for self-supervised learning. “lr” means learning rate. 4.3 BERT LXMERT VilBert MACD + COCO MACD + Filckr30k SNLI Acc 35.09 39.03 43"
2020.emnlp-main.444,D19-1514,0,0.0616383,"multimodal in SSL is not new. According to (Su et al., 2020), we brieﬂy divide previous multimodal SSL approaches into two categories based on their encoder infrastructures. As shown in Fig. 1a, the ﬁrst category uses one joint encoder to represent the multimodal inputs (Sun et al., 2019; Alberti et al., 2019; Li et al., 2019, 2020; Su et al., 2020). Obviously, if the downstream task is only for plain text, we cannot extract the representation of text separately from the joint encoder. So the ﬁrst category is infeasible for the natural language inference. The second category (Lu et al., 2019; Tan and Bansal, 2019; Sun et al., 2019) ﬁrst encodes the text and the image separately by two encoders. Then it represents the multimodal information via a joint encoder over the lower layer encoders. This is shown in Fig. 1b. Although the textual representation can be extracted from the text encoder in the lower layer, such representation does not go through the joint learning module and contains little visual knowledge. In summary, the encoders in previous multimodal SSL approaches are coupled. If only textual inputs are given, they cannot effectively incorporate visual knowledge in their representations. Thus"
2020.emnlp-main.444,W18-5446,0,0.137442,"Table 1: Statistics of datasets. (14) − (1 − )cosine(f (x), f  (x))] 4 #Text Dev 47675 202654 1500 875 9842 3321 Experiments Setup All the experiments run over a computer with 4 Nvdia Tesla V100 GPUs. Datasets We use Flickr30k (Young et al., 2014) and COCO (Lin et al., 2014) as the text2image dataset Dt2i for self-supervised learning. We use STS-B (Cer et al., 2017) and SNLI (Bowman et al.) as the downstream NLI tasks for evaluation. STSB is a collection of sentence pairs, each of which has a human-annotated similarity score from 1 to 5. The task is to predict these scores. We follow GLUE (Wang et al., 2018) and use Pearson and 4.2 Model Details Encoder details We use BERT-base as the text encoder fglobal . The local information fword (x(i) ) is the feature vector of the i-th word through BERT. We use Resnet-50 as the image encoder gglobal . We use the encoding before the ﬁnal pooling layer as the representations of M 2 patches gpatch (y (i) ). To guarantee that the image encoder and the text encoder are in the same space, we project the feature vectors of the image encoder to the dimension of 768, which is the dimension of BERT. Unsupervised NLI We compute the similarity of two sentences via the"
2020.emnlp-main.444,Q14-1006,0,0.14614,"Missing"
2020.emnlp-main.700,D19-1255,1,0.855705,"nell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 2018) and GPT-2 (Radford et al., 2019) use a leftto-right Transformer deco"
2020.emnlp-main.700,W11-0609,0,0.0284332,"ERNIE-GENLARGE d PALM PALMLARGE BLEU-4 15.16 16.38 22.88 22.28 24.03 22.78 24.11 MTR 19.12 20.25 24.94 25.13 26.31 25.02 25.85 RG-L 44.48 51.80 50.58 52.36 50.96 52.38 Table 4: Question generation results on the SQuAD dataset. MTR is short for METEOR and RG is short for ROUGE. a (Du and Cardie, 2018); b (Zhao et al., 2018); c (Dong et al., 2019); d (Xiao et al., 2020). 3.6 Fine-tuning on Response Generation Conversational response generation aims to produce a flexible response to a conversation (Vinyals and Le, 2015). Following MASS, we conduct experiments on the Cornell Movie Dialog corpus5 (Danescu-Niculescu-Mizil and Lee, 2011) that contains 140K conversation pairs, and use the training/test splits provided by the dataset. The same training hyperparameters from generative QA fine-tuning are adopted on the response generation task. We report the results in perplexity following (Vinyals and Le, 2015) (lower is better). We compare PALM with the competing methods including the baseline trained on the data pairs available and the pre-trained BERT+LM and MASS. Following MASS, we train every model on 10K pairs randomly sampled and all 110K training pairs. As shown in Table 5, PALM significantly performs better than all the"
2020.emnlp-main.700,P18-1177,0,0.0595509,"Missing"
2020.emnlp-main.700,N19-1409,0,0.0206538,"gradation resulted from ablating pre-training clearly demonstrates the power of PALM in leveraging an unlabeled corpus for downstream generation. 4 Related Work ELMo (Peters et al., 2018) is an early prominent pre-training method based on bidirectional LSTMs. It concatenates left-only and right-only representations, but does not pre-train interactions between these features. GPT (Radford, 2018), GPT2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) are proposed to base language modeling on the Transformer architecture, and use only the Transformer decoder for pre-training. Edunov et al. (Edunov et al., 2019) examine different strategies (e.g., ELMo) to add contextualized embeddings to sequence-to-sequence models, and observe the most improvement by adding the learned embeddings to the encoder. BERT (Devlin et al., 2018) introduces Masked Language Modelling, which allows pre-training to learn interactions between left and right context words. Recent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). However, BERT does not make predict"
2020.emnlp-main.700,D18-1443,0,0.0254376,"d, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 20"
2020.emnlp-main.700,P18-1031,0,0.0189712,"fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-theart results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to com"
2020.emnlp-main.700,D18-1054,0,0.0464293,"Missing"
2020.emnlp-main.700,W04-1013,0,0.0214703,"user queries issued to the Bing search engine and the contextual passages are from real web documents. The data has been split into a training set (153,725 QA pairs), a dev set (12,467 QA pairs) and a test set (101,092 questions with unpublished answers). To evaluate the generative capability, we focus on the Q&A + Natural Language Generation task, the goal of which is to provide the best answer available in natural language that could be used by a smart device / digital assistant. The answers are human-generated and not necessarily sub-spans of the contextual passages, so we use the ROUGE-L (Lin, 2004) metric for our evaluation to measure the quality of generated answers against the ground truth. We fine-tune the pre-trained PALM on the MARCO training set for 10 epochs. We set the batch size to 64, the learning rate to 1e-5, and the maximum input length to 512. The other hyperparameters are kept the same as pre-training. In fine-tuning PALM, the encoder takes as input x a contextual passage concatenated with a question at the end, and the decoder takes an answer as input y. During decoding, we use beam search with a beam of size 5. Table 2 presents the answer generation results on the test"
2020.emnlp-main.700,D19-1387,0,0.0578447,"Missing"
2020.emnlp-main.700,2021.ccl-1.108,0,0.129719,"Missing"
2020.emnlp-main.700,P19-1220,0,0.19461,"ing in the two stages, respectively. 2.3 Copying Tokens from Context In a human-written document, subsequent text often refers back to entities and tokens present earlier in the preceding text. Therefore, it would increase coherence of text generated in downstream to incorporate the copy mechanism into pre-training on an unlabeled corpus. This allows the model to learn from pre-training when and how to copy tokens in generating text, and the knowledge is transferred to downstream fine-tuning. PALM incorporates the copy mechanism by plugging in the pointer-generator network (See et al., 2017b; Nishida et al., 2019) on top of the decoder in Transformer. Figure 2 illustrates the pointer-generator network, which allows every token to be either generated from a vocabulary or copied from context in generating text. Extended vocabulary distribution. Let the extended vocabulary, V , be the union of words in the vocabulary and all tokens present in context. P v (yt ) then denotes the probability distribution of the t-th word token, yt , over the extended vocabulary, defined as: P v (yt ) = softmax(W e (W v st + bv )), (2) where st denotes the output representation of t-th token from the decoder. The output embe"
2020.emnlp-main.700,N18-1202,0,0.137717,"han reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-theart results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on w"
2020.emnlp-main.700,D15-1044,0,0.0675287,"on on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been propose"
2020.emnlp-main.700,P17-1099,0,0.640208,"as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language gen"
2020.emnlp-main.700,P18-1178,0,0.0422221,"Missing"
2020.emnlp-main.700,D16-1264,0,0.0618804,"Dong et al., 2019), T5 (Raffel et al., 2019), BART (Lewis et al., 2019), PEGASUS (Zhang et al., 2019) and ERNIE-GEN (Xiao et al., 2020). By consistently outperforming the pre-training methods, PALM confirms its effectiveness in leveraging unsupervision signals for language generation. 3.5 Fine-tuning on Question Generation We conduct experiments for the answer-aware question generation task. Given an input passage and an answer span, question generation aims to generate a question that leads to the answer. Following the practice in (Zhao et al., 2018; Dong et al., 2019), we use the SQuAD 1.1 (Rajpurkar et al., 2016) dataset, and the BLEU-4, METEOR and ROUGEL metrics for evaluation. As shown in Table 4, PALM outperforms all previous question generation systems and achieves a new state-of-the-art result on BLEU-4 and ROUGE-L for question generation on the SQuAD 1.1 dataset. Method CorefNQGa MP-GSNb UNILMc ERNIE d ERNIE-GENLARGE d PALM PALMLARGE BLEU-4 15.16 16.38 22.88 22.28 24.03 22.78 24.11 MTR 19.12 20.25 24.94 25.13 26.31 25.02 25.85 RG-L 44.48 51.80 50.58 52.36 50.96 52.38 Table 4: Question generation results on the SQuAD dataset. MTR is short for METEOR and RG is short for ROUGE. a (Du and Cardie, 20"
2020.emnlp-main.700,D18-1424,0,0.0990213,"lf-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 2018) and GPT-2 (Radford et al., 2019) use a leftto-right Transformer decoder to generate a text sequence token-by-"
2020.emnlp-main.700,P17-4012,0,\N,Missing
2020.emnlp-main.700,E17-2047,0,\N,Missing
2020.emnlp-main.700,P18-1015,0,\N,Missing
2020.emnlp-main.700,N19-1423,0,\N,Missing
2020.emnlp-main.700,D19-1304,0,\N,Missing
2020.findings-emnlp.140,I17-2069,0,0.0165153,"g to separate “content” and “style” of text and manipulate the style to induce transfer at inference time (Li et al., 2018; Fu et al., 2018; John et al., 2019). However, some works show that the disentanglement cannot be met and is not necessary, and leverage techniques like reconstruction and back-translation introduced in unsupervised machine translation (Lample et al., 2018), transformer (Dai et al., 2019) to achieve unsupervised style transfer. Different from style transfer, stylized response generation requires that the response is coherent with its context and the content can be varied. Akama et al. (2017) first train a basic model on a large-scale dialogue corpus and then fine-tune the model with a small stylized corpus. Niu and Bansal (2018) propose three weakly-supervised methods to generate polite responses using non-parallel data. Gao et al. (2019) build a structured latent space sharing between conversation modeling and style transfer. However, limited by the sparsity of the latent space, it is difficult to balance the style and contextual coherence while sampling in the neighborhood of the latent code of context at inference time. Pretraining Methods have led remarkable success in variou"
2020.findings-emnlp.140,N19-1423,0,0.0215877,"nsal (2018) propose three weakly-supervised methods to generate polite responses using non-parallel data. Gao et al. (2019) build a structured latent space sharing between conversation modeling and style transfer. However, limited by the sparsity of the latent space, it is difficult to balance the style and contextual coherence while sampling in the neighborhood of the latent code of context at inference time. Pretraining Methods have led remarkable success in various NLP tasks which demonstrates its great capabilities in language understanding and text generation (Radford et al., 2018, 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Conneau and Lample, 2019; Clark et al., 2020). Recently, the pretraining methods have also been used to tackle the key challenges in dialogue systems such as context representation (Mehri et al., 2019), response selection (Henderson and Su, 2019), knowledge-grounded response 1549 generation (Zhao et al., 2020) and personalized response generation (Zheng et al., 2019). In particular, the large-scale pre-trained open-domain dialogue systems (Zhang et al., 2019b; Adiwardana et al., 2020) make a large step towards human-like chatbot against previous works whi"
2020.findings-emnlp.140,D12-1139,0,0.0820403,"Missing"
2020.findings-emnlp.140,D19-1190,0,0.152941,"Missing"
2020.findings-emnlp.140,P16-1094,0,0.0422656,"ask and achieve promising results (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Since then, various architectures have been proposed to address the key challenges in open-domain dialogue systems, including suppressing the generic responses (Li et al., 2015; Zhao et al., 2017; Xing et al., 2017a), context modeling (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a), controlling the attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019) and incorporating different types knowledge into generation (Li et al., 2016; Zhang et al., 2018b; Zhou et al., 2017; Zhao et al., 2020). In this work, we study the problem of stylized response generation, which aims to incorporate the style information from non-parallel data into the generation process. Stylized Text Generation has attracted broad interest in recent years, especially the style transfer, which aims to alter one or more attributes of text while preserving the content. A prevalent idea of unsupervised style transfer is learning to separate “content” and “style” of text and manipulate the style to induce transfer at inference time (Li et al., 2018; Fu et"
2020.findings-emnlp.140,N18-1169,0,0.130778,"Missing"
2020.findings-emnlp.140,I17-1061,0,0.297754,"tyle. Such research could facilitate developers to customize their dialogue systems in terms of response styles, and thus broaden applications of the systems, from a social companion (Shum et al., 2018) or a virtual assistant (Ram et al., 2018) to a variety of vertical scenarios such as customer service (requiring a polite style), virtual characters in games (requiring specific personas), assistants in specific domains (requiring domain knowledge), etc. Normally, a target style is specified by a non-conversational corpus (e.g., novels, news, blogs, etc.) apart from the paired dialogue corpus (Luan et al., 2017; Niu and Bansal, 2018; Gao et al., 2019). Thus, the major challenge of the task lies in the scarcity of paired data for learning the correspondence between conversation contexts and proper responses in the desired style, which is a key factor in success of the neural dialogue models developed so far. As a result, it is very likely that a response either digresses from the context of the current dialogue (Luan et al., 2017; Gao et al., 2019), or loses fidelity to the target style (Niu and Bansal, 2018). We consider addressing the challenge by taking advantage of the large scale pre-trained lan"
2020.findings-emnlp.140,P19-1373,0,0.0117602,"arsity of the latent space, it is difficult to balance the style and contextual coherence while sampling in the neighborhood of the latent code of context at inference time. Pretraining Methods have led remarkable success in various NLP tasks which demonstrates its great capabilities in language understanding and text generation (Radford et al., 2018, 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Conneau and Lample, 2019; Clark et al., 2020). Recently, the pretraining methods have also been used to tackle the key challenges in dialogue systems such as context representation (Mehri et al., 2019), response selection (Henderson and Su, 2019), knowledge-grounded response 1549 generation (Zhao et al., 2020) and personalized response generation (Zheng et al., 2019). In particular, the large-scale pre-trained open-domain dialogue systems (Zhang et al., 2019b; Adiwardana et al., 2020) make a large step towards human-like chatbot against previous works which rely on complex frameworks developed over many years. On this basis, we propose to study the open-domain stylized response generation with pre-trained models in this work. 3 oxt+1 , Ht+1 = Transformer(ext , Ht ), Approach We employ Dialo"
2020.findings-emnlp.140,Q18-1027,0,0.423159,"could facilitate developers to customize their dialogue systems in terms of response styles, and thus broaden applications of the systems, from a social companion (Shum et al., 2018) or a virtual assistant (Ram et al., 2018) to a variety of vertical scenarios such as customer service (requiring a polite style), virtual characters in games (requiring specific personas), assistants in specific domains (requiring domain knowledge), etc. Normally, a target style is specified by a non-conversational corpus (e.g., novels, news, blogs, etc.) apart from the paired dialogue corpus (Luan et al., 2017; Niu and Bansal, 2018; Gao et al., 2019). Thus, the major challenge of the task lies in the scarcity of paired data for learning the correspondence between conversation contexts and proper responses in the desired style, which is a key factor in success of the neural dialogue models developed so far. As a result, it is very likely that a response either digresses from the context of the current dialogue (Luan et al., 2017; Gao et al., 2019), or loses fidelity to the target style (Niu and Bansal, 2018). We consider addressing the challenge by taking advantage of the large scale pre-trained language models. The basi"
2020.findings-emnlp.140,P02-1040,0,0.107114,"he validation/test sets, and each context has at least 4 responses. Task arXiv-style Holmes-style Training Dconv Dstyle Reddit arXiv 10,000,000 1,347,538 Reddit Holmes 10,000,000 38,309 Validation Test Dval Dtest arXiv-style Reddit 2,000 2,000 Holmes-style Reddit 2,000 2,000 Table 1: Tasks and datasets 5.2 Evaluation Methodology We compare different models with both automatic metrics and human judgment. Automatic Metrics. For automatic evaluation, we measure the quality of generated responses from three aspects: Style Consistency, Relevance, and Diversity. The relevance is measured with BLEU (Papineni et al., 2002) and Rouge (Lin, 2004) 7 . To evaluate diversity, we follow Li et al. (2015) and use Distinct-1 (Dist-1) and Distinct-2 (Dist-2) as metrics which are calculated as ratios of distinct unigrams and bigrams in responses, respectively. In terms of style consistency, existing work only measures the style intensity using classifiers (Gao et al., 2019). However, the style of text is an amalgam, and differences between two styles are reflected in multiple linguistic dimensions (Verma and Srinivasan, 2019). Thus, we propose to evaluate the style of response from three perspectives: (1) Intensity: we re"
2020.findings-emnlp.140,P12-1000,0,0.228947,"Missing"
2020.findings-emnlp.140,P15-1152,0,0.0460083,"re three-fold: (1) proposal of tackling the problem of stylized response generation with pre-trained language models; (2) proposal of a word-level objective and a sentence-level objective in fine-tuning of a pre-trained language model for the task; and (3) empirical verification of the effectiveness of the proposed method on public datasets. 2 Related Work Open-domain Dialogue Generation has received more and more attention in NLP community. Inspired by neural machine translation, early works apply the sequence-to-sequence model to this task and achieve promising results (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Since then, various architectures have been proposed to address the key challenges in open-domain dialogue systems, including suppressing the generic responses (Li et al., 2015; Zhao et al., 2017; Xing et al., 2017a), context modeling (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a), controlling the attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019) and incorporating different types knowledge into generation (Li et al., 2016; Zhang et al., 2018b; Zhou et al., 2017; Zhao et al., 2020"
2020.findings-emnlp.140,P19-1538,1,0.806335,"n has received more and more attention in NLP community. Inspired by neural machine translation, early works apply the sequence-to-sequence model to this task and achieve promising results (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Since then, various architectures have been proposed to address the key challenges in open-domain dialogue systems, including suppressing the generic responses (Li et al., 2015; Zhao et al., 2017; Xing et al., 2017a), context modeling (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a), controlling the attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019) and incorporating different types knowledge into generation (Li et al., 2016; Zhang et al., 2018b; Zhou et al., 2017; Zhao et al., 2020). In this work, we study the problem of stylized response generation, which aims to incorporate the style information from non-parallel data into the generation process. Stylized Text Generation has attracted broad interest in recent years, especially the style transfer, which aims to alter one or more attributes of text while preserving the content. A prevalent idea of unsupervised"
2020.findings-emnlp.140,P19-1362,0,0.105059,"ilability of huge amount of human conversations on social media, there has been significant progress on building open-domain dialogue systems with natural language generation techniques. Though neural generative models are notorious for replying with bland responses (Li et al., 2015), some very recent work demonstrates that response generation models learned with pre-training techniques (Radford et al., 2019) can effectively overcome the deficiency suffered by previous models and are capable of having smooth conversations with humans through reasonable and specific replies (Wolf et al., 2019; Zhang et al., 2019b). The compelling performance exhibited by the pre-trained dialogue models encourages us to explore more difficult yet important problems in conversational AI. In this work, we study stylized response generation, that is responses provided by a ∗ Corresponding Author model should not only be coherent with the conversation contexts, but also be consistent with a designated style. Such research could facilitate developers to customize their dialogue systems in terms of response styles, and thus broaden applications of the systems, from a social companion (Shum et al., 2018) or a virtual assista"
2020.findings-emnlp.140,P18-1102,0,0.0135858,"on in NLP community. Inspired by neural machine translation, early works apply the sequence-to-sequence model to this task and achieve promising results (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Since then, various architectures have been proposed to address the key challenges in open-domain dialogue systems, including suppressing the generic responses (Li et al., 2015; Zhao et al., 2017; Xing et al., 2017a), context modeling (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a), controlling the attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019) and incorporating different types knowledge into generation (Li et al., 2016; Zhang et al., 2018b; Zhou et al., 2017; Zhao et al., 2020). In this work, we study the problem of stylized response generation, which aims to incorporate the style information from non-parallel data into the generation process. Stylized Text Generation has attracted broad interest in recent years, especially the style transfer, which aims to alter one or more attributes of text while preserving the content. A prevalent idea of unsupervised style transfer is learning to separate"
2020.findings-emnlp.140,P18-1205,0,0.0236017,"on in NLP community. Inspired by neural machine translation, early works apply the sequence-to-sequence model to this task and achieve promising results (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Since then, various architectures have been proposed to address the key challenges in open-domain dialogue systems, including suppressing the generic responses (Li et al., 2015; Zhao et al., 2017; Xing et al., 2017a), context modeling (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a), controlling the attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019) and incorporating different types knowledge into generation (Li et al., 2016; Zhang et al., 2018b; Zhou et al., 2017; Zhao et al., 2020). In this work, we study the problem of stylized response generation, which aims to incorporate the style information from non-parallel data into the generation process. Stylized Text Generation has attracted broad interest in recent years, especially the style transfer, which aims to alter one or more attributes of text while preserving the content. A prevalent idea of unsupervised style transfer is learning to separate"
2020.findings-emnlp.140,P17-1061,0,0.0331139,"anguage model for the task; and (3) empirical verification of the effectiveness of the proposed method on public datasets. 2 Related Work Open-domain Dialogue Generation has received more and more attention in NLP community. Inspired by neural machine translation, early works apply the sequence-to-sequence model to this task and achieve promising results (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015). Since then, various architectures have been proposed to address the key challenges in open-domain dialogue systems, including suppressing the generic responses (Li et al., 2015; Zhao et al., 2017; Xing et al., 2017a), context modeling (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a), controlling the attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019) and incorporating different types knowledge into generation (Li et al., 2016; Zhang et al., 2018b; Zhou et al., 2017; Zhao et al., 2020). In this work, we study the problem of stylized response generation, which aims to incorporate the style information from non-parallel data into the generation process. Stylized Text Generation has attracted broad inte"
2020.findings-emnlp.412,P19-1285,0,0.0313814,"s maximum sequence length limits (512). A vanilla strategy is to truncate or split the documents: Dai and Callan (2019) applied BERT ranking on each passage segmented from the document independently and explored different ways to combine the passage ranking scores, using the score of the first passage (BERT-FirstP), the best passage (BERT-MaxP) (also studied in Yan et al. (2020)), or the sum of all passage scores (BERT-SumP). More sophisticated approaches have also been developed to introduce structures to transformer attentions. Transformer-XL employs recurrence on a sequence of text pieces (Dai et al., 2019), Transformer-XH (Zhao et al., 2020) models a group of text sequences by linking them with eXtra Hop attention paths, and Transformer Kernel Long (TKL) (Hofst¨atter et al., 2020) uses a sliding window over the document terms and matches them with the query terms using matching kernels (Xiong et al., 2017). On the efficiency front, Kitaev et al. (2020) proposed Reformer that employed locality-sensitive hashing and reversible residual layers to improve the efficiency of Transformers. Child et al. (2019) introduced sparse transformers to reduce √ the quadratic complexity to O(L L) by applying spa"
2020.findings-emnlp.412,W19-4828,0,0.113708,"000 or 2000 tokens, which is often the case in document ranking (Craswell et al., 2020), the self-attention part becomes the main bottleneck in both computation and GPU memory. This leads to various retrofitted solutions that adapted the document ranking tasks to standard BERT which takes at most 512 tokens per sequence (Dai and Callan, 2019; Yang et al., 2019; Yan et al., 2020; Nogueira et al., 2019). 4596 4 QDS-Transformer Recent research has shown that with sufficient training and fully-connected self-attention, BERT learns attention patterns that capture meaningful structures in language (Clark et al., 2019) or for specific tasks (Zhao et al., 2020). However, this is not yet the case in long document ranking as computing becomes the bottleneck. This section first presents how we overcome this bottleneck by injecting IR-specific inductive bias as sparse attention patterns. Then we discuss the efficient implementation of sparse attention. 4.1 Query-Directed Sparse Attention Mathematically, inducing sparsity in self-attention is to modify the attention adjacency matrix A by only keeping connections that are meaningful for the task. For document retrieval, we include two groups of informative connect"
2020.findings-emnlp.412,D19-1223,0,0.0350529,"Missing"
2020.findings-emnlp.412,N19-1423,0,0.561024,"er over previous approaches, as they either retrofit long documents into BERT or use sparse attention without emphasizing IR principles. We further quantify the computing complexity and demonstrates that our sparse attention with TVM implementation is twice more efficient that the fully-connected selfattention. All source codes, trained model, and predictions of this work are available at https://github.com/hallogameboy/ QDS-Transformer. 1 Figure 1: An example illustration of the attention mechanism used in Query-Directed Sparse Transformer. Introduction Pre-trained Transformers such as BERT (Devlin et al., 2019) effectively transfer language understanding to better relevance estimation in many search ranking tasks (Nogueira and Cho, 2019; Nogueira et al., 2019; Yang et al., 2019). Nevertheless, the effectiveness comes at the quadratic cost O(n2 ) in computing complexity corresponds to the text length n, prohibiting its direct application to long documents. Prior work adopts quick workarounds such as document truncation or splitting-and-pooling to retrofit the document ranking task to pretrained transformers. Whilst there have been successes with careful architecture design, those bandit-solutions ine"
2020.findings-emnlp.412,D17-1110,0,0.0499226,"Missing"
2020.findings-emnlp.412,2021.ccl-1.108,0,0.0714318,"Missing"
2020.findings-emnlp.412,N16-1174,0,0.39201,"ia et al., 2019; Sukhbaatar et al., 2019; Beltagy et al., 2020). 4.1.2 Query-Directed Global Attention The local attention itself does not fully capture the relevance matches between the query and documents. We introduce several query-directed attention patterns to incorporate inductive biases widely used in document representation and ranking. Hierarchical Document Structures. A common intuition in document representation is to leverage the hierarchical structures within documents, for example, words, sentences, paragraphs, and sections, and compose them into hierarchical attention networks (Yang et al., 2016). We use a two-level word-sentence-document hierarchy and inject this hierarchical structure by adding fully connected attention paths to all the sentences. Specifically, we first prepend a special token [SOS] (start-of-sentence) to each sentence in the document, and form the following attention connections: Asent [i, j] = 1, iff tj = [SOS]. (8) Matching with the Query. For retrieval tasks, arguably the most important principle is to capture the semantic matching between queries and documents. Inducing this information is as simple as adding dedicated attention paths on query terms: Aquery [i,"
2020.findings-emnlp.412,N16-1000,0,0.176926,"Missing"
2020.findings-emnlp.412,P19-1032,0,0.148053,"Kernel Long (TKL) (Hofst¨atter et al., 2020) uses a sliding window over the document terms and matches them with the query terms using matching kernels (Xiong et al., 2017). On the efficiency front, Kitaev et al. (2020) proposed Reformer that employed locality-sensitive hashing and reversible residual layers to improve the efficiency of Transformers. Child et al. (2019) introduced sparse transformers to reduce √ the quadratic complexity to O(L L) by applying sparse factorizations to the attention matrix, making the use of self-attention possible for extremely long sequences. Subsequent work (Sukhbaatar et al., 2019; Correia et al., 2019) leverage a similar idea in a more adaptive way. Combining local windowed attention with a task motivated global attention, Beltagy et al. (2020) presented Longformer with an attention mechanism that scales linearly with sequence length. 3 Preliminaries on Document Ranking Given a query q and a set of candidate documents D = {d}, the document ranking task is to produce the ranking score f (q, d) for each candidate 4595 Fully-connected Layer Ranking Score f (q, d) L-th Transformer Layer (L 1)-th Transformer Layer (L 2)-th Transformer Layer .. . ··· ··· ··· ··· ··· ··· ···"
2020.findings-emnlp.412,P19-1452,0,0.0634481,"Missing"
2020.ijclclp-2.1,O07-6001,0,0.198166,"Missing"
2020.ijclclp-2.1,O99-3003,1,0.663956,"Missing"
2020.ijclclp-2.1,D15-1166,0,0.156148,"Missing"
2020.ijclclp-2.1,O12-2003,0,0.046416,"Missing"
2021.acl-long.308,P17-1178,0,0.0223577,"ision training using 64 Nvidia Telsa V100 32GB GPUs. Appendix A shows additional details. 5 http://opus.nlpl.eu/ 5.1 Experiments on Cross-lingual Understanding Tasks Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details about the datasets. Fine-tuning Setting Following previous works (Conneau et al., 201"
2021.acl-long.308,W18-6319,0,0.012074,"ize Baseline 42.9 Liu et al. (2020a) 43.8 WMT14 En-De BLEU SacreBLEU 30 29 40.4 41.8 28.7 30.1 27.8 29.5 Randomly Initialize + More Bilingual Data* Baseline* 30.6 29.5 Cross-lingual Model Initialize mBART 43.2 mRASP 44.3 XLM-R 43.8 VECO 44.5 29.1 29.9 30.6 41.0 41.7 41.2 42.0 30.0 30.3 30.9 31.7 sacreBLEU Model 28 27 26 VECO Init. XLM-R Init. Random Init. 25 10 15 20 25 Epochs 30 35 Table 3: (left) Results on machine translation. (right) Learning curves of different initialization methods. tokenized SacreBLEU 7 to avoid the influence of different tokenization and normalization between models (Post, 2018). Fine-tuning Setting We fine-tune our model using fairseq 8 toolkit and adopt comparable training settings with baselines. We run WMT 14 EnDe and En-Fr MT experiments on 16 and 32 V100 GPUs, respectively. The batch size is 64k for EnDe and 256k for En-Fr. The total training updates are set to 100k. The learning rate is 1e-4/2e-4, with linear warm-up over the first 16k steps and linear decay. We average the last 10 checkpoints and use beam search with a beam size of 5. Baselines We consider two types of Transformer baselines: randomly initialized and cross-lingual models initialized. For rando"
2021.acl-long.308,D19-1071,0,0.0281093,"lable in the downstream task. Specifically, we concatenated the two repreL sentations [HL x : Sx ] to predict the label of x, L L [Hy : Sy ] to predict the label of y. 4 . 3.2 For pre-trained encoders like XLM, it is not a trivial problem to incorporate them into the sequenceto-sequence architecture – the mainstream backbone model of generation tasks (Zhu et al., 2020). One of the drawbacks or challenges could be that the encoder-to-decoder attention is not pre-trained. Therefore, the parameters of the decoder need to be re-adjusted along with the encoder in the following fine-tuning process (Ren et al., 2019). However, under the framework of V E C O , the cross-attention is jointly pre-trained along with the whole network, making it easy to provide full initialization for sequence-to-sequence models. Specifically, the self-attention module is used to initialize both the corresponding modules in the encoder and decoder for contextual modeling, while the cross-attention module is used to initialize the encoder-to-decoder attention. It’s okay whether you continue to tie the self-attention parameters during fine-tuning. Directly pre-training a sequenceto-sequence model like mBART (Liu et al., 2020b) c"
2021.acl-long.308,D19-1382,0,0.0282293,"rgence. Then, we jointly train the whole model. We pre-train our model with mixed-precision training using 64 Nvidia Telsa V100 32GB GPUs. Appendix A shows additional details. 5 http://opus.nlpl.eu/ 5.1 Experiments on Cross-lingual Understanding Tasks Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details"
2021.acl-long.308,W17-2512,0,0.015424,"s Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details about the datasets. Fine-tuning Setting Following previous works (Conneau et al., 2019; Hu et al., 2020), we consider two typical fine-tuning settings: (1) Cross-lingual Transfer which fine-tunes the pre-trained model using English golden data only and dire"
2021.acl-long.493,N19-1423,0,0.522311,"oduction Document understanding is an essential problem in NLP, which aims to read and analyze textual documents. In addition to plain text, many realworld applications require to understand scanned documents with rich text. As shown in Figure 1, such scanned documents contain various structured information, like tables, digital forms, receipts, and invoices. The information of a document image is usually presented in natural language, but the format can be organized in many ways from multicolumn layout to various tables/forms. Inspired by the recent development of pretrained language models (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019) in various NLP tasks, recent studies on document image pretraining (Zhang et al., 2020; Xu et al., 2019) have pushed the limits of a variety of document image understanding tasks, which learn the interaction between text and layout information across scanned document images. Xu et al. (2019) propose LayoutLM, which is a pre-training method of text and layout for document image understanding tasks. It uses 2Dposition embeddings to model the word-level layout information. However, it is not enough to model the word-level layout information, and the model sh"
2021.acl-long.493,D18-1476,0,0.0575342,"Missing"
2021.acl-long.493,2021.ccl-1.108,0,0.0536789,"Missing"
2021.acl-long.493,D16-1264,0,0.0509289,"tasks, each of which contains form images. These three tasks are form understanding task, document visual question answering task, and document image classification task. For the form understanding task, StructuralLM predicts B, I, E, S, O tags for each token, and then uses sequential labeling to find the four types of entities including the question, answer, header, or other. For the document visual question answering task, we treat it as an extractive QA task and build a token-level classifier on the top of token representations, which is usually used in Machine Reading Comprehension (MRC) (Rajpurkar et al., 2016; Wang et al., 2018). For the document image classification task, StructuralLM predicts the class labels using the representation of the [CLS] token. 3 Experiments 3.1 Pre-training Configuration Pre-training Dataset. Following LayoutLM, we pre-train StructuralLM on the IIT-CDIP Test Collection 1.0 (Lewis et al., 2006). It is a large-scale scanned document image dataset, which contains more than 6 million documents, with more than 11 million scanned document images. The pretraining dataset (IIT-CDIP Test Collection) only contains pure texts while missing their corresponding bounding boxes. Ther"
2021.acl-long.493,D19-1348,0,0.0269467,"6315 Figure 5: Examples of the output of LayoutLM and StructuralLM on the FUNSD dataset. The division of |means that the two phrases are independent labels. et al., 2005; Wei et al., 2013), they are usually time-consuming to design manually features and difficult to obtain a high-level abstract semantic context. In addition, these methods usually relied on visual cues but ignored textual information. 4.2 Deep Learning Approaches Nowadays, deep learning methods have become the mainstream for many machine learning problems (Yang et al., 2017; Borges Oliveira and Viana, 2017; Katti et al., 2018; Soto and Yoo, 2019). (Yang et al., 2017) propose a pixel-by-pixel classification to solve the document semantic structure extraction problem. Specifically, they propose a multimodal neural network that considers visual and textual information, while this work is an end-toend approach. (Katti et al., 2018) first propose a fully convolutional encoder-decoder network to predict a segmentation mask and bounding boxes. In this way, the model significantly outperforms approaches based on sequential text or document images. In addition, (Soto and Yoo, 2019) incorporate contextual information into the Faster R-CNN model"
2021.acl-long.493,P18-1158,1,0.847945,"tains form images. These three tasks are form understanding task, document visual question answering task, and document image classification task. For the form understanding task, StructuralLM predicts B, I, E, S, O tags for each token, and then uses sequential labeling to find the four types of entities including the question, answer, header, or other. For the document visual question answering task, we treat it as an extractive QA task and build a token-level classifier on the top of token representations, which is usually used in Machine Reading Comprehension (MRC) (Rajpurkar et al., 2016; Wang et al., 2018). For the document image classification task, StructuralLM predicts the class labels using the representation of the [CLS] token. 3 Experiments 3.1 Pre-training Configuration Pre-training Dataset. Following LayoutLM, we pre-train StructuralLM on the IIT-CDIP Test Collection 1.0 (Lewis et al., 2006). It is a large-scale scanned document image dataset, which contains more than 6 million documents, with more than 11 million scanned document images. The pretraining dataset (IIT-CDIP Test Collection) only contains pure texts while missing their corresponding bounding boxes. Therefore, we need to re"
2021.acl-long.97,N19-1253,0,0.0280083,"ultilingual model can generalize to unseen languages; perhaps training on multiple languages regularizes the model to produce replies in the input language. Overall, the best method to generalize the generation model across languages is to use machine-translated data. 1214 6 Future Work opens up opportunities for future research. Our experiments use four training settings (Section 3.2), but there are many other settings to explore. For example, we can use other combinations of training languages, which may work better for some target languages (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a). We are also interested in training on both organic data and MT data; i.e., mixing the zero-shot and MT setting. We can also compare other models on MRS. For the English monolingual setting, we can initialize the generation model with state-of-the-art language models (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2020c). For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ 2 SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 20"
2021.acl-long.97,W19-4330,0,0.0165415,"setting. We can also compare other models on MRS. For the English monolingual setting, we can initialize the generation model with state-of-the-art language models (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2020c). For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ 2 SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020). For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020). Another idea is to combine the two models. Given an input message, we first use a generation model to create a set of candidate replies. We then use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization,"
2021.acl-long.97,2020.tacl-1.30,0,0.0403378,"two approaches have different advantages. Generation models are more powerful because they are not constrained by the response set. In comparison, retrieval models are easier to train and runs faster, and a curated response set guarantees the coherence and the safety of the model output. The two frameworks make reply suggestion an interesting task for studying cross-lingual generalization. Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b). In contrast, reply suggestion has two formulations that require different cross-lingual generalization strategies. While some recent work explores cross-lingual transfer 1207 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1207–1220 August 1–6, 2021. ©2021 Association for Computational Linguistics Language English Spanish German Portuguese French Japanese Swedish Italian Dutch Russian Code Family EN West Germanic Romance West Germani"
2021.acl-long.97,D18-1269,0,0.0228379,"eply from scratch (Kannan et al., 2016). The two approaches have different advantages. Generation models are more powerful because they are not constrained by the response set. In comparison, retrieval models are easier to train and runs faster, and a curated response set guarantees the coherence and the safety of the model output. The two frameworks make reply suggestion an interesting task for studying cross-lingual generalization. Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b). In contrast, reply suggestion has two formulations that require different cross-lingual generalization strategies. While some recent work explores cross-lingual transfer 1207 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1207–1220 August 1–6, 2021. ©2021 Association for Computational Linguistics Language English Spanish German Portuguese French Japanese Swedish Italian Dutch Russian Code Fa"
2021.acl-long.97,D17-1078,0,0.0173095,"earning. Interestingly, the multilingual model can generalize to unseen languages; perhaps training on multiple languages regularizes the model to produce replies in the input language. Overall, the best method to generalize the generation model across languages is to use machine-translated data. 1214 6 Future Work opens up opportunities for future research. Our experiments use four training settings (Section 3.2), but there are many other settings to explore. For example, we can use other combinations of training languages, which may work better for some target languages (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a). We are also interested in training on both organic data and MT data; i.e., mixing the zero-shot and MT setting. We can also compare other models on MRS. For the English monolingual setting, we can initialize the generation model with state-of-the-art language models (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2020c). For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ 2 SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2"
2021.acl-long.97,N19-2006,1,0.489487,"chatbots (Adiwardana et al., 2020; Huang et al., 2020). While both are conversational AI tasks (Gao et al., 2019), the goals are different: reply suggestion systems help the user quickly reply to a message, while chatbots aim to continue the conversation and focus more on multi-turn dialogues. Ideally, we want our model to generate replies in any language. However, reply suggestion models require large training sets, so previous work mostly ∗ † Work mostly done as an intern at Microsoft Research. Work done at Microsoft Research. focuses on English (Kannan et al., 2016; Henderson et al., 2017; Deb et al., 2019). To investigate reply suggestion for other languages with possibly limited data, we build a multilingual dataset, dubbed MRS (Multilingual Reply Suggestion). From publicly available Reddit threads, we extract messagereply pairs, response sets, and machine-translated examples in ten languages (Table 1). One interesting aspect of the reply suggestion problem is that there are two modeling approaches. Some models follow the retrieval framework and select the reply from a predetermined response set (Henderson et al., 2017). Others follow the generation framework and generate the reply from scratc"
2021.acl-long.97,Q18-1039,0,0.0275643,"andidate replies. We then use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generation model. Future work can study methods to improve cross-lingual generalization methods. Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the loop (Yuan et al., 2020). MRS 7 Conclusion We present MRS, a multilingual dataset for reply suggestion. We compare a generation and a retrieval baseline on MRS. The two models have different strengths in the English monolingual setting and require different strategies to transfer across languages. MRS provides a benchmark for future research in both reply suggestion and cross-lingual transfer learning. E"
2021.acl-long.97,N19-1423,0,0.530295,"se set by translating the English response set to other languages with Microsoft Translator. The non-English response set is sometimes smaller than the English set, because different English responses may have the same translation. 2.2 Filtering Toxic Examples Exchanges on Reddit are sometimes uncivil, inappropriate, or even abusive (Massanari, 2017; Mohan et al., 2017). We try to filter out toxic contents, as they are not desirable for reply suggestion systems. We use two toxicity detection models. First, we use an in-house multilingual model. The model is initialized with multilingual BERT (Devlin et al., 2019, MBERT) and fine-tuned on a mixture of proprietary and public datasets with toxic and offensive language labels. The model outputs a score from zero to one, with a higher score corresponding to a higher level of toxicity. Second, we use Perspective API2 , a publicly available model. Perspective API has limited free access (one query per second), so we only use the API on the English validation, test, and response set. For other languages, we rely on our in-house model. We filter message-reply pairs if it has greater than 0.9 score according to the in-house model, or greater than 0.5 score acc"
2021.acl-long.97,P06-2034,0,0.0892941,"al., 2020a; Xue et al., 2020). For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020). Another idea is to combine the two models. Given an input message, we first use a generation model to create a set of candidate replies. We then use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generation model. Future work can study methods to improve cross-lingual generalization methods. Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the loop (Yuan et al., 2020). MRS 7 Conclusion We p"
2021.acl-long.97,2020.findings-emnlp.301,0,0.0324301,"a mixture of proprietary and public datasets with toxic and offensive language labels. The model outputs a score from zero to one, with a higher score corresponding to a higher level of toxicity. Second, we use Perspective API2 , a publicly available model. Perspective API has limited free access (one query per second), so we only use the API on the English validation, test, and response set. For other languages, we rely on our in-house model. We filter message-reply pairs if it has greater than 0.9 score according to the in-house model, or greater than 0.5 score according to Perspective API (Gehman et al., 2020). About one percent of examples are filtered. After filtering the data, we manually validate three hundred random examples and do not find any toxic examples, which confirms that our filter method have a high recall. While we hope the filtered dataset leads to better reply suggestion models, existing filtering methods are not perfect and can introduce other biases (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). Therefore, models trained on all MRS data may still have undesirable behavior. MRS is intended to be used as a benchmark for testing cross-lingual generalization of gen"
2021.acl-long.97,N19-1383,0,0.0182449,"use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generation model. Future work can study methods to improve cross-lingual generalization methods. Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the loop (Yuan et al., 2020). MRS 7 Conclusion We present MRS, a multilingual dataset for reply suggestion. We compare a generation and a retrieval baseline on MRS. The two models have different strengths in the English monolingual setting and require different strategies to transfer across languages. MRS provides a benchmark for future research in both reply suggestion and cross-lingual transfer learning. Ethical Considerations Data"
2021.acl-long.97,2020.acl-main.653,0,0.181193,"ages. Generation models are more powerful because they are not constrained by the response set. In comparison, retrieval models are easier to train and runs faster, and a curated response set guarantees the coherence and the safety of the model output. The two frameworks make reply suggestion an interesting task for studying cross-lingual generalization. Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b). In contrast, reply suggestion has two formulations that require different cross-lingual generalization strategies. While some recent work explores cross-lingual transfer 1207 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1207–1220 August 1–6, 2021. ©2021 Association for Computational Linguistics Language English Spanish German Portuguese French Japanese Swedish Italian Dutch Russian Code Family EN West Germanic Romance West Germanic Romance Romance Japonic North Germa"
2021.acl-long.97,N16-1014,0,0.262396,"rence reply when computing the final metrics; i.e., the model only has to provide one “correct” reply to have a full score. We compare models primarily with ROUGE, since the metric has the best correlation in the pilot study. Nevertheless, word overlap scores have known limitations (Liu et al., 2016), as there are different ways to reply to a message. We encourage future research to investigate other metrics to understand different aspects of the model. As examples, we also report two diversity scores: the proportion of distinct unigrams (Dist-1) and bigrams (Dist-2) in the generated replies (Li et al., 2016). While ROUGE measures the relevance of the replies, higher diversity can also increase CTR (Deb et al., 2019). We can improve the diversity of the three replies with diversity-promoting decoding (Li et al., 2016; Vijayakumar et al., 2018; Zhang et al., 2018) or latent variable models (Deb et al., 2019), but we leave this direction to future work. For our English monolingual experiments, we also complement automatic metrics with human judgments (Human in Figure 2). For each example, we display the input message and sets of three suggested replies from both generation and retrieval models to th"
2021.acl-long.97,2020.acl-main.487,0,0.0118371,"n our in-house model. We filter message-reply pairs if it has greater than 0.9 score according to the in-house model, or greater than 0.5 score according to Perspective API (Gehman et al., 2020). About one percent of examples are filtered. After filtering the data, we manually validate three hundred random examples and do not find any toxic examples, which confirms that our filter method have a high recall. While we hope the filtered dataset leads to better reply suggestion models, existing filtering methods are not perfect and can introduce other biases (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). Therefore, models trained on all MRS data may still have undesirable behavior. MRS is intended to be used as a benchmark for testing cross-lingual generalization of generation and retrieval models. The dataset should not be directly used in production systems. To use the dataset in practice, additional work is required to address other possible biases and toxic or inappropriate content that may exist in the data. 3 Experiment Settings After presenting the dataset, we explain how we use MRS to compare reply suggestion models. We describe the two frameworks for reply suggestion, our experiment"
2021.acl-long.97,W04-1013,0,0.0191605,"RR and precision on a subset of examples where the reference reply is in the response set so that we can directly measure the rank of the reference response in the response set. This set also allows us to compute MRR for individual responses, so we can compute macro-MRR, the average MRR over each response in the set. Higher macro-MRR can indicate diversity but has a worse correlation than computing MRR over the entire test set. For the generation model, we consider model perplexity (Adiwardana et al., 2020). Finally, we consider two word overlap scores, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which can be used for both retrieval and generation models. Our pilot study shows that ROUGE has the best correlation. However, individual ROUGE F1 scores (ROUGE -1/2/3) are sensitive to small changes in sequence lengths (more so because our responses are generally short). Therefore, we use a weighted average of the three scores: ROUGE -1 ROUGE -2 ROUGE -3 + + . 6 3 2 (2) This weighted score leads to the highest correlation with CTR. Intuitively, the weights balance the differences in the average magnitude of each metric and thus reduce variance on short responses. Popular reply suggestion s"
2021.acl-long.97,D16-1230,0,0.0325801,"simulate this setting, we predict three replies for each message. For the retrieval model, we use the three highest-scoring replies from the response set. For the generation model, we use top-three results from beam search. Out of the three replies, we only use the reply with the highest ROUGE compared to the reference reply when computing the final metrics; i.e., the model only has to provide one “correct” reply to have a full score. We compare models primarily with ROUGE, since the metric has the best correlation in the pilot study. Nevertheless, word overlap scores have known limitations (Liu et al., 2016), as there are different ways to reply to a message. We encourage future research to investigate other metrics to understand different aspects of the model. As examples, we also report two diversity scores: the proportion of distinct unigrams (Dist-1) and bigrams (Dist-2) in the generated replies (Li et al., 2016). While ROUGE measures the relevance of the replies, higher diversity can also increase CTR (Deb et al., 2019). We can improve the diversity of the three replies with diversity-promoting decoding (Li et al., 2016; Vijayakumar et al., 2018; Zhang et al., 2018) or latent variable models"
2021.acl-long.97,2020.tacl-1.47,0,0.0206726,"ges (Ammar et al., 2016; Cotterell and Heigold, 2017; Ahmad et al., 2019; Lin et al., 2019; Zhang et al., 2020a). We are also interested in training on both organic data and MT data; i.e., mixing the zero-shot and MT setting. We can also compare other models on MRS. For the English monolingual setting, we can initialize the generation model with state-of-the-art language models (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2020c). For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ 2 SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020). For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020). Another idea is to combine the two models. Given an input message, we first use a generation model to create a set of candidate replies. We then use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al.,"
2021.acl-long.97,P02-1040,0,0.109385,"onses. Alternatively, we compute MRR and precision on a subset of examples where the reference reply is in the response set so that we can directly measure the rank of the reference response in the response set. This set also allows us to compute MRR for individual responses, so we can compute macro-MRR, the average MRR over each response in the set. Higher macro-MRR can indicate diversity but has a worse correlation than computing MRR over the entire test set. For the generation model, we consider model perplexity (Adiwardana et al., 2020). Finally, we consider two word overlap scores, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which can be used for both retrieval and generation models. Our pilot study shows that ROUGE has the best correlation. However, individual ROUGE F1 scores (ROUGE -1/2/3) are sensitive to small changes in sequence lengths (more so because our responses are generally short). Therefore, we use a weighted average of the three scores: ROUGE -1 ROUGE -2 ROUGE -3 + + . 6 3 2 (2) This weighted score leads to the highest correlation with CTR. Intuitively, the weights balance the differences in the average magnitude of each metric and thus reduce variance on short responses. Popu"
2021.acl-long.97,2020.emnlp-main.617,0,0.0454384,"Missing"
2021.acl-long.97,2020.aacl-main.56,0,0.0321193,"atural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generation model. Future work can study methods to improve cross-lingual generalization methods. Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the loop (Yuan et al., 2020). MRS 7 Conclusion We present MRS, a multilingual dataset for reply suggestion. We compare a generation and a retrieval baseline on MRS. The two models have different strengths in the English monolingual setting and require different strategies to transfer across languages. MRS provides a benchmark for future research in both reply suggestion and cross-lingual transfer learning. Ethical Considerations Data Collection. No human annotators are involved while creating MRS. The examples and response sets of MRS come from publicly available Redd"
2021.acl-long.97,P17-2079,0,0.0261147,"h other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020). Another idea is to combine the two models. Given an input message, we first use a generation model to create a set of candidate replies. We then use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generation model. Future work can study methods to improve cross-lingual generalization methods. Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the loop (Yuan et al., 2020). MRS 7 Conclusion We present MRS, a multilingual dataset for reply suggestion. We compare a g"
2021.acl-long.97,2020.emnlp-main.365,0,0.0217906,"are other models on MRS. For the English monolingual setting, we can initialize the generation model with state-of-the-art language models (Radford et al., 2019; Brown et al., 2020; Zhang et al., 2020c). For cross-lingual settings, we can initialize the generation model with several recent pre-trained multilingual SEQ 2 SEQ models (Chi et al., 2020, 2021; Liu et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020). For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020). Another idea is to combine the two models. Given an input message, we first use a generation model to create a set of candidate replies. We then use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generatio"
2021.acl-long.97,P19-1163,0,0.0161283,"nguages, we rely on our in-house model. We filter message-reply pairs if it has greater than 0.9 score according to the in-house model, or greater than 0.5 score according to Perspective API (Gehman et al., 2020). About one percent of examples are filtered. After filtering the data, we manually validate three hundred random examples and do not find any toxic examples, which confirms that our filter method have a high recall. While we hope the filtered dataset leads to better reply suggestion models, existing filtering methods are not perfect and can introduce other biases (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). Therefore, models trained on all MRS data may still have undesirable behavior. MRS is intended to be used as a benchmark for testing cross-lingual generalization of generation and retrieval models. The dataset should not be directly used in production systems. To use the dataset in practice, additional work is required to address other possible biases and toxic or inappropriate content that may exist in the data. 3 Experiment Settings After presenting the dataset, we explain how we use MRS to compare reply suggestion models. We describe the two frameworks for reply"
2021.acl-long.97,L18-1560,0,0.0255854,"nan et al., 2016). The two approaches have different advantages. Generation models are more powerful because they are not constrained by the response set. In comparison, retrieval models are easier to train and runs faster, and a curated response set guarantees the coherence and the safety of the model output. The two frameworks make reply suggestion an interesting task for studying cross-lingual generalization. Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b). In contrast, reply suggestion has two formulations that require different cross-lingual generalization strategies. While some recent work explores cross-lingual transfer 1207 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1207–1220 August 1–6, 2021. ©2021 Association for Computational Linguistics Language English Spanish German Portuguese French Japanese Swedish Italian Dutch Russian Code Family EN West Germanic"
2021.acl-long.97,2020.emnlp-main.647,0,0.0448037,"Missing"
2021.acl-long.97,N04-1023,0,0.150853,"et al., 2020; Tran et al., 2020; Lewis et al., 2020a; Xue et al., 2020). For retrieval models, we can experiment with other multilingual encoders that use different pre-training tasks (Artetxe and Schwenk, 2019; Chidambaram et al., 2019; Reimers and Gurevych, 2020; Feng et al., 2020). Another idea is to combine the two models. Given an input message, we first use a generation model to create a set of candidate replies. We then use a retrieval model to compute relevance scores and rerank these candidates. Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generation model. Future work can study methods to improve cross-lingual generalization methods. Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the lo"
2021.acl-long.97,L16-1521,0,0.0204803,"ramework and generate the reply from scratch (Kannan et al., 2016). The two approaches have different advantages. Generation models are more powerful because they are not constrained by the response set. In comparison, retrieval models are easier to train and runs faster, and a curated response set guarantees the coherence and the safety of the model output. The two frameworks make reply suggestion an interesting task for studying cross-lingual generalization. Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b). In contrast, reply suggestion has two formulations that require different cross-lingual generalization strategies. While some recent work explores cross-lingual transfer 1207 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1207–1220 August 1–6, 2021. ©2021 Association for Computational Linguistics Language English Spanish German Portuguese French Japanese Swedish Italian"
2021.acl-long.97,W02-2024,0,0.03756,". Others follow the generation framework and generate the reply from scratch (Kannan et al., 2016). The two approaches have different advantages. Generation models are more powerful because they are not constrained by the response set. In comparison, retrieval models are easier to train and runs faster, and a curated response set guarantees the coherence and the safety of the model output. The two frameworks make reply suggestion an interesting task for studying cross-lingual generalization. Most cross-lingual generalization benchmarks use classification and sequence labeling tasks (Tjong Kim Sang, 2002; Nivre et al., 2016; Strassel and Tracey, 2016; Conneau et al., 2018; Schwenk and Li, 2018; Clark et al., 2020; Hu et al., 2020; Lewis et al., 2020b). In contrast, reply suggestion has two formulations that require different cross-lingual generalization strategies. While some recent work explores cross-lingual transfer 1207 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1207–1220 August 1–6, 2021. ©2021 Association for Computational Linguistics Language English Spanish Ge"
2021.acl-long.97,2020.emnlp-main.359,0,0.013752,"zero-shot setting. This is possible because the MT system may create artifacts that do not appear in organic data (Artetxe et al., 2020). For the multilingual model, the training language ROUGE scores are lower than monolingual training (gray cells in Table 3). However, multilingual training sometimes leads to better ROUGE on unseen languages compared to transferring from only English (zero-shot). Previous work observes similar results on other tasks, where multilingual training hurts training languages but helps generalization to unseen languages (Johnson et al., 2017; Conneau et al., 2020; Wang et al., 2020). Finally, Appendix A shows similar results when initializing with XLM - R (Conneau et al., 2020). Generation Model. Table 4 shows results for the generation model. In the monolingual setting, the generation model has higher scores than the retrieval model on most languages, consistent with the English result (Figure 2). However, unlike the retrieval model, the generation model fails to generalize across languages in the zero-shot setting, despite using Unicoder-XDAE for initialization. We do not show zero-shot results in Table 4, because ROUGE are close to zero for non-English languages. Afte"
2021.acl-long.97,D19-1077,0,0.236061,"ments. The two models have different strengths. The generation model has higher word overlap scores and is favored by humans on average, but inference is slower, and the output is sometimes contradictory or repetitive (Holtzman et al., 2020). In contrast, the retrieval model is faster and always produces coherent replies, but the replies are sometimes too generic or irrelevant due to the fixed response set. Next, we test models in other languages. We compare different training settings and investigate two cross-lingual generalization methods: initializing with pre-trained multilingual models (Wu and Dredze, 2019; Conneau et al., 2020; Liang et al., 2020) and training on machine-translated data (Banea et al., 2008). Interestingly, the two models prefer different methods: multilingual pretraining works better for the retrieval model, while the generation model prefers machine translation. In summary, we present MRS, a multilingual reply suggestion dataset. We use MRS to provide the first systematic comparison between generation and retrieval models for reply suggestion in both monolingual and multilingual settings. MRS is also a useful benchmark for future research in reply suggestion and cross-lingual"
2021.acl-long.97,2021.naacl-main.42,1,0.694719,"Reranking the output of a generation model helps other natural language processing tasks (Shen et al., 2004; Collins and Koo, 2005; Ge and Mooney, 2006), and previous work uses a similar idea for chatbots (Qiu et al., 2017). Our experiment shows that reply suggestion poses unique challenges for cross-lingual generalization, especially for the generation model. Future work can study methods to improve cross-lingual generalization methods. Some examples include applying adversarial learning (Chen et al., 2018, 2019; Huang et al., 2019), using adapters (Pfeiffer et al., 2020), adaptive transfer (Xia et al., 2021), mixing pre-training and fine-tuning (Phang et al., 2020), and bringing a human in the loop (Yuan et al., 2020). MRS 7 Conclusion We present MRS, a multilingual dataset for reply suggestion. We compare a generation and a retrieval baseline on MRS. The two models have different strengths in the English monolingual setting and require different strategies to transfer across languages. MRS provides a benchmark for future research in both reply suggestion and cross-lingual transfer learning. Ethical Considerations Data Collection. No human annotators are involved while creating MRS. The examples"
2021.acl-long.97,2020.emnlp-main.482,1,0.882217,"Missing"
2021.acl-long.97,2020.acl-main.201,1,0.884627,"we train models in a zero-shot cross-lingual setting. We train the model on the English training set and use the model on the test set for another language. This setting simulates the scenario where we want to build models for a low-resource language using our large English set. To generalize across languages, we initialize the models with pre-trained multilingual models (details in Section 4). These models work well in other tasks (Wu and Dredze, 2019; Liang et al., 2020). We test if they also work for reply suggestion, as different tasks often prefer different multilingual representations (Zhang et al., 2020b). Machine Translation (MT). Another strategy for cross-lingual generalization is to train on machine-translated data (Banea et al., 2008). We train models on nineteen million English training examples machine-translated to the target language with Microsoft Translator. We compare against the zero-shot setting to compare the two cross-lingual generalization strategies. Multilingual. Finally, we build a multilingual model by jointly training on the five languages with the most training data: English, Spanish, German, Portuguese, and French. We oversample nonEnglish training data to have the sa"
2021.acl-long.97,2020.acl-demos.30,0,0.216003,"we train models in a zero-shot cross-lingual setting. We train the model on the English training set and use the model on the test set for another language. This setting simulates the scenario where we want to build models for a low-resource language using our large English set. To generalize across languages, we initialize the models with pre-trained multilingual models (details in Section 4). These models work well in other tasks (Wu and Dredze, 2019; Liang et al., 2020). We test if they also work for reply suggestion, as different tasks often prefer different multilingual representations (Zhang et al., 2020b). Machine Translation (MT). Another strategy for cross-lingual generalization is to train on machine-translated data (Banea et al., 2008). We train models on nineteen million English training examples machine-translated to the target language with Microsoft Translator. We compare against the zero-shot setting to compare the two cross-lingual generalization strategies. Multilingual. Finally, we build a multilingual model by jointly training on the five languages with the most training data: English, Spanish, German, Portuguese, and French. We oversample nonEnglish training data to have the sa"
2021.acl-short.118,2020.emnlp-main.700,1,0.881008,"eneration remains an open question for generative QA which aims to produce correct and coherent answers. Specifically, we observed that generative models often generate answers semantically drifting away from the given 942 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 942–947 August 1–6, 2021. ©2021 Association for Computational Linguistics passage and question, known as the “semantic drift” problem. As shown in Table 1, the baseline generative model PALM (Bi et al., 2020) generates an answer that has almost contrary semantics with the gold answer. In general, a generative model often suffers from two critical problems: (1) summarizing content irrelevant to a given question, and (2) drifting away from a correct answer during generation. In this paper, we address these problems by a novel Rationale-Enriched Answer Generator (REAG), which incorporates an extractive mechanism into a generative model in order to leverage relevant information to a given question in the contextual passage. Specifically, we add an extraction task on the encoder to obtain the rationale"
2021.acl-short.118,D19-1255,1,0.813768,"les the encoder to learn the relevance between a question and a passage; On the other hand, the extracted rationale can be further used to guide the answer generation. Based on the extracted rationale and original input, the decoder is expected to summarize content relevant to a given question and generates an answer with high confidence. Finally, we jointly train REAG on the MS MARCO QA+NLG task based on the common bottom layers. The experimental results show that REAG improves the semantic accuracy of answers over the other state-of-the-art models. 2 2.1 2.2 Generative Reading Comprehension Bi et al. (2019) proposed a Knowledge-Enriched Answer Generator (KEAG) to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. Nishida et al. (2019a) proposed a multi-style generative model to generate an abstractive summary from the given question, passages and multi-style. Related Work Machine Reading Comprehension In recent years, machine reading comprehension has made great progress with the development of SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016). The current mainstream studies tre"
2021.acl-short.118,P17-1171,0,0.0230347,"literally present in the passages. Directly extracting a consecutive answer span is often inadequate. Therefore, the ability of generating an abstractive answer is needed, which requires a QA model to summarize the main content in a paragraph that is relevant to a given question. Introduction Question Answering (QA) has come a long way from answer sentence selection, relationship QA to machine reading comprehension (MRC). Recently, QA has become an essential problem in natural language understanding and a major milestone towards human-level machine intelligence. Current mainstream approaches (Chen et al., 2017; Wang et al., 2018; Yan et al., 2018) treat MRC as a process of extracting a consecutive piece of text from a document to a given question. Despite the great success in extractive MRC (Wang et al., 2018; Chen et al., 2020), in real-world applications, correct answers may span different Answering questions in natural language can be beneficial to a variety of QA applications, and has led to the development of smart devices such as Siri, Cortana and Alexa. However, compared with answer extraction, answer generation for reading comprehension is more challenging, and has been less explored. A maj"
2021.acl-short.118,2020.emnlp-main.549,0,0.0269967,"Missing"
2021.acl-short.118,N19-1423,0,0.0294798,"e great progress with the development of SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016). The current mainstream studies treat machine reading comprehension as answer span extraction from one passage (Rajpurkar et al., 2016, 2018) or multi-passages (Nguyen et al., 2016), which is usually done by predicting the start and end position of an answer. SLQA (Wang et al., 2018) improved answer quality with a hierarchical attention fusion network, which conducted attention and fusion horizontally and vertically across layers between a passage and a question. Recently, the BERT model Devlin et al. (2019) has proved effective for reading comprehension via unsupervised pre-training. 943 2.3 Reliable Text Generation Compared with answer extraction, answer generation for reading comprehension is more challenging, and the major challenge in generative reading comprehension lies in out-of-control generation. Recently, some studies have been carried out on increasing the reliability of generation in the encoder-decoder framework(Liu et al., 2018; Li et al., 2018). 3 3.1 Rationale-Enriched Answer Generation Rationale Span Extraction In a generative reading comprehension task, every answer has its cor"
2021.acl-short.118,N18-2009,1,0.852747,"Missing"
2021.acl-short.118,D18-1444,0,0.0249633,"usion network, which conducted attention and fusion horizontally and vertically across layers between a passage and a question. Recently, the BERT model Devlin et al. (2019) has proved effective for reading comprehension via unsupervised pre-training. 943 2.3 Reliable Text Generation Compared with answer extraction, answer generation for reading comprehension is more challenging, and the major challenge in generative reading comprehension lies in out-of-control generation. Recently, some studies have been carried out on increasing the reliability of generation in the encoder-decoder framework(Liu et al., 2018; Li et al., 2018). 3 3.1 Rationale-Enriched Answer Generation Rationale Span Extraction In a generative reading comprehension task, every answer has its corresponding rationale, an extractive span in the passage, which can be derived by matching the passage text with the answer. The rationale can usually be located in a certain continuous area of the passage. We use continuous text span as the rationale to minimize the difficulty of the extraction task. Compared with the gold answer, the text span with the highest F1-score in passage is identified as the rationale for training supervision. Ba"
2021.acl-short.118,P19-1220,0,0.0754823,"oder is expected to summarize content relevant to a given question and generates an answer with high confidence. Finally, we jointly train REAG on the MS MARCO QA+NLG task based on the common bottom layers. The experimental results show that REAG improves the semantic accuracy of answers over the other state-of-the-art models. 2 2.1 2.2 Generative Reading Comprehension Bi et al. (2019) proposed a Knowledge-Enriched Answer Generator (KEAG) to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. Nishida et al. (2019a) proposed a multi-style generative model to generate an abstractive summary from the given question, passages and multi-style. Related Work Machine Reading Comprehension In recent years, machine reading comprehension has made great progress with the development of SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016). The current mainstream studies treat machine reading comprehension as answer span extraction from one passage (Rajpurkar et al., 2016, 2018) or multi-passages (Nguyen et al., 2016), which is usually done by predicting the start and end position of an answer. SLQA (W"
2021.acl-short.118,P18-2124,0,0.0692017,"Missing"
2021.acl-short.118,D16-1264,0,0.0508049,"the other state-of-the-art models. 2 2.1 2.2 Generative Reading Comprehension Bi et al. (2019) proposed a Knowledge-Enriched Answer Generator (KEAG) to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. Nishida et al. (2019a) proposed a multi-style generative model to generate an abstractive summary from the given question, passages and multi-style. Related Work Machine Reading Comprehension In recent years, machine reading comprehension has made great progress with the development of SQuAD (Rajpurkar et al., 2016) and MS MARCO (Nguyen et al., 2016). The current mainstream studies treat machine reading comprehension as answer span extraction from one passage (Rajpurkar et al., 2016, 2018) or multi-passages (Nguyen et al., 2016), which is usually done by predicting the start and end position of an answer. SLQA (Wang et al., 2018) improved answer quality with a hierarchical attention fusion network, which conducted attention and fusion horizontally and vertically across layers between a passage and a question. Recently, the BERT model Devlin et al. (2019) has proved effective for reading comprehension via"
2021.acl-short.118,P18-1158,1,0.933481,"n the passages. Directly extracting a consecutive answer span is often inadequate. Therefore, the ability of generating an abstractive answer is needed, which requires a QA model to summarize the main content in a paragraph that is relevant to a given question. Introduction Question Answering (QA) has come a long way from answer sentence selection, relationship QA to machine reading comprehension (MRC). Recently, QA has become an essential problem in natural language understanding and a major milestone towards human-level machine intelligence. Current mainstream approaches (Chen et al., 2017; Wang et al., 2018; Yan et al., 2018) treat MRC as a process of extracting a consecutive piece of text from a document to a given question. Despite the great success in extractive MRC (Wang et al., 2018; Chen et al., 2020), in real-world applications, correct answers may span different Answering questions in natural language can be beneficial to a variety of QA applications, and has led to the development of smart devices such as Siri, Cortana and Alexa. However, compared with answer extraction, answer generation for reading comprehension is more challenging, and has been less explored. A major challenge in gen"
2021.emnlp-main.546,D15-1075,0,0.0292952,"Missing"
2021.emnlp-main.546,D14-1082,0,0.043702,"s to denote sentences in Dc ∪ Ds and E to denote the entity sets for simplicity in later discussion. |E |= 2 if s ∈ Dc and |E |= 1 otherwise. 3.2 Text Feature Representations w ∈ Rd0 . GloVe assigns a fixed vector while BERT computes a token2 representation by its textual context. The encoding output of s is denoted by S0 = {w1 , . . . , e1 , . . . , e2 , . . . , wn } where ei denotes the embedding of entity i, wi denotes the embedding of a non-entity word, and wi , ej ∈ Rd0 . The dependency graph of s, denoted by Gs , is obtained by applying a dependency parser to s such as Stanford Parser (Chen and Manning, 2014) or spaCy3 . Gs is a syntactic view of s (Marcheggiani and Titov, 2017; Li et al., 2016) that is composed of vertices of words and directed edges of dependency relations. Advantageously, complex syntactic relations between distant words in the sentence can be easily detected with a small number of hops over dependency edges (Ma et al., 2020). 3.3 Contextual Features for CPC Global Semantic Context To model more extended context of the entities, we use a bidirectional LSTM (BiLSTM) to encode the entire sentence in both directions. Bi-directional recurrent neural network is widely used in extrac"
2021.emnlp-main.546,2020.acl-main.340,0,0.0286296,"nnel context fea6818 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6818–6830 c November 7–11, 2021. 2021 Association for Computational Linguistics ture extractor to learn the global and local context. In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneficial to the comparison classification. ABSA aims to detect the specific emotional inclination toward an aspect within a sentence (Ma et al., 2018; Hu et al., 2019; Phan and Ogunbona, 2020; Chen and Qian, 2020; Wang et al., 2020). For example, the sentence I liked the service and the staff but not the food suggests positive sentiments toward service and staff but a negative one toward food. These aspect entities, such as service, staff, and food, are studied individually. The well-studied ABSA approaches can be beneficial to CPC when the compared entities in a CPC sentence are considered as the aspects in ABSA. Incorporating the individual sentiments learned by ABSA methods into CPC has several advantages. Firstly, for a comparison to hold, the preferred entity usually receives a positive sentiment"
2021.emnlp-main.546,D17-1070,0,0.0326711,"Missing"
2021.emnlp-main.546,N19-1423,0,0.0688275,"iment Analysis ABSA derives from sentiment analysis (SA) which infers the sentiment associated with a specific entity in a sentence. Traditional approaches of ABSA utilize SVM for classification (Kiritchenko et al., 2014; Wagner et al., 2014; Zhang et al., 2014) while neural network-based approaches employ variants of RNN (Nguyen and Shirai, 2015; Aydin and Güngör, 2020), LSTM (Tang et al., 2016; Wang et al., 2016; Bao et al., 2019), GAT (Wang et al., 2020), and GCN (Pouran Ben Veyseh et al., 2020; Xu et al., 2020). More recent works1 widely use complex contextualized NLP models such as BERT (Devlin et al., 2019). Sun et al. (2019) transform ABSA into a Question Answering task by constructing auxiliary sentences. Phan and Ogunbona (2020) build a pipeline of Aspect Extraction and ABSA and used wide and concentrated features for sentiment classification. ABSA is related to CPC by nature. In general, entities with positive sentiments are preferred over the ones with neutral or negative sentiments. Therefore, the performance of a CPC model can be enhanced by the ABSA techniques. There is one challenge that blocks the knowledge transfer of sentiment analysis from the ABSA data to the CPC task: domain shift"
2021.emnlp-main.546,N19-1312,0,0.0283972,"te on the jth layer of an (GRL) and a domain classifier (DC) (Ganin and edge between vertices u and v is defined as Lempitsky, 2015) for the domain adaptive senti  ment feature learning that maintains the discrimina(j) (j) (j) (j) guv ∈ R, guv = σ h(j) u · β duv + γluv , tiveness and the domain-invariance. GRL+DC is a straightforward, generic, and effective modification 4 Labels are defined as the combinations of directions and to neural networks for domain adaptation (Kamath dependency types. For example, edge ((u, v), nsubj) and edge ((v, u), nsubj−1 ) have different labels. et al., 2019; Gu et al., 2019; Belinkov et al., 2019; 6821 Li et al., 2018). It can effectively close the shift between complex distributions (Ganin and Lempitsky, 2015) such as Dc and Ds . Let A denote the sentiment analyzer which alternatively learns sentiment information from Ds and provides sentimental clues to the compared entities in Dc . Specifically, each CPC instance is split into two ABSA samples with the same text before being fed into A (see the “Split to 2” in Figure 1). One takes e1 as the queried aspect the other takes e2 . A(S0 , Gs , E) = ( hs,1 , hs,2 hs if s ∈ Dc , if s ∈ Ds . Rds . hs,1 , hs,2 , and hs"
2021.emnlp-main.546,P19-1556,0,0.0598542,"Missing"
2021.emnlp-main.546,S14-2076,0,0.0314992,"2019). ED-GAT (Ma et al., 2020), a more recent work, uses the dependency graph to better recognize longdistance comparisons and avoid falsely identifying unrelated comparison predicates. However, it fails to capture semantic information of the entities as they are replaced by “entityA” and “entityB”. Furthermore, having multiple GAT layers severely increases training difficulty. 2.2 Aspect-Based Sentiment Analysis ABSA derives from sentiment analysis (SA) which infers the sentiment associated with a specific entity in a sentence. Traditional approaches of ABSA utilize SVM for classification (Kiritchenko et al., 2014; Wagner et al., 2014; Zhang et al., 2014) while neural network-based approaches employ variants of RNN (Nguyen and Shirai, 2015; Aydin and Güngör, 2020), LSTM (Tang et al., 2016; Wang et al., 2016; Bao et al., 2019), GAT (Wang et al., 2020), and GCN (Pouran Ben Veyseh et al., 2020; Xu et al., 2020). More recent works1 widely use complex contextualized NLP models such as BERT (Devlin et al., 2019). Sun et al. (2019) transform ABSA into a Question Answering task by constructing auxiliary sentences. Phan and Ogunbona (2020) build a pipeline of Aspect Extraction and ABSA and used wide and concent"
2021.emnlp-main.546,W17-2326,0,0.0275765,"opose a Sentiment AnalyPython and MATLAB and comparatively Python is sis Enhanced COmparative classification Network preferred over MATLAB in the context. (SAECON), a CPC approach that considers not The CPC task can profoundly impact various only syntactic but also semantic features of the real-world application scenarios. Search engine entities. The semantic features here refer to the users may query not only factual questions but also context of the entities from which a sentiment analcomparative ones to meet their specific informa- ysis model can infer the sentiments toward the tion needs (Gupta et al., 2017). Recommendation entities. Specifically, the encoded sentence and * These authors contributed equally. entities are fed into a dual-channel context fea6818 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6818–6830 c November 7–11, 2021. 2021 Association for Computational Linguistics ture extractor to learn the global and local context. In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneficial to the comparison classification. ABSA aims to de"
2021.emnlp-main.546,2020.acl-main.512,0,0.526511,"t al. (2019) first formalize the quality CPC models can significantly benCPC problem, build and publish the CompSent-19 efit applications such as comparative quesdataset, and experiment with numerous general mation answering and review-based recommenchine learning models such as Support Vector Madation. Among the existing approaches, nonchine (SVM), representation-based classification, deep learning methods suffer from inferior perand XGBoost. However, these attempts consider formances. The state-of-the-art graph neural CPC as a sentence classification while ignoring the network-based ED-GAT (Ma et al., 2020) only considers syntactic information while ignoring semantics and the contexts of the entities (Ma et al., the critical semantic relations and the senti2020). ments to the compared entities. We propose ED-GAT (Ma et al., 2020) marks the first entitySentiment Analysis Enhanced COmparative aware CPC approach that captures long-distance Network (SAECON) which improves CPC acsyntactic relations between the entities of interest curacy with a sentiment analyzer that learns by applying graph attention networks (GAT) to desentiments to individual entities via domain pendency parsing graphs. However,"
2021.emnlp-main.546,D18-1383,0,0.141758,"rformance of a CPC model can be enhanced by the ABSA techniques. There is one challenge that blocks the knowledge transfer of sentiment analysis from the ABSA data to the CPC task: domain shift. Existing ABSA datasets are centered around specific topics such as restaurants and laptops, while the CPC data has mixed topics (Panchenko et al., 2019) that are all distant from restaurants. In other words, sentences of ABSA and CPC datasets are drawn from different distributions, also known as domains. The difference in the distributions is referred to as a “domain shift” (Ganin and Lempitsky, 2015; He et al., 2018) and it is harmful to an accurate knowledge transfer. To mitigate the domain shift, we design a domain adaptive layer to remove the domainspecific feature such as topics and preserve the domain-invariant feature such as sentiments of the text so that the sentiment analyzer can smoothly 1 transfer knowledge from sentiment analysis to comDue to the limited space, we are unable to exhaustively parative classification. cover all references. Works discussed are classic examples. 6819 hg,1 hl,1 hg,2 <latexit sha1_base64=""dM3+Oes9MLnRkxI4Tq6iTjoIBH8="">AAACg3icbVFNS8NAEN1EqzV+RT16CZaKoNQkFPUiVL14rGA/o"
2021.emnlp-main.546,P19-1051,0,0.0181165,"equally. entities are fed into a dual-channel context fea6818 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6818–6830 c November 7–11, 2021. 2021 Association for Computational Linguistics ture extractor to learn the global and local context. In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneficial to the comparison classification. ABSA aims to detect the specific emotional inclination toward an aspect within a sentence (Ma et al., 2018; Hu et al., 2019; Phan and Ogunbona, 2020; Chen and Qian, 2020; Wang et al., 2020). For example, the sentence I liked the service and the staff but not the food suggests positive sentiments toward service and staff but a negative one toward food. These aspect entities, such as service, staff, and food, are studied individually. The well-studied ABSA approaches can be beneficial to CPC when the compared entities in a CPC sentence are considered as the aspects in ABSA. Incorporating the individual sentiments learned by ABSA methods into CPC has several advantages. Firstly, for a comparison to hold, the preferre"
2021.emnlp-main.546,D17-1159,0,0.125171,"for simplicity in later discussion. |E |= 2 if s ∈ Dc and |E |= 1 otherwise. 3.2 Text Feature Representations w ∈ Rd0 . GloVe assigns a fixed vector while BERT computes a token2 representation by its textual context. The encoding output of s is denoted by S0 = {w1 , . . . , e1 , . . . , e2 , . . . , wn } where ei denotes the embedding of entity i, wi denotes the embedding of a non-entity word, and wi , ej ∈ Rd0 . The dependency graph of s, denoted by Gs , is obtained by applying a dependency parser to s such as Stanford Parser (Chen and Manning, 2014) or spaCy3 . Gs is a syntactic view of s (Marcheggiani and Titov, 2017; Li et al., 2016) that is composed of vertices of words and directed edges of dependency relations. Advantageously, complex syntactic relations between distant words in the sentence can be easily detected with a small number of hops over dependency edges (Ma et al., 2020). 3.3 Contextual Features for CPC Global Semantic Context To model more extended context of the entities, we use a bidirectional LSTM (BiLSTM) to encode the entire sentence in both directions. Bi-directional recurrent neural network is widely used in extracting semantics (Li et al., 2019). Given the indices of e1 and e2 in s,"
2021.emnlp-main.546,W19-4516,0,0.211872,"long Qin*, Zihan Liu*, and Wei Wang Department of Compute Science, University of California, Los Angeles {zyli,weiwang}@cs.ucla.edu {louisqin,zihanliu}@ucla.edu Abstract providers can analyze product reviews with comparative statements to understand the advantages We study Comparative Preference Classificaand disadvantages of the product comparing with tion (CPC) which aims at predicting whether similar ones. a preference comparison exists between two Several models have been proposed to solve this entities in a given sentence and, if so, which entity is preferred over the other. Highproblem. Panchenko et al. (2019) first formalize the quality CPC models can significantly benCPC problem, build and publish the CompSent-19 efit applications such as comparative quesdataset, and experiment with numerous general mation answering and review-based recommenchine learning models such as Support Vector Madation. Among the existing approaches, nonchine (SVM), representation-based classification, deep learning methods suffer from inferior perand XGBoost. However, these attempts consider formances. The state-of-the-art graph neural CPC as a sentence classification while ignoring the network-based ED-GAT (Ma et al., 2"
2021.emnlp-main.546,D14-1162,0,0.0842736,"Missing"
2021.emnlp-main.546,2020.acl-main.293,0,0.338201,"s are fed into a dual-channel context fea6818 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6818–6830 c November 7–11, 2021. 2021 Association for Computational Linguistics ture extractor to learn the global and local context. In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneficial to the comparison classification. ABSA aims to detect the specific emotional inclination toward an aspect within a sentence (Ma et al., 2018; Hu et al., 2019; Phan and Ogunbona, 2020; Chen and Qian, 2020; Wang et al., 2020). For example, the sentence I liked the service and the staff but not the food suggests positive sentiments toward service and staff but a negative one toward food. These aspect entities, such as service, staff, and food, are studied individually. The well-studied ABSA approaches can be beneficial to CPC when the compared entities in a CPC sentence are considered as the aspects in ABSA. Incorporating the individual sentiments learned by ABSA methods into CPC has several advantages. Firstly, for a comparison to hold, the preferred entity usually receives"
2021.emnlp-main.546,S15-2082,0,0.0803614,"Missing"
2021.emnlp-main.546,S14-2004,0,0.12414,"Missing"
2021.emnlp-main.546,S14-2036,0,0.026579,"2020), a more recent work, uses the dependency graph to better recognize longdistance comparisons and avoid falsely identifying unrelated comparison predicates. However, it fails to capture semantic information of the entities as they are replaced by “entityA” and “entityB”. Furthermore, having multiple GAT layers severely increases training difficulty. 2.2 Aspect-Based Sentiment Analysis ABSA derives from sentiment analysis (SA) which infers the sentiment associated with a specific entity in a sentence. Traditional approaches of ABSA utilize SVM for classification (Kiritchenko et al., 2014; Wagner et al., 2014; Zhang et al., 2014) while neural network-based approaches employ variants of RNN (Nguyen and Shirai, 2015; Aydin and Güngör, 2020), LSTM (Tang et al., 2016; Wang et al., 2016; Bao et al., 2019), GAT (Wang et al., 2020), and GCN (Pouran Ben Veyseh et al., 2020; Xu et al., 2020). More recent works1 widely use complex contextualized NLP models such as BERT (Devlin et al., 2019). Sun et al. (2019) transform ABSA into a Question Answering task by constructing auxiliary sentences. Phan and Ogunbona (2020) build a pipeline of Aspect Extraction and ABSA and used wide and concentrated features for se"
2021.emnlp-main.546,2020.acl-main.295,0,0.0772943,"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6818–6830 c November 7–11, 2021. 2021 Association for Computational Linguistics ture extractor to learn the global and local context. In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneficial to the comparison classification. ABSA aims to detect the specific emotional inclination toward an aspect within a sentence (Ma et al., 2018; Hu et al., 2019; Phan and Ogunbona, 2020; Chen and Qian, 2020; Wang et al., 2020). For example, the sentence I liked the service and the staff but not the food suggests positive sentiments toward service and staff but a negative one toward food. These aspect entities, such as service, staff, and food, are studied individually. The well-studied ABSA approaches can be beneficial to CPC when the compared entities in a CPC sentence are considered as the aspects in ABSA. Incorporating the individual sentiments learned by ABSA methods into CPC has several advantages. Firstly, for a comparison to hold, the preferred entity usually receives a positive sentiment while its rival get"
2021.emnlp-main.546,P18-1202,0,0.0189123,"BSA inherently correlates with the CPC task. Therefore, it is natural to incorporate a sentiment analyzer into SAECON as an auxiliary task to take advantage of the abundant training resources of ABSA to boost the performance on CPC. There are two paradigms for auxiliary tasks: (1) incorporating fixed parameters that are pretrained solely with the auxiliary dataset; (2) incorporating the architecture only with untrained parameters and jointly optimizing them u∈N (v) from scratch with the main task simultaneously (Li where ρ(·) denotes an aggregation function such et al., 2018; He et al., 2018; Wang and Pan, 2018). (j+1) ×d(j) as mean and sum, W(j) ∈ Rd and b(j) ∈ Option (1) ignores the domain shift between Dc (j+1) Rd are trainable parameters, and d(j+1) and and Ds , which degrades the quality of the learned d(j) denote latent feature dimensions of the (j + sentiment features since the domain identity in1)th and the jth layers, respectively. formation is noisy and unrelated to the CPC task. SGCN improves GCN by considering different SAECON uses option (2). For a smooth and efedge directions and diverse edge types, and assigns ficient knowledge transfer from Ds to Dc under different parameters to diffe"
2021.emnlp-main.546,D16-1058,0,0.0324148,"s to capture semantic information of the entities as they are replaced by “entityA” and “entityB”. Furthermore, having multiple GAT layers severely increases training difficulty. 2.2 Aspect-Based Sentiment Analysis ABSA derives from sentiment analysis (SA) which infers the sentiment associated with a specific entity in a sentence. Traditional approaches of ABSA utilize SVM for classification (Kiritchenko et al., 2014; Wagner et al., 2014; Zhang et al., 2014) while neural network-based approaches employ variants of RNN (Nguyen and Shirai, 2015; Aydin and Güngör, 2020), LSTM (Tang et al., 2016; Wang et al., 2016; Bao et al., 2019), GAT (Wang et al., 2020), and GCN (Pouran Ben Veyseh et al., 2020; Xu et al., 2020). More recent works1 widely use complex contextualized NLP models such as BERT (Devlin et al., 2019). Sun et al. (2019) transform ABSA into a Question Answering task by constructing auxiliary sentences. Phan and Ogunbona (2020) build a pipeline of Aspect Extraction and ABSA and used wide and concentrated features for sentiment classification. ABSA is related to CPC by nature. In general, entities with positive sentiments are preferred over the ones with neutral or negative sentiments. Therefo"
2021.emnlp-main.546,S14-2041,0,0.0553564,"Missing"
2021.emnlp-main.546,N19-1035,0,0.0147261,"rives from sentiment analysis (SA) which infers the sentiment associated with a specific entity in a sentence. Traditional approaches of ABSA utilize SVM for classification (Kiritchenko et al., 2014; Wagner et al., 2014; Zhang et al., 2014) while neural network-based approaches employ variants of RNN (Nguyen and Shirai, 2015; Aydin and Güngör, 2020), LSTM (Tang et al., 2016; Wang et al., 2016; Bao et al., 2019), GAT (Wang et al., 2020), and GCN (Pouran Ben Veyseh et al., 2020; Xu et al., 2020). More recent works1 widely use complex contextualized NLP models such as BERT (Devlin et al., 2019). Sun et al. (2019) transform ABSA into a Question Answering task by constructing auxiliary sentences. Phan and Ogunbona (2020) build a pipeline of Aspect Extraction and ABSA and used wide and concentrated features for sentiment classification. ABSA is related to CPC by nature. In general, entities with positive sentiments are preferred over the ones with neutral or negative sentiments. Therefore, the performance of a CPC model can be enhanced by the ABSA techniques. There is one challenge that blocks the knowledge transfer of sentiment analysis from the ABSA data to the CPC task: domain shift. Existing ABSA dat"
2021.emnlp-main.546,C16-1311,0,0.0227122,"s. However, it fails to capture semantic information of the entities as they are replaced by “entityA” and “entityB”. Furthermore, having multiple GAT layers severely increases training difficulty. 2.2 Aspect-Based Sentiment Analysis ABSA derives from sentiment analysis (SA) which infers the sentiment associated with a specific entity in a sentence. Traditional approaches of ABSA utilize SVM for classification (Kiritchenko et al., 2014; Wagner et al., 2014; Zhang et al., 2014) while neural network-based approaches employ variants of RNN (Nguyen and Shirai, 2015; Aydin and Güngör, 2020), LSTM (Tang et al., 2016; Wang et al., 2016; Bao et al., 2019), GAT (Wang et al., 2020), and GCN (Pouran Ben Veyseh et al., 2020; Xu et al., 2020). More recent works1 widely use complex contextualized NLP models such as BERT (Devlin et al., 2019). Sun et al. (2019) transform ABSA into a Question Answering task by constructing auxiliary sentences. Phan and Ogunbona (2020) build a pipeline of Aspect Extraction and ABSA and used wide and concentrated features for sentiment classification. ABSA is related to CPC by nature. In general, entities with positive sentiments are preferred over the ones with neutral or negative"
2021.emnlp-main.546,P15-1037,0,0.0588382,"Missing"
2021.findings-emnlp.147,P19-1134,0,0.0137329,"o produce the final detection results. The contributions of this paper are as follows: • We demonstrate results that substantially improve on recall in span detection to align with the use case in the systematic review application domain on two benchmark datasets. 2 2.1 Related Work Neural Language Models Pre-trained deep neural language models, such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Lee et al., 2020) and its variants (Liu et al., 2019; Lan et al., 2020), have brought significant performance improvements in a wide range of NLP tasks, such as relation extraction (Alt et al., 2019), entity resolution (Li et al., 2021), and question answering (Lee et al., 2020). These language models generally benefit from large scale text corpora. A major advantage of these methods come from the way long dependency token relations are captured to produce contextualised representations. To adapt language models for use in the biomedical domain, researchers took pre-trained language models and re-trained them with domain-specific corpora, including PubMed abstracts and full text articles. These were then applied to a diverse range of NLP tasks, such as named entity recognition (Lee et al."
2021.findings-emnlp.147,D19-1371,0,0.0138661,"o standardised representations of the and avoid searching and screening—the two populations, interventions, comparators, and outmost time-consuming systematic review processes. We propose and test a novel approach comes (PICO) and aggregate information across to PICO span detection. The major differstudies that answer equivalent clinical questions. ence between our proposed method and prePICO extraction is a well-studied problem strucvious approaches comes from detecting spans ture and was used as one of the examples in the without needing annotated span data and using development of SciBERT (Beltagy et al., 2019). only crowdsourced sentence-level annotations. Many PICO extraction methods focus on annoExperiments on two datasets show that PICO span detection results achieve much higher retating sentences that include PICO information. sults for recall when compared to fully suHowever, if the goal is to fully automate a process pervised methods with PICO sentence detecfor augmenting a systematic review with new studtion at least as good as human annotations. ies as their results become available, even a perfect By removing the reliance on expert annotaannotation of just the sentence is not enough. An ti"
2021.findings-emnlp.147,N19-1423,0,0.368347,"reviews. Full automation of the task requires the ability to identify the text spans that represent the PICO in1 Introduction formation. Systematic reviews are a critical part of regulaNamed entity recognition (NER) seeks to identory and clinical decision-making because they are tify the types and boundaries of targeted spans designed to robustly make sense of all available in unstructured text data. Machine learning NER evidence from primary research, especially clinical methods have been based on BiLSTM-CRF (Lamtrials, accounting for study design quality and het- ple et al., 2016) and BERT (Devlin et al., 2019), erogeneity. Searching and screening for reports of and these require human annotated data for trainclinical trials are time-consuming tasks that require ing. For other application domains where span specialised expertise but are a necessary compo- annotation is designed for entities such as people, nent of systematic reviews. The rapid rate at which locations, and organisations, crowdsourcing of lapapers were published about COVID-19 (Wang and bels is somewhat easier because a broader range 1705 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1705–1715 November 7"
2021.findings-emnlp.147,2020.acl-main.718,0,0.0316323,"Missing"
2021.findings-emnlp.147,D19-1345,0,0.0177632,"th the sentence classification (the model on the left) and masked span prediction (the model on the right). The BLUE encoder is firstly trained on the sentence prediction using the contextualised embedding of [CLS] token (the left part of the figure). Then the fine-tuned BLUE encoder predicts the score with span masked (the right part of the figure). The predicted scores of candidate spans along with the raw token sequence is collected for inference. f (C1 , . . . , C|C |) (details of the functions are presented in Section 4.1). Compared with other stateof-the-art text classification methods (Huang et al., 2019; Zhang et al., 2020), our sentence classification method is relatively simple with just one extended module. The corresponding loss function we apply here is the cross entropy function: L=− X ylog(p(y |h). s∈D 3.3 Masked Span Prediction (3) original token sequence with [MASK] token. This masked token sequence is put into the fine-tuned neural language model to get the prediction score scorehi,ji . The score scorehi,ji is then compared with the original score score to infer the impact by the span hi, ji, i.e., the contribution of the span in classifying the sentence as a PICO sentence. We defi"
2021.findings-emnlp.147,2021.ccl-1.108,0,0.0455441,"Missing"
2021.findings-emnlp.147,P16-1101,0,0.0250217,"ns, especially for the boundaries of the spans. To improve the annotation quality, Zlabinger et al. (2020) simplified the annotation task from document level to sentence level and additionally guided workers with similar sentences that had already been annotated using an unsupervised semantic shorttext similarity method. In this paper, we instead only require sentence-level crowdsourced annotations without a boundary information developing novel method to predict PICO spans without training data. 2.3 Span based Methods Compared with token based methods such as BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016), span based methods treat the spans (i.e. consecutive tokens), as the targets. In one stream of research advances in span based methods, the aim is to extract the hidden representations of each token with the raw token sequence as input, then either use boundary token representations (Ouchi et al., 2018; Ebner et al., 2020) or aggregate all the token representations (Liu et al., 2020) as the span representation. All possible spans are enumerated, classified, and decoded. An alternative stream of span-based research aims to mask a span in the token sequence and recover the masked tokens with h"
2021.findings-emnlp.147,2020.tacl-1.5,0,0.0152965,"eat the spans (i.e. consecutive tokens), as the targets. In one stream of research advances in span based methods, the aim is to extract the hidden representations of each token with the raw token sequence as input, then either use boundary token representations (Ouchi et al., 2018; Ebner et al., 2020) or aggregate all the token representations (Liu et al., 2020) as the span representation. All possible spans are enumerated, classified, and decoded. An alternative stream of span-based research aims to mask a span in the token sequence and recover the masked tokens with hidden representations (Joshi et al., 2020). Both research streams use supervised or self-supervised methods and high-quality annotations as training data. In this paper, we instead extract the information stored in the span using only sentence-level annotations derived from crowdsourcing. 3 Method 3.1 Task Definition Generally, to construct a dataset for PICO span detection, the annotators are presented with the full text, i.e., the entire abstract of a clinical trial report (Nguyen et al., 2017; Nye et al., 2018). To improve the quality of the annotation, Zlabinger et al. (2020) proposed a novel annotation task, asking annotators to"
2021.findings-emnlp.147,P17-1028,0,0.290528,"in each of the study arms, and the set of primary outcomes measured during the • We propose a span detection approach for trial. Like named entity recognition (NER), PICO PICO extraction that uses only low-quality, detection aims to identify certain spans in the text crowd-sourced, sentence-level annotations as corresponding to the each of the categories: popinputs, which reduces the need for timeulation, interventions, and outcomes. While is it consuming annotations from experts. possible to apply general NER methods for the • We evaluate a novel structure for identify- PICO extraction task (Nguyen et al., 2017; Nye ing candidate PICO sentences and masked et al., 2018; Kang et al., 2021), there are several span inference together. The masked span key differences that make general NER methods inference task replaces input spans with pre- less effective. These differences include spans that defined mask tokens and the language model often do not have distinguishing features such as is used to infer which spans contribute most capitalised tokens and PICO elements that are not to the PICO sentence classification results. limited to noun phrases. 1706 Most PICO extraction methods are fully supervised and"
2021.findings-emnlp.147,P18-1019,0,0.298335,"with pre- less effective. These differences include spans that defined mask tokens and the language model often do not have distinguishing features such as is used to infer which spans contribute most capitalised tokens and PICO elements that are not to the PICO sentence classification results. limited to noun phrases. 1706 Most PICO extraction methods are fully supervised and need annotated data, which requires expertise and can be time consuming. To acquire enough PICO annotations for training, researchers have developed methods that use crowdsourcing as an alternative (Nguyen et al., 2017; Nye et al., 2018). This results in low-quality annotations, especially for the boundaries of the spans. To improve the annotation quality, Zlabinger et al. (2020) simplified the annotation task from document level to sentence level and additionally guided workers with similar sentences that had already been annotated using an unsupervised semantic shorttext similarity method. In this paper, we instead only require sentence-level crowdsourced annotations without a boundary information developing novel method to predict PICO spans without training data. 2.3 Span based Methods Compared with token based methods su"
2021.findings-emnlp.147,D18-1191,0,0.322183,"imilarity method. In this paper, we instead only require sentence-level crowdsourced annotations without a boundary information developing novel method to predict PICO spans without training data. 2.3 Span based Methods Compared with token based methods such as BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016), span based methods treat the spans (i.e. consecutive tokens), as the targets. In one stream of research advances in span based methods, the aim is to extract the hidden representations of each token with the raw token sequence as input, then either use boundary token representations (Ouchi et al., 2018; Ebner et al., 2020) or aggregate all the token representations (Liu et al., 2020) as the span representation. All possible spans are enumerated, classified, and decoded. An alternative stream of span-based research aims to mask a span in the token sequence and recover the masked tokens with hidden representations (Joshi et al., 2020). Both research streams use supervised or self-supervised methods and high-quality annotations as training data. In this paper, we instead extract the information stored in the span using only sentence-level annotations derived from crowdsourcing. 3 Method 3.1 Ta"
2021.findings-emnlp.147,N16-1030,0,0.170672,"low-quality annotations, especially for the boundaries of the spans. To improve the annotation quality, Zlabinger et al. (2020) simplified the annotation task from document level to sentence level and additionally guided workers with similar sentences that had already been annotated using an unsupervised semantic shorttext similarity method. In this paper, we instead only require sentence-level crowdsourced annotations without a boundary information developing novel method to predict PICO spans without training data. 2.3 Span based Methods Compared with token based methods such as BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016), span based methods treat the spans (i.e. consecutive tokens), as the targets. In one stream of research advances in span based methods, the aim is to extract the hidden representations of each token with the raw token sequence as input, then either use boundary token representations (Ouchi et al., 2018; Ebner et al., 2020) or aggregate all the token representations (Liu et al., 2020) as the span representation. All possible spans are enumerated, classified, and decoded. An alternative stream of span-based research aims to mask a span in the token sequence and recover the"
2021.findings-emnlp.147,W19-5006,0,0.129699,"sual synonyms and subordinate clauses that might include numerical information (e.g. drug dosage, or test result threshold); and can incorporate domainspecific knowledge in intelligent and useful ways. In this paper, we address these challenges by proposing a novel PICO annotation task design. We propose a simplified requirement for annotation where we only need to know whether a sentence includes any PICO information. We use this coarser set of annotations to learn and infer PICO sentence types and span detections. The pre-trained neural language model (Devlin et al., 2019; Lee et al., 2020; Peng et al., 2019) is firstly used as feature representation learning model for sentence classification to be fine-tuned, and identify whether a given sentence contains PICO spans. This makes the proposed approach also capable of inferring PICO sentences even without crowdsourced annotations. To get the PICO spans, we apply a masked span prediction task to assist the inference process. The fine-tuned language model is then used as the taskspecific knowledge provider for the PICO spans. Scored spans are then fed into an inference algorithm to produce the final detection results. The contributions of this paper a"
2021.findings-emnlp.147,N18-1202,0,0.0081304,"O spans, we apply a masked span prediction task to assist the inference process. The fine-tuned language model is then used as the taskspecific knowledge provider for the PICO spans. Scored spans are then fed into an inference algorithm to produce the final detection results. The contributions of this paper are as follows: • We demonstrate results that substantially improve on recall in span detection to align with the use case in the systematic review application domain on two benchmark datasets. 2 2.1 Related Work Neural Language Models Pre-trained deep neural language models, such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Lee et al., 2020) and its variants (Liu et al., 2019; Lan et al., 2020), have brought significant performance improvements in a wide range of NLP tasks, such as relation extraction (Alt et al., 2019), entity resolution (Li et al., 2021), and question answering (Lee et al., 2020). These language models generally benefit from large scale text corpora. A major advantage of these methods come from the way long dependency token relations are captured to produce contextualised representations. To adapt language models for use in the biomedical domain, researchers"
2021.findings-emnlp.147,D18-1309,0,0.0253789,"Missing"
2021.findings-emnlp.147,2020.acl-main.31,0,0.0119913,"sification (the model on the left) and masked span prediction (the model on the right). The BLUE encoder is firstly trained on the sentence prediction using the contextualised embedding of [CLS] token (the left part of the figure). Then the fine-tuned BLUE encoder predicts the score with span masked (the right part of the figure). The predicted scores of candidate spans along with the raw token sequence is collected for inference. f (C1 , . . . , C|C |) (details of the functions are presented in Section 4.1). Compared with other stateof-the-art text classification methods (Huang et al., 2019; Zhang et al., 2020), our sentence classification method is relatively simple with just one extended module. The corresponding loss function we apply here is the cross entropy function: L=− X ylog(p(y |h). s∈D 3.3 Masked Span Prediction (3) original token sequence with [MASK] token. This masked token sequence is put into the fine-tuned neural language model to get the prediction score scorehi,ji . The score scorehi,ji is then compared with the original score score to infer the impact by the span hi, ji, i.e., the contribution of the span in classifying the sentence as a PICO sentence. We define the contribution o"
2021.findings-emnlp.147,2020.findings-emnlp.274,0,0.500169,"le, nent of systematic reviews. The rapid rate at which locations, and organisations, crowdsourcing of lapapers were published about COVID-19 (Wang and bels is somewhat easier because a broader range 1705 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1705–1715 November 7–11, 2021. ©2021 Association for Computational Linguistics of people can annotate data without specialised training. Even when annotated by domain experts, there is still substantial inconsistency across annotators (Lee and Sun, 2019), though sentence level annotations tend to be more consistent (Zlabinger et al., 2020). There is a gap in both the volume of available training data and demonstrated performance between NER in general domains such as news and biomedical applications compared to PICO extraction. There is a clear need for new approaches that can handle the more complicated and challenging token structure of biomedical entities including unusual synonyms and subordinate clauses that might include numerical information (e.g. drug dosage, or test result threshold); and can incorporate domainspecific knowledge in intelligent and useful ways. In this paper, we address these challenges by proposing a n"
2021.findings-emnlp.413,D15-1075,0,0.0413008,"n queries; iv) mix only on nonanswer contexts. Empirical results suggest that mixing only on non-answer contexts leads to the best and most consistent outcome. Regarding CAL and CRM, we sum up the start position loss and end position loss as the final loss. 4 Experiments 4.1 Dataset Yahoo! Answers (Chang et al., 2008) consists of questions and their corresponding answers along with the categories that are assigned to questions. We carry out the same pre-processing as in (Chen et al., 2020). IMDB (Lin et al., 2011) is a typical dataset for binary sentiment analysis including 50k samples. SNLI (Bowman et al., 2015) is a popular text entailment dataset that contains 570k human annotated sentence pairs. SQuAD 1.1 (Rajpurkar et al., 2016) consists of 100k question/answer pairs. Given a question and a Wikipedia passage containing the answer, the task is to predict the answer span in the passage. SQuAD 2.0 ((Rajpurkar et al., 2018)) combines the existing SQuAD 1.1 data with over 50k unanswerable questions written adversarially by crowdworkers. For each dataset, experiments are conducted on multiple data sizes. Experiments are controlled in an incremental manner where we gradually increase the training set si"
2021.findings-emnlp.413,2020.acl-main.194,0,0.213715,"ir labels to generate new samples (˜ x, y˜) from (xi , yi ) and (xj , yj ), formally as x ˜ = λxi + (1 − λ)xj , • We investigate the problem of spurious correlations from a causality perspective which has not been widely studied in conventional statistical learning. y˜ = λyi + (1 − λ)yj . (1) Followed by mixup, more works (Verma et al., 2019a,b,c; Berthelot et al., 2019; Yun et al., 2019) are proposed, mainly focusing on image-format • We propose CMIX for counterfactual repre- data. Recently, such regularization techniques were sentation interpolation to approximate do- brought into NLP tasks Chen et al. (2020). 4810 Adversarial Training. Adversarial training has been proven to be an effective approach for improving the robustness of neural network models (Miyato et al., 2017; Madry et al., 2018; Tramèr et al., 2017; Shrivastava et al., 2017). Specifically, Miyato et al. (2017) applied adversarial and virtual adversarial training to text domain by applying perturbations to the word embeddings, which achieved state-of-the-art results on multiple semi-supervised and purely supervised tasks. By minimizing the resultant adversarial risk inside different regions around input samples, Zhu et al. (2019) pr"
2021.findings-emnlp.413,N19-1423,0,0.0304041,"irst generates a counterfactual representation through latent space interpolation in an adversarial manner, and then performs Counterfactual Risk Minimization (CRM) on each original-counterfactual pair to adjust samplewise loss weight dynamically, which encourages the model to explore the true causal effect. Extensive experiments demonstrate that CAT achieves substantial performance improvement over SOTA across different downstream tasks, including sentence classification, natural language inference and question answering. 1 1 Introduction Large-scale pre-trained language models such as BERT (Devlin et al., 2019), as one of the recent breakthroughs, have revolutionized the model development paradigm in natural language processing (NLP) and improved traditional task-specific models by a large margin. Although the pre-training and fine-tuning framework has been shown to be effective in transferring the pre-learned knowledge to downstream tasks and boosting the model performance, it could be a double-edged sword if there exists statistical bias in the training dataset, especially in small data scenarios (Yue et al., 2020). Taking sentiment analysis as an example, the downstream classifier can easily mist"
2021.findings-emnlp.413,P17-2090,0,0.0241142,"019) and Liu et al. (2019) proposed to build the pre-trained language models utilizing even larger corpus to reduce the bias. Data Augmentation. Data augmentation is another solution and has become a de facto technique used in state-of-the-art machine learning models. Zhang et al. (2015) performed text augmentation by replacing words or phrases with their synonyms, and recently Wei and Zou (2019) proposed more operations. Using word embedding, (Wang and Yang, 2015) tried to find a similar word for replacement. In addition, back translations (Sennrich et al., 2016) and contextual augmentation (Fadaee et al., 2017; Kobayashi, 2018) techniques have been proposed to replace target words. Through shrinking the weight of the training data relative to L2 regularization, mixup (Zhang et al., 2018) trained a neural network on convex combinations of pairs of examples and their labels to generate new samples (˜ x, y˜) from (xi , yi ) and (xj , yj ), formally as x ˜ = λxi + (1 − λ)xj , • We investigate the problem of spurious correlations from a causality perspective which has not been widely studied in conventional statistical learning. y˜ = λyi + (1 − λ)yj . (1) Followed by mixup, more works (Verma et al., 201"
2021.findings-emnlp.413,P11-2000,0,0.109018,"ix, which is the same as in sentence classification task; ii) mix only on context; iii) mix only on queries; iv) mix only on nonanswer contexts. Empirical results suggest that mixing only on non-answer contexts leads to the best and most consistent outcome. Regarding CAL and CRM, we sum up the start position loss and end position loss as the final loss. 4 Experiments 4.1 Dataset Yahoo! Answers (Chang et al., 2008) consists of questions and their corresponding answers along with the categories that are assigned to questions. We carry out the same pre-processing as in (Chen et al., 2020). IMDB (Lin et al., 2011) is a typical dataset for binary sentiment analysis including 50k samples. SNLI (Bowman et al., 2015) is a popular text entailment dataset that contains 570k human annotated sentence pairs. SQuAD 1.1 (Rajpurkar et al., 2016) consists of 100k question/answer pairs. Given a question and a Wikipedia passage containing the answer, the task is to predict the answer span in the passage. SQuAD 2.0 ((Rajpurkar et al., 2018)) combines the existing SQuAD 1.1 data with over 50k unanswerable questions written adversarially by crowdworkers. For each dataset, experiments are conducted on multiple data sizes"
2021.findings-emnlp.413,2021.ccl-1.108,0,0.0487877,"Missing"
2021.findings-emnlp.413,P18-2124,0,0.0146905,"008) consists of questions and their corresponding answers along with the categories that are assigned to questions. We carry out the same pre-processing as in (Chen et al., 2020). IMDB (Lin et al., 2011) is a typical dataset for binary sentiment analysis including 50k samples. SNLI (Bowman et al., 2015) is a popular text entailment dataset that contains 570k human annotated sentence pairs. SQuAD 1.1 (Rajpurkar et al., 2016) consists of 100k question/answer pairs. Given a question and a Wikipedia passage containing the answer, the task is to predict the answer span in the passage. SQuAD 2.0 ((Rajpurkar et al., 2018)) combines the existing SQuAD 1.1 data with over 50k unanswerable questions written adversarially by crowdworkers. For each dataset, experiments are conducted on multiple data sizes. Experiments are controlled in an incremental manner where we gradually increase the training set size. For sentence classification and natural language inference tasks, we randomly select a fixed test set of size 2000. For question answering, the full dev set is used for evaluation. 4.2 Implementation For a fair comparsion, We employ BERTBASE , BERTLARGE , RoBERTaBASE , RoBERTaLARGE , and BERTBASE with TMix (Chen"
2021.findings-emnlp.413,D16-1264,0,0.0282395,"o the best and most consistent outcome. Regarding CAL and CRM, we sum up the start position loss and end position loss as the final loss. 4 Experiments 4.1 Dataset Yahoo! Answers (Chang et al., 2008) consists of questions and their corresponding answers along with the categories that are assigned to questions. We carry out the same pre-processing as in (Chen et al., 2020). IMDB (Lin et al., 2011) is a typical dataset for binary sentiment analysis including 50k samples. SNLI (Bowman et al., 2015) is a popular text entailment dataset that contains 570k human annotated sentence pairs. SQuAD 1.1 (Rajpurkar et al., 2016) consists of 100k question/answer pairs. Given a question and a Wikipedia passage containing the answer, the task is to predict the answer span in the passage. SQuAD 2.0 ((Rajpurkar et al., 2018)) combines the existing SQuAD 1.1 data with over 50k unanswerable questions written adversarially by crowdworkers. For each dataset, experiments are conducted on multiple data sizes. Experiments are controlled in an incremental manner where we gradually increase the training set size. For sentence classification and natural language inference tasks, we randomly select a fixed test set of size 2000. For"
2021.findings-emnlp.413,P16-1009,0,0.0438693,"lations is using large-scale dataset. Yang et al. (2019) and Liu et al. (2019) proposed to build the pre-trained language models utilizing even larger corpus to reduce the bias. Data Augmentation. Data augmentation is another solution and has become a de facto technique used in state-of-the-art machine learning models. Zhang et al. (2015) performed text augmentation by replacing words or phrases with their synonyms, and recently Wei and Zou (2019) proposed more operations. Using word embedding, (Wang and Yang, 2015) tried to find a similar word for replacement. In addition, back translations (Sennrich et al., 2016) and contextual augmentation (Fadaee et al., 2017; Kobayashi, 2018) techniques have been proposed to replace target words. Through shrinking the weight of the training data relative to L2 regularization, mixup (Zhang et al., 2018) trained a neural network on convex combinations of pairs of examples and their labels to generate new samples (˜ x, y˜) from (xi , yi ) and (xj , yj ), formally as x ˜ = λxi + (1 − λ)xj , • We investigate the problem of spurious correlations from a causality perspective which has not been widely studied in conventional statistical learning. y˜ = λyi + (1 − λ)yj . (1)"
2021.findings-emnlp.413,N18-2072,0,0.0171223,"2019) proposed to build the pre-trained language models utilizing even larger corpus to reduce the bias. Data Augmentation. Data augmentation is another solution and has become a de facto technique used in state-of-the-art machine learning models. Zhang et al. (2015) performed text augmentation by replacing words or phrases with their synonyms, and recently Wei and Zou (2019) proposed more operations. Using word embedding, (Wang and Yang, 2015) tried to find a similar word for replacement. In addition, back translations (Sennrich et al., 2016) and contextual augmentation (Fadaee et al., 2017; Kobayashi, 2018) techniques have been proposed to replace target words. Through shrinking the weight of the training data relative to L2 regularization, mixup (Zhang et al., 2018) trained a neural network on convex combinations of pairs of examples and their labels to generate new samples (˜ x, y˜) from (xi , yi ) and (xj , yj ), formally as x ˜ = λxi + (1 − λ)xj , • We investigate the problem of spurious correlations from a causality perspective which has not been widely studied in conventional statistical learning. y˜ = λyi + (1 − λ)yj . (1) Followed by mixup, more works (Verma et al., 2019a,b,c; Berthelot"
2021.findings-emnlp.413,2020.emnlp-main.590,0,0.19848,"ariables that help to unveil the true casual effect behind observation data and realize more robust inference. Specifically, counterfactuals are one feasible way to discover the causation. Counterfactual examples are defined as the ones that minimallydifferent from original ones but lead to different labels (Teney et al., 2020). Recent work shows that counterfactual samples can significantly improve model generalization and boost model performance. Kaushik et al. (2019) and Teney et al. (2020) use additional human-labeled or augmented counterfactual examples to mitigate spurious correlations. Zeng et al. (2020) proposes a two-stage training approach for named entity recognition tasks for better generalization. Although these methods gain significant improvements in model performance, they are limited in practice since they are either designed for specific tasks or requiring human-labeled samples. Additionally, no optimization is conducted on those counterfactual examples which could further improve the performance shown in our work. Our work. We revisit the above problem in NLP 4809 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4809–4820 November 7–11, 2021. ©2021 Asso"
2021.findings-emnlp.413,D15-1306,0,0.0189497,"arge-scale Pre-trained Language Model. The most widely-used solution for alleviating spurious correlations is using large-scale dataset. Yang et al. (2019) and Liu et al. (2019) proposed to build the pre-trained language models utilizing even larger corpus to reduce the bias. Data Augmentation. Data augmentation is another solution and has become a de facto technique used in state-of-the-art machine learning models. Zhang et al. (2015) performed text augmentation by replacing words or phrases with their synonyms, and recently Wei and Zou (2019) proposed more operations. Using word embedding, (Wang and Yang, 2015) tried to find a similar word for replacement. In addition, back translations (Sennrich et al., 2016) and contextual augmentation (Fadaee et al., 2017; Kobayashi, 2018) techniques have been proposed to replace target words. Through shrinking the weight of the training data relative to L2 regularization, mixup (Zhang et al., 2018) trained a neural network on convex combinations of pairs of examples and their labels to generate new samples (˜ x, y˜) from (xi , yi ) and (xj , yj ), formally as x ˜ = λxi + (1 − λ)xj , • We investigate the problem of spurious correlations from a causality perspecti"
2021.findings-emnlp.413,D19-1670,0,0.0117032,"different tasks particularly when data is limited. 2 Related Work Large-scale Pre-trained Language Model. The most widely-used solution for alleviating spurious correlations is using large-scale dataset. Yang et al. (2019) and Liu et al. (2019) proposed to build the pre-trained language models utilizing even larger corpus to reduce the bias. Data Augmentation. Data augmentation is another solution and has become a de facto technique used in state-of-the-art machine learning models. Zhang et al. (2015) performed text augmentation by replacing words or phrases with their synonyms, and recently Wei and Zou (2019) proposed more operations. Using word embedding, (Wang and Yang, 2015) tried to find a similar word for replacement. In addition, back translations (Sennrich et al., 2016) and contextual augmentation (Fadaee et al., 2017; Kobayashi, 2018) techniques have been proposed to replace target words. Through shrinking the weight of the training data relative to L2 regularization, mixup (Zhang et al., 2018) trained a neural network on convex combinations of pairs of examples and their labels to generate new samples (˜ x, y˜) from (xi , yi ) and (xj , yj ), formally as x ˜ = λxi + (1 − λ)xj , • We inves"
2021.findings-emnlp.66,2020.acl-main.582,0,0.336394,"8; results. The root of the problem is the lack of large Chen et al., 2018; Bauman et al., 2017; Liu et al., review corpora with aspect and sentiment annotations. The existing ones are either too small or 2019). Cutting-edge natural language processing too domain-specific (Wang and Pan, 2018) to be (NLP) techniques are applied to extract the latent user sentiments, item properties, and the compli- applied to general use cases. Progress on sentiment term extraction (Dai and Song, 2019; Tian cated interactions between the two components. However, existing approaches have disadvan- et al., 2020; Chen et al., 2020a) takes advantage of neural networks and linguistic knowledge and tages bearing room for improvement. Firstly, they dismiss the phenomenon that users may hold dif- partially makes it possible to use unsupervised term annotation to tackle the lack-of-huge-corpus issue. ferent attentions toward various properties of the merchandise. An item property is the combination In this paper, we seek to understand how reof an aspect of the item and the characteristic asso- views and ratings are affected by users’ perception 763 Findings of the Association for Computational Linguistics: EMNLP 2021, pages"
2021.findings-emnlp.66,2020.acl-main.338,0,0.432363,"8; results. The root of the problem is the lack of large Chen et al., 2018; Bauman et al., 2017; Liu et al., review corpora with aspect and sentiment annotations. The existing ones are either too small or 2019). Cutting-edge natural language processing too domain-specific (Wang and Pan, 2018) to be (NLP) techniques are applied to extract the latent user sentiments, item properties, and the compli- applied to general use cases. Progress on sentiment term extraction (Dai and Song, 2019; Tian cated interactions between the two components. However, existing approaches have disadvan- et al., 2020; Chen et al., 2020a) takes advantage of neural networks and linguistic knowledge and tages bearing room for improvement. Firstly, they dismiss the phenomenon that users may hold dif- partially makes it possible to use unsupervised term annotation to tackle the lack-of-huge-corpus issue. ferent attentions toward various properties of the merchandise. An item property is the combination In this paper, we seek to understand how reof an aspect of the item and the characteristic asso- views and ratings are affected by users’ perception 763 Findings of the Association for Computational Linguistics: EMNLP 2021, pages"
2021.findings-emnlp.66,P19-1520,0,0.364713,"us research on review-based rec- to uninterpretable and sometimes counterintuitive ommendation has been fruitful (Chin et al., 2018; results. The root of the problem is the lack of large Chen et al., 2018; Bauman et al., 2017; Liu et al., review corpora with aspect and sentiment annotations. The existing ones are either too small or 2019). Cutting-edge natural language processing too domain-specific (Wang and Pan, 2018) to be (NLP) techniques are applied to extract the latent user sentiments, item properties, and the compli- applied to general use cases. Progress on sentiment term extraction (Dai and Song, 2019; Tian cated interactions between the two components. However, existing approaches have disadvan- et al., 2020; Chen et al., 2020a) takes advantage of neural networks and linguistic knowledge and tages bearing room for improvement. Firstly, they dismiss the phenomenon that users may hold dif- partially makes it possible to use unsupervised term annotation to tackle the lack-of-huge-corpus issue. ferent attentions toward various properties of the merchandise. An item property is the combination In this paper, we seek to understand how reof an aspect of the item and the characteristic asso- view"
2021.findings-emnlp.66,N19-1423,0,0.00672001,"anguage into embeddings, then learns explicit and implicit features, and finally computes the score regression. One distinctive feature of APRE is that it explicitly models the aspect information by incorporating a da -dimensional aspect representation ai ∈ Rda in each side of the substructures for review encoding. Let A(u) = (u) (u) {a1 , . . . , ak } denotes the k aspect embeddings for users and A(t) for items. k is decided by the number of unique aspects in the AS-pair set. Language encoding The reviews are encoded into low-dimensional token embedding sequences by a fixed pre-trained BERT (Devlin et al., 2019), a powerful transformer-based contextualized language encoder. For each review r in Ru or Rt , 1 the resulting encoding H0 ∈ R(|r|+2)×de consists Section A.2.1† explains this procedure in detail by pseudocode of Algorithm 1†. of (|r |+ 2) de -dimensional contextualized vectors: 766 IMPLICIT vt EXPLICIT APRE - Item Reviews Encoder Gt rating pred. Fex (·) s ˆu,t <latexit sha1_base64=""jA5OQwvMa/E4znz6Mb3JRY5xU8g=""&gt;AAAB9XicbVBNS8NAEJ34WetX1aOXxSJ4kJKIoseiF48V7Ae0tWy2m3bpZhN2J0oJ+R9ePCji1f/izX/jts1BWx8MPN6bYWaeH0th0HW/naXlldW19cJGcXNre2e3tLffMFGiGa+zSEa65VPDpVC8jgIlb8Wa09CXvOmPbiZ+85FrIyJ1j+OYd0M6"
2021.findings-emnlp.66,2020.acl-main.370,0,0.0258126,"located in the overlap of ABSA and Recommender Systems. 2.1 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) (Xu et al., 2020; Wang et al., 2018) predicts sentiments toward aspects mentioned in the text. Natural language is modeled by graphs in (Zhang et al., 2019; Wang et al., 2020) such as Pointwise Mutual Information (PMI) graphs and dependency graphs. Phan and Ogunbona (2020) and Tang et al. (2020) utilize contextualized language encoding to capture the context of aspect terms. Chen et al. (2020b) focuses on the consistency of the emotion surrounding the aspects, and Du et al. (2020) equips pre-trained BERT with domain-awareness of sentiments. Our work is informed by these progress which utilize PMI, dependency tree, and BERT for syntax feature extraction and language encoding. 2.2 Aspect or Sentiment Terms Extraction Aspect and sentiment terms extraction is a presupposition of ABSA. However, manually annotating data for training, which requires the hard labor of experts, is only feasible on small datasets in particular domains such as Laptop and Restaurant (Pontiki et al., 2014, 2015) which are overused in ABSA. Recently, RINANTE (Dai and Song, 2019) and SDRN (Chen et al"
2021.findings-emnlp.66,P17-1036,0,0.0298165,"ancies of other methods and expand the coverage of terms shown in Table 10†. pobj cc nsubj quality acomp is superior nsubj and comfort acomp is excellent. Extracted AS-pair candidates: (Sound quality, superior), (comfort, excellent) Figure 2: Two dependency-based rules for AS-pair candidates extraction. Effective dependency relations and aspects and sentiments candidates are highlighted. Discussion ASPE is different from Aspect Extraction (AE) (Tulkens and van Cranenburgh, 2020; Luo et al., 2019; Wei et al., 2020; Ma et al., 2019; Angelidis and Lapata, 2018; Xu et al., 2018; Shu et al., 2017; He et al., 2017a) which extracts aspects only and infers sentiment polarities in {pos, neg, (neu)}. AS-pair co-extraction, however, offers more diversified emotional signals than the bipolar sentiment measurement of AE. 3.3 APRE APRE, depicted in Figure 3, predicts ratings given reviews and the corresponding AS-pairs. It first encodes language into embeddings, then learns explicit and implicit features, and finally computes the score regression. One distinctive feature of APRE is that it explicitly models the aspect information by incorporating a da -dimensional aspect representation ai ∈ Rda in each side of"
2021.findings-emnlp.66,P19-1056,0,0.01835,"rall sentiment set used in AS-pair generation: ST = STPMI ∪ STNN ∪ STLex . The three sets compensate for the discrepancies of other methods and expand the coverage of terms shown in Table 10†. pobj cc nsubj quality acomp is superior nsubj and comfort acomp is excellent. Extracted AS-pair candidates: (Sound quality, superior), (comfort, excellent) Figure 2: Two dependency-based rules for AS-pair candidates extraction. Effective dependency relations and aspects and sentiments candidates are highlighted. Discussion ASPE is different from Aspect Extraction (AE) (Tulkens and van Cranenburgh, 2020; Luo et al., 2019; Wei et al., 2020; Ma et al., 2019; Angelidis and Lapata, 2018; Xu et al., 2018; Shu et al., 2017; He et al., 2017a) which extracts aspects only and infers sentiment polarities in {pos, neg, (neu)}. AS-pair co-extraction, however, offers more diversified emotional signals than the bipolar sentiment measurement of AE. 3.3 APRE APRE, depicted in Figure 3, predicts ratings given reviews and the corresponding AS-pairs. It first encodes language into embeddings, then learns explicit and implicit features, and finally computes the score regression. One distinctive feature of APRE is that it explici"
2021.findings-emnlp.66,P19-1344,0,0.0146348,"eneration: ST = STPMI ∪ STNN ∪ STLex . The three sets compensate for the discrepancies of other methods and expand the coverage of terms shown in Table 10†. pobj cc nsubj quality acomp is superior nsubj and comfort acomp is excellent. Extracted AS-pair candidates: (Sound quality, superior), (comfort, excellent) Figure 2: Two dependency-based rules for AS-pair candidates extraction. Effective dependency relations and aspects and sentiments candidates are highlighted. Discussion ASPE is different from Aspect Extraction (AE) (Tulkens and van Cranenburgh, 2020; Luo et al., 2019; Wei et al., 2020; Ma et al., 2019; Angelidis and Lapata, 2018; Xu et al., 2018; Shu et al., 2017; He et al., 2017a) which extracts aspects only and infers sentiment polarities in {pos, neg, (neu)}. AS-pair co-extraction, however, offers more diversified emotional signals than the bipolar sentiment measurement of AE. 3.3 APRE APRE, depicted in Figure 3, predicts ratings given reviews and the corresponding AS-pairs. It first encodes language into embeddings, then learns explicit and implicit features, and finally computes the score regression. One distinctive feature of APRE is that it explicitly models the aspect information b"
2021.findings-emnlp.66,2020.emnlp-main.568,0,0.152942,"s aspect-based NLP techniques to extract rior accuracy over the leading baselines. explicit and definitive aspects. However, existing aspect-based models mainly use latent or implicit 1 Introduction aspects (Chin et al., 2018) whose real semantics Reviews and ratings are valuable assets for the rec- are unjustifiable. Similar to Latent Dirichlet Allocation (LDA, Blei et al., 2003), the semantics ommender systems of e-commerce websites since of the derived aspects (topics) are mutually overthey immediately describe the users’ subjective feelings about the purchases. Learning user pref- lapped (Huang et al., 2020b). These models undermine the resultant aspect distinctiveness and lead erences from such feedback is straightforward and efficacious. Previous research on review-based rec- to uninterpretable and sometimes counterintuitive ommendation has been fruitful (Chin et al., 2018; results. The root of the problem is the lack of large Chen et al., 2018; Bauman et al., 2017; Liu et al., review corpora with aspect and sentiment annotations. The existing ones are either too small or 2019). Cutting-edge natural language processing too domain-specific (Wang and Pan, 2018) to be (NLP) techniques are applied"
2021.findings-emnlp.66,2020.acl-main.293,0,0.0349761,"R3 rather than the item being a headset. Aspect-level attitude modeling is more accurate, informative, and personalized than preference modeling. 2 Related Work Our work is related to four lines of literature which are located in the overlap of ABSA and Recommender Systems. 2.1 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) (Xu et al., 2020; Wang et al., 2018) predicts sentiments toward aspects mentioned in the text. Natural language is modeled by graphs in (Zhang et al., 2019; Wang et al., 2020) such as Pointwise Mutual Information (PMI) graphs and dependency graphs. Phan and Ogunbona (2020) and Tang et al. (2020) utilize contextualized language encoding to capture the context of aspect terms. Chen et al. (2020b) focuses on the consistency of the emotion surrounding the aspects, and Du et al. (2020) equips pre-trained BERT with domain-awareness of sentiments. Our work is informed by these progress which utilize PMI, dependency tree, and BERT for syntax feature extraction and language encoding. 2.2 Aspect or Sentiment Terms Extraction Aspect and sentiment terms extraction is a presupposition of ABSA. However, manually annotating data for training, which requires the hard labor of"
2021.findings-emnlp.66,S15-2082,0,0.0732915,"Missing"
2021.findings-emnlp.66,S14-2004,0,0.0442659,"terms. Chen et al. (2020b) focuses on the consistency of the emotion surrounding the aspects, and Du et al. (2020) equips pre-trained BERT with domain-awareness of sentiments. Our work is informed by these progress which utilize PMI, dependency tree, and BERT for syntax feature extraction and language encoding. 2.2 Aspect or Sentiment Terms Extraction Aspect and sentiment terms extraction is a presupposition of ABSA. However, manually annotating data for training, which requires the hard labor of experts, is only feasible on small datasets in particular domains such as Laptop and Restaurant (Pontiki et al., 2014, 2015) which are overused in ABSA. Recently, RINANTE (Dai and Song, 2019) and SDRN (Chen et al., 2020a) automatically extract Note. Due to the page limits, some support- both terms using rule-guided data augmentation ive materials, marked by “†”, are presented in and double-channel opinion-relation co-extraction, the Supplementary Materials. We strongly rec- respectively. However, the supervised approaches ommend readers check out these materials. The are too domain-specific to generalize to out-ofsource code of our work is available on GitHub domain or open-domain corpora. Conducting doat ht"
2021.findings-emnlp.66,P17-2023,0,0.0285039,"te for the discrepancies of other methods and expand the coverage of terms shown in Table 10†. pobj cc nsubj quality acomp is superior nsubj and comfort acomp is excellent. Extracted AS-pair candidates: (Sound quality, superior), (comfort, excellent) Figure 2: Two dependency-based rules for AS-pair candidates extraction. Effective dependency relations and aspects and sentiments candidates are highlighted. Discussion ASPE is different from Aspect Extraction (AE) (Tulkens and van Cranenburgh, 2020; Luo et al., 2019; Wei et al., 2020; Ma et al., 2019; Angelidis and Lapata, 2018; Xu et al., 2018; Shu et al., 2017; He et al., 2017a) which extracts aspects only and infers sentiment polarities in {pos, neg, (neu)}. AS-pair co-extraction, however, offers more diversified emotional signals than the bipolar sentiment measurement of AE. 3.3 APRE APRE, depicted in Figure 3, predicts ratings given reviews and the corresponding AS-pairs. It first encodes language into embeddings, then learns explicit and implicit features, and finally computes the score regression. One distinctive feature of APRE is that it explicitly models the aspect information by incorporating a da -dimensional aspect representation ai ∈ Rd"
2021.findings-emnlp.66,2020.acl-main.588,0,0.0243643,"g a headset. Aspect-level attitude modeling is more accurate, informative, and personalized than preference modeling. 2 Related Work Our work is related to four lines of literature which are located in the overlap of ABSA and Recommender Systems. 2.1 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) (Xu et al., 2020; Wang et al., 2018) predicts sentiments toward aspects mentioned in the text. Natural language is modeled by graphs in (Zhang et al., 2019; Wang et al., 2020) such as Pointwise Mutual Information (PMI) graphs and dependency graphs. Phan and Ogunbona (2020) and Tang et al. (2020) utilize contextualized language encoding to capture the context of aspect terms. Chen et al. (2020b) focuses on the consistency of the emotion surrounding the aspects, and Du et al. (2020) equips pre-trained BERT with domain-awareness of sentiments. Our work is informed by these progress which utilize PMI, dependency tree, and BERT for syntax feature extraction and language encoding. 2.2 Aspect or Sentiment Terms Extraction Aspect and sentiment terms extraction is a presupposition of ABSA. However, manually annotating data for training, which requires the hard labor of experts, is only feasib"
2021.findings-emnlp.70,D16-1170,1,0.82418,"otion reasoner to generate the response. Specifically, a gated attention mechanism is designed to incorporate emotion cause information into the response generator. accurately perceive and respond to implicit emotions. Li et al. (2020a) exploits user feedback and multi-granularity emotion, and introduces an adversarial learning framework to capture the nuances of user emotion. Emotion cause extraction (ECE), aims at exploring the reason for emotion change and what causes a certain emotion. Lee et al. (2010); Chen et al. (2010) first define it as a word-level and clauselevel task respectively. Gui et al. (2016) proposes the first open dataset for ECE, and it serves as a standard benchmark up till now. Xia and Ding (2019) reforms ECE into emotion-cause pair extraction task. Similar to ECE, Poria et al. (2020) first introduces the task of recognizing emotion cause in conversations. Our framework that explicitly considers the emotion cause for empathetic response generation is shown in Figure 1. Our framework contains two components: an emotion reasoner and a response generator. The emotion reasoner is used to predict a context emotion label and locate words related to the emotion cause, based on the d"
2021.findings-emnlp.70,N18-2008,0,0.0217038,"d significantly outperform other compared methods, resulting in more empathetic responses. 2 Related Work In recent years, neural approaches to open-domain dialogue systems have achieved great progress (Serban et al., 2016; Wolf et al., 2019; Zhang et al., 2020b; Zhou et al., 2020; Xu et al., 2020; Wang et al., 2021). Especially, incorporating personality and emotional features can make dialogue systems more human-like. For emotion-aware response generation, it aims at generating responses corresponding to specific emotions. Several methods are proposed to tackle this task (Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019; Song et al., 2019; Shen and Feng, 2020; Xu et al., 2021; Majumder et al., 2021). Empathetic response generation is a sub-task of emotion-aware response generation, Rashkin et al. (2019) first proposes a standard benchmark that contains large-scale empathetic conversations. Lin et al. (2020) adapts GPT2 (Radford et al., 2019) to generate empathetic responses via transfer learning and continues to improve its response quality via active learning and negative training. Welivita and Pu (2020) develops a taxonomy of empathetic listener intents by human judges to generate mor"
2021.findings-emnlp.70,W10-0206,0,0.0494795,"the dialogue context. The response generator makes use of the emotional information obtained from the emotion reasoner to generate the response. Specifically, a gated attention mechanism is designed to incorporate emotion cause information into the response generator. accurately perceive and respond to implicit emotions. Li et al. (2020a) exploits user feedback and multi-granularity emotion, and introduces an adversarial learning framework to capture the nuances of user emotion. Emotion cause extraction (ECE), aims at exploring the reason for emotion change and what causes a certain emotion. Lee et al. (2010); Chen et al. (2010) first define it as a word-level and clauselevel task respectively. Gui et al. (2016) proposes the first open dataset for ECE, and it serves as a standard benchmark up till now. Xia and Ding (2019) reforms ECE into emotion-cause pair extraction task. Similar to ECE, Poria et al. (2020) first introduces the task of recognizing emotion cause in conversations. Our framework that explicitly considers the emotion cause for empathetic response generation is shown in Figure 1. Our framework contains two components: an emotion reasoner and a response generator. The emotion reasoner"
2021.findings-emnlp.70,N16-1014,0,0.580082,"r your children! Table 1: An example of empathetic responding from empathetic-dialogues dataset. An empathetic dialogue model is required to generate an appropriate response given the dialogue context. The utterance highlighted in blue contains the emotion cause. Introduction In recent years, open-domain dialogue systems are becoming increasingly ubiquitous and have been extensively leveraged for mental healthcare and entertainment (Oh et al., 2017; Zhou et al., 2020; Sharma et al., 2020). In part, this progress is driven by advances in neural response generation models (Vinyals and Le, 2015; Li et al., 2016a,c; Gao et al., 2019a,b) which have shown success in generating fluent and relevant responses, given a wide variety of user inputs. However, people can still feel a clear gap between humans and machines when conversing with them. One of the primary reasons is that existing dialogue systems lack emotion understanding and empathy (Rashkin et al., 2019). Empathetic responding is a desirable communicative ∗ † Equal Contribution Corresponding author skill that can make more natural communication in daily conversations (Callender, 2015). Table 1 shows an example of empathetic responding from empath"
2021.findings-emnlp.70,D16-1127,0,0.260494,"r your children! Table 1: An example of empathetic responding from empathetic-dialogues dataset. An empathetic dialogue model is required to generate an appropriate response given the dialogue context. The utterance highlighted in blue contains the emotion cause. Introduction In recent years, open-domain dialogue systems are becoming increasingly ubiquitous and have been extensively leveraged for mental healthcare and entertainment (Oh et al., 2017; Zhou et al., 2020; Sharma et al., 2020). In part, this progress is driven by advances in neural response generation models (Vinyals and Le, 2015; Li et al., 2016a,c; Gao et al., 2019a,b) which have shown success in generating fluent and relevant responses, given a wide variety of user inputs. However, people can still feel a clear gap between humans and machines when conversing with them. One of the primary reasons is that existing dialogue systems lack emotion understanding and empathy (Rashkin et al., 2019). Empathetic responding is a desirable communicative ∗ † Equal Contribution Corresponding author skill that can make more natural communication in daily conversations (Callender, 2015). Table 1 shows an example of empathetic responding from empath"
2021.findings-emnlp.70,2020.coling-main.394,0,0.366315,"ng from empathetic-dialogues dataset (Rashkin et al., 2019). A speaker is talking about a situation that happened to him/her related to a lonely feeling and a listener needs to respond with an appropriate emotion. Therefore, empathy is important in conversations. However, endowing dialogue systems with the capability of emotion understanding and empathetic responding is challenging. Most of the existing approaches improve empathetic response generation from two directions. The first usually promotes the model’s emotion understanding (Lubis et al., 2018; Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020b). In this line of work, models are often trained to predict an emotion state of the speaker and generate a response based on the emotion state. The second focuses on improving response generation strategy (Welivita and Pu, 2020; Shin et al., 2020; Majumder et al., 2020). For example, Shin et al. (2020) proposes to use the look-ahead of user emotion to model empathetic response generation and improve the empathetic responding model via Reinforcement Learning. Majumder et al. (2020) presents an ap807 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 807–819 November"
2021.findings-emnlp.70,D19-1012,0,0.339071,"mpathetic responding from empathetic-dialogues dataset (Rashkin et al., 2019). A speaker is talking about a situation that happened to him/her related to a lonely feeling and a listener needs to respond with an appropriate emotion. Therefore, empathy is important in conversations. However, endowing dialogue systems with the capability of emotion understanding and empathetic responding is challenging. Most of the existing approaches improve empathetic response generation from two directions. The first usually promotes the model’s emotion understanding (Lubis et al., 2018; Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020b). In this line of work, models are often trained to predict an emotion state of the speaker and generate a response based on the emotion state. The second focuses on improving response generation strategy (Welivita and Pu, 2020; Shin et al., 2020; Majumder et al., 2020). For example, Shin et al. (2020) proposes to use the look-ahead of user emotion to model empathetic response generation and improve the empathetic responding model via Reinforcement Learning. Majumder et al. (2020) presents an ap807 Findings of the Association for Computational Linguistics: EMNLP 2021, pages"
2021.findings-emnlp.70,2020.emnlp-main.721,0,0.0649622,"Missing"
2021.findings-emnlp.70,P02-1040,0,0.108744,"Missing"
2021.findings-emnlp.70,P19-1534,0,0.294511,"y ubiquitous and have been extensively leveraged for mental healthcare and entertainment (Oh et al., 2017; Zhou et al., 2020; Sharma et al., 2020). In part, this progress is driven by advances in neural response generation models (Vinyals and Le, 2015; Li et al., 2016a,c; Gao et al., 2019a,b) which have shown success in generating fluent and relevant responses, given a wide variety of user inputs. However, people can still feel a clear gap between humans and machines when conversing with them. One of the primary reasons is that existing dialogue systems lack emotion understanding and empathy (Rashkin et al., 2019). Empathetic responding is a desirable communicative ∗ † Equal Contribution Corresponding author skill that can make more natural communication in daily conversations (Callender, 2015). Table 1 shows an example of empathetic responding from empathetic-dialogues dataset (Rashkin et al., 2019). A speaker is talking about a situation that happened to him/her related to a lonely feeling and a listener needs to respond with an appropriate emotion. Therefore, empathy is important in conversations. However, endowing dialogue systems with the capability of emotion understanding and empathetic respondi"
2021.findings-emnlp.70,2020.emnlp-main.425,0,0.0314639,"! I wanted to join a group for local moms Target: That’s a good idea! This way you can meet friends for yourself, also maybe for your children! Table 1: An example of empathetic responding from empathetic-dialogues dataset. An empathetic dialogue model is required to generate an appropriate response given the dialogue context. The utterance highlighted in blue contains the emotion cause. Introduction In recent years, open-domain dialogue systems are becoming increasingly ubiquitous and have been extensively leveraged for mental healthcare and entertainment (Oh et al., 2017; Zhou et al., 2020; Sharma et al., 2020). In part, this progress is driven by advances in neural response generation models (Vinyals and Le, 2015; Li et al., 2016a,c; Gao et al., 2019a,b) which have shown success in generating fluent and relevant responses, given a wide variety of user inputs. However, people can still feel a clear gap between humans and machines when conversing with them. One of the primary reasons is that existing dialogue systems lack emotion understanding and empathy (Rashkin et al., 2019). Empathetic responding is a desirable communicative ∗ † Equal Contribution Corresponding author skill that can make more nat"
2021.findings-emnlp.70,2020.acl-main.52,0,0.0187721,"in more empathetic responses. 2 Related Work In recent years, neural approaches to open-domain dialogue systems have achieved great progress (Serban et al., 2016; Wolf et al., 2019; Zhang et al., 2020b; Zhou et al., 2020; Xu et al., 2020; Wang et al., 2021). Especially, incorporating personality and emotional features can make dialogue systems more human-like. For emotion-aware response generation, it aims at generating responses corresponding to specific emotions. Several methods are proposed to tackle this task (Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019; Song et al., 2019; Shen and Feng, 2020; Xu et al., 2021; Majumder et al., 2021). Empathetic response generation is a sub-task of emotion-aware response generation, Rashkin et al. (2019) first proposes a standard benchmark that contains large-scale empathetic conversations. Lin et al. (2020) adapts GPT2 (Radford et al., 2019) to generate empathetic responses via transfer learning and continues to improve its response quality via active learning and negative training. Welivita and Pu (2020) develops a taxonomy of empathetic listener intents by human judges to generate more controlled and interpretable responses. Shin et al. (2020) u"
2021.findings-emnlp.70,P19-1359,0,0.0198155,"methods, resulting in more empathetic responses. 2 Related Work In recent years, neural approaches to open-domain dialogue systems have achieved great progress (Serban et al., 2016; Wolf et al., 2019; Zhang et al., 2020b; Zhou et al., 2020; Xu et al., 2020; Wang et al., 2021). Especially, incorporating personality and emotional features can make dialogue systems more human-like. For emotion-aware response generation, it aims at generating responses corresponding to specific emotions. Several methods are proposed to tackle this task (Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019; Song et al., 2019; Shen and Feng, 2020; Xu et al., 2021; Majumder et al., 2021). Empathetic response generation is a sub-task of emotion-aware response generation, Rashkin et al. (2019) first proposes a standard benchmark that contains large-scale empathetic conversations. Lin et al. (2020) adapts GPT2 (Radford et al., 2019) to generate empathetic responses via transfer learning and continues to improve its response quality via active learning and negative training. Welivita and Pu (2020) develops a taxonomy of empathetic listener intents by human judges to generate more controlled and interpretable responses."
2021.findings-emnlp.70,2020.cl-1.2,0,0.0339102,"Missing"
2021.findings-emnlp.70,2020.coling-main.429,0,0.212576,", empathy is important in conversations. However, endowing dialogue systems with the capability of emotion understanding and empathetic responding is challenging. Most of the existing approaches improve empathetic response generation from two directions. The first usually promotes the model’s emotion understanding (Lubis et al., 2018; Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020b). In this line of work, models are often trained to predict an emotion state of the speaker and generate a response based on the emotion state. The second focuses on improving response generation strategy (Welivita and Pu, 2020; Shin et al., 2020; Majumder et al., 2020). For example, Shin et al. (2020) proposes to use the look-ahead of user emotion to model empathetic response generation and improve the empathetic responding model via Reinforcement Learning. Majumder et al. (2020) presents an ap807 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 807–819 November 7–11, 2021. ©2021 Association for Computational Linguistics proach to mimic the emotion of the speaker while accounting for their affective polarity. However, both kinds of existing methods only consider using the surface informa"
2021.findings-emnlp.70,P19-1096,0,0.0177235,"emotion cause information into the response generator. accurately perceive and respond to implicit emotions. Li et al. (2020a) exploits user feedback and multi-granularity emotion, and introduces an adversarial learning framework to capture the nuances of user emotion. Emotion cause extraction (ECE), aims at exploring the reason for emotion change and what causes a certain emotion. Lee et al. (2010); Chen et al. (2010) first define it as a word-level and clauselevel task respectively. Gui et al. (2016) proposes the first open dataset for ECE, and it serves as a standard benchmark up till now. Xia and Ding (2019) reforms ECE into emotion-cause pair extraction task. Similar to ECE, Poria et al. (2020) first introduces the task of recognizing emotion cause in conversations. Our framework that explicitly considers the emotion cause for empathetic response generation is shown in Figure 1. Our framework contains two components: an emotion reasoner and a response generator. The emotion reasoner is used to predict a context emotion label and locate words related to the emotion cause, based on the dialogue context. The response generator is responsible for incorporating the information obtained from the emoti"
2021.findings-emnlp.70,2020.acl-demos.30,0,0.19618,"n be summarized as follows: in empathetic response generation. • To incorporate emotion cause into response generation, we devise a gated attention mechanism and explore both hard and soft gating strategies, which allow the model to focus on emotion cause related words. • Experimental results show that our proposed models benefit from the emotion cause and significantly outperform other compared methods, resulting in more empathetic responses. 2 Related Work In recent years, neural approaches to open-domain dialogue systems have achieved great progress (Serban et al., 2016; Wolf et al., 2019; Zhang et al., 2020b; Zhou et al., 2020; Xu et al., 2020; Wang et al., 2021). Especially, incorporating personality and emotional features can make dialogue systems more human-like. For emotion-aware response generation, it aims at generating responses corresponding to specific emotions. Several methods are proposed to tackle this task (Zhou et al., 2018; Huang et al., 2018; Colombo et al., 2019; Song et al., 2019; Shen and Feng, 2020; Xu et al., 2021; Majumder et al., 2021). Empathetic response generation is a sub-task of emotion-aware response generation, Rashkin et al. (2019) first proposes a standard benchma"
2021.naacl-industry.18,2020.lrec-1.467,0,0.01609,"hion by fine-tuning the model in one region, and then continue training in another. The actual sequence of how this is conducted is important. We observed that keeping English at the last stage provides the best performance. This is likely because English data (which frequently contains bilingual data through code-switching) covers a large proportion in pre-training corpora, thus serving as an anchor in subsequent training stage to maintain the universal properties of the model. 3.2 Multi-task Learning A total of 79M translation pairs from WikiMatrix (Schwenk et al., 2019) and MultiParaCrawl (Aulamo et al., 2020) data including the languages considered in production are extracted as training data. In addition, we conduct an ablation study on auxiliary task selection by comparing with Masked Language Model (MLM) (Devlin et al., 2018) trained on 370M samples from Wikipedia. The multi-task training alternates between SR and auxiliary tasks according to a set proportion of mini-batches in an epoch. The proportion controls the trade-offs between the tasks, to achieve the desired levels of performance in the system. 3.3 Data Augmentation Native supervised data (m-r pairs) is currently not available for low-"
2021.naacl-industry.18,2021.naacl-main.280,1,0.815328,"Missing"
2021.naacl-industry.18,P19-4007,0,0.137716,"of responses to bias the predictions towards more common ones. Translated responses inherit the penalty score from the corresponding English responses. Using this score in equation 2 we first select top N1 responses, and down-select to top N2 after deduplication using lexical clustering, before presenting to users. Score = φ(mi ) · φK (rk )) + αLMK (rk ) 3 (2) Universal SR Model The universal SR model consists of parallel encoder architecture trained using symmetric loss function 139 similar to the core SR model. We initialize the m-r encoders with InfoXLM (Chi et al., 2020), an XLM-Roberta (Conneau et al., 2019) equivalent multi-lingual model as shown in as Figure 1(a) which creates language-agnostic text representation across 100 languages. The encoder is pre-trained with both publicly available and internal proprietary corpora and has shown good cross-lingual transfer capabilities on benchmarks such as XNLI (Conneau et al., 2018). Using a universal pre-trained model in itself enables language expansion. However, as we discuss next, data movement constraints made training the universal model tricky, with performance frequently worse than single mono-lingual models. 3.1 Continual Learning Joint train"
2021.naacl-industry.18,D18-1269,0,0.0229146,"φ(mi ) · φK (rk )) + αLMK (rk ) 3 (2) Universal SR Model The universal SR model consists of parallel encoder architecture trained using symmetric loss function 139 similar to the core SR model. We initialize the m-r encoders with InfoXLM (Chi et al., 2020), an XLM-Roberta (Conneau et al., 2019) equivalent multi-lingual model as shown in as Figure 1(a) which creates language-agnostic text representation across 100 languages. The encoder is pre-trained with both publicly available and internal proprietary corpora and has shown good cross-lingual transfer capabilities on benchmarks such as XNLI (Conneau et al., 2018). Using a universal pre-trained model in itself enables language expansion. However, as we discuss next, data movement constraints made training the universal model tricky, with performance frequently worse than single mono-lingual models. 3.1 Continual Learning Joint training of universal encoders has led to enormous progress on standard benchmarks and industrial applications such as (Ranasinghe and Zampieri, 2020; Gencoglu, 2020). However, privacy policies restrict the data movement across geographic clusters. This prevents the joint training at a single compute cluster. As a result, we trai"
2021.naacl-industry.18,N19-2006,1,0.939083,"quickly respond with a short, generic, To reduce the cost of model management, we and relevant response, without users having to type propose to build a single universal SR model, cain the reply. SR is an increasingly popular feature pable of serving multiple languages and markets. in many commercial applications such as Gmail, To overcome data constraints, we propose to use Outlook, Skype, Facebook Messenger, Microsoft augmentation with machine-translated (MT) data Teams, and Uber (Kannan et al., 2016; Henderson for languages without supervised data. To overet al., 2017a; Shang et al., 2015; Deb et al., 2019; come privacy constraints, we propose a continual Yue Weng, 2019). While the initial versions of learning framework, where the model is trained sethis feature mostly targeted English users, making quentially across regions. To alleviate catastrophic it available in multiple languages and markets is forgetting (French, 1999; McCloskey and Cohen, important not only from the perspective of prod- 1989) in the continual learning process, we reinuct expansion but also from a linguistic inclusivity force the universal properties via multi-task learnpoint of view. ing approach with public task-agnost"
2021.naacl-industry.18,P19-1536,0,0.0543399,"Missing"
2021.naacl-industry.18,P15-1152,0,0.0506031,"Missing"
2021.naacl-industry.18,2020.cl-1.2,0,0.0307627,"Missing"
2021.naacl-industry.18,2021.ccl-1.108,0,0.0388259,"Missing"
2021.naacl-industry.18,W19-4302,0,0.0202868,"e jointly train the (Lample and Conneau, 2019) in continual learning model in EUR for FR, DE, and IT. Next, we move to preserve the universal properties of the model. the model to NAM and continue train with EN, ES, 140 and PT along with auxiliary task. Finally, in LRL, we train the model on machine translated m-r pairs along with original EN data in 2 different ways: (1) jointly train with auxiliary task, or (2) infuse the model with low-resource language adapters. In all stages, we freeze the embedding layer of the encoder during fine-tuning. According to previous studies (Lee et al., 2019; Peters et al., 2019), freezing partial layers can maintain the model quality while reducing training time during fine-tuning. We observed that freezing embedding layer provides a good balance between micro-batch size per GPU (low if no layers are frozen) and learning capacity of the model (low if many layers are frozen). 3.5 Universal Model Graph for Serving For deployment, we create a composite graph with pre-computed response vectors of all languages embedded into the main model. A separate language identifier switches the prediction vectors to the predicted language of the input at run-time. Besides, several a"
2021.naacl-industry.18,2020.emnlp-main.470,0,0.0164283,"ss 100 languages. The encoder is pre-trained with both publicly available and internal proprietary corpora and has shown good cross-lingual transfer capabilities on benchmarks such as XNLI (Conneau et al., 2018). Using a universal pre-trained model in itself enables language expansion. However, as we discuss next, data movement constraints made training the universal model tricky, with performance frequently worse than single mono-lingual models. 3.1 Continual Learning Joint training of universal encoders has led to enormous progress on standard benchmarks and industrial applications such as (Ranasinghe and Zampieri, 2020; Gencoglu, 2020). However, privacy policies restrict the data movement across geographic clusters. This prevents the joint training at a single compute cluster. As a result, we train the model sequentially in a continual learning fashion by fine-tuning the model in one region, and then continue training in another. The actual sequence of how this is conducted is important. We observed that keeping English at the last stage provides the best performance. This is likely because English data (which frequently contains bilingual data through code-switching) covers a large proportion in pre-traini"
2021.naacl-main.454,N13-1006,0,0.0421747,"Missing"
2021.naacl-main.454,Q16-1026,0,0.0377179,"Missing"
2021.naacl-main.454,P18-1031,0,0.161158,"ataset MSR (Levow, 2006) for unlabeled data. Weibo (Peng and Dredze, 2015) is a Chinese NER dataset derived from social media contents. Only a small fraction of data is labeled in this dataset. A dropout layer is concatenated to the BERT, followed by a fully connected layer that computes the logit(x) for the labels at each location. We further apply an additional CRF-layer as defined in (2) to account for the temporal dependencies among the labels, similar to the work of Meng et al. (2019). The teacher model is trained using the standardized fine-tune paradigm (Devlin et al., 2019). Following Howard and Ruder (2018); Sun et al. (2019a), we set different learning rates for each layer. A larger rate is used for CRF, and for the BERT the learning rates decays by a factor of 0.9 as the layers approaches the input. Student Model. For our student model, we want it to be light, fast yet still sufficiently expressive. To this end, we use the BiLSTM+CRF architecture proposed in Huang et al. (2015). This model exploits a Bidirectional LSTM to map input sequence x into a sequence of feature vectors, which accounts for the context from both directions. The rest of the construction follows what has been described for"
2021.naacl-main.454,W05-1506,0,0.167377,"ntial scaling, Kim and Rush (2016) investigated using beam search for teacher output to select k-best candidates for KD in neural machine translation (NMT), and Mun’im et al. (2019) utilized an similar technique for KD in Large Vocabulary Continuous Speech Recognition (LVSCR) tasks. k-best Viterbi Decoding. Viterbi decoding is a dynamic programming technique to find a sequence with the highest score in an exponential-growth domain, with only linear complexity (Viterbi, 1967). Generalization has been proposed to extend its original scope to find the top-K sequences that are most probable, see (Huang and Chiang, 2005; Nielsen, 2011) for details. A summary of the algorithm can be found in the supplementary material. In the context of KD in sequence tasks, a trained teacher model assigns varying probability to all sequence combinations. Our motivation is that the k-best Viterbi can be repurposed to pick out the K-most probable label sequences predicted by the teacher model, which plays an analogous role to the softlabels (Figure 1a), without incurring unmanageable computational overhead. Conditional Random Field. CRF (Lafferty et al., 2001) is a classic and powerful energy-based model that is capable of cap"
2021.naacl-main.454,P12-1064,0,0.0646957,"Missing"
2021.naacl-main.454,D16-1139,0,0.193221,"ded for the construction of our model. Sequence-level Knowledge Distillation. Knowledge distillation (KD) is originally developed for classification tasks (Hinton et al., 2015; Tang et al., 2019). When dealing with sequence outputs (e.g., machine translation, sequence labeling), where each unique combination of the output sequence is treated as different category, then the standard distillation objective is no longer appropriate, if feasible at all. This is because the number of unique combinations for a length L sequence with T possible tags scale at T L . To combat such exponential scaling, Kim and Rush (2016) investigated using beam search for teacher output to select k-best candidates for KD in neural machine translation (NMT), and Mun’im et al. (2019) utilized an similar technique for KD in Large Vocabulary Continuous Speech Recognition (LVSCR) tasks. k-best Viterbi Decoding. Viterbi decoding is a dynamic programming technique to find a sequence with the highest score in an exponential-growth domain, with only linear complexity (Viterbi, 1967). Generalization has been proposed to extend its original scope to find the top-K sequences that are most probable, see (Huang and Chiang, 2005; Nielsen, 2"
2021.naacl-main.454,W06-0115,0,0.054932,"from Reuters RCV corpus. Unlabeled data from the Reuters RCV corpus is used for our data augmentation experiments. OntoNotes (Consortium et al., 2011) is an annotated multilingual corpus consists of texts from a wide variety of sources, such as telephone conversation, broadcast and newswire. For our NER experiment, we consider • English NER is derived from OntoNotes Release 5.0 and processed according to Pradhan et al. (2013). • Chinese NER is derived from OntoNotes Release 4.0 and processed according to Che et al. (2013). Text data from other OntoNotes tasks are used as unlabeled data. MSRA (Levow, 2006) is a Chinese NER dataset with its corpus derived from news domain. We use the Chinese word segmentation dataset MSR (Levow, 2006) for unlabeled data. Weibo (Peng and Dredze, 2015) is a Chinese NER dataset derived from social media contents. Only a small fraction of data is labeled in this dataset. A dropout layer is concatenated to the BERT, followed by a fully connected layer that computes the logit(x) for the labels at each location. We further apply an additional CRF-layer as defined in (2) to account for the temporal dependencies among the labels, similar to the work of Meng et al. (2019)"
C02-1010,P00-1050,1,0.887476,"Missing"
C02-1010,C00-2131,0,0.0314432,"Missing"
C02-1010,C92-2101,0,\N,Missing
C02-1010,W00-0726,0,\N,Missing
C02-1010,P93-1004,0,\N,Missing
C02-1010,C96-1078,0,\N,Missing
C02-1010,P00-1015,1,\N,Missing
C02-1010,J97-3002,0,\N,Missing
C02-1010,P98-2221,0,\N,Missing
C02-1010,C98-2216,0,\N,Missing
D07-1078,A00-2018,0,0.0483426,"t collapses the two NNP’s, so as to generalize this rule, getting rule R 5 and rule R6 . We also need to consistently syntactify the root labels of R4 and the new frontier label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R 6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 746–754, Pra"
D07-1078,J07-2003,0,0.115537,"ization methods in terms of BLEU on Chinese-to-English translation tasks. 7.1 Experimental setup Our bitext consists of 16M words, all in the mainland-news domain. Our development set is a 925-line subset of the 993-line NIST02 evaluation set. We removed long sentences from the NIST02 evaluation set to speed up discriminative training. The test set is the full 919-line NIST03 evaluation set. We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al., 2006). The decoder prunes hypotheses using strategies described in (Chiang, 2007). The parse trees on the English side of the bitexts were generated using a parser (Soricut, 2004) implementing the Collins parsing models (Collins, 1997). We used the EM procedure described in (Knight and Graehl, 2004) to perform the inside-outside algorithm on synchronous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes 752 Table 1 shows the BLEU scores of mixed-cased and detokenized translations of different systems. We"
D07-1078,P97-1003,0,0.0992221,"w tree node that collapses the two NNP’s, so as to generalize this rule, getting rule R 5 and rule R6 . We also need to consistently syntactify the root labels of R4 and the new frontier label of R6 such that these two rules can be composed. Since labeling is not a concern of this paper, we simply label new nodes with X-bar where X here is the parent label. With all these in place, we now can translate the foreign sentence by composing R 6 and R4 in Figure 1. Binarizing the syntax trees for syntax-based machine translation is similar in spirit to generalizing parsing models via markovization (Collins, 1997; Charniak, 2000). But in translation modeling, it is unclear how to effectively markovize the translation rules, especially when the rules are complex like those proposed by Galley et al. (2006). In this paper, we explore the generalization ability of simple binarization methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and 746 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning,"
D07-1078,P03-2041,0,0.134446,"CA, 90292 {wwang,kknight,dmarcu}@languageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translate"
D07-1078,N04-1035,1,0.126799,"mployed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars built from restructured (binarized) training trees still contain non-binary, multi-level rules and thus still require the binarization transformation so as to be employed by a CKY parser. The translation model we are using in this paper belongs to the xRs formalism (Knight and Graehl, 2004), which has been proved successful for machine translation in (Galley et al., 2004; Galley et al., 2006; Marcu et al., 2006). 3 Concepts We focus on tree-to-string (in noisy-channel model sense) translation models. Translation models of this type are typically trained on tuples of a sourcelanguage sentence f, a target language (e.g., English) parse tree π that yields e and translates from f, and the word alignments a between e and f. Such a tuple is called an alignment graph in (Galley et al., 2004). The graph (1) in Figure 2 is such an alignment graph. (1) unbinarized tree NPB NNP1 NNP2 NNP3 NNP4* viktor chernomyrdin VIKTOR−CHERNOMYRDIN (2) left-binarization (3) right-/hea"
D07-1078,P06-1121,1,0.147896,"ng,kknight,dmarcu}@languageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase"
D07-1078,J99-4004,0,0.10699,"g all the way to the left, for example, from tree (1) to tree (2) and to tree (4) in Figure 2, does not enable us to acquire a substructure that yields NNP3 NNP4 and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 4.2 Parallel binarization Simple binarizations transform a parse tree into another single parse tree. Parallel binarization will transform a parse tree into a binarization forest, desirably packed to enable dynamic programming when extracting translation rules from it. Borrowing terms from parsing semirings (Goodman, 1999), a packed forest is composed of additive forest nodes (⊕-nodes) and multiplicative forest nodes (⊗-nodes). In the binarization forest, a ⊗node corresponds to a tree node in the unbinarized tree; and this ⊗-node composes several ⊕-nodes, forming a one-level substructure that is observed in the unbinarized tree. A ⊕-node corresponds to alternative ways of binarizing the same tree node in the unbinarized tree and it contains one or more ⊗nodes. The same ⊕-node can appear in more than one place in the packed forest, enabling sharing. Figure 3 shows a packed forest obtained by packing trees (4) an"
D07-1078,N04-1014,1,0.65413,"1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR-CHERNOMYRDIN into an English NPB tree fragment yielding an English phrase. Also suppose that we want to translate a Chinese phrase VIKTOR-CHERNOMYRDIN AND HIS COLLEAGUE into English. What we desire is that if we have another rule R2 as shown in Figure 1, we could somehow compose it with R1 to obtain the desirable translation. We unfortunately cannot do this because R1 and R2 are not further decomposable and their substructures cannot be re-used. The requirement that all translation rules have exactly one root node"
D07-1078,W06-1606,1,0.615337,"nguageweaver.com Abstract We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTO"
D07-1078,J93-2004,0,0.0333178,"style parses are not optimal for syntaxbased machine translation. We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation. We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result. 1 Introduction Syntax-based translation models (Eisner, 2003; Galley et al., 2006; Marcu et al., 2006) are usually built directly from Penn Treebank (PTB) (Marcus et al., 1993) style parse trees by composing treebank grammar rules. As a result, often no substructures corresponding to partial PTB constituents are extracted to form translation rules. Syntax translation models acquired by composing treebank grammar rules assume that long rewrites are not decomposable into smaller steps. This effectively restricts the generalization power of the induced model. For example, suppose we have an xRs (Knight and Graehl, 2004) rule R 1 in Figure 1 that translates the Chinese phrase RUSSIA MINISTER VIKTOR-CHERNOMYRDIN into an English NPB tree fragment yielding an English phras"
D07-1078,P04-1084,0,0.0211306,"the binarization bias for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same lan747 guage and assign the same probability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, sy"
D07-1078,W05-0908,0,0.0127138,"nous derivation forests and to generate the Viterbi derivation forest. We used the rule extractor described in (Galley et al., 2006) to extract rules from (e-parse, f, a)-tuples, but we made an important modification: new nodes 752 Table 1 shows the BLEU scores of mixed-cased and detokenized translations of different systems. We see that all the binarization methods improve the baseline system that does not apply any binarization algorithm. The EM-binarization performs the best among all the restructuring methods, leading to 1.0 BLEU point improvement. We also computed the bootstrap p-values (Riezler and Maxwell, 2005) for the pairwise BLEU comparison between the baseline system and any of the system trained from binarized trees. The significance test shows that the EM binarization result is statistically significant better than the baseline system (p &gt; 0.005), even though the baseline is already quite strong. To our best knowledge, 37.94 is the highest BLEU score on this test set to date. Also as shown in Table 1, the grammars trained from the binarized training trees are almost two times of the grammar size with no binarization. The extra rules are substructures factored out by these binarization methods."
D07-1078,N06-1033,1,0.539983,"for each tree node from the parallel alternatives. The EM-binarization yields best translation performance. The rest of the paper is organized as follows. Section 2 describes related research. Section 3 defines the concepts necessary for describing the binarizations methods. Section 4 describes the tree binarization methods in details. Section 5 describes the forest-based rule extraction algorithm, and section 6 explains how we restructure the trees using the EM algorithm. The last two sections are for experiments and conclusions. 2 Related Research Several researchers (Melamed et al., 2004; Zhang et al., 2006) have already proposed methods for binarizing synchronous grammars in the context of machine translation. Grammar binarization usually maintains an equivalence to the original grammar such that binarized grammars generate the same lan747 guage and assign the same probability to each string as the original grammar does. Grammar binarization is often employed to make the grammar fit in a CKY parser. In our work, we are focused on binarization of parse trees. Tree binarization generalizes the resulting grammar and changes its probability distribution. In tree binarization, synchronous grammars bu"
D07-1078,J02-1005,0,\N,Missing
D07-1078,J08-3004,1,\N,Missing
D07-1079,J93-2003,0,0.0123288,"ased machine translation model on several levels. We briefly describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and K"
D07-1079,P05-1033,0,0.253777,"on of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great nu"
D07-1079,J03-4003,0,0.00486268,"t linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables — there are no source words in them. Because of this, they potentially apply to any sentence. Lexical rules (their counterpart) far outnumb"
D07-1079,W06-1628,0,0.194461,"Missing"
D07-1079,P03-2041,0,0.113635,"accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a sci"
D07-1079,P06-1097,1,0.521845,"s |fwords ), and p(fwords |ewords ). 4 Differences in Phrasal Coverage Both the ATS model and the GHKM model extract linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables — there are no source words in th"
D07-1079,N04-1035,1,0.716228,"titution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phrase-based and syntax-based extraction methods and phrase pair coverage. We also add"
D07-1079,P06-1121,1,0.651314,"s/mt/ mt06eval official results.html 756 Phrase pairs are extracted over the entire training corpus. Due to differing alignments, some phrase pairs that cannot be learned from one example may be learned from another. These pairs are then counted, once for each time they are seen in a training example, and these counts are used as the basis for maximum likelihood probability features, such as p(f |e) and p(e|f ). 3 Syntax-based Extraction The GHKM syntax-based extraction method for learning statistical syntax-based translation rules, presented first in (Galley et al., 2004) and expanded on in (Galley et al., 2006), is similar to phrase-based extraction in that it extracts rules consistent with given word alignments. A primary difference is the use of syntax trees on the target side, rather than sequences of words. The basic unit of translation is the translation rule, consisting of a sequence of words and variables in the source language, a syntax tree in the target language having words or variables at the leaves, and again a vector of feature values which describe this pair’s likelihood. Translation rules can: • look like phrase pairs with syntax decoration: NPB(NNP(prime) NNP(minister) ↔ NNP(keizo)"
D07-1079,P03-1011,0,0.0154978,"he translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is impor"
D07-1079,2006.amta-papers.8,1,0.137531,"on. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phr"
D07-1079,N03-1017,1,0.0988731,"We briefly describe each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear"
D07-1079,W06-1606,1,0.648295,"them. Since this is a clear deficiency, we will focus on analyzing these phrase pairs (which we call ATS-useful) and the reasons they were not learned. Table 4 shows a breakdown, categorizing each of these missing ATS-useful phrase pairs and the reasons they were not able to be learned. The most common reason is straightforward: by extracting only the minimally-sized rules, GHKM is unable to learn many larger phrases that ATS learns. If GHKM can make a word-level analysis, it will do that, at the expense of a phrase-level analysis. Galley et al. (2006) propose one solution to this problem and Marcu et al. (2006) propose another, both of which we explore in Sections 5.1 and 5.2. context is too constrained. For example, ATS can easily learn the phrase ® ↔ prime minister and is then free to use it in many contexts. But GHKM learns 45 different rules, each that translate this phrase pair in a unique context. Figure 6 shows a sampling. Notice that though many variations are present, the decoder is unable to use any of these rules to produce certain noun phrases, such as “current Japanese Prime Minister Shinzo Abe”, because no rule has the proper number of English modifiers. NPB(NNP(prime) NNP(minister) x"
D07-1079,P04-1083,0,0.041989,"all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific underst"
D07-1079,J03-1002,0,0.00571114,"e , fwords |ehead ), p(ewords |fwords ), and p(fwords |ewords ). 4 Differences in Phrasal Coverage Both the ATS model and the GHKM model extract linguistic knowledge from parallel corpora, but each has fundamentally different constraints and assumptions. To compare the models empirically, we extracted phrase pairs (for the ATS model) and translation rules (for the GHKM model) from parallel training corpora described in Table 1. The ATS model was limited to phrases of length 10 on the source side, and length 20 on the target side. A superset of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). The English side of training data was parsed using an implementation of Collins’ model 2 (Collins, 2003). Document IDs # of segments # of words in foreign corpus # of words in English corpus Chinese LDC2003E07 LDC2003E14 LDC2005T06 329,031 7,520,779 9,864,294 Arabic LDC2004T17 LDC2004T18 LDC2005E46 140,511 3,147,420 4,067,454 Table 1: parallel corpora used to train both models Table 2 shows the total number of GHKM rules extracted, and a breakdown of the different kinds of rules. Non-lexical rules are those whose source side is composed entirely of variables"
D07-1079,J04-4002,0,0.085683,"each model, highlighting points where they differ. We include a quantitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these impor"
D07-1079,P03-1021,0,0.0125312,"rees, so that wide constituents are broken down into multiple levels of tree structure. The approach we take here is head-out binarization (Wang et al., 2007), where any constituent with more than two children is split into partial constituents. The children to the left of the head word Category of ATS-useful phrase pairs Too large Extra target words in GHKM rules Extra source words in GHKM rules Other (e.g. parse failures) Total missing useful phrase pairs Chinese 12 218 424 9 663 Arabic 9 27 792 7 835 with four references for measuring BLEU. Tuning was done using Maximum BLEU hill-climbing (Och, 2003). Features used for the ATS system were the standard set. For the syntax-based translation system, we used a similar set of features. Table 8: reasons that ATS-useful phrase pairs are still not extracted as phrasal rules, with composed and SPMT model 1 rules in place Development set Test set are binarized one direction, while the children to the right are binarized the other direction. The top node retains its original label (e.g. NPB), while the new partial constituents are labeled with a bar (e.g. NPB). Figure 7 shows an example. Figure 7: head-out binarization in the target language: S, NPB"
D07-1079,P05-1034,0,0.0730264,"al machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution i"
D07-1079,D07-1078,1,0.313261,"ot the smallest rule that can explain the phrase pair, but it is still valuable for its syntactic context. 5.3 Restructuring Trees Table 8 updates the causes of missing ATS-useful phrase pairs. Most are now caused by syntactic constraints, thus we need to address these in some way. GHKM translation rules are affected by large, flat constituents in syntax trees, as in the prime minister example earlier. One way to soften this constraint is to binarize the trees, so that wide constituents are broken down into multiple levels of tree structure. The approach we take here is head-out binarization (Wang et al., 2007), where any constituent with more than two children is split into partial constituents. The children to the left of the head word Category of ATS-useful phrase pairs Too large Extra target words in GHKM rules Extra source words in GHKM rules Other (e.g. parse failures) Total missing useful phrase pairs Chinese 12 218 424 9 663 Arabic 9 27 792 7 835 with four references for measuring BLEU. Tuning was done using Maximum BLEU hill-climbing (Och, 2003). Features used for the ATS system were the standard set. For the syntax-based translation system, we used a similar set of features. Table 8: reaso"
D07-1079,P98-2230,0,0.0514584,"antitative comparison of the phrase pairs that each model has to work with, as well as the reasons why some phrase pairs are not learned by the syntax-based model. We then evaluate proposed improvements to the syntax-based extraction techniques in light of phrase pairs captured. We also compare the translation accuracy for all variations. 1 Introduction String models are popular in statistical machine translation. Approaches include word substitution systems (Brown et al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each h"
D07-1079,P01-1067,1,0.221445,"t al., 1993), phrase substitution systems (Koehn et al., 2003; Och and Ney, 2004), and synchronous context-free grammar systems (Wu and Wong, 1998; Chiang, 2005), all of which train on string pairs and seek to establish connections between source and target strings. By contrast, explicit syntax approaches seek to directly model the relations learned from parsed data, including models between source trees and target trees (Gildea, 2003; Eisner, 2003; Melamed, 2004; Cowan et al., 2006), source trees and target strings (Quirk et al., 2005; Huang et al., 2006), or source strings and target trees (Yamada and Knight, 2001; Galley et al., 2004). It is unclear which of these important pursuits will best explain human translation data, as each has adA great number of MT models have been recently proposed, and other papers have gone over the expressive advantages of syntax-based approaches. But it is rare to see an in-depth, quantitative study of strengths and weaknesses of particular models with respect to each other. This is important for a scientific understanding of how these models work in practice. Our main novel contribution is a comparison of phrase-based and syntax-based extraction methods and phrase pair"
D07-1079,N06-1033,1,0.126112,"-based extraction in that it extracts rules consistent with given word alignments. A primary difference is the use of syntax trees on the target side, rather than sequences of words. The basic unit of translation is the translation rule, consisting of a sequence of words and variables in the source language, a syntax tree in the target language having words or variables at the leaves, and again a vector of feature values which describe this pair’s likelihood. Translation rules can: • look like phrase pairs with syntax decoration: NPB(NNP(prime) NNP(minister) ↔ NNP(keizo) BÁÈ® D# NNP(obuchi)) (Zhang et al., 2006). During decoding, features from each translation rule are combined with a language model using a log-linear model to compute the score of the entire translation. The GHKM extractor learns translation rules from an aligned parallel corpus where the target side has been parsed. This corpus is conceptually a list of tuples of <source sentence, target tree, bi-directional word alignments&gt; which serve as training examples, one of which is shown in Figure 3. • carry extra contextual constraints: VP(VBD(said) x0 :SBAR-C) ↔ x 0  can translate to (according to this rule, said only if some Chinese se"
D07-1079,C98-2225,0,\N,Missing
D13-1092,W09-2206,0,0.0298505,"tes f . For a feature fi , the top five words and fi constitute must-links. For example, the co-occurrence of ”price” and ”cheap” is very high, then the must-link between ”price” and ”cheap” can be identified. Cannot-link: It specifies that two data instances cannot be in the same cluster. If a word and a feature never co-occur in our corpus, we assume them to form a cannot-link. For example, the word lowcost has never co-occurred with the product feature screen, so they constitute a cannot-link in our corpus. In this paper, the pre-defined process, must-link, and cannot-link are derived from Andrzejewski and Zhu (2009)’s work, all must-links and cannot-links are incorporated our constrained topic model. We multiply an indicator function δ(wi , zj ), which represents a hard constraint, to the Equation 1 as the final probability for topic updating (see Equation 4). P (zi = j|z−i , w, α, β) = i n−i,j +β W w′ (w′ ) n−i,j + W β )( ∑ i n−i,j +α T j (d ) i n−i,j + Tα ) (4) As illustrated by Equations 1 and 4, δ(wi , zj ), which represents intervention or help from preexisting knowledge of must-links and cannot-links, plays a key role in this study. In the topic updating for each word in each document, we assume th"
D18-1521,Q18-1034,0,0.0144473,"t GN-GloVe effectively isolates the protected attributes and preserves the word proximity. 2 Related Work Word Embeddings Word embeddings serve as a fundamental building block for a broad range of NLP applications (dos Santos and Gatti, 2014; Bahdanau et al., 2014; Zeng et al., 2015) and various approaches (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) have been proposed for training the word vectors. Improvements have been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been observed in applications such as online advertising systems (Sweeney, 2013), web search (Kay et al., 2015), and online reviews (Wallace and Paul, 2016). Besides, Zhao et al. (2017) and Rudinger et al. (2018) show that coreference resolution systems are gender biased. The systems can successfully predict the link between “the president"
D18-1521,D15-1075,0,0.0310934,"ne of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been observed in applications such as online advertising systems (Sweeney, 2013), web search (Kay et al., 2015), and online reviews (Wallace and Paul, 2016). Besides, Zhao et al. (2017) and Rudinger et al. (2018) show that coreference resolution systems are gender biased. The systems can successfully predict the link between “the president” with male pronoun but fail with the female one. Rudinger et al. (2017) use pointwise mutual information to test the SNLI (Bowman et al., 2015) corpus and demonstrate gender stereotypes as well as varying degrees of racial, religious, and age-based stereotypes in the corpus. A temporal analysis about word embeddings (Garg et al., 2018) captures changes in gender and ethnic stereotypes over time. Researchers attributed such problem partly to the biases in the datasets (Zhao et al., 2017; Yao and Huang, 2017) and word embeddings (Garg et al., 2017; Caliskan et al., 2017) but did not provide constructive solutions. 3 Methodology In this paper, we take GloVe (Pennington et al., 2014) as the base embedding model and gender as the protecte"
D18-1521,P12-1015,0,0.0436162,"Missing"
D18-1521,S12-1047,0,0.148259,"Missing"
D18-1521,D17-1018,0,0.0752264,"Missing"
D18-1521,Q15-1016,0,0.0431465,"to learn word embeddings with protected attributes; 2) By capturing protected attributes in certain dimensions, our approach ameliorates the interpretability of word representations; 3) Qualitative and quantitative experiments demonstrate that GN-GloVe effectively isolates the protected attributes and preserves the word proximity. 2 Related Work Word Embeddings Word embeddings serve as a fundamental building block for a broad range of NLP applications (dos Santos and Gatti, 2014; Bahdanau et al., 2014; Zeng et al., 2015) and various approaches (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) have been proposed for training the word vectors. Improvements have been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been observed in applications such as online advertising systems (Sweeney, 2013), web search (Kay"
D18-1521,W13-3512,0,0.425626,"erpretability of word representations; 3) Qualitative and quantitative experiments demonstrate that GN-GloVe effectively isolates the protected attributes and preserves the word proximity. 2 Related Work Word Embeddings Word embeddings serve as a fundamental building block for a broad range of NLP applications (dos Santos and Gatti, 2014; Bahdanau et al., 2014; Zeng et al., 2015) and various approaches (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) have been proposed for training the word vectors. Improvements have been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been observed in applications such as online advertising systems (Sweeney, 2013), web search (Kay et al., 2015), and online reviews (Wallace and Paul, 2016). Besides, Zhao et al. (2017) and Rudinger et al. (2018) show that coreference resoluti"
D18-1521,N13-1090,0,0.599911,"our best knowledge, GN-GloVe is the first method to learn word embeddings with protected attributes; 2) By capturing protected attributes in certain dimensions, our approach ameliorates the interpretability of word representations; 3) Qualitative and quantitative experiments demonstrate that GN-GloVe effectively isolates the protected attributes and preserves the word proximity. 2 Related Work Word Embeddings Word embeddings serve as a fundamental building block for a broad range of NLP applications (dos Santos and Gatti, 2014; Bahdanau et al., 2014; Zeng et al., 2015) and various approaches (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) have been proposed for training the word vectors. Improvements have been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been observed in applications such as online adverti"
D18-1521,D14-1162,0,0.124157,"oach and requires the gender-neutral words to be identified by a classifier before employing the projection. If the classifier makes a mistake, the error will be propagated and affect the performance of the model. Second, their method completely removes gender information from those words which are essential in some domains such as medicine and social science (Back et al., 2010; McFadden et al., 1992). To overcome these limitations, we propose a learning scheme, Gender-Neutral Global Vectors (GN-GloVe) for training word embedding models with protected attributes (e.g., gender) based on GloVe (Pennington et al., 2014).2 GN-GloVe represents protected attributes in certain dimen1 Gender-definition words are the words associated with gender by definition (e,g., mother, waitress); the remainder are gender-neutral words. 2 The code and data are released at https://github. com/uclanlp/gn_glove 4847 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847–4853 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics sions while neutralizing the others during training. As the information of the protected attribute is restricted in"
D18-1521,N18-1202,0,0.0525606,"d Embeddings Word embeddings serve as a fundamental building block for a broad range of NLP applications (dos Santos and Gatti, 2014; Bahdanau et al., 2014; Zeng et al., 2015) and various approaches (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) have been proposed for training the word vectors. Improvements have been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been observed in applications such as online advertising systems (Sweeney, 2013), web search (Kay et al., 2015), and online reviews (Wallace and Paul, 2016). Besides, Zhao et al. (2017) and Rudinger et al. (2018) show that coreference resolution systems are gender biased. The systems can successfully predict the link between “the president” with male pronoun but fail with the female one. Rudinger et al. (2017) use pointwise mutual information to te"
D18-1521,D15-1203,0,0.0167657,"ributions are summarized as follows: 1) To our best knowledge, GN-GloVe is the first method to learn word embeddings with protected attributes; 2) By capturing protected attributes in certain dimensions, our approach ameliorates the interpretability of word representations; 3) Qualitative and quantitative experiments demonstrate that GN-GloVe effectively isolates the protected attributes and preserves the word proximity. 2 Related Work Word Embeddings Word embeddings serve as a fundamental building block for a broad range of NLP applications (dos Santos and Gatti, 2014; Bahdanau et al., 2014; Zeng et al., 2015) and various approaches (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) have been proposed for training the word vectors. Improvements have been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been ob"
D18-1521,D17-1323,1,0.693553,"been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been observed in applications such as online advertising systems (Sweeney, 2013), web search (Kay et al., 2015), and online reviews (Wallace and Paul, 2016). Besides, Zhao et al. (2017) and Rudinger et al. (2018) show that coreference resolution systems are gender biased. The systems can successfully predict the link between “the president” with male pronoun but fail with the female one. Rudinger et al. (2017) use pointwise mutual information to test the SNLI (Bowman et al., 2015) corpus and demonstrate gender stereotypes as well as varying degrees of racial, religious, and age-based stereotypes in the corpus. A temporal analysis about word embeddings (Garg et al., 2018) captures changes in gender and ethnic stereotypes over time. Researchers attributed such problem partly t"
D18-1521,N18-2003,1,0.783428,"for representing the meaning of words in a vector space. These models have become a fundamental NLP technique and have been widely used in various applications. However, prior studies show that such models learned from humangenerated corpora are often prone to exhibit social biases, such as gender stereotypes (Bolukbasi et al., 2016; Caliskan et al., 2017). For example, the word “programmer” is neutral to gender by its definition, but an embedding model trained on a news corpus associates “programmer” closer with “male” than “female”. Such a bias substantially affects downstream applications. Zhao et al. (2018) show that a coreference resolution system is sexist due to the word embedding component used in the system. This concerns the practitioners who use the embedding model to build gender-sensitive applications such as a resume filtering system or a job recommendation system as the automated system may discriminate candidates based on their gender, as reflected by their name. Besides, biased embeddings may implicitly affect downstream applications used in our daily lives. For example, when searching for “computer scientist” using a search engine, as this phrase is closer to male names than female"
D18-1521,W17-1609,0,0.106412,"Missing"
D18-1521,N18-2002,0,0.200547,"Missing"
D18-1521,C14-1008,0,0.0108002,"n reducing other societal stereotypes. Our contributions are summarized as follows: 1) To our best knowledge, GN-GloVe is the first method to learn word embeddings with protected attributes; 2) By capturing protected attributes in certain dimensions, our approach ameliorates the interpretability of word representations; 3) Qualitative and quantitative experiments demonstrate that GN-GloVe effectively isolates the protected attributes and preserves the word proximity. 2 Related Work Word Embeddings Word embeddings serve as a fundamental building block for a broad range of NLP applications (dos Santos and Gatti, 2014; Bahdanau et al., 2014; Zeng et al., 2015) and various approaches (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) have been proposed for training the word vectors. Improvements have been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype"
D18-1521,N16-1160,0,0.0487368,"Missing"
D18-1521,W17-2613,1,0.833701,"ly isolates the protected attributes and preserves the word proximity. 2 Related Work Word Embeddings Word embeddings serve as a fundamental building block for a broad range of NLP applications (dos Santos and Gatti, 2014; Bahdanau et al., 2014; Zeng et al., 2015) and various approaches (Mikolov et al., 2013b; Pennington et al., 2014; Levy et al., 2015) have been proposed for training the word vectors. Improvements have been made by leveraging semantic lexicons and morphology (Luong et al., 2013; Faruqui et al., 2014), disambiguating mulˇ tiple senses (Suster et al., 2016; Arora et al., 2018; Upadhyay et al., 2017), and modeling contextualized information by deep neural networks (Peters et al., 2018). However, none of these works attempts to tackle the problem of stereotypes exhibited in embeddings. Stereotype Analysis Implicit stereotypes have been observed in applications such as online advertising systems (Sweeney, 2013), web search (Kay et al., 2015), and online reviews (Wallace and Paul, 2016). Besides, Zhao et al. (2017) and Rudinger et al. (2018) show that coreference resolution systems are gender biased. The systems can successfully predict the link between “the president” with male pronoun but"
D19-1255,D18-1454,0,0.0490605,"Missing"
D19-1255,D11-1142,0,0.0150424,"rack Obama born in the U.S.?, one must know (among other things) that Hawaii is a state in the U.S., which is external knowledge not present in the text corpus. Therefore, a QA model needs to be enriched with external knowledge properly to be able to answer many nontrivial questions. Such knowledge can be commonsense knowledge or factual background knowledge about entities and events that is not explicitly expressed but can be found in a knowledge base such as ConceptNet (Speer et al., 2016), Freebase (Pellissier Tanon et al., 2016) and domain-specific KBs collected by information extraction (Fader et al., 2011; Mausam et al., 2012). Thus, we aim to design a neural model that encodes pre-selected knowledge relevant to given questions, and that learns to include the available knowledge as an enrichment to given textual information. In this paper, we propose a new neural architecture, Knowledge-Enriched Answer Generator (KEAG), specifically designed to generate natural answers with integration of external knowledge. KEAG is capable of leveraging symbolic knowledge from a knowledge base as it generates each word in an answer. In particular, we assume that each word is generated from one of the four inf"
D19-1255,N18-1017,0,0.0216215,"Zhong et al. (2018) proposed commonsense-based pre-training to improve answer selection. Long et al. (2017) made use of knowledge in the form of entity descriptions to predict missing entities in a given document. There have also been a few studies on incorporating knowledge into QA models without passage reading. GenQA (Yin et al., 2016) combines knowledge retrieval and seq2seq learning to produce fluent answers, but it only deals with simple questions containing one single fact. COREQA (He et al., 2017) extends it with a copy mechanism to learn to copy words from a given question. Moreover, Fu and Feng (2018) introduced a new attention mechanism that attends across the generated history and memory to explicitly avoid repetition, and incorporated knowledge to enrich generated answers. Some work on knowledge-enhanced natural language (NLU) understanding can be adapted to the question answering task. CRWE (Weissenborn, 2017) dynamically integrates background knowledge in a NLU model in the form of free-text statements, and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. In contrast, KBLSTM (Yang and Mitchell, 2017) l"
D19-1255,P17-1019,0,0.017385,"ents on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention. Zhong et al. (2018) proposed commonsense-based pre-training to improve answer selection. Long et al. (2017) made use of knowledge in the form of entity descriptions to predict missing entities in a given document. There have also been a few studies on incorporating knowledge into QA models without passage reading. GenQA (Yin et al., 2016) combines knowledge retrieval and seq2seq learning to produce fluent answers, but it only deals with simple questions containing one single fact. COREQA (He et al., 2017) extends it with a copy mechanism to learn to copy words from a given question. Moreover, Fu and Feng (2018) introduced a new attention mechanism that attends across the generated history and memory to explicitly avoid repetition, and incorporated knowledge to enrich generated answers. Some work on knowledge-enhanced natural language (NLU) understanding can be adapted to the question answering task. CRWE (Weissenborn, 2017) dynamically integrates background knowledge in a NLU model in the form of free-text statements, and yields refined word representations to a task-specific NLU architecture"
D19-1255,W04-1013,0,0.0132576,"been split into a training set (153,725 QA pairs), a dev set (12,467 QA pairs) and a test set (101,092 questions with unpublished answers). Since true answers are not available in the test set, we hold out the dev set for evaluation in our experiments, and test models for each question on its associated passages by concatenating them all together. We tune the hyper-parameters by crossvalidation on the training set. The answers are human-generated and not necessarily sub-spans of the passages, so the official evaluation tool of MARCO uses the metrics BLEU-1 (Papineni et al., 2002) and ROUGEL (Lin, 2004). We use both metrics for our evaluation to measure the quality of generated answers against the ground truth. For external knowledge, we use ConceptNet (Speer et al., 2016), one of the most widely used commonsense knowledge bases. Our KEAG is generic and thus can also be applied to other knowledge bases. ConceptNet is a semantic network representing words and phrases as well as the commonsense relationships between them. After filtering out non-English entities and relation types with few facts, we have 2,823,089 fact triples and 32 relation types for the model to consume. 7.2 Model BiDAF BiD"
D19-1255,D17-1086,0,0.023562,"ctly decodes an answer. Gao et al. (2019) focused on product-aware answer generation based on large-scale unlabeled e-commerce reviews and product attributes. Furthermore, natural answer generation can be reformulated as query-focused summarization which is addressed by Nema et al. (2017). The role of knowledge in certain types of QA tasks has been remarked on. Mihaylov and Frank (2018) showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention. Zhong et al. (2018) proposed commonsense-based pre-training to improve answer selection. Long et al. (2017) made use of knowledge in the form of entity descriptions to predict missing entities in a given document. There have also been a few studies on incorporating knowledge into QA models without passage reading. GenQA (Yin et al., 2016) combines knowledge retrieval and seq2seq learning to produce fluent answers, but it only deals with simple questions containing one single fact. COREQA (He et al., 2017) extends it with a copy mechanism to learn to copy words from a given question. Moreover, Fu and Feng (2018) introduced a new attention mechanism that attends across the generated history and memor"
D19-1255,D12-1048,0,0.0250711,"he U.S.?, one must know (among other things) that Hawaii is a state in the U.S., which is external knowledge not present in the text corpus. Therefore, a QA model needs to be enriched with external knowledge properly to be able to answer many nontrivial questions. Such knowledge can be commonsense knowledge or factual background knowledge about entities and events that is not explicitly expressed but can be found in a knowledge base such as ConceptNet (Speer et al., 2016), Freebase (Pellissier Tanon et al., 2016) and domain-specific KBs collected by information extraction (Fader et al., 2011; Mausam et al., 2012). Thus, we aim to design a neural model that encodes pre-selected knowledge relevant to given questions, and that learns to include the available knowledge as an enrichment to given textual information. In this paper, we propose a new neural architecture, Knowledge-Enriched Answer Generator (KEAG), specifically designed to generate natural answers with integration of external knowledge. KEAG is capable of leveraging symbolic knowledge from a knowledge base as it generates each word in an answer. In particular, we assume that each word is generated from one of the four information sources: 1. q"
D19-1255,P18-1076,0,0.0529677,"needs to have start and end labels (a span) for every QA pair. Mitra (2017) proposed a seq2seq-based model that learns alignment between a question and passage words to produce rich question-aware passage representation by which it directly decodes an answer. Gao et al. (2019) focused on product-aware answer generation based on large-scale unlabeled e-commerce reviews and product attributes. Furthermore, natural answer generation can be reformulated as query-focused summarization which is addressed by Nema et al. (2017). The role of knowledge in certain types of QA tasks has been remarked on. Mihaylov and Frank (2018) showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention. Zhong et al. (2018) proposed commonsense-based pre-training to improve answer selection. Long et al. (2017) made use of knowledge in the form of entity descriptions to predict missing entities in a given document. There have also been a few studies on incorporating knowledge into QA models without passage reading. GenQA (Yin et al., 2016) combines knowledge retrieval and seq2seq learning to produce fluent answers, but it only deals with simple questions containing one singl"
D19-1255,K16-1028,0,0.0202911,", and then generates a new word from the chosen source to make up a final answer. An overview of the neural architecture of KEAG is depicted in Figure 1. 3.2 states (Eq for question q, and Ep for passage p). In each timestep t, the decoder, which is a unidirectional LSTM, takes an answer word as input, and outputs a decoder hidden state srt . We calculate attention distributions aqt and apt on the question and the passage, respectively, as in (Bahdanau et al., 2015): Sequence-to-sequence model KEAG is built upon an extension of the sequenceto-sequence attentional model (Bahdanau et al., 2015; Nallapati et al., 2016; See et al., 2017). The words of question q and passage p are fed one-byone into two different encoders, respectively. Each of the two encoders, which are both bidirectional LSTMs, produces a sequence of encoder hidden gp |tanh(Wp Ep + Up srt + Vp cq + bp )), (2) where gq , Wq , Uq , bq , gp , Wp , Up and bp are learnable parameters. The attention distributions can be viewed as probability distributions over source words, which tells the decoder where to look to generate the next word. The coverage mechanism is added to the attentions to avoid generating repetitive text (See et al., 2017). In"
D19-1255,P17-1098,0,0.120638,"owever, this model still relies heavily on the extraction to perform the generation and thus needs to have start and end labels (a span) for every QA pair. Mitra (2017) proposed a seq2seq-based model that learns alignment between a question and passage words to produce rich question-aware passage representation by which it directly decodes an answer. Gao et al. (2019) focused on product-aware answer generation based on large-scale unlabeled e-commerce reviews and product attributes. Furthermore, natural answer generation can be reformulated as query-focused summarization which is addressed by Nema et al. (2017). The role of knowledge in certain types of QA tasks has been remarked on. Mihaylov and Frank (2018) showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention. Zhong et al. (2018) proposed commonsense-based pre-training to improve answer selection. Long et al. (2017) made use of knowledge in the form of entity descriptions to predict missing entities in a given document. There have also been a few studies on incorporating knowledge into QA models without passage reading. GenQA (Yin et al., 2016) combines knowledge retrieval and seq2"
D19-1255,P02-1040,0,0.104003,"om real web documents. The data has been split into a training set (153,725 QA pairs), a dev set (12,467 QA pairs) and a test set (101,092 questions with unpublished answers). Since true answers are not available in the test set, we hold out the dev set for evaluation in our experiments, and test models for each question on its associated passages by concatenating them all together. We tune the hyper-parameters by crossvalidation on the training set. The answers are human-generated and not necessarily sub-spans of the passages, so the official evaluation tool of MARCO uses the metrics BLEU-1 (Papineni et al., 2002) and ROUGEL (Lin, 2004). We use both metrics for our evaluation to measure the quality of generated answers against the ground truth. For external knowledge, we use ConceptNet (Speer et al., 2016), one of the most widely used commonsense knowledge bases. Our KEAG is generic and thus can also be applied to other knowledge bases. ConceptNet is a semantic network representing words and phrases as well as the commonsense relationships between them. After filtering out non-English entities and relation types with few facts, we have 2,823,089 fact triples and 32 relation types for the model to consu"
D19-1255,D14-1162,0,0.0821713,"les KEAG to handle out-of-vocabulary words by generating a word from given text or knowledge. At both training and test stages, we truncate a passage to 800 words, and limit the length of an answer to 120 words. We train on a single Tesla M40 GPU with the batch size of 16. At test time, answers are generated using beam search with the beam size of 4. 7.3 Model Comparisons Table 1 compares KEAG with the following stateof-the-art extractive/generative QA models, which do not make use of external knowledge: Implementation Details In KEAG, we use 300-dimensional pre-trained Glove word embeddings (Pennington et al., 2014) for initialization with update during training. The dimension of hidden states is set to 256 for every 2526 1. BiDAF (Seo et al., 2017): A multi-stage hierarchical process that represents the context at different levels of granularity, and using the bi-directional attention flow mechanism for answer extraction 2. BiDAF+Seq2Seq: A BiDAF model followed by an additional sequence-to-sequence model for answer generation 3. S-Net (Tan et al., 2018): An extraction-thensynthesis framework to synthesize answers from extracted evidences 4. S-Net+Seq2Seq: An S-Net model followed by an additional sequenc"
D19-1255,P17-1099,0,0.0365983,"ew word from the chosen source to make up a final answer. An overview of the neural architecture of KEAG is depicted in Figure 1. 3.2 states (Eq for question q, and Ep for passage p). In each timestep t, the decoder, which is a unidirectional LSTM, takes an answer word as input, and outputs a decoder hidden state srt . We calculate attention distributions aqt and apt on the question and the passage, respectively, as in (Bahdanau et al., 2015): Sequence-to-sequence model KEAG is built upon an extension of the sequenceto-sequence attentional model (Bahdanau et al., 2015; Nallapati et al., 2016; See et al., 2017). The words of question q and passage p are fed one-byone into two different encoders, respectively. Each of the two encoders, which are both bidirectional LSTMs, produces a sequence of encoder hidden gp |tanh(Wp Ep + Up srt + Vp cq + bp )), (2) where gq , Wq , Uq , bq , gp , Wp , Up and bp are learnable parameters. The attention distributions can be viewed as probability distributions over source words, which tells the decoder where to look to generate the next word. The coverage mechanism is added to the attentions to avoid generating repetitive text (See et al., 2017). In Equation 2, we int"
D19-1255,P18-1178,0,0.0287195,"chical process that represents the context at different levels of granularity, and using the bi-directional attention flow mechanism for answer extraction 2. BiDAF+Seq2Seq: A BiDAF model followed by an additional sequence-to-sequence model for answer generation 3. S-Net (Tan et al., 2018): An extraction-thensynthesis framework to synthesize answers from extracted evidences 4. S-Net+Seq2Seq: An S-Net model followed by an additional sequence-to-sequence model for answer generation 5. QFS (Nema et al., 2017): A model that adapts the query-focused summarization model to answer generation 6. VNET (Wang et al., 2018): An MRC model that enables answer candidates from different Model gQA w/ KBLSTM gQA w/ CRWE MHPGM KEAG Rouge-L 49.33 49.79 50.51 51.68 Bleu-1 42.81 43.35 44.73 45.97 Model gQA gQA w/ KBLSTM gQA w/ CRWE MHPGM KEAG Table 2: Metrics of KEAG and knowledge-enriched QA models on the MARCO dataset. passages to verify each other based on their content representations 7. gQA (Mitra, 2017): A generative approach to question answering by incorporating the copying mechanism and the coverage vector Table 1 shows the comparison of QA models in Rouge-L and Bleu-1. From the table we observe that abstractive"
D19-1255,P17-1132,0,0.0805323,"reover, Fu and Feng (2018) introduced a new attention mechanism that attends across the generated history and memory to explicitly avoid repetition, and incorporated knowledge to enrich generated answers. Some work on knowledge-enhanced natural language (NLU) understanding can be adapted to the question answering task. CRWE (Weissenborn, 2017) dynamically integrates background knowledge in a NLU model in the form of free-text statements, and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. In contrast, KBLSTM (Yang and Mitchell, 2017) leverages continuous representations of knowledge bases to enhance the learning of recurrent neural networks for machine reading. Furthermore, Bauer et al. (2018) proposed MHPGM, a QA architecture that fills in the gaps of inference with commonsense knowledge. The model, however, does not allow an answer word to come directly from knowledge. We adapt these knowledge-enhanced NLU architectures to answer generation, as baselines for our experiments. 3 Knowledge-aware Answer Generation Knowledge-aware answer generation is a question answering paradigm, where a QA model is expected to generate an"
D19-1255,W16-0106,0,0.0389321,"summarization which is addressed by Nema et al. (2017). The role of knowledge in certain types of QA tasks has been remarked on. Mihaylov and Frank (2018) showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention. Zhong et al. (2018) proposed commonsense-based pre-training to improve answer selection. Long et al. (2017) made use of knowledge in the form of entity descriptions to predict missing entities in a given document. There have also been a few studies on incorporating knowledge into QA models without passage reading. GenQA (Yin et al., 2016) combines knowledge retrieval and seq2seq learning to produce fluent answers, but it only deals with simple questions containing one single fact. COREQA (He et al., 2017) extends it with a copy mechanism to learn to copy words from a given question. Moreover, Fu and Feng (2018) introduced a new attention mechanism that attends across the generated history and memory to explicitly avoid repetition, and incorporated knowledge to enrich generated answers. Some work on knowledge-enhanced natural language (NLU) understanding can be adapted to the question answering task. CRWE (Weissenborn, 2017) dy"
D19-1496,N18-1170,0,0.18667,"fense against adversarial attacks (Yuan et al., 2019). In the field of NLP, most of the existing studies focus on the former. For example, Ebrahimi et al. (2017); Alzantot et al. (2018) replace a word with synonyms or similar words while Gao et al. (2018); Liang et al. (2017); Ebrahimi et al. (2017) conduct characterlevel manipulations to fool the models. Moreover, it is not straightforward to adapt existing approaches for blocking adversarial attacks, such as data augmentation (Krizhevsky et al., 2012; Ribeiro et al., 2018; Ren et al., 2019) and adversarial training (Goodfellow et al., 2015; Iyyer et al., 2018; Marzinotto et al., 2019; Cheng et al., 2019; Zhu et al., 2019), to NLP applications. Hence, the defense against adversarial attacks in NLP remains a challenging and unsolved problem. Recognizing and removing the inconspicuous perturbations are the core of defense against adversarial attacks. For instance, in computer vision, denoising auto-encoders (Warde-Farley and Bengio, 2017; Gu and Rigazio, 2015) are applied to remove the noises introduced by perturbations; Prakash et al. (2018) manipulate the images to make the trained models more robust to the perturbations; Samangouei et al. (2018) a"
D19-1496,D18-1316,1,0.934519,"ls by adding a few inconspicuous perturbations into input data, such as masking images with unrecognizable filters and making low-key modifications for texts. Therefore, developing techniques to equip models against adversarial attacks becomes a prominent research problem. ∗ Equal contribution. Listing order is random. Existing studies on adversarial attacks can be classified into two groups, generation of adversarial examples and defense against adversarial attacks (Yuan et al., 2019). In the field of NLP, most of the existing studies focus on the former. For example, Ebrahimi et al. (2017); Alzantot et al. (2018) replace a word with synonyms or similar words while Gao et al. (2018); Liang et al. (2017); Ebrahimi et al. (2017) conduct characterlevel manipulations to fool the models. Moreover, it is not straightforward to adapt existing approaches for blocking adversarial attacks, such as data augmentation (Krizhevsky et al., 2012; Ribeiro et al., 2018; Ren et al., 2019) and adversarial training (Goodfellow et al., 2015; Iyyer et al., 2018; Marzinotto et al., 2019; Cheng et al., 2019; Zhu et al., 2019), to NLP applications. Hence, the defense against adversarial attacks in NLP remains a challenging and"
D19-1496,P11-1015,0,0.0640328,"rm moviemaking at its beast. Old-form moviemaking at its be s t. Old-form moviemaking at its bets. Old-form moviemaking at its aggrandize. Old-form moviemaking at its way. Table 2: Examples of each type of attack ber of embeddings in the embedding corpus C. 4 Experiments In this section, we conduct extensive experiments to evaluate the performance of DISP in improving model robustness. 4.1 Experimental Settings Experimental Datasets. Experiments are conducted on two benchmark datasets: (1) Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) and (2) Internet Movie Database (IMDb) (Maas et al., 2011). SST-2 and IMDb are both sentiment classification datasets which involve binary labels annotating sentiment of sentences in movie reviews. Detailed statistics of two datasets are listed in Table 1. Attack Generation. We consider three types of character-level attacks and two types of word-level attacks. The character-level attacks consist of insertion, deletion, and swap. Insertion and deletion attacks inject and remove a character, respectively, while a swap attack flips two adjacent characters. The word-level attacks include random and embed. A random attack randomly samples a word to repla"
D19-1496,N19-2021,0,0.0648996,"Missing"
D19-1496,L18-1008,0,0.0871069,"Missing"
D19-1496,P19-1103,0,0.0758892,"classified into two groups, generation of adversarial examples and defense against adversarial attacks (Yuan et al., 2019). In the field of NLP, most of the existing studies focus on the former. For example, Ebrahimi et al. (2017); Alzantot et al. (2018) replace a word with synonyms or similar words while Gao et al. (2018); Liang et al. (2017); Ebrahimi et al. (2017) conduct characterlevel manipulations to fool the models. Moreover, it is not straightforward to adapt existing approaches for blocking adversarial attacks, such as data augmentation (Krizhevsky et al., 2012; Ribeiro et al., 2018; Ren et al., 2019) and adversarial training (Goodfellow et al., 2015; Iyyer et al., 2018; Marzinotto et al., 2019; Cheng et al., 2019; Zhu et al., 2019), to NLP applications. Hence, the defense against adversarial attacks in NLP remains a challenging and unsolved problem. Recognizing and removing the inconspicuous perturbations are the core of defense against adversarial attacks. For instance, in computer vision, denoising auto-encoders (Warde-Farley and Bengio, 2017; Gu and Rigazio, 2015) are applied to remove the noises introduced by perturbations; Prakash et al. (2018) manipulate the images to make the train"
D19-1496,P18-1079,0,0.0978005,"sarial attacks can be classified into two groups, generation of adversarial examples and defense against adversarial attacks (Yuan et al., 2019). In the field of NLP, most of the existing studies focus on the former. For example, Ebrahimi et al. (2017); Alzantot et al. (2018) replace a word with synonyms or similar words while Gao et al. (2018); Liang et al. (2017); Ebrahimi et al. (2017) conduct characterlevel manipulations to fool the models. Moreover, it is not straightforward to adapt existing approaches for blocking adversarial attacks, such as data augmentation (Krizhevsky et al., 2012; Ribeiro et al., 2018; Ren et al., 2019) and adversarial training (Goodfellow et al., 2015; Iyyer et al., 2018; Marzinotto et al., 2019; Cheng et al., 2019; Zhu et al., 2019), to NLP applications. Hence, the defense against adversarial attacks in NLP remains a challenging and unsolved problem. Recognizing and removing the inconspicuous perturbations are the core of defense against adversarial attacks. For instance, in computer vision, denoising auto-encoders (Warde-Farley and Bengio, 2017; Gu and Rigazio, 2015) are applied to remove the noises introduced by perturbations; Prakash et al. (2018) manipulate the image"
D19-1496,I17-2062,0,0.110183,"Missing"
D19-1496,D13-1170,0,0.00747921,"n is the numExample Old-form moviemaking at its best. Old-form moviemaking at its beast. Old-form moviemaking at its be s t. Old-form moviemaking at its bets. Old-form moviemaking at its aggrandize. Old-form moviemaking at its way. Table 2: Examples of each type of attack ber of embeddings in the embedding corpus C. 4 Experiments In this section, we conduct extensive experiments to evaluate the performance of DISP in improving model robustness. 4.1 Experimental Settings Experimental Datasets. Experiments are conducted on two benchmark datasets: (1) Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) and (2) Internet Movie Database (IMDb) (Maas et al., 2011). SST-2 and IMDb are both sentiment classification datasets which involve binary labels annotating sentiment of sentences in movie reviews. Detailed statistics of two datasets are listed in Table 1. Attack Generation. We consider three types of character-level attacks and two types of word-level attacks. The character-level attacks consist of insertion, deletion, and swap. Insertion and deletion attacks inject and remove a character, respectively, while a swap attack flips two adjacent characters. The word-level attacks include random"
D19-1496,P19-1366,0,0.141784,"ld of NLP, most of the existing studies focus on the former. For example, Ebrahimi et al. (2017); Alzantot et al. (2018) replace a word with synonyms or similar words while Gao et al. (2018); Liang et al. (2017); Ebrahimi et al. (2017) conduct characterlevel manipulations to fool the models. Moreover, it is not straightforward to adapt existing approaches for blocking adversarial attacks, such as data augmentation (Krizhevsky et al., 2012; Ribeiro et al., 2018; Ren et al., 2019) and adversarial training (Goodfellow et al., 2015; Iyyer et al., 2018; Marzinotto et al., 2019; Cheng et al., 2019; Zhu et al., 2019), to NLP applications. Hence, the defense against adversarial attacks in NLP remains a challenging and unsolved problem. Recognizing and removing the inconspicuous perturbations are the core of defense against adversarial attacks. For instance, in computer vision, denoising auto-encoders (Warde-Farley and Bengio, 2017; Gu and Rigazio, 2015) are applied to remove the noises introduced by perturbations; Prakash et al. (2018) manipulate the images to make the trained models more robust to the perturbations; Samangouei et al. (2018) apply generative adversarial networks to generate perturbation-fr"
D19-1496,D09-1129,0,\N,Missing
D19-1496,P18-2006,0,\N,Missing
D19-1496,N19-1336,0,\N,Missing
F13-1026,I05-2045,0,0.0197361,"Missing"
F13-1026,C04-1051,0,0.176404,"Missing"
F13-1026,eichler-etal-2008-unsupervised,0,0.0406469,"Missing"
F13-1026,D11-1142,0,0.0816668,"Missing"
F13-1026,ferret-2010-testing,1,0.895539,"Missing"
F13-1026,W12-0702,0,0.0548574,"Missing"
F13-1026,P04-1053,0,0.0805501,"Missing"
F13-1026,D12-1094,0,0.0426977,"Missing"
F13-1026,P09-1113,0,0.124317,"Missing"
F13-1026,N10-1047,0,0.0686786,"Missing"
F13-1026,D11-1048,0,0.0534093,"Missing"
F13-1026,P06-2094,0,0.0679576,"Missing"
F13-1026,N06-1039,0,0.0533402,"Missing"
F13-1026,wang-etal-2012-evaluation,1,0.888778,"Missing"
F13-1026,D11-1135,0,0.0503165,"Missing"
J10-2004,P98-1006,0,0.0838333,"Missing"
J10-2004,J93-2003,0,0.0129373,"Missing"
J10-2004,A00-2018,0,0.126804,"yle trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment tr"
J10-2004,J07-2003,0,0.240062,"putational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transducer rules must be broken down, or binarized, into rules that contain at most two variables—more efﬁcient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efﬁciency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smal"
J10-2004,D09-1037,0,0.100934,"Missing"
J10-2004,P97-1003,0,0.0914444,"ng, but in this work we concentrate only on target-language syntax. The target-language generation problem presents a difﬁcult challenge, whereas the source sentence is ﬁxed and usually already grammatical. To prepare training data for such a system, we begin with a bilingual text that has been automatically processed into segment pairs. We require that the segments be single sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training"
J10-2004,D07-1079,1,0.88608,"traction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning"
J10-2004,P06-1121,1,0.821544,"and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts count(rule) in this way: P(rule) = count(LHS -root(rule)) . When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up computing the joint probability P(e, c). We smooth the rule counts with Good–Turing smoothing (Good 1953). This extraction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enu"
J10-2004,N04-1035,1,0.876041,"re 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction— we envision a generative top–down process by which an English tree is gradually transformed (by probabilistic rules) into an observed Chinese string. We represent a collection of such rules as a tree transducer (Knight and Graehl 2005). In order to construct this transducer from parsed and word-aligned data, we use the GHKM rule extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the minimal rules extracted from the example (tree, string, alignment) triple in Figure 1. Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts count(rule) in this way: P(rule) = count(LHS -root(rule)) . When we apply these probabilities"
J10-2004,J99-4004,0,0.0216439,"ubstructure that yields NNP2 , NNP3 , and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 254 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 7 Packed forest obtained by packing trees (3) and (6) in Figure 6. 3.2.2 Parallel Binarization. Simple binarizations transform a parse tree into another single parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed to enable dynamic programming when we extract translation rules from it. Borrowing terms from parsing semirings (Goodman 1999), a packed forest is composed of additive forest nodes (⊕-nodes) and multiplicative forest nodes (⊗-nodes). In the binarization forest, a ⊗-node corresponds to a tree node in the unbinarized tree or a new tree node introduced during tree binarization; and this ⊗-node composes several ⊕-nodes, forming a one-level substructure that is observed in the unbinarized tree or in one of its binarized tree. A ⊕-node corresponds to alternative ways of binarizing the same tree node and it contains one or more ⊗-nodes. The same ⊕-node can appear in more than one place in the packed forest, enabling sharing"
J10-2004,N06-1031,1,0.897685,"a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this article, we take a broad view and investigate how changes to syntactic structures, syntactic labels, and word alignments can lead to substantial improvements in translation quality on top of a strong baseline. We design our methods around problems that arise in MT data whose parses and alignments use some Penn Treebankstyle annotations. We believe that some of the techniques will apply to other annotation schemes, but conc"
J10-2004,J98-4004,0,0.110894,"use problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufﬁcient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough from these two samples to translate the new phrase .W #L Z Æ{ 3ä VIKTOR CHERNOMYRDIN AND HIS COLL"
J10-2004,P03-1054,0,0.00631755,"in Figure 11(b) yields ungrammatical translations like he likes reading she does not like reading. Tree binarization enables the reuse of substructures, but causes over-generation of trees at the same time. We solve the coarse-nonterminal problem by reﬁning/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) reﬁnes a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of subst"
J10-2004,J08-3004,1,0.883026,"Missing"
J10-2004,W04-3250,0,0.0206993,"ecause the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efﬁcient RAM usage. To test the signiﬁcance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the"
J10-2004,W02-1018,1,0.803481,"Missing"
J10-2004,J93-2004,0,0.0464715,"Missing"
J10-2004,D07-1038,1,0.870717,"Missing"
J10-2004,P04-1084,0,0.0245554,"Missing"
J10-2004,D08-1022,0,0.0550391,"t rules from the admissible nodes in the packed forest. Rules that can be extracted from the original unrestructured tree can be extracted from the packed forest as well. Parallel binarization results in parse forests. Thus translation rules need to be extracted from training data consisting of (e-forest, f, a)-tuples. 3.3 Extracting Translation Rules from (e-forest, f, a)-tuples Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization of the (e-parse, f, a)-based rule extraction algorithm in Galley et al. (2006). A similar problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation forest (Galley et al. 2006), which consists of overlapping translation rules. The algorithm recursively traverses the e-forest top–down, extracts rules only at admissible e-forest 256 Wang et al. Re-structuring, Re-labeling, and Re-aligning nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the following two procedures, depending on which condition is met. r r Condition 1: If we reach an additive e-forest node, for each of its children, which"
J10-2004,J04-4002,0,0.187296,"Missing"
J10-2004,P03-1021,0,0.0115692,"idates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The ﬁnal score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reﬂected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this art"
J10-2004,P06-1055,0,0.704636,"e ﬁve or more direct children. It is well known in monolingual parsing research that these ﬂat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce ﬂat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are ﬂattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We ﬁnd that ﬂat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufﬁcient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough"
J10-2004,P07-1065,0,0.0152868,"is from the newswire domain, and we chose it to represent a wide period of time rather than a single year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efﬁcient RAM usage. To test the signiﬁcance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many ﬂat structures. For example, base noun phrases frequently have ﬁve or more direct children. It is well known in monolingual parsing research that th"
J10-2004,C96-2141,0,0.0804986,"Missing"
J10-2004,D07-1078,1,0.89473,"Missing"
J10-2004,J97-3002,0,0.829465,"we notice that re-structuring tends to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already beneﬁts from tree re-structuring. All p-values are computed against Baseline1. E XPERIMENT NIST08 BLEU p NIST08-NW BLEU p Baseline1 (no re-structuring and no re-labeling) Linguistically motivated re-labeling 29.12 29.57 — 0.029"
J10-2004,P01-1067,1,0.646868,"uracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already beneﬁts from tree re-structuring. All p-values are computed against Baseline1. E XPERIMENT NIST08 BLEU p NIST08-NW BLEU p Baseline1 (no re-structuring and no re-labeling) Linguistically motivated re-labeling 29.12 29.57 — 0.029 35.33 35.85 — 0.050 Baseline2 (EM re-structuring but no re-labeli"
J10-2004,P02-1039,1,0.437148,"ts and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efﬁciently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure. 249 Computational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transduce"
J10-2004,J02-1005,0,\N,Missing
J10-2004,D08-1033,0,\N,Missing
J10-2004,J12-2006,0,\N,Missing
J10-2004,C98-1006,0,\N,Missing
J10-2004,N06-1033,1,\N,Missing
N06-1001,W04-3237,0,0.0273301,"capitalization tag sequence. Associating a tag in the output with the corresponding A capitalizer is a tagger that recovers the capitalization tag for each input lowercased word, outputting a well-capitalized sentence. Since each lowercased word can have more than one tag, and associating a tag with a lowercased word can result in more than one surface form (e.g., /home/doc MX can be either /home/DOC or /home/Doc), we need a capitalization model to solve the capitalization ambiguities. For example, Lita et al. (2003) use a trigram language model estimated from a corpus with case information; Chelba and Acero (2004) use a maximum entropy Markov model (MEMM) combining features involving words and their cases. Capitalization models presented in most previous approaches are monolingual because the models are estimated only from monolingual texts. However, for capitalizing machine translation outputs, using only monolingual capitalization models is not enough. For example, if the sentence “click ok to save your changes to /home/doc .” in the above example is the translation of the French sentence “CLIQUEZ SUR OK POUR ENREGISTRER VOS MODIFICATIONS DANS /HOME/DOC .”, the correct capitalization result should pr"
N06-1001,N04-1035,1,0.501754,"wo words in E represents the dependency between them captured by monolingual n-gram language models. We also assume that both E and F have phrase boundaries available (denoted by the square brackets), and that A is the phrase alignment. ˜i is the i-th In Figure 3, F˜j is the j-th phrase of F , E phrase of E, and they align to each other. We do not require a word alignment; instead we find it reason˜i can be aligned to any able to think that a word in E adapted to syntax-based machine translation, too. To this end, the translational correspondence is described within a translation rule, i.e., (Galley et al., 2004) (or a synchronous production), rather than a translational phrase pair; and the training data will be derivation forests, instead of the phrase-aligned bilingual corpus. 2 The capitalization model p(E|F, A) itself does not require the existence of e. This means that in principle this model can also be viewed as a capitalized translation model that performs translation and capitalization in an integrated step. In our paper, however, we consider the case where the machine translation output e is given, which is reflected by the the fact that GEN(e) takes e as input in Formula 1. word in F˜j . A"
N06-1001,P03-1020,0,0.768398,"getting the surface form “Click OK to save your changes to /home/DOC .”. We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields. Experiments carried out on three language pairs and a variety of experiment conditions show that our model significantly outperforms a strong monolingual capitalization model baseline, especially when working with small datasets and/or European language pairs. 1 Introduction Capitalization is the process of recovering case information for texts in lowercase. It is also called truecasing (Lita et al., 2003). Usually, capitalization itself tries to improve the legibility of texts. It, however, can affect the word choice or order when interacting with other models. In natural language processing, a good capitalization model has been shown useful for tasks like name entity recognition, automatic content extraction, speech recognition, modern word processors, and machine translation (MT). Capitalization can be viewed as a sequence labeling process. The input to this process is a sentence in lowercase. For each lowercased word in the input sentence, we have several available capitalization tags: init"
N06-1001,W02-1018,1,0.822044,"d bilingual capitalization model using the development set. Since estimation of the feature weights requires the phrase alignment information, we efficiently applied the NPA on the development set. We employed two LM-based capitalizers as baselines for performance comparison: a unigram-based capitalizer and a strong trigram-based one. The unigram-based capitalizer is the usual baseline for capitalization experiments in previous work. The trigram-based baseline is similar to the one in (Lita et al., 2003) except that we used Kneser-Ney smoothing instead of a mixture. A phrase-based SMT system (Marcu and Wong, 2002) was trained on the bitext. The capitalizer was incorporated into the MT system as a postprocessing module — it capitalizes the lowercased MT output. The phrase boundaries and alignments needed by the capitalizer were automatically inferred as part of the decoding process. 6 Experiments 6.2 6.1 Settings We conducted capitalization experiments on three language pairs: English-to-French (E→F) with a bilingual corpus from the Information Technology (IT) domain; French-to-English (F→E) with a bilingual corpus from the general news domain; and Chinese-to-English (C→E) with a bilingual corpus from t"
N06-1001,P99-1021,0,0.0214936,"for each low2 Finput Lower Case {F } Lower Case {f } Train Translation Model Lower Case {e} Train Language Model f Translation Model MT Decoder {E} Train Monolingual Capitalization Model Languagel Model Monolingual Cap Model e Capitalization Eoutput Figure 1: The monolingual capitalization scheme employed by most statistical MT systems. ercased sentence e, they find the label sequence T that maximizes p(T |e). They use a maximum entropy Markov model (MEMM) to combine features of words, cases and context (i.e., tag transitions). Gale et al. (1994) report good results on capitalizing 100 words. Mikheev (1999) performs capitalization using simple positional heuristics. 3 Monolingual Capitalization Scheme Translation and capitalization are usually performed in two successive steps because removing case information from the training of translation models substantially reduces both the source and target vocabulary sizes. Smaller vocabularies lead to a smaller translation model with fewer parameters to learn. For example, if we do not remove the case information, we will have to deal with at least nine probabilities for the English-French word pair (click, cliquez). This is because either “click” or “c"
N06-1001,J04-4002,0,0.0154102,"alignment can be quite computationally expensive as it requires to translate the entire training corpus; also a phrase aligner is not always available. We therefore generate the training data using a na¨ıve phrase aligner (NPA) instead of resorting to a real one. The input to the NPA is a word-aligned bilingual corpus. The NPA stochastically chooses for each sentence pair one segmentation and phrase alignment that is consistent with the word alignment. An aligned phrase pair is consistent with the word alignment if neither phrase contains any word aligning to a word outside the other phrase (Och and Ney, 2004). The NPA chunks the source sentence into phrases according to a probabilistic distribution over source phrase lengths. This distribution can be obtained from the trace output of a phrase-based MT Languages E→F (IT) F→E (news) C→E (news) Entire Corpus (#W) Training Dev Test-Prec. 62M 13K 15K 144M 11K 22K 50M 8K 17K Test-BLEU (#sents) 763 241 919 Table 2: Corpora used in experiments. decoder on a small development set. The NPA has to retry if the current source phrase cannot find any consistent target phrase. Unaligned target words are attached to the left phrase. Heuristics are employed to pre"
N06-1001,2001.mtsummit-papers.68,0,0.0158198,"assess the impact of our capitalizer on end-to-end translation performance; in this case, the capitalizer may operate on ungrammatical sentences. We chose to work with these three language pairs because we wanted to test our capitalization model on both English and French target MT systems and in cases where the source language has no case information (such as in Chinese). We estimated the feature functions, such as the log probabilities in the language model, from the 6 BLEU and Precision We measured the impact of our capitalization model in the context of an end-to-end MT system using BLEU (Papineni et al., 2001). In this context, the capitalizer operates on potentially ill-formed, MTproduced outputs. To this end, we first integrated our bilingual capitalizer into the phrase-based SMT system as a postprocessing module. The decoder of the MT system was modified to provide the capitalizer with the case-preserved source sentence, the lowercased translation, and the phrase boundaries and their alignments. Based on this information, our bilingual capitalizer recovers the case information of the lowercased translation, outputting a capitalized target sentence. The case-restored machine translations were eva"
N06-1001,P04-1007,0,0.356563,"in Formula 1. word in F˜j . A probabilistic model defined on this graph is a Conditional Random Field. Therefore, it is natural to formulate the bilingual capitalization model using CRFs:3 ! (2) ! (3) I X 1 pλ (E|F, A) = exp λi fi (E, F, A) Z(F, A, λ) i=1 where Z(F, A, λ) = X E∈GEN(e) exp I X λi fi (E, F, A) i=1 fi (E, F, A), i = 1...I are the I features, and λ = (λ1 , ..., λI ) is the feature weight vector. Based on this capitalization model, the decoder in the capitalizer looks for the best E ∗ such that ∗ E = arg maxE∈GEN(e,F ) I X λi fi (E, F, A) (4) i=1 4.2 Parameter Estimation Following Roark et al. (2004), Lafferty et al. (2001) and Chen and Rosenfeld (1999), we are looking for the set of feature weights λ maximizing the regularized log-likelihood LLR (λ) of the training data {E (n) , F (n) , A(n) , n = 1, ..., N }. LLR (λ) = N X “ ” ||λ||2 log p E (n) |F (n) , A(n) − 2σ 2 n=1 (5) 4.3 Feature Functions We define features based on the alignment graph in Figure 3. Each feature function is defined on a word. Monolingual language model feature. The monolingual LM feature of word Ei is the logarithm of the probability of the n-gram ending at Ei : fLM (Ei , F, A) = log p(Ei |Ei−1 , ..., Ei−n+1 ) (6)"
N06-1001,P02-1040,0,\N,Missing
N09-1025,P08-1024,0,0.207216,"reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approa"
N09-1025,J93-2003,0,0.0195041,"Missing"
N09-1025,P07-1005,1,0.840775,"xtracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f, e, f+1 ), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f, e, f−1 ) with f−1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseli"
N09-1025,D08-1024,1,0.651902,"st a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best translations according to each of: h(e) · w B(e) + h(e) · w (1) −B(e) + h(e) · w • For each i, select an oracle translation: e∗ = arg max (B(e) + h(e) · w) Let ∆hi j = e ∗ h(ei ) (2) − h(ei j ). • For each ei j , compute the loss `i j = B(e∗i ) − B(ei j ) (3) • Update w to the value of w0 that minimizes: m X 1 0 kw − wk2 + C max (`i j − ∆hi j · w0 ) (4) 1≤ j≤n 2 i=1 where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple—for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update—it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder"
N09-1025,P05-1033,1,0.617392,"particularly as it pertains to improving the best systems we have. Further: • Do syntax-based translation systems have unique and effective levers to pull when designing new features? • Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntaxbased MT system—already the highest-scoring single system in the NIST 2008 Chinese-English common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement. This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. ∗ 218 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never"
N09-1025,J07-2003,1,0.210507,"string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of"
N09-1025,P97-1003,0,0.373709,"x MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node count all target-side, discount # 11 56 61 10990 25 25 132 136 283 Tune 35.4 35.9 36.6 38.4 38.6 38.5 38.7 38.7 39.6 Test 36.1 36.9∗ 37.3∗∗ 37.6∗∗ 39.5 39.8∗ 39.9∗ 40.0∗∗ 40.6∗∗ Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM B scores. ∗ or ∗∗ = significantly better than MERT baseline (p < 0.05 or 0.01, respectively). the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2. Syntax-based rule extraction was performed on a 65 million word subset of the training data. For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data. We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used"
N09-1025,D07-1079,1,0.641497,"Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language mode"
N09-1025,W08-0306,1,0.527338,"l other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node count all target-side, discount # 11 56 61 10990 25 25 132 136 283 Tune 35.4 35.9 36.6 38.4 38.6 38.5 38.7 38.7 39.6 Test 36.1 36.9∗ 37.3∗∗ 37.6∗∗ 39.5 39.8∗ 39.9∗ 40.0∗∗ 40.6∗∗ Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM B scores. ∗ or ∗∗ = significantly better than MERT baseline (p < 0.05 or 0.01, respectivel"
N09-1025,N04-1035,1,0.501726,"3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0 :NPB PP(IN(of x1 :NPB)) ↔ x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of 219 count(rule) count(LHS-root(rule)) When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and o"
N09-1025,P06-1121,1,0.630882,"y trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features a"
N09-1025,P08-1067,0,0.0419096,"the highest-scoring single system in the NIST 2008 Chinese-English common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement. This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. ∗ 218 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Ma"
N09-1025,W02-1006,0,0.0271944,"st frequent one.) We then define, for each triple ( f, e, f+1 ), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f, e, f−1 ) with f−1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node coun"
N09-1025,P06-1096,0,0.752695,"Missing"
N09-1025,D08-1076,0,0.134772,"8). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules"
N09-1025,P08-1114,0,0.131782,"on rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features We now turn to features that make use of source-side context. Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder. This is because the entire source sentence, being fixed, is always available to every feature. 221 Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separatelytunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system’s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance reorderings from short-distance reorderings. To"
N09-1025,P02-1038,0,0.52616,"6). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1 , . . . , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best translations according to each of: h(e) · w B(e) + h(e) · w (1) −B(e) + h(e) · w • For each i, s"
N09-1025,P03-1021,0,0.12973,",001 New Features for Statistical Machine Translation∗ David Chiang and Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA Abstract Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B and +1.1 B, respectively. We analyze the impact of the new feature"
N09-1025,P06-1055,0,0.128993,"in at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1 , . . . , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best transla"
N09-1025,P07-1065,0,0.0152988,"Missing"
N09-1025,P06-1091,0,0.161178,"ng n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as"
N09-1025,D07-1078,1,0.613116,"content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a"
N09-1025,D07-1080,0,0.64867,"d Chiang and Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA Abstract Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B and +1.1 B, respectively. We analyze the impact of the new features and the performance of the learning algorithm. 1 Wei Wang Language W"
N09-1025,J97-3002,0,0.217863,"mal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model"
N09-1025,P02-1039,1,0.475396,"y P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al"
N09-1025,N06-1033,1,0.759347,"nese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Oc"
N09-1025,N04-1021,0,\N,Missing
N18-1164,P11-1118,0,0.719299,"Missing"
N18-1164,P16-1177,0,0.0645293,"Missing"
N18-1164,P14-1062,0,0.0313598,"their relationships are needed. Recent studies have demonstrated the effectiveness of deep learning methods in representation learning (Bengio et al., 2013), aiming to infer low-dimensional distributed representations for sparse data such as text (Hinton and Salakhutdinov, 2006). These representations can be derived not only for words (Mikolov et al., 2013) but also sentences and documents (Le and Mikolov, 2014). In particular, convolutional neural networks (CNNs) have been shown to efficiently and effectively preserve important semantic and syntactic information from embedded text sequences (Blunsom et al., 2014). It has been demonstrated that CNNs produce state-of-the-art results in many NLP tasks such as text classification (Kim, 2014; Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Tang et al., 2014; Poria et al., 2015). Existing approaches, however, do not take advantage of deep learning techniques to model relationships between messages for disentangling conversations. (Mehri and Carenini, 2017) defined many statistical features for use with a random forest for in-thread classification and used a recurrent neural network (RNN) only to model adjacent messages with an external dataset"
N18-1164,D14-1181,0,0.0220354,"ngio et al., 2013), aiming to infer low-dimensional distributed representations for sparse data such as text (Hinton and Salakhutdinov, 2006). These representations can be derived not only for words (Mikolov et al., 2013) but also sentences and documents (Le and Mikolov, 2014). In particular, convolutional neural networks (CNNs) have been shown to efficiently and effectively preserve important semantic and syntactic information from embedded text sequences (Blunsom et al., 2014). It has been demonstrated that CNNs produce state-of-the-art results in many NLP tasks such as text classification (Kim, 2014; Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Tang et al., 2014; Poria et al., 2015). Existing approaches, however, do not take advantage of deep learning techniques to model relationships between messages for disentangling conversations. (Mehri and Carenini, 2017) defined many statistical features for use with a random forest for in-thread classification and used a recurrent neural network (RNN) only to model adjacent messages with an external dataset as a feature. In this paper, we aim to leverage deep learning for conversation disentanglement. Our proposed approach consist"
N18-1164,W12-1607,0,0.517805,"Missing"
N18-1164,I17-1062,0,0.270949,"lov, 2014). In particular, convolutional neural networks (CNNs) have been shown to efficiently and effectively preserve important semantic and syntactic information from embedded text sequences (Blunsom et al., 2014). It has been demonstrated that CNNs produce state-of-the-art results in many NLP tasks such as text classification (Kim, 2014; Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Tang et al., 2014; Poria et al., 2015). Existing approaches, however, do not take advantage of deep learning techniques to model relationships between messages for disentangling conversations. (Mehri and Carenini, 2017) defined many statistical features for use with a random forest for in-thread classification and used a recurrent neural network (RNN) only to model adjacent messages with an external dataset as a feature. In this paper, we aim to leverage deep learning for conversation disentanglement. Our proposed approach consists of two stages: (1) message pair similarity estimation and (2) conversation identification. In the first stage, we propose the Siamese hierarchical convolutional neural network (SHCNN) to estimate conversation-level similarity between pairs of closely posted messages. SHCNN is fram"
N18-1164,P08-1095,0,0.172305,"na, June 1 - 6, 2018. 2018 Association for Computational Linguistics scores (Shen et al., 2006; Mayfield et al., 2012) or similar context messages (Wang and Oard, 2009). However, similarity thresholds for determining new topics vary depending on context. Embedding of earlier messages, resulting in duplication of parts of messages, can alter the similarity score. More specifically, the similarity scores obtained in previous work cannot well represent conversationlevel relationships between messages. Several studies have examined the use of statistical (Du et al., 2017) and linguistic features (Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012) for predicting user annotations of paired message similarity. These studies employed bag-of-words representations which do not capture term similarity and cannot distinguish word importance and relationships between words in a message. Thus, better representations of messages and their relationships are needed. Recent studies have demonstrated the effectiveness of deep learning methods in representation learning (Bengio et al., 2013), aiming to infer low-dimensional distributed representations for sparse data such as text (Hinton and Salakhutdinov, 2006). T"
N18-1164,J10-3004,0,0.504306,"ons m ˆ i and m ˆ j , each from a sub-network. More formally, the absolute difference d is a vector where the k-th element is computed as |m ˆ i (k) m ˆ j (k)|. This approach provides not only fewer parameters but also the flexibility to observe interactions among different dimensions in representations. Our experiments also show it outperforms the other two approaches in similarity estimation (See Section 4). In addition to message contents, contexts such as temporal and user information were also usually considered in previous studies about conversation disentanglement (Wang and Oard, 2009; Elsner and Charniak, 2010, 2011). In this paper, we focus on the performance of message content representations and only incorporate four context features: speaker identicality, absolute time difference and the number of duplicated words with and without weighting by inverse document frequency (Christopher et al., 2008). SHCNN concatenates the context features x(mi , mj ) with the absolute difference d as the input of a fully-connected layer of the same size. The final output of SHCNN yˆ (mi , mj ) is normalized by a logistic sigmoid function (Han and Moraga, 1995), representing the probability P (z(mi ) = z(mj )). 3."
N18-1164,D15-1303,0,0.0249934,"ata such as text (Hinton and Salakhutdinov, 2006). These representations can be derived not only for words (Mikolov et al., 2013) but also sentences and documents (Le and Mikolov, 2014). In particular, convolutional neural networks (CNNs) have been shown to efficiently and effectively preserve important semantic and syntactic information from embedded text sequences (Blunsom et al., 2014). It has been demonstrated that CNNs produce state-of-the-art results in many NLP tasks such as text classification (Kim, 2014; Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Tang et al., 2014; Poria et al., 2015). Existing approaches, however, do not take advantage of deep learning techniques to model relationships between messages for disentangling conversations. (Mehri and Carenini, 2017) defined many statistical features for use with a random forest for in-thread classification and used a recurrent neural network (RNN) only to model adjacent messages with an external dataset as a feature. In this paper, we aim to leverage deep learning for conversation disentanglement. Our proposed approach consists of two stages: (1) message pair similarity estimation and (2) conversation identification. In the fi"
N18-1164,P14-1146,0,0.0222122,"ations for sparse data such as text (Hinton and Salakhutdinov, 2006). These representations can be derived not only for words (Mikolov et al., 2013) but also sentences and documents (Le and Mikolov, 2014). In particular, convolutional neural networks (CNNs) have been shown to efficiently and effectively preserve important semantic and syntactic information from embedded text sequences (Blunsom et al., 2014). It has been demonstrated that CNNs produce state-of-the-art results in many NLP tasks such as text classification (Kim, 2014; Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Tang et al., 2014; Poria et al., 2015). Existing approaches, however, do not take advantage of deep learning techniques to model relationships between messages for disentangling conversations. (Mehri and Carenini, 2017) defined many statistical features for use with a random forest for in-thread classification and used a recurrent neural network (RNN) only to model adjacent messages with an external dataset as a feature. In this paper, we aim to leverage deep learning for conversation disentanglement. Our proposed approach consists of two stages: (1) message pair similarity estimation and (2) conversation iden"
N18-1164,D11-1002,0,0.373402,"e similarity, and assign messages to conversations based on a predefined 1813 3 Reddit: https://www.reddit.com/ threshold. In contrast, supervised methods exploit a set of user annotations (Elsner and Charniak, 2008; Mayfield et al., 2012; Shen et al., 2006; Du et al., 2017; Mehri and Carenini, 2017) to adapt to different datasets. Our approach can be classified as a supervised approach because a small set of user annotations is used to train the SHCNN. In addition to conversations, some studies predict the partial structure of threaded data, especially for online forums (Aumayr et al., 2011; Wang et al., 2011b,a). These studies merely classify parent-child relationships in disentangled, independent threads. Moreover, they focus only on comments to the same post. Indeed, conversation disentanglement is a more difficult task. Estimating the similarity of text pairs is an essential part in our approach. Many studies also focus on similar tasks aside from conversation disentanglement, such as entailment prediction (Mueller and Thyagarajan, 2016; Wang and Jiang, 2017) and question-answering (Severyn and Moschitti, 2015; Amiri et al., 2016; Yin et al., 2016). However, most of their models are complicate"
N18-1164,N09-1023,0,0.354363,"sentangle interleaved conversations can improve a user’s satisfaction with a chat system. One solution for conversation disentanglement is to model the task as a topic detection and tracking (TDT) (Allan, 2002) task by deciding whether each incoming message starts a new topic or belongs to an existing conversation. Messages in the same conversation may have higher similarity 1812 Proceedings of NAACL-HLT 2018, pages 1812–1822 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics scores (Shen et al., 2006; Mayfield et al., 2012) or similar context messages (Wang and Oard, 2009). However, similarity thresholds for determining new topics vary depending on context. Embedding of earlier messages, resulting in duplication of parts of messages, can alter the similarity score. More specifically, the similarity scores obtained in previous work cannot well represent conversationlevel relationships between messages. Several studies have examined the use of statistical (Du et al., 2017) and linguistic features (Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012) for predicting user annotations of paired message similarity. These studies employed bag-of-words represen"
N18-1164,Q16-1019,0,0.0826873,"Missing"
N19-1136,W17-2339,0,0.0132205,"y with tags of 1348 Proceedings of NAACL-HLT 2019, pages 1348–1354 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics various semantic forms and granularities (Peters, 2009; Heymann and Garcia-Molina, 2006). One challenging issue is how to exploit the relations among labels (user-generated tags) (Zhang and Zhou, 2014; Gibaja and Ventura, 2015) to improve the learning performance. Among neural network based methods, a recent attempt is to initialise weights for dedicated neurons in the last layer to memorise the label relations (Kurata et al., 2016; Baker and Korhonen, 2017), however, the limitation is the large number of neurons to be assigned, making it inefficient (or inapplicable) for systems with large number of labels. To incorporate the label semantics inferred from the data or from external knowledge bases into the network, we design two loss regularisers, for similarity and subsumption relations, respectively. The regularisers enforce the output layer of the network to satisfy the semantic constraints of the labels. 2 Proposed Method We propose a parallelled two-layered attention network that simulates users’ reading and annotation behaviour for document"
N19-1136,C16-1090,0,0.0309729,"Jannach, 2014). Common socially annotated textual resources include questions, papers, (micro-)blogs, product reviews, etc. In practice, however, only a limited number of resources is annotated with tags. Annotating a large number of documents requires much cognitive effort and can be time-consuming. This has driven research on document annotation based on existing tag sets (Bel´em et al., 2017; Nie et al., 2014). Recent studies formalise the automated social annotation task as a multi-label classification problem (Gibaja and Ventura, 2015) and apply deep learning approaches (Li et al., 2016; Huang et al., 2016; Hassan et al., 2018). A strong baseline is the use of Bi-directional RNN (Schuster and Paliwal, 1997) with GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997). Another more recent improvement is achieved through Hierarchical Attention Network (HAN) (Yang et al., 2016) which discriminates important words and sentences from others, as adapted in (Hassan et al., 2018) for annotation. These models, however, suffer from two issues: (i) simply scanning over the words and sentences, the models do not fully mimic the way users read and annotate documents, and (ii) semantic relations, s"
N19-1136,N18-2041,0,0.0185216,"2.1 Hierarchical Attention Hierarchical Attention captures the structure of a document by a word-level attention on each word’s hidden state to create a sentence representation, then a sentence-level attention to form a content representation (Yang et al., 2016). The attention coefficients are computed based on the dot product between a non-linearly transformed weight vector of the hidden state and an “informative” vector, which encodes “what is the most informative word (or sentence)” in the sequence. This “informative” vector is commonly treated as a sequence of weights (Yang et al., 2016; Kumar et al., 2018; Hassan et al., 2018), trained along with other weights in the network. We applied parallelled word-level attention on the title and each sentence in the content. The attention coefficient and the final representation of a sequence is calculated as (taking words in title as an example): ct = X i αi hi = X i exp(vwt • vi ) P hi j exp(vwt • vj ) (1) where vi = tanh(Wt hi + bt ) is the output of a fully-connected layer of the hidden state hi for each word in the title, vwt is the “informative” vector for titles, and ct is the resulting title representation. We can compute each sentence represent"
N19-1136,N16-1063,0,0.0567716,"Missing"
N19-1136,C16-1284,0,0.072019,"Missing"
N19-1136,N16-1174,0,0.742354,"e time-consuming. This has driven research on document annotation based on existing tag sets (Bel´em et al., 2017; Nie et al., 2014). Recent studies formalise the automated social annotation task as a multi-label classification problem (Gibaja and Ventura, 2015) and apply deep learning approaches (Li et al., 2016; Huang et al., 2016; Hassan et al., 2018). A strong baseline is the use of Bi-directional RNN (Schuster and Paliwal, 1997) with GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997). Another more recent improvement is achieved through Hierarchical Attention Network (HAN) (Yang et al., 2016) which discriminates important words and sentences from others, as adapted in (Hassan et al., 2018) for annotation. These models, however, suffer from two issues: (i) simply scanning over the words and sentences, the models do not fully mimic the way users read and annotate documents, and (ii) semantic relations, similarity and subsumption, among the labels are not considered. Our model focuses on simulating users’ reading and annotation behaviour with attention mechanisms. The title of a document is highly abstract while informative about the topics and has a direct impact on users’ annotatio"
P06-1121,N04-1014,1,\N,Missing
P06-1121,N04-1035,1,\N,Missing
P06-1121,C00-2092,0,\N,Missing
P06-1121,P01-1067,1,\N,Missing
P06-1121,J04-4002,0,\N,Missing
P06-1121,P05-1033,0,\N,Missing
P06-1121,J97-3002,0,\N,Missing
P06-1121,W02-1039,0,\N,Missing
P06-1121,J08-3004,1,\N,Missing
P06-1121,N06-1033,1,\N,Missing
P10-2055,H05-1115,0,0.0842511,"Missing"
P12-1061,P08-1081,0,0.0220649,"n, the semantic interactions between different sentence sites are crucial, that is, some context co-occurrences should be encouraged and others should be penalized for requirements of information novelty and non-redundancy in the generated summary. Here we consider both local (sentences from the same answer) and global (sentences from different answers) settings. This give rise to four contextual factors that we will explore for modeling the pairwise semantic interactions based on question segmentation. In this paper, we utilize a simple but effective lightweight question segmentation method (Ding et al., 2008; Wang et al., 2010). It mainly involves the following two steps: Step 1. Question sentence detection: every sentence in the original multi-sentence question is classified into question sentence and non-question (context) sentence. The question mark and 5W1H features are applied. Step 2. Context assignment: every context sentence is assigned to the most relevant question sentence. We compute the semantic similarity(Simpson and Crowe, 2005) between sentences or sub quesFigure 1: Four kinds of the contextual factors are considered for answer summarization in our general CRF based models. tions a"
P12-1061,W06-1643,0,0.0707576,"Missing"
P12-1061,W04-1013,0,0.00952528,"Missing"
P12-1061,C08-1063,0,0.377495,"Missing"
P12-1061,P07-1059,0,0.0800987,"Missing"
P12-1061,P10-1078,0,0.435579,"Missing"
P12-1061,zhou-etal-2006-summarizing,0,0.032358,"Missing"
P12-1061,J00-4006,0,\N,Missing
P18-1151,D14-1067,0,0.0283216,"RDF triples via the predicate. This representation allows easy data share between KBs. However, usually the elements of a triple are stored as Uniform Resource Identifiers (URIs), and many predicates (words or phrases) are not intuitive; this representation is difficult to comprehend by humans. Translating RDF triples into natural sentences helps humans to comprehend the knowledge embedded in the triples, and building a natural language based user interface is an important task in user interaction studies (Damljanovic et al., 2010). This task has many applications, such as question answering (Bordes et al., 2014; Fader et al., 2014), profile summarizing (Lebret et al., 2016; Chisholm et al., 2017), and automatic weather forecasting (Mei et al., 2016). For example, the SPARQL inference of a Q&A system (Unger et al., 2012) returns a set of RDF triples which need to be translated into natural sentences to provide a more readable answer for the users. Table 1 illustrates such an example. Suppose a user is asking a question about “John Doe”. By querying a KB, a Q&A system retrieves three triples “hJohn Doe,birth place,Londoni”, “hJohn Doe,birth date,1967-01-10i”, and “hLondon,capital of,Englandi.” We aim"
P18-1151,E17-1060,0,0.0315995,"s. However, usually the elements of a triple are stored as Uniform Resource Identifiers (URIs), and many predicates (words or phrases) are not intuitive; this representation is difficult to comprehend by humans. Translating RDF triples into natural sentences helps humans to comprehend the knowledge embedded in the triples, and building a natural language based user interface is an important task in user interaction studies (Damljanovic et al., 2010). This task has many applications, such as question answering (Bordes et al., 2014; Fader et al., 2014), profile summarizing (Lebret et al., 2016; Chisholm et al., 2017), and automatic weather forecasting (Mei et al., 2016). For example, the SPARQL inference of a Q&A system (Unger et al., 2012) returns a set of RDF triples which need to be translated into natural sentences to provide a more readable answer for the users. Table 1 illustrates such an example. Suppose a user is asking a question about “John Doe”. By querying a KB, a Q&A system retrieves three triples “hJohn Doe,birth place,Londoni”, “hJohn Doe,birth date,1967-01-10i”, and “hLondon,capital of,Englandi.” We aim to generate a natural sentence that incorporates the information of the triples and is"
P18-1151,D14-1179,0,0.0866571,"Missing"
P18-1151,W13-2102,0,0.115121,"Missing"
P18-1151,P11-2031,0,0.0494468,"Missing"
P18-1151,W11-2107,0,0.0463865,"Missing"
P18-1151,W13-0108,0,0.084656,"a and Wilks (2004) follow a 1628 traditional NLG approach to generate sentences from RDF data in the medical domain. They start with filtering repetitive RDF data (document planning) and then group coherent triples (microplanning). After that, they aggregate the sentences generated for coherent triples to produce the final sentences (aggregation and realization). Cimiano et al. (2013) generate cooking recipes from semantic web data. They focus on using a large corpus to extract lexicon in the cooking domain. The lexicon is then used with a traditional NLG approach to generate cooking recipes. Duma and Klein (2013) learn a sentence template from a parallel RDF data and text corpora. They first align entities in RDF triples with entities mentioned in sentences. Then, they extract templates from the aligned sentences by replacing the entity mention with a unique token. This method works well on RDF triples in a seen domain but fails on RDF triples in a previously unseen domain. Recently, several methods using neural networks are proposed. Lebret et al. (2016) generate the first sentence of a biography using a conditional neural language model. This model is trained to predict the next word of a sentence n"
P18-1151,P17-1017,0,0.261835,"coder model by first concatenating the elements of the RDF triples into a linear sequence and then feeding the sequence as the model input to learn the corresponding natural sentence. We implemented such a model (detailed in Section 3.2) that ranked top in the WebNLG Challenge 20172 . This Challenge has a primary objective of generating syntactically correct natural sentences from a set of RDF triples. Our model achieves the highest global scores on the automatic evaluation, outperforming competitors that use rule-based methods, statistical machine translation, and neural machine translation (Gardent et al., 2017b). While our previous model achieves a good result, simply concatenating the elements in the RDF triples may lose the relationship between entities that affects the semantics of the resulting sentence (cf. Table 3). To address this issue, in this paper, we propose a novel graph-based triple encoder model that maintain the structure of RDF triples as a small knowledge graph named the GTR-LSTM model. This model computes the hidden state of each entity in a graph to preserve the relationships between entities in a triple (intra-triple relationships) and the relationships between entities in rela"
P18-1151,W17-3518,0,0.449094,"coder model by first concatenating the elements of the RDF triples into a linear sequence and then feeding the sequence as the model input to learn the corresponding natural sentence. We implemented such a model (detailed in Section 3.2) that ranked top in the WebNLG Challenge 20172 . This Challenge has a primary objective of generating syntactically correct natural sentences from a set of RDF triples. Our model achieves the highest global scores on the automatic evaluation, outperforming competitors that use rule-based methods, statistical machine translation, and neural machine translation (Gardent et al., 2017b). While our previous model achieves a good result, simply concatenating the elements in the RDF triples may lose the relationship between entities that affects the semantics of the resulting sentence (cf. Table 3). To address this issue, in this paper, we propose a novel graph-based triple encoder model that maintain the structure of RDF triples as a small knowledge graph named the GTR-LSTM model. This model computes the hidden state of each entity in a graph to preserve the relationships between entities in a triple (intra-triple relationships) and the relationships between entities in rela"
P18-1151,W08-0510,0,0.0386482,"implement the existing models, the adapted model, and the proposed model using Keras3 . We use three common evaluation metrics including BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006). For the metric computation and significance testing, we use MultEval (Clark et al., 2011). 4.1 Tested Models We compare our proposed graph-based triple encoder (GTR-LSTM, Section 3.4) with three existing model including the adapted standard BLSTM encoder (BLSTM, Section 3.2), Neural Wikipedian (Vougiouklis et al., 2017) (TFF), and statistical machine translation (Hoang and Koehn, 2008) (SMT) trained on a 6-gram language model. We also compare with the adapted standard triple encoder (TLSTM, Section 3.3). 4.2 Hyperparameters We use grid search to find the best hyperparameters for the neural networks. We use GloVe (Pennington et al., 2014) trained on the GKB and WebNLG training data and full English Wikipedia data dump to get 300-dimension word embeddings. We use 512 hidden units for both encoder and decoder. We use a 0.5 dropout rate for regularization on both encoder and decoder to avoid overfitting. We train our model on NVIDIA Tesla K40c. We find that using adaptive learn"
P18-1151,D16-1128,0,0.114517,"Missing"
P18-1151,D15-1166,0,0.0247375,"ph can contain cycles that cause difficulty in determining the starting and ending vertices. Our traversal procedure ensures that the hidden states of all vertices are updated based on their adjacent vertices (local neighbors). To further capture the global information of the graph, we apply an attention model on the GTR-LSTM triple encoder. The attention model takes the hidden states of all vertices computed by the encoder and the previous hidden state of the decoder to compute the final input vector of each decoder time-step. Figure 5 illustrates the attention model of GTR-LSTM. Inspired by Luong et al. (2015), we adapt the following equation to compute the weight of each vertex. T exp(hd t W xn ) αn = P|X| |X| X d T j=1 exp(h t W xj ) (8) Decoder The decoder of the proposed framework is a standard LSTM. It is trained to generate the output sequence by predicting the next output word wt conditioned on the hidden state hd t . The current hidden state hd t is conditioned on the hidden state of the previous time-step hd t−1 , the output of the previous time-step wt−1 , and input vector representation hT . The hidden state and the output of the decoder at time-step t are computed as: hd t = f (hd t−1 ,"
P18-1151,N16-1086,0,0.421463,"Uniform Resource Identifiers (URIs), and many predicates (words or phrases) are not intuitive; this representation is difficult to comprehend by humans. Translating RDF triples into natural sentences helps humans to comprehend the knowledge embedded in the triples, and building a natural language based user interface is an important task in user interaction studies (Damljanovic et al., 2010). This task has many applications, such as question answering (Bordes et al., 2014; Fader et al., 2014), profile summarizing (Lebret et al., 2016; Chisholm et al., 2017), and automatic weather forecasting (Mei et al., 2016). For example, the SPARQL inference of a Q&A system (Unger et al., 2012) returns a set of RDF triples which need to be translated into natural sentences to provide a more readable answer for the users. Table 1 illustrates such an example. Suppose a user is asking a question about “John Doe”. By querying a KB, a Q&A system retrieves three triples “hJohn Doe,birth place,Londoni”, “hJohn Doe,birth date,1967-01-10i”, and “hLondon,capital of,Englandi.” We aim to generate a natural sentence that incorporates the information of the triples and is easier to be understood by the user. In this example,"
P18-1151,P02-1040,0,0.105728,"Missing"
P18-1151,D14-1162,0,0.0878415,"ation and significance testing, we use MultEval (Clark et al., 2011). 4.1 Tested Models We compare our proposed graph-based triple encoder (GTR-LSTM, Section 3.4) with three existing model including the adapted standard BLSTM encoder (BLSTM, Section 3.2), Neural Wikipedian (Vougiouklis et al., 2017) (TFF), and statistical machine translation (Hoang and Koehn, 2008) (SMT) trained on a 6-gram language model. We also compare with the adapted standard triple encoder (TLSTM, Section 3.3). 4.2 Hyperparameters We use grid search to find the best hyperparameters for the neural networks. We use GloVe (Pennington et al., 2014) trained on the GKB and WebNLG training data and full English Wikipedia data dump to get 300-dimension word embeddings. We use 512 hidden units for both encoder and decoder. We use a 0.5 dropout rate for regularization on both encoder and decoder to avoid overfitting. We train our model on NVIDIA Tesla K40c. We find that using adaptive learning rates for the optimization is efficient and leads the model to converge faster. Thus, we use Adam (Kingma and Ba, 2015) with a learning rate of 0.0002 instead of stochastic gradient descent. The update of parameters in training is computed using a mini"
P18-1151,2006.amta-papers.25,0,0.0590521,"Missing"
P18-1151,P15-1150,0,0.150393,"Missing"
P18-1158,P16-1223,0,0.391132,"e understanding of natural languages and the ability to do further inference and reasoning. Restricted by the limited volume of the annotated dataset, early studies mainly rely on a pipeline of NLP models to complete this task, such as semantic parsing and linguistic annotation (Das et al., 2014). Not until the release of large-scale clozestyle dataset, such as Children’s Book Test (Hill et al., 2015) and CNN/Daily Mail (Hermann et al., 2015), some preliminary end-to-end deep learning methods have begun to bloom and achieve superior results in reading comprehension task (Hermann et al., 2015; Chen et al., 2016; Cui et al., 2016). However, these cloze-style datasets still have their limitations, where the goal is to predict the single missing word (often a named entity) in a passage. It requires less reasoning than previously thought and no need to comprehend the whole passage (Chen et al., 2016). Therefore, Stanford publish a new large-scale dataset SQuAD (Rajpurkar et al., 2016), in which all the question and answers are manually created through crowdsourcing. Different from cloze-style reading comprehension dataset, SQuAD constrains answers to all possible text spans within the reference passage,"
P18-1158,P17-1171,0,0.287081,"bility of SQuAD benchmark dataset, rapid progress has been made these years. The work (Wang and Jiang, 2016) and (Seo et al., 2016) are among the first to investigate into this dataset, where Wang and Jiang propose an end-to-end architecture based on match-LSTM and pointer networks (Wang and Jiang, 2016), and Seo et al. introduce the bi-directional attention flow network which captures the questiondocument context at different levels of granularity (Seo et al., 2016). Chen et al. devise a simple and effective document reader, by introducing a bilinear match function and a few manual features (Chen et al., 2017a). Wang et al. propose 1705 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1705–1714 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics a gated attention-based recurrent network where self-match attention mechanism is first incorporated (Wang et al., 2017). In (Liu et al., 2017b) and (Shen et al., 2017), the multi-turn memory networks are designed to simulate multi-step reasoning in machine reading comprehension. The idea of our approach derives from the normal human reading pattern. First, p"
P18-1158,D16-1244,0,0.162573,"Missing"
P18-1158,D14-1162,0,0.087377,". Experiments conducted on SQuAD and adversarial example datasets (Jia and Liang, 2017) demonstrate that the proposed framework outperform previous methods by a large margin. Details of different components will be described in the following sections. 3.4 Language Model & Encoder Layer Encoder layer of the model transform the discrete word tokens of question and passage to a sequence of continuous vector representations. We use a pre-trained word embedding model and a char embedding model to lay the foundation for our model. For the word embedding model, we adopt the popular glove embeddings (Pennington et al., 2014) which are widely used in deep learning-based NLP domain. For the char embedding model, the ELMo language model (Peters et al., 2018) is used due to its superior performance in a wide range of NLP tasks. As a result, we obtain twontypes o of encoding n on vectors, i.e., word Q m embeddings et , eP and char embedt n om n t=1 on t=1 dings cQ , cP . t t t=1 t=1 To further utilize contextual cues from surrounding words to refine the embedding of the words, we then put a shared Bi-LSTM network on top of the embeddings provided by the previous layers to model the temporal interactions between words."
P18-1158,N18-1202,0,0.612394,"ntation for questions and passages separately; 2) attention layer in which hierarchical attention networks are designed to capture the relation between question and passage at different levels of granularity; 3) match layer where refined question and passage are matched under a pointer-network (Vinyals et al., 2015) answer boundary predictor. In encoder layer, to better represent the questions and passages in multiple aspects, we combine two different embeddings to give the fundamental word representations. In addition to the typical glove word embeddings, we also utilize the ELMo embeddings (Peters et al., 2018) derived from a pre-trained language model, which shows superior performance in a wide range of NLP problems. Different from the original fusion way for intermediate layer representations, we design a representation-aware fusion method to compute the output ELMo embeddings and the context information is also incorporated by further passing through a bi-directional LSTM network. The key in machine reading comprehension solution lies in how to incorporate the question context into the paragraph, in which attention mechanism is most widely used. Recently, many different attention functions and ty"
P18-1158,D16-1264,0,0.773254,"n’s Book Test (Hill et al., 2015) and CNN/Daily Mail (Hermann et al., 2015), some preliminary end-to-end deep learning methods have begun to bloom and achieve superior results in reading comprehension task (Hermann et al., 2015; Chen et al., 2016; Cui et al., 2016). However, these cloze-style datasets still have their limitations, where the goal is to predict the single missing word (often a named entity) in a passage. It requires less reasoning than previously thought and no need to comprehend the whole passage (Chen et al., 2016). Therefore, Stanford publish a new large-scale dataset SQuAD (Rajpurkar et al., 2016), in which all the question and answers are manually created through crowdsourcing. Different from cloze-style reading comprehension dataset, SQuAD constrains answers to all possible text spans within the reference passage, which requires more logical reasoning and content understanding. Benefiting from the availability of SQuAD benchmark dataset, rapid progress has been made these years. The work (Wang and Jiang, 2016) and (Seo et al., 2016) are among the first to investigate into this dataset, where Wang and Jiang propose an end-to-end architecture based on match-LSTM and pointer networks (W"
P18-1158,D17-1215,0,0.145415,"n layer with shallow semantic fusion, a self-attention layer with deep semantic fusion and a memorywise bilinear alignment function. The proposed network has two distinctive characteristics: (i) A fine-grained fusion approach to blend attention vectors for a better understanding of the relationship between question and passage; (ii) A multi-granularity attention mechanism applied at the word and sentence-level, enabling it to properly attend to the most important content when constructing the question and passage representation. Experiments conducted on SQuAD and adversarial example datasets (Jia and Liang, 2017) demonstrate that the proposed framework outperform previous methods by a large margin. Details of different components will be described in the following sections. 3.4 Language Model & Encoder Layer Encoder layer of the model transform the discrete word tokens of question and passage to a sequence of continuous vector representations. We use a pre-trained word embedding model and a char embedding model to lay the foundation for our model. For the word embedding model, we adopt the popular glove embeddings (Pennington et al., 2014) which are widely used in deep learning-based NLP domain. For t"
P18-1158,P17-1147,0,0.0446096,"start and end position is the sum of the negative log probabilities of the true start and end indices by the predicted distributions, averaged over all examples: N 1 X L(θ) = − log ps (yis ) + log pe (yie ) N i Experiments In this section, we first present the datasets used for evaluation. Then we compare our end-to-end Hierarchical Attention Fusion Networks with existing machine reading models. Finally, we conduct experiments to validate the effectiveness of our proposed components. We evaluate our model on the task of question answering using recently released SQuAD and TriviaQA Wikipedia (Joshi et al., 2017), which have gained a huge attention over the past year. An adversarial evaluation for the Stanford Question Answering SQuAD is also used to demonstrate the robust of our model under adversarial attacks (Jia and Liang, 2017). 4.1 Single model LR Baseline (Rajpurkar et al., 2016) Match-LSTM (Wang and Jiang, 2016) DrQA (Chen et al., 2017a) DCN+ (Xiong et al., 2017) Interactive AoA Reader+ (Cui et al., 2016) FusionNet (Huang et al., 2017) SAN (Liu et al., 2017b) AttentionReader+ (unpublished) BiDAF + Self Attention + ELMo (Peters et al., 2018) r-net+ (Wang et al., 2017) SLQA+ Ensemble model Fusio"
P18-1158,P16-1086,0,0.0402715,"for MRC first encodes the symbolic representation of the question and passage in an embedding space, then identify answers with particular attention functions in that space. In terms of the question and passage attention or matching strategy, we roughly categorize these attention-based models into two large groups: one-way attention and two-way attention. In one-way attention model, question is first summarized into a single vector and then directly matched with the passage. Most of the end-toend neural network methods on the cloze-style datasets are based on this model (Hermann et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Dhingra et al., 2016). Hermann et al. are the first to apply the attention-based neural network methods to MRC task and introduce an attentive reader and an impatient reader (Hermann et al., 2015), by leveraging a two layer LSTM network. Chen et al. (Chen et al., 2016) further design a bilinear attention function based on the attentive reader, which shows superior performance on CNN/Daily Mail dataset. However, part of information may be lost when summarizing the question and a finegrained attention on both the question and passage words should be more reasonable. Therefor"
P18-1158,P16-2022,0,0.0341121,"unit tations Q has been designed to combine the original contextual representations and the corresponding attention vectors for question and passage separately: ˜ P0 = Fuse(P, Q) (8) ˜ Q0 = Fuse(Q, P) (9) P > Q > > P Sij = Att(uQ t , ut ) = ReLU(Wlin ut ) · ReLU(Wlin ut ) (3) where Fuse(·, ·) is a typical fusion kernel. The simplest way of fusion is a concatenation or addition of the two representations, followed by some linear or non-linear transformation. Recently, a heuristic matching trick with difference and element-wise product is found effective in combining different representations (Mou et al., 2016; Chen et al., 2017b): where Wlin is a trainable weight matrix. This decomposition avoids the quadratic complexity that is trivially parallelizable (Parikh et al., 2016). Now we use the unnormalized attention weights Sij to compute the attentions between question and passage, which is further used to obtain the attended vectors in passage to question and question to passage direction, respectively. P2Q Attention signifies which question words are most relevant to each passage word, given as below: αj = softmax(S:j ) (4) ˜ = tanh(Wf [P; Q; ˜ P ◦ Q; ˜ P − Q] ˜ + bf ) m(P, Q) (10) where ◦ denotes"
P18-1158,P17-1018,0,0.588978,"on flow network which captures the questiondocument context at different levels of granularity (Seo et al., 2016). Chen et al. devise a simple and effective document reader, by introducing a bilinear match function and a few manual features (Chen et al., 2017a). Wang et al. propose 1705 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1705–1714 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics a gated attention-based recurrent network where self-match attention mechanism is first incorporated (Wang et al., 2017). In (Liu et al., 2017b) and (Shen et al., 2017), the multi-turn memory networks are designed to simulate multi-step reasoning in machine reading comprehension. The idea of our approach derives from the normal human reading pattern. First, people scan through the whole passage to catch a glimpse of the main body of the passage. Then with the question in mind, people make connection between passage and question, and understand the main intent of the question related with the passage theme. A rough answer span is then located from the passage and the attention can be focused on to the located co"
P18-1158,P17-1055,0,\N,Missing
P18-1158,P18-1224,0,\N,Missing
P19-1123,D11-1033,0,0.592961,"eural machine translation (NMT), thanks to better modeling and data. As a result, NMT has found successful use cases in, for example, domain translation and helping other NLP applications, e.g., (Buck et al., 2018; McCann et al., 2017). As these tasks start to scale to more domains, a challenge starts to surface: Given a source monolingual corpus, how to use it to improve an NMT model to translate same-domain sentences well? Data selection plays an important role in this context. In machine translation, data selection has been a fundamental research topic. One idea (van der Wees et al., 2017; Axelrod et al., 2011) for this problem is to use language models to select parallel data out of a background parallel corpus, seeded by the source monolingual sentences. This approach, however, performs poorly on noisy data, such as large-scale, web-crawled datasets, because data noise hurts NMT performance (Khayrallah and Koehn, 2018). The lower learning curve in Figure 1 shows the effect of noise on domain-data selection. NMT community has realized the harm of data noise to translation quality, leading to efforts in data denoising (Koehn et al., 2018), as has been popular in computer vision (Hendrycks et al., 20"
P19-1123,J93-2003,0,0.0889569,"duced to evolve the training criteria Qt over time (Zhang et al., 2018; Wang et al., 2018b; van der Wees et al., 2017; Kocmi and Bojar, 2017; Platanios et al., 2019). In these curricula, tasks in M are sequenced in order of increasing relevance. Earlier tasks are exposed to a diversity of examples and later tasks progressively concentrate on data subsets more relevant to the final task. 2.3 More Related Work Junczys-Dowmunt (2018) introduces a practical and effective method to combine (static) features for data filtering. Mansour et al. (2011) combine an n-gram LM and IBM translation Model 1 (Brown et al., 1993) for domain data filtering. We compose different types of dynamic online selection rather than combining static features. Back translation (BT), e.g., (Sennrich et al., 2016), is another important approach to using monolingual data for NMT. Here we use monolingual data to seed data selection, rather than generating parallel data directly from it. Furthermore, we study the use of source-language monolingual data, in which case BT cannot be applied directly. 3 Problem Setting ] D XY is a background parallel dataset between languages X and Y . It may be crawled from the web: large (hundreds of mi"
P19-1123,K16-1031,0,0.0469489,"P (y|x) , ∀(x, y) ∈ D (3) Measuring Domain and Noise in Data Data selection for MT usually uses a scoring function to rank sentence pairs. Cross entropy difference (Moore and Lewis, 2010) between two language models is usually used for selecting domain sentences, e.g., (van der Wees et al., 2017; Axelrod et al., 2011). For a source sentence x of length |x|, with a general-domain language model (LM), e and an in-domain LM, ϑ, b the parameterized as ϑ, 1 domain-relevance of x is calculated as:       log P x; ϑb − log P x; ϑe e ϑb = ϕ x; ϑ, (1) |x| Alternative measures (Wang et al., 2017; Chen and Huang, 2016; Chen et al., 2016) also show effectiveness. With Eq. 1 to select data, the data distribution (domain quality) in the in-domain monolinb is transferred into gual data used to train P (x; ϑ) the selected data through the scoring. Data selection has also been used for data denoising (Junczys-Dowmunt, 2018; Wang et al., 2018b), by using NMT models and trusted data to measure the noise level in a sentence pair. One e such a scoring function uses a baseline NMT, θ, b trained on noisy data and a cleaner NMT, θ, obtained by fine-tuning θe on a small trusted parallel dataset, and measures quality in"
P19-1123,2016.amta-researchers.8,0,0.0884002,"(3) Measuring Domain and Noise in Data Data selection for MT usually uses a scoring function to rank sentence pairs. Cross entropy difference (Moore and Lewis, 2010) between two language models is usually used for selecting domain sentences, e.g., (van der Wees et al., 2017; Axelrod et al., 2011). For a source sentence x of length |x|, with a general-domain language model (LM), e and an in-domain LM, ϑ, b the parameterized as ϑ, 1 domain-relevance of x is calculated as:       log P x; ϑb − log P x; ϑe e ϑb = ϕ x; ϑ, (1) |x| Alternative measures (Wang et al., 2017; Chen and Huang, 2016; Chen et al., 2016) also show effectiveness. With Eq. 1 to select data, the data distribution (domain quality) in the in-domain monolinb is transferred into gual data used to train P (x; ϑ) the selected data through the scoring. Data selection has also been used for data denoising (Junczys-Dowmunt, 2018; Wang et al., 2018b), by using NMT models and trusted data to measure the noise level in a sentence pair. One e such a scoring function uses a baseline NMT, θ, b trained on noisy data and a cleaner NMT, θ, obtained by fine-tuning θe on a small trusted parallel dataset, and measures quality in a sentence pair (x,"
P19-1123,kocmi-bojar-2017-curriculum,0,0.158135,"ntermediate tasks, mt , are sorted in increasing relevance to mf as a series of “stepping stones” to mf , making curriculum learning a form of transfer learning that transfers knowledge through M to benefit mf . A performance metric P(C, mf ) is used to evaluate mf . There has already been rich research in CL for NMT. Fine-tuning a baseline on in-domain parallel data is a good strategy (Thompson et al., 2018; Sajjad et al., 2017; Freitag and Al-Onaizan, 2016). van der Wees et al. (2017) introduce a domain curriculum. Wang et al. (2018b) define noise level and introduce a denoising curriculum. Kocmi and Bojar (2017) use linguistically-motivated features to classify examples into bins for scheduling. Kumar et al. (2019) use reinforcement learning to learn a denoising curriculum based on noise level of examples. Zhang et al. (2018) explore CL in general for NMT and observe faster training convergence. Zhang et al. (2019) use CL to adapt generic NMT models to a specific domain. Platanios et al. (2019) propose a CL framework to simplify and speed up training and achieve better results; a nice study in sampling schedules was carried out. CL therefore is a natural formulation for dynamic online data selection."
P19-1123,W18-6453,0,0.0424644,"mental research topic. One idea (van der Wees et al., 2017; Axelrod et al., 2011) for this problem is to use language models to select parallel data out of a background parallel corpus, seeded by the source monolingual sentences. This approach, however, performs poorly on noisy data, such as large-scale, web-crawled datasets, because data noise hurts NMT performance (Khayrallah and Koehn, 2018). The lower learning curve in Figure 1 shows the effect of noise on domain-data selection. NMT community has realized the harm of data noise to translation quality, leading to efforts in data denoising (Koehn et al., 2018), as has been popular in computer vision (Hendrycks et al., 2018). The upper curve in Figure 1 shows the effect of clean-data selection on the same noisy data. These denoising methods, however, cannot be directly used for the problem in question as they require trusted parallel data as input. We introduce a method to dynamically combine clean-data selection and domain-data selection. We treat them as independent curricula, and compose them into a “co-curriculum”. We summarize our contributions as: 1. “Co-curricular learning”, for transfer learning across data quality. It extends the single cur"
P19-1123,P18-1007,0,0.0250517,"omponent of φ θbi = θb∗ Setup We consider two background datasets and two test domains, so we have four experiment configurations. Each configuration has as inputs a background dataset, an in-domain source-language corpus and a (small) trusted parallel dataset that is out-of-domain. The inputs of a configuration are shown in Figure 2. As alternative background datasets, we use the English→French Paracrawl data,5 (300 million pairs), and the WMT14 training data (40 million pairs). The former is severely noisier than the later. We adopt sentence-piece model and apply open-source implementation (Kudo, 2018) to segment data into sub-word units with a source-target shared 32000 sub-word vocabulary. We use two test domains: the English→French IWSLT15 test set, in spoken language domain; and the English→French WMT14 test set, in news domain. For IWSLT15, we use the English side of its provided parallel training data (220 thousand ID , but use the parallel version as examples) as DX [ OD for the WMT14 domain. The IWSLT14 test D XY i = i+1 θbi is then compared against the original θe for scoring. The updated φ and the constant ϕ work together to generate a new co-curriculum in the next iteration going"
P19-1123,N19-1208,0,0.253891,"Missing"
P19-1123,2011.iwslt-papers.5,0,0.0220136,"batch sampling is important for CL. Several alternatives have been introduced to evolve the training criteria Qt over time (Zhang et al., 2018; Wang et al., 2018b; van der Wees et al., 2017; Kocmi and Bojar, 2017; Platanios et al., 2019). In these curricula, tasks in M are sequenced in order of increasing relevance. Earlier tasks are exposed to a diversity of examples and later tasks progressively concentrate on data subsets more relevant to the final task. 2.3 More Related Work Junczys-Dowmunt (2018) introduces a practical and effective method to combine (static) features for data filtering. Mansour et al. (2011) combine an n-gram LM and IBM translation Model 1 (Brown et al., 1993) for domain data filtering. We compose different types of dynamic online selection rather than combining static features. Back translation (BT), e.g., (Sennrich et al., 2016), is another important approach to using monolingual data for NMT. Here we use monolingual data to seed data selection, rather than generating parallel data directly from it. Furthermore, we study the use of source-language monolingual data, in which case BT cannot be applied directly. 3 Problem Setting ] D XY is a background parallel dataset between lan"
P19-1123,P10-2041,0,0.165108,"(Bengio et al., 2009) has been used to further improve traditional static selection. In CL, a curriculum, C, is a sequence of training criteria over training steps. A training criterion, Qt (y|x), at step t is associated with a set of weights, Wt (x, y), over training examples (x, y) in a dataset D, where y is the translation for x. Qt (y|x) is a re-weighting of the training distribution P (y|x): Related Work Qt (y|x) ∝ Wt (x, y) P (y|x) , ∀(x, y) ∈ D (3) Measuring Domain and Noise in Data Data selection for MT usually uses a scoring function to rank sentence pairs. Cross entropy difference (Moore and Lewis, 2010) between two language models is usually used for selecting domain sentences, e.g., (van der Wees et al., 2017; Axelrod et al., 2011). For a source sentence x of length |x|, with a general-domain language model (LM), e and an in-domain LM, ϑ, b the parameterized as ϑ, 1 domain-relevance of x is calculated as:       log P x; ϑb − log P x; ϑe e ϑb = ϕ x; ϑ, (1) |x| Alternative measures (Wang et al., 2017; Chen and Huang, 2016; Chen et al., 2016) also show effectiveness. With Eq. 1 to select data, the data distribution (domain quality) in the in-domain monolinb is transferred into gual data"
P19-1123,N19-1119,0,0.108033,"Missing"
P19-1123,W18-6478,0,0.513137,"od et al., 2011). For a source sentence x of length |x|, with a general-domain language model (LM), e and an in-domain LM, ϑ, b the parameterized as ϑ, 1 domain-relevance of x is calculated as:       log P x; ϑb − log P x; ϑe e ϑb = ϕ x; ϑ, (1) |x| Alternative measures (Wang et al., 2017; Chen and Huang, 2016; Chen et al., 2016) also show effectiveness. With Eq. 1 to select data, the data distribution (domain quality) in the in-domain monolinb is transferred into gual data used to train P (x; ϑ) the selected data through the scoring. Data selection has also been used for data denoising (Junczys-Dowmunt, 2018; Wang et al., 2018b), by using NMT models and trusted data to measure the noise level in a sentence pair. One e such a scoring function uses a baseline NMT, θ, b trained on noisy data and a cleaner NMT, θ, obtained by fine-tuning θe on a small trusted parallel dataset, and measures quality in a sentence pair (x, y):       log P y|x; θb −log P y|x; θe e θb = φ x, y; θ, (2) |y| Using NMT models for selection can also lead to faster convergence (Wang et al., 2018a). With Eq. 2, the distribution (data quality) in the trusted parallel data is transferred into the selected data. These scoring"
P19-1123,E17-2045,0,0.122457,"ples data from Qt to train on, resulting in a task (or model), mt . Therefore, C corresponds to a sequence of tasks, M = hm1 , ..., mt ..., mf i, where mf is the final task of interest. Intermediate tasks, mt , are sorted in increasing relevance to mf as a series of “stepping stones” to mf , making curriculum learning a form of transfer learning that transfers knowledge through M to benefit mf . A performance metric P(C, mf ) is used to evaluate mf . There has already been rich research in CL for NMT. Fine-tuning a baseline on in-domain parallel data is a good strategy (Thompson et al., 2018; Sajjad et al., 2017; Freitag and Al-Onaizan, 2016). van der Wees et al. (2017) introduce a domain curriculum. Wang et al. (2018b) define noise level and introduce a denoising curriculum. Kocmi and Bojar (2017) use linguistically-motivated features to classify examples into bins for scheduling. Kumar et al. (2019) use reinforcement learning to learn a denoising curriculum based on noise level of examples. Zhang et al. (2018) explore CL in general for NMT and observe faster training convergence. Zhang et al. (2019) use CL to adapt generic NMT models to a specific domain. Platanios et al. (2019) propose a CL framew"
P19-1123,W18-2709,0,0.0243429,"ace: Given a source monolingual corpus, how to use it to improve an NMT model to translate same-domain sentences well? Data selection plays an important role in this context. In machine translation, data selection has been a fundamental research topic. One idea (van der Wees et al., 2017; Axelrod et al., 2011) for this problem is to use language models to select parallel data out of a background parallel corpus, seeded by the source monolingual sentences. This approach, however, performs poorly on noisy data, such as large-scale, web-crawled datasets, because data noise hurts NMT performance (Khayrallah and Koehn, 2018). The lower learning curve in Figure 1 shows the effect of noise on domain-data selection. NMT community has realized the harm of data noise to translation quality, leading to efforts in data denoising (Koehn et al., 2018), as has been popular in computer vision (Hendrycks et al., 2018). The upper curve in Figure 1 shows the effect of clean-data selection on the same noisy data. These denoising methods, however, cannot be directly used for the problem in question as they require trusted parallel data as input. We introduce a method to dynamically combine clean-data selection and domain-data se"
P19-1123,P16-1009,0,0.464182,"hese curricula, tasks in M are sequenced in order of increasing relevance. Earlier tasks are exposed to a diversity of examples and later tasks progressively concentrate on data subsets more relevant to the final task. 2.3 More Related Work Junczys-Dowmunt (2018) introduces a practical and effective method to combine (static) features for data filtering. Mansour et al. (2011) combine an n-gram LM and IBM translation Model 1 (Brown et al., 1993) for domain data filtering. We compose different types of dynamic online selection rather than combining static features. Back translation (BT), e.g., (Sennrich et al., 2016), is another important approach to using monolingual data for NMT. Here we use monolingual data to seed data selection, rather than generating parallel data directly from it. Furthermore, we study the use of source-language monolingual data, in which case BT cannot be applied directly. 3 Problem Setting ] D XY is a background parallel dataset between languages X and Y . It may be crawled from the web: large (hundreds of millions of pairs), diverse and noisy. ID is an in-domain monolingual corpus in DX source language X. It contains thousands to millions of sentences and specifies the testing d"
P19-1123,W18-6313,0,0.039819,", an online learner samples data from Qt to train on, resulting in a task (or model), mt . Therefore, C corresponds to a sequence of tasks, M = hm1 , ..., mt ..., mf i, where mf is the final task of interest. Intermediate tasks, mt , are sorted in increasing relevance to mf as a series of “stepping stones” to mf , making curriculum learning a form of transfer learning that transfers knowledge through M to benefit mf . A performance metric P(C, mf ) is used to evaluate mf . There has already been rich research in CL for NMT. Fine-tuning a baseline on in-domain parallel data is a good strategy (Thompson et al., 2018; Sajjad et al., 2017; Freitag and Al-Onaizan, 2016). van der Wees et al. (2017) introduce a domain curriculum. Wang et al. (2018b) define noise level and introduce a denoising curriculum. Kocmi and Bojar (2017) use linguistically-motivated features to classify examples into bins for scheduling. Kumar et al. (2019) use reinforcement learning to learn a denoising curriculum based on noise level of examples. Zhang et al. (2018) explore CL in general for NMT and observe faster training convergence. Zhang et al. (2019) use CL to adapt generic NMT models to a specific domain. Platanios et al. (2019"
P19-1123,N19-1189,0,0.223081,"Missing"
P19-1123,D17-1155,0,0.0390593,"t (y|x) ∝ Wt (x, y) P (y|x) , ∀(x, y) ∈ D (3) Measuring Domain and Noise in Data Data selection for MT usually uses a scoring function to rank sentence pairs. Cross entropy difference (Moore and Lewis, 2010) between two language models is usually used for selecting domain sentences, e.g., (van der Wees et al., 2017; Axelrod et al., 2011). For a source sentence x of length |x|, with a general-domain language model (LM), e and an in-domain LM, ϑ, b the parameterized as ϑ, 1 domain-relevance of x is calculated as:       log P x; ϑb − log P x; ϑe e ϑb = ϕ x; ϑ, (1) |x| Alternative measures (Wang et al., 2017; Chen and Huang, 2016; Chen et al., 2016) also show effectiveness. With Eq. 1 to select data, the data distribution (domain quality) in the in-domain monolinb is transferred into gual data used to train P (x; ϑ) the selected data through the scoring. Data selection has also been used for data denoising (Junczys-Dowmunt, 2018; Wang et al., 2018b), by using NMT models and trusted data to measure the noise level in a sentence pair. One e such a scoring function uses a baseline NMT, θ, b trained on noisy data and a cleaner NMT, θ, obtained by fine-tuning θe on a small trusted parallel dataset, an"
P19-1123,P18-2048,0,0.229736,"source sentence x of length |x|, with a general-domain language model (LM), e and an in-domain LM, ϑ, b the parameterized as ϑ, 1 domain-relevance of x is calculated as:       log P x; ϑb − log P x; ϑe e ϑb = ϕ x; ϑ, (1) |x| Alternative measures (Wang et al., 2017; Chen and Huang, 2016; Chen et al., 2016) also show effectiveness. With Eq. 1 to select data, the data distribution (domain quality) in the in-domain monolinb is transferred into gual data used to train P (x; ϑ) the selected data through the scoring. Data selection has also been used for data denoising (Junczys-Dowmunt, 2018; Wang et al., 2018b), by using NMT models and trusted data to measure the noise level in a sentence pair. One e such a scoring function uses a baseline NMT, θ, b trained on noisy data and a cleaner NMT, θ, obtained by fine-tuning θe on a small trusted parallel dataset, and measures quality in a sentence pair (x, y):       log P y|x; θb −log P y|x; θe e θb = φ x, y; θ, (2) |y| Using NMT models for selection can also lead to faster convergence (Wang et al., 2018a). With Eq. 2, the distribution (data quality) in the trusted parallel data is transferred into the selected data. These scoring functions usually"
P19-1123,W18-6314,1,0.893473,"source sentence x of length |x|, with a general-domain language model (LM), e and an in-domain LM, ϑ, b the parameterized as ϑ, 1 domain-relevance of x is calculated as:       log P x; ϑb − log P x; ϑe e ϑb = ϕ x; ϑ, (1) |x| Alternative measures (Wang et al., 2017; Chen and Huang, 2016; Chen et al., 2016) also show effectiveness. With Eq. 1 to select data, the data distribution (domain quality) in the in-domain monolinb is transferred into gual data used to train P (x; ϑ) the selected data through the scoring. Data selection has also been used for data denoising (Junczys-Dowmunt, 2018; Wang et al., 2018b), by using NMT models and trusted data to measure the noise level in a sentence pair. One e such a scoring function uses a baseline NMT, θ, b trained on noisy data and a cleaner NMT, θ, obtained by fine-tuning θe on a small trusted parallel dataset, and measures quality in a sentence pair (x, y):       log P y|x; θb −log P y|x; θe e θb = φ x, y; θ, (2) |y| Using NMT models for selection can also lead to faster convergence (Wang et al., 2018a). With Eq. 2, the distribution (data quality) in the trusted parallel data is transferred into the selected data. These scoring functions usually"
P19-1123,D17-1147,0,0.105919,"Missing"
P19-1251,D14-1181,0,0.0132016,"including the most significant 128 words correlated to high AQIs in χ2 statistics (Sch¨utze et al., 2008). The incoming tweets are filtered by the aforementioned keywords in the three groups. The tweets containing at least one of these keywords are likely to be relevant to the topics about air quality. We denote the corpus of relevant tweets as D0 (l, t). The features extracted from relevant tweets are expected to be more robust. 2.2 Stage 2: Feature Extraction To extract features from text data, the effectiveness of convolutional neural networks (CNNs) has been demonstrated in many studies (Kim, 2014). In this paper, CNNs with max-over-time pooling are applied to derive the representation for every tweet. We then propose max-over-tweet pooling to aggregate tweet representations across all relevant tweets as the corpus representation. Finally, the features can be acquired by concatenating the 3 EPA: https://www.epa.gov/ 2628 https://www.nlm.nih.gov/medical-terms.html Tweet Filtering Relevant Tweets Unfiltered Tweet Stream ··· ··· ··· ··· ··· D(l, t) Air Quality Prediction Feature Extraction .. . Embedding Convolutional Layer Layer j c1 W1 .. .. . . .. . .. . WN .. . Max-over-time Max-over-t"
P19-1299,Q17-1010,0,0.156594,"Missing"
P19-1299,N18-1111,1,0.83271,"model is unable to utilize features that are shared only between English and German. To address these shortcomings, we propose a new MLTL model that not only exploits languageinvariant features, but also allows the target language to dynamically and selectively leverage language-specific features through a probabilistic attention-style mixture of experts mechanism (see §3). This allows our model to learn effectively what to share between various languages. Another contribution of this paper is that, when combined with the recent unsupervised cross-lingual word embeddings (Lample et al., 2018; Chen and Cardie, 2018b), our model is able to operate in a zero-resource setting where neither task-specific target language annotations nor general-purpose cross-lingual resources (e.g. parallel corpora or machine translation (MT) systems) are available. This is an advantage over many existing CLTL works, making our model more widely applicable to many lower-resource languages. We evaluate our model on multiple MLTL tasks ranging from text classification to named entity recognition and semantic slot filling, including a real-world industry dataset. Our model beats all baseline models trained, like ours, without c"
P19-1299,D18-1024,1,0.860918,"model is unable to utilize features that are shared only between English and German. To address these shortcomings, we propose a new MLTL model that not only exploits languageinvariant features, but also allows the target language to dynamically and selectively leverage language-specific features through a probabilistic attention-style mixture of experts mechanism (see §3). This allows our model to learn effectively what to share between various languages. Another contribution of this paper is that, when combined with the recent unsupervised cross-lingual word embeddings (Lample et al., 2018; Chen and Cardie, 2018b), our model is able to operate in a zero-resource setting where neither task-specific target language annotations nor general-purpose cross-lingual resources (e.g. parallel corpora or machine translation (MT) systems) are available. This is an advantage over many existing CLTL works, making our model more widely applicable to many lower-resource languages. We evaluate our model on multiple MLTL tasks ranging from text classification to named entity recognition and semantic slot filling, including a real-world industry dataset. Our model beats all baseline models trained, like ours, without c"
P19-1299,N18-1032,1,0.773415,"static weights at the task level, while our model can dynamically select what to share at the instance level. A very recent work (Guo et al., 2018) attempts to model the relation between the target domain and each source domain. Our model combines the strengths of these methods and is able to simul3099 JC JD 1 JD Language Label Language Discriminator D straint and many useful features may be wiped out by adversarial training if they are shared only between the target language and a subset of source languages. Therefore, we propose to use a mixture-of-experts (MoE) model (Shazeer et al., 2017; Gu et al., 2018) to learn the private features. The idea is to have a set of language expert networks, one per source language, each responsible for learning language-specific features for that source language during training. However, instead of hard-switching between the experts, each sample uses a convex combination of all experts, dictated by an expert gate. Thus, at test time, the trained expert gate can decide the optimal expert weights for the unseen target language based on its similarity to the source languages. Figure 1 shows an overview of our MAN-MoE model for multilingual model transfer. The boxe"
P19-1299,D18-1498,0,0.115728,"Missing"
P19-1299,D14-1080,1,0.880726,"Missing"
P19-1299,D17-1302,0,0.0218392,"les from different languages extracted by the same neural net remain divergent, and hence weight sharing is not sufficient for learning a language-invariant feature space that generalizes well across languages. As such, previ2 In contrast, supervised CLTL assumes the availability of annotations in the target language. 3098 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3098–3112 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ous work has explored using language-adversarial training (Chen et al., 2016; Kim et al., 2017) to extract features that are invariant with respect to the shift in language, using only (non-parallel) unlabeled texts from each language. On the other hand, in the MLTL setting, where multiple source languages exist, languageadversarial training will only use, for model transfer, the features that are common among all source languages and the target, which may be too restrictive in many cases. For example, when transferring from English, Spanish and Chinese to German, language-adversarial training will retain only features that are invariant across all four languages, which can be too spars"
P19-1299,D14-1181,0,0.00319016,"dd character-level word embeddings (Dos Santos and Zadrozny, 2014) that captures sub-word information. When character embeddings are used, we add a single CharCNN that is shared across all languages, and the final word representation is the concatenation of the word embedding and the char-level embedding. The CharCNN can then be trained end to end with the rest of the model. MAN Shared Feature Extractor Fs is a multinomial adversarial network (Chen and Cardie, 2018a), which is an adversarial pair of a feature extractor (e.g. LSTM or CNN) and a language discriminator D. D is a text classifier (Kim, 2014) that takes the shared features (extracted by Fs ) of an input sequence and predicts which language it comes from. On the other hand, Fs strives to fool D so that it cannot identify the language of a sample. The hypothesis is that if D cannot recognize the language of the input, the shared features then do not contain language information and are hence language-invariant. Note that D is trained only using unlabeled texts, and can therefore be trained on all languages including the target language. MoE Private Feature Extractor Fp is a key difference from previous work, shown in Figure 2. The f"
P19-1299,C12-1089,0,0.0197063,"16) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), showing improved performance compared to using labeled data from one source language as in bilingual transfer. Another important direction for CLTL is to learn cross-lingual word representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there have been several notable work for learning fully unsupervised cross-lingual word embeddings, both for the bilingual (Zhang et al., 2017; Lample et al., 2018; Artetxe et al., 2018) and multilingual case (Chen and Cardie, 2018b). These efforts pave the road for performing CLTL without cross-lingual resources. Finally, a related field to MLTL is multi-source domain adaptation (Mansour et al., 2009), where most prior work relies on the learning of domaininvariant features (Zhao et al., 2018; Chen and Cardie, 2018a). Ruder et al. (2019) pr"
P19-1299,D15-1166,0,0.0454517,"Fp is able to dynamically determine what knowledge to use from each individual source language at a token level. MoE Task-Specific Predictor C is the final module that make predictions for the end task, and may take different forms depending on the task. For instance, for sequence tagging tasks, the shared and private features are first concatenated for each token, and then past through a MoE module similar to Fp (as shown in Figure 6 in the Appendix). It is straightforward to adapt C to work for other tasks. For example, for text classification, a pooling layer such as dot-product attention (Luong et al., 2015) is added at the bottom to fuse token-level features into a single sentence feature vector. C first concatenates the shared and private features to form a single feature vector for each token. It then has another MoE module that outputs a softmax probability over all labels for each token. The idea is that it may be favorable to put different weights between the language-invariant and language-specific features for different target languages. Again consider the example of English, German, Spanish and Chinese. When transferring to Chinese from the other three, the source lan3101 Algorithm 1 MAN"
P19-1299,D17-1269,0,0.127447,"Missing"
P19-1299,D11-1006,0,0.199876,"a target language using annotated data from other languages (source languages) (Yarowsky et al., 2001). In this paper, we concentrate on the more challenging unsupervised CLTL setting, where no target language labeled data is used for training.2 Traditionally, most research on CLTL has been devoted to the standard bilingual transfer (BLTL) case where training data comes from a single source language. In practice, however, it is often the case that we have labeled data in a few languages, and would like to be able to utilize all of the data when transferring to other languages. Previous work (McDonald et al., 2011) indeed showed that transferring from multiple source languages could result in significant performance improvement. Therefore, in this work, we focus on the multi-source CLTL scenario, also known as multilingual transfer learning (MLTL), to further boost the target language performance. One straightforward method employed in CLTL is weight sharing, namely directly applying the model trained on the source language to the target after mapping both languages to a common embedding space. As shown in previous work (Chen et al., 2016), however, the distributions of the hidden feature vectors of sam"
P19-1299,P12-1066,0,0.0388569,"systems or parallel corpora are utilized to replace taskspecific annotated data (Wan, 2009; Prettenhofer and Stein, 2010). With the advent of deep learning, especially adversarial neural networks (Goodfellow et al., 2014; Ganin et al., 2016), progress has been made towards model-based CLTL methods. Chen et al. (2016) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), showing improved performance compared to using labeled data from one source language as in bilingual transfer. Another important direction for CLTL is to learn cross-lingual word representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there have been several notable work for learning fully unsupervised cross-lingual word embeddings, both for the bilingual (Zhang et al., 2017; Lample et al., 2018; Artetxe et al., 2018) and multilingual case (Chen and Cardie, 2"
P19-1299,P17-1135,0,0.225946,"Missing"
P19-1299,W15-1512,0,0.126077,"on dataset (Prettenhofer and Stein, 2010). The dataset is a binary classification dataset where each review is classified into positive or negative sentiment. It has four languages: English, German, French and Japanese. As shown in Table 5, MT-BOW uses machine translation to translate the bag of words of a target sentence into the source language, while CL-SCL learns a cross-lingual feature space via structural correspondence learning (Prettenhofer and Stein, 2010). CR-RL (Xiao and Guo, 2013) learns bilingual word representations where part of the word vector is shared among languages. Bi-PV (Pham et al., 2015) extracts bilingual paragraph vector by sharing the representation between parallel documents. UMM (Xu and Wan, 2017) is a multilingual framework that could utilize parallel corpora between multiple language pairs, and pivot as needed when direct bitexts are not available for a specific source-target pair. Finally CLDFA (Xu and Yang, 2017) proposes cross-lingual distillation on parallel corpora for CLTL. Unlike other works listed, however, they adopt a task-specific parallel corpus (translated Amazon reviews) that are difficult to obtain in practice, making the num3105 German Domain music avg"
P19-1299,P10-1114,0,0.66091,"natural language processing. In order to alleviate the need for obtaining annotated data for each task in each language, cross-lingual transfer learning (CLTL) has long been studied (Yarowsky et al., 2001; Bel et al., 2003, inter alia). For unsupervised CLTL in particular, where no target language training data is available, most prior research investigates the bilingual transfer setting. Traditionally, research focuses on resource-based methods, where general-purpose cross-lingual resources such as MT systems or parallel corpora are utilized to replace taskspecific annotated data (Wan, 2009; Prettenhofer and Stein, 2010). With the advent of deep learning, especially adversarial neural networks (Goodfellow et al., 2014; Ganin et al., 2016), progress has been made towards model-based CLTL methods. Chen et al. (2016) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), sh"
P19-1299,W02-2024,0,0.765396,"51 2014 1366 480 Domain Navigation Calendar Files Chinese 1497 926 843 1986 1081 970 #Train #Dev #Test #Slot 7472 2056 1289 1114 309 256 1173 390 215 8 4 5 Examples [Driving]transportation type directions to [Walmart]place name in [New York ]location . Add [school meeting]title to my calendar on [Monday]start date at [noon]start time . Search for [notes]data type with [grocery list]keyword . Table 1: Statistics for the Multilingual Semantic Slot Filling dataset with examples from each domain. academic datasets, namely the CoNLL multilingual named entity recognition (sequence tagging) dataset (Sang, 2002; Sang and Meulder, 2003), and the multilingual Amazon reviews (text classification) dataset (Prettenhofer and Stein, 2010). 4.1 Cross-Lingual Semantic Slot Filling As shown in Table 1, we collect data for four languages: English, German, Spanish, and Chinese, over three domains: Navigation, Calendar, and Files. Each domain has a set of pre-determined slots (the slots are the same across languages), and the user utterances in each language and domain are annotated by crowd workers with the correct slots (see the examples in Table 1). We employ the standard BIO tagging scheme to formulate the s"
P19-1299,W03-0419,0,0.615088,"Missing"
P19-1299,K16-1022,0,0.077161,"gical features such as capitalization is crucial for NER. We hence add character-level word embeddings for this task (§3.1) to capture subword fea3104 de es nl avg 59.3 61.0 60.6 65.1 66.0 58.4 64.0 61.6 65.4 64.5 52.7 60.3 56.8 63.0 62.3 Methods without cross-lingual resources MAN-MoE 55.1 59.5 BWE+CharCNN (1-to-1) 51.5 61.0 BWE+CharCNN (3-to-1) 55.8 70.4 Xie et al. (2018)* 56.9 71.0 MAN-MoE+CharCNN 56.7 71.0 MAN-MoE+CharCNN+UMWE 56.0 73.5 61.8 67.3 69.8 71.3 70.9 72.4 58.8 60.0 65.3 66.4 66.2 67.3 Methods with cross-lingual resources T¨ackstr¨om et al. (2012) 40.4 Nothman et al. (2013) 55.8 Tsai et al. (2016) 48.1 Ni et al. (2017) 58.5 Mayhew et al. (2017) 57.5 * Average Gate Weights Target Language 0.40 0.35 0.30 0.25 en fr ja Target Lang: de en de ja Target Lang: fr en de fr Target Lang: ja Figure 3: Average expert gate weights aggregated on a language level for the Amazon Reviews dataset. Contemporaneous work Table 4: F1 scores for the CoNLL NER dataset on German (de), Spanish (es) and Dutch (nl). tures and alleviate the OOV problem. For German, however, all nouns are capitalized, and the capitalization features learned on the other three languages would lead to poor results. Therefore, for Ger"
P19-1299,P09-1027,0,0.0919103,"llenge for natural language processing. In order to alleviate the need for obtaining annotated data for each task in each language, cross-lingual transfer learning (CLTL) has long been studied (Yarowsky et al., 2001; Bel et al., 2003, inter alia). For unsupervised CLTL in particular, where no target language training data is available, most prior research investigates the bilingual transfer setting. Traditionally, research focuses on resource-based methods, where general-purpose cross-lingual resources such as MT systems or parallel corpora are utilized to replace taskspecific annotated data (Wan, 2009; Prettenhofer and Stein, 2010). With the advent of deep learning, especially adversarial neural networks (Goodfellow et al., 2014; Ganin et al., 2016), progress has been made towards model-based CLTL methods. Chen et al. (2016) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzil"
P19-1299,D13-1153,0,0.0490677,"w). 4.3 Cross-Lingual Text Classification on Amazon Reviews Finally, we report results on a multilingual text classification dataset (Prettenhofer and Stein, 2010). The dataset is a binary classification dataset where each review is classified into positive or negative sentiment. It has four languages: English, German, French and Japanese. As shown in Table 5, MT-BOW uses machine translation to translate the bag of words of a target sentence into the source language, while CL-SCL learns a cross-lingual feature space via structural correspondence learning (Prettenhofer and Stein, 2010). CR-RL (Xiao and Guo, 2013) learns bilingual word representations where part of the word vector is shared among languages. Bi-PV (Pham et al., 2015) extracts bilingual paragraph vector by sharing the representation between parallel documents. UMM (Xu and Wan, 2017) is a multilingual framework that could utilize parallel corpora between multiple language pairs, and pivot as needed when direct bitexts are not available for a specific source-target pair. Finally CLDFA (Xu and Yang, 2017) proposes cross-lingual distillation on parallel corpora for CLTL. Unlike other works listed, however, they adopt a task-specific parallel"
P19-1299,D18-1034,0,0.229425,"Missing"
P19-1299,P18-1072,0,0.0561644,"Missing"
P19-1299,D17-1053,0,0.0469431,"ied into positive or negative sentiment. It has four languages: English, German, French and Japanese. As shown in Table 5, MT-BOW uses machine translation to translate the bag of words of a target sentence into the source language, while CL-SCL learns a cross-lingual feature space via structural correspondence learning (Prettenhofer and Stein, 2010). CR-RL (Xiao and Guo, 2013) learns bilingual word representations where part of the word vector is shared among languages. Bi-PV (Pham et al., 2015) extracts bilingual paragraph vector by sharing the representation between parallel documents. UMM (Xu and Wan, 2017) is a multilingual framework that could utilize parallel corpora between multiple language pairs, and pivot as needed when direct bitexts are not available for a specific source-target pair. Finally CLDFA (Xu and Yang, 2017) proposes cross-lingual distillation on parallel corpora for CLTL. Unlike other works listed, however, they adopt a task-specific parallel corpus (translated Amazon reviews) that are difficult to obtain in practice, making the num3105 German Domain music avg books dvd music avg Methods with general-purpose cross-lingual resources MT-BOW1 79.68 77.92 77.22 78.27 80.76 CL-SCL"
P19-1299,P17-1130,0,0.0670095,"anguage, while CL-SCL learns a cross-lingual feature space via structural correspondence learning (Prettenhofer and Stein, 2010). CR-RL (Xiao and Guo, 2013) learns bilingual word representations where part of the word vector is shared among languages. Bi-PV (Pham et al., 2015) extracts bilingual paragraph vector by sharing the representation between parallel documents. UMM (Xu and Wan, 2017) is a multilingual framework that could utilize parallel corpora between multiple language pairs, and pivot as needed when direct bitexts are not available for a specific source-target pair. Finally CLDFA (Xu and Yang, 2017) proposes cross-lingual distillation on parallel corpora for CLTL. Unlike other works listed, however, they adopt a task-specific parallel corpus (translated Amazon reviews) that are difficult to obtain in practice, making the num3105 German Domain music avg books dvd music avg Methods with general-purpose cross-lingual resources MT-BOW1 79.68 77.92 77.22 78.27 80.76 CL-SCL1 79.50 76.92 77.79 78.07 78.49 CR-RL2 79.89 77.14 77.27 78.10 78.25 Bi-PV3 79.51 78.60 82.45 80.19 84.25 UMM4 81.65 81.27 81.32 81.41 80.27 78.83 78.80 74.83 79.60 80.27 75.78 77.92 78.71 80.09 79.41 78.46 78.40 77.26 81.31"
P19-1299,N13-1126,0,0.0762296,"Missing"
P19-1299,H01-1035,0,0.696461,"the first author was an intern at Microsoft Research. 1 The code is available at https://github.com/ microsoft/Multilingual-Model-Transfer. large-scale annotated datasets. However, such an advantage is not available to most of the world languages since many of them lack the the labeled data necessary for training deep neural nets for a variety of NLP tasks. As it is prohibitive to obtain training data for all languages of interest, crosslingual transfer learning (CLTL) offers the possibility of learning models for a target language using annotated data from other languages (source languages) (Yarowsky et al., 2001). In this paper, we concentrate on the more challenging unsupervised CLTL setting, where no target language labeled data is used for training.2 Traditionally, most research on CLTL has been devoted to the standard bilingual transfer (BLTL) case where training data comes from a single source language. In practice, however, it is often the case that we have labeled data in a few languages, and would like to be able to utilize all of the data when transferring to other languages. Previous work (McDonald et al., 2011) indeed showed that transferring from multiple source languages could result in s"
P19-1299,N12-1052,0,0.307376,"Missing"
P19-1299,D15-1213,0,0.0272914,"data (Wan, 2009; Prettenhofer and Stein, 2010). With the advent of deep learning, especially adversarial neural networks (Goodfellow et al., 2014; Ganin et al., 2016), progress has been made towards model-based CLTL methods. Chen et al. (2016) propose languageadversarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), showing improved performance compared to using labeled data from one source language as in bilingual transfer. Another important direction for CLTL is to learn cross-lingual word representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there have been several notable work for learning fully unsupervised cross-lingual word embeddings, both for the bilingual (Zhang et al., 2017; Lample et al., 2018; Artetxe et al., 2018) and multilingual case (Chen and Cardie, 2018b). These efforts pave the road for performing CLTL without cross-lingual re"
P19-1299,D13-1141,0,0.0421518,"sarial training that does not directly depend on parallel corpora, but instead only requires a set of bilingual word embeddings (BWEs). On the other hand, the multilingual transfer setting, although less explored, has also been studied (McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Hajmohammadi et al., 2014; Zhang and Barzilay, 2015; Guo et al., 2016), showing improved performance compared to using labeled data from one source language as in bilingual transfer. Another important direction for CLTL is to learn cross-lingual word representations (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there have been several notable work for learning fully unsupervised cross-lingual word embeddings, both for the bilingual (Zhang et al., 2017; Lample et al., 2018; Artetxe et al., 2018) and multilingual case (Chen and Cardie, 2018b). These efforts pave the road for performing CLTL without cross-lingual resources. Finally, a related field to MLTL is multi-source domain adaptation (Mansour et al., 2009), where most prior work relies on the learning of domaininvariant features (Zhao et al., 2018; Chen and Cardie, 2018a). Ruder et al. (2019) propose a general fr"
P19-1529,D18-1162,0,0.0369491,"Missing"
P19-1529,W05-0620,0,0.0543505,"Missing"
P19-1529,C18-1251,0,0.0609233,"Missing"
P19-1529,P18-2058,0,0.047851,"ress three questions: 1) How should syntactic information be encoded as word-level features? 2) What is the best way of integrating syntactic information? and 3) What effect does the choice of syntactic representation have on the performance? We study these questions in the context of Semantic Role Labelling (SRL). A SRL system extracts the predicate-argument structure of a sentence.2 Syntax was an essential component of early SRL systems (Xue and Palmer, 2004; Punyakanok et al., 2008). The state-of-the-art neural SRL systems use a neural sequence labelling model without any syntax knowledge (He et al., 2018, 2017; Tan et al., 2018). We show below that injecting external syntactic knowledge into a neural SRL sequence labelling model can improve the performance, and our best model sets a new stateof-the-art for a non-ensemble SRL system. In this paper we express the external syntactic information as vectors of discrete features, because this enables us to explore different ways of injecting the syntactic information into the neural SRL model. Specifically, we propose three different syntax encoding methods: a) a full constituency tree representation (Full-C); b) an SRLspecific span representation"
P19-1529,P17-1044,0,0.127535,"Missing"
P19-1529,P18-1249,0,0.0693605,"Missing"
P19-1529,P17-1064,0,0.0311314,"n can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL’05 and CoNLL’12 benchmarks.1 1 Introduction Properly integrating external information into neural networks has received increasing attention recently (Wu et al., 2018; Li et al., 2017; Strubell et al., 2018). Previous research on this topic can be roughly categorized into three classes: i) Input: The external information are presented as additional input features (i.e., dense real-valued vectors) to the neural network (Collobert et al., 2011). ii) Output: The neural network is trained to predict the main task and the external information in a multi-task approach (Changpinyo et al., 2018). iii) Auto-encoder: This approach, recently proposed by Wu et al. (2018), simultaneously combines the Input and Output during neural models training. The simplicity of these methods allow"
P19-1529,D17-1159,0,0.0523409,"dhan et al. (2013) incorporate constituent-structure span-based information, while Hajiˇc et al. (2009) incorporate dependency-structure information. This information can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system. Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system. Xia et al. (2019) represent dependency parses using position-based categorical features of tree structures in a neural model. Strubell et al. (2018) use dependency trees as a supervision signal to train one of attention heads in a self-attentive neural model. 3 Syntactic Representation This section introduces our representations of constituency and dependency syntax trees. 3.1 Full-C: Full Constituency Representation G´omez-Rodr´ıguez and Vilares (2018) propose a ful"
P19-1529,D18-1191,0,0.407935,"Missing"
P19-1529,J05-1004,0,0.0853794,"eedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5338–5343 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the SRL CoNLL’05 and CoNLL’12 benchmarks. We show that using either of the constituency representations in either the Input or the AutoEncoder configurations produces the best performance. These results are noticeably better than a strong baseline and set a new state-of-the-art for non-ensemble SRL systems. 2 Related Work Semantic Role Labeling (SRL) generally refers to the PropBank style of annotation (Palmer et al., 2005). Broadly speaking, prior work on SRL makes use of syntactic information in two different ways. Carreras and M`arquez (2005); Pradhan et al. (2013) incorporate constituent-structure span-based information, while Hajiˇc et al. (2009) incorporate dependency-structure information. This information can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model t"
P19-1529,N18-1202,0,0.109135,"Missing"
P19-1529,D18-1310,0,0.113834,"tactic information can be used most effectively in the Semantic Role Labeling (SRL) task. We evaluate three different ways of encoding syntactic parses and three different ways of injecting them into a state-of-the-art neural ELMo-based SRL sequence labelling model. We show that using a constituency representation as input features improves performance the most, achieving a new state-of-the-art for non-ensemble SRL models on the in-domain CoNLL’05 and CoNLL’12 benchmarks.1 1 Introduction Properly integrating external information into neural networks has received increasing attention recently (Wu et al., 2018; Li et al., 2017; Strubell et al., 2018). Previous research on this topic can be roughly categorized into three classes: i) Input: The external information are presented as additional input features (i.e., dense real-valued vectors) to the neural network (Collobert et al., 2011). ii) Output: The neural network is trained to predict the main task and the external information in a multi-task approach (Changpinyo et al., 2018). iii) Auto-encoder: This approach, recently proposed by Wu et al. (2018), simultaneously combines the Input and Output during neural models training. The simplicity of the"
P19-1529,N19-1075,0,0.0560144,"ormation can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system. Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system. Xia et al. (2019) represent dependency parses using position-based categorical features of tree structures in a neural model. Strubell et al. (2018) use dependency trees as a supervision signal to train one of attention heads in a self-attentive neural model. 3 Syntactic Representation This section introduces our representations of constituency and dependency syntax trees. 3.1 Full-C: Full Constituency Representation G´omez-Rodr´ıguez and Vilares (2018) propose a full representation of constituency parsing trees where the string position between wi and wi+1 is associated with the pair (n(wi ) − n(wi−1 ), l(wi"
P19-1529,W04-3212,0,0.116622,"source code is available in https:// github.com/GaryYufei/bestParseSRL this gap by integrating syntactic information to the sequence labelling task. We address three questions: 1) How should syntactic information be encoded as word-level features? 2) What is the best way of integrating syntactic information? and 3) What effect does the choice of syntactic representation have on the performance? We study these questions in the context of Semantic Role Labelling (SRL). A SRL system extracts the predicate-argument structure of a sentence.2 Syntax was an essential component of early SRL systems (Xue and Palmer, 2004; Punyakanok et al., 2008). The state-of-the-art neural SRL systems use a neural sequence labelling model without any syntax knowledge (He et al., 2018, 2017; Tan et al., 2018). We show below that injecting external syntactic knowledge into a neural SRL sequence labelling model can improve the performance, and our best model sets a new stateof-the-art for a non-ensemble SRL system. In this paper we express the external syntactic information as vectors of discrete features, because this enables us to explore different ways of injecting the syntactic information into the neural SRL model. Specif"
P19-1529,W13-3516,0,0.0539215,"Missing"
P19-1529,J08-2005,0,0.0516893,"ble in https:// github.com/GaryYufei/bestParseSRL this gap by integrating syntactic information to the sequence labelling task. We address three questions: 1) How should syntactic information be encoded as word-level features? 2) What is the best way of integrating syntactic information? and 3) What effect does the choice of syntactic representation have on the performance? We study these questions in the context of Semantic Role Labelling (SRL). A SRL system extracts the predicate-argument structure of a sentence.2 Syntax was an essential component of early SRL systems (Xue and Palmer, 2004; Punyakanok et al., 2008). The state-of-the-art neural SRL systems use a neural sequence labelling model without any syntax knowledge (He et al., 2018, 2017; Tan et al., 2018). We show below that injecting external syntactic knowledge into a neural SRL sequence labelling model can improve the performance, and our best model sets a new stateof-the-art for a non-ensemble SRL system. In this paper we express the external syntactic information as vectors of discrete features, because this enables us to explore different ways of injecting the syntactic information into the neural SRL model. Specifically, we propose three d"
P19-1529,P16-1113,0,0.0344688,"style of annotation (Palmer et al., 2005). Broadly speaking, prior work on SRL makes use of syntactic information in two different ways. Carreras and M`arquez (2005); Pradhan et al. (2013) incorporate constituent-structure span-based information, while Hajiˇc et al. (2009) incorporate dependency-structure information. This information can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system. Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system. Xia et al. (2019) represent dependency parses using position-based categorical features of tree structures in a neural model. Strubell et al. (2018) use dependency trees as a supervision signal to train one of attention heads in a self-attentive neural model. 3 Syntactic Representation This s"
P19-1529,D18-1548,0,0.0953321,"Missing"
P19-1529,D18-1412,0,0.0760596,"These results are noticeably better than a strong baseline and set a new state-of-the-art for non-ensemble SRL systems. 2 Related Work Semantic Role Labeling (SRL) generally refers to the PropBank style of annotation (Palmer et al., 2005). Broadly speaking, prior work on SRL makes use of syntactic information in two different ways. Carreras and M`arquez (2005); Pradhan et al. (2013) incorporate constituent-structure span-based information, while Hajiˇc et al. (2009) incorporate dependency-structure information. This information can be incorporated into an SRL system in several different ways. Swayamdipta et al. (2018) use span information from constituency parse trees as an additional training target in a multi-task learning approach, similar to one of the approaches we evaluate here. Roth and Lapata (2016) use an LSTM model to represent the dependency paths between predicates and arguments and feed the output as the input features to their SRL system. Marcheggiani and Titov (2017) use Graph Convolutional Network (Niepert et al., 2016) to encode the dependency parsing trees into their LSTM-based SRL system. Xia et al. (2019) represent dependency parses using position-based categorical features of tree stru"
W06-1606,J93-2003,0,0.0178886,"Missing"
W06-1606,P05-1033,0,0.882263,"(Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained on significantly less data, have shown promising results when compared on the same test sets with mature phrase-based systems. To our knowledge though, no previous research has demonstrated that a syntax-based statistical translation system could produce better results than a phrase-based system on a large-scale, well-established, open domain translation task. In this paper we present such a system. Our translation models rely upon and naturally exploit submodels (feature functions) that have 2.1 An intuitive introduction to SPMT After being exposed to 100M+ words o"
W06-1606,J04-4002,0,0.831104,"de with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained on significantly less"
W06-1606,J03-4003,0,0.00529773,"by the following derivation: r4 (r9 (r7 ), r3 (r6 (r12 (r8 )))). Figure 2: English parse tree derivation of the Chinese string COMINGFROM FRANCE AND RUSSIA pDE ASTRO- -NAUTS. ple, c(Θ) = (π, F, A). The probability of each derivation θi is given by the product of the probabilities of all the rules p(rj ) in the derivation (see equation 4). P r(π, F, A) = X Y p(rj ) (4) θi ∈Θ,c(Θ)=(π,F,A) rj ∈θi In order to acquire the rules specific to our model and to induce their probabilities, we parse the English side of our corpus with an in-house implementation (Soricut, 2005) of Collins parsing models (Collins, 2003) and we word-align the parallel corpus with the Giza++2 implementation of the IBM models (Brown et al., 1993). We use the automatically derived hEnglish-parse-tree, English-sentence, Foreign-sentence, Word-levelalignmenti tuples in order to induce xRS rules for several models. 2.2.2 SPMT Model 1 In our simplest model, we assume that each tuple (π, F, A) in our automatically annotated corpus could be produced by applying a combination of minimally syntactified, lexicalized, phrase-based compatible xRS rules, and minimal/necessary, non-lexicalized xRS rules. We call a rule non-lexicalized whenev"
W06-1606,N04-1035,1,0.399768,"minimal rules (lexicalized and non-lexicalized) by applying the algorithm proposed by Galley et al. (2006) and then remove the lexicalized rules. We remove the Galley et al.’s lexicalized rules because they are either already accounted for by the minimally syntactified, lexicalized, phrasebased-compatible xRS rules or they subsume noncontinuous source-target phrase pairs. It is worth mentioning that, in our framework, a rule is defined to be “minimal” with respect to a foreign/source language phrase, i.e., it is the minimal xRS rule that yields that source phrase. In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (π, F, A) tuple. Under SPMT model 1, the tree in Figure 2 can be produced, for example, by the following derivation: r4 (r9 (r7 ), r3 (r6 (r12 (r8 )))). Figure 2: English parse tree derivation of the Chinese string COMINGFROM FRANCE AND RUSSIA pDE ASTRO- -NAUTS. ple, c(Θ) = (π, F, A). The probability of each derivation θi is given by the product of the probabilities of all the rules p(rj ) in the derivation (see equation 4). P r(π, F, A) = X Y p(rj ) (4) θi ∈Θ,c(Θ)=(π,F,A) rj ∈θi In order to acquire the rules s"
W06-1606,P03-1021,0,0.664719,"iments described in this paper, we use the following submodels (feature functions): Syntax-based-like submodels: • m1inv(ri ) is the IBM model 1 inverse probability computed over the bags of words that occur on the source and target sides of a rule. • lm(e) is the language model probability of the target translation under an ngram language model. • wp(e) is a word penalty model designed to favor longer translations. All these models are combined log-linearly during decoding. The weights of the models are computed automatically using a variant of the Maximum Bleu training procedure proposed by Och (2003). The phrase-based-like submodels have been proved useful in phrase-based approaches to SMT (Och and Ney, 2004). The first two syntaxbased submodels implement a “fused” translation and lexical grounded distortion model (p root ) and a syntax-based distortion model (p cfg ). The indicator submodels are used to determine the extent to which our system prefers lexicalized vs. nonlexicalized rules; simple vs. composed rules; and high vs. low count rules. • proot (ri ) is the root normalized conditional probability of all the rules in a model. • pcfg (ri ) is the CFG-like probability of the non-lex"
W06-1606,P05-1034,0,0.755662,"Missing"
W06-1606,N04-4026,0,0.108335,"Missing"
W06-1606,N03-1017,1,0.0780121,"Section 5, we conclude with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spite of being trained o"
W06-1606,N06-1033,1,0.131843,"by definition pcfg = 1. 48 3 Decoding 4 Experiments 4.1 3.1 Decoding with one SPMT model We evaluate our models on a Chinese to English machine translation task. We use the same training corpus, 138.7M words of parallel Chinese-English data released by LDC, in order to train several statistical-based MT systems: We decode with each of our SPMT models using a straightforward, bottom-up, CKY-style decoder that builds English syntactic constituents on the top of Chinese sentences. The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006). The CKY-style decoder computes the probability of English syntactic constituents in a bottom up fashion, by log-linearly interpolating all the submodel scores described in Section 2.3. • PBMT, a strong state of the art phrase-based system that implements the alignment template model (Och and Ney, 2004); this is the system ISI has used in the 2004 and 2005 NIST evaluations. • four SPMT systems (M1, M1C, M2, M2C) that implement each of the models discussed in this paper; The decoder is capable of producing nbest derivations and nbest lists (Knight and Graehl, 2005), which are used for Maximum"
W06-1606,W02-1018,1,0.779424,"odels empirically. In Section 5, we conclude with a brief discussion. We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases. The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5. 1 Introduction 2 SPMT: statistical Machine Translation with Syntactified Phrases During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy. Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent. Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora. And syntax-inspired formal models (Chiang, 2005), in spit"
W06-1606,E06-1005,0,0.0241966,"Missing"
W06-1606,P06-1121,1,\N,Missing
W12-6314,W04-3230,0,0.102535,"Missing"
W12-6314,C04-1067,0,\N,Missing
W12-6314,W10-4129,0,\N,Missing
W12-6314,Y06-1012,0,\N,Missing
W12-6314,W10-4128,1,\N,Missing
W16-1310,D11-1048,0,0.0304506,"ifficulties mentioned above. As in other areas of NLP, recent work on IE has adopted statistical models for text. Discriminative models such as conditional random fields (CRFs) are trainable, bottom-up classifiers usable for the early stages of a pipeline approach; they require less manual labor than rule-based methods, although they do require supervised data. Generative models describe a stochastic process whose output is text; given some actual text, an inference algorithm can reconstruct the underlying hidden variables that would explain the observed text. Several other groups (see, e.g., Rink and Harabagiu (2011) or Yao et al. (2011) and many variants cited by Grycner et al. (2014)) are developing generative models for IE and relation discovery, but their models generate text from a descriptive model of text, rather than from a model of an underlying real world. That is, the hidden variables include the dictionaries describing the words that each relation uses to express itself and the types for each of its arguments, but not the facts that explain why the text is there. Thus, they cannot truly reconstruct such a world from text. The difficulty can be illustrated very simply: if one generates a very l"
W16-1310,D11-1135,0,0.276057,"As in other areas of NLP, recent work on IE has adopted statistical models for text. Discriminative models such as conditional random fields (CRFs) are trainable, bottom-up classifiers usable for the early stages of a pipeline approach; they require less manual labor than rule-based methods, although they do require supervised data. Generative models describe a stochastic process whose output is text; given some actual text, an inference algorithm can reconstruct the underlying hidden variables that would explain the observed text. Several other groups (see, e.g., Rink and Harabagiu (2011) or Yao et al. (2011) and many variants cited by Grycner et al. (2014)) are developing generative models for IE and relation discovery, but their models generate text from a descriptive model of text, rather than from a model of an underlying real world. That is, the hidden variables include the dictionaries describing the words that each relation uses to express itself and the types for each of its arguments, but not the facts that explain why the text is there. Thus, they cannot truly reconstruct such a world from text. The difficulty can be illustrated very simply: if one generates a very large sample of author"
W18-6314,D11-1033,0,0.835577,"pic in the machine translation (MT) field. Recent research has found that data noise has a bigger impact on neural machine translation (NMT) than on statistical machine translation (Khayrallah and Koehn, 2018), but learning what data quality (or noise) means in NMT and how to make NMT training robust to data noise remains an open research question. On the other hand, a rich body of MT data research focuses on domain data relevance and selection for domain adaptation purpose. As a result, effective and successful methods have been published and shown to work for both SMT and NMT. For example, (Axelrod et al., 2011) introduce a metric for measuring the data relevance to a domain by using n-gram language models (LM). (van der Wees et al., 2017) employ a neuralnetwork version of it and propose a graduallyrefining strategy to dynamically schedule data during NMT training. In these methods, a large amount of in-domain data are used to help measure data domain relevance. • How to measure noise? • How does noise dynamically interact with the training progress? • How to denoise the model training with a small, trusted parallel dataset? In the denoising scenario, the trusted data would be the counterpart of in-d"
W18-6314,W17-4712,0,0.0274715,"raining. It uses a small amount of trusted data to help models measure noise in a sentence pair. The noise is defined based on comparison between a pair of a noisy NMT model and another, slightly denoised NMT model, inspired by the contrastive in-domain LM vs out-of-domain LM idea. It employs online data selection to sort sentence pairs by noise level so that the model is trained on gradually noise-reduced data batches. We show that language model based domain data selection method as is does not work well whereas the proposed approach is quite effective in denoising NMT training. 134 4 2017; Britz et al., 2017; Matsoukas et al., 2009). In the context of denoising, the quality that the ordering uses would be the amount of noise in a sentence pair, not (only) how much the data fits the domain of interest. SMT models tend to be fairly robust to data noise and denoising in SMT seems to have been a lightly studied topic. For example, (Mediani, 2017) uses a small, clean seed corpus and designs classifier filter to identify noisy data with lexical features; and also there is a nice list of works accumulated over years, compiled on the SMT Research Survey Wiki1 . The importance of NMT denoising has been in"
W18-6314,K16-1031,0,0.12152,", (van der Wees et al., 2017) employ a neural-network version of it along with a dynamic data selection idea and achieve better domain data selection outcome. (Mansour et al., 2011) compute the CED using IBM translation Model 1 and achieve the best domain data selection/filtering effect for SMT combined with LM selection; The case of partial or misalignments with a bilingual scoring mechanism rather than LMs is also discussed. Another effective method to distinguish domain relevance is to build a classifier. A small amount of trusted parallel data is used in classifier training. For example, (Chen and Huang, 2016) use semisupervised convolutional neural networks (CNNs) as LMs to select domain data. Trusted data is used to adapt the classifier/selector. (Chen et al., 2016) introduce a bilingual data selection method that uses CNNs on bitokens; The method uses parallel trusted data and is targeted at selecting data to improve SMT; In addition to domain relevance, the work also examines its noise-screening capability; The method is tried on NMT and does not seem to improve. Previous work on domain data selection has shown that the order in which data are scheduled matters a lot for NMT training, a researc"
W18-6314,P18-1008,1,0.810911,"training loss term (thus the training process). 5 Our Approach We first define how to measure noise with the help of the small trusted dataset. Then we use it to control the schedule of the data batches to train the NMT model. Online NMT Training We usually train NMT models with online optimization, e.g., stochastic gradient descent. At a time step t, we have an NMT model p(y|x; θt ) translating from sentence x to y with parameterization θt . The model choice could be, for example, RNN-based (Wu et al., 2016), CNNbased (Gehring et al., 2017), Transformer model (Vaswani et al., 2017) or RNMT+ (Chen et al., 2018). To move p(y|x; θt ) to next step, t + 1, a random data batch bt is normally used to compute the cross entropy loss. The prediction accuracy of p(y|x; θt ) does not depend on the data of this batch alone, but on all data the model has seen so far. 1 The Denoising Problem 5.1 Incremental denoising with trusted data e trained on noisy data D, e Given a model p(y|x; θ) a practical way to denoise it with a small amount b would be to simply fine-tune the of trusted data D model on the trusted data, considering that a small trusted dataset alone is not enough to reliably train an NMT model from scr"
W18-6314,W18-2709,0,0.367368,"different quality that has been shown to affect NMT performance in particular. In MT, the use of web crawl, automatic methods for parallel data mining, sentence alignment provide us with parallel data of variable quality from many points of view: sentence breaking, poor sentence alignments, translations, domain adequacy, tokenization and so forth. To deal with such data noise, a commonly used practice is (static) data filtering with simple heuristics or classification. The NMT community increasingly realizes that this type of quality matters for general NMT translation accuracy. For example, (Khayrallah and Koehn, 2018) studies the types of data noise and their impact on NMT; WMT 2018 introduces a Parallel Corpus Filtering task on noisy webcrawled data. Unfortunately, the ingredients that made domain data selection methods successful have not been studied in the NMT denoising context. Specifically, Measuring domain relevance of data and identifying or selecting well-fit domain data for machine translation (MT) is a well-studied topic, but denoising is not yet. Denoising is concerned with a different type of data quality and tries to reduce the negative impact of data noise on MT training, in particular, neur"
W18-6314,N18-1136,0,0.0628559,"Missing"
W18-6314,W04-3250,0,0.0770732,"tstrapped test at p < 0.05, P3 is significantly better than P2, P3 than P1, P2 than P1, on all test sets. W3 is significantly better than W1 on n2014. P2 vs P3 shows that the online denoising approach reduced the training noise further more and gains +1.2 n2014 BLEU, +1.9 d2015 BLEU and +2.2 patent BLEU, on top of incremental denoising on trusted data. On the WMT dataset, W2 vs W3 shows that, even though the trusted data does not directly help, the online denoising helps by +0.7 n2014 BLEU, +0.6 d2015 BLEU and +0.4 patent BLEU. We carried out paired bootstrapped statistical significance test (Koehn, 2004) between systems, at p < 0.05, P3 is significantly better than P2, P3 than P1, P2 than P1, across all test sets; W3 is significantly better than W1 only on n2014. seems slightly better than the WMT one in discerning noisier sentence pairs. We speculate this is because the noisy Paracrawl data “amplifies” the contrastive effect of the pair of models. 6.4 +7.5 BLEU on patent. The Paracrawl experiments and the above rating ranking curves (Figure 1) indicate the power of simple incremental denoising on trusted data (Section 5.1) when the background data is very noisy. In NMT domain adaptation lite"
W18-6314,D17-1155,0,0.078132,"Missing"
W18-6314,D17-1147,0,0.0837062,"Missing"
W18-6314,2011.iwslt-papers.5,0,0.0428863,"cross entropy difference (CED) between an in-domain and an out-of-domain language models. For example, (Moore and Lewis, 2010) selects LM training data with CED according to an in-domain LM and a generic LM. (Axelrod et al., 2011) propose the contrastive data selection idea to select parallel domain data. It ranks data by the bilingual CED that is computed, for each language, with a generic n-gram LM and a domain one. Even more recently, (van der Wees et al., 2017) employ a neural-network version of it along with a dynamic data selection idea and achieve better domain data selection outcome. (Mansour et al., 2011) compute the CED using IBM translation Model 1 and achieve the best domain data selection/filtering effect for SMT combined with LM selection; The case of partial or misalignments with a bilingual scoring mechanism rather than LMs is also discussed. Another effective method to distinguish domain relevance is to build a classifier. A small amount of trusted parallel data is used in classifier training. For example, (Chen and Huang, 2016) use semisupervised convolutional neural networks (CNNs) as LMs to select domain data. Trusted data is used to adapt the classifier/selector. (Chen et al., 2016"
W18-6314,D09-1074,0,0.0592484,"mall amount of trusted data to help models measure noise in a sentence pair. The noise is defined based on comparison between a pair of a noisy NMT model and another, slightly denoised NMT model, inspired by the contrastive in-domain LM vs out-of-domain LM idea. It employs online data selection to sort sentence pairs by noise level so that the model is trained on gradually noise-reduced data batches. We show that language model based domain data selection method as is does not work well whereas the proposed approach is quite effective in denoising NMT training. 134 4 2017; Britz et al., 2017; Matsoukas et al., 2009). In the context of denoising, the quality that the ordering uses would be the amount of noise in a sentence pair, not (only) how much the data fits the domain of interest. SMT models tend to be fairly robust to data noise and denoising in SMT seems to have been a lightly studied topic. For example, (Mediani, 2017) uses a small, clean seed corpus and designs classifier filter to identify noisy data with lexical features; and also there is a nice list of works accumulated over years, compiled on the SMT Research Survey Wiki1 . The importance of NMT denoising has been increasingly realized. (Kha"
W18-6314,P10-2041,0,0.119934,"133–143 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64014 zh zh-gloss en 2 gongche zhan zai nali? bus stop is where? Where is the bus stop? For bus 81. Related Research One line of research that is related to our work is data selection for machine translation. It has been mostly studied in the domain adaptation context. Under this context, a popular metric to measure domain relevance of data is based on cross entropy difference (CED) between an in-domain and an out-of-domain language models. For example, (Moore and Lewis, 2010) selects LM training data with CED according to an in-domain LM and a generic LM. (Axelrod et al., 2011) propose the contrastive data selection idea to select parallel domain data. It ranks data by the bilingual CED that is computed, for each language, with a generic n-gram LM and a domain one. Even more recently, (van der Wees et al., 2017) employ a neural-network version of it along with a dynamic data selection idea and achieve better domain data selection outcome. (Mansour et al., 2011) compute the CED using IBM translation Model 1 and achieve the best domain data selection/filtering effec"
W18-6314,W18-6319,0,0.0606779,"Missing"
W18-6314,E17-2045,0,0.0549295,"hod that uses CNNs on bitokens; The method uses parallel trusted data and is targeted at selecting data to improve SMT; In addition to domain relevance, the work also examines its noise-screening capability; The method is tried on NMT and does not seem to improve. Previous work on domain data selection has shown that the order in which data are scheduled matters a lot for NMT training, a research that is relevant to curriculum learning (Bengio et al., 2009) in machine learning literature. (van der Wees et al., 2017) show the effectiveness of a nice “gradually-refining” dynamic data schedule. (Sajjad et al., 2017) find the usefulness of a similar idea, called model stacking for NMT domain adaptation. Data ordering could be viewed as a way of data weighting, which can be also done by example weighting/mixing, e.g., (Wang et al., Table 1: A noisy sentence pair. a part of the English sentence does not align to anything on the Chinese side, yet the pair contains some translation and the sentences are fluent. An LM-based domain-data selection method would generally treat it as a suitable domain example for building a travel NMT model and may not consider this noise. A simple data filtering method based on l"
wang-etal-2012-evaluation,P04-1053,0,\N,Missing
wang-etal-2012-evaluation,P08-1004,0,\N,Missing
wang-etal-2012-evaluation,N06-1039,0,\N,Missing
