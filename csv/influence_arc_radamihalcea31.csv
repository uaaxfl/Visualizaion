2006.amta-papers.14,H92-1022,0,0.0184319,"and the corresponding votes: Upload an image for a selected concept (+5); Image validation – well related to the concept 4 A System for Automatic Pictorial Translations The automatic translation of an input text into pictures is a non-trivial task, since the goal is to generate pictorial representations that are highly correlated with the words in the source sentence, thus effecting a level of understanding for the pictorial translations which would be comparable to that for the linguistic representations alone. Starting with an input sentence, the text is tokenized and part-of-speech tagged (Brill, 1992), and word lemmas are identified using a WordNet-based lemmatizer. Next, we attempt to identify the most likely meaning for each open-class word using a publicly available state-of-the-art sense tagger that 122 1 http://tell.fll.purdue.edu/JapanProj/FLClipart/ identifies the meaning of words in unrestricted text with respect to the WordNet sense inventory (Mihalcea and Csomai, 2005). Once the text is pre-processed, and the open-class words are labeled with their parts-of-speech and corresponding word meanings, we use PicNet to identify pictorial representations for each noun and verb. We suppl"
2006.amta-papers.14,P05-3014,1,0.802638,"thus effecting a level of understanding for the pictorial translations which would be comparable to that for the linguistic representations alone. Starting with an input sentence, the text is tokenized and part-of-speech tagged (Brill, 1992), and word lemmas are identified using a WordNet-based lemmatizer. Next, we attempt to identify the most likely meaning for each open-class word using a publicly available state-of-the-art sense tagger that 122 1 http://tell.fll.purdue.edu/JapanProj/FLClipart/ identifies the meaning of words in unrestricted text with respect to the WordNet sense inventory (Mihalcea and Csomai, 2005). Once the text is pre-processed, and the open-class words are labeled with their parts-of-speech and corresponding word meanings, we use PicNet to identify pictorial representations for each noun and verb. We supply PicNet with the lemma, part-of-speech, and sense number, and retrieve the highest ranked picture from the collection of concept/image associations available in PicNet. To avoid introducing errors in the pictorial translation, we use only those concept/image associations that rank above a threshold score of 4, indicating a high quality association. 5 Experiments and Evaluation Thro"
2006.amta-papers.14,P02-1040,0,0.0813909,"Missing"
2006.amta-papers.14,2003.mtsummit-papers.51,0,0.0220617,"ndard reference translation was preserved in the candidate interpretation. The assessment is done on a scale from 1 (“none of it”) to 5 (“all the information”)5 . Second, we use two automatic evaluations of quality traditionally used in machine translation evaluation. The NIST evaluation (Doddington, 2002) is based on the Bleu score (Papineni et al., 2002). It is an information-weighted measure of the precision of unigrams, bigrams, trigrams, fourgrams, and five-grams in the candidate interpretations with respect to the “gold-standard” reference translation. The other metric is the GTM score (Turian et al., 2003), which measures the similarity between texts in terms of precision, recall, and Fmeasure. Both measures were found to have good performance at discriminating translation quality, with high correlations to human judgments. 5.2 Results For each sentence in our testbed and for each possible translation, we collected interpretations from fifteen different users, accounting for a total of 2,250 interpretations. No Chinese speakers were allowed to participate in the evaluations, since Chinese was the “unknown” language used in our experiments. The user group included different ethnic groups, e.g. H"
2020.aacl-main.44,P18-1198,0,0.0861355,"nthetic tasks, such as word similarity and word analogy tasks (Mikolov et al., 2013; Pennington et al., 2014). However, word embeddings are widely used because of their superior performance on a variety of downstream NLP tasks when compared to other word representations. Performance on downstream tasks has been used to evaluate sentence embeddings, however such approaches cannot gauge the content that is actually captured in the embeddings. To systematically ascertain what information is encoded in sentence vectors, researchers have turned to probing tasks (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018). These are meant to address the question “what information is encoded in a sentence vector” at a higher level. In our work, we find inspiration in the research by Conneau et al. (2018), who propose a formalized evaluation technique for sentence embeddings using a suite of ten classification tasks focusing on: (1) surface information (e.g., length, word content), (2) syntactic information (e.g., bigram shift, tree depth), and (3) semantic information (e.g., tense). The deep learning methods gave the best results overall, but the bag-of-vectors approach was a solid baseline for the word content"
2020.aacl-main.44,N19-1423,0,0.0129105,"corporate both movement patterns and text data improves our ability to model downstream tasks. We see that although we are not able to recover as much surface level information from embeddings of location sequences as we are from a simpler representation, the additional semantic information that is encoded allows us to better predict some user attributes. 2 Related Work Embedding Evaluation and Probing. Word embeddings are now widely used to create word representations using methods such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019). BERT and ELMo can be used to create contextualized word embeddings, in which the vector representing an individual word varies depending on the context in which it appears. Previous methods including word2vec and GloVe did not make this distinction; adding context helped BERT achieve state-of-the-art results on many downstream NLP tasks. One traditional benchmark for word embeddings is performance on synthetic tasks, such as word similarity and word analogy tasks (Mikolov et al., 2013; Pennington et al., 2014). However, word embeddings are widely used because of their superior performance on"
2020.aacl-main.44,N15-1184,0,0.0335297,"Missing"
2020.aacl-main.44,D14-1162,0,0.0914265,"al Linguistics show that using dense location embeddings that incorporate both movement patterns and text data improves our ability to model downstream tasks. We see that although we are not able to recover as much surface level information from embeddings of location sequences as we are from a simpler representation, the additional semantic information that is encoded allows us to better predict some user attributes. 2 Related Work Embedding Evaluation and Probing. Word embeddings are now widely used to create word representations using methods such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019). BERT and ELMo can be used to create contextualized word embeddings, in which the vector representing an individual word varies depending on the context in which it appears. Previous methods including word2vec and GloVe did not make this distinction; adding context helped BERT achieve state-of-the-art results on many downstream NLP tasks. One traditional benchmark for word embeddings is performance on synthetic tasks, such as word similarity and word analogy tasks (Mikolov et al., 2013; Pennington et al., 2014). However, word embeddi"
2020.aacl-main.44,N18-1202,0,0.0462001,"ense location embeddings that incorporate both movement patterns and text data improves our ability to model downstream tasks. We see that although we are not able to recover as much surface level information from embeddings of location sequences as we are from a simpler representation, the additional semantic information that is encoded allows us to better predict some user attributes. 2 Related Work Embedding Evaluation and Probing. Word embeddings are now widely used to create word representations using methods such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019). BERT and ELMo can be used to create contextualized word embeddings, in which the vector representing an individual word varies depending on the context in which it appears. Previous methods including word2vec and GloVe did not make this distinction; adding context helped BERT achieve state-of-the-art results on many downstream NLP tasks. One traditional benchmark for word embeddings is performance on synthetic tasks, such as word similarity and word analogy tasks (Mikolov et al., 2013; Pennington et al., 2014). However, word embeddings are widely used because"
2020.aacl-main.44,D16-1159,0,0.0122148,"word embeddings is performance on synthetic tasks, such as word similarity and word analogy tasks (Mikolov et al., 2013; Pennington et al., 2014). However, word embeddings are widely used because of their superior performance on a variety of downstream NLP tasks when compared to other word representations. Performance on downstream tasks has been used to evaluate sentence embeddings, however such approaches cannot gauge the content that is actually captured in the embeddings. To systematically ascertain what information is encoded in sentence vectors, researchers have turned to probing tasks (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018). These are meant to address the question “what information is encoded in a sentence vector” at a higher level. In our work, we find inspiration in the research by Conneau et al. (2018), who propose a formalized evaluation technique for sentence embeddings using a suite of ten classification tasks focusing on: (1) surface information (e.g., length, word content), (2) syntactic information (e.g., bigram shift, tree depth), and (3) semantic information (e.g., tense). The deep learning methods gave the best results overall, but the bag-of-vectors approach"
2020.acl-main.292,P18-1099,0,0.04463,"Missing"
2020.acl-main.292,W11-1707,0,0.0297849,"cross-domain SA, word polarities might vary among different domains. For example, heavy can be a positive feature for a truck, but a negative feature for a smartphone. It is, however, difficult to assign contextual-polarities solely from data, especially when there is no supervision (Boia et al., 2014). In this domain-specific scenario, commonsense knowledge provides a dynamic way to enhance the context and help models understand sentimentbearing terms and opinion targets through its structural relations (Cambria et al., 2018). They also often aid in unearthing implicitly expressed sentiment (Balahur et al., 2011). Second, domains often share relations through latent semantic concepts (Kim et al., 2017a). For example, notions of wallpaper (from electronics) and sketch (from books) can be associated via related concepts such as design (see Fig. 1). Multi-relational KBs provide a natural way to leverage such inter-domain relationships. These connections can help models understand target-specific terms by associating to known domain-general or even source-specific concepts. Following these intuitions, we propose a twostep modular framework, KinGDOM (KnowledgeGuided Domain adaptation), which utilizes commo"
2020.acl-main.292,K15-1006,0,0.0197772,"on GCNs. Such ideas have been explored in vector-based approaches (Glorot et al., 2011; Chen et al., 2012). Sentiment Analysis. One line of work models domain-dependent word embeddings (Sarma et al., 2018; Shi et al., 2018; K Sarma et al., 2019) or domain-specific sentiment lexicons (Hamilton et al., 2016a), while others attempt to learn representations based on co-occurrences of domainspecific with domain-independent terms (Blitzer et al., 2007a; Pan et al., 2010; Sharma et al., 2018). Our work is related to approaches that address domain-specificity in the target domain (Peng et al., 2018b; Bhatt et al., 2015). Works like Liu et al. (2018) attempts to model target-specificity by mapping domain-general information to domainspecific representations by using domain descriptor vectors. In contrast, we address relating domainspecific terms by modeling their relations with the other terms in knowledge bases like ConceptNet. crepancies in their feature distributions. In our setup, we consider two domains: source Ds and target domain Dt with different marginal data distributions, i.e., PDs (x) ≠ PDt (x). This scenario, also known as the covariate shift (Elsahar and Gall´e, 2019), is predominant in SA appli"
2020.acl-main.292,D19-1255,0,0.0242833,"lude learning domain-specific sentiment words/lexicons (Sarma et al., 2018; Hamilton et al., 2016b), co-occurrence based learning (Blitzer et al., 2007a), domainadversarial learning (Ganin et al., 2016), among others. In this work, we adopt the domainadversarial framework and attempt to improve it further by infusing commonsense knowledge using ConceptNet – a large-scale knowledge graph (Speer et al., 2017). Augmenting neural models with external knowledge bases (KB) has shown benefits across a range of NLP applications (Peters et al., 2019; Li et al., 2019; IV et al., 2019; liu et al., 2019; Bi et al., 2019). Despite their popularity, efforts to incorporate KBs into the domain-adaptation framework has been sporadic (Wang et al., 2008; Xiang et al., 2010). To this end, we identify multiple advantages of using commonsense KBs for domain adaptation. First, KBs help in grounding text to real enti3198 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3198–3210 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ties, factual knowledge, and commonsense concepts. Commonsense KBs, in particular, provide a rich source of background concepts–rela"
2020.acl-main.292,P07-1056,0,0.173959,"ons (Zhang et al., 2018). Current models trained for this task, however, cannot be reliably deployed due to the distributional mismatch between the training and evaluation domains (Daum´e III and Marcu, 2006). Domain adaptation, a case of transductive transfer learning, is a widely studied field of research that can be effectively used to tackle this problem (Wilson and Cook, 2018). Research in the field of cross-domain SA has proposed diverse approaches, which include learning domain-specific sentiment words/lexicons (Sarma et al., 2018; Hamilton et al., 2016b), co-occurrence based learning (Blitzer et al., 2007a), domainadversarial learning (Ganin et al., 2016), among others. In this work, we adopt the domainadversarial framework and attempt to improve it further by infusing commonsense knowledge using ConceptNet – a large-scale knowledge graph (Speer et al., 2017). Augmenting neural models with external knowledge bases (KB) has shown benefits across a range of NLP applications (Peters et al., 2019; Li et al., 2019; IV et al., 2019; liu et al., 2019; Bi et al., 2019). Despite their popularity, efforts to incorporate KBs into the domain-adaptation framework has been sporadic (Wang et al., 2008; Xiang"
2020.acl-main.292,C18-1279,0,0.0235806,"private encoders that model both domain-invariant and specific features (Li et al., 2012; Bousmalis et al., 2016a; Kim et al., 2017b; Chang et al., 2019), using adversarial and orthogonality losses (Liu et al., 2017; Li et al., 2018). Although we do not use private encoders, we posit that our model is capable of capturing domain-specificity via the sentencespecific concept graph. Also, our approach is flexible enough to be adapted to the setup of sharedprivate encoders. External Knowledge. Use of external knowledge has been explored in both inductive and transductive settings (Banerjee, 2007; Deng et al., 2018). Few works have explored external knowledge in domain adaptation based on Wikipedia as auxiliary information, using co-clustering (Wang et al., 2008) and semi-supervised learning (SSL) (Xiang et al., 2010). SSL has also been explored by Alam 3199 et al. (2018) in the Twitter domain. Although we share a similar motivation, there exist crucial differences. Primarily, we learn graph embeddings at the concept level, not across complete instances. Also, we do not classify each concept node in the graph, which renders SSL inapplicable to our setup. Domain Adaptation on Graphs. With the advent of gr"
2020.acl-main.292,D19-1222,0,0.0334321,"Missing"
2020.acl-main.292,D19-1015,1,0.89464,"Missing"
2020.acl-main.292,D16-1057,0,0.126588,"lysis (SA) is a popular NLP task used in many applications (Zhang et al., 2018). Current models trained for this task, however, cannot be reliably deployed due to the distributional mismatch between the training and evaluation domains (Daum´e III and Marcu, 2006). Domain adaptation, a case of transductive transfer learning, is a widely studied field of research that can be effectively used to tackle this problem (Wilson and Cook, 2018). Research in the field of cross-domain SA has proposed diverse approaches, which include learning domain-specific sentiment words/lexicons (Sarma et al., 2018; Hamilton et al., 2016b), co-occurrence based learning (Blitzer et al., 2007a), domainadversarial learning (Ganin et al., 2016), among others. In this work, we adopt the domainadversarial framework and attempt to improve it further by infusing commonsense knowledge using ConceptNet – a large-scale knowledge graph (Speer et al., 2017). Augmenting neural models with external knowledge bases (KB) has shown benefits across a range of NLP applications (Peters et al., 2019; Li et al., 2019; IV et al., 2019; liu et al., 2019; Bi et al., 2019). Despite their popularity, efforts to incorporate KBs into the domain-adaptation"
2020.acl-main.292,P16-1141,0,0.0772723,"lysis (SA) is a popular NLP task used in many applications (Zhang et al., 2018). Current models trained for this task, however, cannot be reliably deployed due to the distributional mismatch between the training and evaluation domains (Daum´e III and Marcu, 2006). Domain adaptation, a case of transductive transfer learning, is a widely studied field of research that can be effectively used to tackle this problem (Wilson and Cook, 2018). Research in the field of cross-domain SA has proposed diverse approaches, which include learning domain-specific sentiment words/lexicons (Sarma et al., 2018; Hamilton et al., 2016b), co-occurrence based learning (Blitzer et al., 2007a), domainadversarial learning (Ganin et al., 2016), among others. In this work, we adopt the domainadversarial framework and attempt to improve it further by infusing commonsense knowledge using ConceptNet – a large-scale knowledge graph (Speer et al., 2017). Augmenting neural models with external knowledge bases (KB) has shown benefits across a range of NLP applications (Peters et al., 2019; Li et al., 2019; IV et al., 2019; liu et al., 2019; Bi et al., 2019). Despite their popularity, efforts to incorporate KBs into the domain-adaptation"
2020.acl-main.292,P19-1598,0,0.0303044,"ically for domain adaptation; MT-Tri (Ruder and Plank, 2018) is similar to Asym, but uses multi-task learning; Domain Separation Networks (DSN) (Bousmalis et al., 2016b) learns to extract shared and private components of each domain. As per Peng et al. (2018a), it stands as the present state-of-the-art method for unsupervised domain adaptation; Task Refinement Learning (TRL) (Ziser and Reichart, 2019) Task Refinement Learning is an unsupervised domain adaptation framework which iteratively trains a Pivot Based Language Model to gradually increase the information exposed about each pivot; TAT (Liu et al., 2019) is the transferable adversarial training setup to generate examples which helps in modelling the domain shift. TAT adversarially trains classifiers to make consistent predictions over these transferable examples; CoCMD (Peng et al., 2018a) is a co-training method based on the CMD regularizer which trains a classifier on simultaneously extracted domain specific and invariant features. CoCOMD, however, is SSL-based as it uses labeled data from the target domain. Although it falls outside the regime of unsupervised domain adaptation, we report its results to provide a full picture to the reader."
2020.acl-main.292,P07-1034,0,0.313395,"ments, that KinGDOM surpasses state-of-the-art methods on the Amazon-reviews dataset (Blitzer et al., 2007b), thus validating our claim that external knowledge can aid the task of cross-domain SA. In the remaining paper, §2 explains related works and compares KinGDOM to them; §3 presents task definition and preliminaries; §4 introduces our proposed framework, KinGDOM; §5 discusses experimental setup followed by results and extensive analyses in §6; finally, §7 concludes this paper. 2 Related Work Domain adaptation methods can be broadly categorized into three approaches: a) instanceselection (Jiang and Zhai, 2007; Chen et al., 2011; Cao et al., 2018), b) self-labeling (He and Zhou, 2011) and c) representation learning (Glorot et al., 2011; Chen et al., 2012; Tzeng et al., 2014). Our focus is on the third category which has emerged as a popular approach in this deep representation learning era (Ruder, 2019; Poria et al., 2020). Domain-adversarial Training. Our work deals with domain-adversarial approaches (Kouw and Loog, 2019), where we extend DANN Ganin et al. (2016). Despite its popularity, DANN cannot model domain-specific information (e.g. indicators of tasty, delicious for kitchen domain) (Peng et"
2020.acl-main.292,D19-1557,0,0.016831,"rossdomain connected graphs, co-regularized training (Ni et al., 2018) and joint-embedding (Xu et al., 2017) have been explored. We also utilize GCNs to learn node representations in our cross-domain ConceptNet graph. However, rather than using explicit divergence measures or domain-adversarial losses for domain invariance, we uniquely adopt a shared-autoencoder strategy on GCNs. Such ideas have been explored in vector-based approaches (Glorot et al., 2011; Chen et al., 2012). Sentiment Analysis. One line of work models domain-dependent word embeddings (Sarma et al., 2018; Shi et al., 2018; K Sarma et al., 2019) or domain-specific sentiment lexicons (Hamilton et al., 2016a), while others attempt to learn representations based on co-occurrences of domainspecific with domain-independent terms (Blitzer et al., 2007a; Pan et al., 2010; Sharma et al., 2018). Our work is related to approaches that address domain-specificity in the target domain (Peng et al., 2018b; Bhatt et al., 2015). Works like Liu et al. (2018) attempts to model target-specificity by mapping domain-general information to domainspecific representations by using domain descriptor vectors. In contrast, we address relating domainspecific te"
2020.acl-main.292,P17-1119,0,0.243223,"a positive feature for a truck, but a negative feature for a smartphone. It is, however, difficult to assign contextual-polarities solely from data, especially when there is no supervision (Boia et al., 2014). In this domain-specific scenario, commonsense knowledge provides a dynamic way to enhance the context and help models understand sentimentbearing terms and opinion targets through its structural relations (Cambria et al., 2018). They also often aid in unearthing implicitly expressed sentiment (Balahur et al., 2011). Second, domains often share relations through latent semantic concepts (Kim et al., 2017a). For example, notions of wallpaper (from electronics) and sketch (from books) can be associated via related concepts such as design (see Fig. 1). Multi-relational KBs provide a natural way to leverage such inter-domain relationships. These connections can help models understand target-specific terms by associating to known domain-general or even source-specific concepts. Following these intuitions, we propose a twostep modular framework, KinGDOM (KnowledgeGuided Domain adaptation), which utilizes commonsense KB for domain adaptation. KinGDOM first trains a shared graph autoencoder using a g"
2020.acl-main.292,D19-1022,0,0.0568691,"Missing"
2020.acl-main.292,N18-2076,0,0.349152,"s, we posit that our model is capable of capturing domain-specificity via the sentencespecific concept graph. Also, our approach is flexible enough to be adapted to the setup of sharedprivate encoders. External Knowledge. Use of external knowledge has been explored in both inductive and transductive settings (Banerjee, 2007; Deng et al., 2018). Few works have explored external knowledge in domain adaptation based on Wikipedia as auxiliary information, using co-clustering (Wang et al., 2008) and semi-supervised learning (SSL) (Xiang et al., 2010). SSL has also been explored by Alam 3199 et al. (2018) in the Twitter domain. Although we share a similar motivation, there exist crucial differences. Primarily, we learn graph embeddings at the concept level, not across complete instances. Also, we do not classify each concept node in the graph, which renders SSL inapplicable to our setup. Domain Adaptation on Graphs. With the advent of graph neural networks, graph-based methods have become a new trend (Ghosal et al., 2019) in diverse NLP tasks such as emotion recognition in conversations (Poria et al., 2019). Graphbased domain adaptation is categorized based on the availability of cross-domain"
2020.acl-main.292,P17-1001,0,0.0886816,"Missing"
2020.acl-main.292,N18-1050,0,0.02275,"plored in vector-based approaches (Glorot et al., 2011; Chen et al., 2012). Sentiment Analysis. One line of work models domain-dependent word embeddings (Sarma et al., 2018; Shi et al., 2018; K Sarma et al., 2019) or domain-specific sentiment lexicons (Hamilton et al., 2016a), while others attempt to learn representations based on co-occurrences of domainspecific with domain-independent terms (Blitzer et al., 2007a; Pan et al., 2010; Sharma et al., 2018). Our work is related to approaches that address domain-specificity in the target domain (Peng et al., 2018b; Bhatt et al., 2015). Works like Liu et al. (2018) attempts to model target-specificity by mapping domain-general information to domainspecific representations by using domain descriptor vectors. In contrast, we address relating domainspecific terms by modeling their relations with the other terms in knowledge bases like ConceptNet. crepancies in their feature distributions. In our setup, we consider two domains: source Ds and target domain Dt with different marginal data distributions, i.e., PDs (x) ≠ PDt (x). This scenario, also known as the covariate shift (Elsahar and Gall´e, 2019), is predominant in SA applications and arises primarily w"
2020.acl-main.292,D19-1187,0,0.0159474,"roaches, which include learning domain-specific sentiment words/lexicons (Sarma et al., 2018; Hamilton et al., 2016b), co-occurrence based learning (Blitzer et al., 2007a), domainadversarial learning (Ganin et al., 2016), among others. In this work, we adopt the domainadversarial framework and attempt to improve it further by infusing commonsense knowledge using ConceptNet – a large-scale knowledge graph (Speer et al., 2017). Augmenting neural models with external knowledge bases (KB) has shown benefits across a range of NLP applications (Peters et al., 2019; Li et al., 2019; IV et al., 2019; liu et al., 2019; Bi et al., 2019). Despite their popularity, efforts to incorporate KBs into the domain-adaptation framework has been sporadic (Wang et al., 2008; Xiang et al., 2010). To this end, we identify multiple advantages of using commonsense KBs for domain adaptation. First, KBs help in grounding text to real enti3198 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3198–3210 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ties, factual knowledge, and commonsense concepts. Commonsense KBs, in particular, provide a rich source of backgr"
2020.acl-main.292,P18-1233,0,0.224402,"i, 2007; Chen et al., 2011; Cao et al., 2018), b) self-labeling (He and Zhou, 2011) and c) representation learning (Glorot et al., 2011; Chen et al., 2012; Tzeng et al., 2014). Our focus is on the third category which has emerged as a popular approach in this deep representation learning era (Ruder, 2019; Poria et al., 2020). Domain-adversarial Training. Our work deals with domain-adversarial approaches (Kouw and Loog, 2019), where we extend DANN Ganin et al. (2016). Despite its popularity, DANN cannot model domain-specific information (e.g. indicators of tasty, delicious for kitchen domain) (Peng et al., 2018b). Rectifications include shared-private encoders that model both domain-invariant and specific features (Li et al., 2012; Bousmalis et al., 2016a; Kim et al., 2017b; Chang et al., 2019), using adversarial and orthogonality losses (Liu et al., 2017; Li et al., 2018). Although we do not use private encoders, we posit that our model is capable of capturing domain-specificity via the sentencespecific concept graph. Also, our approach is flexible enough to be adapted to the setup of sharedprivate encoders. External Knowledge. Use of external knowledge has been explored in both inductive and trans"
2020.acl-main.292,D14-1162,0,0.0842732,"Missing"
2020.acl-main.292,D19-1005,0,0.0600102,"Missing"
2020.acl-main.292,N19-5004,0,0.028386,"d preliminaries; §4 introduces our proposed framework, KinGDOM; §5 discusses experimental setup followed by results and extensive analyses in §6; finally, §7 concludes this paper. 2 Related Work Domain adaptation methods can be broadly categorized into three approaches: a) instanceselection (Jiang and Zhai, 2007; Chen et al., 2011; Cao et al., 2018), b) self-labeling (He and Zhou, 2011) and c) representation learning (Glorot et al., 2011; Chen et al., 2012; Tzeng et al., 2014). Our focus is on the third category which has emerged as a popular approach in this deep representation learning era (Ruder, 2019; Poria et al., 2020). Domain-adversarial Training. Our work deals with domain-adversarial approaches (Kouw and Loog, 2019), where we extend DANN Ganin et al. (2016). Despite its popularity, DANN cannot model domain-specific information (e.g. indicators of tasty, delicious for kitchen domain) (Peng et al., 2018b). Rectifications include shared-private encoders that model both domain-invariant and specific features (Li et al., 2012; Bousmalis et al., 2016a; Kim et al., 2017b; Chang et al., 2019), using adversarial and orthogonality losses (Liu et al., 2017; Li et al., 2018). Although we do not"
2020.acl-main.292,P18-1096,0,0.264852,"adaptation in SA (Blitzer et al., 2007b). This corpus consists of Amazon product reviews and ranges across four domains: Books, DVDs, Electronics, and Kitchen appliances. Each review is associated with a rating denoting its sentiment polarity. Reviews with rating up to 3 stars are considered to contain negative sentiment and 4 or 5 stars as positive sentiment. The dataset follows a balanced distribution between both labels yielding 2k unlabelled training instances for each domain. Testing contains 3k - 6k samples for evaluation. We follow similar pre-processing as bone by Ganin et al. (2016); Ruder and Plank (2018) where each review is encoded into a 5000-dimensional tfidf weighted bag-of-words (BOW) feature vector of unigrams and bigrams. 4.4 5.2 ′ through the 3) We then make a forward pass of GW encoder of the pre-trained graph autoencoder model. This results in feature vectors hj for all ′ unique nodes j in GW . 4) Finally, we average over the feature vectors hj ′ for all unique nodes in GW , to obtain the commonsense graph features xcg for document x. Step 2b) Domain-adversarial Training We feed the commonsense graph feature xcg pooled ′ from GW for document x (§4.3) into the DANN architecture (see"
2020.acl-main.292,W18-3407,0,0.118474,"uction Sentiment Analysis (SA) is a popular NLP task used in many applications (Zhang et al., 2018). Current models trained for this task, however, cannot be reliably deployed due to the distributional mismatch between the training and evaluation domains (Daum´e III and Marcu, 2006). Domain adaptation, a case of transductive transfer learning, is a widely studied field of research that can be effectively used to tackle this problem (Wilson and Cook, 2018). Research in the field of cross-domain SA has proposed diverse approaches, which include learning domain-specific sentiment words/lexicons (Sarma et al., 2018; Hamilton et al., 2016b), co-occurrence based learning (Blitzer et al., 2007a), domainadversarial learning (Ganin et al., 2016), among others. In this work, we adopt the domainadversarial framework and attempt to improve it further by infusing commonsense knowledge using ConceptNet – a large-scale knowledge graph (Speer et al., 2017). Augmenting neural models with external knowledge bases (KB) has shown benefits across a range of NLP applications (Peters et al., 2019; Li et al., 2019; IV et al., 2019; liu et al., 2019; Bi et al., 2019). Despite their popularity, efforts to incorporate KBs int"
2020.acl-main.292,P18-1089,0,0.0225729,"explicit divergence measures or domain-adversarial losses for domain invariance, we uniquely adopt a shared-autoencoder strategy on GCNs. Such ideas have been explored in vector-based approaches (Glorot et al., 2011; Chen et al., 2012). Sentiment Analysis. One line of work models domain-dependent word embeddings (Sarma et al., 2018; Shi et al., 2018; K Sarma et al., 2019) or domain-specific sentiment lexicons (Hamilton et al., 2016a), while others attempt to learn representations based on co-occurrences of domainspecific with domain-independent terms (Blitzer et al., 2007a; Pan et al., 2010; Sharma et al., 2018). Our work is related to approaches that address domain-specificity in the target domain (Peng et al., 2018b; Bhatt et al., 2015). Works like Liu et al. (2018) attempts to model target-specificity by mapping domain-general information to domainspecific representations by using domain descriptor vectors. In contrast, we address relating domainspecific terms by modeling their relations with the other terms in knowledge bases like ConceptNet. crepancies in their feature distributions. In our setup, we consider two domains: source Ds and target domain Dt with different marginal data distributions,"
2020.acl-main.292,P18-1232,0,0.0264515,"et al., 2019). For crossdomain connected graphs, co-regularized training (Ni et al., 2018) and joint-embedding (Xu et al., 2017) have been explored. We also utilize GCNs to learn node representations in our cross-domain ConceptNet graph. However, rather than using explicit divergence measures or domain-adversarial losses for domain invariance, we uniquely adopt a shared-autoencoder strategy on GCNs. Such ideas have been explored in vector-based approaches (Glorot et al., 2011; Chen et al., 2012). Sentiment Analysis. One line of work models domain-dependent word embeddings (Sarma et al., 2018; Shi et al., 2018; K Sarma et al., 2019) or domain-specific sentiment lexicons (Hamilton et al., 2016a), while others attempt to learn representations based on co-occurrences of domainspecific with domain-independent terms (Blitzer et al., 2007a; Pan et al., 2010; Sharma et al., 2018). Our work is related to approaches that address domain-specificity in the target domain (Peng et al., 2018b; Bhatt et al., 2015). Works like Liu et al. (2018) attempts to model target-specificity by mapping domain-general information to domainspecific representations by using domain descriptor vectors. In contrast, we address rel"
2020.acl-main.292,P19-1193,0,0.0313804,"al., 2010). To this end, we identify multiple advantages of using commonsense KBs for domain adaptation. First, KBs help in grounding text to real enti3198 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3198–3210 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ties, factual knowledge, and commonsense concepts. Commonsense KBs, in particular, provide a rich source of background concepts–related by commonsense links–which can enhance the semantics of a piece of text by providing both domainspecific and domain-general concepts (Yang et al., 2019; Zhong et al., 2019; Agarwal et al., 2015; Zhong et al., 2019) (see Fig. 1). For cross-domain SA, word polarities might vary among different domains. For example, heavy can be a positive feature for a truck, but a negative feature for a smartphone. It is, however, difficult to assign contextual-polarities solely from data, especially when there is no supervision (Boia et al., 2014). In this domain-specific scenario, commonsense knowledge provides a dynamic way to enhance the context and help models understand sentimentbearing terms and opinion targets through its structural relations (Cambria"
2020.acl-main.292,D19-1016,0,0.0307925,"Missing"
2020.acl-main.292,P19-1591,0,0.0199123,"feature representations by utilizing equivalent representation of probability distributions by moment sequences; Asym (Saito et al., 2017) is the asymmetric tri-training framework that uses three neural networks asymmetrically for domain adaptation; MT-Tri (Ruder and Plank, 2018) is similar to Asym, but uses multi-task learning; Domain Separation Networks (DSN) (Bousmalis et al., 2016b) learns to extract shared and private components of each domain. As per Peng et al. (2018a), it stands as the present state-of-the-art method for unsupervised domain adaptation; Task Refinement Learning (TRL) (Ziser and Reichart, 2019) Task Refinement Learning is an unsupervised domain adaptation framework which iteratively trains a Pivot Based Language Model to gradually increase the information exposed about each pivot; TAT (Liu et al., 2019) is the transferable adversarial training setup to generate examples which helps in modelling the domain shift. TAT adversarially trains classifiers to make consistent predictions over these transferable examples; CoCMD (Peng et al., 2018a) is a co-training method based on the CMD regularizer which trains a classifier on simultaneously extracted domain specific and invariant features."
2020.coling-main.144,W18-5513,0,0.043231,"nd truth text. Overall, the experiments show that focused summaries produced by Biased TextRank meaningfully improve over normal summaries when compared against a biased reference. We believe Biased TextRank is a better fit than conventional extractive summarization methods when there is a clear focus or bias required in the desired summary. 4.3 Explanation Extraction Introduced as “explanation generation” for fact-checking by Atanasova et al. (2020), this task focuses on extracting explanations from articles elaborating on the veracity of statements in the PolitiFact-based LIAR-PLUS dataset (Alhindi et al., 2018). The dataset consists of 2,533 data points split into 1,278 validation and 1,255 test. Each data point consists of a statement, its veracity (e.g., true, false, mostly-true), a detailed article justifying the assigned veracity of the statement by fact-checkers, and a closing paragraph summarizing the explanation of the verdict. The goal is to extract the closing statement (explanation) from the lengthy justifying article. Table 3 shows an example of explanation extraction on this dataset when using Biased TextRank. We designate the justification article as the input text and use the statement"
2020.coling-main.144,Q19-1038,0,0.015373,"traction – we believe this approach is generalizable to other applications that require content extraction and/or content ranking. The paper makes the following three main contributions: 1. Biased TextRank: We introduce an unsupervised graph-based algorithm for focused contentextraction that does not require training data, is fast, resource-efficient and easy to implement and fine-tune. Biased TextRank is language agnostic, in the sense that as long as document embedding models exist for a language, Biased TextRank can be directly applied. With the recent emergence of technologies like LASER (Artetxe and Schwenk, 2019) with pretrained language embeddings for 100+ languages, such representations are readily available for many languages. 2. Evaluation and extensive analyses of Biased TextRank: We show the effectiveness of Biased TextRank through experiments on two tasks: focused summarization and explanation extraction. We also perform an ablation study to show the effects of the TextRank damping factor and the similarity threshold parameters, providing insight on how Biased TextRank parameters should be tuned. 3. Focused summarization dataset: We introduce and make available a novel dataset for focused summa"
2020.coling-main.144,2020.acl-main.656,0,0.278153,"related work has been published in NLP venues (Daum´e III and Marcu, 2006), query-focused summarization has been mainly studied by the Information Retrieval community. 2.3 Explanation Extraction Model explainability (Poursabzi-Sangdeh et al., 2018; Lundberg and Lee, 2017) and natural language explanation extraction and generation (Kumar and Talukdar, 2020; Thorne et al., 2019) are broad and important topics of ongoing research within the AI and NLP communities. However explanation extraction in the context of fact-checking and misinformation detection has remained relatively understudied. In Atanasova et al. (2020), the authors address the task of extracting fact-checking explanations, in which statements documenting the veracity of a fact-checked statement are used to derive a short summary explanation. The authors propose a BERT (Devlin et al., 2019) based sentence selection model that identifies top relevant sentences from the input as candidate explanations. In similar context, highlighting natural language explanations for fact-checking and misinformation detection applications has been studied within the research community (Lu and Li, 2020; Popat et al., 2018). 3 Biased TextRank Biased TextRank bu"
2020.coling-main.144,P04-3031,0,0.18624,"ion. 4.1 Experimental Settings We implement Biased TextRank using the NLTK library and SBERT in Python. For sentence embedding retrieval, we use the pretrained, base SBERT model. We run our experiments on a machine using one Nvidia 1080 Ti GPU and the GPU is only used to make embedding retrieval faster. A run of Biased TextRank for large documents on a graph with approximately 1,000 nodes takes an average 1.6 seconds to complete. This measurement also includes the embedding retrieval time.2 Since all of our experiments focus on sentence extraction, we use the sentence tokenizer from the NLTK (Bird and Loper, 2004) library. During our evaluations we use the ROUGE (Lin, 2004) as the main performance metric. 4.2 Focused Summarization Focused summarization, much like query-focused summarization (its counterpart in information retrieval), aims to generate summaries for an input text with a given focus. To evaluate the applicability of Biased TextRank for extracting focused summaries, we collected a dataset of news reportage from Democrat and Republican media’s interpretations of the U.S. presidential debates from 1980 to 2016. We use the collected news reportage that summarize the events of the debates and"
2020.coling-main.144,C16-1053,0,0.0244714,"ocused Summarization Although focused summarization has not been widely studied within the NLP community, query-focused or query-biased summarization is a known problem in the context of Information Retrieval (Wang et al., 2007; Metzler and Kanungo, 2008; Zhao et al., 2009). Wang et al. (2007) proposed two extractive query-biased summarization methods (classification and ranking-based) for web page summarization. They extracted features from both the content and context of a web page and feed them to an SVM that solves both the classification and ranking problem formulations. More recently in Cao et al. (2016) the authors proposed AttSum, a system that leverages joint learning of query relevance and sentence salience ranking, the two main modules of query-focused summarization and achieve competitive results on the DUC (Dang, 2005) datasets. While related work has been published in NLP venues (Daum´e III and Marcu, 2006), query-focused summarization has been mainly studied by the Information Retrieval community. 2.3 Explanation Extraction Model explainability (Poursabzi-Sangdeh et al., 2018; Lundberg and Lee, 2017) and natural language explanation extraction and generation (Kumar and Talukdar, 2020"
2020.coling-main.144,P06-1039,0,0.208852,"Missing"
2020.coling-main.144,W19-4510,0,0.0884601,"al. (2019), training one large transformer-based model produces approximately four times more CO2 emissions than a car in its lifetime. These considerable negative environmental outcomes call for lighter and less resource-intensive alternative methods. TextRank (Mihalcea and Tarau, 2004) is a light-weight unsupervised graph-based content extraction algorithm that was initially designed for summarization and keyword extraction applications. Since its introduction, it has been adapted and used in numerous other applications and settings, including opinion mining (Petasis and Karkaletsis, 2016; Deguchi and Yamaguchi, 2019), credibility assessment (Balcerzak et al., 2014) and lyrics summarization (Son and Shin, 2018), among others. Most recently, TextRank has been included in the latest release of the popular spaCy library.1 There have been online tutorials and updating studies (Barrios et al., 2015) that demonstrate TextRank’s relevance years after its initial release. Some of the TextRank extensions that have been proposed in recent years rely on the idea of personalized (or topic-sensitive) PageRank (Haveliwala, 2003) and its successor algorithms. For instance, PositionRank (Florescu and Caragea, 2017) change"
2020.coling-main.144,N19-1423,0,0.00647989,"018; Lundberg and Lee, 2017) and natural language explanation extraction and generation (Kumar and Talukdar, 2020; Thorne et al., 2019) are broad and important topics of ongoing research within the AI and NLP communities. However explanation extraction in the context of fact-checking and misinformation detection has remained relatively understudied. In Atanasova et al. (2020), the authors address the task of extracting fact-checking explanations, in which statements documenting the veracity of a fact-checked statement are used to derive a short summary explanation. The authors propose a BERT (Devlin et al., 2019) based sentence selection model that identifies top relevant sentences from the input as candidate explanations. In similar context, highlighting natural language explanations for fact-checking and misinformation detection applications has been studied within the research community (Lu and Li, 2020; Popat et al., 2018). 3 Biased TextRank Biased TextRank builds upon the original TextRank algorithm, but changes how random restart probabilities are assigned, therefore giving higher likelihood to the nodes that are more relevant to a certain “focus” of the task. 3.1 Node Scoring with Random Restar"
2020.coling-main.144,2020.acl-main.771,0,0.053361,"tly in Cao et al. (2016) the authors proposed AttSum, a system that leverages joint learning of query relevance and sentence salience ranking, the two main modules of query-focused summarization and achieve competitive results on the DUC (Dang, 2005) datasets. While related work has been published in NLP venues (Daum´e III and Marcu, 2006), query-focused summarization has been mainly studied by the Information Retrieval community. 2.3 Explanation Extraction Model explainability (Poursabzi-Sangdeh et al., 2018; Lundberg and Lee, 2017) and natural language explanation extraction and generation (Kumar and Talukdar, 2020; Thorne et al., 2019) are broad and important topics of ongoing research within the AI and NLP communities. However explanation extraction in the context of fact-checking and misinformation detection has remained relatively understudied. In Atanasova et al. (2020), the authors address the task of extracting fact-checking explanations, in which statements documenting the veracity of a fact-checked statement are used to derive a short summary explanation. The authors propose a BERT (Devlin et al., 2019) based sentence selection model that identifies top relevant sentences from the input as cand"
2020.coling-main.144,2020.acl-main.610,0,0.0148984,"ish texts only. However, we believe that Biased TextRank is language-agnostic in the sense that if we have the proper tools to parse and embed non-English documents, the algorithm will be directly applicable. With recent advances in multilingual contextual embedding technologies like LASER (which provides embeddings for more than 100 languages), we think it is possible to immediately apply it to languages other than English. In future work we would like to explore the application of Biased TextRank beyond sentence extraction. For instance the “term-set expansion” task most recently tackled by Kushilevitz et al. (2020), in which an initial seed set of keywords are expanded by similar keywords found in a corpus, could be another application of our algorithm. The task can be modeled as a keyword extraction task similar to the example found in the original TextRank paper and the restart probabilities can be assigned based on node proximity to the initial seed set. 7 Conclusion In this paper, we introduced Biased TextRank, an unsupervised graph-based algorithm for directed extraction of content from text. Biased TextRank is unsupervised, fast and resource-efficient, languageagnostic and easy to implement. We de"
2020.coling-main.144,W04-1013,0,0.044109,"TK library and SBERT in Python. For sentence embedding retrieval, we use the pretrained, base SBERT model. We run our experiments on a machine using one Nvidia 1080 Ti GPU and the GPU is only used to make embedding retrieval faster. A run of Biased TextRank for large documents on a graph with approximately 1,000 nodes takes an average 1.6 seconds to complete. This measurement also includes the embedding retrieval time.2 Since all of our experiments focus on sentence extraction, we use the sentence tokenizer from the NLTK (Bird and Loper, 2004) library. During our evaluations we use the ROUGE (Lin, 2004) as the main performance metric. 4.2 Focused Summarization Focused summarization, much like query-focused summarization (its counterpart in information retrieval), aims to generate summaries for an input text with a given focus. To evaluate the applicability of Biased TextRank for extracting focused summaries, we collected a dataset of news reportage from Democrat and Republican media’s interpretations of the U.S. presidential debates from 1980 to 2016. We use the collected news reportage that summarize the events of the debates and apply Biased TextRank to reproduce the biased interpretations"
2020.coling-main.144,2020.acl-main.48,0,0.0613653,"detection has remained relatively understudied. In Atanasova et al. (2020), the authors address the task of extracting fact-checking explanations, in which statements documenting the veracity of a fact-checked statement are used to derive a short summary explanation. The authors propose a BERT (Devlin et al., 2019) based sentence selection model that identifies top relevant sentences from the input as candidate explanations. In similar context, highlighting natural language explanations for fact-checking and misinformation detection applications has been studied within the research community (Lu and Li, 2020; Popat et al., 2018). 3 Biased TextRank Biased TextRank builds upon the original TextRank algorithm, but changes how random restart probabilities are assigned, therefore giving higher likelihood to the nodes that are more relevant to a certain “focus” of the task. 3.1 Node Scoring with Random Restart Probabilities TextRank Node Scoring. TextRank operates on graphs that are built from natural language texts. For instance, in the original TextRank application, the graphs are built from sentences in a text, or from individual words. The text spans are connected through links that are extracted f"
2020.coling-main.144,W04-3252,1,0.316398,"ummarization (Hermann et al., 2015; Dang, 2005) and beyond. While the state-of-the-art solutions for these tasks mainly rely on training neural network architectures on very large datasets, there have been questions around the sustainability of these solutions and their effects on the environment. As highlighted in work by Strubell et al. (2019), training one large transformer-based model produces approximately four times more CO2 emissions than a car in its lifetime. These considerable negative environmental outcomes call for lighter and less resource-intensive alternative methods. TextRank (Mihalcea and Tarau, 2004) is a light-weight unsupervised graph-based content extraction algorithm that was initially designed for summarization and keyword extraction applications. Since its introduction, it has been adapted and used in numerous other applications and settings, including opinion mining (Petasis and Karkaletsis, 2016; Deguchi and Yamaguchi, 2019), credibility assessment (Balcerzak et al., 2014) and lyrics summarization (Son and Shin, 2018), among others. Most recently, TextRank has been included in the latest release of the popular spaCy library.1 There have been online tutorials and updating studies ("
2020.coling-main.144,W16-2811,0,0.130595,"hlighted in work by Strubell et al. (2019), training one large transformer-based model produces approximately four times more CO2 emissions than a car in its lifetime. These considerable negative environmental outcomes call for lighter and less resource-intensive alternative methods. TextRank (Mihalcea and Tarau, 2004) is a light-weight unsupervised graph-based content extraction algorithm that was initially designed for summarization and keyword extraction applications. Since its introduction, it has been adapted and used in numerous other applications and settings, including opinion mining (Petasis and Karkaletsis, 2016; Deguchi and Yamaguchi, 2019), credibility assessment (Balcerzak et al., 2014) and lyrics summarization (Son and Shin, 2018), among others. Most recently, TextRank has been included in the latest release of the popular spaCy library.1 There have been online tutorials and updating studies (Barrios et al., 2015) that demonstrate TextRank’s relevance years after its initial release. Some of the TextRank extensions that have been proposed in recent years rely on the idea of personalized (or topic-sensitive) PageRank (Haveliwala, 2003) and its successor algorithms. For instance, PositionRank (Flor"
2020.coling-main.144,D18-1003,0,0.0149563,"ained relatively understudied. In Atanasova et al. (2020), the authors address the task of extracting fact-checking explanations, in which statements documenting the veracity of a fact-checked statement are used to derive a short summary explanation. The authors propose a BERT (Devlin et al., 2019) based sentence selection model that identifies top relevant sentences from the input as candidate explanations. In similar context, highlighting natural language explanations for fact-checking and misinformation detection applications has been studied within the research community (Lu and Li, 2020; Popat et al., 2018). 3 Biased TextRank Biased TextRank builds upon the original TextRank algorithm, but changes how random restart probabilities are assigned, therefore giving higher likelihood to the nodes that are more relevant to a certain “focus” of the task. 3.1 Node Scoring with Random Restart Probabilities TextRank Node Scoring. TextRank operates on graphs that are built from natural language texts. For instance, in the original TextRank application, the graphs are built from sentences in a text, or from individual words. The text spans are connected through links that are extracted from text, which refle"
2020.coling-main.144,P18-2124,0,0.0283898,"graph nodes to the focus of the task. We present two applications of Biased TextRank: focused summarization and explanation extraction, and show that our algorithm leads to improved performance on two different datasets by significant ROUGE-N score margins. Much like its predecessor, Biased TextRank is unsupervised, easy to implement and orders of magnitude faster and lighter than current state-ofthe-art Natural Language Processing methods for similar tasks. 1 Introduction Content and information extraction are central to many Natural Language Processing (NLP) tasks, from question answering (Rajpurkar et al., 2018; Reddy et al., 2019) to text summarization (Hermann et al., 2015; Dang, 2005) and beyond. While the state-of-the-art solutions for these tasks mainly rely on training neural network architectures on very large datasets, there have been questions around the sustainability of these solutions and their effects on the environment. As highlighted in work by Strubell et al. (2019), training one large transformer-based model produces approximately four times more CO2 emissions than a car in its lifetime. These considerable negative environmental outcomes call for lighter and less resource-intensive"
2020.coling-main.144,Q19-1016,0,0.0128231,"s of the task. We present two applications of Biased TextRank: focused summarization and explanation extraction, and show that our algorithm leads to improved performance on two different datasets by significant ROUGE-N score margins. Much like its predecessor, Biased TextRank is unsupervised, easy to implement and orders of magnitude faster and lighter than current state-ofthe-art Natural Language Processing methods for similar tasks. 1 Introduction Content and information extraction are central to many Natural Language Processing (NLP) tasks, from question answering (Rajpurkar et al., 2018; Reddy et al., 2019) to text summarization (Hermann et al., 2015; Dang, 2005) and beyond. While the state-of-the-art solutions for these tasks mainly rely on training neural network architectures on very large datasets, there have been questions around the sustainability of these solutions and their effects on the environment. As highlighted in work by Strubell et al. (2019), training one large transformer-based model produces approximately four times more CO2 emissions than a car in its lifetime. These considerable negative environmental outcomes call for lighter and less resource-intensive alternative methods."
2020.coling-main.144,D19-1410,0,0.0311906,"ts by ranking document pieces. In order to do this, we need to parse the documents into those pieces. For instance, if the algorithm is used for sentence extraction, we parse the input into sentences. If it is to be used for keyword extraction, we parse the input into tokens. EMBED. Transforming documents into graphs requires mathematical representations of the nodes of the graph. This mathematical representation will enable similarity comparison between nodes, an integral part of the TextRank algorithm. With recent advances in contextual embedding technologies, we find Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) to be a good model to embed English texts. For non-English sentence embedding, contextual embedding models like LASER (Artetxe and Schwenk, 1645 2019) are useful. Word embedding models like Word2Vec (Mikolov et al., 2013) can similarly be used in the case of keyword extraction. After embedding document pieces DPi , i = 1..n into embedding vectors Ei , i = 1..n of fixed length, we can build a representative graph of the input document. GRAPH CONSTRUCTION. To build a graph representation of the input, we follow the same graph building strategy as in the original TextRank algorithm. For sentence"
2020.coling-main.144,P19-1355,0,0.0126837,"r than current state-ofthe-art Natural Language Processing methods for similar tasks. 1 Introduction Content and information extraction are central to many Natural Language Processing (NLP) tasks, from question answering (Rajpurkar et al., 2018; Reddy et al., 2019) to text summarization (Hermann et al., 2015; Dang, 2005) and beyond. While the state-of-the-art solutions for these tasks mainly rely on training neural network architectures on very large datasets, there have been questions around the sustainability of these solutions and their effects on the environment. As highlighted in work by Strubell et al. (2019), training one large transformer-based model produces approximately four times more CO2 emissions than a car in its lifetime. These considerable negative environmental outcomes call for lighter and less resource-intensive alternative methods. TextRank (Mihalcea and Tarau, 2004) is a light-weight unsupervised graph-based content extraction algorithm that was initially designed for summarization and keyword extraction applications. Since its introduction, it has been adapted and used in numerous other applications and settings, including opinion mining (Petasis and Karkaletsis, 2016; Deguchi and"
2020.coling-main.144,N19-1101,0,0.0146875,"he authors proposed AttSum, a system that leverages joint learning of query relevance and sentence salience ranking, the two main modules of query-focused summarization and achieve competitive results on the DUC (Dang, 2005) datasets. While related work has been published in NLP venues (Daum´e III and Marcu, 2006), query-focused summarization has been mainly studied by the Information Retrieval community. 2.3 Explanation Extraction Model explainability (Poursabzi-Sangdeh et al., 2018; Lundberg and Lee, 2017) and natural language explanation extraction and generation (Kumar and Talukdar, 2020; Thorne et al., 2019) are broad and important topics of ongoing research within the AI and NLP communities. However explanation extraction in the context of fact-checking and misinformation detection has remained relatively understudied. In Atanasova et al. (2020), the authors address the task of extracting fact-checking explanations, in which statements documenting the veracity of a fact-checked statement are used to derive a short summary explanation. The authors propose a BERT (Devlin et al., 2019) based sentence selection model that identifies top relevant sentences from the input as candidate explanations. In"
2020.coling-main.253,P14-2134,0,0.14781,"which are extremely difficult to model theoretically. Generating humor is considered by some researchers as an AI-complete problem (Stock and Strapparava, 2002), hence humor generation was largely studied in specific settings. Language preferences vary with user demographics (Tresselt and Mayzner, 1964; Eckert and McConnellGinet, 2013; Garimella et al., 2016; Lin et al., 2018; Loveys et al., 2018), and this has led to approaches leveraging the demographic information of users to obtain better language representations and classification performances for various NLP tasks (Volkova et al., 2013; Bamman et al., 2014; Hovy, 2015; Garimella et al., 2017). Humor is a universal phenomenon that is used across all countries, genders, and age groups (Apte, 1985). Likewise, there are variations in how humor is enacted and understood due to demographic differences (Kramarae, 1981; Duncan et al., 1990; Goodman, 1992; Alden et al., 1993; Hay, 2000; Robinson and Smith-Lovin, 2001). However, to the best of our knowledge, the effect of user demographics in computational humor generation has not been studied. In this paper, we introduce YODA L IB, an automatic humor generation framework for Mad R Libs —a story-based fi"
2020.coling-main.253,N16-1016,0,0.0400501,"tional study to automatically generate humor in a Mad Lib setting while also incorporating the demographic information of the audience, and analyzing its effect in terms of various linguistic dimensions. 2 Related Work There is a long history of research in general theories of humor (Attardo and Raskin, 1991; Wilkins and Eisenbraun, 2009; Attardo, 2010; Morreall, 2012; O’Shannon, 2012; Weems, 2014). In computational linguistics, a large body of humor research involves humor recognition, and it typically focuses on specific types of jokes (Mihalcea and Strapparava, 2006; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Zhang and Liu, 2014; Bertero and Fung, 2016; Hossain et al., 2019). Research work on humor generation has been largely limited to specific joke types and short texts, such as riddles (Binsted et al., 1997), acronyms (Stock and Strapparava, 2002), or one-liners (Petrovi´c and Matthews, 2013). In general, it is very difficult to apply humor theories directly to generate humor, as they require a high degree of commonsense understanding of the world. Owing to the subjective nature of humor, there have been recent efforts in collecting datasets for humor; Blinov et al., (2019) collecte"
2020.coling-main.253,P19-1394,0,0.0227183,", 2011; Bertero and Fung, 2016; Raz, 2012; Zhang and Liu, 2014; Bertero and Fung, 2016; Hossain et al., 2019). Research work on humor generation has been largely limited to specific joke types and short texts, such as riddles (Binsted et al., 1997), acronyms (Stock and Strapparava, 2002), or one-liners (Petrovi´c and Matthews, 2013). In general, it is very difficult to apply humor theories directly to generate humor, as they require a high degree of commonsense understanding of the world. Owing to the subjective nature of humor, there have been recent efforts in collecting datasets for humor; Blinov et al., (2019) collected a dataset of jokes and funny dialogues in Russian from various online resources, and complemented them carefully with unfunny texts with similar lexical properties. They developed a fine-tuned language model for text classification with a significant gain over baseline methods. Hasan et al., (2019) introduced the first multimodal language (including text, visual and acoustic modalities) dataset of humor detection, and proposed a framework for understanding and modeling humor in a multimodal setting. In socio-linguistics, the relationship between humor and gender is widely studied. H"
2020.coling-main.253,N19-1423,0,0.484149,"completion stage to join individually funny sentences to form complete Mad Lib stories that are humorous. Fig. 1 shows an example filled-in Mad Lib. 2814 Proceedings of the 28th International Conference on Computational Linguistics, pages 2814–2825 Barcelona, Spain (Online), December 8-13, 2020 This paper makes four main contributions: (1) We collect a novel dataset for location-specific humor generation in Mad Lib stories, which we carefully annotate using Amazon Mechanical Turk (AMT).1 (2) We propose YODA L IB, a location-specific humor generation framework that builds on top of BERT-based (Devlin et al., 2019) components while also accounting for location and story coherence. YODA L IB typically generates funnier Mad Lib stories than those created by humans and a previously published semi-automatic framework. (3) We present qualitative and quantitative analyses to explain what makes the generated stories humorous, and how they differ from the other completions. (4) Finally, we outline the similarities and differences in humor preferences between two countries: India (IN) and United States (US), in terms of certain linguistic attributes. To the best of our knowledge, ours is the first computational"
2020.coling-main.253,W16-4301,1,0.80194,". More recently, location has become central in sociolinguistics (Johnstone, 2010). Alden et al., (1993) indicated that humor styles vary with countries, and humorous communications from Korea, Germany, Thailand and US, had variable content for funny advertising, while sharing certain universal cognitive structures. The effect of demographic background on language use has gained significant attention in computational linguistics, with several efforts focused on understanding the similarities and differences in the language preferences, opinions and behaviors of people (Garimella et al., 2016; Garimella and Mihalcea, 2016; Wilson et al., 2016; Lin et al., 2018; Loveys et al., 2018; Welch et al., 2020). Conversely, there has been work to leverage these demographic differences in language preferences between various groups, to develop better models for NLP tasks, such as sentiment analysis (Volkova et al., 2013), word representations (Bamman et al., 2014), sentiment, topic and author attribute classification (Hovy, 2015), and word associations (Garimella et al., 2017). However, to our knowledge, none of this recent work accounts for demographic information in humor recognition or generation tasks. We find inspir"
2020.coling-main.253,C16-1065,1,0.925821,"omputer-generated humor is an essential aspect in developing personable human-computer interactions. However, humor is subjective and can be interpreted in different ways by different people. Humor requires creativity, world knowledge, and cognitive mechanisms, which are extremely difficult to model theoretically. Generating humor is considered by some researchers as an AI-complete problem (Stock and Strapparava, 2002), hence humor generation was largely studied in specific settings. Language preferences vary with user demographics (Tresselt and Mayzner, 1964; Eckert and McConnellGinet, 2013; Garimella et al., 2016; Lin et al., 2018; Loveys et al., 2018), and this has led to approaches leveraging the demographic information of users to obtain better language representations and classification performances for various NLP tasks (Volkova et al., 2013; Bamman et al., 2014; Hovy, 2015; Garimella et al., 2017). Humor is a universal phenomenon that is used across all countries, genders, and age groups (Apte, 1985). Likewise, there are variations in how humor is enacted and understood due to demographic differences (Kramarae, 1981; Duncan et al., 1990; Goodman, 1992; Alden et al., 1993; Hay, 2000; Robinson and"
2020.coling-main.253,D17-1242,1,0.930838,"model theoretically. Generating humor is considered by some researchers as an AI-complete problem (Stock and Strapparava, 2002), hence humor generation was largely studied in specific settings. Language preferences vary with user demographics (Tresselt and Mayzner, 1964; Eckert and McConnellGinet, 2013; Garimella et al., 2016; Lin et al., 2018; Loveys et al., 2018), and this has led to approaches leveraging the demographic information of users to obtain better language representations and classification performances for various NLP tasks (Volkova et al., 2013; Bamman et al., 2014; Hovy, 2015; Garimella et al., 2017). Humor is a universal phenomenon that is used across all countries, genders, and age groups (Apte, 1985). Likewise, there are variations in how humor is enacted and understood due to demographic differences (Kramarae, 1981; Duncan et al., 1990; Goodman, 1992; Alden et al., 1993; Hay, 2000; Robinson and Smith-Lovin, 2001). However, to the best of our knowledge, the effect of user demographics in computational humor generation has not been studied. In this paper, we introduce YODA L IB, an automatic humor generation framework for Mad R Libs —a story-based fill-in the blank game— which also acco"
2020.coling-main.253,D17-1067,1,0.888581,"in et al., 2018; Loveys et al., 2018; Welch et al., 2020). Conversely, there has been work to leverage these demographic differences in language preferences between various groups, to develop better models for NLP tasks, such as sentiment analysis (Volkova et al., 2013), word representations (Bamman et al., 2014), sentiment, topic and author attribute classification (Hovy, 2015), and word associations (Garimella et al., 2017). However, to our knowledge, none of this recent work accounts for demographic information in humor recognition or generation tasks. We find inspiration in recent work by Hossain et al., (2017), who collected a humor generation dataset with Mad Lib stories, and proposed a semi-automated approach to aid humans in writing funny stories. We go one step further and propose a fully-automated BERT-based demographic-aware humor generation framework. We further study the influence of location on humor preferences via AMT and seek to emulate such preferences in our automatically generated stories. Our work is similar to (Mostafazadeh et al., 2017), as our goal is to engage readers from different demographic groups by generating funnier versions of stories to read. 1 We release the location-s"
2020.coling-main.253,N19-1012,1,0.929289,"orporating the demographic information of the audience, and analyzing its effect in terms of various linguistic dimensions. 2 Related Work There is a long history of research in general theories of humor (Attardo and Raskin, 1991; Wilkins and Eisenbraun, 2009; Attardo, 2010; Morreall, 2012; O’Shannon, 2012; Weems, 2014). In computational linguistics, a large body of humor research involves humor recognition, and it typically focuses on specific types of jokes (Mihalcea and Strapparava, 2006; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Zhang and Liu, 2014; Bertero and Fung, 2016; Hossain et al., 2019). Research work on humor generation has been largely limited to specific joke types and short texts, such as riddles (Binsted et al., 1997), acronyms (Stock and Strapparava, 2002), or one-liners (Petrovi´c and Matthews, 2013). In general, it is very difficult to apply humor theories directly to generate humor, as they require a high degree of commonsense understanding of the world. Owing to the subjective nature of humor, there have been recent efforts in collecting datasets for humor; Blinov et al., (2019) collected a dataset of jokes and funny dialogues in Russian from various online resourc"
2020.coling-main.253,P15-1073,0,0.141605,"ifficult to model theoretically. Generating humor is considered by some researchers as an AI-complete problem (Stock and Strapparava, 2002), hence humor generation was largely studied in specific settings. Language preferences vary with user demographics (Tresselt and Mayzner, 1964; Eckert and McConnellGinet, 2013; Garimella et al., 2016; Lin et al., 2018; Loveys et al., 2018), and this has led to approaches leveraging the demographic information of users to obtain better language representations and classification performances for various NLP tasks (Volkova et al., 2013; Bamman et al., 2014; Hovy, 2015; Garimella et al., 2017). Humor is a universal phenomenon that is used across all countries, genders, and age groups (Apte, 1985). Likewise, there are variations in how humor is enacted and understood due to demographic differences (Kramarae, 1981; Duncan et al., 1990; Goodman, 1992; Alden et al., 1993; Hay, 2000; Robinson and Smith-Lovin, 2001). However, to the best of our knowledge, the effect of user demographics in computational humor generation has not been studied. In this paper, we introduce YODA L IB, an automatic humor generation framework for Mad R Libs —a story-based fill-in the bl"
2020.coling-main.253,P11-2016,0,0.184351,"rs is the first computational study to automatically generate humor in a Mad Lib setting while also incorporating the demographic information of the audience, and analyzing its effect in terms of various linguistic dimensions. 2 Related Work There is a long history of research in general theories of humor (Attardo and Raskin, 1991; Wilkins and Eisenbraun, 2009; Attardo, 2010; Morreall, 2012; O’Shannon, 2012; Weems, 2014). In computational linguistics, a large body of humor research involves humor recognition, and it typically focuses on specific types of jokes (Mihalcea and Strapparava, 2006; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Zhang and Liu, 2014; Bertero and Fung, 2016; Hossain et al., 2019). Research work on humor generation has been largely limited to specific joke types and short texts, such as riddles (Binsted et al., 1997), acronyms (Stock and Strapparava, 2002), or one-liners (Petrovi´c and Matthews, 2013). In general, it is very difficult to apply humor theories directly to generate humor, as they require a high degree of commonsense understanding of the world. Owing to the subjective nature of humor, there have been recent efforts in collecting datasets for humor; Blinov"
2020.coling-main.253,P18-1066,0,0.0509244,"is an essential aspect in developing personable human-computer interactions. However, humor is subjective and can be interpreted in different ways by different people. Humor requires creativity, world knowledge, and cognitive mechanisms, which are extremely difficult to model theoretically. Generating humor is considered by some researchers as an AI-complete problem (Stock and Strapparava, 2002), hence humor generation was largely studied in specific settings. Language preferences vary with user demographics (Tresselt and Mayzner, 1964; Eckert and McConnellGinet, 2013; Garimella et al., 2016; Lin et al., 2018; Loveys et al., 2018), and this has led to approaches leveraging the demographic information of users to obtain better language representations and classification performances for various NLP tasks (Volkova et al., 2013; Bamman et al., 2014; Hovy, 2015; Garimella et al., 2017). Humor is a universal phenomenon that is used across all countries, genders, and age groups (Apte, 1985). Likewise, there are variations in how humor is enacted and understood due to demographic differences (Kramarae, 1981; Duncan et al., 1990; Goodman, 1992; Alden et al., 1993; Hay, 2000; Robinson and Smith-Lovin, 2001"
2020.coling-main.253,W18-0608,0,0.0528146,"pect in developing personable human-computer interactions. However, humor is subjective and can be interpreted in different ways by different people. Humor requires creativity, world knowledge, and cognitive mechanisms, which are extremely difficult to model theoretically. Generating humor is considered by some researchers as an AI-complete problem (Stock and Strapparava, 2002), hence humor generation was largely studied in specific settings. Language preferences vary with user demographics (Tresselt and Mayzner, 1964; Eckert and McConnellGinet, 2013; Garimella et al., 2016; Lin et al., 2018; Loveys et al., 2018), and this has led to approaches leveraging the demographic information of users to obtain better language representations and classification performances for various NLP tasks (Volkova et al., 2013; Bamman et al., 2014; Hovy, 2015; Garimella et al., 2017). Humor is a universal phenomenon that is used across all countries, genders, and age groups (Apte, 1985). Likewise, there are variations in how humor is enacted and understood due to demographic differences (Kramarae, 1981; Duncan et al., 1990; Goodman, 1992; Alden et al., 1993; Hay, 2000; Robinson and Smith-Lovin, 2001). However, to the bes"
2020.coling-main.253,P13-2041,0,0.0626198,"Missing"
2020.coling-main.253,N12-2012,0,0.26491,"cally generate humor in a Mad Lib setting while also incorporating the demographic information of the audience, and analyzing its effect in terms of various linguistic dimensions. 2 Related Work There is a long history of research in general theories of humor (Attardo and Raskin, 1991; Wilkins and Eisenbraun, 2009; Attardo, 2010; Morreall, 2012; O’Shannon, 2012; Weems, 2014). In computational linguistics, a large body of humor research involves humor recognition, and it typically focuses on specific types of jokes (Mihalcea and Strapparava, 2006; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Zhang and Liu, 2014; Bertero and Fung, 2016; Hossain et al., 2019). Research work on humor generation has been largely limited to specific joke types and short texts, such as riddles (Binsted et al., 1997), acronyms (Stock and Strapparava, 2002), or one-liners (Petrovi´c and Matthews, 2013). In general, it is very difficult to apply humor theories directly to generate humor, as they require a high degree of commonsense understanding of the world. Owing to the subjective nature of humor, there have been recent efforts in collecting datasets for humor; Blinov et al., (2019) collected a dataset"
2020.coling-main.253,D13-1187,0,0.205189,"cognitive mechanisms, which are extremely difficult to model theoretically. Generating humor is considered by some researchers as an AI-complete problem (Stock and Strapparava, 2002), hence humor generation was largely studied in specific settings. Language preferences vary with user demographics (Tresselt and Mayzner, 1964; Eckert and McConnellGinet, 2013; Garimella et al., 2016; Lin et al., 2018; Loveys et al., 2018), and this has led to approaches leveraging the demographic information of users to obtain better language representations and classification performances for various NLP tasks (Volkova et al., 2013; Bamman et al., 2014; Hovy, 2015; Garimella et al., 2017). Humor is a universal phenomenon that is used across all countries, genders, and age groups (Apte, 1985). Likewise, there are variations in how humor is enacted and understood due to demographic differences (Kramarae, 1981; Duncan et al., 1990; Goodman, 1992; Alden et al., 1993; Hay, 2000; Robinson and Smith-Lovin, 2001). However, to the best of our knowledge, the effect of user demographics in computational humor generation has not been studied. In this paper, we introduce YODA L IB, an automatic humor generation framework for Mad R L"
2020.coling-main.253,2020.emnlp-main.334,1,0.7469,"et al., (1993) indicated that humor styles vary with countries, and humorous communications from Korea, Germany, Thailand and US, had variable content for funny advertising, while sharing certain universal cognitive structures. The effect of demographic background on language use has gained significant attention in computational linguistics, with several efforts focused on understanding the similarities and differences in the language preferences, opinions and behaviors of people (Garimella et al., 2016; Garimella and Mihalcea, 2016; Wilson et al., 2016; Lin et al., 2018; Loveys et al., 2018; Welch et al., 2020). Conversely, there has been work to leverage these demographic differences in language preferences between various groups, to develop better models for NLP tasks, such as sentiment analysis (Volkova et al., 2013), word representations (Bamman et al., 2014), sentiment, topic and author attribute classification (Hovy, 2015), and word associations (Garimella et al., 2017). However, to our knowledge, none of this recent work accounts for demographic information in humor recognition or generation tasks. We find inspiration in recent work by Hossain et al., (2017), who collected a humor generation"
2020.coling-main.253,W16-5619,1,0.706924,"become central in sociolinguistics (Johnstone, 2010). Alden et al., (1993) indicated that humor styles vary with countries, and humorous communications from Korea, Germany, Thailand and US, had variable content for funny advertising, while sharing certain universal cognitive structures. The effect of demographic background on language use has gained significant attention in computational linguistics, with several efforts focused on understanding the similarities and differences in the language preferences, opinions and behaviors of people (Garimella et al., 2016; Garimella and Mihalcea, 2016; Wilson et al., 2016; Lin et al., 2018; Loveys et al., 2018; Welch et al., 2020). Conversely, there has been work to leverage these demographic differences in language preferences between various groups, to develop better models for NLP tasks, such as sentiment analysis (Volkova et al., 2013), word representations (Bamman et al., 2014), sentiment, topic and author attribute classification (Hovy, 2015), and word associations (Garimella et al., 2017). However, to our knowledge, none of this recent work accounts for demographic information in humor recognition or generation tasks. We find inspiration in recent work"
2020.coling-main.604,P14-2134,0,0.257714,"Missing"
2020.coling-main.604,P19-1285,0,0.0630832,"Missing"
2020.coling-main.604,C04-1088,0,0.139666,"Missing"
2020.coling-main.604,O16-2003,0,0.141751,"Missing"
2020.coling-main.604,P18-2111,0,0.0859153,"d up text entry. Another application is dialog systems that follow the speaking style of certain professionals (e.g., counselors, advisors). Finally, personalized word representations could particularly help users with atypical writing styles that are not currently well served by models trained to suit the majority. 2 Related Work Prior work has considered user embeddings, where one vector is learned for each user in the data (we learn a set of vectors per user, one for each word in the vocabulary). User embeddings have been used for dialog generation (Li et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), and sarcasm detection (Kolchinski and Potts, 2018). Amer et al. (2016) learn a set of embeddings from the books that a user adds to their profile. Some approaches also use network information (Zeng et al., 2017; Huang et al., 2016). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6856 Proceedings of the 28th International Conference on Computational Linguistics, pages 6856–6862 Barcelona, Spain (Online), December 8-13, 2020 User Example Use Nea"
2020.coling-main.604,D18-1140,0,0.293882,"fessionals (e.g., counselors, advisors). Finally, personalized word representations could particularly help users with atypical writing styles that are not currently well served by models trained to suit the majority. 2 Related Work Prior work has considered user embeddings, where one vector is learned for each user in the data (we learn a set of vectors per user, one for each word in the vocabulary). User embeddings have been used for dialog generation (Li et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), and sarcasm detection (Kolchinski and Potts, 2018). Amer et al. (2016) learn a set of embeddings from the books that a user adds to their profile. Some approaches also use network information (Zeng et al., 2017; Huang et al., 2016). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6856 Proceedings of the 28th International Conference on Computational Linguistics, pages 6856–6862 Barcelona, Spain (Online), December 8-13, 2020 User Example Use Nearest Neighbors A B All doctors think this is bad for her health ... it is usually bad for your health"
2020.coling-main.604,P16-1094,0,0.295155,"xt generation for auto-completion to speed up text entry. Another application is dialog systems that follow the speaking style of certain professionals (e.g., counselors, advisors). Finally, personalized word representations could particularly help users with atypical writing styles that are not currently well served by models trained to suit the majority. 2 Related Work Prior work has considered user embeddings, where one vector is learned for each user in the data (we learn a set of vectors per user, one for each word in the vocabulary). User embeddings have been used for dialog generation (Li et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), and sarcasm detection (Kolchinski and Potts, 2018). Amer et al. (2016) learn a set of embeddings from the books that a user adds to their profile. Some approaches also use network information (Zeng et al., 2017; Huang et al., 2016). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6856 Proceedings of the 28th International Conference on Computational Linguistics, pages 6856–6862 Barcelona, Spain"
2020.coling-main.604,P19-1542,0,0.0463705,"onduct, experiences, online medical, preventative, insurance, safety, healthcare Table 1: Nearest neighbors of “health” for two personalized embedding spaces and the generic space. Personalization has been studied for marketing, webpage layout, recommendations, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Our prior work (Welch et al., 2019a; Welch et al., 2019b) explored predicting response time, common messages, and author relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed personas and Madotto et al. (2019) used meta-learning to improve this process. Goal-oriented dialog has used demographics (i.e., age, gender) to condition system response generation, showing that this relatively coarse grained personalization improves system performance (Joshi et al., 2017). 3 Personalized Word Embeddings Definition. Personalized word embeddings are vector representations of words derived from the text produced by a single author. We use the text produced by a Reddit user s in their posts Cs to create their word embeddings. We apply the method described below to this set and produce an embedding matrix, |V |×k"
2020.coling-main.604,P14-5010,0,0.00861709,"Missing"
2020.coling-main.604,D14-1162,0,0.09167,"Missing"
2020.coling-main.604,N03-1033,0,0.263913,"Missing"
2020.coling-main.604,2020.emnlp-main.334,1,0.7814,"Missing"
2020.coling-main.604,P18-1205,0,0.0269034,"alth ... N/A preventative, insurance, reform, medical, education professional, mental, conduct, experiences, online medical, preventative, insurance, safety, healthcare Table 1: Nearest neighbors of “health” for two personalized embedding spaces and the generic space. Personalization has been studied for marketing, webpage layout, recommendations, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Our prior work (Welch et al., 2019a; Welch et al., 2019b) explored predicting response time, common messages, and author relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed personas and Madotto et al. (2019) used meta-learning to improve this process. Goal-oriented dialog has used demographics (i.e., age, gender) to condition system response generation, showing that this relatively coarse grained personalization improves system performance (Joshi et al., 2017). 3 Personalized Word Embeddings Definition. Personalized word embeddings are vector representations of words derived from the text produced by a single author. We use the text produced by a Reddit user s in their posts Cs to create their word embedding"
2020.emnlp-main.334,2020.acl-main.236,0,0.0388493,"Missing"
2020.emnlp-main.334,P14-2134,0,0.382498,"Missing"
2020.emnlp-main.334,W18-1112,0,0.0515852,"Missing"
2020.emnlp-main.334,P15-1073,0,0.0515631,"underrepresented groups and these contextualized models require billions of tokens for training. Recent work has also shown that static embeddings are competitive with contextualized ones in some settings (Arora et al., 2020). Personalization. The closest work is Garimella et al. (2017)’s exploration of demographic-specific word embedding spaces. They trained word embeddings for male and female speakers who live in the USA and India using skip-gram architectures that learn a separate word matrix for each demographic group (e.g., male speakers from the USA). Another line of work used discrete (Hovy, 2015) or continuous values (Lynn et al., 2017) to learn speaker embeddings: a single vector for each user. The speaker embedding is appended to the input of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferen"
2020.emnlp-main.334,P15-2079,0,0.0172264,"n both cases, we show that our proposed embeddings improve performance over generic word representations. 2 Related Work Embedding Bias. Recent work on embeddings has revealed and attempted to remove racial, gender, religious, and other biases (Manzini et al., 2019; Bolukbasi et al., 2016). The bias in our corpora and embeddings have a societal impact and risks exclusion and demographic misrepresentation (Hovy and Spruit, 2016). This means that users of certain regions, ages, or genders may find NLP technologies more difficult to use. For instance, when using standard corpora for POS tagging, Hovy and Søgaard (2015) found that models perform significantly lower on younger people and ethnic minorities. Similarly, results on textbased geotagging show best results for men over 40 (Pavalanathan and Eisenstein, 2015). Similar results are starting to be found in embeddings produced by contextual embedding methods (May et al., 2019; Kurita et al., 2019). We focus on non-contextual embedding methods because of their computational efficiency, which is crucial if many separate representations are being learned. Additionally, there may not be a large amount of available data for underrepresented groups and these co"
2020.emnlp-main.334,P16-2096,0,0.0225756,"ations of using or drawing conclusions from this method. We explore the value of compositional demographic word embeddings on two English NLP tasks: language modeling and word associations. In both cases, we show that our proposed embeddings improve performance over generic word representations. 2 Related Work Embedding Bias. Recent work on embeddings has revealed and attempted to remove racial, gender, religious, and other biases (Manzini et al., 2019; Bolukbasi et al., 2016). The bias in our corpora and embeddings have a societal impact and risks exclusion and demographic misrepresentation (Hovy and Spruit, 2016). This means that users of certain regions, ages, or genders may find NLP technologies more difficult to use. For instance, when using standard corpora for POS tagging, Hovy and Søgaard (2015) found that models perform significantly lower on younger people and ethnic minorities. Similarly, results on textbased geotagging show best results for men over 40 (Pavalanathan and Eisenstein, 2015). Similar results are starting to be found in embeddings produced by contextual embedding methods (May et al., 2019; Kurita et al., 2019). We focus on non-contextual embedding methods because of their computa"
2020.emnlp-main.334,P19-1285,0,0.0705302,"Missing"
2020.emnlp-main.334,O16-2003,0,0.205684,"a applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages, and speaker relationships from personal conversation data. Zhang et al. (2018) conditioned d"
2020.emnlp-main.334,P18-2111,0,0.0755574,"rn speaker embeddings: a single vector for each user. The speaker embedding is appended to the input of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2"
2020.emnlp-main.334,2020.lrec-1.299,0,0.0291453,"was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages, and speaker relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed persona"
2020.emnlp-main.334,D18-1140,0,0.206186,"nput of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages"
2020.emnlp-main.334,W17-1601,0,0.0125312,"phic variables and only covers a subset of the potential values of each demographic. For instance, we do not use the same granularity across locations, include all locations, religions, or gender identities. We simplify age into ranges. The groups ‘secular’, ‘agnostic’, and ‘atheist’ are grouped into one broader group. Our sample is further biased by the choice of platform as each platform contains text from different populations. Users in our sample are predominately young, male, atheist, and live in the United States. When using gender as a study variable, we followed the recommendations of Larson (2017). Our “gender” extraction method does not refer to biological sex. After running gender extraction patterns, users are assigned to either the ‘male’, ‘female’, or ‘unknown’ label, meaning that on the basis of these phrases one’s gender identity is assumed to be binary or to be a gender identity unknown to our model, which may include those who are transgender, non-binary, or those who do not wish to disclose their gender. However, we are aware that the use of regular expressions for the extraction of demographic attributes can lead to false positives and false negatives (error rates are provid"
2020.emnlp-main.334,P16-1094,0,0.285363,"from the USA). Another line of work used discrete (Hovy, 2015) or continuous values (Lynn et al., 2017) to learn speaker embeddings: a single vector for each user. The speaker embedding is appended to the input of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-completion (Jaech and Ostendorf, 2018), authorship attribution (Ebrahimi and Dou, 2016), sarcasm detection (Kolchinski and Potts, 2018), sentiment analysis (Zeng et al., 2017), and cold-start language modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied t"
2020.emnlp-main.334,2020.acl-main.592,0,0.0409865,"Missing"
2020.emnlp-main.334,D17-1119,0,0.0481594,"contextualized models require billions of tokens for training. Recent work has also shown that static embeddings are competitive with contextualized ones in some settings (Arora et al., 2020). Personalization. The closest work is Garimella et al. (2017)’s exploration of demographic-specific word embedding spaces. They trained word embeddings for male and female speakers who live in the USA and India using skip-gram architectures that learn a separate word matrix for each demographic group (e.g., male speakers from the USA). Another line of work used discrete (Hovy, 2015) or continuous values (Lynn et al., 2017) to learn speaker embeddings: a single vector for each user. The speaker embedding is appended to the input of the recurrent or output layer, and trained simultaneously with the rest of the model. This idea applies to any contextual information type and was introduced as a way to condition language models on topics learned by topic modeling (Mikolov and Zweig, 2012). It has since been used as a way of representing users in tasks such as task-oriented and open-domain dialog (Wen et al., 2013; Li et al., 2016), information retrieval based on book preferences (Amer et al., 2016), query auto-compl"
2020.emnlp-main.334,P19-1542,0,0.0330596,"red how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages, and speaker relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed personas and Madotto et al. (2019) used meta-learning to improve this process. Goal-oriented dialog has used demographics (i.e. age, gender) to condition system response generation, showing that this relatively coarse grained personalization improves system performance (Joshi et al., 2017). Social Media. We use social media data with demographic attributes inferred from user posts. Prior work has explored extraction or prediction of attributes such as age, gender, region, and political orientation (Rao et al., 2010; Rangel et al., 2013). Work on analyzing the demographics of social media users also includes race/ethnicity, inc"
2020.emnlp-main.334,P14-5010,0,0.00540837,"Missing"
2020.emnlp-main.334,N19-1062,0,0.0285017,"ender, and religion. We examine differences in word usage and association captured by the demographics we extracted and discuss the limitations and ethical considerations of using or drawing conclusions from this method. We explore the value of compositional demographic word embeddings on two English NLP tasks: language modeling and word associations. In both cases, we show that our proposed embeddings improve performance over generic word representations. 2 Related Work Embedding Bias. Recent work on embeddings has revealed and attempted to remove racial, gender, religious, and other biases (Manzini et al., 2019; Bolukbasi et al., 2016). The bias in our corpora and embeddings have a societal impact and risks exclusion and demographic misrepresentation (Hovy and Spruit, 2016). This means that users of certain regions, ages, or genders may find NLP technologies more difficult to use. For instance, when using standard corpora for POS tagging, Hovy and Søgaard (2015) found that models perform significantly lower on younger people and ethnic minorities. Similarly, results on textbased geotagging show best results for men over 40 (Pavalanathan and Eisenstein, 2015). Similar results are starting to be found"
2020.emnlp-main.334,N19-1063,0,0.135391,"ual embeddings may give similar representations, but it has different salient meanings in the personal space of each user. User A tends to talk more about preventative care and insurance, while user B tends to talk about people’s experiences affecting their mental health. The typical approach in natural language processing (NLP) is to use one-size-fits-all language representations, which do not account for variation between people. This may not matter for people whose language style is well represented in the data, but could lead to worse support for others (Pavalanathan and Eisenstein, 2015; May et al., 2019; Kurita et al., 2019). While the way we produce language is not a direct consequence of our demographics or any other grouping, it is possible that by tailoring word embeddings to a group we can more effectively model and support the way they use language. Additionally, personalized embeddings can be useful for applications such as predictive typing systems that auto-complete sentences by providing suggestions to users, or dialog systems that follow the style of certain individuals or professionals (e.g., counselors, advisors). They can also be used to match the communication style of a user,"
2020.emnlp-main.334,D18-1298,0,0.0436508,"Missing"
2020.emnlp-main.334,D15-1256,0,0.0386318,"Missing"
2020.emnlp-main.334,D14-1162,0,0.0983371,"A doctors think this is bad for her health ... preventative, insurance, reform, medical, education B it is usually bad for your health ... professional, mental, conduct, experiences, online All N/A medical, preventative, insurance, safety, healthcare Table 1: Nearest neighbors of the word “health” for two different users in personalized and a generic embedding space. Introduction Word embeddings are used in many natural language processing tasks as a way of representing language. Embeddings can be efficiently trained on large corpora using methods like word2vec or GloVe (Mikolov et al., 2013; Pennington et al., 2014), which learn one vector per word. These embeddings capture syntactic and semantic properties of the language of all individuals who contributed to the corpus. However, they are unable to account for user-specific word preferences (e.g., using the same word in different ways across different contexts), particularly for individuals whose usage deviates from the majority. These individual preferences are reflected in the word’s nearest neighbors. As an example, Table 1 shows the way two users use the word “health” and the word’s five nearest neighbors in their respective personalized embedding s"
2020.emnlp-main.334,2020.coling-main.604,1,0.7814,"Missing"
2020.emnlp-main.334,2020.emnlp-main.696,1,0.887735,"hics improves the most, but we also see improvements when only one demographic value is known. 4Dem Table 4: Perplexity on the demographic data. Our demographic-based approach improves performance. The difference between the last row and generic words is significant (p &lt; 0.00001 with a permutation test). We explored various hyperparameter configurations on our validation set and found the best results using dropout with the same mask for generic and demographic-specific embeddings, untied weights, and fixed input embeddings. Untying and fixing input embeddings is supported by concurrent work (Welch et al., 2020b). Each model is trained for 50 epochs. We use the version from the epoch that had the best validation set perplexity, a standard metric in language modeling that measures the accuracy of the predicted probability distribution. 5.1 2+Dem Gender In both experiments, we use the language model developed by Merity et al. (2018b,a). As discussed in § 2, this model was recently state-of-the-art and has been the basis of many variations. We modify it to initialize the word embeddings with the ones we provide and to concatenate multiple embedding vectors as input to the recurrent layers. The rest of"
2020.emnlp-main.334,P18-1205,0,0.0178778,"uage modeling Huang et al. (2016). Finally, a recent study by King and Cook (2020) compared how to improve a language model with user-specific data using priming and interpolation, depending on the amount of data available, learning a new model for each user. More generally, personalization has been extensively applied to marketing, webpage layout, product and news recommendation, query completion, and dialog (Eirinaki and Vazirgiannis, 2003; Das et al., 2007). Welch et al. (2019a,b) explored predicting response time, common messages, and speaker relationships from personal conversation data. Zhang et al. (2018) conditioned dialog systems on artificially constructed personas and Madotto et al. (2019) used meta-learning to improve this process. Goal-oriented dialog has used demographics (i.e. age, gender) to condition system response generation, showing that this relatively coarse grained personalization improves system performance (Joshi et al., 2017). Social Media. We use social media data with demographic attributes inferred from user posts. Prior work has explored extraction or prediction of attributes such as age, gender, region, and political orientation (Rao et al., 2010; Rangel et al., 2013)."
2020.emnlp-main.696,2020.acl-main.236,0,0.0667457,"Missing"
2020.emnlp-main.696,D18-1549,0,0.0222574,"n Data Pretraining and Freezing. Word vectors are frequently used in downstream tasks and recent work has shown that their effectiveness depends on domain similarity (Peters et al., 2019; Arora et al., 2020) For language modeling, Kocmi and Bojar (2017) explored random and pretrained embeddings and found improvements, but did not consider tying and freezing. In-domain data is also useful for continuing to train contextual embedding models before fine-tuning (Gu et al., 2020; Gururangan et al., 2020), and for monolingual pretraining in machine translation (Neishi et al., 2017; Qi et al., 2018; Artetxe et al., 2018). This matches our observations, but does not cover the interactions between freezing and tying we consider. Handling Rare Words. These remain challenging even for large transformer models (Schick and Sch¨utze, 2020). Recent work has explored copying mechanisms and character based generation (Kawakami et al., 2017), with some success. These ideas are complementary to the results of our work, extending coverage to the open vocabulary case. Due to space and computational constraints we only consider English. For other languages, inflectional morphology and other factors may impact the effectiven"
2020.emnlp-main.721,D16-1110,0,0.0231958,"econd, through extensive feature ablation experiments, we shed light on the role played by emotion mimicry and emotion grouping for the task of empathetic response generation. 2 Related Work Open domain conversational models have made good progress in recent years (Serban et al., 2016; Vinyals and Le, 2015; Wolf et al., 2019). Many of them can generate persona-consistent (Zhang et al., 2018) and diverse (Cai et al., 2018) responses, but those are not necessarily empathetic. Producing empathetic responses requires apt handling of emotions and sentiments (Fung et al., 2016; Winata et al., 2017; Bertero et al., 2016). Zhou et al. (2018) model psychological concepts as memory states in LSTM (Hochreiter and Schmidhuber, 1997) and employ emotion-category embeddings in the decoding process. Wang and Wan (2018) presents a GAN (Goodfellow et al., 2014) based framework with emotion-specific generators. On a larger scale, (Zhou and Wang, 2018) use the emojis in Twitter posts as emotion labels and introduce an attention-based (Luong et al., 2015) Seq-to-Seq (Sutskever et al., 2014) model with Conditional Variational Autoencoder (Sohn et al., 2015) for emotional response generation. However, they only produce affec"
2020.emnlp-main.721,D18-1507,0,0.0572891,"et al., 2014; Krebs, 1975). The study of empathy has found a wide range of applications in healthcare, including psychotherapy (Bohart and Greenberg, 1997) or more broadly as a mechanism to improve the quality of care (Mercer and Reynolds, 2002). Computational models of empathy have been proposed only in recent years, partly because of ∗ signifies equal contribution the complexity of this behavior which makes it difficult to emulate with computational approaches. In natural language processing, the methods proposed to date address the tasks of understanding expressions of empathy in newswire (Buechel et al., 2018), counseling conversations (P´erezRosas et al., 2017), or generating empathy in dialogue (Shen et al., 2020; Lin et al., 2019). Work has also been done on the construction of empathy lexicons (Sedoc et al., 2020) or large empathy dialogue datasets (Rashkin et al., 2019). In this paper, we address the task of generating empathetic responses that mimic the emotion of the speaker while accounting for their affective charge (positive or negative). We adopt the idea of emotion mixture, as the state-of-the-art MoEL (Lin et al., 2019), to achieve the appropriate balance of emotions in positive and ne"
2020.emnlp-main.721,N19-1423,0,0.0066602,"hkin et al. (2018), to explicitly infuse emotion into the context representation c, we train a emotion classifier on c. We train emotion embeddings EE ∈ Rnemo ×Dh (nemo = 32 is the number of emotion classes) to represent each emotion. We maximize the similarity between c and the user-emotion representation EE (e), e being the user-emotion label, using cross-entropy loss Lcls : (1) s = EE WE cT , P = softmax(s), Rk×Demb . where EC (C) ∈ Also as in MoEL, a transformer encoder (Vaswani et al., 2017) is used for context propagation within the utterances and words in C. Moreover, inspired by BERT (Devlin et al., 2019), one additional token CT X is prepended to the context sequence C to encode the entirety of the context: H = TRctx Enc (EC ([CT X] ⊕ C])), (2) where TRctx Enc is the transformer encoder of output size Dh and H ∈ R(k+1)×Dh contains the contextenriched representations of the contextual words. A context-enriched representation of the CT X token, c, is taken as the overall context representation: c = H0 . (3) Lcls = − log P[e], (4) (5) (6) where WE ∈ RDh ×Dh and s, P ∈ Rnemo . 3.3 Response Generation (Decoder) Our primary assumption behind this model is that the empathetic agent mimics the user’s"
2020.emnlp-main.721,N16-3018,0,0.0235519,"d on a large empathy dialogue dataset. Second, through extensive feature ablation experiments, we shed light on the role played by emotion mimicry and emotion grouping for the task of empathetic response generation. 2 Related Work Open domain conversational models have made good progress in recent years (Serban et al., 2016; Vinyals and Le, 2015; Wolf et al., 2019). Many of them can generate persona-consistent (Zhang et al., 2018) and diverse (Cai et al., 2018) responses, but those are not necessarily empathetic. Producing empathetic responses requires apt handling of emotions and sentiments (Fung et al., 2016; Winata et al., 2017; Bertero et al., 2016). Zhou et al. (2018) model psychological concepts as memory states in LSTM (Hochreiter and Schmidhuber, 1997) and employ emotion-category embeddings in the decoding process. Wang and Wan (2018) presents a GAN (Goodfellow et al., 2014) based framework with emotion-specific generators. On a larger scale, (Zhou and Wang, 2018) use the emojis in Twitter posts as emotion labels and introduce an attention-based (Luong et al., 2015) Seq-to-Seq (Sutskever et al., 2014) model with Conditional Variational Autoencoder (Sohn et al., 2015) for emotional response"
2020.emnlp-main.721,P82-1020,0,0.807317,"Missing"
2020.emnlp-main.721,D19-1012,0,0.685553,"hart and Greenberg, 1997) or more broadly as a mechanism to improve the quality of care (Mercer and Reynolds, 2002). Computational models of empathy have been proposed only in recent years, partly because of ∗ signifies equal contribution the complexity of this behavior which makes it difficult to emulate with computational approaches. In natural language processing, the methods proposed to date address the tasks of understanding expressions of empathy in newswire (Buechel et al., 2018), counseling conversations (P´erezRosas et al., 2017), or generating empathy in dialogue (Shen et al., 2020; Lin et al., 2019). Work has also been done on the construction of empathy lexicons (Sedoc et al., 2020) or large empathy dialogue datasets (Rashkin et al., 2019). In this paper, we address the task of generating empathetic responses that mimic the emotion of the speaker while accounting for their affective charge (positive or negative). We adopt the idea of emotion mixture, as the state-of-the-art MoEL (Lin et al., 2019), to achieve the appropriate balance of emotions in positive and negative emotion groups. However, inspired by Serban et al. (2017), we introduce stochasticity into the mixture at emotiongroup"
2020.emnlp-main.721,D16-1230,0,0.124399,"Missing"
2020.emnlp-main.721,D15-1166,0,0.0148849,"Missing"
2020.emnlp-main.721,D16-1147,0,0.0280722,"Missing"
2020.emnlp-main.721,P02-1040,0,0.106828,"i-TR). Following Rashkin et al. (2018), a transformer encoder-decoder (Vaswani et al., 2017) generates a response as the user emotion is classified from the encoder output — equivalent to c in Eq. (3). Mixture of Empathetic Listeners (MoEL). This state-of-the-art method (Lin et al., 2019) performs user-emotion classification as Multi-TR. However, in contrast to our method, it employs 1 https://github.com/facebookresearch/ EmpatheticDialogues 8972 emotion-specific decoders whose outputs are aggregated and fed to a final decoder to generate the empathetic response. 4.3 Evaluation Although BLEU (Papineni et al., 2002) has long been used to compare system-generated response against the human-gold response, Liu et al. (2016) argues against its efficacy in open-domain where the gold response is not necessarily the only correct response. Therefore, as MoEL, we keep BLEU mostly as reference. Following MoEL and Rashkin et al. (2018), we rely on human-evaluated metrics: Human Ratings. Firstly, we randomly sample four instances of each of the 32 emotion labels from the test set, resulting in a total of 128 instances, compared to the 100 instances used for the evaluation of MoEL. Next, we ask three human annotators"
2020.emnlp-main.721,D14-1162,0,0.083482,"Missing"
2020.emnlp-main.721,P17-1131,1,0.8623,"Missing"
2020.emnlp-main.721,P19-1534,0,0.162883,"empathy have been proposed only in recent years, partly because of ∗ signifies equal contribution the complexity of this behavior which makes it difficult to emulate with computational approaches. In natural language processing, the methods proposed to date address the tasks of understanding expressions of empathy in newswire (Buechel et al., 2018), counseling conversations (P´erezRosas et al., 2017), or generating empathy in dialogue (Shen et al., 2020; Lin et al., 2019). Work has also been done on the construction of empathy lexicons (Sedoc et al., 2020) or large empathy dialogue datasets (Rashkin et al., 2019). In this paper, we address the task of generating empathetic responses that mimic the emotion of the speaker while accounting for their affective charge (positive or negative). We adopt the idea of emotion mixture, as the state-of-the-art MoEL (Lin et al., 2019), to achieve the appropriate balance of emotions in positive and negative emotion groups. However, inspired by Serban et al. (2017), we introduce stochasticity into the mixture at emotiongroup level for varied responses. This becomes particularly important in cases where the input utterance can be responded with ambivalent, yet befitti"
2020.emnlp-main.721,2020.lrec-1.206,0,0.151233,"are (Mercer and Reynolds, 2002). Computational models of empathy have been proposed only in recent years, partly because of ∗ signifies equal contribution the complexity of this behavior which makes it difficult to emulate with computational approaches. In natural language processing, the methods proposed to date address the tasks of understanding expressions of empathy in newswire (Buechel et al., 2018), counseling conversations (P´erezRosas et al., 2017), or generating empathy in dialogue (Shen et al., 2020; Lin et al., 2019). Work has also been done on the construction of empathy lexicons (Sedoc et al., 2020) or large empathy dialogue datasets (Rashkin et al., 2019). In this paper, we address the task of generating empathetic responses that mimic the emotion of the speaker while accounting for their affective charge (positive or negative). We adopt the idea of emotion mixture, as the state-of-the-art MoEL (Lin et al., 2019), to achieve the appropriate balance of emotions in positive and negative emotion groups. However, inspired by Serban et al. (2017), we introduce stochasticity into the mixture at emotiongroup level for varied responses. This becomes particularly important in cases where the inp"
2020.emnlp-main.721,2020.sigdial-1.2,1,0.748662,"g psychotherapy (Bohart and Greenberg, 1997) or more broadly as a mechanism to improve the quality of care (Mercer and Reynolds, 2002). Computational models of empathy have been proposed only in recent years, partly because of ∗ signifies equal contribution the complexity of this behavior which makes it difficult to emulate with computational approaches. In natural language processing, the methods proposed to date address the tasks of understanding expressions of empathy in newswire (Buechel et al., 2018), counseling conversations (P´erezRosas et al., 2017), or generating empathy in dialogue (Shen et al., 2020; Lin et al., 2019). Work has also been done on the construction of empathy lexicons (Sedoc et al., 2020) or large empathy dialogue datasets (Rashkin et al., 2019). In this paper, we address the task of generating empathetic responses that mimic the emotion of the speaker while accounting for their affective charge (positive or negative). We adopt the idea of emotion mixture, as the state-of-the-art MoEL (Lin et al., 2019), to achieve the appropriate balance of emotions in positive and negative emotion groups. However, inspired by Serban et al. (2017), we introduce stochasticity into the mixtu"
2020.emnlp-main.721,P18-1205,0,0.0228954,"sponses that are appropriate and empathetic for positive or negative statements. We show that this approach leads to performance exceeding the state-of-the-art when trained and evaluated on a large empathy dialogue dataset. Second, through extensive feature ablation experiments, we shed light on the role played by emotion mimicry and emotion grouping for the task of empathetic response generation. 2 Related Work Open domain conversational models have made good progress in recent years (Serban et al., 2016; Vinyals and Le, 2015; Wolf et al., 2019). Many of them can generate persona-consistent (Zhang et al., 2018) and diverse (Cai et al., 2018) responses, but those are not necessarily empathetic. Producing empathetic responses requires apt handling of emotions and sentiments (Fung et al., 2016; Winata et al., 2017; Bertero et al., 2016). Zhou et al. (2018) model psychological concepts as memory states in LSTM (Hochreiter and Schmidhuber, 1997) and employ emotion-category embeddings in the decoding process. Wang and Wan (2018) presents a GAN (Goodfellow et al., 2014) based framework with emotion-specific generators. On a larger scale, (Zhou and Wang, 2018) use the emojis in Twitter posts as emotion labe"
2020.emnlp-main.721,P18-1104,0,0.0209091,"). Many of them can generate persona-consistent (Zhang et al., 2018) and diverse (Cai et al., 2018) responses, but those are not necessarily empathetic. Producing empathetic responses requires apt handling of emotions and sentiments (Fung et al., 2016; Winata et al., 2017; Bertero et al., 2016). Zhou et al. (2018) model psychological concepts as memory states in LSTM (Hochreiter and Schmidhuber, 1997) and employ emotion-category embeddings in the decoding process. Wang and Wan (2018) presents a GAN (Goodfellow et al., 2014) based framework with emotion-specific generators. On a larger scale, (Zhou and Wang, 2018) use the emojis in Twitter posts as emotion labels and introduce an attention-based (Luong et al., 2015) Seq-to-Seq (Sutskever et al., 2014) model with Conditional Variational Autoencoder (Sohn et al., 2015) for emotional response generation. However, they only produce affective responses with userprovided emotion, which may not necessarily be empathetic to the speakers. Wu and Wu (2019) introduce a dual-decoder network to generate responses with given sentiment (positive or negative). Shin et al. (2020) formulate a reinforcement learning problem to maximize user’s sentimental feeling towards"
2020.findings-emnlp.224,Q16-1033,0,0.0222887,"standing research problem in Artificial Intelligence (AI). With the growing popularity of conversational AI research, the topic of emotion recognition in conversations has received significant attention from the research community (Li et al., 2020; Ghosal et al., 2019; Zhang et al., 2019). Identifying emotions in conversations is a core step toward fine-grained conversation understanding, which in turn is essential for downstream tasks such as emotion-aware chat agents (Lin et al., 2019; Rashkin et al., 2019), visual question answering (Tapaswi et al., 2016; Azab, 2019), health conversations (Althoff et al., 2016; P´erez-Rosas et al., 2017) and others. Natural conversations are complex as they are governed by several distinct variables that affect the flow of a conversation and the emotional dynamics of the participants. These variables include Figure 1: Commonsense knowledge can lead to explainable dialogue understanding. It will help models to understand, reason, and explain events and situations. In this particular example, commonsense inference is applied to a sequence of utterances in a twoparty conversation. Person A’s first utterance indicates that he/she is tired of arguing with person B. The"
2020.findings-emnlp.224,P19-1470,0,0.0497219,"speaker Effect of listeners Reaction of listeners IS cs (.) ES cs (.) RS cs (.) ELcs (.) RLcs (.) Mental state Mental state Event Mental state Event Cause Effect Effect Effect Effect Table 1: Functional notations of commonsense knowledge used in C OMET. The functions take as input the utterance u and returns the feature indicated in the leftmost column. Intent and effect on speaker and listeners can be categorized into mental states, whereas their reactions are events. Intent is also a causal variable whereas the rest are effects. In this work, we use the commonsense transformer model COMET (Bosselut et al., 2019) to extract the commonsense features. COMET is trained on several commonsense knowledge graphs to perform automatic knowledge base construction. The model is given a triplet {s, r, o} from the graph and is trained to generate the object phrase o from concatenated subject phrase s and relation phrase r. COMET is an encoder-decoder model that uses the pretrained autoregressive language model GPT (Radford et al., 2018) as the base generative model. To perform the task of generative commonsense knowledge construction, COMET is trained on ATOMIC (The Atlas of Machine Commonsense) (Sap et al., 2019)"
2020.findings-emnlp.224,L18-1252,0,0.0644591,"and the emotional dynamics of the participants. In Figure 1, we illustrate one such scenario where commonsense knowledge is utilized to infer emotions of the utterances in a dialogue. Natural language is often indicative of one’s emotion. Hence, emotion recognition has been enjoying popularity in the field of NLP (Kratzwald et al., 2018; Colneriˆc and Demsar, 2018), due to its widespread applications in opinion mining, recommender systems, healthcare, and so on. Only in the past few years has emotion recognition in conversation (ERC) gained attention from the NLP community (Yeh et al., 2019; Chen et al., 2018; Majumder et al., 2019; Zhou et al., 2018) due to the growing availability of public conversational data. ERC can be used to analyze conversations that take place on social media. It can also aid in analyzing conversations in real time, which can be instrumental in legal trials, interviews, ehealth services, and more. Unlike vanilla emotion recognition of sentences/utterances, ERC ideally requires context modeling of the individual utterances. This context can be attributed to the preceding utterances, and relies on the temporal sequence of utterances. Compared to the recently published works"
2020.findings-emnlp.224,D19-1015,1,0.867127,"Missing"
2020.findings-emnlp.224,D18-1280,1,0.897896,"Missing"
2020.findings-emnlp.224,N18-1193,1,0.902392,"es and tv-shows (Poria et al., 2019a; Zahiri and Choi, 2018). The main approach towards conversational emotion recognition is to perform contextual modeling in either textual or multimodal setting with deep-learning based algorithms. Poria et al. (2017) used recurrent neural networks for multimodal emotion recognition followed by (Majumder et al., 2019), where party and global states were used for modeling the emotional dynamics. An external knowledge base was used in (Zhong et al., 2019) with transformer networks to perform emotion recognition. Some of the other 2471 important works include (Hazarika et al., 2018a,b; Zadeh et al., 2018b; Chen et al., 2017; Zadeh et al., 2018a). 3 Methodology 3.1 Task definition Given the transcript of a conversation along with speaker information for each constituent utterance, the ERC task aims to identify the emotion of each utterance from a set of pre-defined emotions. Figure 1 illustrates one such conversation between two people, where each utterance is labeled by the underlying emotion. Formally, given an input sequence of N utterances [(u1 , p1 ), (u2 , p2 ), . . . , (uN , pN )], where each utterance ui = [ui,1 , ui,2 , . . . , ui,T ] consists of T words ui,j sp"
2020.findings-emnlp.224,D14-1181,0,0.00397374,"ks to keep track of the individual speaker states. We report and compare the performance of C OSMIC on test data in Table 4. State-of-the-art models use GloVe embeddings to extract contextindependent features. As features extracted from transformer based networks such as BERT and RoBERTa generally outperform traditional word embeddings such as word2vec and GloVe, we also report results of the models when used with BERT or RoBERTa features. 5.2 Results and Analysis Baseline and State-of-the art Methods For a comprehensive evaluation of C OSMIC, we compare it against the following methods: CNN (Kim, 2014) is a convolutional neural network model trained on top of pretrained GloVe Comparison with the State-of-the-Art Methods IEMOCAP and DailyDialog: IEMOCAP and DailyDialog contain dyadic conversations with mostly natural and coherent utterances. We observe that RoBERTa features improve the DialogueRNN models, and other BERT based models perform similarly. C OSMIC improves over all the models, 2476 IEMOCAP (Ro)BERT(a)-based GloVe-based Methods W-Avg F1 DailyDialog MELD W-Avg W-Avg Macro F1 Micro F1 F1 (3-cls) F1 (7-cls) EmoryNLP W-Avg W-Avg F1 (3-cls) F1 (7-cls) CNN ICON KET ConGCN DialogueRNN 52"
2020.findings-emnlp.224,I17-1099,0,0.138692,"Missing"
2020.findings-emnlp.224,D19-1012,0,0.0243731,"fect on B: Thinks what to do Commonsense Inference Influenced by the other person Introduction Emotion recognition is a long-standing research problem in Artificial Intelligence (AI). With the growing popularity of conversational AI research, the topic of emotion recognition in conversations has received significant attention from the research community (Li et al., 2020; Ghosal et al., 2019; Zhang et al., 2019). Identifying emotions in conversations is a core step toward fine-grained conversation understanding, which in turn is essential for downstream tasks such as emotion-aware chat agents (Lin et al., 2019; Rashkin et al., 2019), visual question answering (Tapaswi et al., 2016; Azab, 2019), health conversations (Althoff et al., 2016; P´erez-Rosas et al., 2017) and others. Natural conversations are complex as they are governed by several distinct variables that affect the flow of a conversation and the emotional dynamics of the participants. These variables include Figure 1: Commonsense knowledge can lead to explainable dialogue understanding. It will help models to understand, reason, and explain events and situations. In this particular example, commonsense inference is applied to a sequence o"
2020.findings-emnlp.224,2021.ccl-1.108,0,0.0750947,"Missing"
2020.findings-emnlp.224,W10-0204,0,0.118666,"social media. It can also aid in analyzing conversations in real time, which can be instrumental in legal trials, interviews, ehealth services, and more. Unlike vanilla emotion recognition of sentences/utterances, ERC ideally requires context modeling of the individual utterances. This context can be attributed to the preceding utterances, and relies on the temporal sequence of utterances. Compared to the recently published works on ERC (Chen et al., 2018; Majumder et al., 2019; Zhou et al., 2018; Qin et al., 2020; Zhong et al., 2019; Zhang et al., 2019), both lexicon-based (Wu et al., 2006; Mohammad and Turney, 2010; Shaheen et al., 2014) and modern 1 In multimodal conversations, there are other variables that can be observed, such as facial expressions, gestures, pitch, and acoustic indicators. deep learning-based (Kratzwald et al., 2018; Colneriˆc and Demsar, 2018) vanilla emotion recognition approaches fail to work well on ERC datasets as this work ignores the conversation specific factors such as the presence of contextual cues, the temporality in speakers’ turns, or speaker-specific information. In this paper, we introduce C OSMIC, a commonsense-guided framework for emotion identification in convers"
2020.findings-emnlp.224,E17-1106,1,0.888658,"Missing"
2020.findings-emnlp.224,P17-1081,1,0.813211,"heir relation with human emotion. Acoustic information and visual cues were later used for emotion recognition by Datcu and Rothkrantz (2014). However, emotion recognition in conversations has gained popularity only recently due to the emergence of publicly available conversational datasets collected from social media platforms and scripted situations such as movies and tv-shows (Poria et al., 2019a; Zahiri and Choi, 2018). The main approach towards conversational emotion recognition is to perform contextual modeling in either textual or multimodal setting with deep-learning based algorithms. Poria et al. (2017) used recurrent neural networks for multimodal emotion recognition followed by (Majumder et al., 2019), where party and global states were used for modeling the emotional dynamics. An external knowledge base was used in (Zhong et al., 2019) with transformer networks to perform emotion recognition. Some of the other 2471 important works include (Hazarika et al., 2018a,b; Zadeh et al., 2018b; Chen et al., 2017; Zadeh et al., 2018a). 3 Methodology 3.1 Task definition Given the transcript of a conversation along with speaker information for each constituent utterance, the ERC task aims to identify"
2020.findings-emnlp.224,P19-1050,1,0.872364,"t he/she is tired of arguing with person B. The tone of the utterance also implies that person B is getting yelled at by person A, which invokes a reaction of irritation in person B. Person B then asks what he/she can do to help and says this while being angry. This again makes person A annoyed and influences him/her to respond with anger. This kind of inferred commonsense knowledge about the reaction, effect, and intent of the speaker and the listener helps in predicting the emotional dynamics of the participants. topic, viewpoint, speaker personality, argumentation logic, intent, and so on (Poria et al., 2019b). Additionally, individual utterances are also governed by the mental state, intent, and emotional state of the participants at the time when they are uttered. In this conversation model, only the utterances can be observed as the conversation unfolds, 2470 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2470–2481 c November 16 - 20, 2020. 2020 Association for Computational Linguistics while other variables such as speaker state and intent remain latent as they are not directly observed by the other participants. Similarly, the emotional state of the speakers can"
2020.findings-emnlp.224,P19-1534,0,0.0219444,"what to do Commonsense Inference Influenced by the other person Introduction Emotion recognition is a long-standing research problem in Artificial Intelligence (AI). With the growing popularity of conversational AI research, the topic of emotion recognition in conversations has received significant attention from the research community (Li et al., 2020; Ghosal et al., 2019; Zhang et al., 2019). Identifying emotions in conversations is a core step toward fine-grained conversation understanding, which in turn is essential for downstream tasks such as emotion-aware chat agents (Lin et al., 2019; Rashkin et al., 2019), visual question answering (Tapaswi et al., 2016; Azab, 2019), health conversations (Althoff et al., 2016; P´erez-Rosas et al., 2017) and others. Natural conversations are complex as they are governed by several distinct variables that affect the flow of a conversation and the emotional dynamics of the participants. These variables include Figure 1: Commonsense knowledge can lead to explainable dialogue understanding. It will help models to understand, reason, and explain events and situations. In this particular example, commonsense inference is applied to a sequence of utterances in a twopa"
2020.findings-emnlp.224,D19-1016,0,0.179884,"Missing"
2020.lrec-1.187,D16-1110,0,0.0291541,"behavioral therapy to a user (Fitzpatrick et al., 2017). For this chatbot agent to be effective, it needs to respond differently when the user is stressed and upset versus when the user is calm and upset, which is a common strategy in counselor training (Thompson et al., 2013). While virtual agents have made successful strides in understanding the task-based intent of the user, social human-computer interaction can still benefit from further research (Clark et al., 2019). Successful integration of virtual agents into reallife social interaction requires machines to be emotionally intelligent (Bertero et al., 2016; Yuan, 2015). But humans are complex in nature, and emotion is not expressed in isolation (Griffiths, 2003). Instead, it is affected by various external factors. These external factors lead to interleaved user states, which are a culmination of situational behavior, experienced emotions, psychological or physiological state, and personality traits. One of the external factors that affects psychological state is stress. Stress can affect everyday behavior and emotion, and in severe states, is associated with delusions, depression and anxiety due to impact on emotion regulation mechanisms (King"
2020.lrec-1.187,L18-1252,0,0.241906,"sychological stressors. This modality has been previously noted in literature for measuring stress (Horvath, 1978), usually measured in polygraph tests. We perform baseline experiments to show that the modalities collected in the dataset are indeed informative for identifying stress and emotion. 2.2. Lack of Naturalness A common data collection paradigm for emotion is to ask actors to portray particular emotions. These are usually either short snippets of information (Busso et al., 2008), a single sentence in a situation (Busso et al., 2017), or obtained from sitcoms and rehearsed broadcasts (Chen et al., 2018). A common problem with this approach is that the resulting emotion display is not natural (Jürgens et al., 2015). These are more exaggerated versions of singular emotion expression rather than the general, and messier, emotion expressions that are common in the real world (Audibert et al., 2010; Batliner et al., 1995; Fernández-Dols and Crivelli, 2013). Further, expressions in the real world are influenced by both conversation setting and psychological setting. While some datasets have also collected spontaneous data (Busso et al., 2008; Busso et al., 2017), these utterances, though emotional"
2020.lrec-1.187,W09-1904,0,0.0462613,"a paired t-test for subject wise PSS scores, and find that the mean scores are significantly different for both sets (16.11 vs 18.53) at p &lt; 0.05. This implied that our hypothesis of exams eliciting persistently more stress than normal is often true. In our dataset, we also provide levels of stress which are binned into three categories based on weighted average (using questions for which the t-test score was significant). 4. 4.1. Emotional Annotation Crowdsourcing Crowdsourcing has previously been shown to be an effective and inexpensive method for obtaining multiple annotations per segment (Hsueh et al., 2009; Burmania and Busso, 2017). We posted our experiments as Human Intelligence Tasks (HITs) on Amazon Mechanical Turk and used selection and training mechanisms to ensure quality (Jaiswal et al., 2019a). HITs were defined as sets of utterances in a monologue. The workers were presented with a single utterance and were asked to annotate the activation and valence values of that utterance using Self-Assessment Manikins (Bradley and Lang, 1994). Unlike the strategy adopted in (Chen et al., 2018), the workers could not go back and revise the previous estimate of the emotion. We did this to ensure si"
2020.lrec-1.536,W04-0506,0,0.0875952,"h in the audio channel) and pre-computed features for every video. In this paper, we describe the LifeQA dataset, present several analyses, and evaluate the performance of several baselines that highlight the difficulty of the task. 2. 2.1. Related Work Text-based Question Answering Question answering based on text has been extensively explored (Richardson et al., 2013; Hermann et al., 2015; Weston et al., 2015). Early question answering systems were developed for restricted domains, relied on manually crafted features, and had limited capabilities (Katz et al., 2002; Soricut and Brill, 2004; Benamara, 2004). Recently, the rise of deep learning methods motivated the need for large question answering datasets to leverage the capabilities of such models. With that goal in mind, several large-scale reading comprehension datasets were introduced (Rajpurkar et al., 2016; Richardson et al., 2013; Bajgar et al., 2016; 4352 Nguyen et al., 2016). (Rajpurkar et al., 2016) introduced the SQuAD dataset, which is composed of Wikipedia articles and the answers are specified as spans from a text passage. Similarly, (Richardson et al., 2013) collected the MCTest dataset, a multiple-choice open-domain reading com"
2020.lrec-1.536,D18-1167,0,0.420559,"and crucial problems for artificial intelligence. In this task, we are given a video and must answer natural language questions about its content, such as “What game is the little girl playing?”. Answering these questions requires a rich understanding of the visual and auditory content in the video, as well as the ability to relate this content to natural language concepts. Like many challenging tasks, much of the recent progress on Video QA is due to the introduction of several large-scale datasets, which consist primarily of movies and TV shows (Tapaswi et al., 2016; Rohrbach et al., 2017; Lei et al., 2018). Movies and TV shows provide for countless hours of clean, crisply-edited video and accurately-captioned audio, and are therefore easily adapted into datasets. However, these same features mean that movies and TV are not representative of day-today life. Therefore, these datasets cannot be used to evaluate how well models perform when applied to realistic videos of day-to-day life. To address this issue, we introduce Life Question Answering (LifeQA), a Video QA benchmark dataset that consists of videos and questions about day-to-day life. LifeQA is drawn from hand-picked YouTube videos, which"
2020.lrec-1.536,D14-1162,0,0.0843444,"Missing"
2020.lrec-1.536,D16-1264,0,0.113357,"Missing"
2020.lrec-1.536,D13-1020,0,0.0411339,"it a suitable complement for existing datasets and a challenging benchmark for existing Video QA systems. To enable future research, we are making LifeQA publicly available, along with automatically and manually generated transcriptions (from the speech in the audio channel) and pre-computed features for every video. In this paper, we describe the LifeQA dataset, present several analyses, and evaluate the performance of several baselines that highlight the difficulty of the task. 2. 2.1. Related Work Text-based Question Answering Question answering based on text has been extensively explored (Richardson et al., 2013; Hermann et al., 2015; Weston et al., 2015). Early question answering systems were developed for restricted domains, relied on manually crafted features, and had limited capabilities (Katz et al., 2002; Soricut and Brill, 2004; Benamara, 2004). Recently, the rise of deep learning methods motivated the need for large question answering datasets to leverage the capabilities of such models. With that goal in mind, several large-scale reading comprehension datasets were introduced (Rajpurkar et al., 2016; Richardson et al., 2013; Bajgar et al., 2016; 4352 Nguyen et al., 2016). (Rajpurkar et al.,"
2020.lrec-1.536,N04-1008,0,0.146771,"criptions (from the speech in the audio channel) and pre-computed features for every video. In this paper, we describe the LifeQA dataset, present several analyses, and evaluate the performance of several baselines that highlight the difficulty of the task. 2. 2.1. Related Work Text-based Question Answering Question answering based on text has been extensively explored (Richardson et al., 2013; Hermann et al., 2015; Weston et al., 2015). Early question answering systems were developed for restricted domains, relied on manually crafted features, and had limited capabilities (Katz et al., 2002; Soricut and Brill, 2004; Benamara, 2004). Recently, the rise of deep learning methods motivated the need for large question answering datasets to leverage the capabilities of such models. With that goal in mind, several large-scale reading comprehension datasets were introduced (Rajpurkar et al., 2016; Richardson et al., 2013; Bajgar et al., 2016; 4352 Nguyen et al., 2016). (Rajpurkar et al., 2016) introduced the SQuAD dataset, which is composed of Wikipedia articles and the answers are specified as spans from a text passage. Similarly, (Richardson et al., 2013) collected the MCTest dataset, a multiple-choice open-d"
2020.lrec-1.536,E17-1001,0,0.0420471,"Missing"
2020.lrec-1.536,Q19-1014,0,0.0293401,"n et al., 2013) collected the MCTest dataset, a multiple-choice open-domain reading comprehension dataset. Given a paragraph, a question, and a set of multiple answers, the task of a QA system is to select the correct answer. 2.2. Multimodal Question Answering Recently, question answering systems have been constructed to answer questions about other modalities, such as images (Visual QA) and video (Video QA). For the former, several datasets have been proposed such as VQA (Agrawal et al., 2017), Visual7W (Zhu et al., 2016), VisDial (Das et al., 2017), GQA (Hudson and Manning, 2019) and DREAM (Sun et al., 2019). These benchmarks aim to help building visual understanding systems that can reason about the contents of a given image. Given an image and a question, the system would either select a correct answer from multiple choices or generate a free-form textual answer. Video QA is more challenging, in that it allows for a broader range of question types, and requires the use of temporal information. Many datasets have been proposed for Video QA, such as LSMDC 16 (Rohrbach et al., 2017), TGIF-QA (Jang et al., 2017), MovieQA (Tapaswi et al., 2016), PororoQA (Kim et al., 2017), MarioQA (Mun et al., 2017"
2020.lrec-1.536,P13-1171,0,0.0268452,"odality at a time and without knowing the correct answer a-priori. Question-only. We implement several baselines that use only the questions and their candidate answers. Three of these baselines use only the answers, without the question; Random chooses one out of the four options uniformly at random, and Longest answer and shortest answer choose the answer with most or fewest number of tokens, respectively. The first two baselines that also use the question are based on computing some measure of similarity between the question and candidate answers. The first is Word matching, as defined by (Yih et al., 2013), which finds the answer with the most overlapping words with the question. The second is Most similar answer, which looks at word-level similarity, which we compute by using the average GloVe embedding (Pennington et al., 2014) of the question and each answer, and selecting the answer with the highest cosine similarity with the question. We use GloVe embeddings (Pennington et al., 2014) with size 300 pretrained on 6B tokens from Wikipedia 2014 (Rajpurkar et al., 2016) and Gigaword5 (Parker et al., 2011). Finally, we implement ST-VQA-Text, a variant of SpatioTemporal VQA (ST-VQA) (Jang et al.,"
2020.lrec-1.771,H05-2018,0,0.00808964,"fairness, in-group loyalty, sanctity, and authority (Garten et al., 2016). Values (V) The hierarchical lexicon for personal values (Wilson et al., 2018) was created by sorting and expanding a set of seed terms collected from surveys of the values of people from around the world. The words in these classes describe the types of things that are important to people in their everyday lives. While the tool allows for the generation of various word classes from the tree structure of the lexicon, we use the author’s originally recommended set of 50 value categories. Opinion Finder (OF) This lexicon (Wilson et al., 2005) is used specifically to search for subjectivity in language. All words in the lexicon are grouped into either the Positive class or the Negative class depending on the word’s connotation. 3.2. Dominance Scores In order to compare texts written by users from areas classified as urban to text written by users from rural areas, we calculate dominance scores (Mihalcea and Pulman, 2009) for each lexicon category. This allows us to compare the relative difference in usage of words from the lexicon class between the two groups. For a class of words C = W1 , W2 , ..., WN , the coverage of the class i"
2020.nlpcovid19-2.6,N19-2015,0,0.0278792,"or. Therefore, the goal of an MI counselor is to elicit their client’s own motivation for changing by asking open questions and reflecting back on the client’s statements. MI has been shown to correlate with positive behavior changes in a large variety of client goals, such as weight management (Small et al., 2009), chronic care intervention (Brodie et al., 2008), and substance abuse prevention (D’Amico et al., 2008). 2.3 Dialogue Systems With the development of deep learning techniques, dialogue systems have been applied to a large variety of tasks to meet increasing demands. In recent work, Afzal et al. (2019) built a dialogue-based tutoring system to guide learners through varying levels of content granularity to facilitate a better understanding of content. Henderson et al. (2019) applied a response retrieval approach in restaurant search and booking to provide and enable the users to ask various questions about a restaurant. Ortega et al. (2019) built an open-source dialogue system framework that navigates students through course selection. There are also dialogue system building tools such as Google’s Dialogflow2 and IBM’s Watson assistant,3 which enable numerous dialogue systems for customer s"
2020.nlpcovid19-2.6,D19-3031,0,0.0602873,"Missing"
2020.nlpcovid19-2.6,P19-3016,0,0.0607803,"Missing"
2020.nlpcovid19-2.6,strapparava-valitutti-2004-wordnet,0,0.572296,"Missing"
2020.nlpcovid19-2.8,W14-3207,0,0.0365165,"alyzed written text. For instance, researchers have used LIWC to study linguistic patterns in essays written by college students with and without depression (Rude et al., 2004) or in poems written by suicidal vs non-suicidal poets (Stirman and Pennebaker, 2001). More recently, there has been a proliferation of studies applying LIWC to online text, including social media data. LIWC has been used to study language patterns on social media for a variety of mental health disorders, including depression, anxiety, suicidality, and bipolar disorder (De Choudhury et al., 2013; Shen and Rudzicz, 2017; Coppersmith et al., 2014, 2016). In addition to LIWC, other methods used to study the linguistic patterns of mental illness include character and word models (Coppersmith et al., 2014; Tsugawa et al., 2013) and topic modeling (Resnik et al., 2015; Preot¸iuc-Pietro et al., 2015). 2.2 Studying Mental Health via Social Media In the past decade, social media has emerged as a powerful tool for understanding human behavior, and correspondingly mental health. A growing number of studies have applied computational methods to data collected from social media platforms in order to characterize behavior associated with mental h"
2020.nlpcovid19-2.8,W16-0311,0,0.0487265,"Missing"
2020.nlpcovid19-2.8,D11-1024,0,0.0909298,"Missing"
2020.nlpcovid19-2.8,W15-1203,0,0.0539492,"Missing"
2020.nlpcovid19-2.8,W15-1212,0,0.020732,"ets (Stirman and Pennebaker, 2001). More recently, there has been a proliferation of studies applying LIWC to online text, including social media data. LIWC has been used to study language patterns on social media for a variety of mental health disorders, including depression, anxiety, suicidality, and bipolar disorder (De Choudhury et al., 2013; Shen and Rudzicz, 2017; Coppersmith et al., 2014, 2016). In addition to LIWC, other methods used to study the linguistic patterns of mental illness include character and word models (Coppersmith et al., 2014; Tsugawa et al., 2013) and topic modeling (Resnik et al., 2015; Preot¸iuc-Pietro et al., 2015). 2.2 Studying Mental Health via Social Media In the past decade, social media has emerged as a powerful tool for understanding human behavior, and correspondingly mental health. A growing number of studies have applied computational methods to data collected from social media platforms in order to characterize behavior associated with mental health illnesses and to detect and forecast mental health outcomes (see Chancellor and De Choudhury (2020) for a comprehensive review). Reddit is a particularly well-suited platform for studying mental health due to its sem"
2020.nlpcovid19-2.8,W17-3107,0,0.0143476,"st studies using LIWC analyzed written text. For instance, researchers have used LIWC to study linguistic patterns in essays written by college students with and without depression (Rude et al., 2004) or in poems written by suicidal vs non-suicidal poets (Stirman and Pennebaker, 2001). More recently, there has been a proliferation of studies applying LIWC to online text, including social media data. LIWC has been used to study language patterns on social media for a variety of mental health disorders, including depression, anxiety, suicidality, and bipolar disorder (De Choudhury et al., 2013; Shen and Rudzicz, 2017; Coppersmith et al., 2014, 2016). In addition to LIWC, other methods used to study the linguistic patterns of mental illness include character and word models (Coppersmith et al., 2014; Tsugawa et al., 2013) and topic modeling (Resnik et al., 2015; Preot¸iuc-Pietro et al., 2015). 2.2 Studying Mental Health via Social Media In the past decade, social media has emerged as a powerful tool for understanding human behavior, and correspondingly mental health. A growing number of studies have applied computational methods to data collected from social media platforms in order to characterize behavio"
2020.nlpcovid19-2.8,2020.nlpcovid19-acl.12,0,0.031664,"d. Their results showed that rates of mental health queries increased leading up to the issuance of stay-at-homeorders, but then plateaued after they went into effect; however they did not consider the longer-term implications of the stay-at-home orders on mental health. Li et al. (2020) measured psycholinguistic attributes of posts on Weibo, a Chinese social media platform, before and after the Chinese National Health Commission declared COVID-19 to be an epidemic. Their findings showed that expressions of negative emotions and sensitivity to social risks increased following the declaration. Wolohan (2020) used a Long Short-Term Memory model to classify depression among Reddit users in April 2020, finding a higher than normal depression rate. Our work similarly aims to measure changes in online behavior as a means of understanding the relationship between COVID-19 and mental health. However, two notable differences are: (1) instead of analyzing the short-term impact of a specific COVID-related event, we examine more general changes that have occurred during a threemonth period of the outbreak; and (2) we focus our analysis on activity within mental health forums, which allows us to examine the"
2020.sigdial-1.2,C18-1281,0,0.0155506,"3.2 Human Evaluation for Reflection Generation To assess our automatic reflection generation systems’ ability to produce relevant and coherent reflections, we also conducted a human evaluation 15 Figure 2: Human evaluation mean scores and standard deviations on the three criteria: relevance, reflectionlikeness, and quality. (The former two criteria are in 3-point Likert scales. Quality uses a 5-point Likert scale; “*” indicate statistically significant improvement (p&lt;0.01) over the seq2seq baseline) personal preference and the level of background knowledge can both be sources of disagreement (Amidei et al., 2018). We plan to use more sophisticated evaluation schemes in future work, such as magnitude estimation or RankME (Novikova et al., 2018), instead of a plain Likert scale. 6 6.1 Results Automatic Metrics Figure 3: Spearman’s correlation between human evaluation metrics and automatic metrics Table 3 reports scores for our models and the seq2seq baseline. From this table, we observe that all our proposed models outperform the seq2seq baseline as measured by the different metrics. In addition, our models with context augmentation (i.e., including retrieval of the most similar reflection and content e"
2020.sigdial-1.2,N19-1148,0,0.0270394,"es Welch, Rada Mihalcea, Ver´onica P´erez-Rosas Department of Computer Science and Engineering, University of Michigan {shensq, cfwelch, mihalcea, vrncapr}@umich.edu Abstract process can take up to ten times as long as the duration of the session itself, and thus it does not scale up (Atkins et al., 2014). Previous work has focused on developing automatic tools for counseling evaluation and training tasks, including automatic coding (i.e., recognizing a counselor behavior) and forecasting (i.e., predicting the most appropriate behavior for the next counselor’s utterance) (Tanana et al., 2016; Park et al., 2019; Cao et al., 2019). These tools aim to facilitate the evaluation of a counseling encounter and, to some extent, provide generic guidance during the conversation. Although these systems help counselors by suggesting the timing of a certain counseling behavior, they do not offer any help on how to acomplish it. Among the different skills to be learned by counselors, reflective listening has been shown to be an important skill related to positive therapeutic outcomes (Moyers et al., 2009). Reflective listening is a conversational strategy used by counselors to show that they understand their cli"
2020.sigdial-1.2,P17-4012,0,0.0154965,"e Alexander Street dataset. We then fine-tune the generator using the MI dataset. Reflection generation with retrieval and content expansion strategies. We extend the finetuned model to include the retrieval of the most similar reflection and content expansion strategies described in section 4.1 and 4.2. We experiment with incremental models that incorporate one strategy at the time. Finally, we compare our models with a seq2seq model, which is frequently used as a baseline for conditional text generation problems (Vinyals and Le, 2015). We use the seq2seq implementation available in OpenNMT (Klein et al., 2017). The encoder and decoder are 2-layers GRU (Gated Recurrent Units) (Cho et al., 2014) with 512 hidden units. We train the model for 10 epochs with an Adam optimizer at a learning rate of 0.001. Alexander Street Dataset: This is a collection of psychotherapy videos that are published by Alexander Street Press.4 The videos and its corresponding transcripts, containing psychotherapy conversations between clients and therapists on several behavioral and mental issues, are available through a library subscription. From this library, we downloaded the transcripts available under the Counseling & The"
2020.sigdial-1.2,W16-0305,1,0.587122,"Missing"
2020.sigdial-1.2,W04-1013,0,0.0126474,"ors evaluate one candidate at a time, without knowledge of its origin. Quality is evaluated using a 5-point Likert scale (i.e., 5: very good, 4: good, 3: acceptable, 2: poor and 1: very poor). We chose a 3-point Likert scale (i.e., 1: not at all, 2: somewhat, 3: very much) to evaluate relevance and reflection-likeness, since a finer scale may exceed the annotators’ discriminating power (Jacoby and Matell, 1971). More specifically, we use the following prompts: ROUGE metrics: We use the ROUGE metric, a word overlap metric frequently used in the evaluation of neural language generation systems (Lin, 2004), including ROUGE-N, and ROUGE-L. We decided to use ROUGE over other n-grambased metrics, such as B LEU, because our task of generating reflective responses shares some similarity with the task of text summarization, where ROUGE is the metric of choice. Additionally, evaluations that we ran with other n-gram-based metrics had results consistent with those obtained with ROUGE. Embedding-based metrics: We also use three embedding-based metrics, namely greedy matching, embedding average, and vector extrema (Liu et al., 2016). The first matches each token in one sentence to its nearest neighbor in"
2020.sigdial-1.2,D16-1230,0,0.107178,"Missing"
2020.sigdial-1.2,P14-5010,0,0.00309191,"ry classifier that aims to classify whether a sequence contains a valid reflection according to the given context. We score each sequence using the probabilities provided by the classifier and choose the one with the highest score as the best example reflection to be added to our current dialogue context. To build the reflection-in-context classifier, we use a GPT-2 model and modify it by adding a clasContent Expansion We augment the context content by applying synset expansion to synonyms and verbs. We first apply part-of-speech (POS) tagging on the context utterances using Stanford CoreNLP (Manning et al., 2014) to identify nouns and verbs and then obtain their corresponding synonyms for all their meanings using the English WordNet (Miller, 1998). We then produce one rephrase for each utterance in the context by replacing the original nouns and/or verbs with a randomly selected synonym with the same POS tag. Our system uses the resulting utterances to augment the current context. 5 5.1 Experimental Setup Counseling Datasets We use the Motivational Interviewing (MI) counseling dataset from P´erez-Rosas et al. (2016) as the main corpus for training our retrieval and genera13 Total sessions Vocabulary s"
2020.sigdial-1.2,N18-2012,0,0.0636322,"Missing"
2021.clpsych-1.18,N19-1423,0,0.0089681,". sist of ten codes for therapist language plus two In general, we find that non-coded language additional codes for annotated language from thertends to have higher transcription error than codedapists and clients. We thus conduct a multi-label language (two-tailed Mann-Whitney U-test, p < classification task to assign each utterance in the 0.05 for both WER and semantic distance). Within conversation to any of these 12 labels. non-applicable codes, we note that NAT shows Our experiments are performed using a BERT higher WER and semantic distance. Since in Tamodel as our baseline classifier (Devlin et al., 2019) ble 3 we saw that client language tends to have and our evaluated are conducted using 5-fold crosshigher error overall than therapist language, this validation. BERT is a transformer-based model may indicate that transcription error is correlated that has been widely used in NLP. We chose this to speech content or topic, because NAC covers model since pretrained parameters fine-tuned on all client utterances, while NAT is only applied for large natural language corpora are readily available, non-MITI labeled utterances. and also because due to its design the additional When the ASR system is"
2021.clpsych-1.18,P08-1044,0,0.132404,"Missing"
2021.clpsych-1.18,2020.emnlp-main.733,0,0.0187472,"ovided in former embeddings (Reimers and Gurevych, 2019). the dataset and also word categories from the LinWe chose the sentence transformer over alternative guistic Inquiry and Word Count (LIWC) lexicon methods of sentence embeddings such as BERT or (Pennebaker et al., 2001). word2vec, since recent research has shown that off- Behavioral codes. We measure WER and semanthe-shelf transformer models without fine-tuning tic distance on utterances coded with the ten counoften lead to representations that perform poorly selor behaviors included in the dataset and also on semantic similarity tasks (Li et al., 2020). examined transcription error in uncoded utterances from both, therapists and clients. For WER, we 4.1 Results first concatenated all the utterances labeled with a given code in each single conversation, and then avTable 3 summarizes the results obtained by eraged the obtained WER across all conversations. speaker’s role (i.e., therapist, client) and gender (i.e., male, female). Overall, transcription of thera- Semantic distances for each utterance are averaged over all utterances in the dataset. pist’s speech shows significantly lower error than client speech in terms of WER, but not on se-"
2021.clpsych-1.18,W16-0305,1,0.827164,"therapy domain, where a large fraction of therapy sessions are conducted in spoken language, ASR can help reduce the burden of manual transcription, potentially allowing for large- 3 Dataset scale analysis of interactions between counselors and patients. 3.1 Data Source There have been several efforts on applying NLP on conversation analysis and utterance coding tasks in the psychotherapy domain. NLP was used to We evaluate utterances and behavioral codes from evaluate counselor behaviors and strategies (Zhang 213 counseling sessions compiled by Pérez-Rosas and Danescu-Niculescu-Mizil, 2020b; Pérez-Rosas et al. (2016). The sessions were originally drawn et al., 2019; Xiao et al., 2015), or to provide feed- from various sources, including two studies on back by generating appropriate responses to client smoking cessation and medication adherence. The utterances (Shen et al., 2020). full set comprises a total of 97.8 hours of audio with average session duration of 20.8 minutes. All While most of previous work was conducted the sessions were manually anonymized to remove on manual transcriptions, there are only a few identifiable information such as counselor and pacases where automatically generated transcri"
2021.clpsych-1.18,E17-1106,1,0.894233,"Missing"
2021.clpsych-1.18,P19-1088,1,0.882854,"Missing"
2021.clpsych-1.18,D19-1410,0,0.0134598,"state, addiction, or medication can cause Table 4: WER and Semantic Distance statistics for ten more harm than the incorrect transcription of other MITI codes and non-annotated utterances in the dataset types of words. Seeking to evaluate the role of by therapists (NAT) and clients (NAC). Plus and minus domain on ASR quality in our automatically tranvalues denote standard deviation. scribed conversations, we focus on speech that is relevant to counseling quality. To identify such For the emb(·) function we use sentence trans- speech, we use the behavioral coding provided in former embeddings (Reimers and Gurevych, 2019). the dataset and also word categories from the LinWe chose the sentence transformer over alternative guistic Inquiry and Word Count (LIWC) lexicon methods of sentence embeddings such as BERT or (Pennebaker et al., 2001). word2vec, since recent research has shown that off- Behavioral codes. We measure WER and semanthe-shelf transformer models without fine-tuning tic distance on utterances coded with the ten counoften lead to representations that perform poorly selor behaviors included in the dataset and also on semantic similarity tasks (Li et al., 2020). examined transcription error in uncode"
2021.clpsych-1.18,2020.sigdial-1.2,1,0.734527,"re have been several efforts on applying NLP on conversation analysis and utterance coding tasks in the psychotherapy domain. NLP was used to We evaluate utterances and behavioral codes from evaluate counselor behaviors and strategies (Zhang 213 counseling sessions compiled by Pérez-Rosas and Danescu-Niculescu-Mizil, 2020b; Pérez-Rosas et al. (2016). The sessions were originally drawn et al., 2019; Xiao et al., 2015), or to provide feed- from various sources, including two studies on back by generating appropriate responses to client smoking cessation and medication adherence. The utterances (Shen et al., 2020). full set comprises a total of 97.8 hours of audio with average session duration of 20.8 minutes. All While most of previous work was conducted the sessions were manually anonymized to remove on manual transcriptions, there are only a few identifiable information such as counselor and pacases where automatically generated transcripts tient names and references to counseling sites’ lohave been used, limiting the use of computational cation. The sessions were transcribed using manual methods in psychiatry (Imel et al., 2015). The and crowd-sourced methods. The transcription set main reason behi"
2021.clpsych-1.18,2020.findings-emnlp.295,0,0.0718212,"Missing"
2021.clpsych-1.18,W17-1606,0,0.0291314,"0.18±0.16 0.18±0.17 0.13±0.12 0.18±0.16 0.16±0.15 0.17±0.16 0.15±0.14 0.18±0.17 0.17±0.15 0.20±0.18 0.37±0.26 0.30±0.10 is also confirmed when the speaker roles are considered. This result is aligned with previous findings that ASR systems tend to perform better on female speakers due to being more consistent to standard pronunciations than male speakers (Adda-Decker and Lamel, 2005; Goldwater et al., 2008). However, it is important to mention that other work on ASR evaluation have encountered the opposite trend, where transcription of female speakers speech obtained higher WER than of males (Tatman, 2017). A factor that potentially affected our analysis is that due to the unavailability of identity data for speakers in the dataset, we treated each session as featuring a unique set of speakers. This might have been caused by the over-representation of speakers who appear multiple times in the dataset. 5 Domain-relevant Evaluation Although the domain-agnostic evaluation can provide insights into the aggregate performance of an ASR system, a domain informed evaluation can help to better understand the quality of derived transcriptions and its potential impact on downstream tasks. In the counselin"
2021.clpsych-1.18,2020.acl-main.470,0,0.145873,"pants. Recent efforts have addressed the automatic anal- fying domain-relevant keywords using behavioral ysis and evaluation of psychotherapy quality, in- codes and keywords identified using the Linguiscluding the study of conversational dynamics be- tic Inquiry and Word Count (LIWC) (Pennebaker tween therapists and clients, the analysis of em- et al., 2001). Finally, we study the effect of the noisy ASR on the downstream behavioral coding pathy and emotional responses, and the automatic assessment of therapist’s skills (Althoff et al., 2016; task and empirically show that additional local conZhang and Danescu-Niculescu-Mizil, 2020a; Pérez- text in the form of neighboring utterances can help alleviate the impact of ASR errors. Rosas et al., 2017). Most of these research studies have been conWe believe that studying the role of ASR sysducted using small collections of manually tran- tems in the NLP pipeline is an important step to scribed counseling conversations due to the need develop and evaluate robust systems for better unof an accurate representation of what is being said derstanding of counseling dialogues. 159 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 159–168"
2021.emnlp-main.392,P15-1034,0,0.0151022,"ences with at least one action from the final list of actions we collected from ConceptNet (see the previous section). For each selected sentence, we also collect its context consisting of the sentences before and after. We do this in order to increase the search space for the reasons for the actions mentioned in the selected sentences. We want to keep the sentences that contain action reasons. We tried multiple methods to automatically determine the sentences more likely to include causal relations using Semantic Role Labeling (SRL) (Ouchi et al., 2018), Open Information Extraction (OpenIE) (Angeli et al., 2015) and searching for causal markers. We found that SRL and OpenIE do not work well on our data, likely due to the fact that the transcripts are more noisy than the datasets these models were trained on. Most of the language in the transcripts does not follow simple patterns such as “I clean because it is dirty.” Instead, the language consists of natural everyday speech such as “Look at how dirty this is, I think I should clean it.” We find that a strategy sufficient for our purposes is to search for causal markers such as “because”, “since”, “so that is why”, “thus”, “therefore” in the sentence"
2021.emnlp-main.392,P19-1643,1,0.691103,"ne videos from YouTube. Previous work has leveraged webly-labeled data for the purpose of identifying commonsense knowledge. One of the most extensive efforts is NELL (Never Ending Language Learner) (Mitchell et al., 2015), a system that learns everyday knowledge by crawling the web, reading documents and analysing their linguistic patterns. A closely related effort is NEIL (Never Ending Image Learner), which learns commonsense knowledge from images on the web (Chen et al., 2013). Large scale video datasets (Miech et al., 2019) on instructional videos and lifestyle vlogs (Fouhey et al., 2018; Ignat et al., 2019) are other examples of web supervision. The latter are similar to our work as they analyse online vlogs, but unlike our work, their focus is on action detection and not on the reasons behind actions. Initial Actions with reasons in ConceptNet Actions with at least 3 reasons in CN Actions with at least 25 video-clips 9,759 139 102 25 Table 1: Statistics for number of collected actions at each stage of data filtering. We also collect a set of human actions and their reasons from ConceptNet (Speer et al., 2017). Actions include verbs such as: clean, write, eat, and other verbs describing everyday"
2021.emnlp-main.392,P17-1044,0,0.0223696,"consetion reasons in online videos. We focus on quences of events (e.g., if “there is clutter,” “cleanthe widespread genre of lifestyle vlogs, in ing” is required), or to enable social reasoning (e.g., which people perform actions while verbally describing them. We introduce and make pubwhen “guests are expected,” “cleaning” may be licly available the W HYACT dataset, consistneeded – see Figure 1). Most of the work to date ing of 1,077 visual actions manually annotated on causal systems has relied on the use of semantic with their reasons. We describe a multimodal parsers to identify reasons (He et al., 2017), howmodel that leverages visual and textual inforever this approach does not work well on more mation to automatically infer the reasons correalistic every-day settings. As an example, conresponding to an action presented in the video. sider the statement “This is a mess and my friends 1 Introduction are coming over. I need to start cleaning.” Current causal systems are unable to identify “this is Significant research effort has been recently dea mess” and “friends are coming over” as reasons, voted to the task of action recognition (Carreira and are thus failing to use them as context for un"
2021.emnlp-main.392,D18-1191,0,0.0119177,"ing spaCy (Honnibal et al., 2020). Next, we select the sentences with at least one action from the final list of actions we collected from ConceptNet (see the previous section). For each selected sentence, we also collect its context consisting of the sentences before and after. We do this in order to increase the search space for the reasons for the actions mentioned in the selected sentences. We want to keep the sentences that contain action reasons. We tried multiple methods to automatically determine the sentences more likely to include causal relations using Semantic Role Labeling (SRL) (Ouchi et al., 2018), Open Information Extraction (OpenIE) (Angeli et al., 2015) and searching for causal markers. We found that SRL and OpenIE do not work well on our data, likely due to the fact that the transcripts are more noisy than the datasets these models were trained on. Most of the language in the transcripts does not follow simple patterns such as “I clean because it is dirty.” Instead, the language consists of natural everyday speech such as “Look at how dirty this is, I think I should clean it.” We find that a strategy sufficient for our purposes is to search for causal markers such as “because”, “si"
2021.emnlp-main.392,D14-1162,0,0.0852561,"al information, we represent the video as a bag of object labels and a collection of video captions. For object detection we use Detectron2 (Wu et al., 2019), a state-of-the-art object detection algorithm. We generate automatic captions for the videos using a state-of-the-art dense captioning model (Iashin and Rahtu, 2020). The input to the model are visual features extracted from I3D model pretrained on Kinetics (Carreira and Zisserman, 2017), audio features extracted with VGGish model (Hershey et al., 2017) pre-trained on YouTube-8M (AbuEl-Haija et al., 2016) and caption tokens using GloVe (Pennington et al., 2014). 4.2 Baselines Using the representations described in Section 4.1, we implement several textual and visual models. 4.2.1 Textual Similarity Given an action, a video transcript associated with the action, and a list of the candidate action reasons, we compute the cosine similarity between the textual representations of the transcript and all the candidate reasons. We predict as correct those reasons that have a cosine similarity with the transcript greater than a threshold of 0.1. The threshold is fine-tuned on development data. Because the transcript might contain information that may be unre"
2021.emnlp-main.476,W13-3520,0,0.0128216,"Missing"
2021.emnlp-main.476,2020.acl-main.236,0,0.0446804,"Missing"
2021.emnlp-main.476,W16-2501,0,0.0351993,"Missing"
2021.emnlp-main.476,N19-1423,0,0.0161792,"n downstream tasks, and work that uses embeddings to study specific properties of language. However, research to date on word embedding stability has been exclusively done on English and so is not representative of all languages. In this work, we explore the stability of word embeddings in a wide range of languages. Better understanding the differences caused by diverse languages will provide a foundation for building embeddings and NLP tools in all languages.1 In English and other very high resource languages, it has become common practice to use contextualized word embeddings, such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019). These algorithms require huge amounts of computational resources and data. For example, it takes 2.5 days to train XLNet with 512 TPU v3 chips. In addition to requiring heavy computational resources, most contextualized embedding algorithms need large amounts of data. BERT uses 3.3 billion words of training data. In contrast to these large corpora, many datasets from low-resource languages are fairly small (Maxwell and Hughes, 2006). To support scenarios where using huge amounts of data and computational resources is not feasible, it is important to continue dev"
2021.emnlp-main.476,W19-2501,0,0.0151433,"s need large amounts of data. BERT uses 3.3 billion words of training data. In contrast to these large corpora, many datasets from low-resource languages are fairly small (Maxwell and Hughes, 2006). To support scenarios where using huge amounts of data and computational resources is not feasible, it is important to continue developing our understanding of context-independent word embeddings, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). These algorithms continue to be used in a wide variety of situations, including the computational humanities (Abdulrahim, 2019; Hellrich et al., 2019) and languages where only small corpora are available (Joshi et al., 2019). In this work, we consider how stability varies for different languages, and how linguistic properties are related to stability—a previously understudied relationship. Using regression modeling, we capture relationships between linguistic properties and average stability of a language, and we draw out insights about how linguistic features relate to stability. For instance, we find that embeddings in languages with more affixing tend to be less stable. Our findings provide crucial context for research that uses word emb"
2021.emnlp-main.476,2020.lrec-1.352,0,0.0426831,"te that work on The first part of our work is a comparison of stabilembedding evaluation should take into considera- ity across languages. Before presenting our meation stability, using multiple training runs to con- surements, we define stability and analyze some firm results. Similarly, stability should be con- important methodological decisions. sidered when studying the impact of embeddings on downstream tasks. Leszczynski et al. (2020) 2 Available online at https://sites.google.com/ specifically looked at the downstream instability site/rmyeid/projects/polyglot. 3 Available by contacting McCarthy et al. (2020). of word embeddings, and found that there is a 4 To work with a maximum number of languages, we only stability-memory tradeoff, and higher stability can consider the complete Protestant Bible (i.e., all of the verses be achieved by increasing the embedding dimen- that appear in the English King James Version of the Bible). 5 sion. Available online at https://wals.info. 5892 4.1.1 Model 1: indie, punk, progressive, pop, roll, band, blues, brass, class, alternative Model 2: punk, indie, alternative, progressive, band, sedimentary, bands, psychedelic, climbing, pop Model 3: punk, pop, indie, alt"
2021.emnlp-main.476,D14-1162,0,0.0917236,"e, it takes 2.5 days to train XLNet with 512 TPU v3 chips. In addition to requiring heavy computational resources, most contextualized embedding algorithms need large amounts of data. BERT uses 3.3 billion words of training data. In contrast to these large corpora, many datasets from low-resource languages are fairly small (Maxwell and Hughes, 2006). To support scenarios where using huge amounts of data and computational resources is not feasible, it is important to continue developing our understanding of context-independent word embeddings, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). These algorithms continue to be used in a wide variety of situations, including the computational humanities (Abdulrahim, 2019; Hellrich et al., 2019) and languages where only small corpora are available (Joshi et al., 2019). In this work, we consider how stability varies for different languages, and how linguistic properties are related to stability—a previously understudied relationship. Using regression modeling, we capture relationships between linguistic properties and average stability of a language, and we draw out insights about how linguistic features relate to stability. For instan"
2021.emnlp-main.476,N18-4005,0,0.0770362,"We consider two widely used algorithms: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Our work analyzes embeddings in multiple languages, which is important because embeddings are commonly used across many languages. In particular, there has been interest in embeddings for low-resource languages (Chimalamarri et al., 2020; Stringham and Izbicki, 2020). In this work, we use stability to measure the quality of word embeddings. Similar to the work we present here on stability, other research looks at how nearest neighbors vary as properties of the embedding spaces change. Pierrejean and Tanguy (2018) found that the lowest frequency and the highest frequency words have the highest variation among nearest neighbors. Additional research has explored how semantic and syntactic properties of words change with different embedding algorithm and parameter choices (Artetxe et al., 2018; Yaghoobzadeh and Schütze, 2016). Unlike our work, previous studies only considered English. 3 Data In order to explore the stability of word embeddings in different languages, we work with two datasets, Wikipedia and the Bible. While Wikipedia has more data, the Bible covers more languages. Wikipedia is a comparabl"
2021.emnlp-main.476,C18-1228,0,0.0317626,"Missing"
2021.emnlp-main.476,2020.emnlp-main.285,0,0.039239,"), which often rely on raw embeddings created by GloVe or word2vec. If these embeddings are unstable, then research using them 1 Code is available at https://lit.eecs.umich. needs to take this into account in terms of methodedu/downloads.html. ologies and error analysis. 5891 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5891–5901 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Related Work Word embeddings are low-dimensional vectors used to represent words, normally in downstream tasks, such as word sense disambiguation (Scarlini et al., 2020) and text summarization (Moradi et al., 2020). They have been shown to capture both syntactic and semantic properties of words, making them useful in a wide range of NLP tasks (Wang et al., 2020b). In this work, we explore word embeddings that generate one embedding per word, regardless of the word’s context. We consider two widely used algorithms: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Our work analyzes embeddings in multiple languages, which is important because embeddings are commonly used across many languages. In particular, there has been interest in embeddi"
2021.emnlp-main.476,2020.eval4nlp-1.17,0,0.030617,"en shown to capture both syntactic and semantic properties of words, making them useful in a wide range of NLP tasks (Wang et al., 2020b). In this work, we explore word embeddings that generate one embedding per word, regardless of the word’s context. We consider two widely used algorithms: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). Our work analyzes embeddings in multiple languages, which is important because embeddings are commonly used across many languages. In particular, there has been interest in embeddings for low-resource languages (Chimalamarri et al., 2020; Stringham and Izbicki, 2020). In this work, we use stability to measure the quality of word embeddings. Similar to the work we present here on stability, other research looks at how nearest neighbors vary as properties of the embedding spaces change. Pierrejean and Tanguy (2018) found that the lowest frequency and the highest frequency words have the highest variation among nearest neighbors. Additional research has explored how semantic and syntactic properties of words change with different embedding algorithm and parameter choices (Artetxe et al., 2018; Yaghoobzadeh and Schütze, 2016). Unlike our work, previous studie"
2021.emnlp-main.476,P16-1013,0,0.0694145,"Missing"
2021.emnlp-main.476,N18-1190,1,0.733922,"insights about correlations with affixing, language gender systems, and other features. This has implications for embedding use, particularly in research that uses them to study language trends. 1 Introduction Word embeddings have become an established part of natural language processing (NLP) (Collobert et al., 2011; Wang et al., 2020a). Stability, defined as the overlap between the nearest neighbors of a word in different embedding spaces, was introduced to measure variations in local embedding neighborhoods across changes in data, algorithms, and word properties (Antoniak and Mimno, 2018; Wendlandt et al., 2018). These studies found that many common English embedding spaces are surprisingly unstable, which has implications for work that uses embeddings as features in downstream tasks, and work that uses embeddings to study specific properties of language. However, research to date on word embedding stability has been exclusively done on English and so is not representative of all languages. In this work, we explore the stability of word embeddings in a wide range of languages. Better understanding the differences caused by diverse languages will provide a foundation for building embeddings and NLP to"
2021.emnlp-main.476,P16-1023,0,0.0208676,"anguages (Chimalamarri et al., 2020; Stringham and Izbicki, 2020). In this work, we use stability to measure the quality of word embeddings. Similar to the work we present here on stability, other research looks at how nearest neighbors vary as properties of the embedding spaces change. Pierrejean and Tanguy (2018) found that the lowest frequency and the highest frequency words have the highest variation among nearest neighbors. Additional research has explored how semantic and syntactic properties of words change with different embedding algorithm and parameter choices (Artetxe et al., 2018; Yaghoobzadeh and Schütze, 2016). Unlike our work, previous studies only considered English. 3 Data In order to explore the stability of word embeddings in different languages, we work with two datasets, Wikipedia and the Bible. While Wikipedia has more data, the Bible covers more languages. Wikipedia is a comparable corpus, whereas the Bible is a parallel corpus. Wikipedia Corpus. We use pre-processed Wikipedia dumps in 40 languages taken from AlRfou’ et al. (2013).2 The size of these Wikipedia corpora varies from 329,136 sentences (Tagalog) to 75,241,648 sentences (English), with an average of 9,292,394 sentences. For all"
2021.emnlp-main.476,W16-4123,0,0.0190838,"ling to identify patterns in the results. Based on the observations above, we use results from GloVe across five downsampled corpora for Wikipedia, and results across five random seeds for the Bible. 5 Regression Modeling We now explore linguistic factors that correlate with stability. To draw conclusions about specific linguistic features, we use a ridge regression model (Hoerl and Kennard, 1970)10 to predict the average stability of all words in a language given features reflecting language properties. Regression models have previously been used to measure the impact of individual features (Singh et al., 2016). Ridge regression regularizes the magnitude of the model weights, producing a more interpretable model than non-regularized linear regression. We experiment with different regularization strengths and use the best-performing value (α = 10).11 We choose to use a linear model here because of its interpretability. While more complicated models might yield additional insight, we show that there are interesting connections to be drawn from a linear model. 5.1 Model Input and Output Our model takes linguistic features of a language as input and predicts stability as output. Since WALS properties ar"
2021.emnlp-main.515,D18-2029,0,0.0941221,"le dataset of debate videos. We design interpretable debate-centric features (DCF) such as content variation, clarity, pauses, hand movement, emotional appeal, and so on based on theories of argument quality (Wachsmuth et al., 2017a; Braga and Marques, 2004; Straßmann et al., 2016). Moreover, we propose a hierarchical multimodal model named MARQ (Multimodal ARgument Quality assessor) to predict high vs. low quality arguments in long debate speeches (6 minutes recordings & 1500 words). Sentence level rich unimodal embeddings are extracted from pretrained models (e.g Universal Sentence Encoder (Cer et al., 2018), Wav2Vec2 (Baevski et al., 2020)) to reduce long sequential dependency. A set of LSTM encoders and a Multihead Self-Attention layer are used to capture the interaction across the intra-modal, inter-modal, and DCF information. Our main contributions are: Dataset IBM-Rank-Args IBM-Rank-Pairs IBM-Rank-30k UKPConvArg1 DTC DBATES N Modalities Mean Seq Len 6.3k {l} 22.98 words 14k {l} 23.47 words 30K {l} 18.22 words 11k {l} 48.3 words 400 {l,a,v} 22.5 seconds 716 {l,a,v} 6 minutes, 1500 words Table 1: Comparison between DBATES and other datasets. Here l, a, v represent language (text), audio, and v"
2021.emnlp-main.515,2020.acl-main.511,0,0.0245299,"eside text, other nonverbal features have correlation with the performance of a debate. The DBATES dataset also presents a global challenge applicable to any multimodal assessment task – representing multimodal signals in a long video, which was not addressed by the authors. In this study, we use this dataset and make two major contributions – 1) first to study multimodal argument quality assessment beyond logistic regression; 2) address a technical challenge of multimodal representation for long videos (6 minutes on average). The automatic assessment of argument quality (Toledo et al., 2019; Gienapp et al., 2020) has been receiving growing interest in the NLP community. Identifying argument quality has applications in diverse domains, including but not limited to argument search (Wachsmuth et al., 2017b,c), finding counter arguments (Wachsmuth et al., 2018), automated decision making (Bench-Capon et al., 2009), writing support (Stab and Gurevych, 2014) and essay evaluation (Nguyen and Litman, 2018). Wachsmuth et al. (2017a) proposed a taxonomy of dimensions for quantifying argument quality, where they summarized several high level dimensions behind the structure of good arguments such as clarity, cohe"
2021.emnlp-main.515,P19-1093,0,0.0158831,"sed a taxonomy of dimensions for quantifying argument quality, where they summarized several high level dimensions behind the structure of good arguments such as clarity, coherence, effectiveness, emotional appeal, etc. However, the subjective nature of these dimensions makes the task of automatic argument quality scoring difficult. Earlier research on automatic argument quality assessment focused on comparative pairwise approach, where the task is to identify higher quality argument from a given pair of arguments (Habernal and Gurevych, 2016b; Simpson and Gurevych, 2018; Potash et al., 2019; Gleize et al., 2019). Recently, Toledo et al. (2019) introduced straightforward point-wise argument quality metric that scales with the data size linearly. They introduced IBM-RANK (6.3K text arguments) that was crowd sourced and then annotated with an individual qual- 8 Conclusion ity score. Following similar approach, Gretz et al. (2020) proposed IBM-RANK-30k – the largest In this paper, we presented a comprehensive study dataset of argument quality score prediction in free on multimodal argument quality assessment. The text. Both of them utilized BERT (Devlin et al., debate-centric features reveal interpretabl"
2021.emnlp-main.515,P16-1150,0,0.19006,"and essay evaluation (Nguyen and Litman, 2018). Wachsmuth et al. (2017a) proposed a taxonomy of dimensions for quantifying argument quality, where they summarized several high level dimensions behind the structure of good arguments such as clarity, coherence, effectiveness, emotional appeal, etc. However, the subjective nature of these dimensions makes the task of automatic argument quality scoring difficult. Earlier research on automatic argument quality assessment focused on comparative pairwise approach, where the task is to identify higher quality argument from a given pair of arguments (Habernal and Gurevych, 2016b; Simpson and Gurevych, 2018; Potash et al., 2019; Gleize et al., 2019). Recently, Toledo et al. (2019) introduced straightforward point-wise argument quality metric that scales with the data size linearly. They introduced IBM-RANK (6.3K text arguments) that was crowd sourced and then annotated with an individual qual- 8 Conclusion ity score. Following similar approach, Gretz et al. (2020) proposed IBM-RANK-30k – the largest In this paper, we presented a comprehensive study dataset of argument quality score prediction in free on multimodal argument quality assessment. The text. Both of them u"
2021.emnlp-main.515,D19-1211,1,0.886202,"Missing"
2021.emnlp-main.515,W19-4517,0,0.0174974,"et al. (2017a) proposed a taxonomy of dimensions for quantifying argument quality, where they summarized several high level dimensions behind the structure of good arguments such as clarity, coherence, effectiveness, emotional appeal, etc. However, the subjective nature of these dimensions makes the task of automatic argument quality scoring difficult. Earlier research on automatic argument quality assessment focused on comparative pairwise approach, where the task is to identify higher quality argument from a given pair of arguments (Habernal and Gurevych, 2016b; Simpson and Gurevych, 2018; Potash et al., 2019; Gleize et al., 2019). Recently, Toledo et al. (2019) introduced straightforward point-wise argument quality metric that scales with the data size linearly. They introduced IBM-RANK (6.3K text arguments) that was crowd sourced and then annotated with an individual qual- 8 Conclusion ity score. Following similar approach, Gretz et al. (2020) proposed IBM-RANK-30k – the largest In this paper, we presented a comprehensive study dataset of argument quality score prediction in free on multimodal argument quality assessment. The text. Both of them utilized BERT (Devlin et al., debate-centric featur"
2021.emnlp-main.515,2020.acl-main.214,1,0.76851,"the The results of the multimodal argument quality pre- multimodal argument quality prediction. Though diction are presented in Table 2. A Logistic Regres- the MulT and FMT models do not use DCF fea6393 tures, they perform worse than the MARQ model not using DCF features. This indicates the importance of MARQ type architecture in modeling a low-resource multimodal dataset of long sequences. 7 Related Work ques, 2004; Straßmann et al., 2016; Hasan et al., 2019c). That’s why there exists vast amount prior research that utilize multimodal data to understand human communication behavior properly (Rahman et al., 2020; Hasan et al., 2021; Zadeh et al., 2018a; Tsai et al., 2019; Samrose et al., 2019; Sen et al., 2018; Hasan et al., 2019b; Zadeh et al., 2018b; Hasan et al., 2019a). Petukhova et al. (2017) discuss the design and evaluation of a Virtual Debate Coach (VDC) for training young politicians to improve their debate skills. They used logistic regression to identify multimodal features correlated with debate performance. Their DTC dataset comprised of 400 debate videos collected from professional debaters. Another similar work (Hirata et al., 2019) also uses logistic regression of multimodal features"
2021.emnlp-main.515,D19-1410,0,0.0134698,"rom the pre-trained models. Then a system of LSTM encoders is used to learn the temporal relations in the unimodal sentence embeddings. Finally, the unimodal sentence embeddings go through a Multi Head Self Attention layer to create multimodal representation and enrich it with debate-centric features. Sentence Encoder is used to extract embeddings of each sentence (Cer et al., 2018). The sentence level embeddings of language can be represented as ZLS = U niversalSentenceEncoder(L); where s ZLS ∈ RN ×dl and dsl = dimension of the universal sentence encoder embedding. We also use Sentence-BERT (Reimers and Gurevych, 2019) to extract sentence embeddings and experiment with both variations. Acoustic: The wav2vec2 (Baevski et al., 2020) is a pretrained transformer model of speech recognition that learns the representations of raw audio in self-supervised manner. It converts the speech input into discrete latent representations and learns the contextual representations via contrastive task. We use the base model that was trained on the 960 hours of Librispeech data (Panayotov et al., 2015). To extract the sentence level acoustic representations, the input audio file of the debate is split into N (#sentence) segmen"
2021.emnlp-main.515,Q18-1026,0,0.018729,"and Litman, 2018). Wachsmuth et al. (2017a) proposed a taxonomy of dimensions for quantifying argument quality, where they summarized several high level dimensions behind the structure of good arguments such as clarity, coherence, effectiveness, emotional appeal, etc. However, the subjective nature of these dimensions makes the task of automatic argument quality scoring difficult. Earlier research on automatic argument quality assessment focused on comparative pairwise approach, where the task is to identify higher quality argument from a given pair of arguments (Habernal and Gurevych, 2016b; Simpson and Gurevych, 2018; Potash et al., 2019; Gleize et al., 2019). Recently, Toledo et al. (2019) introduced straightforward point-wise argument quality metric that scales with the data size linearly. They introduced IBM-RANK (6.3K text arguments) that was crowd sourced and then annotated with an individual qual- 8 Conclusion ity score. Following similar approach, Gretz et al. (2020) proposed IBM-RANK-30k – the largest In this paper, we presented a comprehensive study dataset of argument quality score prediction in free on multimodal argument quality assessment. The text. Both of them utilized BERT (Devlin et al.,"
2021.emnlp-main.515,C14-1142,0,0.0311707,"– 1) first to study multimodal argument quality assessment beyond logistic regression; 2) address a technical challenge of multimodal representation for long videos (6 minutes on average). The automatic assessment of argument quality (Toledo et al., 2019; Gienapp et al., 2020) has been receiving growing interest in the NLP community. Identifying argument quality has applications in diverse domains, including but not limited to argument search (Wachsmuth et al., 2017b,c), finding counter arguments (Wachsmuth et al., 2018), automated decision making (Bench-Capon et al., 2009), writing support (Stab and Gurevych, 2014) and essay evaluation (Nguyen and Litman, 2018). Wachsmuth et al. (2017a) proposed a taxonomy of dimensions for quantifying argument quality, where they summarized several high level dimensions behind the structure of good arguments such as clarity, coherence, effectiveness, emotional appeal, etc. However, the subjective nature of these dimensions makes the task of automatic argument quality scoring difficult. Earlier research on automatic argument quality assessment focused on comparative pairwise approach, where the task is to identify higher quality argument from a given pair of arguments ("
2021.emnlp-main.515,D19-1564,0,0.0647101,"ever, often longer text certain attributes such as clarity in the text, hand sequences are needed to validate an argument on movements, and spoken style increase the effec- a certain topic. Even when a model like BERT tiveness of the argument (Wachsmuth et al., 2017a; (Devlin et al., 2018) can be trained to associate Braga and Marques, 2004; Straßmann et al., 2016). raw text with a quality metric (Gretz et al., 2020; 6387 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6387–6397 c November 7–11, 2021. 2021 Association for Computational Linguistics Toledo et al., 2019), it can be difficult to interpret which features lead to the output score. Such kind of interpretability is crucial to design a feedback system for people who want to improve their communication skills (Fung et al., 2015). In this paper, we study argument quality assessment in a multimodal context using DBATES (Sen et al., 2021) – the largest (N=716) publicly available dataset of debate videos. We design interpretable debate-centric features (DCF) such as content variation, clarity, pauses, hand movement, emotional appeal, and so on based on theories of argument quality (Wachsmuth et al., 201"
2021.emnlp-main.515,P18-1023,0,0.0233968,"ot addressed by the authors. In this study, we use this dataset and make two major contributions – 1) first to study multimodal argument quality assessment beyond logistic regression; 2) address a technical challenge of multimodal representation for long videos (6 minutes on average). The automatic assessment of argument quality (Toledo et al., 2019; Gienapp et al., 2020) has been receiving growing interest in the NLP community. Identifying argument quality has applications in diverse domains, including but not limited to argument search (Wachsmuth et al., 2017b,c), finding counter arguments (Wachsmuth et al., 2018), automated decision making (Bench-Capon et al., 2009), writing support (Stab and Gurevych, 2014) and essay evaluation (Nguyen and Litman, 2018). Wachsmuth et al. (2017a) proposed a taxonomy of dimensions for quantifying argument quality, where they summarized several high level dimensions behind the structure of good arguments such as clarity, coherence, effectiveness, emotional appeal, etc. However, the subjective nature of these dimensions makes the task of automatic argument quality scoring difficult. Earlier research on automatic argument quality assessment focused on comparative pairwise"
2021.emnlp-main.515,P18-1208,0,0.0408728,"Missing"
2021.emnlp-main.515,P19-1656,0,0.128453,"mong language, acoustic and visual. The selfwith DCF features only to assess the importance of attention heads calculate the weighted summation different debate-centric features. of values (V ); where the weights are computed MulT (Multimodal Transformer for Unaligned from the scalar dot product of query (Q) and key Multimodal Language Sequences): It has a set of (K) vector. cross modal transformer encoders that captures the T bimodal interaction between the modalities. Then QK Attention(Q, K, V ) = sof tmax( √ )V (1) it summarizes all biomodal information to model dh the multimodal sequence (Tsai et al., 2019). FMT (Factorized Multimodal Transformer for Multiple self-attention heads operating in parallel Multimodal Sequence Learning): It uses seven create Multi-Head Self Attention Layer - each distinct self-attention heads to model the multipotentially focusing on complementary aspects of modal dynamics in a factorized manner, capturing the multimodal input. First, we concatenate the all possible uni-modal, bi-modal, and tri-modal unimodal representations of the language, acoustic U U ⊕ ZU ; interactions, simultaneously (Zadeh et al., 2019). and visual. So, ZL,A,V = ZLU ⊕ ZA V where ⊕ represents th"
2021.emnlp-main.515,E17-1017,0,0.266584,"ablished research area salesmen upselling a product or presidential de- in NLP, assessment in a multimodal context is unbates, to people arguing whether to get vaccinated derstudied. Most of the previous work focused or to wear a mask. on argument quality prediction on short text seWhile the points of the argument may be valid, quences (22-48 words). However, often longer text certain attributes such as clarity in the text, hand sequences are needed to validate an argument on movements, and spoken style increase the effec- a certain topic. Even when a model like BERT tiveness of the argument (Wachsmuth et al., 2017a; (Devlin et al., 2018) can be trained to associate Braga and Marques, 2004; Straßmann et al., 2016). raw text with a quality metric (Gretz et al., 2020; 6387 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6387–6397 c November 7–11, 2021. 2021 Association for Computational Linguistics Toledo et al., 2019), it can be difficult to interpret which features lead to the output score. Such kind of interpretability is crucial to design a feedback system for people who want to improve their communication skills (Fung et al., 2015). In this paper, we stud"
2021.emnlp-main.515,W17-5106,0,0.215845,"ablished research area salesmen upselling a product or presidential de- in NLP, assessment in a multimodal context is unbates, to people arguing whether to get vaccinated derstudied. Most of the previous work focused or to wear a mask. on argument quality prediction on short text seWhile the points of the argument may be valid, quences (22-48 words). However, often longer text certain attributes such as clarity in the text, hand sequences are needed to validate an argument on movements, and spoken style increase the effec- a certain topic. Even when a model like BERT tiveness of the argument (Wachsmuth et al., 2017a; (Devlin et al., 2018) can be trained to associate Braga and Marques, 2004; Straßmann et al., 2016). raw text with a quality metric (Gretz et al., 2020; 6387 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6387–6397 c November 7–11, 2021. 2021 Association for Computational Linguistics Toledo et al., 2019), it can be difficult to interpret which features lead to the output score. Such kind of interpretability is crucial to design a feedback system for people who want to improve their communication skills (Fung et al., 2015). In this paper, we stud"
2021.emnlp-main.515,E17-1105,0,0.198047,"ablished research area salesmen upselling a product or presidential de- in NLP, assessment in a multimodal context is unbates, to people arguing whether to get vaccinated derstudied. Most of the previous work focused or to wear a mask. on argument quality prediction on short text seWhile the points of the argument may be valid, quences (22-48 words). However, often longer text certain attributes such as clarity in the text, hand sequences are needed to validate an argument on movements, and spoken style increase the effec- a certain topic. Even when a model like BERT tiveness of the argument (Wachsmuth et al., 2017a; (Devlin et al., 2018) can be trained to associate Braga and Marques, 2004; Straßmann et al., 2016). raw text with a quality metric (Gretz et al., 2020; 6387 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6387–6397 c November 7–11, 2021. 2021 Association for Computational Linguistics Toledo et al., 2019), it can be difficult to interpret which features lead to the output score. Such kind of interpretability is crucial to design a feedback system for people who want to improve their communication skills (Fung et al., 2015). In this paper, we stud"
2021.emnlp-main.683,J08-1001,0,0.109373,"achieved impressive performance (Chowdhury et al., 2021). However, there exist some key differences that make the generative approaches fundamentally different and incomparable from the family of pair-wise (sentence pair classification-based) approaches: Coherence and the problem of sentence order prediction have been extensively studied in literature due to their applicability in various downstream problems. Early work in this direction mainly used domain knowledge and handcrafted linguistic features to model the relation between sentences in a document (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2008). Sentence ordering methods in recent literature are primarily based on neural network architectures, and can be broadly categorized into two main families - i) Sequence generation methods and ii) Pair-wise methods. Sequence generation methods use the entire sequence of the randomly ordered document to model 1. local and global information. This information is then used to predict the correct order. The sentences and documents are typically encoded using a recurrent or transformer-based network (Gong et al., 2016; Yin et al., 2020; Kumar et al., 2020). Hierarchical encoding schemes are also co"
2021.emnlp-main.683,N04-1015,0,0.0210741,"generation methods have achieved impressive performance (Chowdhury et al., 2021). However, there exist some key differences that make the generative approaches fundamentally different and incomparable from the family of pair-wise (sentence pair classification-based) approaches: Coherence and the problem of sentence order prediction have been extensively studied in literature due to their applicability in various downstream problems. Early work in this direction mainly used domain knowledge and handcrafted linguistic features to model the relation between sentences in a document (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2008). Sentence ordering methods in recent literature are primarily based on neural network architectures, and can be broadly categorized into two main families - i) Sequence generation methods and ii) Pair-wise methods. Sequence generation methods use the entire sequence of the randomly ordered document to model 1. local and global information. This information is then used to predict the correct order. The sentences and documents are typically encoded using a recurrent or transformer-based network (Gong et al., 2016; Yin et al., 2020; Kumar et al., 2020). Hierarchical"
2021.emnlp-main.683,N19-1423,0,0.00725366,"Convolutional Network to model document-level contextual information and temporal commonsense knowledge for sentence order prediction. In the graph network, the edge classification objective was applied for pairwise relative order prediction of the sentence pairs. This was followed by a topological sorting for the final order prediction of the sentences. STaCK achieves state-of-the-art results in several benchmark datasets. The choice of the transformer encoder plays a crucial role in the sentence order prediction task. Several choices of transformer-based models are available, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), DeBERTa (He et al., 2020), etc. Different objective functions are used to pre-train these models, which directly affects how these models Acknowledgments perform on the downstream sentence ordering task. In particular, BERT is pre-trained with the masked This work is supported by the AcRF MoE Tierlanguage modelling (MLM) and the next sentence 2 grant titled: “CSK-NLP: Leveraging Commonprediction (NSP) objective. RoBERTa and De- sense Knowledge for NLP” and the A*STAR under BERTa models are pre-trained only with the MLM its RIE 2020 Advan"
2021.emnlp-main.683,P03-1069,0,0.254299,"eger sequence generation methods have achieved impressive performance (Chowdhury et al., 2021). However, there exist some key differences that make the generative approaches fundamentally different and incomparable from the family of pair-wise (sentence pair classification-based) approaches: Coherence and the problem of sentence order prediction have been extensively studied in literature due to their applicability in various downstream problems. Early work in this direction mainly used domain knowledge and handcrafted linguistic features to model the relation between sentences in a document (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2008). Sentence ordering methods in recent literature are primarily based on neural network architectures, and can be broadly categorized into two main families - i) Sequence generation methods and ii) Pair-wise methods. Sequence generation methods use the entire sequence of the randomly ordered document to model 1. local and global information. This information is then used to predict the correct order. The sentences and documents are typically encoded using a recurrent or transformer-based network (Gong et al., 2016; Yin et al., 2020; Kumar et a"
2021.emnlp-main.683,2020.acl-main.703,0,0.0281529,"by a different principle of sentence ordering. These models aim to predict the relative order of each pair of sentences in the document. The final order is then constrained on all of the predicted relative orders. The constraint solving problem is generally tackled with topological sorting (Prabhumoye et al., 2020), or more sophisticated neural network models (Zhu et al., 2021). Our proposed STaCK framework falls under this family of Pair-wise models. In STaCK, temporal commonsense is modelled using the Commonsense Transformers (COMET) model (Hwang et al., 2020). The COMET model uses a BART (Lewis et al., 2020) sequence-to-sequence encoder decoder framework and is pretrained on the ATOMIC20 20 commonsense knowledge graph (Hwang et al., 2020). The pretraining objective is to take a triplet {s, r, o} from the knowledge graph, and generate the object phrase o from concatenated subject and relation phrase s and r. The set of relations R include temporal relations ‘is before’ and ‘is after’. COMET is pretrained on approximately 50,000 of such temporal triplets along with other commonsense relations from ATOMIC20 20 . The pretraining on commonsense knowledge ensures that COMET is capable of distinguishing"
2021.emnlp-main.683,2021.ccl-1.108,0,0.0705722,"Missing"
2021.emnlp-main.683,N16-1098,0,0.0165778,"0.6582 0.6642 12.26 12.41 0.6194 0.6172 20.79 20.03 0.8534 0.8470 55.96 54.32 Table 2: Comparison of results of our model against various methods. Scores are reported at best validation τ . The performance difference between STaCK (w/ and w/o csk) and the baselines are statistically significant according to paired t-test with p < 0.05. abstract varies significantly, ranging from two to forty. SIND. This is a sequential vision to language dataset (Huang et al., 2016) used for the task of visual storytelling. Each story contains five images and their descriptions in natural language. ROCStory. (Mostafazadeh et al., 2016) introduce a dataset of short stories capturing a rich set of causal and temporal commonsense relations between daily events. The dataset has been used for evaluating story understanding, generation, and script learning. All stories have five sentences. For ROCStory, we use the train, val, test split as used in Zhu et al. (2021). For the other datasets, we use the splits following the original papers. Some statistics about the datasets is shown in Table 1. 4.2 Evaluation Metrics Method NeuRIPS AAN NSF SIND ROCStory First Last STaCK Abs LCS 93.3 93.2 86.2 83.2 96.1 78.8 82.6 56.9 66.3 82.1 63.6"
2021.emnlp-main.683,2020.acl-main.248,0,0.0579202,"nsense knowledge to model global information and predict the relative order of sentences. Our graph network accumulates temporal evidence using knowledge of ‘past’ and ‘future’ and formulates sentence ordering as a constrained edge classification problem. We report results on five different datasets, and empirically show that the proposed method is naturally suitable for order prediction, thus demonstrating the role of temporal commonsense knowledge. The implementation of this work is available at: https://github.com/declare-lab/ sentence-ordering. and (2) pair-wise methods (Zhu et al., 2021; Prabhumoye et al., 2020). While the former considers tagging the entire sequence, the latter takes one sentence pair at a time and predicts their relative order. Pair-wise methods ignore the importance of document level global information, i.e., while predicting the relative order of two sentences (si , sj ), other sentences sk from the same document do not play any role. Global document information Document A Document B Jennifer has her nal exam tomorrow. Jennifer will miss her friends after college. She got so stressed, she pulled an all-nighter. She has a great job lined up after graduation. Happens after Happen s"
2021.findings-acl.124,P19-1534,0,0.0232753,"l factors such as emotions, prior assumptions, intent, or personality traits. It is thus not surprising that the landscape of dialogue understanding research embraces several challenging tasks, such as, emotion recognition in conversations (ERC), dialogue intent classification, user-state representation, and others. These tasks are often performed at utterance level and can be conjoined together under the umbrella of utterancelevel dialogue understanding. Due to the fastgrowing research interest in dialogue understanding, several novel approaches have recently been proposed (Qin et al., 2020; Rashkin et al., 2019; Xing et al., 2020; Lian et al., 2019; Saha et al., 2020) to address the tasks by adopting speakerspecific and contextual modeling. However, to the best of our knowledge, the role of context has not been thoroughly explored across these tasks, partly due also to the lack of an unified framework across various utterance-level dialogue understanding tasks. In this work, we explore the role of context in utterance-level dialogue understanding. We use a contextual utterance-level dialogue understanding baseline (bcLSTM (Poria et al., 2017)) as 1435 Findings of the Association for Computational Li"
2021.findings-acl.124,2020.acl-main.402,0,0.0367374,"rsonality traits. It is thus not surprising that the landscape of dialogue understanding research embraces several challenging tasks, such as, emotion recognition in conversations (ERC), dialogue intent classification, user-state representation, and others. These tasks are often performed at utterance level and can be conjoined together under the umbrella of utterancelevel dialogue understanding. Due to the fastgrowing research interest in dialogue understanding, several novel approaches have recently been proposed (Qin et al., 2020; Rashkin et al., 2019; Xing et al., 2020; Lian et al., 2019; Saha et al., 2020) to address the tasks by adopting speakerspecific and contextual modeling. However, to the best of our knowledge, the role of context has not been thoroughly explored across these tasks, partly due also to the lack of an unified framework across various utterance-level dialogue understanding tasks. In this work, we explore the role of context in utterance-level dialogue understanding. We use a contextual utterance-level dialogue understanding baseline (bcLSTM (Poria et al., 2017)) as 1435 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1435–1449 August 1–6, 20"
2021.findings-acl.124,P19-1004,0,0.0407142,"Missing"
2021.findings-acl.124,P19-1566,0,0.0626761,"Missing"
2021.findings-acl.124,D19-1016,0,0.0559391,"Missing"
2021.findings-acl.273,D17-2017,0,0.0390738,"Missing"
2021.findings-acl.273,P16-2096,0,0.0323988,"keeping ethical principles as a pre-requisite, has the goal of creating positive impact and addressing society’s biggest challenges. Work in this space includes (Wang et al., 2020; Bhatia et al., 2020; Killian et al., 2019; Lampos et al., 2020). 3099 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3099–3113 August 1–6, 2021. ©2021 Association for Computational Linguistics Active conversations about ethics and social good have expanded broadly, in the NLP community as well as the broader AI and ML communities. Starting with early discussions in works such as (Hovy and Spruit, 2016; Leidner and Plachouras, 2017), the communities introduced the first workshop on ethics in NLP (Hovy et al., 2017) and the AI for social good workshop (Luck et al., 2018), which inspired various follow-up workshops at venues like ICML and ICLR. The upcoming NLP for Positive Impact Workshop (Field et al., 2020) finds inspiration from these early papers and workshops. In 2020, NeurIPS required all research papers to submit broader impact statements (Castelvecchi, 2020; Gibney, 2020). NLP conferences followed suit and introduced optional ethical and impact statements, starting with ACL in 2021 ("
2021.findings-acl.273,W17-4205,0,0.0163801,"or learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz energy et al., 2020) • NLP to analyze cultural values regarding climate change (Jiang et al., 2017; Koenecke and Feliu-Fab`a, 2019) • Cross-cultural models of climate change perceptions (proposed) Table 3: Top priorities and some NLP research related to each of them. This list may not be exhaustive. We also propose a high-impact research problem in each of the areas which has received less attention so far. in each NLP subarea, we can get a glimpse of the value misalignment in general. Table 2 shows the annual spending of some cause areas. Note that the ranking of the expenditure does not align with our priority list for social good. For example, luxury goods are not as important as global"
2021.findings-acl.273,D17-1302,0,0.0263778,"018; Evensen et al., 2019) • Assistive speech generation (proposed) Education • NLP for educational question answering (Atapattu et al., 2015; Lende and Raghuwanshi, 2016) • Improving textbooks (Agrawal et al., 2010) • Automated grading (Madnani and Cahill, 2018; Taghipour and Ng, 2016) • Plagiarism detection (Chong et al., 2010) • Tools for learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz energy et al., 2020) • NLP to analyze cultural values regarding climate change (Jiang et al., 2017; Koenecke and Feliu-Fab`a, 2019) • Cross-cultural models of climate change perceptions (proposed) Table 3: Top priorities and some NLP research related to each of them. This list may not be exhaustive. We also propose a high-impact research problem in each"
2021.findings-acl.273,D15-1246,0,0.073586,"Missing"
2021.findings-acl.273,W17-1604,0,0.0223482,"ples as a pre-requisite, has the goal of creating positive impact and addressing society’s biggest challenges. Work in this space includes (Wang et al., 2020; Bhatia et al., 2020; Killian et al., 2019; Lampos et al., 2020). 3099 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3099–3113 August 1–6, 2021. ©2021 Association for Computational Linguistics Active conversations about ethics and social good have expanded broadly, in the NLP community as well as the broader AI and ML communities. Starting with early discussions in works such as (Hovy and Spruit, 2016; Leidner and Plachouras, 2017), the communities introduced the first workshop on ethics in NLP (Hovy et al., 2017) and the AI for social good workshop (Luck et al., 2018), which inspired various follow-up workshops at venues like ICML and ICLR. The upcoming NLP for Positive Impact Workshop (Field et al., 2020) finds inspiration from these early papers and workshops. In 2020, NeurIPS required all research papers to submit broader impact statements (Castelvecchi, 2020; Gibney, 2020). NLP conferences followed suit and introduced optional ethical and impact statements, starting with ACL in 2021 (Association for Computational L"
2021.findings-acl.273,C18-1094,0,0.0207766,"or agriculture (Yunpeng et al., 2019) • NLP for food allocation (proposed) Health • NLP to analyze clinical notes (Dernoncourt & Wellet al., 2017a,b; Luo et al., 2018; Gopinath et al., being 2020; Leiter et al., 2020a,b) • NLP for psychotherapy and counseling (Biester et al., 2020; Xu et al., 2020; P´erez-Rosas et al., 2019) • NLP for happiness (Asai et al., 2018; Evensen et al., 2019) • Assistive speech generation (proposed) Education • NLP for educational question answering (Atapattu et al., 2015; Lende and Raghuwanshi, 2016) • Improving textbooks (Agrawal et al., 2010) • Automated grading (Madnani and Cahill, 2018; Taghipour and Ng, 2016) • Plagiarism detection (Chong et al., 2010) • Tools for learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz e"
2021.findings-acl.273,2020.acl-main.441,0,0.0177671,"., 2020a,b) • NLP for psychotherapy and counseling (Biester et al., 2020; Xu et al., 2020; P´erez-Rosas et al., 2019) • NLP for happiness (Asai et al., 2018; Evensen et al., 2019) • Assistive speech generation (proposed) Education • NLP for educational question answering (Atapattu et al., 2015; Lende and Raghuwanshi, 2016) • Improving textbooks (Agrawal et al., 2010) • Automated grading (Madnani and Cahill, 2018; Taghipour and Ng, 2016) • Plagiarism detection (Chong et al., 2010) • Tools for learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz energy et al., 2020) • NLP to analyze cultural values regarding climate change (Jiang et al., 2017; Koenecke and Feliu-Fab`a, 2019) • Cross-cultural models of climate change perceptions (proposed) Table"
2021.findings-acl.273,P19-1088,1,0.888957,"Missing"
2021.findings-acl.273,P19-1163,0,0.0200347,"., 2020; P´erez-Rosas et al., 2019) • NLP for happiness (Asai et al., 2018; Evensen et al., 2019) • Assistive speech generation (proposed) Education • NLP for educational question answering (Atapattu et al., 2015; Lende and Raghuwanshi, 2016) • Improving textbooks (Agrawal et al., 2010) • Automated grading (Madnani and Cahill, 2018; Taghipour and Ng, 2016) • Plagiarism detection (Chong et al., 2010) • Tools for learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz energy et al., 2020) • NLP to analyze cultural values regarding climate change (Jiang et al., 2017; Koenecke and Feliu-Fab`a, 2019) • Cross-cultural models of climate change perceptions (proposed) Table 3: Top priorities and some NLP research related to each of them. This list may not"
2021.findings-acl.273,P19-1164,0,0.02813,"r et al., 2020; Xu et al., 2020; P´erez-Rosas et al., 2019) • NLP for happiness (Asai et al., 2018; Evensen et al., 2019) • Assistive speech generation (proposed) Education • NLP for educational question answering (Atapattu et al., 2015; Lende and Raghuwanshi, 2016) • Improving textbooks (Agrawal et al., 2010) • Automated grading (Madnani and Cahill, 2018; Taghipour and Ng, 2016) • Plagiarism detection (Chong et al., 2010) • Tools for learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz energy et al., 2020) • NLP to analyze cultural values regarding climate change (Jiang et al., 2017; Koenecke and Feliu-Fab`a, 2019) • Cross-cultural models of climate change perceptions (proposed) Table 3: Top priorities and some NLP research related to each of them"
2021.findings-acl.273,P19-1355,0,0.0676897,"Missing"
2021.findings-acl.273,2020.nlpcovid19-2.14,0,0.015557,"ic health crisis. When the pandemic broke out, Allen AI collected the CORD-19 dataset (Wang et al., 2020) with the goal of helping public health experts efficiently sift through the myriad of COVID-19 research papers that emerged in a short time period. Subsequently, NLP services such as 1 Our data and code are available at http://github. com/zhijing-jin/nlp4sg_acl2021. Amazon Kendra were deployed to help organize the research knowledge around COVID-19 (Bhatia et al., 2020). The NLP research community worked on several problems like the question-answering and summarization system CAiRE-COVID (Su et al., 2020), the expressive interviewing conversational system (Welch et al., 2020) and annotation schemas to help fight COVID-19 misinformation online (Alam et al., 2020; Hossain et al., 2020). As NLP transits from theory into practice and into daily lives, unintended negative consequences that early theoretical researchers did not anticipate have also emerged, from the toxic language of Microsoft’s Twitter bot Tay (Shah and Chokkattu, 2016), to the leak of privacy of Amazon Alexa (Chung et al., 2017). A current highly-debated topic in NLP ethics is GPT-3 (Brown et al., 2020), whose risks and harms incl"
2021.findings-acl.273,2020.nlpcovid19-2.6,1,0.765064,"Missing"
2021.findings-acl.273,2020.lrec-1.772,1,0.787106,"Missing"
2021.findings-acl.273,2020.acl-main.195,0,0.018631,"ion (proposed) Education • NLP for educational question answering (Atapattu et al., 2015; Lende and Raghuwanshi, 2016) • Improving textbooks (Agrawal et al., 2010) • Automated grading (Madnani and Cahill, 2018; Taghipour and Ng, 2016) • Plagiarism detection (Chong et al., 2010) • Tools for learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz energy et al., 2020) • NLP to analyze cultural values regarding climate change (Jiang et al., 2017; Koenecke and Feliu-Fab`a, 2019) • Cross-cultural models of climate change perceptions (proposed) Table 3: Top priorities and some NLP research related to each of them. This list may not be exhaustive. We also propose a high-impact research problem in each of the areas which has received less attention so far"
2021.findings-acl.273,D16-1193,0,0.030786,"al., 2019) • NLP for food allocation (proposed) Health • NLP to analyze clinical notes (Dernoncourt & Wellet al., 2017a,b; Luo et al., 2018; Gopinath et al., being 2020; Leiter et al., 2020a,b) • NLP for psychotherapy and counseling (Biester et al., 2020; Xu et al., 2020; P´erez-Rosas et al., 2019) • NLP for happiness (Asai et al., 2018; Evensen et al., 2019) • Assistive speech generation (proposed) Education • NLP for educational question answering (Atapattu et al., 2015; Lende and Raghuwanshi, 2016) • Improving textbooks (Agrawal et al., 2010) • Automated grading (Madnani and Cahill, 2018; Taghipour and Ng, 2016) • Plagiarism detection (Chong et al., 2010) • Tools for learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz energy et al., 2020) • NLP"
2021.findings-acl.273,D16-1163,0,0.0290872,"ess (Asai et al., 2018; Evensen et al., 2019) • Assistive speech generation (proposed) Education • NLP for educational question answering (Atapattu et al., 2015; Lende and Raghuwanshi, 2016) • Improving textbooks (Agrawal et al., 2010) • Automated grading (Madnani and Cahill, 2018; Taghipour and Ng, 2016) • Plagiarism detection (Chong et al., 2010) • Tools for learners with disabilities (proposed) Equality • Interpretability (K¨ohn, 2015; Belinkov et al., 2017; Nie et al., 2020) • Ethics of NLP (Hovy and Spruit, 2016; Stanovsky et al., 2019; Sap et al., 2019) • NLP for low-resource languages (Zoph et al., 2016; Kim et al., 2017) • NLP on resource-limited devices (Sun et al., 2020) • NLP tools that signal bias in human language and speech (proposed) Clean • Raising public awareness of water sanitation water (proposed) Clean • Green NLP (Strubell et al., 2019; Schwartz energy et al., 2020) • NLP to analyze cultural values regarding climate change (Jiang et al., 2017; Koenecke and Feliu-Fab`a, 2019) • Cross-cultural models of climate change perceptions (proposed) Table 3: Top priorities and some NLP research related to each of them. This list may not be exhaustive. We also propose a high-impact resear"
2021.findings-acl.392,L18-1591,1,0.901026,"Missing"
2021.findings-acl.392,P19-1088,1,0.892588,"Missing"
2021.findings-acl.392,strapparava-valitutti-2004-wordnet,0,0.066773,"Missing"
2021.findings-acl.392,D18-1004,0,0.0271365,"urne et al., 2009; Nambisan, 2011). Computational approaches have aided studies in mental health forums, helping reveal positive relationships between linguistic accommodation and social support across subreddits (Sharma and De Choudhury, 2018). One example of insights from this work is that topicfocused communities like subreddits may enable more peer-engagement than non-community based platforms (Sharma et al., 2020). Other studies have revealed certain trade-offs of online support platforms, such as disparities in the level of support offered toward support-seekers of various demographics (Wang and Jurgens, 2018; Nobles et al., 2020) and in condolences extended across different topics of distress (Zhou and Jurgens, 2020). Studying MHP behaviors in such scenarios might help develop approaches that balance these trade-offs. Computational approaches applied in these forums have also shed light on population-level health trends and health information needs, with examinations into how depression and post-traumatic stress disorder (PTSD) affect different demographic strata (Amir et al., 2019). Data mining has also been applied to understand adverse drug reactions (Wang et al., 2014) and public reactions to"
2021.findings-emnlp.27,E12-1062,0,0.0314843,"policies. Opinion Mining from Social Media. Social media, such as Twitter, is a popular source to collect public opinions (Thelwall et al., 2011; Paltoglou and Thelwall, 2012; Pak and Paroubek, 2010; Rosenthal et al., 2015). Arunachalam and Sarkar (2013) suggest that Twitter can be a useful resource for governments to collect public opinion. Existing usage of Twitter for political analyses mostly targets at election result prediction (Beverungen and Kalita, 2011; Mohammad et al., 2015; Tjong Kim Sang and Bos, 2012), and opinion towards political parties (Pla and Hurtado, 2014) and presidents (Marchetti-Bowick and Chambers, 2012). To the best of our knowledge, this work is one of the first to use Twitter sentiment for causal analysis of policies. 3 Governor-Targeted Public Opinion To investigate the causality between public opinion and each state governor’s policy decisions, we first describe how we mine public opinion in this Section; we then describe the process we use to collect policies and other confounders in Section 4. We collect governor-targeted public opinion in two steps: (1) retrieve governor-related COVID tweets (Section 3.1), and (2) train a sentiment classification model for the COVID tweets and compile"
2021.findings-emnlp.27,pak-paroubek-2010-twitter,0,0.0235024,"olicy−−−→public support). Bol et al. tolerance leads to more pro-gay legislation in re- (2021); Ajzenman et al. (2020), how pandemic 289 characteristics affect Twitter sentiment (Gencoglu and Gruber, 2020), and how political partisanship causes impacts policies (i.e., partisanship−−−→policy designs) (Adolph et al., 2021). However, there is no existing work using public sentiments (e.g., from social media) to model COVID policies. Opinion Mining from Social Media. Social media, such as Twitter, is a popular source to collect public opinions (Thelwall et al., 2011; Paltoglou and Thelwall, 2012; Pak and Paroubek, 2010; Rosenthal et al., 2015). Arunachalam and Sarkar (2013) suggest that Twitter can be a useful resource for governments to collect public opinion. Existing usage of Twitter for political analyses mostly targets at election result prediction (Beverungen and Kalita, 2011; Mohammad et al., 2015; Tjong Kim Sang and Bos, 2012), and opinion towards political parties (Pla and Hurtado, 2014) and presidents (Marchetti-Bowick and Chambers, 2012). To the best of our knowledge, this work is one of the first to use Twitter sentiment for causal analysis of policies. 3 Governor-Targeted Public Opinion To inve"
2021.findings-emnlp.27,C14-1019,0,0.0292603,".g., from social media) to model COVID policies. Opinion Mining from Social Media. Social media, such as Twitter, is a popular source to collect public opinions (Thelwall et al., 2011; Paltoglou and Thelwall, 2012; Pak and Paroubek, 2010; Rosenthal et al., 2015). Arunachalam and Sarkar (2013) suggest that Twitter can be a useful resource for governments to collect public opinion. Existing usage of Twitter for political analyses mostly targets at election result prediction (Beverungen and Kalita, 2011; Mohammad et al., 2015; Tjong Kim Sang and Bos, 2012), and opinion towards political parties (Pla and Hurtado, 2014) and presidents (Marchetti-Bowick and Chambers, 2012). To the best of our knowledge, this work is one of the first to use Twitter sentiment for causal analysis of policies. 3 Governor-Targeted Public Opinion To investigate the causality between public opinion and each state governor’s policy decisions, we first describe how we mine public opinion in this Section; we then describe the process we use to collect policies and other confounders in Section 4. We collect governor-targeted public opinion in two steps: (1) retrieve governor-related COVID tweets (Section 3.1), and (2) train a sentiment"
2021.findings-emnlp.27,S17-2088,0,0.0662125,"Missing"
2021.findings-emnlp.27,S15-2078,0,0.026581,". Bol et al. tolerance leads to more pro-gay legislation in re- (2021); Ajzenman et al. (2020), how pandemic 289 characteristics affect Twitter sentiment (Gencoglu and Gruber, 2020), and how political partisanship causes impacts policies (i.e., partisanship−−−→policy designs) (Adolph et al., 2021). However, there is no existing work using public sentiments (e.g., from social media) to model COVID policies. Opinion Mining from Social Media. Social media, such as Twitter, is a popular source to collect public opinions (Thelwall et al., 2011; Paltoglou and Thelwall, 2012; Pak and Paroubek, 2010; Rosenthal et al., 2015). Arunachalam and Sarkar (2013) suggest that Twitter can be a useful resource for governments to collect public opinion. Existing usage of Twitter for political analyses mostly targets at election result prediction (Beverungen and Kalita, 2011; Mohammad et al., 2015; Tjong Kim Sang and Bos, 2012), and opinion towards political parties (Pla and Hurtado, 2014) and presidents (Marchetti-Bowick and Chambers, 2012). To the best of our knowledge, this work is one of the first to use Twitter sentiment for causal analysis of policies. 3 Governor-Targeted Public Opinion To investigate the causality bet"
2021.findings-emnlp.27,W12-0607,0,0.050298,"owever, there is no existing work using public sentiments (e.g., from social media) to model COVID policies. Opinion Mining from Social Media. Social media, such as Twitter, is a popular source to collect public opinions (Thelwall et al., 2011; Paltoglou and Thelwall, 2012; Pak and Paroubek, 2010; Rosenthal et al., 2015). Arunachalam and Sarkar (2013) suggest that Twitter can be a useful resource for governments to collect public opinion. Existing usage of Twitter for political analyses mostly targets at election result prediction (Beverungen and Kalita, 2011; Mohammad et al., 2015; Tjong Kim Sang and Bos, 2012), and opinion towards political parties (Pla and Hurtado, 2014) and presidents (Marchetti-Bowick and Chambers, 2012). To the best of our knowledge, this work is one of the first to use Twitter sentiment for causal analysis of policies. 3 Governor-Targeted Public Opinion To investigate the causality between public opinion and each state governor’s policy decisions, we first describe how we mine public opinion in this Section; we then describe the process we use to collect policies and other confounders in Section 4. We collect governor-targeted public opinion in two steps: (1) retrieve governor"
2021.findings-emnlp.27,2020.emnlp-demos.6,0,0.0177895,"the-art BERT model pretrained on COVID tweets by Müller et al. (2020).2 We finetune this pretrained COVID BERT on the Twitter sentiment analysis data from SemEval 2017 Task 4 Subtask A (Rosenthal et al., 2017). Given tweets collected from a diverse range of topics on Twitter, the model learns a three-way classification (positive, negative, neutral). In the training set, there are 19,902 samples with positive sentiments, 22,591 samples with neutral sentiments, and 7,840 samples with negative sentiments. We tokenize the input using the BERT tokenizer provided by the Transformers Python package (Wolf et al., 2020). We add [CLS] and [SEP] tokens at start and end of the input, respectively. The input is first encoded by the pretrained COVID BERT. Then, we use the contextualized vector C of the [CLS] token as the aggregate sentence representation. The model is finetuned on the classification task by training an additional feed-forward layer log(softmax(CW )) that assigns the softmax probability distribution to each sentiment class. Prior to training, we preprocess the tweets by deleting the retweet tags, and pseudonymising each tweet by replacing all URLs with a common text token. We also replace all unic"
2021.findings-emnlp.360,W19-3018,0,0.0232077,"ur approach are what data should be leveraged and what models should be built to understand and describe the training data. Another way to view this nuance is that feature engineering extracts task-level features that suit the data for a given task. Micromodels, on the other hand, build task-agnostic, domain-level features that can be applied on multiple tasks. Lastly, features from prior work are typically syntactic, statistical, or derivative features, such as lexical term frequencies (Coppersmith et al., 2014), extractions from metadata (Guntuku et al., 2017), or sentiment analyses scores (Chen et al., 2019). In addition to these features, we are able to build contextual features using contextualized language models, which are able to capture more nuanced concepts reflecting domain expertise. While word embeddings have been used as features before (Mohammadi et al., 2019), they are often difficult to interpret. On the other hand, because the researcher defines the behavior of each aggregator, our resulting feature vector is easy to interpret. Because a micromodel architecture orchestrates multiple models, it may appear similar to ensemble learning. The key difference is that every model in an ens"
2021.findings-emnlp.360,2020.acl-main.493,0,0.0124833,"assification, and suicidal risk assessment. 2 Background and Related Work We find inspiration in previous work that addressed explainability, reusability, efficiency under lowresource scenarios, and integration of domain expertise. We focus primarily on research that was carried out in the domain of mental health. Explainability. Neural networks are black-box models that lack transparency and explainability. Structural analyses of neural networks (Vig et al., 2020), such as probing, has become a popular approach to investigate linguistic properties learned by language models (Wu et al., 2021; Chi et al., 2020; Belinkov et al., 2018; Hewitt and Manning, 2019; Tenney et al., 2018). However, these analyses do not explain how the models use their latent information for their tasks and how they reach their decisions. These drawbacks are especially problematic in the mental health domain (Carr, 2020). Linear models implemented with feature engineering can be analyzed via global feature importance scores, but they do not necessarily provide explanations at a query-level. Model-agnostic explanation frameworks such as SHAP or LIME values (Lundberg and Lee, 2017; Ribeiro et al., 2016) can provide query-leve"
2021.findings-emnlp.360,W15-1204,0,0.0582819,"Missing"
2021.findings-emnlp.360,N19-1419,0,0.012873,"t. 2 Background and Related Work We find inspiration in previous work that addressed explainability, reusability, efficiency under lowresource scenarios, and integration of domain expertise. We focus primarily on research that was carried out in the domain of mental health. Explainability. Neural networks are black-box models that lack transparency and explainability. Structural analyses of neural networks (Vig et al., 2020), such as probing, has become a popular approach to investigate linguistic properties learned by language models (Wu et al., 2021; Chi et al., 2020; Belinkov et al., 2018; Hewitt and Manning, 2019; Tenney et al., 2018). However, these analyses do not explain how the models use their latent information for their tasks and how they reach their decisions. These drawbacks are especially problematic in the mental health domain (Carr, 2020). Linear models implemented with feature engineering can be analyzed via global feature importance scores, but they do not necessarily provide explanations at a query-level. Model-agnostic explanation frameworks such as SHAP or LIME values (Lundberg and Lee, 2017; Ribeiro et al., 2016) can provide query-level, or local, feature importance scores, but they"
2021.findings-emnlp.360,W19-3025,0,0.0327049,"Missing"
2021.findings-emnlp.360,N19-1423,0,0.0398785,"Missing"
2021.findings-emnlp.360,2020.emnlp-main.650,1,0.741344,"data can be time consuming This may improve accuracy, but at the cost of in- and expensive. We use BERT and Universal Senterpretability. Given the sensitive and high-risk do- tence Encoders (Cer et al., 2018) to rapidly colmain of healthcare, where even the most accurate lect representative samples for each micromodel. models become impractical without explainability Our approach is inspired by work on collecting (Caruana et al., 2015), we use EBMs in this work. data for dialogue systems. Specifically, Kang et al. Third, researchers can give their own definition (2018), Larson et al. (2019), Larson et al. (2020), of &quot;Text Utterances&quot; (i). In the CLPsych 2015 and Stasaski et al. (2020) proposed ways to build a Shard Task (Section 4.1), we define each &quot;Text Ut- diverse dataset by iteratively collecting data, startterance&quot; to be a single tweet from a user. However, ing from a seed set and crowdsourcing paraphrases. 4260 Figure 2 depicts our pipeline for building our micromodel datasets. For each micromodel, we build an example corpus and gather paraphrases. While crowdsourcing can be thought of a generative approach for paraphrasing, we take a retrieval approach by using a BERT model to search for seman"
2021.findings-emnlp.360,W18-0609,0,0.215534,"h domain (Carr, 2020). Linear models implemented with feature engineering can be analyzed via global feature importance scores, but they do not necessarily provide explanations at a query-level. Model-agnostic explanation frameworks such as SHAP or LIME values (Lundberg and Lee, 2017; Ribeiro et al., 2016) can provide query-level, or local, feature importance scores, but they are approximate explanations of the underlying model. Our approach provides (1) global and local feature importance scores, and (2) evidence from input text data that led to its output. either fine-tune their embeddings (Orabi et al., 2018) or have task-specific layers (Matero et al., 2019). While task-specific designs can boost accuracy, they are difficult to extend to multiple applications. Furthermore, Harrigian et al. (2020) show that models trained for a task in the mental health domain do not generalize across test sets that originate from different sources. Because our micromodels are built on task-agnostic data, they are reusable for multiple applications within a domain. Efficiency in Low-Resource Scenarios. Obtaining data in the mental health domain is difficult because of the sensitive nature of data and the need for"
2021.findings-emnlp.360,W15-1205,0,0.078348,"(2015) Abraham and Fava (1999); Levy and Deykin (1989) Swearer et al. (2001) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Table 1: The micromodels we developed for this work. Model LR CNN UMD WWBP MM Expl? Reuse? D vs C n = 654 P vs C n = 492 D vs P n=573 0.8 0.79 0.86 0.904 0.821 0.817 0.85 0.893 0.916 0.936 0.785 0.87 0.841 0.81 0.892 Table 2: AUC scores for various approaches, where LR is a logistic regression model, CNN is a convolutional neural network, and MM is our micromodel approach. UMD is from Resnik et al. (2015), WWBP is from Preotiuc-Pietro et al. (2015) – these two systems were the only ones that reported AUC scores and are directly comparable to ours. We also indicate whether each approach is explainable and reusable. micromodel has its own example corpus built using our data collection pipeline (Section 3.3), and uses a similarity score threshold value of 0.85. We use two aggregators. One is as described in Section 3.2, which returns the ratio of hits in a binary vector. The other aggregator looks for &quot;windows&quot;: segments within each binary vector where many hits occur close to one another. These windows may represent temporal &quot;episodes&quot; –"
2021.findings-emnlp.360,D19-1410,0,0.0124656,"build an example corpus and gather paraphrases. While crowdsourcing can be thought of a generative approach for paraphrasing, we take a retrieval approach by using a BERT model to search for semantically similar sentences in a separate corpus of unstructured text data. In particular, we use anonymized posts from the r/depression subreddit3 , a peer support forum for anyone struggling with a depressive disorder. While any corpus can be used to retrieve paraphrases, it is important that the linguistic phenomena that is of interest will be prevalent in the corpus. We used Sentence Transformers (Reimers and Gurevych, 2019)4 and the &quot;paraphrase-xlm-r-multilingual-v1&quot; pre-trained model for our semantic similarity searches. There are multiple ways to initialize the example corpus. One can build lexical queries by specifying patterns based on parsers or lexicons and apply them on a text corpus. For instance, to find examples of the labeling cognitive distortion (attaching a negative label to oneself), a lexical query might look for sentences that contain a first person pronoun with a nominal subject relation with a negative token according to the LIWC lexicon (Pennebaker et al., 2001). While this may seem like an o"
2021.findings-emnlp.360,W19-3005,0,0.182983,"odels such as BERT (Devlin availability and the need for explanations. Raw data is often limited and annotating it requires spe- et al., 2019). This training occurs once and then the micromodels can be reused across multiple tasks cialized knowledge (Aguirre et al., 2021). When within a single domain. Second, the task-specific a dataset is available for a task, research on modmodel is trained on the dataset of interest. During els will often overfit, developing optimizations that cannot be reused for other datasets or tasks (Gun- this phase the micromodels are not modified. tuku et al., 2017; Matero et al., 2019; Chen et al., We demonstrate the benefits of micromodels in 2019). Attempts to reduce data needs by integrating the important domain of mental health. Recent domain knowledge often result in inefficient and ex- studies have shown a rapid increase in the prevapensive models (Yang et al., 2019; Liu et al., 2020; lence of depression symptoms in various demoXie et al., 2020). Integrating knowledge graphs is graphics (Ettman et al., 2020), along with elevated another alternative (Zhang et al., 2019), but poses levels of suicidal ideation (Czeisler et al., 2020). challenges in domains in which doma"
2021.findings-emnlp.360,W19-3004,0,0.0358055,"n the other hand, build task-agnostic, domain-level features that can be applied on multiple tasks. Lastly, features from prior work are typically syntactic, statistical, or derivative features, such as lexical term frequencies (Coppersmith et al., 2014), extractions from metadata (Guntuku et al., 2017), or sentiment analyses scores (Chen et al., 2019). In addition to these features, we are able to build contextual features using contextualized language models, which are able to capture more nuanced concepts reflecting domain expertise. While word embeddings have been used as features before (Mohammadi et al., 2019), they are often difficult to interpret. On the other hand, because the researcher defines the behavior of each aggregator, our resulting feature vector is easy to interpret. Because a micromodel architecture orchestrates multiple models, it may appear similar to ensemble learning. The key difference is that every model in an ensemble learns the same task, while the micromodels each have a different aim. Micromodels are also intended to be used across tasks, whereas the models in an ensemble are task specific. 4 Evaluation We evaluate our micromodel architecture in terms of accuracy, reusabili"
2021.findings-emnlp.360,W19-3023,0,0.0561118,"Missing"
2021.findings-emnlp.360,W15-1207,0,0.0200577,"Spitzer et al. (2006) Zahn et al. (2015) Abraham and Fava (1999); Levy and Deykin (1989) Swearer et al. (2001) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Cohan et al. (2018) Table 1: The micromodels we developed for this work. Model LR CNN UMD WWBP MM Expl? Reuse? D vs C n = 654 P vs C n = 492 D vs P n=573 0.8 0.79 0.86 0.904 0.821 0.817 0.85 0.893 0.916 0.936 0.785 0.87 0.841 0.81 0.892 Table 2: AUC scores for various approaches, where LR is a logistic regression model, CNN is a convolutional neural network, and MM is our micromodel approach. UMD is from Resnik et al. (2015), WWBP is from Preotiuc-Pietro et al. (2015) – these two systems were the only ones that reported AUC scores and are directly comparable to ours. We also indicate whether each approach is explainable and reusable. micromodel has its own example corpus built using our data collection pipeline (Section 3.3), and uses a similarity score threshold value of 0.85. We use two aggregators. One is as described in Section 3.2, which returns the ratio of hits in a binary vector. The other aggregator looks for &quot;windows&quot;: segments within each binary vector where many hits occur close to one another. These"
2021.findings-emnlp.360,W19-3021,0,0.087588,"Missing"
2021.findings-emnlp.360,W19-3020,0,0.0820343,"Missing"
2021.findings-emnlp.360,W18-0603,0,0.0316259,"each have a different aim. Micromodels are also intended to be used across tasks, whereas the models in an ensemble are task specific. 4 Evaluation We evaluate our micromodel architecture in terms of accuracy, reusability, and efficiency under lowresource scenarios. We also address the explainability properties of our model in Section 5. 4.1 Data were collected. The tasks include (1) classifying depression users versus control users (D vs. C), (2) classifying PTSD users versus control users (P vs. C), and (3) classifying depression users versus PTSD users (D vs. P). CLPsych 2019 Shared Task (Shing et al., 2018; Zirikly et al., 2019). This data is from Reddit users who have posted in the r/SuicideWatch 6 subreddit, a peer support forum for anyone struggling with suicidal thoughts, and were annotated with 4 levels of suicidal risk (no risk, low, moderate, severe). A group of users who have never posted on r/SuicideWatch was used as a control group. The shared task includes 3 tasks: Task A is risk assessment looking only at the users’ posts in r/SuicideWatch. Task B is also risk assessment, but also provides posts across other subreddits. Task C is about screening, with only posts that are not in r/Su"
2021.findings-emnlp.360,D17-1322,0,0.0174154,"multiple applications. Furthermore, Harrigian et al. (2020) show that models trained for a task in the mental health domain do not generalize across test sets that originate from different sources. Because our micromodels are built on task-agnostic data, they are reusable for multiple applications within a domain. Efficiency in Low-Resource Scenarios. Obtaining data in the mental health domain is difficult because of the sensitive nature of data and the need for expert annotators. While researchers have turned to proxy-based annotations, in which data is annotated using automated mechanisms (Yates et al., 2017; Winata et al., 2018), these datasets have caveats and biases (Aguirre et al., 2021; Coppersmith et al., 2015). These data limitations make it difficult to apply standard neural methods. Integrating Domain Expertise. Psychologists have long studied effective methods for assessing patients for various mental health illnesses. Assessment modules such as the Patient Health Questionnaire-9 (PHQ-9) (Kroenke et al., 2001) or PTSD Checklist (PCL) (Ruggiero et al., 2003) allow physicians to reliably screen for the presence or severity of various mental statuses. Similarly, cognitive distortions are i"
2021.findings-emnlp.360,2020.acl-main.446,0,0.0722803,"Missing"
2021.findings-emnlp.360,P19-1139,0,0.0202828,"d for other datasets or tasks (Gun- this phase the micromodels are not modified. tuku et al., 2017; Matero et al., 2019; Chen et al., We demonstrate the benefits of micromodels in 2019). Attempts to reduce data needs by integrating the important domain of mental health. Recent domain knowledge often result in inefficient and ex- studies have shown a rapid increase in the prevapensive models (Yang et al., 2019; Liu et al., 2020; lence of depression symptoms in various demoXie et al., 2020). Integrating knowledge graphs is graphics (Ettman et al., 2020), along with elevated another alternative (Zhang et al., 2019), but poses levels of suicidal ideation (Czeisler et al., 2020). challenges in domains in which domain knowledge Because our micromodels represent domain-level 4257 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4257–4272 November 7–11, 2021. ©2021 Association for Computational Linguistics linguistic patterns, they can be reused for multiple tasks within the same domain, while requiring only half or sometimes just a quarter of the taskspecific annotation data, and also having the benefit of explainability across the entire pipeline. The primary contributions of th"
2021.naacl-main.216,L18-1252,0,0.150529,"elopment of emotionally ring knowledge between different tasks. Dynamic intelligent agents. sampling strategies, which aim at adaptively adThe impact of stress on human behavior can justing the ratio of samples from different tasks, be observed through various modalities. Previ- are widely used to balance the training schedule. 2714 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2714–2725 June 6–11, 2021. ©2021 Association for Computational Linguistics However, strategies based on gradients (Chen et al., 2018b), uncertainty (Kendall et al., 2018) or loss (Liu et al., 2019) cannot leverage the validation performances, while some performance-based strategies (Gottumukkala et al., 2020) are impractical if the metrics for different tasks are not directly comparable (i.e., with different scale ranges). To this end, we propose a novel speed-based strategy that is both effective and efficient in the multi-task learning for stress and emotion. Our method is evaluated on the Multimodal Stressed Emotion (MuSE) dataset (Jaiswal et al., 2019, 2020), which includes both stress and emotion labels, making it the"
2021.naacl-main.216,N19-1423,0,0.0155232,"nscription is less than 5, resulting in 1,484 videos. The contents and scenarios in the OMG-Emotion dataset are completely different from MuSE. We hold out 300 videos as a validation set to enable dynamic sampling. Note that stress detection is a binary classification task, while the two auxiliary emotion tasks have a regressive nature. 4.2 Pre-processing 4.3 MUSER: Architecture We propose MUSER: MUltimodal Stress Detector using Emotion Recognition. The model structure is based on neural networks. Specifically, we use a Transformer (Vaswani et al., 2017) textual encoder pre-trained with BERT (Devlin et al., 2019), and an MLP-based acoustic encoder to generate representations on each modality, and fuse them before classification or regression. Our model architecture is depicted in Figure 1. 4.3.1 Textual Encoder For the textual encoder, we use a Transformer neural network pre-trained with BERT on BookCorpus and English Wikipedia (Devlin et al., 2019). Our Transformer model has 12 layers, 12 attention heads, and 768-dimensional hidden states. The averaged hidden states on the top level are projected to 256-dimensional representations by a fully-connected layer. 4.3.2 Acoustic Encoder Our acoustic encode"
2021.naacl-main.216,2020.acl-main.86,0,0.457658,"culty dealing with the conditions (Dobson and Smith, 2000; Muthukumar and Nachiappan, 2013). Stress detection is a classification task that predicts whether a certain target is under stress. The task has drawn research attention for two reasons: first, stress detection plays an important role in applications related to psychological well-being (Cohen et al., 1991), cognitive behavior therapies (Tull et al., 2007), and safe driving (Gao et al., 2014; Chen et al., 2017); second, stress is a known regMulti-task learning (Pasunuru and Bansal, 2017; ulator of human emotion mechanisms (Tull et al., Gottumukkala et al., 2020; Guo et al., 2018a; Gong 2007), and thus research on stress detection can et al., 2019) has proven to be effective for transferpotentially benefit the development of emotionally ring knowledge between different tasks. Dynamic intelligent agents. sampling strategies, which aim at adaptively adThe impact of stress on human behavior can justing the ratio of samples from different tasks, be observed through various modalities. Previ- are widely used to balance the training schedule. 2714 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguist"
2021.naacl-main.216,C18-1039,0,0.145984,"itions (Dobson and Smith, 2000; Muthukumar and Nachiappan, 2013). Stress detection is a classification task that predicts whether a certain target is under stress. The task has drawn research attention for two reasons: first, stress detection plays an important role in applications related to psychological well-being (Cohen et al., 1991), cognitive behavior therapies (Tull et al., 2007), and safe driving (Gao et al., 2014; Chen et al., 2017); second, stress is a known regMulti-task learning (Pasunuru and Bansal, 2017; ulator of human emotion mechanisms (Tull et al., Gottumukkala et al., 2020; Guo et al., 2018a; Gong 2007), and thus research on stress detection can et al., 2019) has proven to be effective for transferpotentially benefit the development of emotionally ring knowledge between different tasks. Dynamic intelligent agents. sampling strategies, which aim at adaptively adThe impact of stress on human behavior can justing the ratio of samples from different tasks, be observed through various modalities. Previ- are widely used to balance the training schedule. 2714 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Languag"
2021.naacl-main.216,2020.lrec-1.187,1,0.923344,"ly have access to partial information about the expression of stress, while multiple modalities can potentially be informative at the same time (Aigrain et al., 2016). As demonstrated by previous work on human sentiment and emotion prediction (Zadeh et al., 2016, 2018; Yao et al., 2020), multimodal features usually results in better performances. 2.2 Multimodal Stress Detection Commonly-used modalities for stress detection include video, audio, text and physiological signals such as thermal maps from sensors (Aigrain et al., 2016; Alberdi et al., 2016; Lane et al., 2015; Jaques et al., 2016). Jaiswal et al. (2020) proposed the Multimodal Stressed Emotion (MuSE) dataset, which includes records from all the commonly-used modalities. Each video clip is annotated for both stress detection and emotion recognition. Unimodal and multimodal baselines are provided for each task. Bara et al. (2020) developed a multimodal deep learning method that learns modality-independent representations in an unsupervised approach. However, none of these models leverage the intrinsic connections between stress and emotion. Our experiments are conducted on the MuSE dataset using only the textual and acoustic modalities, to be"
2021.naacl-main.216,P17-1117,0,0.0287784,"or physical tension, as a response to the environment when people have difficulty dealing with the conditions (Dobson and Smith, 2000; Muthukumar and Nachiappan, 2013). Stress detection is a classification task that predicts whether a certain target is under stress. The task has drawn research attention for two reasons: first, stress detection plays an important role in applications related to psychological well-being (Cohen et al., 1991), cognitive behavior therapies (Tull et al., 2007), and safe driving (Gao et al., 2014; Chen et al., 2017); second, stress is a known regMulti-task learning (Pasunuru and Bansal, 2017; ulator of human emotion mechanisms (Tull et al., Gottumukkala et al., 2020; Guo et al., 2018a; Gong 2007), and thus research on stress detection can et al., 2019) has proven to be effective for transferpotentially benefit the development of emotionally ring knowledge between different tasks. Dynamic intelligent agents. sampling strategies, which aim at adaptively adThe impact of stress on human behavior can justing the ratio of samples from different tasks, be observed through various modalities. Previ- are widely used to balance the training schedule. 2714 Proceedings of the 2021 Conference"
2021.naacl-main.216,N19-1266,1,0.827959,"l family past money tentat feel sad negate anger achieve quant activation and valence scores (1∼9), which is more fine-grained than categorical definitions (happy, angry, etc.). The OMG-Emotion dataset we use as external auxiliary task is annotated in the same way with a score range of 0∼1. 2.4 Multi-task Learning Because of the different task natures, balancing the training procedure with all the tasks is a critical problem for multi-task learning. Loss-balancing strategies (Chen et al., 2018b; Kendall et al., 2018; Liu et al., 2019; Gong et al., 2019; Guo et al., 2018a; Lample et al., 2017; Yao et al., 2019) are suitable for situations in which there are multiple training objectives that can be combined via weighted summation for each data point. In contrast, for multi-task learning across different datasets, a sampling strategy should be applied to decide the mixing ratio (how many batches to sample from each task) in each epoch. To this end, Pasunuru et al. (2017) used a fixed sampling ratio; Guo et al. (2018b) proposed a dynamic sampling strategy based on reinforcement learning, which depends on the estimation of Q-values; Gottumukkala et al. (2020) used a dynamic sampling procedure based on t"
2021.naacl-main.216,P18-1208,0,0.129973,"based strategy that is both effective and efficient in the multi-task learning for stress and emotion. Our method is evaluated on the Multimodal Stressed Emotion (MuSE) dataset (Jaiswal et al., 2019, 2020), which includes both stress and emotion labels, making it the ideal benchmark for an in-depth analysis of their inter-dependence. To test the generalization ability of our method, we also use an external emotion dataset for the auxiliary task. Multimodal emotion recognition is a wellstudied field with many existing datasets (Busso et al., 2008, 2016; Chen et al., 2018a; Barros et al., 2018; Zadeh et al., 2018). We choose the OMGEmotion dataset (Barros et al., 2018) as the external auxiliary task because it is representative and challenging, with numerical emotion scores instead of categorical labels. Our paper makes four main contributions. First, we show the inter-dependence between stress and emotion via quantitative analyses on linguistic and acoustic features, and propose to use emotion recognition as an auxiliary task for stress detection. Second, we establish a stress detection model with a transformer structure, as well as a novel speed-based dynamic sampling strategy for multi-task learning"
2021.nlp4if-1.7,W18-5513,0,0.0863693,"ents, we use SBERT (Reimers and Gurevych, 2019) contextual embeddings to transform text into sentence vectors and cosine similarity as similarity measure. Table 1: An example data point from the LIAR-PLUS dataset, with ground truth explanations, and explanations generated by our methods. NLP community (Atanasova et al., 2020; Fan et al., 2020; Kotonya and Toni, 2020). In (Atanasova et al., 2020) the authors proposed a supervised BERT (Devlin et al., 2019) based model for jointly predicting the veracity of a claim by extracting supporting explanations from fact-checked claims in the LIAR-PLUS (Alhindi et al., 2018) dataset. Kotonya and Toni (2020) constructed a dataset for a similar task in the public health domain and provided baseline models for explainable fact verification using this dataset. Fan et al. (2020) used explanations about a claim to assist fact-checkers and showed that explanations improved both the efficiency and the accuracy of the fact-checking process. 3 Extractive: Biased TextRank 3.2 Abstractive: GPT-2 Based We implement an abstractive explanation generation method based on GPT-2, a transformer-based language model introduced in Radford et al. (2019) and trained on 8 million web pa"
2021.nlp4if-1.7,2020.acl-main.656,0,0.260468,"s et al., 2018; Thorne and Vlachos, 2018; Lu and Li, 2020), the majority of this work aims to categorize claims, rather than generate explanations that support or deny them. This is a challenging problem that has been mainly tackled by expert journalists who manually verify the information surrounding a given claim and provide a detailed verdict based on supporting or refuting evidence. More recently, there has been a growing interest in creating computational tools able to assist during this process by providing supporting explanations for a given claim based on the news content and context (Atanasova et al., 2020; Fan et al., 2020). While a true or false veracity label does not provide enough information and a detailed fact-checking report or news article might take long to read, bitesized explanations can bridge this gap and improve the transparency of automated news evaluation systems. 2 Related Work While explainability in AI has been a central subject of research in recent years (Poursabzi-Sangdeh et al., 2018; Lundberg and Lee, 2017; Core et al., 2006), the generation of natural language explanations is still relatively understudied. Camburu et al. (2018) propose e-SNLI, a natural language (NL) i"
2021.nlp4if-1.7,D19-5602,0,0.0246437,"Missing"
2021.nlp4if-1.7,C18-1287,1,0.89267,"Missing"
2021.nlp4if-1.7,2020.acl-main.54,0,0.0215628,"ided baseline models for explainable fact verification using this dataset. Fan et al. (2020) used explanations about a claim to assist fact-checkers and showed that explanations improved both the efficiency and the accuracy of the fact-checking process. 3 Extractive: Biased TextRank 3.2 Abstractive: GPT-2 Based We implement an abstractive explanation generation method based on GPT-2, a transformer-based language model introduced in Radford et al. (2019) and trained on 8 million web pages containing 40 GBs of text. Aside from success in language generation tasks (Budzianowski and Vuli´c, 2019; Ham et al., 2020), the pretrained GPT-2 model enables us to generate abstractive explanations for a relatively small dataset through transfer learning. In order to generate explanations that are closer in domain and style to the reference explanation, we conduct an initial fine-tuning step. While fine tuning, we provide the news article, the claim, and its corresponding explanation as an input to the model and explicitly mark the beginning and the end of each input argument with bespoke tokens. At test time, we provide the article and query inputs in similar format but leave the explanation field to be complet"
2021.nlp4if-1.7,D19-1410,0,0.01998,"he bias term. The bias query is embedded into Biased TextRank using a similar idea introduced by Haveliwala (2002) for topic-sensitive PageRank. The similarity between the text segments that form the graph and the “bias” is used to set the restart probabilities of the random walker in a run of PageRank over the text graph. That means the more similar each text segment is to the bias query, the more likely it is for that node to be visited in each restart and therefore, it has a better chance of ranking higher than the less similar nodes to the bias query. During our experiments, we use SBERT (Reimers and Gurevych, 2019) contextual embeddings to transform text into sentence vectors and cosine similarity as similarity measure. Table 1: An example data point from the LIAR-PLUS dataset, with ground truth explanations, and explanations generated by our methods. NLP community (Atanasova et al., 2020; Fan et al., 2020; Kotonya and Toni, 2020). In (Atanasova et al., 2020) the authors proposed a supervised BERT (Devlin et al., 2019) based model for jointly predicting the veracity of a claim by extracting supporting explanations from fact-checked claims in the LIAR-PLUS (Alhindi et al., 2018) dataset. Kotonya and Toni"
2021.nlp4if-1.7,2020.coling-main.144,1,0.813396,"Missing"
2021.nlp4if-1.7,C18-1283,0,0.0285407,"rvised graph-based algorithm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise. 1 Introduction Navigating the media landscape is becoming increasingly challenging given the abundance of misinformation, which reinforces the importance of keeping our news consumption focused and informed. While fake news and misinformation have been a recent focus of research studies (PérezRosas et al., 2018; Thorne and Vlachos, 2018; Lu and Li, 2020), the majority of this work aims to categorize claims, rather than generate explanations that support or deny them. This is a challenging problem that has been mainly tackled by expert journalists who manually verify the information surrounding a given claim and provide a detailed verdict based on supporting or refuting evidence. More recently, there has been a growing interest in creating computational tools able to assist during this process by providing supporting explanations for a given claim based on the news content and context (Atanasova et al., 2020; Fan et al., 2020"
2021.nlp4if-1.7,2020.emnlp-main.623,0,0.0645418,"Missing"
2021.nlp4if-1.7,2020.emnlp-main.448,0,0.0148389,"explanations: an extractive unsupervised method based on Biased TextRank, and an abstractive method based on GPT-2. 46 Dataset LIAR-PLUS HNR to generate explanations. We stop the generation after the model outputs the explicit end of the text token introduced in the fine-tuning process. Overall, this fine-tuning strategy is able to generate explanations that follow a style similar to the reference explanation. However, we identify cases where the model generates gibberish and/or repetitive text, which are problems previously reported in the literature while using GPT-2 (Holtzman et al., 2019; Welleck et al., 2020). To address these issues, we devise a strategy to remove unimportant sentences that could introduce noise to the generation process. We first use Biased TextRank to rank the importance of the article sentences towards the question/claim. Then, we repeatedly remove the least important sentence (up to 5 times) and input the modified text into the GPT-2 generator. This approach keeps the text generation time complexity in the same order of magnitude as before and reduces the generation noise rate to close to zero. 4 4.1 Av. Words 98.89 87.82 Av. Sent. 5.20 4.63 Table 2: Dataset statistics for ex"
2021.nlp4if-1.7,2020.acl-main.771,0,0.0246482,"sparency of automated news evaluation systems. 2 Related Work While explainability in AI has been a central subject of research in recent years (Poursabzi-Sangdeh et al., 2018; Lundberg and Lee, 2017; Core et al., 2006), the generation of natural language explanations is still relatively understudied. Camburu et al. (2018) propose e-SNLI, a natural language (NL) inference dataset augmented with human-annotated NL explanations. In their paper, Camburu et al. generated NL explanations for premise and hypothesis pairs for an inference task using the InferSent (Conneau et al., 2017) architecture. Kumar and Talukdar (2020) propose the task of generating “faithful” (i.e., aligned with the model’s internal decision making) NL explanations and propose NILE, a method that jointly produces NLI labels and faithful NL explanations. Generating explanations in the context of news and fact-checking is a timely and novel topic in the 45 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 45–50 June 6, 2021. ©2021 Association for Computational Linguistics Claim: nearly half of Oregon’s children are poor. Fact-Check Report: ...Jim Francesconi...said...""Nearly half of Oregon’s children are poor."" He sai"
2021.nlp4if-1.7,W04-1013,0,0.0693629,"rther study explanations in this dataset, we randomly select 50 articles along with their corresponding questions and explanations. We then manually label sentences in the original article that are relevant to the quality aspect being measured.2 During this process we only include explanations that are deemed as “satisfactory,” which means that relevant information is included in the original article. Evaluation Experimental Setup We use a medium (355M hyper parameters) GPT-2 model (Radford et al., 2019) as implemented in the Huggingface transformers (Wolf et al., 2019) library. We use ROUGE (Lin, 2004), a common measure for language generation assessment as our main evaluation metric for the generated explanations and report the F score on three variations of ROUGE: ROUGE-1, ROUGE-2 and ROUGE-L. We compare our methods against two baselines. The first is an explanation obtained by applying TextRank on the input text. The second, called “embedding similarity”, ranks the input sentences by their embedding cosine similarity to the question and takes the top five sentences as an explanation. 4.2 Total count 12,679 16,500 Datasets LIAR-PLUS. The LIAR-PLUS (Alhindi et al., 2018) dataset contains 1"
2021.nlp4if-1.7,2020.acl-main.48,0,0.0196599,"hm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise. 1 Introduction Navigating the media landscape is becoming increasingly challenging given the abundance of misinformation, which reinforces the importance of keeping our news consumption focused and informed. While fake news and misinformation have been a recent focus of research studies (PérezRosas et al., 2018; Thorne and Vlachos, 2018; Lu and Li, 2020), the majority of this work aims to categorize claims, rather than generate explanations that support or deny them. This is a challenging problem that has been mainly tackled by expert journalists who manually verify the information surrounding a given claim and provide a detailed verdict based on supporting or refuting evidence. More recently, there has been a growing interest in creating computational tools able to assist during this process by providing supporting explanations for a given claim based on the news content and context (Atanasova et al., 2020; Fan et al., 2020). While a true or"
2021.nlp4if-1.7,W04-3252,1,0.105657,"Missing"
2021.socialnlp-1.13,W10-0204,0,0.0440305,"Missing"
2021.socialnlp-1.13,P18-1125,0,0.0436771,"Missing"
banea-etal-2008-bootstrapping,E06-1026,0,\N,Missing
banea-etal-2008-bootstrapping,E06-1025,0,\N,Missing
banea-etal-2008-bootstrapping,E06-1027,0,\N,Missing
banea-etal-2008-bootstrapping,N06-1026,0,\N,Missing
banea-etal-2008-bootstrapping,H05-1044,1,\N,Missing
banea-etal-2008-bootstrapping,H93-1061,0,\N,Missing
banea-etal-2008-bootstrapping,W03-1017,0,\N,Missing
banea-etal-2008-bootstrapping,W03-1014,1,\N,Missing
banea-etal-2008-bootstrapping,P07-1123,1,\N,Missing
banea-etal-2008-bootstrapping,P02-1053,0,\N,Missing
banea-etal-2008-bootstrapping,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
C02-1039,J99-2002,0,\N,Missing
C02-1039,J98-1006,0,\N,Missing
C02-1039,W96-0208,0,\N,Missing
C02-1039,N01-1011,0,\N,Missing
C02-1039,W96-0211,0,\N,Missing
C02-1039,J01-3001,0,\N,Missing
C02-1039,J95-4004,0,\N,Missing
C02-1039,P96-1006,0,\N,Missing
C04-1162,H92-1046,0,0.0815134,"W1 and W2 , each with NW 1 and NW 2 senses defined in a dictionary, for each possible sense pair W1i and W2j , i=1..NW 1 , j=1..NW 2 , first determine their definitions overlap, by counting the number of words they have in common. Next, the sense pair with the highest overlap is selected, and consequently a sense is assigned to each of the two words involved in the initial pair. When applied to open text, the original definition of the algorithm faces an explosion of word sense combinations4 , and alternative solutions are required. One solution is to use simulated annealing, as proposed in (Cowie et al., 1992). Another solution – which we adopt in our experiments – is to use a variation of the Lesk algorithm (Kilgarriff and Rosenzweig, 2000), where meanings of words in the text are determined individually, by finding the highest overlap between the sense definitions of each word and the current context. Rather than seeking to simultaneously determine the meanings of all words in a given text, this approach determines word senses individually, and therefore it avoids the combinatorial explosion of senses. 5.2 Most Frequent Sense WordNet keeps track of the frequency of each word meaning within a sens"
C04-1162,H92-1045,0,0.0924045,"e the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an entire text (Morris and Hirst, 1991). Selectional preferences. Automatically or semiautomatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997). Heuristic-based methods. These methods consist of simple rules that can reliably assign a sense to certain word categories: one sense per collocation (Yarowsky, 1993), and one sense per discourse (Gale et al., 1992). In this paper, we propose a new open-text disambiguation algorithm that combines information drawn from a semantic network (WordNet) with graph-based ranking algorithms (PageRank). We compare our method with other open-text word sense disambiguation algorithms, and show that the accuracy achieved through our new PageRank-based method exceeds the performance obtained by other knowledge-based methods. 3 PageRank on Semantic Networks In this section, we briefly describe PageRank (Brin and Page, 1998), and describe the view of WordNet as a graph, which facilitates the application of the graph-ba"
C04-1162,H93-1061,0,0.295136,"t sense provides the best results:  4 × F R × P R if N = 1 Rank = FR × PR if N > 1 where F R represents the WordNet sense frequency, P R represents the rank computed by PageRank, N is the position in the frequency ordered synset list, and Rank represents the combined rank. 6 Experimental Evaluation We evaluate the accuracy of the word sense disambiguation algorithms on a benchmark of senseannotated texts, in which each open-class word is mapped to the meaning selected by a lexicographer as being the most appropriate one in the context of a sentence. We are using a subset of the SemCor texts (Miller et al., 1993) – five randomly selected files covering different topics: news, sports, entertainment, law, and debates – as well as the data set provided for the English all words task during S ENSEVAL -2. The average size of a file is 600-800 open class words. On each file, we run two sets of evaluations. (1) One set consisting of the basic “uninformed” version of the knowledge-based algorithms, where the sense ordering provided by the dictionary is not taken into account at any point. (2) A second set of experiments consisting of “informed” disambiguation algorithms, which incorporate the sense order rend"
C04-1162,W97-0209,0,0.0347181,"urn divided into two main categories: (1) Local context – where the semantic measures are used to disambiguate words additionally connected by syntactic relations (Stetina et al., 1998). (2) Global context – where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an entire text (Morris and Hirst, 1991). Selectional preferences. Automatically or semiautomatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997). Heuristic-based methods. These methods consist of simple rules that can reliably assign a sense to certain word categories: one sense per collocation (Yarowsky, 1993), and one sense per discourse (Gale et al., 1992). In this paper, we propose a new open-text disambiguation algorithm that combines information drawn from a semantic network (WordNet) with graph-based ranking algorithms (PageRank). We compare our method with other open-text word sense disambiguation algorithms, and show that the accuracy achieved through our new PageRank-based method exceeds the performance obtained by other kno"
C04-1162,W98-0701,0,0.00949419,"o identify the most likely meanings for the words in a given context based on a measure of contextual overlap between the dictionary definitions of the ambiguous words, or between the current context and dictionary definitions provided for a given target word. Semantic similarity. Measures of semantic similarity computed on semantic networks (Rada et al., 1989). Depending on the size of the context they span, these measures are in turn divided into two main categories: (1) Local context – where the semantic measures are used to disambiguate words additionally connected by syntactic relations (Stetina et al., 1998). (2) Global context – where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an entire text (Morris and Hirst, 1991). Selectional preferences. Automatically or semiautomatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997). Heuristic-based methods. These methods consist of simple rules that can reliably assign a sense to certain word categories: one sense per collocation (Yarowsky, 1993), an"
C04-1162,C90-2067,0,0.0820372,"ndancy of information in these two algorithms to the point where their combination cannot improve over the individual algorithms. The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) – where threads of meaning are identified throughout a text. Lexical chains however only take into account possible relations between concepts in a static way, without considering the importance of the concepts that participate in a relation, which is recursively determined by PageRank. Another related line of work is the word sense disambiguation algorithm proposed in (Veronis and Ide, 1990), where a large neural network is built by relating words through their dictionary definitions. The Analogy. In the context of Web surfing, PageRank implements the “random surfer model”, where a user surfs the Web by following links from any given Web page. In the context of text meaning, PageRank implements the concept of text cohesion (Halliday and Hasan, 1976), where from a certain concept C in a text, we are likely to “follow” links to related concepts – that is, concepts that have a semantic relation with the current concept C. Intuitively, PageRank-style algorithms work well for finding"
C04-1162,H93-1052,0,0.107372,"etina et al., 1998). (2) Global context – where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an entire text (Morris and Hirst, 1991). Selectional preferences. Automatically or semiautomatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997). Heuristic-based methods. These methods consist of simple rules that can reliably assign a sense to certain word categories: one sense per collocation (Yarowsky, 1993), and one sense per discourse (Gale et al., 1992). In this paper, we propose a new open-text disambiguation algorithm that combines information drawn from a semantic network (WordNet) with graph-based ranking algorithms (PageRank). We compare our method with other open-text word sense disambiguation algorithms, and show that the accuracy achieved through our new PageRank-based method exceeds the performance obtained by other knowledge-based methods. 3 PageRank on Semantic Networks In this section, we briefly describe PageRank (Brin and Page, 1998), and describe the view of WordNet as a graph,"
C04-1162,J91-1002,0,\N,Missing
C10-1004,E06-2031,0,0.0482677,"Missing"
C10-1004,W03-1014,1,0.521582,"Missing"
C10-1004,D08-1014,1,0.778312,"rtment of Computer Science University of Pittsburgh wiebe@cs.pitt.edu Carmen Banea, Rada Mihalcea Department of Computer Science University of North Texas carmenbanea@my.unt.edu rada@cs.unt.edu Abstract generate more viable data. Research that benefited from this additional layering ranges from question answering (Yu and Hatzivassiloglou, 2003), to conversation summarization (Carenini et al., 2008), and text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006a). Although subjectivity tends to be preserved across languages – see the manual study in (Mihalcea et al., 2007), (Banea et al., 2008) hypothesize that subjectivity is expressed differently in various languages due to lexicalization, formal versus informal markers, etc. Based on this observation, our research seeks to answer the following questions. First, can we reliably predict sentencelevel subjectivity in languages other than English, by leveraging on a manually annotated English dataset? Second, can we improve the English subjectivity classification by expanding the feature space through the use of multilingual data? Similarly, can we also improve the classifiers in the other target languages? Finally, third, can we ben"
C10-1004,P09-1027,0,0.410042,"Missing"
C10-1004,P07-1056,0,0.0108534,"Missing"
C10-1004,P06-1134,1,0.910603,"Missing"
C10-1004,P08-1041,0,0.0443809,"Missing"
C10-1004,E06-1025,0,0.0568889,"Missing"
C10-1004,esuli-sebastiani-2006-sentiwordnet,0,0.011988,"Missing"
C10-1004,W03-1017,0,0.0904504,"Missing"
C10-1004,esuli-etal-2008-annotating,0,0.115023,"Missing"
C10-1004,P07-1123,1,0.828014,"Missing"
C10-1004,H05-1073,0,\N,Missing
C10-2074,P07-1126,0,0.0145237,"e that an image can be annotated with keywords capturing the denotative (entities or objects depicted) and connotative (semantics or ideologies interpreted) attributes in the image. For instance, a picture showing a group of athletes and a ball may also be tagged with words like “soccer,” or “sports activity.” Specifically, we use a combination of knowledge sources to model the denotative quality of a word as its picturability, and the connotative attribute as its saliency. The idea of visualness and salience as textual features for discovering named entities in an image was first pursued by (Deschacht and Moens, 2007), using data from the news domain. In contrast, we are able to perform annotation of images from unrestricted domains using content words (nouns, verbs and adjectives). In the following, we first describe three unsupervised extractive approaches for image annotation, followed by a supervised method using a re-ranking hypothesis that combines all the methods. f reqres |aj | |A| bestguessj ∈AM P res∈aj |Rj | (1if best guess = mj ) |AM | (1if best guess = mj ) |IM | Out of ten (oot) measures: P = P R= P modeP = modeR = P aj :ij ∈A aj :ij ∈I aj :ij ∈AM P aj :ij ∈IM P res∈aj |A| P |I| f reqres |Rj"
C10-2074,P08-1032,0,0.0575379,"ng image search engines. Most approaches to automatic image annotation have focused on the generation of image labels using annotation models trained with image features and human annotated keywords (Barnard and Forsyth, 2001; Jeon et al., 2003; Makadia et al., 2008; Wang et al., 2009). Instead of predicting specific words, these methods generally target the generation of semantic classes (e.g. vegetation, animal, building, places etc), which they can achieve with a reasonable amount of success. Recent work has also considered the generation of labels for real-world images (Li and Wang, 2008; Feng and Lapata, 2008). To our knowledge, we are unaware of any other work that performs extractive annotation for images from unrestricted domains through the exclusive use of textual features. 3 Dataset As the methods we propose are extractive, standard image databases with no surrounding text such as Corel (Duygulu et al., 2002) are not suitable, nor are they representative for the challenges associated with raw data from unrestricted domains. We thus create our own dataset using images randomly extracted from the Web. To avoid sparse searches, we use a list of the most frequent words in the British National Cor"
C10-2074,S07-1009,0,0.0188359,"missing or unreliable. We rely instead on human-produced extractive annotations (as described in the previous section), and formulate a new evaluation framework based on the intuition that an image can be substituted with one or more tags that convey the same meaning as the image itself. Ideally, there is a single tag that “best” describes the image overall (i.e. the gold standard tag agreed by the majority of human annotators), but there are also multiple tags that describe the fine-grained concepts present in the image. Our evaluation framework is inspired by the lexical substitution task (McCarthy and Navigli, 2007), where a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. Given this analogy, the evaluation metrics used for lexical substitution can be adapted to the evaluation of image tagging. Specifically, we measure the precision and the recall of a tagging method using four subtasks: best normal: provides precision and recall for the top-ranked tag returned by a method; best mode: provides precision and recall only if the top-ranked tag by a method matches the tag in the gold standard that was most frequently selected"
C10-2074,D08-1027,0,0.0229404,"Missing"
C10-2074,W04-3252,1,\N,Missing
C12-2108,E06-1002,1,0.655186,"duction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Mihalcea and Csomai, 2007) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to the corresponding article, by using links or piped links. Consider, for example, the following Wikipedia annotations from the article about Palermo: “Palermo is a city in [[Southern Italy]], the [[capital city|capital]] of the [[autonomous area|auto"
C12-2108,D07-1074,0,0.0295325,"e vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Mihalcea and Csomai, 2007) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to the corresponding article, by using links or piped links. Consider, for example, the following Wikipedia annotations from the article about Palermo: “Palermo is a city in [[Southern Italy]], the [[capital city|capital]] of the [[autonomous area|autonomous region]] o"
C12-2108,D09-1120,0,0.0201739,"de invatare automata pentru dezambiguizare de sensuri si referinte care pot fi antrenate in prezenta acestor ambiguitati de adnotare. Evaluarea experimentala a acestor modele confirma o imbunatatire semnificativa a performantei de dezambiguizare. KEYWORDS: dezambiguizare de sensuri, Wikipedia. Proceedings of COLING 2012: Posters, pages 1111–1120, COLING 2012, Mumbai, December 2012. 1111 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Mihalcea and Csomai, 2007) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding"
C12-2108,P08-4009,0,0.0315985,"ativa a performantei de dezambiguizare. KEYWORDS: dezambiguizare de sensuri, Wikipedia. Proceedings of COLING 2012: Posters, pages 1111–1120, COLING 2012, Mumbai, December 2012. 1111 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Mihalcea and Csomai, 2007) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to the corresponding article, by using links or pipe"
C12-2108,N07-1025,1,0.824212,"ikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Mihalcea and Csomai, 2007) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to the corresponding article, by using links or piped links. Consider, for example, the following Wikipedia annotations from the article about Palermo: “Palermo is a city in [[Southern Italy]], the [[capital city|capital]] of the [[autonomous area|autonomous region]] of [[Sicily]]”. The bracketed strings [[Souther"
C12-2108,N06-1025,0,0.0356381,"ikipedia. Prezentam modele de invatare automata pentru dezambiguizare de sensuri si referinte care pot fi antrenate in prezenta acestor ambiguitati de adnotare. Evaluarea experimentala a acestor modele confirma o imbunatatire semnificativa a performantei de dezambiguizare. KEYWORDS: dezambiguizare de sensuri, Wikipedia. Proceedings of COLING 2012: Posters, pages 1111–1120, COLING 2012, Mumbai, December 2012. 1111 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Mihalcea and Csomai, 2007) systems. Articles in Wikipedia often contain mentions of concepts or entities that alr"
C12-2108,P11-1082,0,0.0204899,"sensuri si referinte care pot fi antrenate in prezenta acestor ambiguitati de adnotare. Evaluarea experimentala a acestor modele confirma o imbunatatire semnificativa a performantei de dezambiguizare. KEYWORDS: dezambiguizare de sensuri, Wikipedia. Proceedings of COLING 2012: Posters, pages 1111–1120, COLING 2012, Mumbai, December 2012. 1111 1 Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Mihalcea and Csomai, 2007) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mentio"
C16-1065,de-marneffe-etal-2006-generating,0,0.0271647,"Missing"
C16-1065,P03-1054,0,0.0698305,"lections for the target words are added manually, to ensure correct handling of grammatical exceptions. To obtain usage examples for the two cultures for these words, we extract paragraphs from the blog posts that include the selected words with the given part-of-speech. Of these paragraphs, we discard those that contain less than ten words. We also truncate the long paragraphs so they include a maximum of 100 words to the left and right of the target word, disregarding sentence boundaries. The contexts of the target words are then parsed to get the dependency tags related to the target word (Klein and Manning, 2003). We also explicitly balance the data across time. Noting there could be cases where the number of blog posts published in a specific year is higher compared to that in other years due to certain events (e.g., an Olympiad, or a major weather related event), we draw samples for our dataset from several different time periods. Specifically, for each culture, we consider an equal number of instances from four different years (2011-2014). Table 2 shows the per-word average number of data instances obtained in this way for each part-of-speech for each culture for the years 2011-2014. Note that we d"
C16-1065,D09-1146,0,0.148936,"he emotional expressiveness among the northerners and southerners in their own countries, to test Montesquieu’s geography hypothesis (De Secondat and others, 1748). More recently, the findings of Boroditsky et al. (2003) indicate that people’s perception of certain inanimate objects (such as bridge, key, violin, etc.) is influenced by the grammatical genders assigned to these objects in their native languages. To our knowledge, there is only limited work in computational linguistics that explored cross-cultural differences through language analysis. Our work is most closely related to that by Paul and Girju (2009), in which they identify cultural differences in people’s experiences in various countries from the perspective of tourists and locals. Specifically, they analyzed forums and blogs written by tourists and locals about their experiences in three countries, namely Singapore, India, and United Kingdom, using an extension of LDA. One of their findings is that while topic modeling on tourist forums offered an unsupervised aggregation of factual data specific to each country that would be important to travelers (such as destination’s climate, law, and language), topic modeling on blogs authored by l"
C16-1065,strapparava-valitutti-2004-wordnet,0,0.049515,"Missing"
C16-1065,N03-1033,0,0.0445549,"he noise that would otherwise be introduced by machine translation; and (2) they have a significant number of blogs contributed in recent years, which we can use to collect a large number of occurrences for a large set of words. We obtain a large corpus of blog posts by crawling the blogger profiles and posts from Google Blogger. For each profile, we consider up to a maximum of 20 blogs, and for each blog, we consider up to 500 posts. Table 1 gives statistics of the data collected in this process. We process the blog posts by removing the HTML tags and tagging them with part-of-speech labels (Toutanova et al., 2003). Country Australia United States Profiles 469 374 Blogs 1129 1267 Posts 320316 471257 Table 1: Blog statistics for the two target cultures. Next, we create our pool of candidate target words by identifying the top 1, 500 content words based on their frequency in the blog posts, additionally placing a constraint that they cover all open-class parts-ofspeech: of the 1, 500 words, 500 are nouns, 500 verbs, 250 adjectives, and 250 adverbs. These numbers are chosen based on the number of examples that exist for the target words; e.g., most (> 490) of the 500 selected nouns have more than 300 examp"
C16-1065,H05-2018,0,0.0475164,"s for a word to be selected as a contextual feature. Contextual features express the overall intention of the blogger while writing about the target word. Socio-linguistic features. These features include (1) fractions of words that fall under each of the 70 Linguistic Inquiry and Word Count (LIWC) categories (Pennebaker et al., 2001); the 2001 version of LIWC includes about 2,200 words and word stems grouped into broad categories relevant to psychological processes (e.g., emotion, cognition); (2) fractions of words belonging to each of the five fine-grained polarity classes in OpinionFinder (Wilson et al., 2005), namely strongly negative, weakly negative, neutral, weakly positive, and strongly positive; (3) fractions of words belonging to each of five Morality classes (Ignatow and Mihalcea, 2012), i.e., authority, care, fairness, ingroup, sanctity; and (4) fractions of words belonging to each of the six Wordnet Affect classes (Strapparava et al., 2004), namely anger, disgust, fear, joy, sadness, and surprise. These features provide social and psychological insights into the perceptions bloggers have about the words they use. Syntactic features. These features consist of parser dependencies (De Marnef"
C16-1233,P05-1045,0,0.0071426,"Missing"
C16-1233,S15-2128,0,0.0245558,"negation terms, punctuation, point-wise mutual information scores, part of speech tags, and other features (Li et al., 2013; Zhang and Lan, 2015). The results presented were marked as either constrained or unconstrained systems. Unconstrained systems were allowed to use data outside the training data provided, while constrained systems could not. The top two scoring models were unconstrained but the top scoring constrained system used Brown clusters in addition to other features. These are counts of how many words in the sentence belong to semantic clusters of words derived in previous work (Hamdan et al., 2015). Other entries used similar features with several entries using SVM models and a single entry that relied on an unsupervised model. 3 Data As we are not aware of any dataset consisting of statements describing courses and instructors, and the sentiment that the writers (students) have toward them, we collected our own dataset. We extracted sentences from a Facebook student group where students describe their experience with classes in the Computer Science department at the University of Michigan, as well as from a survey run with students in the same department. The final data set consists of"
C16-1233,P11-1016,0,0.0358689,"is the task of taking a query opinion holder and orientation and returning the set of entities that satisfy this condition. In terms of the quintuple we use to represent sentiment, these are related tasks because although slot filling and temporal slot filling are not exactly sentiment tasks, they are concerned with the entity, aspect, and time values. The sentiment slot filling task is concerned with the entity, orientation, and opinion holder. In the task of target dependent sentiment analysis, the goal is to take a query entity and find the sentiment toward this entity in a set of tweets (Jiang et al., 2011). This is usually done with a small set of entities where the corpus is constructed by querying Twitter for tweets that contain the substring matching the entity. Jiang et al. perform this task in three steps. They first identify whether subjectivity exists, then the polarity of the sentiment toward the target, and then use a graph based method to improve classification accuracy using retweets, i.e., tweets from the same users that mention the same entities. In our task, we use a larger set of entities that have many ways of being mentioned; this makes the entity identification part of the tas"
C16-1233,D13-1171,0,0.0324576,"Missing"
C16-1233,pak-paroubek-2010-twitter,0,0.0611861,"task. 1 Introduction Sentiment analysis is the computational study of people’s opinions or emotions; it is a challenging problem that is increasingly being used for decision making by individuals and organizations (Pang and Lee, 2008). There is a significant body of research on sentiment analysis, addressing entire documents (Agarwal and Bhattacharyya, 2005), including blogs (Godbole et al., 2007; Annett and Kondrak, 2008) and reviews (Yi et al., 2003; Cabral and Hortacsu, 2010); sentences (Yu and Hatzivassiloglou, 2003; Nigam and Hurst, 2004) or otherwise short spans of texts such as tweets (Pak and Paroubek, 2010; Kouloumpis et al., 2011); and phrases (Wilson et al., 2005; Turney, 2002). More recent work has also addressed the task of aspect sentiment (Pontiki et al., 2015; Thet et al., 2010; Lakkaraju et al., 2014), which aims to address the sentiment toward attributes of the target entity, such as the service in a restaurant (Sauper and Barzilay, 2013), or the camera of a mobile phone (Chamlertwat et al., 2012). In this paper we address the task of targeted sentiment, defined as the task of identifying the sentiment (positive, negative) or lack thereof (neutral) that a writer holds toward entities m"
C16-1233,S14-2004,0,0.0342708,"ntify whether subjectivity exists, then the polarity of the sentiment toward the target, and then use a graph based method to improve classification accuracy using retweets, i.e., tweets from the same users that mention the same entities. In our task, we use a larger set of entities that have many ways of being mentioned; this makes the entity identification part of the task more difficult. We also do not have a social network structure to leverage 2472 to improve performance. Aspect-based sentiment analysis has been the focus of recent SemEval tasks as well as a TAC task (Ellis et al., 2014; Pontiki et al., 2014; Pontiki et al., 2015). The 2014 sentiment task was continued in 2015, and again in 2016. Researchers submitted a variety of models to evaluate the sentiment of aspects on sets of reviews for laptops, restaurants, and hotels. The highest scoring systems in the SemEval 2015 Task 12 used maximum entropy and support vector machine (SVM) models with bag of words (BoW), verb and adjective lemmas, bigrams after verbs, negation terms, punctuation, point-wise mutual information scores, part of speech tags, and other features (Li et al., 2013; Zhang and Lan, 2015). The results presented were marked as"
C16-1233,S15-2082,0,0.0605711,"Missing"
C16-1233,W03-1014,0,0.126888,"en that word and the target entity. For instance, for the example shown in Figure 1, the distance between “awful” and the target entity “203” is six, while the distance between “good” and “203” is eight. Figure 1: Example sentence, “I thought that CS 203 was going to be good, but it was awful”, showing the parse tree weighting for counts using the number of node hops between a given word and the target entity. Weighted sentiment lexicons. We also implement a feature based on the presence/absence of words from two sentiment lexicons: Bing Liu’s lexicon (Hu and Liu, 2004), and the MPQA lexicon (Riloff and Wiebe, 2003) (Wilson et al., 2005). These are two of the most commonly used lexicons in recent sentiment work, and contain 6,789 and 8,222 words respectively, labeled as positive, negative, or neutral. For each word in the utterance, we now generate four features: one simply reflecting the weight of the word (calculated as described before, as a distance to the target entity), and the other three reflecting 2476 whether the word appears as a positive, negative, or neutral word in any of the lexicons; these three latter features are again represented as weighted distance scores. We use an SVM classifier, w"
C16-1233,D13-1170,0,0.018302,"Missing"
C16-1233,P02-1053,0,0.0109923,"ons or emotions; it is a challenging problem that is increasingly being used for decision making by individuals and organizations (Pang and Lee, 2008). There is a significant body of research on sentiment analysis, addressing entire documents (Agarwal and Bhattacharyya, 2005), including blogs (Godbole et al., 2007; Annett and Kondrak, 2008) and reviews (Yi et al., 2003; Cabral and Hortacsu, 2010); sentences (Yu and Hatzivassiloglou, 2003; Nigam and Hurst, 2004) or otherwise short spans of texts such as tweets (Pak and Paroubek, 2010; Kouloumpis et al., 2011); and phrases (Wilson et al., 2005; Turney, 2002). More recent work has also addressed the task of aspect sentiment (Pontiki et al., 2015; Thet et al., 2010; Lakkaraju et al., 2014), which aims to address the sentiment toward attributes of the target entity, such as the service in a restaurant (Sauper and Barzilay, 2013), or the camera of a mobile phone (Chamlertwat et al., 2012). In this paper we address the task of targeted sentiment, defined as the task of identifying the sentiment (positive, negative) or lack thereof (neutral) that a writer holds toward entities mentioned in a statement. Targeted sentiment has been only recently introduc"
C16-1233,H05-1044,0,0.58859,"udy of people’s opinions or emotions; it is a challenging problem that is increasingly being used for decision making by individuals and organizations (Pang and Lee, 2008). There is a significant body of research on sentiment analysis, addressing entire documents (Agarwal and Bhattacharyya, 2005), including blogs (Godbole et al., 2007; Annett and Kondrak, 2008) and reviews (Yi et al., 2003; Cabral and Hortacsu, 2010); sentences (Yu and Hatzivassiloglou, 2003; Nigam and Hurst, 2004) or otherwise short spans of texts such as tweets (Pak and Paroubek, 2010; Kouloumpis et al., 2011); and phrases (Wilson et al., 2005; Turney, 2002). More recent work has also addressed the task of aspect sentiment (Pontiki et al., 2015; Thet et al., 2010; Lakkaraju et al., 2014), which aims to address the sentiment toward attributes of the target entity, such as the service in a restaurant (Sauper and Barzilay, 2013), or the camera of a mobile phone (Chamlertwat et al., 2012). In this paper we address the task of targeted sentiment, defined as the task of identifying the sentiment (positive, negative) or lack thereof (neutral) that a writer holds toward entities mentioned in a statement. Targeted sentiment has been only re"
C16-1233,W03-1017,0,0.172156,"Through several comparative evaluations, we show that our system outperforms previous work on a similar task. 1 Introduction Sentiment analysis is the computational study of people’s opinions or emotions; it is a challenging problem that is increasingly being used for decision making by individuals and organizations (Pang and Lee, 2008). There is a significant body of research on sentiment analysis, addressing entire documents (Agarwal and Bhattacharyya, 2005), including blogs (Godbole et al., 2007; Annett and Kondrak, 2008) and reviews (Yi et al., 2003; Cabral and Hortacsu, 2010); sentences (Yu and Hatzivassiloglou, 2003; Nigam and Hurst, 2004) or otherwise short spans of texts such as tweets (Pak and Paroubek, 2010; Kouloumpis et al., 2011); and phrases (Wilson et al., 2005; Turney, 2002). More recent work has also addressed the task of aspect sentiment (Pontiki et al., 2015; Thet et al., 2010; Lakkaraju et al., 2014), which aims to address the sentiment toward attributes of the target entity, such as the service in a restaurant (Sauper and Barzilay, 2013), or the camera of a mobile phone (Chamlertwat et al., 2012). In this paper we address the task of targeted sentiment, defined as the task of identifying t"
C16-1233,S15-2125,0,0.179275,"ll as a TAC task (Ellis et al., 2014; Pontiki et al., 2014; Pontiki et al., 2015). The 2014 sentiment task was continued in 2015, and again in 2016. Researchers submitted a variety of models to evaluate the sentiment of aspects on sets of reviews for laptops, restaurants, and hotels. The highest scoring systems in the SemEval 2015 Task 12 used maximum entropy and support vector machine (SVM) models with bag of words (BoW), verb and adjective lemmas, bigrams after verbs, negation terms, punctuation, point-wise mutual information scores, part of speech tags, and other features (Li et al., 2013; Zhang and Lan, 2015). The results presented were marked as either constrained or unconstrained systems. Unconstrained systems were allowed to use data outside the training data provided, while constrained systems could not. The top two scoring models were unconstrained but the top scoring constrained system used Brown clusters in addition to other features. These are counts of how many words in the sentence belong to semantic clusters of words derived in previous work (Hamdan et al., 2015). Other entries used similar features with several entries using SVM models and a single entry that relied on an unsupervised"
C16-1233,D15-1073,0,0.328277,"entiment toward attributes of the target entity, such as the service in a restaurant (Sauper and Barzilay, 2013), or the camera of a mobile phone (Chamlertwat et al., 2012). In this paper we address the task of targeted sentiment, defined as the task of identifying the sentiment (positive, negative) or lack thereof (neutral) that a writer holds toward entities mentioned in a statement. Targeted sentiment has been only recently introduced as a task, to our knowledge with contributions from only two research groups that focused primarily on settings with scarce resources (Mitchell et al., 2013; Zhang et al., 2015). While previous work on data sets such as product reviews can give an accurate measure of sentiment toward products (as explicit targets of the opinions being expressed in the reviews), some corpora include additional challenges. Targeted sentiment addresses the challenge of identifying entities in running text (e.g., Twitter, student comments), and attributing separate sentiment to each mentioned entity. In our work, we focus on an application-driven task, namely that of understanding students’ sentiment towards courses and instructors as expressed in their comments. As an example, consider"
C18-1156,K16-1017,0,0.690786,"storical posts of users to understand sarcastic tendencies (Rajadesingan et al., 2015; Zhang et al., 2016). Khattri et al. (2015) try to discover users’ sentiments towards entities in their histories to find contrasting evidence. Wallace et al. (2015) utilize sentiments and noun phrases used within a forum to gather context typical to that forum. Such forum-based modeling simulates user 1838 communities. Our work follows a similar motivation as we explore the context provided by user profiling and the topical knowledge embedded in the discourse of comments in discussion forums (subreddits2 ). Amir et al. (2016) performed user modeling by learning embeddings that capture homophily. This work is the closest to our approach given the fact that we too learn user embeddings to acquire context. However, we take a different approach that involves stylometric and personality description of the users. Empirical evidence shows that these proposed features are better than previous user modeling approaches. Moreover, we learn discourse features which has not been explored before in the context of this task. 3 Method 3.1 Task Definition The task involves detection of sarcasm for comments made in online discussio"
C18-1156,P16-2003,0,0.0279175,"lity traits (Matthews et al., 2003). We use sigmoid instead of softmax to facilitate multi-label classification. This is calculated as: 1 ) q = α( W1 o + b 2 ) y ˆ = σ( W2 q + b (6) (7) 1 ∈ Rdp and b 2 ∈ R5 are parameters and α(.) represents non-linear activation. W1 ∈ Rdp ×3M , W2 ∈ R5×dp , b 3.4.3 Fusion We take a multi-view learning approach to combine both stylometric and personality features into a comprehensive embedding for each user. We use CCA to perform this fusion. CCA captures maximal information between two views and creates a combined representation (Hardoon et al., 2004; Benton et al., 2016). In the event of having more than two views, fusion can be performed using an extension of CCA called Generalized CCA (see Appendix). Canonical Correlation Analysis: Let us consider the learnt stylometric embedding matrix D ∈ Rds ×Nu and personality embedding matrix P ∈ Rdp ×Nu containing the respective embedding vectors of user ui in their ith columns. The matrices are then mean-centered and standardized across all user columns. We call these new matrices as X1 and X2 , respectively. Let the correlation matrix for X1 be R11 = X1 X1 T ∈ Rds ×ds , for X2 be R22 = X2 X2 T ∈ Rdp ×dp and the cros"
C18-1156,W10-2914,0,0.427949,"ntent- and context-based sarcasm detection models. Content-based models: These networks model the problem of sarcasm detection as a standard classification task and try to find lexical and pragmatic indicators to identify sarcasm. Numerous works have taken this path and presented innovative ways to unearth interesting cues for sarcasm. Tepperman et al. (2006) investigate sarcasm detection in spoken dialogue systems using prosodic and spectral cues. Carvalho et al. (2009) use linguistic features like positive predicates, interjections and gestural clues such as emoticons, quotation marks, etc. Davidov et al. (2010), Tsur et al. (2010) use syntactic patterns to construct classifiers. Gonz´alez-Ib´anez et al. (2011) also study the use of emoticons, mainly amongst tweets. Riloff et al. (2013) assert sarcasm to be a contrast to positive sentiment words and negative situations. Joshi et al. (2015) use multiple features comprising lexical, pragmatics, implicit and explicit context incongruity. In the explicit case, they include relevant features to detect thwarted sentimental expectations in the sentence. For implicit incongruity, they generalize Riloff et al. (2013) by identifying verb-noun phrases containin"
C18-1156,P11-2102,0,0.63631,"Missing"
C18-1156,P15-2124,0,0.183041,"vative ways to unearth interesting cues for sarcasm. Tepperman et al. (2006) investigate sarcasm detection in spoken dialogue systems using prosodic and spectral cues. Carvalho et al. (2009) use linguistic features like positive predicates, interjections and gestural clues such as emoticons, quotation marks, etc. Davidov et al. (2010), Tsur et al. (2010) use syntactic patterns to construct classifiers. Gonz´alez-Ib´anez et al. (2011) also study the use of emoticons, mainly amongst tweets. Riloff et al. (2013) assert sarcasm to be a contrast to positive sentiment words and negative situations. Joshi et al. (2015) use multiple features comprising lexical, pragmatics, implicit and explicit context incongruity. In the explicit case, they include relevant features to detect thwarted sentimental expectations in the sentence. For implicit incongruity, they generalize Riloff et al. (2013) by identifying verb-noun phrases containing contrast in both polarities. Context-based models: The usage of contextual sarcasm has increased in recent years, especially in online platforms. Texts found in microblogs, discussion forums, and social media are plagued by grammatical inaccuracies and contain information which is"
C18-1156,W15-2905,0,0.0429417,"y inefficient and the need arises for additional clues (Carvalho et al., 2009). Wallace et al. (2014) demonstrate this need by showing how traditional classifiers fail in instances where humans require additional context. They also indicate the importance of speaker and topical information associated to a text to gather such context. Poria et al. (2016) use additional information by sentiment, emotional and personality representations of the input text. Previous works have mainly used historical posts of users to understand sarcastic tendencies (Rajadesingan et al., 2015; Zhang et al., 2016). Khattri et al. (2015) try to discover users’ sentiments towards entities in their histories to find contrasting evidence. Wallace et al. (2015) utilize sentiments and noun phrases used within a forum to gather context typical to that forum. Such forum-based modeling simulates user 1838 communities. Our work follows a similar motivation as we explore the context provided by user profiling and the topical knowledge embedded in the discourse of comments in discussion forums (subreddits2 ). Amir et al. (2016) performed user modeling by learning embeddings that capture homophily. This work is the closest to our approac"
C18-1156,D14-1181,0,0.0048422,"ch we call as the personality vector pjui . The expectation over the personality vectors for all vi -comments made by the user is then defined as the overall personality feature vector pi of user ui : pi = Ej∈[vi ] [pjui ] = 1 vi j ∑ p vi j=1 ui (4) CNN: Here, we describe the CNN that generates the personality vectors. Given a user’s comment, which is a text S = [w1 , ..., wn ] composed of n words, each word wi is represented as a word embedding  i ∈ Rdem using the pre-trained FastText embeddings (Bojanowski et al., 2016). A single-layered CNN is w then modeled on this input sequence S (Kim, 2014). First, a convolutional layer is applied having three filters F[1,2,3] ∈ Rdem ×h[1,2,3] of heights h[1,2,3] , respectively. For each k ∈ {1, 2, 3}, filter Fk slides across  k of size R∣S∣−hk +1 , S and extracts hk -gram features at each instance. This creates a feature map vector m whose each entry mk,j is obtained as: mk,j = α( Fk ⋅ S[j∶j+hk −1] + bk ) (5) here, bk ∈ R is the bias and α(⋅) is a non-linear activation function. M feature maps are created from each filter Fk giving a total of 3M feature maps as output. Following this, a max-pooling operation is performed across the length of e"
C18-1156,W07-0101,0,0.145315,"on, CASCADE utilizes user embeddings that encode stylometric and personality features of users. When used along with content-based feature extractors such as convolutional neural networks, we see a significant boost in the classification performance on a large Reddit corpus. 1 Introduction Sarcasm is a linguistic tool that uses irony to express contempt. Its figurative nature poses a great challenge for affective systems performing sentiment analysis (Cambria et al., 2017). Previous research in automated sarcasm detection has primarily focused on lexical and pragmatic cues found in sentences (Kreuz and Caucci, 2007). In the literature, interjections, punctuations, and sentimental shifts have been considered as major indicators of sarcasm (Joshi et al., 2017). When such lexical cues are present in sentences, sarcasm detection can achieve high accuracy. However, sarcasm is also expressed implicitly, i.e., without the presence of such lexical cues. This use of sarcasm also relies on context, which involves the presumption of commonsense and background knowledge of an event. When it comes to detecting sarcasm in a discussion forum, it may not only be required to understand the context of previous comments bu"
C18-1156,C16-1151,1,0.955619,"y. However, sarcasm is also expressed implicitly, i.e., without the presence of such lexical cues. This use of sarcasm also relies on context, which involves the presumption of commonsense and background knowledge of an event. When it comes to detecting sarcasm in a discussion forum, it may not only be required to understand the context of previous comments but also the necessary background knowledge about the topic of discussion. The usage of slangs and informal language also diminishes the reliance on lexical cues (Satapathy et al., 2017). This particular type of sarcasm is tough to detect (Poria et al., 2016). Contextual dependencies for sarcasm can take many forms. As an example, a sarcastic post from Reddit1 , “I’m sure Hillary would’ve done that, lmao.” requires background knowledge about the event, i.e., Hillary Clinton’s action at the time the post was made. Similarly, sarcastic posts like “But atheism, yeah *that’s* a religion!” requires the knowledge that topics like atheism often contain argumentative discussions and, hence, they are more prone towards sarcasm. The main aim of this work is sarcasm detection in online discussion forums. In particular, we propose a hybrid network, named CASC"
C18-1156,D13-1066,0,0.786793,"Missing"
C18-1156,P14-2084,0,0.110248,"al expectations in the sentence. For implicit incongruity, they generalize Riloff et al. (2013) by identifying verb-noun phrases containing contrast in both polarities. Context-based models: The usage of contextual sarcasm has increased in recent years, especially in online platforms. Texts found in microblogs, discussion forums, and social media are plagued by grammatical inaccuracies and contain information which is highly temporal and contextual. In such scenarios, mining linguistic information becomes relatively inefficient and the need arises for additional clues (Carvalho et al., 2009). Wallace et al. (2014) demonstrate this need by showing how traditional classifiers fail in instances where humans require additional context. They also indicate the importance of speaker and topical information associated to a text to gather such context. Poria et al. (2016) use additional information by sentiment, emotional and personality representations of the input text. Previous works have mainly used historical posts of users to understand sarcastic tendencies (Rajadesingan et al., 2015; Zhang et al., 2016). Khattri et al. (2015) try to discover users’ sentiments towards entities in their histories to find c"
C18-1156,P15-1100,0,0.184859,"Missing"
C18-1156,C16-1231,0,0.687339,"ion becomes relatively inefficient and the need arises for additional clues (Carvalho et al., 2009). Wallace et al. (2014) demonstrate this need by showing how traditional classifiers fail in instances where humans require additional context. They also indicate the importance of speaker and topical information associated to a text to gather such context. Poria et al. (2016) use additional information by sentiment, emotional and personality representations of the input text. Previous works have mainly used historical posts of users to understand sarcastic tendencies (Rajadesingan et al., 2015; Zhang et al., 2016). Khattri et al. (2015) try to discover users’ sentiments towards entities in their histories to find contrasting evidence. Wallace et al. (2015) utilize sentiments and noun phrases used within a forum to gather context typical to that forum. Such forum-based modeling simulates user 1838 communities. Our work follows a similar motivation as we explore the context provided by user profiling and the topical knowledge embedded in the discourse of comments in discussion forums (subreddits2 ). Amir et al. (2016) performed user modeling by learning embeddings that capture homophily. This work is the"
C18-1287,P12-2034,0,0.0603881,"utomatic Readability Index (ARI). Syntax. Finally, we extract a set of features derived from production rules based on context free grammars (CFG) trees using the Stanford Parser (Klein and Manning, 2003). The CFG derived features consist of all the lexicalized production rules (rules including child nodes) combined with their parent and grandparent node, e.g., *NNˆNP→commission (in this example NN –a noun– is the grandparent node, NP –noun phrase– the parent node, and “commissions” the child node). CFG-based features have been previously shown to be useful for linguistic deception detection (Feng et al., 2012). Features in this set are also encoded as tf-idf values. 5 Automatic Fake News Detection We conduct several experiments with different combinations of feature sets to explore their predictive separately and jointly. We use a linear SVM classifier and conduct our evaluations using five-fold crossvalidation, with accuracy, precision, recall, and F-score as performance metrics. We use the machine learning algorithms implementation available in the caret (Kuhn et al., 2016) and e1071 packages (Meyer et al., 2015) with their default parameters. Tables 4 and 5 show the results obtained for the diff"
C18-1287,P03-1054,0,0.0518351,"LIWC categories (including punctuation).3 Readability. We also extract features that indicate text understandability. These include content features such as the number of characters, complex words, long words, number of syllables, word types, and number of paragraphs, among others content features. We also calculate several readability metrics, including the Flesch-Kincaid, Flesch Reading Ease, Gunning Fog, and the Automatic Readability Index (ARI). Syntax. Finally, we extract a set of features derived from production rules based on context free grammars (CFG) trees using the Stanford Parser (Klein and Manning, 2003). The CFG derived features consist of all the lexicalized production rules (rules including child nodes) combined with their parent and grandparent node, e.g., *NNˆNP→commission (in this example NN –a noun– is the grandparent node, NP –noun phrase– the parent node, and “commissions” the child node). CFG-based features have been previously shown to be useful for linguistic deception detection (Feng et al., 2012). Features in this set are also encoded as tf-idf values. 5 Automatic Fake News Detection We conduct several experiments with different combinations of feature sets to explore their pred"
C18-1287,H05-1067,1,0.709174,"Missing"
C18-1287,P11-1032,0,0.284676,"or instance FakeCheck.org and Snopes.com. However, this is not a straightforward task, as external sources might not be available, particularly for just-published news items. Therefore, the fact-checking approach is predominantly useful for the detection of deception in texts for which external, verifiable information is available. Furthermore, also related to the current paper is work on the automatic identification of deceptive content, which has explored domains such as forums, consumer reviews websites, online advertising, online dating, and crowdfunding platforms (Warkentin et al., 2010; Ott et al., 2011a; Zhang and Guan, 2008; Toma and Hancock, 2010; Shafqat et al., 2016). While fake news detection is closely related to deception detection (i.e. determining whether or not someone is lying), there are important differences between the two tasks. First, fake news producers usually seek political or financial gain as well as selfpromotion while deceivers have motivations that are more socially driven such as self protection, conflict or harm avoidance, impression management or identity concealment. Second, they differ significantly in their target and in the form they propagate: fake news items"
C18-1287,N13-1053,0,0.0149393,"n marks and exclamation marks. Psycholinguistic features. We use the LIWC lexicon to extract the proportions of words that fall into psycholinguistic categories. LIWC is based on large lexicons of word categories that represent psycholinguistic processes (e.g., positive emotions, perceptual processes), summary categories (e.g., words per sentence), as well as part-of-speech categories (e.g., articles, verbs). Previous work on verbal deception detection showed that LIWC is a valuable tool for the deception detection in various contexts (e.g., genuine and fake hotel reviews, (Ott et al., 2011b; Ott et al., 2013); prisoners’ lies (Bond and Lee, 2005)). In our work, we cluster the single LIWC categories into the following feature sets: summary categories (e.g., analytical thinking, emotional tone), linguistic processes (e.g., function words, pronouns), and psychological processes (e.g., affective processes, social processes). We also test a combined feature set of all the LIWC categories (including punctuation).3 Readability. We also extract features that indicate text understandability. These include content features such as the number of characters, complex words, long words, number of syllables, wor"
C18-1287,P14-2072,1,0.886316,"Missing"
C18-1287,D15-1133,1,0.884998,"Missing"
C18-1287,W16-0802,0,0.341701,"fake news content (Conroy et al., 2015). The linguistic approach attempts to identify text properties, such as writing style and content, that can help to discriminate real from fake news articles. The underlying assumption for this approach is that linguistic behaviors such as punctuation usage, word type choices, part-of-speech tags, and emotional valence of a text are rather involuntary and therefore outside of the author’s control, thus revealing important insights into the nature of the text. The linguistic approach has yielded promising results in differentiating satire from real news (Rubin et al., 2016). Relying in a corpus of satire news (from The Onion and The Beaverton) and real news (The Toronto Star and The New York Times) in four domains (civics, science, business, soft news), the authors explored the use of several linguistic features to discriminate between real and satirical news content. The best classification performances were achieved with feature sets representing absurdity, punctuation, and grammar. On the other hand, fact-checking approaches rely on automated verification of propositions made in the news articles (e.g., ”Barack Obama assumed office on a Tuesday”) to assess th"
C18-1287,N18-1074,0,0.0982553,"Missing"
C18-1287,P17-2067,0,0.416913,"Missing"
D07-1040,W04-3247,0,0.352095,"eight of the words in sentences, as well as the sentence position inside a document. These techniques have been successfully implemented in the centroid approach (Radev et al., 2004), which extends the idea of tf.idf weighting (Salton and Buckley, 1997) by introducing word centroids, as well as integrating other features such as position, first-sentence overlap and sentence length. More recently, graph-based methods that rely on sentence connectivity have also been found successful, using algorithms such as node degree (Salton et al., 1997) or eigenvector centrality (Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wolf and Gibson, 2004). In addition to unsupervised methods, supervised machine learning techniques have also been used with considerable success. Assuming the avail381 ability of a collection of documents and their corresponding manually constructed summaries, these methods attempt to identify the key properties of a good summary, such as the presence of named entities, positional scores, or the location of key phrases. Such supervised techniques have been successfully used in the systems proposed by e.g. (Teufel and Moens, 1997; Hirao et al., 2002; Zhou and Hovy, 2003; D’Avanzo and Magnini"
D07-1040,W97-0704,0,0.038689,"Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 380–389, Prague, June 2007. 2007 Association for Computational Linguistics nity, ever since the early approaches to automatic abstraction that laid the foundations of the current text summarization techniques (Luhn, 1958; Edmunson, 1969). The literature typically distinguishes between extraction, concerned with the identification of the information that is important in the input text; and abstraction, which involves a generation step to add fluency to a previously compressed text (Hovy and Lin, 1997). Most of the efforts to date have been concentrated on the extraction step, which is perhaps the most critical component of a successful summarization algorithm, and this is the focus of our current work as well. To our knowledge, no research work to date was specifically concerned with the automatic summarization of books. There is, however, a large and growing body of work concerned with the summarization of short documents, with evaluations typically focusing on news articles. In particular, a significant number of summarization systems have been proposed during the recent Document Underst"
D07-1040,W06-0702,0,0.0186038,"n the systems proposed by e.g. (Teufel and Moens, 1997; Hirao et al., 2002; Zhou and Hovy, 2003; D’Avanzo and Magnini, 2005). In addition to short news documents, which have been the focus of most of the summarization systems proposed to date, work has been also carried out on the summarization of other types of documents. This includes systems addressing the summarization of email threads (Wan and McKeown, 2004), online discussions (Zhou and Hovy, 2005), spoken dialogue (Galley, 2006), product reviews (Hu and Liu, 2004), movie reviews (Zhuang et al., 2006), or short literary fiction stories (Kazantseva and Szpakowicz, 2006). As mentioned before, we are not aware of any work addressing the task of automatic book summarization. 3 A Data Set for the Evaluation of Book Summarization A first challenge we encountered when we started working on the task of book summarization was the lack of a suitable data set, designed specifically for the evaluation of summaries of long documents. Unlike the summarization of short documents, which benefits from the data sets made available through the annual DUC evaluations, we are not aware of any publicly available data sets that can be used for the evaluation of methods for book s"
D07-1040,N03-1020,0,0.115341,"Missing"
D07-1040,P06-1004,0,0.0224753,"attempts to determine the topic shifts, and correspondingly splits the document into smaller segments. Note that although chapter boundaries are available in some of the books in our data set, this is not always the case as there are also books for which the chapters are not explicitly identified. To ensure an uniform treatment of the entire data set, we decided not to use chapter boundaries, and instead apply an automatic text segmentation algorithm. While several text segmentation systems have been proposed to date, we decided to use a graphbased segmentation algorithm using normalizedcuts (Malioutov and Barzilay, 2006), shown to exceed the performance of alternative segmentation methods. Briefly, the segmentation algorithm starts by modeling the text as a graph, where sentences are represented as nodes in the graph, and intersentential similarities are used to draw weighted edges. The similarity between sentences is calculated using cosine similarity, with a smoothing factor that adds the counts of the words in the neighbor sentences. Words are weighted using an adaptation of the tf.idf metric, where a document is uniformly split into chunks that are used for the tf.idf computation. There are two parameters"
D07-1040,W01-0100,0,0.159945,"the following section, we explore techniques for improving the quality of the generated summaries by accounting for the length of the documents. 5 Techniques for Book Summarization We decided to make several changes to our initial system, in order to account for the specifics of the data set we work with. In particular, our data set consists of very large documents, and correspondingly the summarization of such documents requires techniques that account for their length. 384 5.1 Sentence Position In Very Large Documents The general belief in the text summarization literature (Edmunson, 1969; Mani, 2001) is that the position of sentences in a text represents one of the most important sources of information for a summarization system. In fact, a summary constructed using the lead sentences was often found to be a competitive baseline, with only few systems exceeding this baseline during the recent DUC summarization evaluations. Although the position of sentences in a document seems like a pertinent heuristic for the summarization of short documents, and in particular for the newswire genre as used in the DUC evaluations, our hypothesis is that this heuristic may not hold for the summarization"
D07-1040,C04-1079,0,0.011856,"attempt to identify the key properties of a good summary, such as the presence of named entities, positional scores, or the location of key phrases. Such supervised techniques have been successfully used in the systems proposed by e.g. (Teufel and Moens, 1997; Hirao et al., 2002; Zhou and Hovy, 2003; D’Avanzo and Magnini, 2005). In addition to short news documents, which have been the focus of most of the summarization systems proposed to date, work has been also carried out on the summarization of other types of documents. This includes systems addressing the summarization of email threads (Wan and McKeown, 2004), online discussions (Zhou and Hovy, 2005), spoken dialogue (Galley, 2006), product reviews (Hu and Liu, 2004), movie reviews (Zhuang et al., 2006), or short literary fiction stories (Kazantseva and Szpakowicz, 2006). As mentioned before, we are not aware of any work addressing the task of automatic book summarization. 3 A Data Set for the Evaluation of Book Summarization A first challenge we encountered when we started working on the task of book summarization was the lack of a suitable data set, designed specifically for the evaluation of summaries of long documents. Unlike the summarization"
D07-1040,W04-1004,0,0.017094,"entences, as well as the sentence position inside a document. These techniques have been successfully implemented in the centroid approach (Radev et al., 2004), which extends the idea of tf.idf weighting (Salton and Buckley, 1997) by introducing word centroids, as well as integrating other features such as position, first-sentence overlap and sentence length. More recently, graph-based methods that rely on sentence connectivity have also been found successful, using algorithms such as node degree (Salton et al., 1997) or eigenvector centrality (Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wolf and Gibson, 2004). In addition to unsupervised methods, supervised machine learning techniques have also been used with considerable success. Assuming the avail381 ability of a collection of documents and their corresponding manually constructed summaries, these methods attempt to identify the key properties of a good summary, such as the presence of named entities, positional scores, or the location of key phrases. Such supervised techniques have been successfully used in the systems proposed by e.g. (Teufel and Moens, 1997; Hirao et al., 2002; Zhou and Hovy, 2003; D’Avanzo and Magnini, 2005). In addition to"
D07-1040,N03-1037,0,0.0222397,"ea and Tarau, 2004; Erkan and Radev, 2004; Wolf and Gibson, 2004). In addition to unsupervised methods, supervised machine learning techniques have also been used with considerable success. Assuming the avail381 ability of a collection of documents and their corresponding manually constructed summaries, these methods attempt to identify the key properties of a good summary, such as the presence of named entities, positional scores, or the location of key phrases. Such supervised techniques have been successfully used in the systems proposed by e.g. (Teufel and Moens, 1997; Hirao et al., 2002; Zhou and Hovy, 2003; D’Avanzo and Magnini, 2005). In addition to short news documents, which have been the focus of most of the summarization systems proposed to date, work has been also carried out on the summarization of other types of documents. This includes systems addressing the summarization of email threads (Wan and McKeown, 2004), online discussions (Zhou and Hovy, 2005), spoken dialogue (Galley, 2006), product reviews (Hu and Liu, 2004), movie reviews (Zhuang et al., 2006), or short literary fiction stories (Kazantseva and Szpakowicz, 2006). As mentioned before, we are not aware of any work addressing"
D07-1040,P05-1037,0,0.00974618,"good summary, such as the presence of named entities, positional scores, or the location of key phrases. Such supervised techniques have been successfully used in the systems proposed by e.g. (Teufel and Moens, 1997; Hirao et al., 2002; Zhou and Hovy, 2003; D’Avanzo and Magnini, 2005). In addition to short news documents, which have been the focus of most of the summarization systems proposed to date, work has been also carried out on the summarization of other types of documents. This includes systems addressing the summarization of email threads (Wan and McKeown, 2004), online discussions (Zhou and Hovy, 2005), spoken dialogue (Galley, 2006), product reviews (Hu and Liu, 2004), movie reviews (Zhuang et al., 2006), or short literary fiction stories (Kazantseva and Szpakowicz, 2006). As mentioned before, we are not aware of any work addressing the task of automatic book summarization. 3 A Data Set for the Evaluation of Book Summarization A first challenge we encountered when we started working on the task of book summarization was the lack of a suitable data set, designed specifically for the evaluation of summaries of long documents. Unlike the summarization of short documents, which benefits from t"
D07-1040,W04-3252,1,\N,Missing
D07-1040,P04-1049,0,\N,Missing
D08-1014,H05-1073,0,0.0266035,"guages (Romanian and Spanish), we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language. 1 Samer Hassan University of North Texas samer@unt.edu Introduction We have seen a surge in interest towards the application of automatic tools and techniques for the extraction of opinions, emotions, and sentiments in text (subjectivity). A large number of text processing applications have already employed techniques for automatic subjectivity analysis, including automatic expressive text-to-speech synthesis (Alm et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), tracking sentiment timelines in on-line forums and news (Lloyd et al., 2005; Balog et al., 2006), mining opinions from product reviews (Hu and Liu, 2004), and question answering (Yu and Hatzivassiloglou, 2003). First, assuming an English corpus manually annotated for subjectivity, can we use machine translation to generate a subjectivity-annotated corpus in the target language? Second, assuming the availability of a tool for automatic subjectivity analysis in English, can we generate a corpus annotated for subject"
D08-1014,E06-2031,0,0.0371117,"Missing"
D08-1014,E06-1025,0,0.0456582,"a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language. 1 Samer Hassan University of North Texas samer@unt.edu Introduction We have seen a surge in interest towards the application of automatic tools and techniques for the extraction of opinions, emotions, and sentiments in text (subjectivity). A large number of text processing applications have already employed techniques for automatic subjectivity analysis, including automatic expressive text-to-speech synthesis (Alm et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), tracking sentiment timelines in on-line forums and news (Lloyd et al., 2005; Balog et al., 2006), mining opinions from product reviews (Hu and Liu, 2004), and question answering (Yu and Hatzivassiloglou, 2003). First, assuming an English corpus manually annotated for subjectivity, can we use machine translation to generate a subjectivity-annotated corpus in the target language? Second, assuming the availability of a tool for automatic subjectivity analysis in English, can we generate a corpus annotated for subjectivity in the target language by using automatic subjectivity annotations of Eng"
D08-1014,I05-1001,0,0.0244171,"Work Research in sentiment and subjectivity analysis has received increasingly growing interest from the natural language processing community, particularly motivated by the widespread need for opinion-based applications, including product and movie reviews, entity tracking and analysis, opinion summarization, and others. Much of the work in subjectivity analysis has been applied to English data, though work on other languages is growing: e.g., Japanese data are used in (Kobayashi et al., 2004; Suzuki et al., 2006; Takamura et al., 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al., 2005), and German data are used in (Kim and Hovy, 2006). In addition, several participants in the Chinese and Japanese Opinion Extraction tasks of NTCIR6 (Kando and Evans, 2007) performed subjectivity and sentiment analysis in languages other than English. In general, efforts on building subjectivity analysis tools for other languages have been hampered by the high cost involved in creating corpora and lexical resources for a new language. To address this gap, we focus on leveraging resources already developed for one language to derive subjectivity analysis tools for a new language. This motivates"
D08-1014,W06-1642,0,0.0166009,"8 Association for Computational Linguistics 2 3 Related Work Research in sentiment and subjectivity analysis has received increasingly growing interest from the natural language processing community, particularly motivated by the widespread need for opinion-based applications, including product and movie reviews, entity tracking and analysis, opinion summarization, and others. Much of the work in subjectivity analysis has been applied to English data, though work on other languages is growing: e.g., Japanese data are used in (Kobayashi et al., 2004; Suzuki et al., 2006; Takamura et al., 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al., 2005), and German data are used in (Kim and Hovy, 2006). In addition, several participants in the Chinese and Japanese Opinion Extraction tasks of NTCIR6 (Kando and Evans, 2007) performed subjectivity and sentiment analysis in languages other than English. In general, efforts on building subjectivity analysis tools for other languages have been hampered by the high cost involved in creating corpora and lexical resources for a new language. To address this gap, we focus on leveraging resources already developed for one language to derive subjectivity analy"
D08-1014,N06-1026,0,0.0672819,"lysis has received increasingly growing interest from the natural language processing community, particularly motivated by the widespread need for opinion-based applications, including product and movie reviews, entity tracking and analysis, opinion summarization, and others. Much of the work in subjectivity analysis has been applied to English data, though work on other languages is growing: e.g., Japanese data are used in (Kobayashi et al., 2004; Suzuki et al., 2006; Takamura et al., 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al., 2005), and German data are used in (Kim and Hovy, 2006). In addition, several participants in the Chinese and Japanese Opinion Extraction tasks of NTCIR6 (Kando and Evans, 2007) performed subjectivity and sentiment analysis in languages other than English. In general, efforts on building subjectivity analysis tools for other languages have been hampered by the high cost involved in creating corpora and lexical resources for a new language. To address this gap, we focus on leveraging resources already developed for one language to derive subjectivity analysis tools for a new language. This motivates the direction of our research, in which we use ma"
D08-1014,P07-1123,1,0.683406,"building subjectivity analysis tools for other languages have been hampered by the high cost involved in creating corpora and lexical resources for a new language. To address this gap, we focus on leveraging resources already developed for one language to derive subjectivity analysis tools for a new language. This motivates the direction of our research, in which we use machine translation coupled with cross-lingual annotation projections to generate the resources and tools required to perform subjectivity classification in the target language. The work closest to ours is the one reported in (Mihalcea et al., 2007), where a bilingual lexicon and a manually translated parallel text are used to generate the resources required to build a subjectivity classifier in a new language. In that work, we found that the projection of annotations across parallel texts can be successfully used to build a corpus annotated for subjectivity in the target language. However, parallel texts are not always available for a given language pair. Therefore, in this paper we explore a different approach where, instead of relying on manually translated parallel corpora, we use machine translation to produce a corpus in the new la"
D08-1014,H93-1061,0,0.0139269,"ize of the lexicon and the coverage of the classifier. For most of our experiments we use the high-coverage classifier. 129 Figure 2: Experiment two: machine translation of raw training data from source language into target language Table 1 shows the performance of the two OpinionFinder classifiers as measured on the MPQA corpus (Wiebe and Riloff, 2005). high-precision high-coverage P 86.7 79.4 R 32.6 70.6 F 47.4 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SemCor corpus (Miller et al., 1993), consisting of 107 documents with roughly 11,000 sentences. This is a balanced corpus covering a number of topics in sports, politics, fashion, education, and others. The reason for working with this collection is the fact that we also have a manual translation of the SemCor documents from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4). Note that in this experiment the annotation of subjectivity is carried out on the original source language text, and thus expected to be more accurate th"
D08-1014,E06-1026,0,0.0128137,"lulu, October 2008. 2008 Association for Computational Linguistics 2 3 Related Work Research in sentiment and subjectivity analysis has received increasingly growing interest from the natural language processing community, particularly motivated by the widespread need for opinion-based applications, including product and movie reviews, entity tracking and analysis, opinion summarization, and others. Much of the work in subjectivity analysis has been applied to English data, though work on other languages is growing: e.g., Japanese data are used in (Kobayashi et al., 2004; Suzuki et al., 2006; Takamura et al., 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al., 2005), and German data are used in (Kim and Hovy, 2006). In addition, several participants in the Chinese and Japanese Opinion Extraction tasks of NTCIR6 (Kando and Evans, 2007) performed subjectivity and sentiment analysis in languages other than English. In general, efforts on building subjectivity analysis tools for other languages have been hampered by the high cost involved in creating corpora and lexical resources for a new language. To address this gap, we focus on leveraging resources already developed for one languag"
D08-1014,P06-1134,1,0.545202,"automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language. 1 Samer Hassan University of North Texas samer@unt.edu Introduction We have seen a surge in interest towards the application of automatic tools and techniques for the extraction of opinions, emotions, and sentiments in text (subjectivity). A large number of text processing applications have already employed techniques for automatic subjectivity analysis, including automatic expressive text-to-speech synthesis (Alm et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), tracking sentiment timelines in on-line forums and news (Lloyd et al., 2005; Balog et al., 2006), mining opinions from product reviews (Hu and Liu, 2004), and question answering (Yu and Hatzivassiloglou, 2003). First, assuming an English corpus manually annotated for subjectivity, can we use machine translation to generate a subjectivity-annotated corpus in the target language? Second, assuming the availability of a tool for automatic subjectivity analysis in English, can we generate a corpus annotated for subjectivity in the target language by using automatic su"
D08-1014,W03-1017,0,0.0317556,"interest towards the application of automatic tools and techniques for the extraction of opinions, emotions, and sentiments in text (subjectivity). A large number of text processing applications have already employed techniques for automatic subjectivity analysis, including automatic expressive text-to-speech synthesis (Alm et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), tracking sentiment timelines in on-line forums and news (Lloyd et al., 2005; Balog et al., 2006), mining opinions from product reviews (Hu and Liu, 2004), and question answering (Yu and Hatzivassiloglou, 2003). First, assuming an English corpus manually annotated for subjectivity, can we use machine translation to generate a subjectivity-annotated corpus in the target language? Second, assuming the availability of a tool for automatic subjectivity analysis in English, can we generate a corpus annotated for subjectivity in the target language by using automatic subjectivity annotations of English text and machine translation? Finally, third, can these automatically generated resources be used to effectively train tools for subjectivity analysis in the target language? Since our methods are particula"
D09-1020,N06-2015,0,0.038926,"ity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis. Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses. We demonstrate that this ambiguity is also prevalent in a corpus. Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD. They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions. Our work is similar in the sense that we reduce all senses of a word to two senses (S/O). The difference is the criterion driving the grouping. Related work concentrates on syntactic and semantic similarity between senses to group them. In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity and sentiment analysis. Table 4: Effect of SWSD on th"
D09-1020,C04-1200,0,0.115446,"report results using 0.0008, though the accuracy using the other thresholds is statistically significantly better than the accuracy of the original classifier at the same level. 196 Acc NP NR NF NgP NgR NgF PsP PsR PsF OP s/N g/N 77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0 R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0 Table 5: Effect of SWSD on the contextual polarity classifier ON/P R3 R4 Acc NP NR NF PP PR PF 79.0 81.5 92.5 86.7 65.8 40.7 50.3 70.0 83.7 73.8 78.4 44.4 59.3 50.8 81.6 81.7 96.8 88.6 81.1 38.6 52.3 tional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstra"
D09-1020,N06-1026,0,0.00935933,"e subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1 His alarm grew. alarm, dismay, consternation – (fear resulting from the awareness of danger) =&gt; fear, fearfulness, fright – (an emotion experienced in anticipation of some specific pain or danger (usually accompanied by a desire to flee or"
D09-1020,C02-1039,1,0.792672,"-tagged data). The subjectivity sense labels are used to collapse the sense labels in the sense-tagged data into the two new senses, S and O. His alarm grew. Will someone shut that darn alarm off? The alarm went off. We use a supervised approach to SWSD. We train a different classifier for each lexicon entry for which we have training data. Thus, our approach is like targeted WSD (in contrast to allwords WSD), with two labels: S and O. We borrow machine learning features which have been successfully used in WSD. Specifically, given an ambiguous target word, we use the following features from (Mihalcea, 2002): Our sense-tagged data are the lexical sample corpora (training and test data) from S ENSEVAL1 (Kilgarriff and Palmer, 2000), S ENSEVAL2 (Preiss and Yarowsky, 2001), and S ENSEVAL3 (Mihalcea and Edmonds, 2004). We selected all of the S ENSEVAL words that are also in the subjectivity lexicon, and labeled their dictionary senses as S, O, or B according to the annotation scheme described above in Section 2. We did this subjectivity sense labeling according to the sense inventory of the underlying corpus (Hector for S ENSEVAL1; WordNet1.7 for S ENSEVAL2; and WordNet1.7.1 for S ENSEVAL3). CW : the"
D09-1020,P06-1014,0,0.0109781,"2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis. Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses. We demonstrate that this ambiguity is also prevalent in a corpus. Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD. They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions. Our work is similar in the sense that we reduce all senses of a word to two senses (S/O). The difference is the criterion driving the grouping. Related work concentrates on syntactic and semantic similarity between senses to group them. In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity and sentiment"
D09-1020,W04-2807,0,0.0103097,"Missing"
D09-1020,P04-1035,0,0.0560716,") classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1 His alarm grew. alarm, dismay, consternation – (fear resulting from the awareness of danger) =&gt; fear, fearfulness, fright – (an emotion experienced in anticipation of some specific pain or danger"
D09-1020,W03-1014,1,0.19976,"d negative classes. Negative precision goes from 60.4 to 82.1 and positive precision goes from 52.2 to 68.6, with no loss in recall. This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with addi6 Conclusions and Future Work We introduced the task of subjectivity word sense disambiguation (SWSD), and evaluated a supervised method inspired by research in WSD. The 197 system achieves high accuracy, especially on highly ambiguous words, and substantially outperforms WSD on the same data. The positive results provide evidence that SWSD is a feasible variant of WSD, and that the S/O sense groupings are natural ones. We also explored the promise of SWSD for contextual subjectivity analysis. We showed that a subjectivity lexicon can have substantial coverag"
D09-1020,D07-1107,0,0.0177048,"ates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis. Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses. We demonstrate that this ambiguity is also prevalent in a corpus. Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD. They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions. Our work is similar in the sense that we reduce all senses of a word to two senses (S/O). The difference is the criterion driving the grouping. Related work concentrates on syntactic and semantic similarity between senses to group them. In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity and sentiment analysis. Table 4:"
D09-1020,E06-1027,0,0.0925621,"9.1 82.1 29.4 43.2 68.6 32.4 44.0 Table 5: Effect of SWSD on the contextual polarity classifier ON/P R3 R4 Acc NP NR NF PP PR PF 79.0 81.5 92.5 86.7 65.8 40.7 50.3 70.0 83.7 73.8 78.4 44.4 59.3 50.8 81.6 81.7 96.8 88.6 81.1 38.6 52.3 tional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD variant (SWSD) helps with subje"
D09-1020,N07-1039,0,0.0911988,"g 0.0008, though the accuracy using the other thresholds is statistically significantly better than the accuracy of the original classifier at the same level. 196 Acc NP NR NF NgP NgR NgF PsP PsR PsF OP s/N g/N 77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0 R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0 Table 5: Effect of SWSD on the contextual polarity classifier ON/P R3 R4 Acc NP NR NF PP PR PF 79.0 81.5 92.5 86.7 65.8 40.7 50.3 70.0 83.7 73.8 78.4 44.4 59.3 50.8 81.6 81.7 96.8 88.6 81.1 38.6 52.3 tional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivit"
D09-1020,esuli-sebastiani-2006-sentiwordnet,0,0.0602843,"2.4 40.0 R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0 Table 5: Effect of SWSD on the contextual polarity classifier ON/P R3 R4 Acc NP NR NF PP PR PF 79.0 81.5 92.5 86.7 65.8 40.7 50.3 70.0 83.7 73.8 78.4 44.4 59.3 50.8 81.6 81.7 96.8 88.6 81.1 38.6 52.3 tional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a)). We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis. Another set of related work is on subjectivity and polarity labeling of word senses (e.g. (Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008)). They label senses of words in a dictionary. In comparison, we label senses of word instances in a corpus. Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008). (Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses. We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses. They also demonstrate that subjectivity helps with WSD. We show that a coarse-grained WSD"
D09-1020,P02-1053,0,0.0108887,"k at the precision of the positive and negative classes. Negative precision goes from 60.4 to 82.1 and positive precision goes from 52.2 to 68.6, with no loss in recall. This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system. 5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis. These systems typically look for the presence of subjective or sentiment-bearing words in the text. They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with addi6 Conclusions and Future Work We introduced the task of subjectivity word sense disambiguation (SWSD), and evaluated a supervised method inspired by research in WSD. The 197 system achieves high accuracy, especially on highly ambiguous words, and substantially outperforms WSD on the same data. The positive results provide evidence that SWSD is a feasible variant of WSD, and that the S/O sense groupings are natural ones. We also explored the promise of SWSD for contextual subjectivity analysis. We showed that a s"
D09-1020,P06-1134,1,0.965016,"ins all of the negative keywords 190 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190–199, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP first attempt to explicitly use sense-level subjectivity tags in contextual subjectivity and sentiment analysis. 2 In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases. In addition, other properties are annotated, including polarity. For SWSD, we need the notions of subjective and objective senses of words in a dictionary. We adopt the definitions from (Wiebe and Mihalcea, 2006), who describe the annotation scheme as follows. Classifying a sense as S means that, when the sense is used in a text or conversation, one expects it to express subjectivity, and also that the phrase or sentence containing it expresses subjectivity. As noted in (Wiebe and Mihalcea, 2006), sentences containing objective senses may not be objective. Thus, objective senses are defined as follows: Classifying a sense as O means that, when the sense is used in a text or conversation, one does not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subj"
D09-1020,H05-1044,1,0.565807,"m grew. He absorbed the information quickly. UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. What’s the catch? Polarity (also called semantic orientation) is also important to NLP applications. In review mining, for example, we want to know whether an opinion about a product is positive or negative. Nonetheless, as argued by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), there are also motivations for a separate subjective/objective (S/O) classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. T"
D09-1020,H05-2018,1,0.580955,"m grew. He absorbed the information quickly. UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. What’s the catch? Polarity (also called semantic orientation) is also important to NLP applications. In review mining, for example, we want to know whether an opinion about a product is positive or negative. Nonetheless, as argued by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), there are also motivations for a separate subjective/objective (S/O) classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. T"
D09-1020,W03-1017,0,0.669166,"arate subjective/objective (S/O) classification. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006). We will see further evidence of this in Section 4.2.3 in this paper. The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications. The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions. 1 His alarm grew. alarm, dismay, consternation – (fear resulting from the awareness of danger) =&gt; fear, fearfulness, fright – (an emotion experienced in anticipation of some spec"
D09-1020,C08-1104,0,\N,Missing
D09-1124,P07-1020,0,0.0187351,"lies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word"
D09-1124,P06-1070,0,0.00899188,"we address the task of cross-lingual semantic relatedness, and introduce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel text"
D09-1124,heylen-etal-2008-modelling,0,0.0147606,"in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other languages as well. All these methods proposed in the past have been concerned with monolingual word relatedness calculated within the boundaries of one language, as opposed to cross-lingual relatedness, which is the focus of our work. The research area closest to the task of cross7 Related Work Measures of word relatedness were found useful in a large number of natural language process"
D09-1124,D07-1061,0,0.0994357,"tasets based on the standard Miller-Charles (Miller and Charles, 1998) and WordSimilarity353 (Finkelstein et al., 2001) English word relatedness datasets. The Miller-Charles dataset (Miller and Charles, 1998) consists of 30-word pairs ranging from synonymy pairs (e.g., car - automobile) to completely unrelated terms (e.g., noon - string). The relatedness of each word pair was rated by 38 human subjects, using a scale from 0 (not-related) to 4 (perfect synonymy). The dataset is available only in English and has been widely used in previous semantic relatedness evaluations (e.g., (Resnik, 1995; Hughes and Ramage, 2007; Zesch et al., 2008)). The WordSimilarity-353 dataset (also known as Finkelstein-353) (Finkelstein et al., 2001) consists of 353 word pairs annotated by 13 human experts, on a scale from 0 (unrelated) to 10 (very closely related or identical). The Miller-Charles set is a subset in the WordSimilarity-353 data set. Unlike the Miller-Charles data set, which consists only of 1195 English Spanish Arabic coast - shore costa - orilla   -  Word pair car - automobile coche - automovil  -     brother - monk hermano - monje !    - ""# Romanian ¸ta˘ rm - mal mas¸fin˘a - a"
D09-1124,P02-1050,0,0.0111756,"Missing"
D09-1124,O97-1002,0,0.060428,".2 3000 2500 2000 1500 1000 500 0.1 5000 10000 15000 20000 Vector Size 25000 0 30000 5000 10000 15000 20000 Vector length 25000 30000 Figure 4: Lesk vs. cosine similarity for the WordSimilarity-353 data set Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set sure of relatedness,6 their effect is only significant for the top ranked concepts in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be in principle applied to other language"
D09-1124,D07-1060,0,0.419275,"e the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version exists. The paper is organized as follows. We first provide a brief overview of Wikipedia, followed by a description of the method to b"
D09-1124,C00-2163,0,0.0590169,"ce a method that relies on Wikipedia in order to calculate the relatedness of words across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measur"
D09-1124,C02-1070,0,0.176129,"ian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version exists. The paper is organized as follows. We first provide a brief overview of Wikipedia, fo"
D09-1124,W02-2026,0,0.277124,"ords across languages. For instance, given the word factory in English and the word lavoratore in Italian (En. worker), the method can measure the relatedness of these two words despite the fact that they belong to two different languages. Measures of cross-language relatedness are useful for a large number of applications, including cross-language information retrieval (Nie et al., 1999; Monz and Dorr, 2005), cross-language text classification (Gliozzo and Strapparava, 2006), lexical choice in machine translation (Och and Ney, 2000; Bangalore et al., 2007), induction of translation lexicons (Schafer and Yarowsky, 2002), cross-language annotation and resource projections to a second language (Riloff et al., 2002; Hwa et al., 2002; Mohammad et al., 2007). The method we propose is based on a measure of closeness between concept vectors automatically built from Wikipedia, which are mapped via the Wikipedia interlanguage links. Unlike previous methods for cross-language mapping, which are typically limited by the availability of bilingual dictionaries or parallel texts, the method proposed in this paper can be used to measure the relatedness of word pairs in any of the 250 languages for which a Wikipedia version"
D09-1124,P94-1019,0,0.0405598,"s ar↔ro en↔es en↔ro es↔ro 0.4 0.3 0.2 3000 2500 2000 1500 1000 500 0.1 5000 10000 15000 20000 Vector Size 25000 0 30000 5000 10000 15000 20000 Vector length 25000 30000 Figure 4: Lesk vs. cosine similarity for the WordSimilarity-353 data set Figure 6: Number of interlanguage links vs. vector length for the WordSimilarity-353 data set sure of relatedness,6 their effect is only significant for the top ranked concepts in a vector. Therefore, increasing the vectors size to maximize the matching of the projected dimensions does not necessarily lead to accuracy improvements. edge bases (Lesk, 1986; Wu and Palmer, 1994; Resnik, 1995; Jiang and Conrath, 1997; Hughes and Ramage, 2007) or on large corpora (Salton et al., 1997; Landauer et al., 1998; Turney, 2001; Gabrilovich and Markovitch, 2007). Although to a lesser extent, measures of word relatedness have also been applied on other languages, including German (Zesch et al., 2007; Zesch et al., 2008; Mohammad et al., 2007), Chinese (Wang et al., 2008), Dutch (Heylen et al., 2008) and others. Moreover, assuming resources similar to those available for English, e.g., WordNet structures or large corpora, the measures of relatedness developed for English can be"
D09-1124,N07-2052,0,0.0129701,"Missing"
D10-1103,D08-1014,1,0.875171,"Missing"
D10-1103,J96-1002,0,0.0348349,"probability of the class of a target language document as the mixture of the probabilities by each translated model from the source language model, weighed by their translation probabilities. P (c|d, mt ) ≈ ∑ m′t P (m′t |ms , c)P (c|d, m′t ) where mt is the target language classification model and m′t is a candidate model translated from the model ms trained on the labeled training data in the source language. This is a very generic representation for model translation and the model m could be any type of text classification. Specifically in this paper, we take the Maximum Entropy (ME) model(Berger et al., 1996) as an example for the model translation across languages, since the ME model is one of the most widely used text classification models. The maximum entropy classifier takes the form 1 P (c|d) = Z(d) ∏ e 1061 v ∏ λwi f (wti ,c) e s i=1 The model translation probability P (m′t |ms , c) can be modeled as the product of the translation probabilities of each of its individual bag-of-words fea∏l i i tures P (m′t |ms , c) ≈ P (w t |ws , c) and the i=1 classification model can be further written as b c = argmaxc∈C v ∑∏ m′t λwi f (wti ,c) P (wti |wsi , c)e s i=1 where feature translation probabilities"
D10-1103,J93-2003,0,0.052566,"e use of a bilingual dictionary can drastically improve the performance of the models learned from corpora. (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005) studied the use of machine translation tools for the purpose of cross language text classification and mining. These approaches typically translate the training data or test data into the same language, followed by the application of a monolingual classifier. The performance of such classifiers very much depends on the quality of the machine translation tools. Unfortunately, the development of statistical machine translation systems (Brown et al., 1993) is hindered by the lack of availability of parallel corpora and the quality of their output is often erroneous. Several methods were proposed (Shi et al., 2006; Nie et al., 1999) to automatically acquire a large quantity of parallel sentences from the web, but such web data is however predominantly confined to a limited number of domains and language pairs. (Dai et al., 2007) experimented with the use of transfer learning for text classification. Although in this method the transfer learning is performed across different domains in the same language, the underlying principle is similar to CLT"
D10-1103,P06-1070,0,0.0261554,"um Entropy (Nigam et al., 1999) or Support Vector Machines (Vapnik, 1995; Joachims, 1998). If only a small amount of annotated data is available, the alternative is to use semi-supervised bootstrapping methods such as co-training or self-training, which can also integrate raw unlabeled data into the learning model (Blum and Mitchell, 1998; Nigam and Ghani, 2000). Despite the attention that monolingual text classification has received from the research community, there is only very little work that was done on cross-lingual text classification. The work that is most closely related to ours is (Gliozzo and Strapparava, 2006), where a multilingual domain kernel is learned from comparable corpora, and subsequently used for the cross-lingual classification of texts. In experiments run on Italian and English, Gliozzo and Strapparava showed that the multilingual domain kernel exceeds by a large margin a bag-of-words approach. Moreover, they demonstrated that the use of a bilingual dictionary can drastically improve the performance of the models learned from corpora. (Fortuna and Shawe-Taylor, 2005; Olsson et al., 2005) studied the use of machine translation tools for the purpose of cross language text classification a"
D10-1103,P07-1123,1,0.939937,"model to better fit the data distribution of the target language. 1 Introduction Given the accelerated growth of the number of multilingual documents on the Web and elsewhere, the need for effective multilingual and cross-lingual text processing techniques is becoming increasingly important. There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al., 2003), syntactic parsing (Hwa et al., 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al., 2007), and others. In this paper, we address the task of cross-lingual text classification (CLTC), which builds text classifiers for multiple languages by using training data in one language, thereby avoiding the costly and timeconsuming process of labeling training data for each individual language. The main idea underlying our approach to CLTC is that although content can be expressed in different forms in different languages, Mingjun Tian Yahoo! Global R&D Beijing, China mingjun@yahoo-inc.com there is a significant amount of knowledge that is shared for similar topics that can be effectively use"
D10-1103,P03-1058,0,0.00991961,"r exploit the readily available unlabeled data in the target language via semisupervised learning, and adapt the translated model to better fit the data distribution of the target language. 1 Introduction Given the accelerated growth of the number of multilingual documents on the Web and elsewhere, the need for effective multilingual and cross-lingual text processing techniques is becoming increasingly important. There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al., 2003), syntactic parsing (Hwa et al., 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al., 2007), and others. In this paper, we address the task of cross-lingual text classification (CLTC), which builds text classifiers for multiple languages by using training data in one language, thereby avoiding the costly and timeconsuming process of labeling training data for each individual language. The main idea underlying our approach to CLTC is that although content can be expressed in different forms in different languages, Mingjun Tian Yahoo! Global R&D Beijing, Ch"
D10-1103,P04-1035,0,0.00154615,"translated model with unlabeled documents in the target language. Experiments and evaluations are presented in section 5 and finally we conclude the paper in section 6. 2 Related Work Text classification has rightfully received a lot of attention from both the academic and industry communities, being one of the areas in natural language processing that has a very large number of practical applications. Text classification techniques have been applied to many diverse problems, ranging from topic classification (Joachims, 1997), to genre detection (Argamon et al., 1998), opinion identification (Pang and Lee, 2004), spam detection (Sahami et al., 1998), gender and age classification (Schler et al., 2006). Text classification is typically formulated as a learning task, where a classifier learns how to distinguish between categories in a given set, using features automatically extracted from a collection of documents. In addition to the learning methodology itself, the accuracy of the text classifier also depends to a large extent upon the amount of training data available at hand. For instance, distinguishing between two categories for which thousands of manually annotated examples are already available"
D10-1103,P06-1062,1,0.255638,"tudied the use of machine translation tools for the purpose of cross language text classification and mining. These approaches typically translate the training data or test data into the same language, followed by the application of a monolingual classifier. The performance of such classifiers very much depends on the quality of the machine translation tools. Unfortunately, the development of statistical machine translation systems (Brown et al., 1993) is hindered by the lack of availability of parallel corpora and the quality of their output is often erroneous. Several methods were proposed (Shi et al., 2006; Nie et al., 1999) to automatically acquire a large quantity of parallel sentences from the web, but such web data is however predominantly confined to a limited number of domains and language pairs. (Dai et al., 2007) experimented with the use of transfer learning for text classification. Although in this method the transfer learning is performed across different domains in the same language, the underlying principle is similar to CLTC in the sense that different domains or languages may share a significant amount of knowledge in similar classification tasks. (Blum and Mitchell, 1998) employ"
D10-1103,P09-1027,0,0.0903329,"related is the work carried out in the field of sentiment and subjectivity analysis for cross-lingual classification of opinions. For instance, (Mihalcea et al., 2007) use an English corpus annotated for subjectivity along with parallel text to build a subjectivity classifier for Romanian. Similarly, (Banea et al., 2008) propose a method based on machine translation to generate parallel texts, followed by a cross-lingual projection of subjectivity labels, which are used to train subjectivity annotation tools for Romanian and Spanish. A related, yet more sophisticated technique is proposed in (Wan, 2009), where a co-training approach is used to leverage resources from both a source and a target language. The technique is tested on the automatic sentiment classification of product reviews in Chinese, and showed to successfully make use of both crosslanguage and within-language knowledge. 3 Cross Language Model Translation To make the classifier applicable to documents in a foreign language, we introduce a method where model features that are learned from the training data are translated from the source language into the target language. Using this translation process, a feature associated with"
D12-1054,H05-1073,0,0.129543,"g; and the duration of the note. Table 1 shows statistics on the corpus. An example from the corpus, consisting of the first two lines from the Beatles’ song A hard day’s night, is illustrated in Figure 3. S ONGS S ONGS IN “ MAJOR ” KEY S ONGS IN “ MINOR ” KEY L INES A LIGNED SYLLABLES / NOTES 100 59 41 4,976 34,045 Table 1: Some statistics of the corpus Emotion Annotations with Mechanical Turk. In order to explore the classification of emotions in songs, we needed a gold standard consisting of manual emotion annotations of the songs. Following 592 previous work on emotion annotation of text (Alm et al., 2005; Strapparava and Mihalcea, 2007), to annotate the emotions in songs we use the six basic emotions proposed by (Ekman, 1993): ANGER, DISGUST , FEAR , JOY , SADNESS , SURPRISE . To collect the annotations, we use the Amazon Mechanical Turk service, which was previously found to produce reliable annotations with a quality comparable to those generated by experts (Snow et al., 2008). The annotations are collected at line level, with a separate annotation for each of the six emotions. We collect numerical annotations using a scale between 0 and 10, with 0 corresponding to the absence of an emotion"
D12-1054,S07-1094,0,0.0200567,"Missing"
D12-1054,S07-1067,0,0.0532147,"Missing"
D12-1054,S07-1072,0,0.0171951,"Missing"
D12-1054,D08-1027,0,0.141238,"Missing"
D12-1054,S07-1013,1,0.53812,"n of the note. Table 1 shows statistics on the corpus. An example from the corpus, consisting of the first two lines from the Beatles’ song A hard day’s night, is illustrated in Figure 3. S ONGS S ONGS IN “ MAJOR ” KEY S ONGS IN “ MINOR ” KEY L INES A LIGNED SYLLABLES / NOTES 100 59 41 4,976 34,045 Table 1: Some statistics of the corpus Emotion Annotations with Mechanical Turk. In order to explore the classification of emotions in songs, we needed a gold standard consisting of manual emotion annotations of the songs. Following 592 previous work on emotion annotation of text (Alm et al., 2005; Strapparava and Mihalcea, 2007), to annotate the emotions in songs we use the six basic emotions proposed by (Ekman, 1993): ANGER, DISGUST , FEAR , JOY , SADNESS , SURPRISE . To collect the annotations, we use the Amazon Mechanical Turk service, which was previously found to produce reliable annotations with a quality comparable to those generated by experts (Snow et al., 2008). The annotations are collected at line level, with a separate annotation for each of the six emotions. We collect numerical annotations using a scale between 0 and 10, with 0 corresponding to the absence of an emotion, and 10 corresponding to the hig"
D12-1054,strapparava-valitutti-2004-wordnet,1,0.490929,"e equal to the average of the scores on the training data, and measured the correlation between these default scores and the gold standard, consistently led to correlations close to 0 (0.0081-0.0221). 3 594 guistic Inquiry and Word Count (LIWC) and WordNet Affect (WA) to derive coarse textual features. LIWC was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999; Pennebaker and King, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., emotion, cognition). WA (Strapparava and Valitutti, 2004) is a resource that was created starting with WordNet, by annotating synsets with several emotions. It uses several resources for affective information, including the emotion classification of Ortony (Ortony et al., 1987). From WA, we extract the words corresponding to the six basic emotions used in our experiments. For each semantic class, we infer a feature indicating the number of words in a line belonging to that class. Table 3 shows the Pearson correlations obtained for each of the six emotions, when using only unigrams, only semantic classes, or both. Emotion ANGER DISGUST FEAR JOY SADNE"
D12-1054,P08-2034,0,0.0883511,"nnotated corpus is found in (O’Hara, 2011), who presented preliminary research that checks whether the expressive meaning of a particular harmony or harmonic sequence could be deduced from the lyrics it accompanies, by using harmonically annotated chords from the Usenet group alt.guitar.tab. Finally, in natural language processing, there are a few studies that mainly exploited the lyrics component of the songs, while generally ignoring the musical component. For instance, (Mahedero et al., 2005) dealt with language identification, structure extraction, and thematic categorization for lyrics. (Xia et al., 2008) addressed the task of sentiment classification in lyrics, recognizing positive and negative moods in a large dataset of Chinese pop songs, while (Yang and Lee, 2009) approached the problem of emotion identification in lyrics, classifying songs from allmusic.com using a set of 23 emotions. 3 A Corpus of Music and Lyrics Annotated for Emotions To enable our exploration of emotions in songs, we compiled a corpus of 100 popular songs (e.g., Dancing Queen by ABBA, Hotel California by Eagles, Let it Be by The Beatles). Popular songs exert a lot of power on people, both at an individual level as wel"
D15-1133,W12-0403,0,0.0329913,"Missing"
D15-1133,P12-2034,0,0.680961,"gender, age or even education level. There are multiple scenarios where it would be desirable to identify deceivers’ demographics; Related work To date, several studies have explored the identification of deceptive content in a variety of domains, including online dating, forums, social networks, and consumer reviews. (Toma and Hancock, 2010) conducted linguistic analyses in online dating profiles and identified correlations between deceptive profiles and self references, negations, and lower levels of words usage. A study for deception detection on essays and product reviews is presented in (Feng et al., 2012). (Ott et al., 2011) addressed the identification of spam in consumer reviews and also studied the human capability of detecting deceptive reviews, which was found not better than chance. In a following study, (Ott et al., 2013) presented an analysis of the sentiment associated to deceitful reviews focusing particularly in those containing negative 1120 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1120–1125, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. sentiment as it largely affects consumer purchase"
D15-1133,P09-2078,1,0.488279,"Conference on Empirical Methods in Natural Language Processing, pages 1120–1125, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. sentiment as it largely affects consumer purchase decisions. More recently (Yu et al., 2015) presented a study where authors analyze the role of deception in online networks by detecting deceptive groups in a social elimination-game. This previous work has shown the effectiveness of features derived from text analysis, which frequently includes basic linguistic representations such as n-grams and sentence counts statistics (Mihalcea and Strapparava, 2009; Ott et al., 2011) and also more complex linguistic features derived from syntactic context free grammar trees and part of speech tags (Feng et al., 2012; Xu and Zhao, 2012). Other studies have focused on deception clues inspired from psychological studies. For instance, following the hypothesis that deceivers might create less complex sentences (DePaulo et al., 2003), researchers have incorporated syntactic complexity measures into the analysis. (Yancheva and Rudzicz, 2013) presented a study based on the analysis of syntactic units and found that syntactic complexity correlates with deceiver"
D15-1133,P11-1032,0,0.0545347,"ducation level. There are multiple scenarios where it would be desirable to identify deceivers’ demographics; Related work To date, several studies have explored the identification of deceptive content in a variety of domains, including online dating, forums, social networks, and consumer reviews. (Toma and Hancock, 2010) conducted linguistic analyses in online dating profiles and identified correlations between deceptive profiles and self references, negations, and lower levels of words usage. A study for deception detection on essays and product reviews is presented in (Feng et al., 2012). (Ott et al., 2011) addressed the identification of spam in consumer reviews and also studied the human capability of detecting deceptive reviews, which was found not better than chance. In a following study, (Ott et al., 2013) presented an analysis of the sentiment associated to deceitful reviews focusing particularly in those containing negative 1120 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1120–1125, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. sentiment as it largely affects consumer purchase decisions. More rec"
D15-1133,P15-1083,0,0.0202861,"Missing"
D15-1133,N13-1053,0,0.302965,"ariety of domains, including online dating, forums, social networks, and consumer reviews. (Toma and Hancock, 2010) conducted linguistic analyses in online dating profiles and identified correlations between deceptive profiles and self references, negations, and lower levels of words usage. A study for deception detection on essays and product reviews is presented in (Feng et al., 2012). (Ott et al., 2011) addressed the identification of spam in consumer reviews and also studied the human capability of detecting deceptive reviews, which was found not better than chance. In a following study, (Ott et al., 2013) presented an analysis of the sentiment associated to deceitful reviews focusing particularly in those containing negative 1120 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1120–1125, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. sentiment as it largely affects consumer purchase decisions. More recently (Yu et al., 2015) presented a study where authors analyze the role of deception in online networks by detecting deceptive groups in a social elimination-game. This previous work has shown the effectiven"
D15-1133,P06-1055,0,0.0784473,"Missing"
D15-1133,C12-2131,0,0.0812445,"t largely affects consumer purchase decisions. More recently (Yu et al., 2015) presented a study where authors analyze the role of deception in online networks by detecting deceptive groups in a social elimination-game. This previous work has shown the effectiveness of features derived from text analysis, which frequently includes basic linguistic representations such as n-grams and sentence counts statistics (Mihalcea and Strapparava, 2009; Ott et al., 2011) and also more complex linguistic features derived from syntactic context free grammar trees and part of speech tags (Feng et al., 2012; Xu and Zhao, 2012). Other studies have focused on deception clues inspired from psychological studies. For instance, following the hypothesis that deceivers might create less complex sentences (DePaulo et al., 2003), researchers have incorporated syntactic complexity measures into the analysis. (Yancheva and Rudzicz, 2013) presented a study based on the analysis of syntactic units and found that syntactic complexity correlates with deceiver’s age. Psycholinguistics lexicons, such as Linguistic Inquiry and Word Count (LIWC) (Pennebaker and Francis, 1999), have also been used to build deception models using machi"
D15-1133,P13-1093,0,0.226022,"analysis, which frequently includes basic linguistic representations such as n-grams and sentence counts statistics (Mihalcea and Strapparava, 2009; Ott et al., 2011) and also more complex linguistic features derived from syntactic context free grammar trees and part of speech tags (Feng et al., 2012; Xu and Zhao, 2012). Other studies have focused on deception clues inspired from psychological studies. For instance, following the hypothesis that deceivers might create less complex sentences (DePaulo et al., 2003), researchers have incorporated syntactic complexity measures into the analysis. (Yancheva and Rudzicz, 2013) presented a study based on the analysis of syntactic units and found that syntactic complexity correlates with deceiver’s age. Psycholinguistics lexicons, such as Linguistic Inquiry and Word Count (LIWC) (Pennebaker and Francis, 1999), have also been used to build deception models using machine learning approaches (Mihalcea and Strapparava, 2009; Almela et al., 2012) and showed that the use of semantic information is helpful for the automatic identification of deceit. While there is a significant body of work on computational deception detection, except for (Yancheva and Rudzicz, 2013) who co"
D15-1281,W12-0403,0,0.0280335,"Missing"
D15-1281,P12-2034,0,0.256994,"others. In many of these settings, the polygraph test has been used as the main method to identify deceptive behavior. However, this method requires the use of skin-contact devices and human expertise, making it infeasible for large-scale applications. Moreover, polygraph tests were shown to be misleading in multiple cases (Vrij, 2001; Gannon et al., 2009), as human judgment is often biased. Given the difficulties associated with the use of polygraph-like methods, learning-based approaches have been proposed to address the deception detection task using a number of modalities, including text (Feng et al., 2012) and speech (Hirschberg et al., 2005; Newman et al., 2003). Unlike the polygraph methods, learning-based methods for deception detection rely mainly on data collected from deceivers and truth-tellers. The data is usually elicited from human contributors, in a lab setting or via crowdsourcing. An important problem identified in this data-driven research is the lack of real data. Because of the artificial setting, the subjects may not be emotionally aroused, as they may not take the experiments seriously given the lack of motivation and/or penalty. In this paper, we describe what we believe is a"
D15-1281,P14-1147,0,0.0034252,"d to be slightly better than chance (Aamodt and Custer, 2006). Moreover, the performance of the human annotators appears to be significantly below that of our system. 6 Related Work Verbal Deception Detection. To date, several research publications on verbal-based deception detection have explored the identification of deceptive content in a variety of domains, including online dating websites (Toma and Hancock, 2010; Guadagno et al., 2012), forums (Warkentin et al., 2010; Joinson and Dietz-Uhler, 2002), social networks (Ho and Hollister, 2013), and consumer report websites (Ott et al., 2011; Li et al., 2014). Research findings have shown the effectiveness of features derived from text analysis, which frequently includes basic linguistic representations such as n-grams and sentence count statistics (Mihalcea and Strapparava, 2009), and also more complex linguistic features derived from syntactic CFG trees and part of speech tags (Feng et al., 2012; Xu and Zhao, 2012). Research work has also relied on the LIWC lexicon to build deception models using machine learning approaches (Mi´ halcea and Strapparava, 2009; Angela Almela et al., 2012) and showed that the use of psycholinguistic information is h"
D15-1281,P09-2078,1,0.83156,"rams, psycholinguistic features, and syntactic complexity features. Unigrams. We extract unigrams derived from the bag-of-words representation of the video transcripts. The unigram features are encoded as word frequencies and include all the words present in the transcripts. Psycholinguistic Features. The Linguistic Word Count (LIWC) is a psycholinguistics lexicon that has been frequently used to incorporate semantic and psychological information into linguistic analysis (Pennebaker and Francis, 1999). It has been successfully used in previous work on deception detection (Newman et al., 2003; Mihalcea and Strapparava, 2009; Ott et al., 2011). We obtain features for each of the 80 psycholinguistic classes present in the lexicon by calculating the percentage of words in the transcription belonging to each class. Syntactic Complexity. We also extract features to measure the syntactic complexity of the speech produced by the speakers in truthful and deceptive clips. This set of features is motivated by previous research that has suggested that deceivers’ speech has lower complexity (Depaulo et al., 2003). We use the tool described in (Lu, 2010), which generates indexes of syntactic complexity, including general com"
D15-1281,P11-1032,0,0.590371,"and syntactic complexity features. Unigrams. We extract unigrams derived from the bag-of-words representation of the video transcripts. The unigram features are encoded as word frequencies and include all the words present in the transcripts. Psycholinguistic Features. The Linguistic Word Count (LIWC) is a psycholinguistics lexicon that has been frequently used to incorporate semantic and psychological information into linguistic analysis (Pennebaker and Francis, 1999). It has been successfully used in previous work on deception detection (Newman et al., 2003; Mihalcea and Strapparava, 2009; Ott et al., 2011). We obtain features for each of the 80 psycholinguistic classes present in the lexicon by calculating the percentage of words in the transcription belonging to each class. Syntactic Complexity. We also extract features to measure the syntactic complexity of the speech produced by the speakers in truthful and deceptive clips. This set of features is motivated by previous research that has suggested that deceivers’ speech has lower complexity (Depaulo et al., 2003). We use the tool described in (Lu, 2010), which generates indexes of syntactic complexity, including general complexity metrics, le"
D15-1281,perez-rosas-etal-2014-multimodal,1,0.564163,"Missing"
D15-1281,C12-2131,0,0.0681203,"including online dating websites (Toma and Hancock, 2010; Guadagno et al., 2012), forums (Warkentin et al., 2010; Joinson and Dietz-Uhler, 2002), social networks (Ho and Hollister, 2013), and consumer report websites (Ott et al., 2011; Li et al., 2014). Research findings have shown the effectiveness of features derived from text analysis, which frequently includes basic linguistic representations such as n-grams and sentence count statistics (Mihalcea and Strapparava, 2009), and also more complex linguistic features derived from syntactic CFG trees and part of speech tags (Feng et al., 2012; Xu and Zhao, 2012). Research work has also relied on the LIWC lexicon to build deception models using machine learning approaches (Mi´ halcea and Strapparava, 2009; Angela Almela et al., 2012) and showed that the use of psycholinguistic information is helpful for the automatic identification of deceit. Following the hypothesis that deceivers might create less complex sentences in an effort to conceal the truth and being able to recall their lies more easily, several researchers have also studied the relation between text syntactic complexity and deception (Yancheva and Rudzicz, 2013). Nonverbal Deception Detect"
D15-1281,P13-1093,0,0.0128741,"t of speech tags (Feng et al., 2012; Xu and Zhao, 2012). Research work has also relied on the LIWC lexicon to build deception models using machine learning approaches (Mi´ halcea and Strapparava, 2009; Angela Almela et al., 2012) and showed that the use of psycholinguistic information is helpful for the automatic identification of deceit. Following the hypothesis that deceivers might create less complex sentences in an effort to conceal the truth and being able to recall their lies more easily, several researchers have also studied the relation between text syntactic complexity and deception (Yancheva and Rudzicz, 2013). Nonverbal Deception Detection. Earlier approaches to nonverbal deception detection relied 2343 on polygraph tests to detect deceptive behavior. These tests are mainly based on such physiological features such as heart rate, respiration rate, skin temperature. Several studies (Vrij, 2001; Gannon et al., 2009; Derksen, 2012) indicated that relying solely on physiological measurements can be biased and misleading. Chittaranjan et al. (Chittaranjan and Hung, 2010) created an audio visual recording of the “Are you a Werewolf?” game in order to detect deceptive behaviour using non-verbal audio cue"
D15-1281,wittenburg-etal-2006-elan,0,\N,Missing
D15-1283,N12-1009,0,0.0446981,"Missing"
D15-1283,D14-1150,1,0.835021,"in the anchor text of the hyperlinks that point to the web page. Wan (2009) used co-training for cross-lingual sentiment classification of product reviews, where English and Chinese features were considered as two independent views of the data. Furthermore, Gollapalli et al. (2013) used co-training to identify authors’ homepages from the current-day university websites. The paper presents novel features, extracted from the URL of a page, that were used in conjunction with content features, forming two complementary views of the data. Citation networks have been used before in other problems. Caragea et al. (2014) used citation contexts to extract informative features for keyphrase extraction. Lu and Getoor (2003) proposed an approach for document classification that used only citation links, without any textual data from the citation contexts. Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010) and to study author influence in document networks 2358 (Kataria et al., 2011). Moreover, citation c"
D15-1283,C10-1101,0,0.0198185,"Lu and Getoor (2003) proposed an approach for document classification that used only citation links, without any textual data from the citation contexts. Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010) and to study author influence in document networks 2358 (Kataria et al., 2011). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990) For example, in Qazvinian et al. (2010), a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. Mei and Zhai (2008) used information from citation contexts to determine what sentences of a paper are of high impact (as measured by the influence of a target paper on further studies of similar or related topics). These sentences constitute the impact-"
D15-1283,W06-0804,0,0.0387512,", Gollapalli et al. (2013) used co-training to identify authors’ homepages from the current-day university websites. The paper presents novel features, extracted from the URL of a page, that were used in conjunction with content features, forming two complementary views of the data. Citation networks have been used before in other problems. Caragea et al. (2014) used citation contexts to extract informative features for keyphrase extraction. Lu and Getoor (2003) proposed an approach for document classification that used only citation links, without any textual data from the citation contexts. Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010) and to study author influence in document networks 2358 (Kataria et al., 2011). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990) For example, in Qazvinian et al. (2010), a set of important keyphrases is extracted"
D15-1283,W06-1613,0,0.352048,"Missing"
D15-1283,P08-1093,0,0.0148285,"ment classification that used only citation links, without any textual data from the citation contexts. Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010) and to study author influence in document networks 2358 (Kataria et al., 2011). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990) For example, in Qazvinian et al. (2010), a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. Mei and Zhai (2008) used information from citation contexts to determine what sentences of a paper are of high impact (as measured by the influence of a target paper on further studies of similar or related topics). These sentences constitute the impact-based summary of the paper. Despite the use of"
D15-1283,P09-1027,0,0.0251576,"requires a small amount of labeled data in order to make accurate topic classification. Semi-supervised methods essentially involve different means of transferring labels from labeled to unlabeled samples in the process of learning a classifier that can generalize well on new unseen data. Co-training was originally introduced in (Blum and Mitchell, 1998) where it was used to classify web pages into academic course home page or not. This approach has two views of the data as follows: the content of a web page, and the words found in the anchor text of the hyperlinks that point to the web page. Wan (2009) used co-training for cross-lingual sentiment classification of product reviews, where English and Chinese features were considered as two independent views of the data. Furthermore, Gollapalli et al. (2013) used co-training to identify authors’ homepages from the current-day university websites. The paper presents novel features, extracted from the URL of a page, that were used in conjunction with content features, forming two complementary views of the data. Citation networks have been used before in other problems. Caragea et al. (2014) used citation contexts to extract informative features"
D15-1283,C08-1087,0,0.0238308,"oposed an approach for document classification that used only citation links, without any textual data from the citation contexts. Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers. Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010) and to study author influence in document networks 2358 (Kataria et al., 2011). Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990) For example, in Qazvinian et al. (2010), a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary. Mei and Zhai (2008) used information from citation contexts to determine what sentences of a paper are of high impact (as measured by the influence of a target paper on further studies of similar or related topics). These sentences constitute the impact-based summary of the paper."
D15-1283,P11-1051,0,\N,Missing
D17-1242,P14-2134,0,0.503481,"o predict free association norms, namely given a word, to attempt to determine the most likely word that a human would associate with that stimulus. Large word association databases exist, such as the one collected by Deyne et al. (2013), who used a set of 12,000 stimulus words and surveyed 70,000 participants. Yet to our knowledge, no concerted attempt has been made to gather word associations jointly with the demographic characteristics of the people behind them. While not directly seeking to extract word associations but rather trying to represent language meaning through a locality lens, (Bamman et al., 2014) have proposed using distributed representations to model words employed by social media 2286 users from different US states. They were able to show that the regional meaning of words can successfully be carried by word embeddings, for example the word “wicked” was most similar to the word “evil” in Kansas, while in Massachusetts, it was most similar to “super” (based on the cosine similarity of the words’ vectorial representation). In contrast, our rationale in this article is to explore if word associations can be automatically derived from large corpora annotated with user-centered attribut"
D17-1242,W14-4708,0,0.0314739,"Missing"
D17-1242,D11-1098,0,0.0537079,"Missing"
D17-1242,W89-0240,0,0.128405,"ses were compared across languages and people with a relatively common origin (West European), our work seeks to investigate whether similar results are encountered when looking at different locations (namely US versus India). Furthermore, our study is conducted in English from the beginning, to eliminate a third party’s subjectivity in mapping primary responses from one language to another. There have also been attempts in computational linguistics to derive associations not based on survey results (which are static and resource intensive), but based on statistics derived from large corpora (Church et al., 1989; Wettler and Rapp, 1989; Church and Hanks, 1990). Research in semantic similarity can also be used to model associations based on several directions: (1) cooccurrence metrics that rely on large corpora such as PMI (Church and Hanks, 1990), second order PMI (Islam and Inkpen, 2008), or Dice (Dice, 1945); (2) distributional similarity-based measures, that characterize a word by its surrounding context such as LSA (Landauer and Dumais, 1997), ESA (Gabrilovich and Markovitch, 2007), or SSA (Hassan and Mihalcea, 2011); and (3) knowledge-based metrics that rely on resources such as lexica or thesau"
D17-1242,C16-1175,0,0.0619369,"Missing"
D17-1242,C16-1065,1,0.789643,"online nature of the survey, and since we aimed for a high quality dataset, each participant was presented with a set of 50 stimulus words at a time (instead of 100). The demographic section consisted of seven questions covering gender, age, location, occupation, ethnicity, education, and income. Stimuli. The stimulus list consists of a set of approximately 300 words. Among these, 99 words are sourced from the word list proposed by Kent and Rosanoff (1910) (standard list).2 The remaining words are identified using the method for finding word-usage differences between two groups introduced in (Garimella et al., 2016), which relies on large collections of texts authored by the two groups to identify words that can be accurately classified by an automatic classifier as belonging to one group versus another. Using their method, we obtain 100 words as the top most dif2 Note that this list originally included 100 words. The word “foot” was however misspelled in our survey, and instead we gathered answers for “food.” ferent words between US and India (culture list), and another set of 100 words as the top most different words between male and female (gender list). The reunion of these three lists results in 286"
D17-1242,P15-1073,0,0.0400489,"associations, and more advanced applications such as information retrieval (which relies heavily on word associations/similarity), demographic-aware keyword extraction, dialogue personalization, and so forth. 2285 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2285–2295 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Note that a few other researchers have explored demographic-aware NLP models with promising results, primarily focusing on the use of demographics for various forms of text classification (Hovy, 2015) or sentiment and subjectivity classification (Volkova et al., 2013). The paper makes several main contributions. First, we create a novel dataset of demographic-aware word associations, consisting of approximately 300 stimulus words along with 800 responses per word collected from a demographically-diverse group of respondents, for a total of 228,800 responses. Removing spam responses resulted in 176,097 responses. Analyses that we perform on this dataset demonstrate that indeed word associations vary across user dimensions.1 Second, we show that the associations we obtained follow the same p"
D17-1242,D07-1061,0,0.0513726,"milarity can also be used to model associations based on several directions: (1) cooccurrence metrics that rely on large corpora such as PMI (Church and Hanks, 1990), second order PMI (Islam and Inkpen, 2008), or Dice (Dice, 1945); (2) distributional similarity-based measures, that characterize a word by its surrounding context such as LSA (Landauer and Dumais, 1997), ESA (Gabrilovich and Markovitch, 2007), or SSA (Hassan and Mihalcea, 2011); and (3) knowledge-based metrics that rely on resources such as lexica or thesauri (Leacock and Chodorow, 1998; Lesk, 1986; Jarmasz and Szpakowics, 2003; Hughes and Ramag, 2007). However, most of these metrics have so far been applied to model the relatedness between two words, namely given a word pair, to score how similar the two words are; as such, they have not been used to predict free association norms, namely given a word, to attempt to determine the most likely word that a human would associate with that stimulus. Large word association databases exist, such as the one collected by Deyne et al. (2013), who used a set of 12,000 stimulus words and surveyed 70,000 participants. Yet to our knowledge, no concerted attempt has been made to gather word associations"
D17-1242,N10-1037,0,0.0682337,"Missing"
D17-1242,P14-5010,0,0.0139061,"and India. After removing the respondents who did not pass the spam-checking questions, we were left with an average of 752 responses per word, which we then balanced by gender, to retain an equal number of Indian women, Indian men, US women, and US men. This resulted in 492 and 480 responses for the two sets of 50 standard stimulus words, 436 and 468 for the culture words, and 440 and 432 for the gender words. Similar to (Rosenzweig, 1961), all the responses were normalized (i.e. plural was mapped to singular, gerund to infinitive, etc.); in our case we used the Stanford CoreNLP Lemmatizer (Manning et al., 2014), ultimately aggregating the responses into a gold standard. Table 1 shows the top associations for a few sample stimuli, as collected from India and US, and males and females. Finer-grained qualitative analyses also reveal interesting distinctions. For instance bath is overwhelmingly associated by men with water, while US women associate it with bubble, and Indian women with soap. Interestingly, US men seem to provide responses based on collocations, e.g., they answer Kane for citizen (citizen Kane), weight for heavy (heavyweight), or lion for mountain (mountain lion); on the contrary, women"
D17-1242,W11-0611,0,0.0574568,"Missing"
D17-1242,D13-1187,0,0.13677,"Missing"
D17-1242,J90-1003,0,\N,Missing
D17-1242,H89-2012,0,\N,Missing
D18-1280,H05-1073,0,0.0919643,"t and Section 4 describes our proposed approach; Section 5 provides details on experimental setup; Section 6 reports the results and related analysis; finally, Section 7 concludes the paper. 2 Related Works Emotion recognition is an interdisciplinary field of research with contributions from psychology, cognitive science, machine learning, natural language processing, and others (Picard, 2010). Initial research in this area primarily involved visual and audio processing (Ekman, 1993; Datcu and Rothkrantz, 2008). The role of text in emotional analysis became evident with later research such as Alm et al. (2005); Strapparava and Mihalcea (2010). Current research in this domain is mainly performed from a multimodal learning perspective (Poria et al., 2017a; Baltruˇsaitis et al., 2018). Numerous previous approaches have relied on fusion techniques that leverage multiple modalities for affect recognition (Soleymani et al., 2012; Zadeh et al., 2017; Chen et al., 2017; Tzirakis et al., 2017; Zadeh et al., 2018b). Understanding conversations is crucial for machines to replicate human language and discourse. Emotions play an important role in shaping such social interactions (Ruusuvuori, 2013). Richards et"
D18-1280,N18-1193,1,0.556494,"ar et al., 2015; Kumar et al., 2016), machine translation (Bahdanau et al., 2014), speech recognition (Graves et al., 2014), and others. In emotional analysis, Zadeh et al. (2018a) propose a memory-based sequential learning for multi-view signals. Although we utilize memory networks, our work is different as we use memories to encode whole utterances. Also, each memory cell in our network is processed using GRUs to capture temporal dependencies. This technique deviates from the traditional use of embedding matrices to encode information into memory cells. ICON builds on our previous research (Hazarika et al., 2018) that used separate memory networks for both interlocutors participating in a dyadic conversation. In contrast, ICON adopts an interactive scheme that actively models inter-speaker emotional dynamics with fewer trainable parameters. 2595 3 Problem Setting Let us define a conversation U to be a set of asynchronous exchange of utterances between two persons Pa and Pb over time. With T utterances, U = {u1 , u2 , ..., uT } is a totally ordered set which can be arranged as a sequence (u1 , ..., uT ) based on temporal occurrence. Here, each utterance ui is spoken by either Pa or Pb . Furthermore, fo"
D18-1280,P14-1062,0,0.0406397,"(D’mello and Kory, 2015). These features provide complementary information from heterogeneous sources which helps to accumulate comprehensive features. Its need is particularly pronounced in videos as they are often plagued with noisy signals and missing-information within individual modalities (e.g., facial occlusion, loud background music, imperfect transcriptions). 4.1.1 We employ a convolutional neural network (CNN) to extract textual features from the transcript of each utterance. CNNs are capable of learning abstract semantic representations of a sentence based on its words and n-grams (Kalchbrenner et al., 2014). For our purpose, we utilize a simple CNN with a single convolutional layer followed by max-pooling (Kim, 2014). The input to this network consists of pre-trained word embeddings extracted from the 300-dimensional FastText embeddings (Bojanowski et al., 2016). The convolution layer consists of three filters with sizes ft1 , ft2 , ft3 with fout feature maps each. We perform 1D convolutions using these filters followed by max-pooling on its output. The pooled features are finally projected onto a dense layer with dimension dt and its activations are used as the textual representation tu ∈ Rdt ."
D18-1280,D14-1181,0,0.0339938,"comprehensive features. Its need is particularly pronounced in videos as they are often plagued with noisy signals and missing-information within individual modalities (e.g., facial occlusion, loud background music, imperfect transcriptions). 4.1.1 We employ a convolutional neural network (CNN) to extract textual features from the transcript of each utterance. CNNs are capable of learning abstract semantic representations of a sentence based on its words and n-grams (Kalchbrenner et al., 2014). For our purpose, we utilize a simple CNN with a single convolutional layer followed by max-pooling (Kim, 2014). The input to this network consists of pre-trained word embeddings extracted from the 300-dimensional FastText embeddings (Bojanowski et al., 2016). The convolution layer consists of three filters with sizes ft1 , ft2 , ft3 with fout feature maps each. We perform 1D convolutions using these filters followed by max-pooling on its output. The pooled features are finally projected onto a dense layer with dimension dt and its activations are used as the textual representation tu ∈ Rdt . 4.1.2 Table 1: Sample conversation U with test utterance u7 . Context-window K = 5. Here, uλi = ith utterance b"
D18-1280,L16-1075,0,0.109286,"eech bounded by breaths or pauses of the speaker. Emotional dynamics in conversations consist of two important properties: self and inter-personal dependencies (Morris and Keltner, 2000). Self-dependencies, also known as emotional inertia, deal with the aspect of emotional influence that speakers have on themselves during conversations (Kuppens et al., 2010). On the other hand, inter-personal dependencies relate to the emotional influences that the counterparts induce into a speaker. Conversely, during the course of a dialogue, speakers also tend to mirror their counterparts to build rapport (Navarretta et al., 2016). Figure 1 demonstrates a sample conversation from the dataset involving both self and interpersonal dependencies. While most conversational frameworks only focus on self dependencies, ICON leverages both such dependencies to generate affective summaries of conversations. First, it extracts multimodal features from all utterancevideos. Next, given a test utterance to be classified, ICON considers the preceding utterances of both speakers falling within a context-window and models their self-emotional influences using local gated recurrent units (GRUs). Furthermore, to incorporate inter-speaker"
D18-1280,P17-1081,1,0.457447,"Missing"
D18-1280,P16-1226,0,0.0131204,"annels, height, width, and depth of the filter, respectively. After a non-linear reLU activation (LeCun et al., 2015), max-pooling is performed using a sliding window of dimensions (mp , mp , mp ). For an input utterance video, the final features of the third convolutional block is mapped onto a dense layer of dimension dv whose activations are used as the visual features vu ∈ Rdv . 4.1.4 Fusion We generate the final representation of an utterance u by concatenating all three multimodal features: u = tanh((W f [tu ; au ; vu ]) + bf ) (3) Concatenation is one of the most common fusion methods (Shwartz et al., 2016). Its simplicity also allows us to emphasize the contribution of the remaining components of ICON. 4.2 SIM: Self-Influence Module Given a test utterance ut to be classified, this module independently processes the histories of both speakers. SIM consists of two GRUs, GRUas and GRUbs , for Ha and Hb , respectively. For each λ ∈ {a, b}, GRUλs attempts to model the emotional inertia of speaker Pλ which represents the emotional dependency of a speaker with their own previous states. In particular, for each historical ut(j) terance ui<t ∈ Hλ , an internal memory state hλ is computed by GRUλs condit"
D19-1122,Q16-1026,0,0.020277,"4 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model"
D19-1122,N16-1030,0,0.0134605,"duce a corpus of 477 sentences, with 4154 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text"
D19-1122,P15-2047,0,0.0208728,"and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from volunteers through Amazon"
D19-1122,P16-1101,0,0.0191433,"sentences, with 4154 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encode"
D19-1122,P14-5010,0,0.00426101,"Missing"
D19-1122,P16-1105,0,0.0295519,"proaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from volunteers through Amazon Mechanical Turk (AMT) and from test cases of genetic counseling sessions (GCS).2 This questionnaire has been developed by cancer genetic counseling experts to construct a patient’s medical history (Wattendorf and Had"
D19-1122,D14-1162,0,0.0822833,"classification task. Each sentence is used to produce positive and negative instances for the relations it contains. For example, the sentence I have two sons John and Jim who are 23 and 26. will produce two positive instances: (John, age, 23) and (Jim, age, 26). Negative instances are produced by pairing up unrelated arguments (e.g. (John, age, 26)). We train a bi-directional LSTM classifier for this task and fine-tune the parameters using the validation set. The model is trained using the following feature representation: Word embeddings: we use pretrained 300 dimensional Glove embeddings (Pennington et al., 2014) Position features: binary flag representing the position of the target entities Annotation: the tag of the token from the entity identification step. 5 Results The system is trained on training data from both sources (AMT and GCS), and is evaluated on the test data. Since the relation classification model relies on the output of the entity identification model, we evaluate it using the automatically detected entities. Table 6 shows the performance of 1258 the tagger and relation classifier on the test set. We further analyze the performance of the relation classifier in two scenarios: i) when"
D19-1122,W18-5613,0,0.0221254,"llness history (a pre-specified set of 8 illnesses) in discharge summaries and outpatient visit notes. The system is run on 2000 reports randomly selected from 4 different hospitals’ clinical notes. 1000 of the relations detected by the system were manually inspected for evaluation. Lewis et al. (2011) rely on grammatical dependencies and patterns of dependency sequences to detect family history information, constraining one of the arguments of a relation to express a family member (e.g. mother, brother). The system is trained on 299 sentences, 77 of which contained 167 persondiagnosis pairs. Rama et al. (2018) iteratively develop an annotation schema and a synthetic corpus of clinical notes in Norwegian for family history annotation and extraction. They produce a corpus of 477 sentences, with 4154 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation argum"
D19-1122,W95-0107,0,0.190432,"n and extraction. They produce a corpus of 477 sentences, with 4154 entities and 2078 relations. Entity detection and relation extraction are performed separately, using an SVM with lexical, POS and dependency features. These approaches rely on heuristics to detect the arguments of relations: use a predefined list of family relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme fo"
D19-1122,P15-1061,0,0.0270666,"as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from volunteers through Amazon Mechanical Turk (AMT) and"
D19-1122,E12-2021,0,0.0642164,"Missing"
D19-1122,C14-1220,0,0.0138141,"amily relationships and diseases, or use as arguments the noun phrases that are detected close to the suspected relationship markers. Finding relation arguments is difficult because of their variable length. Sequence labelling approaches have proved successful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from voluntee"
D19-1122,P17-1113,0,0.0148418,"cessful for such problems (Ramshaw and Marcus, 1995; Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016). On the relation extraction side, the recently developed deep learning approaches have also proved useful, by successfully combining information about the meaning of the arguments, their context and their grammatical connections (Zeng et al., 2014; Liu et al., 2015; dos Santos et al., 2015). Determining the relation arguments and the relation itself depend on each other, so it would be beneficial to have an approach that performs these two tasks jointly (Miwa and Bansal, 2016). Zheng et al. (2017) propose a new tagging scheme for relations in a text, and use an encodedecoder LSTM-based model to predict for an input sentence the tags that combine relation and argument information. We will use a similar tag set, but in a two step approach. 3 Dataset Our dataset consists of answers to a family history questionnaire collected from volunteers through Amazon Mechanical Turk (AMT) and from test cases of genetic counseling sessions (GCS).2 This questionnaire has been developed by cancer genetic counseling experts to construct a patient’s medical history (Wattendorf and Hadley, 2005). It consis"
E09-1065,P07-1098,0,0.0079623,"feedback 20 Figure 1: Effect of relevance feedback on performance on Wikipedia with feedback from student answers, which was found to bring a significant absolute improvement on the 0-1 Pearson scale of 0.14 over the tf*idf baseline and 0.10 over the LSA BNC model that has been used in the past. In future work, we intend to expand our analysis of both the gold-standard answer and the student answers beyond the bag-of-words paradigm by considering basic logical features in the text (i.e., AND, OR, NOT) as well as the existence of shallow grammatical features such as predicateargument structure(Moschitti et al., 2007) as well as semantic classes for words. Furthermore, it may be advantageous to expand upon the existing measures by applying machine learning techniques to create a hybrid decision system that would exploit the advantages of each measure. The data set introduced in this paper, along with the human-assigned grades, can be downloaded from http://lit.csci.unt.edu/index.php/Downloads. ation of all the measures for the task of short answer grading. We filled this gap by running comparative evaluations of several knowledge-based and corpus-based measures on a data set of short student answers. Our r"
E09-1065,P94-1019,0,0.0764647,"length of the shortest path between two concepts using node-counting (including the end nodes). The Leacock & Chodorow (Leacock and Chodorow, 1998) similarity is determined as: Simlch = − log length 2∗D (2) where length is the length of the shortest path between two concepts using node-counting, and D is the maximum depth of the taxonomy. The Lesk similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu & Palmer (Wu and Palmer, 1994) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: Text-to-text Semantic Similarity We run our comparative evaluations using eight knowledge-based measures of semantic similarity (shortest path, Leacock & Chodorow, Lesk, Wu & Palmer, Resnik, Lin, Jiang & Conrath, Hirst & St. Onge), and two corpus-based measures (LSA and ESA). For the knowledge-based measures, we derive a text-to-text similarity metric by using the methodology proposed in (Mihalcea et al., 2006"
E09-1065,W99-0625,0,0.0635149,"2 Related Work There are a number of approaches that have been proposed in the past for automatic short answer grading. Several state-of-the-art short answer graders (Sukkarieh et al., 2004; Mitchell et al., 2002) require manually crafted patterns which, if matched, indicate that a question has been answered correctly. If an annotated corpus is availProceedings of the 12th Conference of the European Chapter of the ACL, pages 567–575, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 567 as information retrieval and text classification. Another approach (Hatzivassiloglou et al., 1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts). This method also attempts to account for synonymy, word ordering, text length, and word classes. Another line of work attempts to extrapolate text similarity from the arguably simpler problem of word similarity. (Mihalcea et al., 2006) explores the efficacy of applying WordNet-based word-to-word similarity measures (Pedersen et al., 2004) to the comparison of texts and found them generally comparable to corpus"
E09-1065,N04-1024,0,0.0226466,"these instances, we often have to turn to computerassisted assessment. While some forms of computer-assisted assessment do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers that consist of free text which require an analysis of the text in the answer. Research to date has concentrated on two main subtasks of computerassisted assessment: the grading of essays, which is done mainly by checking the style, grammaticality, and coherence of the essay (cf. (Higgins et al., 2004)), and the assessment of short student 2 Related Work There are a number of approaches that have been proposed in the past for automatic short answer grading. Several state-of-the-art short answer graders (Sukkarieh et al., 2004; Mitchell et al., 2002) require manually crafted patterns which, if matched, indicate that a question has been answered correctly. If an annotated corpus is availProceedings of the 12th Conference of the European Chapter of the ACL, pages 567–575, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 567 as information retrieval and"
E09-1065,O97-1002,0,0.152917,"are summed up and normalized with the length of the two input texts. We provide below a short description for each of these similarity metrics. Simres = IC(LCS) (4) where IC is defined as: IC(c) = − log P (c) (5) and P (c) is the probability of encountering an instance of concept c in a large corpus. 570 5.3 Implementation The measure introduced by Lin (Lin, 1998) builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simlin = 2 ∗ IC(LCS) IC(concept1 ) + IC(concept2 ) (6) We also consider the Jiang & Conrath (Jiang and Conrath, 1997) measure of similarity: Simjnc = 1 IC(concept1 ) + IC(concept2 ) − 2 ∗ IC(LCS) (7) Finally, we consider the Hirst & St. Onge (Hirst and St-Onge, 1998) measure of similarity, which determines the similarity strength of a pair of synsets by detecting lexical chains between the pair in a text using the WordNet hierarchy. For the knowledge-based measures, we use the WordNet-based implementation of the word-toword similarity metrics, as available in the WordNet::Similarity package (Patwardhan et al., 2003). For latent semantic analysis, we use the InfoMap package.5 For ESA, we use our own implement"
E09-1065,W03-0208,0,0.0161522,"nd them generally comparable to corpus-based measures such as LSA. An interesting study has been performed at the University of Adelaide (Lee et al., 2005), comparing simpler word and n-gram feature vectors to LSA and exploring the types of vector similarity metrics (e.g., binary vs. count vectors, Jaccard vs. cosine vs. overlap distance measure, etc.). In this case, LSA was shown to perform better than the word and n-gram vectors and performed best at around 100 dimensions with binary vectors weighted according to an entropy measure, though the difference in measures was often subtle. SELSA (Kanejiya et al., 2003) is a system that attempts to add context to LSA by supplementing the feature vectors with some simple syntactical features, namely the part-of-speech of the previous word. Their results indicate that SELSA does not perform as well as LSA in the best case, but it has a wider threshold window than LSA in which the system can be used advantageously. Finally, explicit semantic analysis (ESA) (Gabrilovich and Markovitch, 2007) uses Wikipedia as a source of knowledge for text similarity. It creates for each text a feature vector where each feature maps to a Wikipedia article. Their preliminary expe"
E09-1065,W05-0202,0,\N,Missing
E14-1029,E09-1004,0,0.0182455,"iting scalability. As a countermeasure, in (Akkaya et al., 2011), we showed that non-expert annotations collected through Amazon Mechanical Turk (MTurk) can replace expert annotations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentiment, and subjectivity analysis. These systems typically look for the presence of clues in text. Recently, in (Akkaya et al., 2009), we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not – words in such lexicons may have both subjective and objective usages. We call this problem subjectivity sense ambiguity. Consider the following sentence containing the clue “attack”: (1) A new treatment based on training T-cells to attack cancerous cells ... He was attacked by Milosevic for attempting to carve out a new"
E14-1029,D09-1020,1,0.906521,"notations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentiment, and subjectivity analysis. These systems typically look for the presence of clues in text. Recently, in (Akkaya et al., 2009), we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not – words in such lexicons may have both subjective and objective usages. We call this problem subjectivity sense ambiguity. Consider the following sentence containing the clue “attack”: (1) A new treatment based on training T-cells to attack cancerous cells ... He was attacked by Milosevic for attempting to carve out a new party from the Socialists. 269 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 269–278, c Gothenburg, Swe"
E14-1029,J04-3001,0,0.0516427,"Missing"
E14-1029,W11-0311,1,0.87743,"d sentiment analysis resulting in substantial improvement for both subjectivity and sentiment analysis by avoiding false hits. Although SWSD is a promising tool, it suffers from the knowledge acquisition bottleneck. SWSD is defined as a supervised task, and follows a targeted approach common in the WSD literature for performance reasons. This means, for each target clue, a different classifier is trained requiring separate training data for each target clue. It is expensive and time-consuming to obtain annotated datasets to train SWSD classifiers limiting scalability. As a countermeasure, in (Akkaya et al., 2011), we showed that non-expert annotations collected through Amazon Mechanical Turk (MTurk) can replace expert annotations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentim"
E14-1029,W02-0811,0,0.130092,"an be different usages, both having a subjective meaning. On the other hand, if two instances are labeled having opposing labels, we do not want them to be in the same cluster. Thus, we utilize cannot-links but not must-links. Constraints can be obtained from domain knowledge or from available instance labels. In our work, constraints are generated from instance labels. Each instance pair with opposing labels is considered to be cannot-linked. There are two general strategies to incorporate constraints into clustering. The first is to adapt the similarity between instances (Xing et al., 2002; Klein et al., 2002) by adjusting the underlying distance metric. The main idea is to make the distance between must-linked instances – their neighbourhoods – smaller and the distance between cannot-linked instances – their neighbourhoods – larger. The second strategy is modifying the clustering algorithm itself so that search is biased towards a partitioning for which the constraints hold (Wagstaff and Cardie, 2000; Basu et al., 2002; Demiriz et al., 1999). Our proposed constrained clustering method relies on some ideas from (Klein et al., 2002). Thus, we explain it in more detail. (Klein et al., 2002) utilizes"
E14-1029,C04-1200,0,0.0249805,"ime-consuming to obtain annotated datasets to train SWSD classifiers limiting scalability. As a countermeasure, in (Akkaya et al., 2011), we showed that non-expert annotations collected through Amazon Mechanical Turk (MTurk) can replace expert annotations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentiment, and subjectivity analysis. These systems typically look for the presence of clues in text. Recently, in (Akkaya et al., 2009), we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not – words in such lexicons may have both subjective and objective usages. We call this problem subjectivity sense ambiguity. Consider the following sentence containing the clue “attack”: (1) A new treatment based on training T-cells to attack cancero"
E14-1029,P08-1034,0,0.0284671,"ts to train SWSD classifiers limiting scalability. As a countermeasure, in (Akkaya et al., 2011), we showed that non-expert annotations collected through Amazon Mechanical Turk (MTurk) can replace expert annotations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentiment, and subjectivity analysis. These systems typically look for the presence of clues in text. Recently, in (Akkaya et al., 2009), we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not – words in such lexicons may have both subjective and objective usages. We call this problem subjectivity sense ambiguity. Consider the following sentence containing the clue “attack”: (1) A new treatment based on training T-cells to attack cancerous cells ... He was attacked by Milosevic for attemp"
E14-1029,C02-1039,1,0.645235,"equent lemmas). We do not filter out stop words, since they have been shown to be useful for various semantic similarity tasks in (Bullinaria and Levy, 2007). We use positive point-wise mutual information to compute values of the vector components, which has also been shown to be favourable in (Bullinaria and Levy, 2007). Purandere and Pedersen is the prominent representative of feature-based models. (Purandare and Pedersen, 2004) creates context vectors from local feature representations similar to the feature vectors found in supervised WSD. In this work, we use the following features from (Mihalcea, 2002) to build the local feature representation: (1) the target word itself and its part of speech, (2) surrounding context of 3 words and their part of speech, (3) the head of the noun phrase, (4) the first noun and verb before the target word, (5) the first noun and verb after the target word. average appear-v fine-a interest-n restraint-n skew local dsm add dsm mul mix rep 79.90 80.50 80.50 83.53 85.23 53.83 54.85 54.85 57.40 69.39 70.07 72.26 70.07 74.45 75.18 54.41 54.78 55.88 81.62 81.62 70.45 71.97 75.00 71.21 81.82 Table 1: Evaluation of Various Context Representations 3.1 Evaluation of Con"
E14-1029,W04-2406,0,0.256427,"respond to word lemmas present in the corpus. We adopt the parameters for our semantic space from (Mitchell and Lapata, 2010): window size of 10 and dimension size of 2000 (i.e., the 2000 most frequent lemmas). We do not filter out stop words, since they have been shown to be useful for various semantic similarity tasks in (Bullinaria and Levy, 2007). We use positive point-wise mutual information to compute values of the vector components, which has also been shown to be favourable in (Bullinaria and Levy, 2007). Purandere and Pedersen is the prominent representative of feature-based models. (Purandare and Pedersen, 2004) creates context vectors from local feature representations similar to the feature vectors found in supervised WSD. In this work, we use the following features from (Mihalcea, 2002) to build the local feature representation: (1) the target word itself and its part of speech, (2) surrounding context of 3 words and their part of speech, (3) the head of the noun phrase, (4) the first noun and verb before the target word, (5) the first noun and verb after the target word. average appear-v fine-a interest-n restraint-n skew local dsm add dsm mul mix rep 79.90 80.50 80.50 83.53 85.23 53.83 54.85 54."
E14-1029,N07-1039,0,0.0158411,"ain annotated datasets to train SWSD classifiers limiting scalability. As a countermeasure, in (Akkaya et al., 2011), we showed that non-expert annotations collected through Amazon Mechanical Turk (MTurk) can replace expert annotations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentiment, and subjectivity analysis. These systems typically look for the presence of clues in text. Recently, in (Akkaya et al., 2009), we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not – words in such lexicons may have both subjective and objective usages. We call this problem subjectivity sense ambiguity. Consider the following sentence containing the clue “attack”: (1) A new treatment based on training T-cells to attack cancerous cells ... He was"
E14-1029,W03-1014,1,0.601385,"aining data for each target clue. It is expensive and time-consuming to obtain annotated datasets to train SWSD classifiers limiting scalability. As a countermeasure, in (Akkaya et al., 2011), we showed that non-expert annotations collected through Amazon Mechanical Turk (MTurk) can replace expert annotations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentiment, and subjectivity analysis. These systems typically look for the presence of clues in text. Recently, in (Akkaya et al., 2009), we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not – words in such lexicons may have both subjective and objective usages. We call this problem subjectivity sense ambiguity. Consider the following sentence containing the clue “attack”: (1) A new tr"
E14-1029,J98-1004,0,0.77205,"ubjectivity sense tagged data). We train a different SWSD classifier for each target word as in (Akkaya et al., 2009). Thus, we need a different training dataset for each target word. Our ultimate 3 Context Representations There has been much work on context representations of words for various NLP tasks. Clustering word instances in order to discriminate senses of a word is called Word Sense Discrimination. Context representations for this task rely on two main types of models: distributional semantic models (DSM) and feature-based models. 1 Available corpora 270 at http://mpqa.cs.pitt.edu/ (Schutze, 1998), which is still a competitive model for word-sense discrimination by context clustering, relies on a distributional semantic model (DSM) (Turney and Pantel, 2010; Sahlgren, 2006; Bullinaria and Levy, 2007). A DSM is usually a word-to-word co-occurrence matrix – also called semantic space – such that each row represents the distribution of a target word in a large text corpus. Each row gives the semantic signature of a word, which is basically a high dimensional numeric vector. Note that this high dimensional vector represents word types, not word tokens. Thus, it cannot model a word instance"
E14-1029,D08-1112,0,0.0349391,"Missing"
E14-1029,P02-1053,0,0.0112679,"fier is trained requiring separate training data for each target clue. It is expensive and time-consuming to obtain annotated datasets to train SWSD classifiers limiting scalability. As a countermeasure, in (Akkaya et al., 2011), we showed that non-expert annotations collected through Amazon Mechanical Turk (MTurk) can replace expert annotations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentiment, and subjectivity analysis. These systems typically look for the presence of clues in text. Recently, in (Akkaya et al., 2009), we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not – words in such lexicons may have both subjective and objective usages. We call this problem subjectivity sense ambiguity. Consider the following sent"
E14-1029,W03-1017,0,0.0555593,"get clue. It is expensive and time-consuming to obtain annotated datasets to train SWSD classifiers limiting scalability. As a countermeasure, in (Akkaya et al., 2011), we showed that non-expert annotations collected through Amazon Mechanical Turk (MTurk) can replace expert annotations successfully and might be used to apply SWSD on a large scale. Although non-expert annotations are cheap and fast, they still incur some cost. In this work, we aim to reduce the human annotation effort needed Introduction Subjectivity lexicons (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)) play an important role in opinion, sentiment, and subjectivity analysis. These systems typically look for the presence of clues in text. Recently, in (Akkaya et al., 2009), we showed that subjectivity clues are fairly ambiguous as to whether they express subjectivity or not – words in such lexicons may have both subjective and objective usages. We call this problem subjectivity sense ambiguity. Consider the following sentence containing the clue “attack”: (1) A new treatment based on training T-cel"
E14-1029,W12-3702,1,\N,Missing
E14-1029,S01-1021,0,\N,Missing
E17-1106,Q16-1033,0,0.0798123,"ng MI encounters. They rely on the psycholinguistic categories from the Linguistic Inquiry and Word Count lexicon to measure the degree in which counselor language matches the client language. Also related to our research is work on the social interaction domain. (Danescu-NiculescuMizil et al., 2012) studied power differences from language coordination in group discussions by measuring the similarity of word usage across different linguistic categories. Stylistic influence and symmetry have also been explored in social media interactions (Danescu-Niculescu-Mizil et al., 2011). More recently, (Althoff et al., 2016) explored these phenomena in the mental health domain by analyzing text-message-based counseling and observed that counselors who are more successful act with more control in the conversations and coordinate in a lower degree than their less successful counterparts. In summary, research findings have shown that natural language processing approaches can be successfully applied to clinical narratives for the automatic annotation and analysis of therapists’ and clients’ behaviors. However, developed methods have not yet explored the use of linguistic features that incorporate semantic or syntact"
E17-1106,W15-1209,0,0.0606985,"pply and evaluate our system on the other MI behaviors measures by the MITI coding scheme. Speech and linguistic based methods have also been proposed to evaluate overall MI quality. For instance, (Xiao et al., 2014) presents a study on the automatic evaluation of counselor empathy. The method is based on analyzing correlations between prosody patterns and empathy showed by the therapist during the counseling interactions. Although most of the work on coding of MI within session language has focused on modeling the counselor language, there is also work that investigates the client language. (Tanana et al., 2015) addresses the identification of counselor’s statements discussing client’s change talk. Their approach uses recursive neural networks to model sequences of counselor and client verbal exchanges. (Lord et al., 2015b) analyze the language style synchrony between therapist and client during MI encounters. They rely on the psycholinguistic categories from the Linguistic Inquiry and Word Count lexicon to measure the degree in which counselor language matches the client language. Also related to our research is work on the social interaction domain. (Danescu-NiculescuMizil et al., 2012) studied pow"
E17-2022,N13-1121,0,0.0436872,"Missing"
E17-2022,P11-1137,0,0.0143765,"differentiated by drugs. Our research is also related to the broad theme of latent user attribute prediction, which is an emerging task within the natural language processing community, having recently been employed in fields such as public health (Coppersmith et al., 2015) and politics (Conover et al., 2011; Cohen and Ruths, 2013). Some of the attributes targeted for extraction focus on demographic related information, such as gender/age (Koppel et al., 2002; Mukherjee and Liu, 2010; Burger et al., 2011; Van Durme, 2012; Volkova et al., 2015), race/ethnicity (Pennacchiotti and Popescu, 2011; Eisenstein et al., 2011; Rao et al., 2011; Volkova et al., 2015), location (Bamman et al., 2014), yet other aspects are mined as well, among them emotion and sentiment (Volkova et al., 2015), personality types (Schwartz et al., 2013; Volkova et al., 2015), user political affiliation (Cohen and Ruths, 2013; Volkova and Durme, 2015), mental health diagnosis (Coppersmith et al., 2015) and even lifestyle choices such as coffee preference (Pennacchiotti and Popescu, 2011). The task is typically approached from a machine learning perspective, with data originating from a variety of user generated content, most often micro"
E17-2022,D12-1135,0,0.0192008,"n et al., 2014), social posts (originating from sites such as Facebook, MySpace, Google+) (Gong et al., 2012), or discussion forums on particular topics (Gottipati et al., 2014). Classification labels are then assigned either based on manual annotations (Volkova et al., 2015), self identified user attributes (Pennacchiotti and Popescu, 2011), affiliation with a given discussion forum type, or online surveys set up to link a social media user identification to the responses provided (Schwartz et al., 2013). Learning has typically employed bagof-words lexical features (ngrams) (Van Durme, 2012; Filippova, 2012; Nguyen et al., 2013), with some works focusing on deriving additional signals from the underlying social network structure (Pennacchiotti and Popescu, 2011; Yang et al., 2011; Gong et al., 2012; Volkova and Durme, 2015), syntactic and stylistic features (Bergsma et al., 2012), or the intrinsic social media generation dynamic (Volkova and Durme, 2015). We should note that some works have also explored unsupervised approaches for demographic dimensions extraction, among them large-scale clustering (Bergsma et al., 2013) and probabilistic graphical models (Eisenstein et al., 2010). of research"
E17-2022,D11-1120,0,0.0366011,"classifier over 1,000 random-collected reports of the website www.erowid.org they identified subsets of words differentiated by drugs. Our research is also related to the broad theme of latent user attribute prediction, which is an emerging task within the natural language processing community, having recently been employed in fields such as public health (Coppersmith et al., 2015) and politics (Conover et al., 2011; Cohen and Ruths, 2013). Some of the attributes targeted for extraction focus on demographic related information, such as gender/age (Koppel et al., 2002; Mukherjee and Liu, 2010; Burger et al., 2011; Van Durme, 2012; Volkova et al., 2015), race/ethnicity (Pennacchiotti and Popescu, 2011; Eisenstein et al., 2011; Rao et al., 2011; Volkova et al., 2015), location (Bamman et al., 2014), yet other aspects are mined as well, among them emotion and sentiment (Volkova et al., 2015), personality types (Schwartz et al., 2013; Volkova et al., 2015), user political affiliation (Cohen and Ruths, 2013; Volkova and Durme, 2015), mental health diagnosis (Coppersmith et al., 2015) and even lifestyle choices such as coffee preference (Pennacchiotti and Popescu, 2011). The task is typically approached fro"
E17-2022,P15-2100,0,0.0306003,"e should note that some works have also explored unsupervised approaches for demographic dimensions extraction, among them large-scale clustering (Bergsma et al., 2013) and probabilistic graphical models (Eisenstein et al., 2010). of research on the state of consciousness are focused on alcoholic intoxication and mostly performed on the Alcohol Language Corpus (Schiel et al., 2012), only available in German: for example, speech analysis (Wang et al., 2013; Bone et al., 2014) and a text based system (Jauch et al., 2013) were used to analyse this data. Regarding alcohol intoxication detection, (Joshi et al., 2015) developed a system for automatic detection of drunk people by using their posts on Twitter. (Bedi et al., 2014) performed their analysis on transcriptions from a free speech task, in which the participants were volunteers previously administered with a dose of MDMA (3,4methylenedioxy-methamphetamine). Even if this is an ideal case study for analyzing cognitively the intoxication state, it is difficult to replicate on a large scale. Finally, as far as we know, the only attempt to classify and characterize experiences over different kinds of drugs was the project of (Coyle et al., 2012). Using"
E17-2022,W15-1201,0,0.0244675,"ication state, it is difficult to replicate on a large scale. Finally, as far as we know, the only attempt to classify and characterize experiences over different kinds of drugs was the project of (Coyle et al., 2012). Using a random-forest classifier over 1,000 random-collected reports of the website www.erowid.org they identified subsets of words differentiated by drugs. Our research is also related to the broad theme of latent user attribute prediction, which is an emerging task within the natural language processing community, having recently been employed in fields such as public health (Coppersmith et al., 2015) and politics (Conover et al., 2011; Cohen and Ruths, 2013). Some of the attributes targeted for extraction focus on demographic related information, such as gender/age (Koppel et al., 2002; Mukherjee and Liu, 2010; Burger et al., 2011; Van Durme, 2012; Volkova et al., 2015), race/ethnicity (Pennacchiotti and Popescu, 2011; Eisenstein et al., 2011; Rao et al., 2011; Volkova et al., 2015), location (Bamman et al., 2014), yet other aspects are mined as well, among them emotion and sentiment (Volkova et al., 2015), personality types (Schwartz et al., 2013; Volkova et al., 2015), user political af"
E17-2022,P09-2078,1,0.775812,"ycholinguistic word classes according to the Linguistic Inquiry and Word Count (LIWC) lexicon – a resource developed by Pennebaker and colleagues (Pennebaker and Francis, 1999). The 2015 version of LIWC includes 19,000 words and word stems grouped into 73 broad categories relevant to psychological processes. The LIWC lexicon has been validated by showing significant correlation between human ratings of a large number of written texts and the rating obtained through LIWC-based analyses of the same texts. For each drug type T , we calculate the dominance score associated with each LIWC class C (Mihalcea and Strapparava, 2009). This score is calculated as the ratio between the percentage of words that appear in T and belong to C, and the percentage of words that appear in any other drug type but T and belong to C. A score significantly higher than 1 indicates a LIWC class that is dominant for the drug type T , and thus likely to be a characteristic of the experiences reported by users of this drug. Table 5 shows the top five dominant psycholinguistic word classes associated with each drug type. Interestingly, descriptions of experiences reported by users of empathogens are centered around people (e.g., Affiliation"
E17-2022,D10-1124,0,0.117264,"Missing"
E17-2022,strapparava-valitutti-2004-wordnet,1,0.31404,"ening I do a line whenever I feel like it. At bedtime I tell myself over and over that it’s time to go to sleep. Sometimes I sleep but if I can’t I know I have my friend to help me through the next day. Table 2: Sample entries in the drug dataset. EMP HAL SED STI micro-average Prec. 0.84 0.93 0.86 0.73 Rec. 0.71 0.92 0.86 0.85 F1 0.77 0.92 0.86 0.78 0.88 methodology similar to the one described above, and calculate the dominance score for each of six emotion word classes: anger, disgust, fear, joy, sadness, and surprise (Ortony et al., 1987; Ekman, 1993). As a resource, we use WordNet Affect (Strapparava and Valitutti, 2004), in which words from WordNet are annotated with several emotions. As before, the dominance scores are calculated for the experiences reported for each drug type when compared to the other drug types. Table 7 shows the scores for the four drug types and the six emotions. A score significantly higher than 1 indicates a class that is dominant in that category. Clearly, interesting differences emerge from this table: the use of emphathogens leads to experiences that are high on joy and surprise, whereas the dominant emotion in the use of hallucinogens as compared to the other drugs is fear. Sedat"
E17-2022,D12-1005,0,0.04849,"Missing"
E17-2022,N13-1017,0,0.0713576,"Missing"
E17-2022,W14-2701,0,0.0120002,"n directions 1 www.erowid.org: 95000 unique visitor per day; www.drugs-forum.com: 210000 members with 3.6 million unique visitor per month; www.psychonaut.com: 46000 members. 2 http://medicine.wright.edu/citar/edrugtrends http://medicine.wright.edu/citar/nida-national-earlywarning-system-network-in3-an-innovative-approach 3 136 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 136–142, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ments to news stories or op-ed pieces (Riordan et al., 2014), social posts (originating from sites such as Facebook, MySpace, Google+) (Gong et al., 2012), or discussion forums on particular topics (Gottipati et al., 2014). Classification labels are then assigned either based on manual annotations (Volkova et al., 2015), self identified user attributes (Pennacchiotti and Popescu, 2011), affiliation with a given discussion forum type, or online surveys set up to link a social media user identification to the responses provided (Schwartz et al., 2013). Learning has typically employed bagof-words lexical features (ngrams) (Van Durme, 2012; Filippova, 2012"
E17-2022,P14-2134,0,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,W11-0104,1,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,S07-1016,0,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,D08-1014,1,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,P10-1023,0,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,J94-4003,0,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,J04-1001,0,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,P02-1033,0,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,P03-1058,0,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,C10-1004,1,\N,Missing
fernandez-ordonez-etal-2012-unsupervised,vasilescu-etal-2004-evaluating,0,\N,Missing
H05-1052,H92-1046,0,0.213128,"ithms rely on the same knowledge source, i.e. dictionary definitions, and thus they are directly comparable. Moreover, none of the algorithms take into account the dictionary sense order (e.g. the most frequent sense provided by WordNet), and therefore they are both fully unsupervised. Table 1 shows precision and recall figures 4 for a 3 Given a sequence of words, the original Lesk algorithm attempts to identify the combination of word senses that maximizes the redundancy (overlap) across all corresponding definitions. The algorithm was later improved through a method for simulated annealing (Cowie et al., 1992), which solved the combinatorial explosion of word senses, while still finding an optimal solution. However, recent comparative evaluations of different variants of the Lesk algorithm have shown that the performance of the original algorithm is significantly exceeded by an algorithm variation that relies on the overlap between word senses and current context (Vasilescu et al., 2004). We are thus using this latter Lesk variant in our implementation. 4 Recall is particularly low for each individual part-of-speech because it is calculated with respect to the entire data set. The overall precision"
H05-1052,A92-1018,0,0.0239269,"plication at hand and on the knowledge sources that are available. If an annotated corpus is available, dependencies can be defined as label co-occurrence probabilities approximated with t , l s ), or as conditional probfrequency counts P (lw wj i t |l s ). Optionally, these dependencies abilities P (lw w j i can be lexicalized by taking into account the corret |l s ) × sponding words in the sequence, e.g. P (l w i wj t ). In the absence of an annotated corpus, deP (wi |lw i pendencies can be derived by other means, e.g. partof-speech probabilities can be approximated from a raw corpus as in (Cutting et al., 1992), word-sense dependencies can be derived as definition-based similarities, etc. Label dependencies are set as weights on the arcs drawn between corresponding labels. Arcs can be directed or undirected for joint probabilities or similarity measures, and are usually directed for conditional probabilities. 2.5 Labeling Example Consider again the example from Figure 1, consisting of a sequence of four words, and their possible corresponding labels. In the first step of the algorithm, label dependencies are determined, and let us assume that the values for these dependencies are as indicated throug"
H05-1052,S01-1026,0,0.0254425,"Missing"
H05-1052,W04-0837,0,0.038305,"Missing"
H05-1052,C04-1162,1,0.720146,"Missing"
H05-1052,H93-1061,0,0.490038,"Missing"
H05-1052,S01-1005,0,0.0561819,"unning the ranking algorithm, scores are identified for each word-sense in the graph, indicated between brackets next to each node. Selecting for each word the sense with the largest score results in the following sense assignment: The church#2 bells#1 were also made available for this data. no longer rung#3 on Sundays#1, which is correct according to annotations performed by professional lexicographers. 3.3 Results and Discussion The algorithm was primarily evaluated on the S ENSEVAL -2 English all-words data set, consisting of three documents from Penn Treebank, with 2,456 open-class words (Palmer et al., 2001). Unlike other sense-annotated data sets, e.g. S ENSEVAL -3 or SemCor, S ENSEVAL -2 is the only testbed for all-words word sense disambiguation that includes a sense map, which allows for additional coarse-grained sense evaluations. Moreover, there is a larger body of previous work that was evaluated on this data set, which can be used as a base of comparison. The performance of our algorithm is compared with the disambiguation accuracy obtained with a variation of the Lesk algorithm3 (Lesk, 1986), which selects the meaning of an open-class word by finding the word sense that leads to the high"
H05-1052,W04-0811,0,0.150091,"Missing"
H05-1052,vasilescu-etal-2004-evaluating,0,0.0250696,"Missing"
H05-1052,J91-1002,0,\N,Missing
hokamp-etal-2014-modeling,W02-1001,0,\N,Missing
hokamp-etal-2014-modeling,C04-1129,0,\N,Missing
I05-2004,W04-3247,0,0.756899,"new data type. In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data. Additionally, we also show that a layered application of this singledocument summarization method can result into an efficient multi-document summarization tool. 19 Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004), were either limited to singledocument English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone. In this paper, we show that a method exclusively based on graph-based algorithms can be successfully applied to the summarization of single and multiple documents in any language, and show that the results are competitive with those of state-of-the-art summarization systems. The paper is organized as follows. Section 2 b"
I05-2004,N03-1020,0,0.267495,"ted automatic text summarization. As a consequence, there is a large body of work on algorithms for extractive summarization undertaken as part of the DUC evaluation exercises (http://www-nlpir.nist.gov/projects/duc/). Previous approaches include supervised learning (Hirao et al., 2002), (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, intradocument similarities (Salton et al., 1997), or graph algorithms (Mihalcea and Tarau, 2004), (Erkan and Radev, 2004), (Wolf and Gibson, 2004). It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for text summarization, Single document summarization algo. P ageRank W -U P ageRank W -DB W HIT SA -U W HIT SA -DB P ageRank W -U 35.52 35.02 33.68 35.72 “Meta” summarization algorithm W -U P ageRank W -DB HIT SA 34.99 34.56 34.48 35.19 32.59 32.12 35.20 34.62 W -DB HIT SA 34.65 34.39 34.23 34.73 Table 4: Results for multi-document summarization (U = Undirected; DB = Directed Backward) which emphasizes the need of accurate tools for sentence extraction as an integral part of automatic summarization systems. 5 Conclu"
I05-2004,W03-0510,0,0.00617875,"ted automatic text summarization. As a consequence, there is a large body of work on algorithms for extractive summarization undertaken as part of the DUC evaluation exercises (http://www-nlpir.nist.gov/projects/duc/). Previous approaches include supervised learning (Hirao et al., 2002), (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, intradocument similarities (Salton et al., 1997), or graph algorithms (Mihalcea and Tarau, 2004), (Erkan and Radev, 2004), (Wolf and Gibson, 2004). It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for text summarization, Single document summarization algo. P ageRank W -U P ageRank W -DB W HIT SA -U W HIT SA -DB P ageRank W -U 35.52 35.02 33.68 35.72 “Meta” summarization algorithm W -U P ageRank W -DB HIT SA 34.99 34.56 34.48 35.19 32.59 32.12 35.20 34.62 W -DB HIT SA 34.65 34.39 34.23 34.73 Table 4: Results for multi-document summarization (U = Undirected; DB = Directed Backward) which emphasizes the need of accurate tools for sentence extraction as an integral part of automatic summarization systems. 5 Conclu"
I05-2004,C04-1162,1,0.23662,"s are presented in Section 4, followed by discussions, pointers to related work, and conclusions. 2 Iterative Graph-based Algorithms for Extractive Summarization In this section, we shortly describe two graph-based ranking algorithms and their application to the task of extractive summarization. Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998), have been traditionally and successfully used in Web-link analysis (Brin and Page, 1998), social networks, and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al., 2004), (Erkan and Radev, 2004). In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information. The basic idea implemented by the ranking model is that of “voting” or “recommendation”. When one vertex links to another one, it is basically casting a vote for that other vertex. The higher the number of votes that are cast for a vertex, the higher the importance of the vertex. Let G = (V, E) be a directed graph"
I05-2004,W97-0710,0,0.343478,"techniques for single-document summarization can be turned into an effective method for multi-document summarization. 1 Introduction Algorithms for extractive summarization are typically based on techniques for sentence extraction, and attempt to identify the set of sentences that are most important for the overall understanding of a given document. Some of the most successful approaches consist of supervised algorithms that attempt to learn what makes a good summary by training on collections of summaries built for a relatively large number of training documents, e.g. (Hirao et al., 2002), (Teufel and Moens, 1997). However, the price paid for the high performance of such supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new data type. In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data. Additionally, we also show that a layered application of this singledocument summarization method can result into a"
I05-2004,W04-3252,1,\N,Missing
I05-2004,W04-1004,0,\N,Missing
I05-2004,P04-1049,0,\N,Missing
I11-1162,W11-2503,0,0.425907,"tural language processing such as word relatedness. While we are aware that the limited coverage of ImageNet restricts the applicability of this hybrid image-text method to word relatedness, the continued growth of this resource should provide alleviation. Future work will also consider a comparison of multi-way combinations between knowledgebased, corpus-based and image-based metrics for further advancement of the state-of-the-art. 7 Related Work Recently, some attention has been given to modelling synergistic relationships between the semantics of words and images (Leong and Mihalcea, 2011; Bruni et al., 2011). The research that is most closely related to ours is the work of (Feng and Lapata, 2010), where it has been shown that it is possible to combine visual representations of word meanings into a joint bimodal representation constructed by using probabilistic generative latent topic models. Unlike our approach, however, (Feng and Lapata, 2010) relied on a news corpus where images and words in a document are assumed to be generated by a set of latent topics, rather than a lexical resource such as ImageNet. While they provided a proof-of-concept that using the visual modality leads to an improveme"
I11-1162,N10-1011,0,0.429968,"coverage of ImageNet restricts the applicability of this hybrid image-text method to word relatedness, the continued growth of this resource should provide alleviation. Future work will also consider a comparison of multi-way combinations between knowledgebased, corpus-based and image-based metrics for further advancement of the state-of-the-art. 7 Related Work Recently, some attention has been given to modelling synergistic relationships between the semantics of words and images (Leong and Mihalcea, 2011; Bruni et al., 2011). The research that is most closely related to ours is the work of (Feng and Lapata, 2010), where it has been shown that it is possible to combine visual representations of word meanings into a joint bimodal representation constructed by using probabilistic generative latent topic models. Unlike our approach, however, (Feng and Lapata, 2010) relied on a news corpus where images and words in a document are assumed to be generated by a set of latent topics, rather than a lexical resource such as ImageNet. While they provided a proof-of-concept that using the visual modality leads to an improvement over their purely text-based model (an increase of Spearman correlation of 0.071 on a s"
I11-1162,O97-1002,0,0.0426738,"to finding the maximum visual relatedness between all the possible pairings of synsets representing both words, using the cosine similarity between the visual vectors of the synsets, given below. The dimensionality of the vector, n, is set to 1000, which is the size of the visual codeword vocabulary. Simimg (wi , wj ) = max vk ∈Si ,vm ∈Sj qP Pn p p p=1 vk vm qP p 2 p 2 n n p=1 (vk ) p=1 (vm ) Text Metric: For a comparative study, we evaluate several knowledge-based methods, including Roget and WordNet Edges (Jarmasz, 2003), H&S (Hirst and St-Onge, 1998), L&C (Leacock and Chodorow, 1998), J&C (Jiang and Conrath, 1997), LIN (Lin, 1998), RES (Resnik, 1995), and two corpus-based methods Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). Combined Metric: In the combined setting, we attempt to integrate the output of our imagebased metric with that of existing text-based metrics in a pairwise manner via two combination functions, which were previously noted for 2 Note that a word may appear as surface forms across multiple synsets. In such cases, we randomly sample 50 images from each of the synsets 3 http://www.image-net.org/chall"
I11-1162,W11-0120,1,0.824283,"ove even basic tasks in natural language processing such as word relatedness. While we are aware that the limited coverage of ImageNet restricts the applicability of this hybrid image-text method to word relatedness, the continued growth of this resource should provide alleviation. Future work will also consider a comparison of multi-way combinations between knowledgebased, corpus-based and image-based metrics for further advancement of the state-of-the-art. 7 Related Work Recently, some attention has been given to modelling synergistic relationships between the semantics of words and images (Leong and Mihalcea, 2011; Bruni et al., 2011). The research that is most closely related to ours is the work of (Feng and Lapata, 2010), where it has been shown that it is possible to combine visual representations of word meanings into a joint bimodal representation constructed by using probabilistic generative latent topic models. Unlike our approach, however, (Feng and Lapata, 2010) relied on a news corpus where images and words in a document are assumed to be generated by a set of latent topics, rather than a lexical resource such as ImageNet. While they provided a proof-of-concept that using the visual modality"
I11-1162,N04-3012,0,0.024642,"Missing"
I13-1057,W11-0104,1,0.925442,"Missing"
I13-1057,P91-1034,0,0.582703,"Missing"
I13-1057,P07-1005,0,0.0839513,"Missing"
I13-1057,J04-1001,0,0.0727408,"Missing"
I13-1057,N07-1025,1,0.841574,"are available for more than 280 languages, with a number of entries varying from a few pages to three millions articles or more per language. A large number of the concepts mentioned in Wikipedia are explicitly linked to their corresponding article through the use of links or piped links. Interestingly, these links can be regarded as sense annotations for the corresponding concepts, which is a property particularly valuable for words that are ambiguous. In fact, it is precisely this observation that we rely on in order to generate sense tagged corpora starting with the Wikipedia annotations (Mihalcea, 2007; Dandala et al., 2012). To derive sense annotations for a given ambiguous word, we use the links extracted for all the hyperlinked Wikipedia occurrences of the given word, and map these annotations to word senses, as described in (Dandala et al., 2012). For instance, for the bar example above, we extract five possible annotations: bar (establishment), bar (landform), bar (law), and bar (music). In our experiments, the WSD dataset was built for a subset of the ambiguous words used during the SENSEVAL-2, SENSEVAL-3 evaluations and a subset of ambiguous words in four languages: English, Spanish,"
I13-1057,P02-1033,0,0.137366,"Missing"
I13-1057,P10-1023,0,0.0590059,"Missing"
I13-1057,P04-1039,0,0.0680303,"Missing"
I13-1057,D12-1128,0,0.0360543,"Missing"
I13-1057,fernandez-ordonez-etal-2012-unsupervised,1,0.880058,"Missing"
I13-1057,P96-1006,0,0.277286,"Missing"
I13-1057,D09-1048,0,0.0194798,"differ across languages and proposed a statistical machine learning technique that exploits these mappings for WSD. Subsequently, several works (Gale et al., 1992; Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Diab, 2004; Ng et al., 2003; Chan and Ng, 2005; Chan et al., 2007) explored the use of parallel translations for WSD. Li and Li (2004) introduced a bilingual bootstrapping approach, in which starting with indomain corpora in two different languages, English and Chinese, word translations are automatically disambiguated using information iteratively drawn from the bilingual corpora. Khapra et al. (2009; 2010) proposed another bilingual bootstrapping approach, in which they used an aligned multilingual dictionary and bilingual corpora to show how resource deprived languages can benefit from a resource rich language. They introduced a technique called parameter projections, in which parameters learned using both aligned multilingual Wordnet and bilingual corpora are projected from one language to another language to improve on existing WSD methods. In recent years, the exponential growth of the Web led to an increased interest in multilinguality. Lefever and Hoste (Lefever and Hoste, 2010) in"
I13-1057,P03-1058,0,0.091147,"Missing"
I13-1057,N01-1011,0,0.0561882,"t can mean green plant or factory; similarly the French word feuille can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Two well studied categories of approaches to word sense disambiguation (WSD) are represented by knowledge-based (Lesk, 1986; Galley and McKeown, 2003; Navigli and Velardi, 2005) and data-driven (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001) methods. Knowledgebased methods rely on information drawn from 1 http://www.senseval.org 498 International Joint Conference on Natural Language Processing, pages 498–506, Nagoya, Japan, 14-18 October 2013. trained for the reference and the supporting languages and their probabilistic outputs are integrated at test time into a joint disambiguation decision for the reference language (W IKI M U S ENSE, in Section 4). Experimental results on four languages demonstrate that the Wikipedia annotations are reliable, as the accuracy of the W IKI M ONO S ENSE systems trained on the Wikipedia dataset e"
I13-1057,C10-1063,0,0.0359555,"Missing"
I13-1057,S10-1003,0,0.0618167,"Missing"
I13-1057,P95-1026,0,0.553159,"or instance, the English noun plant can mean green plant or factory; similarly the French word feuille can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Two well studied categories of approaches to word sense disambiguation (WSD) are represented by knowledge-based (Lesk, 1986; Galley and McKeown, 2003; Navigli and Velardi, 2005) and data-driven (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001) methods. Knowledgebased methods rely on information drawn from 1 http://www.senseval.org 498 International Joint Conference on Natural Language Processing, pages 498–506, Nagoya, Japan, 14-18 October 2013. trained for the reference and the supporting languages and their probabilistic outputs are integrated at test time into a joint disambiguation decision for the reference language (W IKI M U S ENSE, in Section 4). Experimental results on four languages demonstrate that the Wikipedia annotations are reliable, as the accuracy of the W IKI M ONO S ENSE systems"
I13-1057,W09-2413,0,\N,Missing
I13-1057,W04-3204,0,\N,Missing
I17-1040,D08-1007,0,0.0240791,") found weak relationships between opinion leadership and innovative buying behavior, but observed that the relationship strength varied by product category. Tucker and Painter (1961), and Sparks and Tucker 394 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 394–403, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP between verb and noun classes. Similar verbnoun relationships have also been formulated in the problem of learning selectional preferences from text (Resnik, 1997; Brockmann and Lapata, 2003; Erk, 2007; Pantel et al., 2007; Bergsma et al., 2008; Van de Cruys, 2014), and more generally, in the problem of probabilistic frame induction (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chen et al., 2013). Another topic of research related to our work is the problem of research idea extraction from academic papers. Gupta and Manning (2011) took the first stab at this problem by implementing a bootstrapping algorithm on dependency tree kernels. Gupta and Manning’s method was later refined by Tsai et al. (2013) who worked with a more crisp set of idea categories. We view this problem as conceptually parallel to ours; however, a key differ"
I17-1040,E03-1034,0,0.0148118,"behavior and psychological traits. Robertson and Myers (1969) found weak relationships between opinion leadership and innovative buying behavior, but observed that the relationship strength varied by product category. Tucker and Painter (1961), and Sparks and Tucker 394 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 394–403, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP between verb and noun classes. Similar verbnoun relationships have also been formulated in the problem of learning selectional preferences from text (Resnik, 1997; Brockmann and Lapata, 2003; Erk, 2007; Pantel et al., 2007; Bergsma et al., 2008; Van de Cruys, 2014), and more generally, in the problem of probabilistic frame induction (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chen et al., 2013). Another topic of research related to our work is the problem of research idea extraction from academic papers. Gupta and Manning (2011) took the first stab at this problem by implementing a bootstrapping algorithm on dependency tree kernels. Gupta and Manning’s method was later refined by Tsai et al. (2013) who worked with a more crisp set of idea categories. We view this problem a"
I17-1040,P11-1098,0,0.0113304,"tionship strength varied by product category. Tucker and Painter (1961), and Sparks and Tucker 394 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 394–403, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP between verb and noun classes. Similar verbnoun relationships have also been formulated in the problem of learning selectional preferences from text (Resnik, 1997; Brockmann and Lapata, 2003; Erk, 2007; Pantel et al., 2007; Bergsma et al., 2008; Van de Cruys, 2014), and more generally, in the problem of probabilistic frame induction (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chen et al., 2013). Another topic of research related to our work is the problem of research idea extraction from academic papers. Gupta and Manning (2011) took the first stab at this problem by implementing a bootstrapping algorithm on dependency tree kernels. Gupta and Manning’s method was later refined by Tsai et al. (2013) who worked with a more crisp set of idea categories. We view this problem as conceptually parallel to ours; however, a key difference is that usage expressions are typically more obscure in text as compared to research ideas. (1971) showed that the"
I17-1040,P05-1045,0,0.0310663,"Missing"
I17-1040,D14-1082,0,0.0500472,"e Downy booster directly into the washer. (Instructions say NOT to put in your dispenser) And it does work fine with high efficiency washers. I do recommend this for times when you may want extra freshness for your clothes or towels. has “Unstopables”, “Olive oil”, “Vinegar”, “Aspirin”, or “Toothpaste” in its title. Once the word embedding is trained, a sentence is represented by the weighted average of the embeddings of all the unique words in it. (C) Syntax: We use bags of constituency and dependency production rules, obtained from the output of the Stanford parser (Klein and Manning, 2003; Chen and Manning, 2014). For constituency grammar, we use terminal and non-terminal rules separately as well as together. For the dependency grammar, we use the (collapsed) dependency types (amod, nsubj, etc.), and the lexicalized dependencies (e.g., (nsubj, Kirkland, seems)) as separate features. (D) Style: We extract thirteen shallow surfacelevel and style features to encode the stylistic properties of a sentence, in the hope that they would be predictive of whether the sentence contains a usage expression. These features are: sentence position, average word length (in chars), sentence length (in words and charact"
I17-1040,N13-1104,0,0.0135167,"roduct category. Tucker and Painter (1961), and Sparks and Tucker 394 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 394–403, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP between verb and noun classes. Similar verbnoun relationships have also been formulated in the problem of learning selectional preferences from text (Resnik, 1997; Brockmann and Lapata, 2003; Erk, 2007; Pantel et al., 2007; Bergsma et al., 2008; Van de Cruys, 2014), and more generally, in the problem of probabilistic frame induction (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chen et al., 2013). Another topic of research related to our work is the problem of research idea extraction from academic papers. Gupta and Manning (2011) took the first stab at this problem by implementing a bootstrapping algorithm on dependency tree kernels. Gupta and Manning’s method was later refined by Tsai et al. (2013) who worked with a more crisp set of idea categories. We view this problem as conceptually parallel to ours; however, a key difference is that usage expressions are typically more obscure in text as compared to research ideas. (1971) showed that there were correlations"
I17-1040,I11-1001,0,0.0269151,"essing, pages 394–403, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP between verb and noun classes. Similar verbnoun relationships have also been formulated in the problem of learning selectional preferences from text (Resnik, 1997; Brockmann and Lapata, 2003; Erk, 2007; Pantel et al., 2007; Bergsma et al., 2008; Van de Cruys, 2014), and more generally, in the problem of probabilistic frame induction (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chen et al., 2013). Another topic of research related to our work is the problem of research idea extraction from academic papers. Gupta and Manning (2011) took the first stab at this problem by implementing a bootstrapping algorithm on dependency tree kernels. Gupta and Manning’s method was later refined by Tsai et al. (2013) who worked with a more crisp set of idea categories. We view this problem as conceptually parallel to ours; however, a key difference is that usage expressions are typically more obscure in text as compared to research ideas. (1971) showed that there were correlations between personality traits and the types of products used. Dolich (1969) posited that products as symbols were organized into congruent relationships with th"
I17-1040,D14-1004,0,0.0410499,"Missing"
I17-1040,P03-1054,0,0.00671724,"sage annotations I put the Downy booster directly into the washer. (Instructions say NOT to put in your dispenser) And it does work fine with high efficiency washers. I do recommend this for times when you may want extra freshness for your clothes or towels. has “Unstopables”, “Olive oil”, “Vinegar”, “Aspirin”, or “Toothpaste” in its title. Once the word embedding is trained, a sentence is represented by the weighted average of the embeddings of all the unique words in it. (C) Syntax: We use bags of constituency and dependency production rules, obtained from the output of the Stanford parser (Klein and Manning, 2003; Chen and Manning, 2014). For constituency grammar, we use terminal and non-terminal rules separately as well as together. For the dependency grammar, we use the (collapsed) dependency types (amod, nsubj, etc.), and the lexicalized dependencies (e.g., (nsubj, Kirkland, seems)) as separate features. (D) Style: We extract thirteen shallow surfacelevel and style features to encode the stylistic properties of a sentence, in the hope that they would be predictive of whether the sentence contains a usage expression. These features are: sentence position, average word length (in chars), sentence len"
I17-1040,P07-1028,0,0.0105901,"raits. Robertson and Myers (1969) found weak relationships between opinion leadership and innovative buying behavior, but observed that the relationship strength varied by product category. Tucker and Painter (1961), and Sparks and Tucker 394 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 394–403, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP between verb and noun classes. Similar verbnoun relationships have also been formulated in the problem of learning selectional preferences from text (Resnik, 1997; Brockmann and Lapata, 2003; Erk, 2007; Pantel et al., 2007; Bergsma et al., 2008; Van de Cruys, 2014), and more generally, in the problem of probabilistic frame induction (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chen et al., 2013). Another topic of research related to our work is the problem of research idea extraction from academic papers. Gupta and Manning (2011) took the first stab at this problem by implementing a bootstrapping algorithm on dependency tree kernels. Gupta and Manning’s method was later refined by Tsai et al. (2013) who worked with a more crisp set of idea categories. We view this problem as conceptua"
I17-1040,Q13-1019,0,0.0507493,"Missing"
I17-1040,N07-1071,0,0.0190677,"rtson and Myers (1969) found weak relationships between opinion leadership and innovative buying behavior, but observed that the relationship strength varied by product category. Tucker and Painter (1961), and Sparks and Tucker 394 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 394–403, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP between verb and noun classes. Similar verbnoun relationships have also been formulated in the problem of learning selectional preferences from text (Resnik, 1997; Brockmann and Lapata, 2003; Erk, 2007; Pantel et al., 2007; Bergsma et al., 2008; Van de Cruys, 2014), and more generally, in the problem of probabilistic frame induction (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chen et al., 2013). Another topic of research related to our work is the problem of research idea extraction from academic papers. Gupta and Manning (2011) took the first stab at this problem by implementing a bootstrapping algorithm on dependency tree kernels. Gupta and Manning’s method was later refined by Tsai et al. (2013) who worked with a more crisp set of idea categories. We view this problem as conceptually parallel to ours;"
I17-1040,D09-1159,0,0.0287389,"extraction system that mined reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. OPINE’s use of relaxation labeling led to strong performance on the tasks of finding opinion phrases and their polarity. Ding et al. (2008) presented a “holistic lexicon-based approach” for mining context-dependent opinion words. The proposed method used an aggregating function for multiple conflicting opinion words in a sentence. The authors further implemented a system called “Opinion Observer” based on their method. Lastly, Wu et al. (2009) implemented a special dependency parser for opinion mining that used phrases (rather than words) as the primitive building blocks. Since many product features are in fact phrases, this approach led to good results for extracting relations between product features and opinion expressions. 3 Building a Usage Expression Dataset Product reviews often contain usage information. Specifically, in addition to opinions on product quality, reviewers often share how, where, or why they use the product. We therefore build our dataset of product usage expressions starting with a collection of product revi"
I17-1040,H05-1043,0,0.156147,"user-image congruence. In natural language processing research, the closest problem to usage expressions is perhaps that of opinion mining from product reviews and product aspects. Dave et al. (2003) classified reviews as expressing positive or negative sentiment. They identified four problems with review classification, including rating inconsistency, ambivalence, data sparseness, and skewed distribution. Hu and Liu (2004) extracted product features from the reviews of a single product, taking user opinion into account. Opinion/product features were mined if a reviewer had commented on them. Popescu and Etzioni (2005) presented OPINE, an unsupervised information extraction system that mined reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. OPINE’s use of relaxation labeling led to strong performance on the tasks of finding opinion phrases and their polarity. Ding et al. (2008) presented a “holistic lexicon-based approach” for mining context-dependent opinion words. The proposed method used an aggregating function for multiple conflicting opinion words in a sentence. The authors further implemented a system called “Opi"
I17-1040,P94-1019,0,0.263292,"Missing"
I17-1040,W97-0209,0,0.493164,"e on consumer behavior and psychological traits. Robertson and Myers (1969) found weak relationships between opinion leadership and innovative buying behavior, but observed that the relationship strength varied by product category. Tucker and Painter (1961), and Sparks and Tucker 394 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 394–403, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP between verb and noun classes. Similar verbnoun relationships have also been formulated in the problem of learning selectional preferences from text (Resnik, 1997; Brockmann and Lapata, 2003; Erk, 2007; Pantel et al., 2007; Bergsma et al., 2008; Van de Cruys, 2014), and more generally, in the problem of probabilistic frame induction (Chambers and Jurafsky, 2011; Cheung et al., 2013; Chen et al., 2013). Another topic of research related to our work is the problem of research idea extraction from academic papers. Gupta and Manning (2011) took the first stab at this problem by implementing a bootstrapping algorithm on dependency tree kernels. Gupta and Manning’s method was later refined by Tsai et al. (2013) who worked with a more crisp set of idea catego"
I17-1040,W16-1712,0,0.0290246,"Missing"
I17-1040,W15-1612,0,0.0216973,"Missing"
I17-1040,H05-2017,0,\N,Missing
I17-1067,N13-1092,0,0.0752651,"Missing"
I17-1067,D16-1235,0,0.0426469,"Missing"
I17-1067,S13-1004,0,0.127128,"Missing"
I17-1067,S12-1051,0,0.0683503,"ach would cause difficulties when dealing with other phrases such as sell a car and drive a car, which both involve an automobile but describe dissimilar actions. Therefore, successful systems should be able to properly focus on the most semantically relevant tokens with a phrase. A final challenge when dealing with human activity phrase relations is evaluation. There should be a good way to determine the effectiveness of a system’s ability to measure relations between these types of phrases, yet other commonly used semantic similarity testbeds (e.g., those presented in various Semeval tasks (Agirre et al., 2012, 2013; Marelli et al., 2014)) are not specifically focused on the domain of human activities. Currently, it is unclear whether or not the top-performing systems on general phrase similarity tasks will necessarily lead to the best results when looking specifically at human activity phrases. • Motivational Alignment: The degree to which the activities are (typically) done with similar motivations. Example of phrases with potentially similar motivations: to eat dinner with family members and to visit relatives. • Perceived Actor Congruence: The degree to which the activities are often done by th"
I17-1067,P12-1092,0,0.0308563,"y and Goldberg, 2014a) and (Bansal et al., 2014) extended the idea of context to incorporate dependency structures into the training process, leading to vectors that were able to better capture certain types of long-distance syntactic relationships. One of the major strengths of neural word embedding methods is that they are able to learn useful representations from extremely large corpora that can then be leveraged as a source of semantic knowledge on other tasks of interest, such as predicting word analogies (Pennington et al., 2014) or the semantic similarity and relatedness of word pairs (Huang et al., 2012). Researchers have taken the powerful semisupervised ability of these word embedding methods to aid in tasks at the phrase-level, as well. The most straightforward way to accomplish a phrase-level representation is to use some binary vector-level operation to compose pre-trained vector representations of individual words that belong to a phrase (Mitchell and Lapata, 2010). Other methods have sought to directly find embeddings for larger sequences of words, such as (Le and Mikolov, 2014) and (Kiros et al., 2015). Semantic textual similarity tasks are often evaluated by computing the correlation"
I17-1067,P14-2050,0,0.396264,"at seek to embed segments of text as vectors into some highdimensional space so that comparisons can be made between them using cosine similarity or other vector based metrics. While word embeddings have existed in various forms in the past (Church and Hanks, 1990; Bengio et al., 2003), 665 many approaches used today draw inspiration directly from shallow neural network based models such as those described in (Mikolov et al., 2013).2 In the common skip-gram variant of these neural embedding models, a neural network is trained to predict a word given its context within some fixed window size. (Levy and Goldberg, 2014a) and (Bansal et al., 2014) extended the idea of context to incorporate dependency structures into the training process, leading to vectors that were able to better capture certain types of long-distance syntactic relationships. One of the major strengths of neural word embedding methods is that they are able to learn useful representations from extremely large corpora that can then be leveraged as a source of semantic knowledge on other tasks of interest, such as predicting word analogies (Pennington et al., 2014) or the semantic similarity and relatedness of word pairs (Huang et al., 2012)."
I17-1067,J90-1003,0,0.463922,"pecifically to showcase diverse phenomena such as pairs containing the same verb, a range of degrees of similarity and relatedness, pairs unlikely to be done by the same type of person, and so forth. These pairs are each annotated by multiple human judges across the 2 Related Work Semantic similarity tasks have been recently dominated by various methods that seek to embed segments of text as vectors into some highdimensional space so that comparisons can be made between them using cosine similarity or other vector based metrics. While word embeddings have existed in various forms in the past (Church and Hanks, 1990; Bengio et al., 2003), 665 many approaches used today draw inspiration directly from shallow neural network based models such as those described in (Mikolov et al., 2013).2 In the common skip-gram variant of these neural embedding models, a neural network is trained to predict a word given its context within some fixed window size. (Levy and Goldberg, 2014a) and (Bansal et al., 2014) extended the idea of context to incorporate dependency structures into the training process, leading to vectors that were able to better capture certain types of long-distance syntactic relationships. One of the"
I17-1067,marelli-etal-2014-sick,0,0.273162,"s when dealing with other phrases such as sell a car and drive a car, which both involve an automobile but describe dissimilar actions. Therefore, successful systems should be able to properly focus on the most semantically relevant tokens with a phrase. A final challenge when dealing with human activity phrase relations is evaluation. There should be a good way to determine the effectiveness of a system’s ability to measure relations between these types of phrases, yet other commonly used semantic similarity testbeds (e.g., those presented in various Semeval tasks (Agirre et al., 2012, 2013; Marelli et al., 2014)) are not specifically focused on the domain of human activities. Currently, it is unclear whether or not the top-performing systems on general phrase similarity tasks will necessarily lead to the best results when looking specifically at human activity phrases. • Motivational Alignment: The degree to which the activities are (typically) done with similar motivations. Example of phrases with potentially similar motivations: to eat dinner with family members and to visit relatives. • Perceived Actor Congruence: The degree to which the activities are often done by the same type of person. Put an"
I17-1067,D14-1162,0,0.0763406,"is trained to predict a word given its context within some fixed window size. (Levy and Goldberg, 2014a) and (Bansal et al., 2014) extended the idea of context to incorporate dependency structures into the training process, leading to vectors that were able to better capture certain types of long-distance syntactic relationships. One of the major strengths of neural word embedding methods is that they are able to learn useful representations from extremely large corpora that can then be leveraged as a source of semantic knowledge on other tasks of interest, such as predicting word analogies (Pennington et al., 2014) or the semantic similarity and relatedness of word pairs (Huang et al., 2012). Researchers have taken the powerful semisupervised ability of these word embedding methods to aid in tasks at the phrase-level, as well. The most straightforward way to accomplish a phrase-level representation is to use some binary vector-level operation to compose pre-trained vector representations of individual words that belong to a phrase (Mitchell and Lapata, 2010). Other methods have sought to directly find embeddings for larger sequences of words, such as (Le and Mikolov, 2014) and (Kiros et al., 2015). Sema"
I17-1067,C16-1009,0,0.0454034,"ly, Simverb-3500 contains verbs that don’t necessarily describe human activities, like chirp and glow, and does not contain phrase-level activities. Several recent works have raised concerns over the standard evaluation approaches used in semantic textual similarity tasks. One potential issue is the use of inadequate metrics depending on the task that a practitioner is interested in tackling. While the Pearson correlation between humanjudged similarity scores and predicted outputs is often used, this type of correlation can be misleading in the presence of outliers or nonlinear relationships (Reimers et al., 2016). Remiers et al. propose a framework for selecting a metric for semantic text similarity tasks, which we take into consideration when selecting our evaluation metric. Additionally, correlation with human judgments does not always give a good indication of success on some downstream applications, the human ratings themselves are somewhat subjective, and statistical significance is rarely reported in comparisons of word embedding methods (Faruqui et al., 2016). However, our goal in this work is not to evaluate the overall quality of distributional semantic models, but to find a method that has h"
I17-1067,D16-1157,0,0.0182539,"nces of each sentence in a large corpus of books (Kiros et al., 2015). The encoder is a recurrent neural network (RNN) which creates a vector from the words in the input sentence, and the RNN decoder generates the neighboring sentences. The model also learns a linear mapping from word-level embeddings into the encoder space to handle rare words that may not appear in the training corpus. Charagram embeddings: Embeddings that represent character sequences (i.e., words or phrases) based on an elementwise nonlinear transformation of embeddings of the character n-grams that comprise the sequence (Wieting et al., 2016). Here we use the pre-trained charagram-phrase model. 4.1.1 Graph-Based Embeddings We also experiment with approaches that seek to incorporate higher order relationships between activity phrases by building semantic graphs that can be exploited to discover relations that hold between the phrases. Each graph G is of the form G = (V, E) where V is a set of human activity phrases and E is some measure of semantic similarity, which is computed differently depending on the graph type. We run Node2vec (Grover and Leskovec, 2016) using the default settings to generate an embedding for each node in th"
I17-1067,Q15-1017,0,0.0211605,"ongruence dimension also used the paragram embeddings, but when averaged across the simplified phrases with all verbs removed. We believe there is still plenty of room for improvement on this task, and we hope that the release of our data will encourage greater participation on this task. Future work should explore methods to handle more subtle semantic differences between activities that we noticed are often missed by the automated methods including the effects of function words and polysemy. It should also be helpful to learn better weight-based composition methods (e.g., those proposed in (Yu and Dredze, 2015)) rather than filtering out words in a Table 5: Spearman correlation between phrase similarity methods and human annotations across four annotated relations: Similarity (SIM), Relatedness (REL), Motivational Alignment (MA) and Perceived Actor Congruence (PAC). Top performing methods for each dimension are in bold font. * indicates correlation coefficient is not statistically significantly lower than the best method for that relational dimension (α = .05). recommended for tasks in which the ranking of all items is important (Reimers et al., 2016). Results for all methods using all phrase variat"
I17-1089,W12-3717,0,0.0211236,"ation. To identify such behavior, word patterns and search engine query detection were suggested as means for detecting pedophilic activity (Macwan and inz. Grzegorz Filcek, 2017). Examples include the detection of predators using lexical and behavioral features and calculating the predator-hood score as a function of features weights (Dhouioui and Akaichi, 2016). A method was developed to identify sexual predation using phrase-matching and rule-based systems and reported the usefulness of statement lengths in chat lines for improving the identification process (Mcghee et al., 2011). A study (Bogdanova et al., 2012) analyzing a corpus of chats for detecting cyberpedophilia found that character n-grams are capable of discriminating pedophiles’ chats. However, higher-level features that modeled behavior and emotion were required to detect conversations with cyberpedophiles from cybersex chat logs (Bogdanova et al., 2014). 3 Data Collection We seek to examine written samples of individuals presenting themselves with their real identity as well as a fake identity. To achieve this, we collect a corpus of writings from several participants, including responses to open-ended questions about their real identity,"
I17-1089,D14-1082,0,0.053541,"oung (≤ 30 years) and old (>30 years). The classifiers are built using the SVM algorithm2 and the different sets of features described in section 4. We perform leave-one-out crossvalidation in all our experiments. In all cases, we use the majority class baseline as a reference value. Features In this section, we describe the sets of features extracted, which are used to build our classifiers. Unigrams We extract unigrams and bigrams derived from the bag of words representation of each identity response. POS These features consist of part-of-speech (POS) tags obtained with the Stanford Parser (Chen and Manning, 2014). Semantic LIWC These features include the 74 semantic classes present in the LIWC lexicon 2015 (Pennebaker et al., 2015). Each feature represents the number of words in a response belonging to a specific semantic class, normalized with respect to the length of the response. 5.1 Classification of Real and Fake Identities Before focusing on our main research questions, we seek to evaluate whether deception detection can be conducted using our fake identity dataset. Thus, we focus on two main classification tasks. First, using our entire dataset, we explore whether we can discriminate between th"
K19-1010,I13-1171,0,0.060773,"se learned word embeddings is that they are able to capture useful semantic information that can be easily used in other tasks of interest such as semantic similarity and relatedness between pair of words (Mikolov et al., 2013a; Pennington et al., 2014; Wilson and Mihalcea, 2017) and dependency parsing (Chen and Manning, 2014; Dyer et al., 2015). However, these models treat names and entities no more than the tokens used to mention them. As a result, these models are unable to well represent names in narIdentifying and analyzing character relations in literary texts is a well studied problem (Agarwal et al., 2013; Makazhanov et al., 2014; Elson et al., 2010; Iyyer et al., 2016). Most of these models depend on analyzing the co-occurrence of the char100 ter Embedding (CBOW). The differences stand in the objective of post-training, given sets of (current speaker, previous speakers, next speakers, context words) as training examples. Formally, given the sequence of speakers at each turn S = s1 , s2 , s3 , , , sT −1 , sT , we define context words C for turn t as the set of words found by a sliding context window in the utterance. We propose our post-training objectives as following: acters and stylistic fe"
K19-1010,N18-1200,1,0.938164,"Land no. Dad, you have to use the machine gun. Get it ready. Eleven o’clock! What happens at eleven o’clock? Twelve, eleven, ten. Eleven o’clock, fire! Dad, are we hit? More or less. Son, I am sorry. They got us. Hang on, dad. We are going in. Table 1: A snippet of conversation between two characters from the “Indiana Jones and the Last Crusade” movie with each dialogue turn annotated with its corresponding speaker name. We aim to generate embedding representations for “Indiana” and “Henry” in a way that captures their relation. three main issues. First, name mentions in dialogues are sparse (Azab et al., 2018), which makes it difficult for these models to learn a good quality representation for these names (Barteld, 2017). Second, in dialogues or narratives, names often do not refer to the same person, and yet these embeddings have a single vector representation for each word in the vocabulary. For example, “Danny” in the dialogue of the “American History X” movie is different from “Danny” in the “Ocean’s Eleven” movie. Finally, the learned embeddings of these names reflect the co-occurrences of these name mentions and other words uttered by these characters, but do not model how related these char"
K19-1010,D14-1082,0,0.0169777,"resentations to improve the performance of neural conversation models. Unlike (Ji et al., 2017; Li et al., 2016), we focus on representing character names in dialogue settings and learning different embeddings for characters from different story dialogues in a way that reflects the relatedness of story characters; more specifically, we propose the use of speaker prediction as an auxiliary supervision to improve the character representation. Related Work Learning distributional representation of words plays an increasingly important role in representing text in many tasks (Bengio et al., 2013; Chen and Manning, 2014). The existence of huge datasets allowed learning high quality word embeddings in an unsupervised way by training a neural network on fake objectives (Mikolov et al., 2013a,b; Turney and Pantel, 2010). A major strength of these learned word embeddings is that they are able to capture useful semantic information that can be easily used in other tasks of interest such as semantic similarity and relatedness between pair of words (Mikolov et al., 2013a; Pennington et al., 2014; Wilson and Mihalcea, 2017) and dependency parsing (Chen and Manning, 2014; Dyer et al., 2015). However, these models trea"
K19-1010,P15-1033,0,0.0185363,"(Bengio et al., 2013; Chen and Manning, 2014). The existence of huge datasets allowed learning high quality word embeddings in an unsupervised way by training a neural network on fake objectives (Mikolov et al., 2013a,b; Turney and Pantel, 2010). A major strength of these learned word embeddings is that they are able to capture useful semantic information that can be easily used in other tasks of interest such as semantic similarity and relatedness between pair of words (Mikolov et al., 2013a; Pennington et al., 2014; Wilson and Mihalcea, 2017) and dependency parsing (Chen and Manning, 2014; Dyer et al., 2015). However, these models treat names and entities no more than the tokens used to mention them. As a result, these models are unable to well represent names in narIdentifying and analyzing character relations in literary texts is a well studied problem (Agarwal et al., 2013; Makazhanov et al., 2014; Elson et al., 2010; Iyyer et al., 2016). Most of these models depend on analyzing the co-occurrence of the char100 ter Embedding (CBOW). The differences stand in the objective of post-training, given sets of (current speaker, previous speakers, next speakers, context words) as training examples. For"
K19-1010,E12-1065,0,0.0268798,"y few times or did not interact at all. However, it is important to include these unrelated pairs while evaluating the quality of the character embeddings, as unrelated pairs might be closer than related ones especially for minor characters that do not speak much during the dialogue. 4.2 3000 Character Relationships 5 Understanding the relationships between characters is a primary task in extracting and analyzing social relation networks from literary novels (Elson et al., 2010; Agarwal et al., 2013). It is also important for improving computational story summarization and generation methods (Elsner, 2012; Gorinski and Lapata, 2015). Character relationship is a more complex 5.1 Experiments Baselines For each task, we compare our character embedding models against five baselines: 1 Annotations on temporal change in the sentiment between each pair of characters is also included, but since our models do not have the ability to track such temporal information, we do not use these annotations. 103 Interaction Frequency. We count the number of exchanged dialogue turns between every pair of characters and normalize it by the total number of turns spoken by a given pair of characters. TF-IDF. We treat"
K19-1010,P10-1015,0,0.398989,"e vector representation for each word in the vocabulary. For example, “Danny” in the dialogue of the “American History X” movie is different from “Danny” in the “Ocean’s Eleven” movie. Finally, the learned embeddings of these names reflect the co-occurrences of these name mentions and other words uttered by these characters, but do not model how related these characters are. Thus, the resulting embeddings cannot be effectively used to further reason about the characters and their relations. The representation of characters in dialogues has been an important task for social network extraction (Elson et al., 2010), character relation modeling (Chaturvedi et al., 2016), and personabased conversation models (Li et al., 2016). However, most of the previous work relies upon the exIntroduction Understanding characters (or more broadly people) plays a critical role in the human-level interpretation of dialogues – be those in stories, movies, or day-to-day conversations. The verbal interaction between characters provides important information (Iyyer et al., 2016; Elson et al., 2010). In these contexts, the names of characters trigger reasoning at a much deeper level than other regular words, due to the charac"
K19-1010,N15-1113,0,0.0192193,"did not interact at all. However, it is important to include these unrelated pairs while evaluating the quality of the character embeddings, as unrelated pairs might be closer than related ones especially for minor characters that do not speak much during the dialogue. 4.2 3000 Character Relationships 5 Understanding the relationships between characters is a primary task in extracting and analyzing social relation networks from literary novels (Elson et al., 2010; Agarwal et al., 2013). It is also important for improving computational story summarization and generation methods (Elsner, 2012; Gorinski and Lapata, 2015). Character relationship is a more complex 5.1 Experiments Baselines For each task, we compare our character embedding models against five baselines: 1 Annotations on temporal change in the sentiment between each pair of characters is also included, but since our models do not have the ability to track such temporal information, we do not use these annotations. 103 Interaction Frequency. We count the number of exchanged dialogue turns between every pair of characters and normalize it by the total number of turns spoken by a given pair of characters. TF-IDF. We treat all the utterances of a cha"
K19-1010,N16-1180,0,0.0418189,"Missing"
K19-1010,D14-1162,0,0.0840364,"ntroduction Understanding characters (or more broadly people) plays a critical role in the human-level interpretation of dialogues – be those in stories, movies, or day-to-day conversations. The verbal interaction between characters provides important information (Iyyer et al., 2016; Elson et al., 2010). In these contexts, the names of characters trigger reasoning at a much deeper level than other regular words, due to the character background, behaviors, social network, and so forth. Currently, the most commonly used word embedding models such as Word2Vec (Mikolov et al., 2013a,b) and Glove (Pennington et al., 2014) represent characters using the embeddings corresponding to the tokens used to name them. Using these models in a dialogue setting to represent the characters poses 99 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 99–109 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics rative understanding task because the word “John” in a given story can be very different from the word “John” in another narrative. In this work, we only focus on representing character names and not the whole embedding space (Ji et al., 2017). traction"
K19-1010,D17-1195,0,0.300205,"(Pennington et al., 2014) represent characters using the embeddings corresponding to the tokens used to name them. Using these models in a dialogue setting to represent the characters poses 99 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 99–109 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics rative understanding task because the word “John” in a given story can be very different from the word “John” in another narrative. In this work, we only focus on representing character names and not the whole embedding space (Ji et al., 2017). traction of linguistic features like explicit forms of address (Makazhanov et al., 2014), the length of the utterance, or the frequency of exchanges between the characters (Elson et al., 2010). In this work, we address the task of representing characters in dialogues, specifically focusing on movies and plays. Given a set of dialogue turns, annotated with the corresponding speaker names, our goal is to generate a vector representation for each of these characters that captures the relation with other characters. We propose a new approach to embed characters in dialogues based not only on wha"
K19-1010,N18-1202,0,0.260074,"cates the size of the speaker window (speaker window of size one means we consider speakers of one preceding turn and one succeeding turn). Our formulation defines probabilities p(si |wi ), p(si |si+j ) and p(wi |si+j ) using the softmax equation. We also define two transformations of our network – lookup table (LUT) initialized by embedding of pre-trained embedding model and Linear Projection Layer W. To examine the generality of our post-training schemes, we also apply them to another pretrained word embedding model. Given a dialogue turn, we encode it using ELMo’s pre-trained BiLSTM model (Peters et al., 2018) to generate a sequence of contextualized vectors for words. We add a linear projection layer on top that takes the generated embedding, in addition to the previous and following speakers, and train it to predict the speaker of the current turn. We refer to this model as Character Embedding (ELMo). Setup Our architecture builds on a pretrained embedding model generated by standard Word2Vec models (Mikolov et al., 2013a,b) or pre-trained contextualized word representations from neural language models (ELMo) (Peters et al., 2018). We start by collecting sets of (current speaker, previous speaker"
K19-1010,I17-1048,0,0.0486474,"Missing"
K19-1010,N16-1099,0,0.0305563,"sentiment) – we find that the model exceeds by a large margin several strong baselines, which indicates that our model effectively captures the various characteristics of characters. Additionally, in the process of evaluating the model, we build a new dataset consisting of 4,761 character relation pairs obtained from eighteen movies, manually annotated with relatedness scores and relations of various granularities. We are making the dataset publicly available. 2 Recently, several approaches have been proposed to build dynamic representations for entities (Henaff et al., 2016; Ji et al., 2017; Kobayashi et al., 2016, 2017). One common approach is to rely on neural language models to encode the local context of an entity and use the resulting context vectors as the embedding for subsequent occurrences of that entity (Kobayashi et al., 2016, 2017). Another approach is to learn a generative model that generates the representation of an entity mention (Ji et al., 2017). Henaff et al. (2016) proposed an explicit entity tracking model by relying on an external memory to store information about entities as they appear in a given sentence. While these rich representations improve the performance on several tasks"
K19-1010,D15-1036,0,0.0269826,"n Algorithm 1. 4 Evaluation Tasks and Datasets We evaluate the quality of our speaker embedding model across two different tasks. Our goal is to evaluate how well each embedding model captures simple and complex character representations and interactions. 4.1 Character Relatedness Measures of semantic relatedness between words indicate the degree to which words are associated with any kind of semantic relationship such as synonymy, antonymy, and so on. Semantic relatedness is commonly used as an absolute intrinsic evaluation task to assess and compare the quality of different word embeddings (Schnabel et al., 2015; Yih and Qazvinian, 2012; Upadhyay et al., 2016) and phrase embeddings (Wilson and Mihalcea, 2017). Similarly, we define character relatedness as the degree to which a pair of characters in a given story are related to each other based on the story plot and their level of interaction throughout the dialogue. Given a pair of characters, we would like the relatedness score between their embedding representations to have a high correlation with their corresponding human-based relatedness score. Thus, the distance of the embeddings between closely related characters should be smaller than the dis"
K19-1010,D18-1167,0,0.29602,"ntions of entities in text as available in toy datasets such as bAbi (Weston et al., 2015). Thus, it is difficult to apply these representations in a dialogue setting due to the sparseness of name mentions in dialogue, as well as the lack of explicit conversation connections between characters (as available in movies) (Azab et al., 2018). Most of the existing story understanding work feeds the model with the vector representations of names based on a global model such as Word2Vec or Glove, which hinders the ability of these models to understand dialogue (Tapaswi et al., 2016; Na et al., 2017; Lei et al., 2018). Recently, Li et al. (2016) relied on TV series scripts in order to learn speaker persona representations and used these representations to improve the performance of neural conversation models. Unlike (Ji et al., 2017; Li et al., 2016), we focus on representing character names in dialogue settings and learning different embeddings for characters from different story dialogues in a way that reflects the relatedness of story characters; more specifically, we propose the use of speaker prediction as an auxiliary supervision to improve the character representation. Related Work Learning distribu"
K19-1010,P16-1094,0,0.0881442,"Missing"
K19-1010,P16-1157,0,0.0220372,"e evaluate the quality of our speaker embedding model across two different tasks. Our goal is to evaluate how well each embedding model captures simple and complex character representations and interactions. 4.1 Character Relatedness Measures of semantic relatedness between words indicate the degree to which words are associated with any kind of semantic relationship such as synonymy, antonymy, and so on. Semantic relatedness is commonly used as an absolute intrinsic evaluation task to assess and compare the quality of different word embeddings (Schnabel et al., 2015; Yih and Qazvinian, 2012; Upadhyay et al., 2016) and phrase embeddings (Wilson and Mihalcea, 2017). Similarly, we define character relatedness as the degree to which a pair of characters in a given story are related to each other based on the story plot and their level of interaction throughout the dialogue. Given a pair of characters, we would like the relatedness score between their embedding representations to have a high correlation with their corresponding human-based relatedness score. Thus, the distance of the embeddings between closely related characters should be smaller than the distance between less related ones. 102 Algorithm 1:"
K19-1010,I17-1067,1,0.935287,"rds plays an increasingly important role in representing text in many tasks (Bengio et al., 2013; Chen and Manning, 2014). The existence of huge datasets allowed learning high quality word embeddings in an unsupervised way by training a neural network on fake objectives (Mikolov et al., 2013a,b; Turney and Pantel, 2010). A major strength of these learned word embeddings is that they are able to capture useful semantic information that can be easily used in other tasks of interest such as semantic similarity and relatedness between pair of words (Mikolov et al., 2013a; Pennington et al., 2014; Wilson and Mihalcea, 2017) and dependency parsing (Chen and Manning, 2014; Dyer et al., 2015). However, these models treat names and entities no more than the tokens used to mention them. As a result, these models are unable to well represent names in narIdentifying and analyzing character relations in literary texts is a well studied problem (Agarwal et al., 2013; Makazhanov et al., 2014; Elson et al., 2010; Iyyer et al., 2016). Most of these models depend on analyzing the co-occurrence of the char100 ter Embedding (CBOW). The differences stand in the objective of post-training, given sets of (current speaker, previou"
K19-1010,N12-1077,0,0.0148458,"tion Tasks and Datasets We evaluate the quality of our speaker embedding model across two different tasks. Our goal is to evaluate how well each embedding model captures simple and complex character representations and interactions. 4.1 Character Relatedness Measures of semantic relatedness between words indicate the degree to which words are associated with any kind of semantic relationship such as synonymy, antonymy, and so on. Semantic relatedness is commonly used as an absolute intrinsic evaluation task to assess and compare the quality of different word embeddings (Schnabel et al., 2015; Yih and Qazvinian, 2012; Upadhyay et al., 2016) and phrase embeddings (Wilson and Mihalcea, 2017). Similarly, we define character relatedness as the degree to which a pair of characters in a given story are related to each other based on the story plot and their level of interaction throughout the dialogue. Given a pair of characters, we would like the relatedness score between their embedding representations to have a high correlation with their corresponding human-based relatedness score. Thus, the distance of the embeddings between closely related characters should be smaller than the distance between less relate"
L16-1592,D11-1120,0,0.026614,"stablish the framework for identifying such objects. To this end, we propose the concept of “possessions,” or textual representation of items that somebody owns or is entitled to, which we define in detail in Section 2.1. To date, research focusing on extracting latent attributes from microblogs has mostly centered around Twitter, as it is a service with a high adoption rate, where many of the users share their tweets publicly. Some of the attributes targeted for extraction are demographics related, such as gender and age (Burger and Henderson, 2006; Mukherjee and Liu, 2010; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012), political affiliation (Conover et al., 2011; Cohen and Ruths, 2013), and even lifestyle choices, such as coffee preference (Pennacchiotti and Popescu, 2011). To our knowledge, this is the first study that seeks to identify object ownership. 2. Possessions We start by defining possessions, and then briefly review the main considerations we established in order to produce consistent annotations. 2.1. What Are Possessions? We define possessions as textual representations of physical, concrete objects that could be considered to be someone’s property such as electronics, clothe"
L16-1592,D10-1021,0,0.0303661,"hip of a given object, this work seeks to establish the framework for identifying such objects. To this end, we propose the concept of “possessions,” or textual representation of items that somebody owns or is entitled to, which we define in detail in Section 2.1. To date, research focusing on extracting latent attributes from microblogs has mostly centered around Twitter, as it is a service with a high adoption rate, where many of the users share their tweets publicly. Some of the attributes targeted for extraction are demographics related, such as gender and age (Burger and Henderson, 2006; Mukherjee and Liu, 2010; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012), political affiliation (Conover et al., 2011; Cohen and Ruths, 2013), and even lifestyle choices, such as coffee preference (Pennacchiotti and Popescu, 2011). To our knowledge, this is the first study that seeks to identify object ownership. 2. Possessions We start by defining possessions, and then briefly review the main considerations we established in order to produce consistent annotations. 2.1. What Are Possessions? We define possessions as textual representations of physical, concrete objects that could be considered to be someone"
L16-1592,D12-1005,0,0.0569653,"Missing"
L18-1492,S16-1176,0,0.0368828,"Missing"
L18-1492,P13-2131,0,0.154104,"nd relations as edges between those nodes. LDC2017T10). The 2014 dataset contains 13,000 sentences, which increased to almost 20,000 in the 2015 set. The 2016 and 2017 datasets contain around 39,000 sentences. The data consist of sentences from English broadcast conversations, weblogs, discussion forums, and newswire data and are annotated with AMR graphs. The datasets build off of each other and include corrections and extensions of the AMR specification (e.g., 2015 introduces wikification and new PropBank frames). The evaluation of these AMR parsers is typically based on the SMATCH F1 tool (Cai and Knight, 2013) which measures the overlap of concept-relation-concept triples in a generated AMR graph as compared to the gold graph. In addition, Damonte et al. (2016) introduced finer-grained evaluations of the subtasks of AMR parsing, which 3119 Percentage of Total Concept Confusion Errors in Baseline or c o ga ni untr za tio y ci ty n co co u un n try try go vt pe -o rs rg and on re b se et w ar ch ee in n ne s w sp go- titut ap e 02 er or go-0 1 co gan un iza try tio go n go priv vtor vt ac g -o y rg pr i or v ga ate ni ge zat io t-0 n un 1 g et us un -05 r ig iver sta ht sit te e y re se ntitl ar e01"
L18-1492,P05-1045,0,0.017185,"l agent, relation, matter, horror, communication, psychological feature, set, process, attribute}. train CAMR to determine an upperbound on the performance of this parser when using world knowledge. We use max-margin learning with AdaGrad instead of the perceptron method in the original code. Previous work has shown this learning method to be effective for a variety of language processing tasks and we observe the same effect (Kummerfeld et al., 2015). In all these experiments, the original CAMR parser naturally constitutes our baseline. Named Entities. We use the Stanford named entity tagger (Finkel et al., 2005), and retrain it on the training set of LDC2014T12. To retrain the tagger we need spans in the AMR training sentences and their assigned labels. Dataset and Evaluations Metrics. We evaluate our work in two ways: one is overall SMATCH performance (Cai and Knight, 2013), which most of the previous work adopts; the other is the finer-grained evaluation introduced by Damonte et al. (2016), which evaluates the quality of each subtask of AMR parsing. We focus on a few particularly relevant metrics: To generate annotated data, we use the alignments automatically generated using the JAMR aligner for A"
L18-1492,P14-1134,0,0.0412891,"ng Representation (AMR), introduced by Banarescu et al. (2012), aims to capture the semantic meaning of sentences using directed acyclic graphs where nodes are labeled with concepts and edges are labeled with relations. An example is shown in Figure 1. A number of recent studies use AMR graphs for downstream tasks (Pan et al., 2015; Liu et al., 2015; Sachan and Xing, 2016; Burns et al., 2016) and growing amounts of annotated data enable the development of statistical parsing algorithms and standardized evaluations. Several parsers have been created that generate an AMR graph given a sentence (Flanigan et al., 2014; Wang et al., 2015b; Damonte et al., 2016), but even the most recent results suggest that there is still significant room to improve the performance for this challenging task. In this paper, we explore the role of world knowledge for the task of semantic parsing with AMR. The paper makes three main contributions. First, we examine the effect of different types of world knowledge for semantic parsing with AMR for the first time.1 Second, we examine the upper bound on world knowledge using gold annotations, and provide new insights into the potential of world knowledge in computational approach"
L18-1492,S16-1186,0,0.0238197,"Missing"
L18-1492,D15-1032,1,0.80586,"s the number of classes small while still having meaningful abstractions. The resulting set of fifteen classes is {group, thing, measure, change, object, substance, causal agent, relation, matter, horror, communication, psychological feature, set, process, attribute}. train CAMR to determine an upperbound on the performance of this parser when using world knowledge. We use max-margin learning with AdaGrad instead of the perceptron method in the original code. Previous work has shown this learning method to be effective for a variety of language processing tasks and we observe the same effect (Kummerfeld et al., 2015). In all these experiments, the original CAMR parser naturally constitutes our baseline. Named Entities. We use the Stanford named entity tagger (Finkel et al., 2005), and retrain it on the training set of LDC2014T12. To retrain the tagger we need spans in the AMR training sentences and their assigned labels. Dataset and Evaluations Metrics. We evaluate our work in two ways: one is overall SMATCH performance (Cai and Knight, 2013), which most of the previous work adopts; the other is the finer-grained evaluation introduced by Damonte et al. (2016), which evaluates the quality of each subtask o"
L18-1492,N15-1114,0,0.0224069,"Missing"
L18-1492,S16-1166,0,0.0222681,"t names represent their word senses. For instance, get-01 means ‘to come into posession’, whereas get-05 means ‘to get to a state’. measure parser effectiveness in terms of capturing named entities, concepts, negations, word sense disambiguations and semantic roles. At the time these experiments were performed many of the high performing parsers were based on JAMR or CAMR (Flanigan et al., 2014; Wang et al., 2015a). We chose to base our parser on CAMR, which was the highest performing entry on SemEval 2016 Task 8 and the parser on which most of the entries for the 2016 shared task were based (May, 2016; Wang et al., 2016). Comparing to previous extensions of CAMR, our work is the first attempt to integrate various forms of world knowledge as features for AMR parsing. 3. AMR Parsing with World Knowledge We hypothesize that introducing world knowledge could potentially increase the overall performance of AMR parsers. In order to identify useful features and effective approaches to improve an existing AMR parser, we first examine the errors produced by AMR parsers. By looking at errors made by CAMR, we found that a significant number of concepts are either mislabeled or missing. The lighter, s"
L18-1492,D16-1183,0,0.0263086,"Missing"
L18-1492,N15-1119,0,0.025793,"Missing"
L18-1492,P16-2079,0,0.0265753,"Missing"
L18-1492,P15-2141,0,0.362505,", introduced by Banarescu et al. (2012), aims to capture the semantic meaning of sentences using directed acyclic graphs where nodes are labeled with concepts and edges are labeled with relations. An example is shown in Figure 1. A number of recent studies use AMR graphs for downstream tasks (Pan et al., 2015; Liu et al., 2015; Sachan and Xing, 2016; Burns et al., 2016) and growing amounts of annotated data enable the development of statistical parsing algorithms and standardized evaluations. Several parsers have been created that generate an AMR graph given a sentence (Flanigan et al., 2014; Wang et al., 2015b; Damonte et al., 2016), but even the most recent results suggest that there is still significant room to improve the performance for this challenging task. In this paper, we explore the role of world knowledge for the task of semantic parsing with AMR. The paper makes three main contributions. First, we examine the effect of different types of world knowledge for semantic parsing with AMR for the first time.1 Second, we examine the upper bound on world knowledge using gold annotations, and provide new insights into the potential of world knowledge in computational approaches to AMR parsing."
L18-1492,N15-1040,0,0.340422,", introduced by Banarescu et al. (2012), aims to capture the semantic meaning of sentences using directed acyclic graphs where nodes are labeled with concepts and edges are labeled with relations. An example is shown in Figure 1. A number of recent studies use AMR graphs for downstream tasks (Pan et al., 2015; Liu et al., 2015; Sachan and Xing, 2016; Burns et al., 2016) and growing amounts of annotated data enable the development of statistical parsing algorithms and standardized evaluations. Several parsers have been created that generate an AMR graph given a sentence (Flanigan et al., 2014; Wang et al., 2015b; Damonte et al., 2016), but even the most recent results suggest that there is still significant room to improve the performance for this challenging task. In this paper, we explore the role of world knowledge for the task of semantic parsing with AMR. The paper makes three main contributions. First, we examine the effect of different types of world knowledge for semantic parsing with AMR for the first time.1 Second, we examine the upper bound on world knowledge using gold annotations, and provide new insights into the potential of world knowledge in computational approaches to AMR parsing."
L18-1492,S16-1181,0,0.0244936,"Missing"
L18-1492,D16-1065,0,0.0275381,"Missing"
L18-1591,Q16-1033,0,0.0615191,"study aspects such as language mirroring, empathy, and reflective listening. (Tanana et al., 2015) addressed the identification of counselor’s statements that discuss client’s change talk using recursive neural networks to model sequences of counselor and client verbal exchanges. (Lord et al., 2015) analyzed the language style synchrony between therapists and clients during MI encounters. Their approach relies on the psycholinguistic categories from the Linguistic Inquiry and Word Count lexicon to measure the degree to which the counselor language matches the client language. More recently, (Althoff et al., 2016) explored language style and symmetry in counseling interactions by analyzing a large sample of text-message-based counseling. Their main findings suggest that counselors who are more successful act with more control in the conversations and show lower levels of verbal coordination (mirroring) than their less successful counterparts. Furthermore, there are ongoing efforts on creating annotated resources that facilitate NLP advances in the analysis of clinical text in applications such as automatic annotation of pathology reports and oncology reports as well as data from biomedical journals (Ro"
L18-1591,P14-5010,0,0.00267656,"dition, we analyzed the sentiment expressed by the counselor during the encounters as a potential predictor of counseling quality. This could provide information on whether counselors focus on positive or negative aspects of the client communication, and how this relates to the conversation quality, i.e., low or high. Thus, we analyze the sentiment expressed by counselors during each turn in the conversation. Given the effort required to manually annotated the sentiment in each conversation, we opted for using an automatic off-the-shelf sentiment classifier from the Stanford Core NLP package (Manning et al., 2014). We obtain a sentiment score for each counselor turn, scored from very negative to very positive, and calculate the percentage of positive, negative, and neutral turns during the conversation. Figure 2 shows the sentiment distribution over the high-quality and low-quality conversations. The box plots in the figure suggest differences between the low and high quality groups, particularly for positive sentiment. In order to look more closely into the positive sentiment trend during the counseling encounters, we plot the distribution of positive turns by the counselor across the low and high qua"
L18-1591,strapparava-valitutti-2004-wordnet,0,0.310318,"eparately, followed by an integrated model that attempts to combine all the linguistic cues to improve the prediction of counseling quality. The different features are as follows: N-grams: These features represent the language used by the counseling-conversation participants and include all the unique words and word-pairs present in the transcript. We extract a vector containing the frequencies of each word and word pair present in the transcript. Semantic information: We use categories from the LIWC (Tausczik and Pennebaker, 2010), Opinion Finder (Wilson et al., 2005) and the Wordnet Affect (Strapparava and Valitutti, 2004) lexicons to derive features that identify identifying words as belonging to certain semantic categories that are potential markers of the conversation quality. Metafeatures: We also extract a set of metafeatures that describe the conversation interaction, including the number of counselor turns, client turns, average words during client and counselor turns, and the ratio of counselor and client words in each turn. Sentiment: These features are designed to capture the sentiment trend in the counselor responses during the conversation. To derive this features, we first obtain the sentiment expr"
L18-1591,W15-1209,0,0.135245,"ective listening, questions, support, and empathy. Methods that combine acoustic and linguistic datastreams have also been proposed to evaluate the quality of counseling interactions. (Xiao et al., 2014) presented a study on the automatic evaluation of counselor empathy based on analyzing correlations between prosody patterns and empathy showed by the therapist during counseling interactions. Second, aiming to improve the understanding of counseling interactions, researchers have started to explore NLP approaches to study aspects such as language mirroring, empathy, and reflective listening. (Tanana et al., 2015) addressed the identification of counselor’s statements that discuss client’s change talk using recursive neural networks to model sequences of counselor and client verbal exchanges. (Lord et al., 2015) analyzed the language style synchrony between therapists and clients during MI encounters. Their approach relies on the psycholinguistic categories from the Linguistic Inquiry and Word Count lexicon to measure the degree to which the counselor language matches the client language. More recently, (Althoff et al., 2016) explored language style and symmetry in counseling interactions by analyzing"
L18-1591,H05-2018,0,0.0621301,"st explore the predictive power of each cue separately, followed by an integrated model that attempts to combine all the linguistic cues to improve the prediction of counseling quality. The different features are as follows: N-grams: These features represent the language used by the counseling-conversation participants and include all the unique words and word-pairs present in the transcript. We extract a vector containing the frequencies of each word and word pair present in the transcript. Semantic information: We use categories from the LIWC (Tausczik and Pennebaker, 2010), Opinion Finder (Wilson et al., 2005) and the Wordnet Affect (Strapparava and Valitutti, 2004) lexicons to derive features that identify identifying words as belonging to certain semantic categories that are potential markers of the conversation quality. Metafeatures: We also extract a set of metafeatures that describe the conversation interaction, including the number of counselor turns, client turns, average words during client and counselor turns, and the ratio of counselor and client words in each turn. Sentiment: These features are designed to capture the sentiment trend in the counselor responses during the conversation. To"
loza-etal-2014-building,N10-1132,0,\N,Missing
loza-etal-2014-building,C04-1079,0,\N,Missing
loza-etal-2014-building,W01-0719,0,\N,Missing
loza-etal-2014-building,C10-2042,0,\N,Missing
loza-etal-2014-building,P08-1041,0,\N,Missing
loza-etal-2014-building,W04-1008,0,\N,Missing
loza-etal-2014-building,N04-4027,0,\N,Missing
mihalcea-2002-bootstrapping,J98-1006,0,\N,Missing
mihalcea-2002-bootstrapping,N01-1011,0,\N,Missing
mihalcea-2002-bootstrapping,H93-1061,0,\N,Missing
mihalcea-2002-bootstrapping,P95-1026,0,\N,Missing
mihalcea-2002-bootstrapping,P94-1020,0,\N,Missing
mihalcea-2002-bootstrapping,J95-4004,0,\N,Missing
mihalcea-2002-bootstrapping,P96-1006,0,\N,Missing
mihalcea-2002-bootstrapping,S01-1021,0,\N,Missing
mohler-mihalcea-2008-babylon,resnik-1998-parallel,0,\N,Missing
mohler-mihalcea-2008-babylon,J03-3002,0,\N,Missing
mohler-mihalcea-2008-babylon,P02-1040,0,\N,Missing
mohler-mihalcea-2008-babylon,P07-2045,0,\N,Missing
mohler-mihalcea-2008-babylon,ma-2006-champollion,0,\N,Missing
mohler-mihalcea-2008-babylon,1999.mtsummit-1.79,0,\N,Missing
mohler-mihalcea-2008-babylon,P06-1062,0,\N,Missing
N04-3006,W03-1007,0,0.0138222,"ect role (this is therefore the accuracy of role assignment), which compares favorably with previous results reported in the literature for this task. Notice also that since this is a rule-based approach, the parser does not need large amounts of annotated data, but it works well the same for words for which only one or two sentences are annotated. 6 Related Work All previous work in semantic parsing has exclusively focused on labeling semantic roles, rather than analyzing the full structure of sentence semantics, and is usually based on statistical models - e.g. (Gildea and Jurafsky, 2000), (Fleischman et al., 2003). To our knowledge, there was no previous attempt on performing semantic annotations using alternative rule-based algorithms. However, a rule-based approach is closer to the way humans interpret the semantic structure of a sentence. Moreover, as mentioned earlier, the FrameNet data is not meant to be “statistically representative”, but rather illustrative for various language constructs, and therefore a rule-based approach is more suitable for this lexical resource. 7 Conclusions We described a rule-based approach to open text semantic parsing. The semantic parser has the capability to analyze"
N04-3006,P00-1065,0,0.119089,"Missing"
N07-1025,W06-2810,0,0.112953,"Missing"
N07-1025,E06-1002,0,0.167932,"a natural Zipfian sense distribution, unlike the equal distributions typically obtained with the methods that rely on the use of monosemous relatives or bootstrapping methods. Finally, the grow pace of Wikipedia is much faster than other more taskfocused and possibly less-engaging activities such as Open Mind Word Expert, and therefore has the potential to lead to significantly higher coverage. With respect to the use of Wikipedia as a resource for natural language processing tasks, the work that is most closely related to ours is perhaps the name entity disambiguation algorithm proposed in (Bunescu and Pasca, 2006), where an SVM kernel is trained on the entries found in Wikipedia for ambiguous named entities. Other language processing tasks with recently proposed solutions relying on Wikipedia are co-reference resolution using Wikipedia-based measures of word similarity (Strube and Ponzetto, 2006), enhanced text classification using encyclopedic knowledge (Gabrilovich and Markovitch, 2006), and the construction of comparable corpora using the multilingual editions of Wikipedia (Adafre and de Rijke, 2006). 8 Conclusions In this paper, we described an approach for using Wikipedia as a source of sense anno"
N07-1025,W02-0817,1,0.530353,"hen To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al., 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002). In this paper, we investigate a new approach for building sense tagged corpora using Wikipedia as a source of sense annotations. Starting with the hyperlinks available in Wikipedia, we show how we can generate sense annotated corpora that can be used for building accurate and robust sense classifiers. Through word sense disambiguation experiments performed on the Wikipedia-based sense tagged corpus generated for a subset of the S ENSE VAL ambiguous words, we show that the Wikipedia annotations are reliable, and the quality of a sense tagging classifier built on this data set exceeds by a lar"
N07-1025,P02-1033,0,0.0709149,"k, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al., 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002). In this paper, we investigate a new approach for building sense tagged corpora using Wikipedia as a source of sense annotations. Starting with the hyperlinks available in Wikipedia, we show how we can generate sense annotated corpora that can be used for building accurate and robust sense classifiers. Through word sense disambiguation experiments performed on the Wikipedia-based sense tagged corpus generated for a subset of the S ENSE VAL ambiguous words, we show that the Wikiped"
N07-1025,P04-1039,0,0.0571508,"a particular word is transformed into a feature vector which is then To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al., 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002). In this paper, we investigate a new approach for building sense tagged corpora using Wikipedia as a source of sense annotations. Starting with the hyperlinks available in Wikipedia, we show how we can generate sense annotated corpora that can be used for building accurate and robust sense classifiers. Through word sense disambiguation experiments performed on the Wikipedia-based sense tagged corpus generated for a subset of the S ENSE VAL ambiguous words, we show that the Wikipedia annotations are reliable, a"
N07-1025,J98-1006,0,0.0958069,"; Pedersen, 2001) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance. In these systems, the sense disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al., 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002). In this paper, we investigate a new approach for building sense tagged corpora using Wikipedia as a source of sense annotations. Starting with the hyperlinks available in Wikipedia, we show how we can generate sense a"
N07-1025,W02-1006,0,0.088944,"ontext implemented through sense-specific keywords determined as a list of at most five words occurring at least three times in the contexts defining a certain word sense. This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of state-ofthe-art word sense disambiguation systems participating in the S ENSEVAL -2 and S ENSEVAL -3 evaluations. The features are integrated in a Naive Bayes classifier, which was selected mainly for its performance in previous work showing that it can lead to a state-of-the-art disambiguation system given the features we consider (Lee and Ng, 2002). 5 word argument arm atmosphere bank bar chair channel circuit degree difference disc dyke fatigue grip image material mouth nature paper party performance plan post restraint sense shelter sort source spade stress Experiments and Results To evaluate the quality of the sense annotations generated using Wikipedia, we performed a word sense disambiguation experiment on a subset of the ambiguous words used during the S ENSEVAL -2 and S ENSEVAL -3 evaluations. Since the Wikipedia annotations are focused on nouns (associated with the entities typically defined by Wikipedia), the sense annotations"
N07-1025,mihalcea-2002-bootstrapping,1,0.88719,"the highest performance. In these systems, the sense disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al., 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002). In this paper, we investigate a new approach for building sense tagged corpora using Wikipedia as a source of sense annotations. Starting with the hyperlinks available in Wikipedia, we show how we can generate sense annotated corpora that can be used for building accurate and robust sense classifiers. Through word sense disambiguation experiments performed"
N07-1025,P96-1006,0,0.704318,"words in any given language carrying more than one meaning. For instance, the English noun plant can mean green plant or factory; similarly the French word feuille can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Among the various knowledge-based (Lesk, 1986; Galley and McKeown, 2003; Navigli and Velardi, 2005) and data-driven (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance. In these systems, the sense disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using monosemous relatives ("
N07-1025,P03-1058,0,0.06447,"ged occurrence of a particular word is transformed into a feature vector which is then To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al., 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002). In this paper, we investigate a new approach for building sense tagged corpora using Wikipedia as a source of sense annotations. Starting with the hyperlinks available in Wikipedia, we show how we can generate sense annotated corpora that can be used for building accurate and robust sense classifiers. Through word sense disambiguation experiments performed on the Wikipedia-based sense tagged corpus generated for a subset of the S ENSE VAL ambiguous words, we show that the Wikipedia annotations ar"
N07-1025,N01-1011,0,0.0342962,"n language carrying more than one meaning. For instance, the English noun plant can mean green plant or factory; similarly the French word feuille can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Among the various knowledge-based (Lesk, 1986; Galley and McKeown, 2003; Navigli and Velardi, 2005) and data-driven (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance. In these systems, the sense disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1"
N07-1025,P95-1026,0,0.812279,"ge number of the words in any given language carrying more than one meaning. For instance, the English noun plant can mean green plant or factory; similarly the French word feuille can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Among the various knowledge-based (Lesk, 1986; Galley and McKeown, 2003; Navigli and Velardi, 2005) and data-driven (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance. In these systems, the sense disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then To address the sense-tagged data bottleneck problem, different methods have been proposed in the past, with various degrees of success. This includes the automatic generation of sense-tagged data using mono"
N07-1025,W04-3204,0,\N,Missing
N09-1002,E06-1027,0,0.515121,"e hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range 11 of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, distinguishing S and O instances has often proven more difficult than subsequent polarity classification. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Thus, effective methods for S/O classification promise to improve performance for sentiment classification. In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). One reason is that different features may be relevant for the two subproblems. For example, negation features are more important for polarity classification than for subjectivity classification. N"
N09-1002,E06-1025,0,0.0528476,"ication system may want to find a wide range 11 of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, distinguishing S and O instances has often proven more difficult than subsequent polarity classification. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Thus, effective methods for S/O classification promise to improve performance for sentiment classification. In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). One reason is that different features may be relevant for the two subproblems. For example, negation features are more important for polarity classification than for subjectivity classification. Note that some of our features require vertical links that are presen"
N09-1002,esuli-sebastiani-2006-sentiwordnet,0,0.163404,"ication system may want to find a wide range 11 of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, distinguishing S and O instances has often proven more difficult than subsequent polarity classification. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Thus, effective methods for S/O classification promise to improve performance for sentiment classification. In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). One reason is that different features may be relevant for the two subproblems. For example, negation features are more important for polarity classification than for subjectivity classification. Note that some of our features require vertical links that are presen"
N09-1002,P07-1054,0,0.220206,"isting resources that do not require manually annotated data; they also implement a supervised system for comparison, which we will call SMsup. The other three groups start with positive and negative seed sets and expand them by adding synonyms and antonyms, and traversing horizontal links in WordNet. AB, ES, and SMsup additionally use information contained in glosses; AB also use hyponyms; SMsup also uses relation and POS features. AB perform multiple runs of their system to assign fuzzy categories to senses. ES use a semi-supervised, multiple-classifier learning approach. In a later paper, (Esuli and Sebastiani, 2007), ES again use information in glosses, applying a random walk ranking algorithm to a graph in which synsets are linked if a member of the first synset appears in the gloss of the second. Like ES and SMsup, we use machine learning, but with more diverse sources of knowledge. Further, several of our features are novel for the task. The LCS features (Section 6.1) detect subjectivity by measuring the similarity of a candidate word sense with a seed set. WM also use a similarity measure, but as a way to filter the output of a measure of distributional similarity (selecting words for a given word se"
N09-1002,C04-1200,0,0.377964,"ntended to conserve water”) He sold his catch at the market. catch, haul – (the quantity that was caught; “the catch was only 10 fish”) =&gt; indefinite quantity – (an estimated quantity) WM performed an agreement study and report that good agreement (κ=0.74) can be achieved between human annotators labeling the subjectivity of senses. For a similar task, (Su and Markert, 2008) also report good agreement. 3 Related Work Many methods have been developed for automatically identifying subjective (opinion, sentiment, attitude, affect-bearing, etc.) words, e.g., (Turney, 2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004; Taboada et al., 2006; Takamura et al., 2006). Five groups have worked on subjectivity sense labeling. WM and Su and Markert (2008) (hereafter SM) assign S/O labels to senses, while Esuli and Sebastiani (hereafter ES) (2006a; 2007), Andreevskaia and Bergler (hereafter AB) (2006b; 2006a), and (Valitutti et al., 2004) assign polarity labels. WM, SM, and ES have evaluated their systems against manually annotated word-sense data. WM’s annotations are described above; SM’s are similar. In the scheme ES use (Cerini et al., 2007), senses are assigned three scores, for positivity, negativity, 12 and"
N09-1002,N06-1026,0,0.0492454,"at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Thus, effective methods for S/O classification promise to improve performance for sentiment classification. In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). One reason is that different features may be relevant for the two subproblems. For example, negation features are more important for polarity classification than for subjectivity classification. Note that some of our features require vertical links that are present in WordNet for nouns and verbs but not for other parts of speech. Thus we address nouns (leaving verbs to future work). There are other motivations for focusing on nouns. Relatively little work in subjectivity and sentiment analysis has focused on subjective nouns. Also, a study (Bruce and Wiebe, 1999) showed that, of the major pa"
N09-1002,P04-1035,0,0.0182573,"assification. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Thus, effective methods for S/O classification promise to improve performance for sentiment classification. In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). One reason is that different features may be relevant for the two subproblems. For example, negation features are more important for polarity classification than for subjectivity classification. Note that some of our features require vertical links that are present in WordNet for nouns and verbs but not for other parts of speech. Thus we address nouns (leaving verbs to future work). There are other motivations for focusing on nouns. Relatively little work in subjectivity and sentiment analysis has focused on subjective nouns. Also, a study (Bruce and"
N09-1002,W03-1014,1,0.63668,"your wrist”; “a device intended to conserve water”) He sold his catch at the market. catch, haul – (the quantity that was caught; “the catch was only 10 fish”) =&gt; indefinite quantity – (an estimated quantity) WM performed an agreement study and report that good agreement (κ=0.74) can be achieved between human annotators labeling the subjectivity of senses. For a similar task, (Su and Markert, 2008) also report good agreement. 3 Related Work Many methods have been developed for automatically identifying subjective (opinion, sentiment, attitude, affect-bearing, etc.) words, e.g., (Turney, 2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004; Taboada et al., 2006; Takamura et al., 2006). Five groups have worked on subjectivity sense labeling. WM and Su and Markert (2008) (hereafter SM) assign S/O labels to senses, while Esuli and Sebastiani (hereafter ES) (2006a; 2007), Andreevskaia and Bergler (hereafter AB) (2006b; 2006a), and (Valitutti et al., 2004) assign polarity labels. WM, SM, and ES have evaluated their systems against manually annotated word-sense data. WM’s annotations are described above; SM’s are similar. In the scheme ES use (Cerini et al., 2007), senses are assigned three scores, for positivity,"
N09-1002,E06-1026,0,0.11017,"be subjective but not have any particular polarity. An example given by (Wilson et al., 2005) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range 11 of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, distinguishing S and O instances has often proven more difficult than subsequent polarity classification. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Thus, effective methods for S/O classification promise to improve performance for sentiment classification. In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). One reason is that different features may be relevant for the two subproblems"
N09-1002,P02-1053,0,0.00471185,"gh to wear on your wrist”; “a device intended to conserve water”) He sold his catch at the market. catch, haul – (the quantity that was caught; “the catch was only 10 fish”) =&gt; indefinite quantity – (an estimated quantity) WM performed an agreement study and report that good agreement (κ=0.74) can be achieved between human annotators labeling the subjectivity of senses. For a similar task, (Su and Markert, 2008) also report good agreement. 3 Related Work Many methods have been developed for automatically identifying subjective (opinion, sentiment, attitude, affect-bearing, etc.) words, e.g., (Turney, 2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004; Taboada et al., 2006; Takamura et al., 2006). Five groups have worked on subjectivity sense labeling. WM and Su and Markert (2008) (hereafter SM) assign S/O labels to senses, while Esuli and Sebastiani (hereafter ES) (2006a; 2007), Andreevskaia and Bergler (hereafter AB) (2006b; 2006a), and (Valitutti et al., 2004) assign polarity labels. WM, SM, and ES have evaluated their systems against manually annotated word-sense data. WM’s annotations are described above; SM’s are similar. In the scheme ES use (Cerini et al., 2007), senses are assigned three"
N09-1002,P06-1134,1,0.900403,"nding subjective words, if it would make sense for word- and sense-level approaches to work in tandem, or should we best view them as competing approaches? We give evidence suggesting that first identifying subjective words and then disambiguating their senses would be an effective approach to subjectivity sense labeling. 10 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 10–18, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics There are several motivations for assigning subjectivity labels to senses. First, (Wiebe and Mihalcea, 2006) provide evidence that word sense labels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation. Similarly, given subjectivity sense labels, word-sense disambiguation may potentially help contextual subjectivity analysis. In addition, as lexical resources such as WordNet are developed further, subjectivity labels would provide principled criteria for refining word senses, as well as for clustering similar meanings to create more coursegrained sense inventories. For many opinion mining applications, polarity (positive, negative) is"
N09-1002,H05-1044,1,0.269656,"rm grew. He absorbed the information quickly. UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. What’s the catch? Polarity (also called semantic orientation) is also important to NLP applications in sentiment analysis and opinion extraction. In review mining, for example, we want to know whether an opinion about a product is positive or negative. Even so, we believe there are strong motivations for a separate subjective/objective (S/O) classification as well. First, expressions may be subjective but not have any particular polarity. An example given by (Wilson et al., 2005) is Jerome says the hospital feels no different than a hospital in the states. An NLP application system may want to find a wide range 11 of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. Second, distinguishing S and O instances has often proven more difficult than subsequent polarity classification. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment"
N09-1002,W03-1017,0,0.020249,"ult than subsequent polarity classification. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Thus, effective methods for S/O classification promise to improve performance for sentiment classification. In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). One reason is that different features may be relevant for the two subproblems. For example, negation features are more important for polarity classification than for subjectivity classification. Note that some of our features require vertical links that are present in WordNet for nouns and verbs but not for other parts of speech. Thus we address nouns (leaving verbs to future work). There are other motivations for focusing on nouns. Relatively little work in subjectivity and sentiment analysis has focused on subjective nouns. Also"
N09-1002,C08-1104,0,\N,Missing
N09-1002,andreevskaia-bergler-2006-semantic,0,\N,Missing
N09-2030,W09-1126,1,0.28404,"all available nodes, i.e., both articles and categories. keyRatio= 0.01 keyRatio= 0.02 keyRatio= 0.04 keyRatio= 0.06 keyRatio= 0.08 keyRatio= 0.16 keyRatio= 0.32 Baseline keyRatio= 0.04 Precision 0.2 0.15 0.05 0 20 40 60 80 0.25 0.2 0.15 keyRatio= 0.01 keyRatio= 0.02 keyRatio= 0.04 keyRatio= 0.06 keyRatio= 0.08 keyRatio= 0.16 keyRatio= 0.32 Baseline keyRatio= 0.04 0.1 0.05 0 0 20 40 60 80 100 Top N topics returned Figure 4: Recall for automatic input text annotations than 350,000). Additional experiments performed against a set of documents from a source other than Wikipedia are reported in (Coursey et al., 2009). 4 Conclusions In this paper, we presented an unsupervised system for automatic topic identification, which relies on a biased graph centrality algorithm applied on a graph built from Wikipedia. Our experiments demonstrate the usefulness of external encyclopedic knowledge for the task of topic identification. Acknowledgments This work has been partially supported by award #CR72105 from the Texas Higher Education Coordinating Board and by an award from Google Inc. The authors are grateful to the Waikato group for making their data set available. References 0.1 0 0.3 Recall 3.2 Automatic Annota"
N10-1133,W00-0408,0,0.0440755,"Missing"
N10-1133,J02-4006,0,0.0230755,"Missing"
N10-1133,W03-0510,0,0.0274706,"Missing"
N10-1133,W04-1013,0,0.0279872,"n exponential complexity as it requires all possible sentence combinations of a document to be generated, constrained by a given word or sentence length. Thus the problem quickly becomes impractical as the number of sentences in a document increases and the compression ratio decreases. In this work, we try to overcome this bottleneck by using a large cluster of computers, and decomposing the task into smaller problems by using the given section boundaries or a linear text segmentation method. As a result of this exploration, we generate a probability density function (pdf) of the ROUGE score (Lin, 2004) distributions for four different domains, which shows the distribution of the evaluation scores for the generated extracts, and allows us to assess the difficulty of each domain for extractive summarization. Furthermore, using these pdfs, we introduce a new success measure for extractive summarization systems. Namely, given a system’s average score over a data set, we show how to calculate the per903 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 903–911, c Los Angeles, California, June 2010. 2010 Association for Computational Linguisti"
N10-1133,D07-1040,1,0.894754,"n our study, however, we only use recall scores. Note that, since for the legal, scientific, and literary domains we consider each section of a document independently, we are not performing a true exhaustive search for these domains, but rather solving a suboptimal problem, as we divide the number of words in the model summary to each section proportional to the section’s length. However, we believe that this is a fair assumption, as it has been shown repeatedly in the past that text segmentation helps improving the performance of text summarization systems (yen Kan et al., 1998; Nakao, 2000; Mihalcea and Ceylan, 2007). 5 Exhaustive Search Algorithm Let Eik = Si1 , Si2 , ..., Sik be the ith extract that has k sentences, and generated from a document D with n sentences D = S1 , S2 , . . . , Sn . Further, let len(Sj ) give the number of words in sentence Sj . We enforce that Eik satisfies the following constraints: len(Eik ) = len(Si1 ) + . . . + len(Sik ) ≥ L len(Eik−1 ) = len(Si1 ) + . . . + len(Sik−1 ) &lt; L where L is the length constraint on all the extracts of document D. We note that for any Eik , the order of the sentences in Eik−1 does not affect the ROUGE scores, since only the last sentence may be 7"
N10-1133,P00-1039,0,0.0367292,"l document. In our study, however, we only use recall scores. Note that, since for the legal, scientific, and literary domains we consider each section of a document independently, we are not performing a true exhaustive search for these domains, but rather solving a suboptimal problem, as we divide the number of words in the model summary to each section proportional to the section’s length. However, we believe that this is a fair assumption, as it has been shown repeatedly in the past that text segmentation helps improving the performance of text summarization systems (yen Kan et al., 1998; Nakao, 2000; Mihalcea and Ceylan, 2007). 5 Exhaustive Search Algorithm Let Eik = Si1 , Si2 , ..., Sik be the ith extract that has k sentences, and generated from a document D with n sentences D = S1 , S2 , . . . , Sn . Further, let len(Sj ) give the number of words in sentence Sj . We enforce that Eik satisfies the following constraints: len(Eik ) = len(Si1 ) + . . . + len(Sik ) ≥ L len(Eik−1 ) = len(Si1 ) + . . . + len(Sik−1 ) &lt; L where L is the length constraint on all the extracts of document D. We note that for any Eik , the order of the sentences in Eik−1 does not affect the ROUGE scores, since only"
N10-1133,W97-0710,0,0.135241,"Missing"
N10-1133,P01-1064,0,0.0358359,"sections, and create extracts for each section such that the length of the extract is proportional to the length of the section in the original document. For the legal and scientific domains, we use the given section boundaries (without considering the subsections for scientific documents). For the novels, we treat each chapter as a single document (since each chapter has its own summary), which is further divided into sections using a publicly available linear 4 http://berouge.com -n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 6 http://hadoop.apache.org/ 5 text segmentation algorithm by (Utiyama and Isahara, 2001).7 In all cases, we let the algorithm pick the number of segments automatically. To evaluate the sections, we modified ROUGE further so that it applies the length constraint to the extracts only, not to the model summaries. This is due to the fact that we evaluate the extracts of each section individually against the whole model summary, which is larger than the extract. This way, we can get an overall ROUGE recall score for a document extract, simply by summing up the recall scores of each section extracts. The precision score for the entire document can also be found by adding the weighted p"
N10-1133,W98-1123,0,0.0124813,"ion in the original document. In our study, however, we only use recall scores. Note that, since for the legal, scientific, and literary domains we consider each section of a document independently, we are not performing a true exhaustive search for these domains, but rather solving a suboptimal problem, as we divide the number of words in the model summary to each section proportional to the section’s length. However, we believe that this is a fair assumption, as it has been shown repeatedly in the past that text segmentation helps improving the performance of text summarization systems (yen Kan et al., 1998; Nakao, 2000; Mihalcea and Ceylan, 2007). 5 Exhaustive Search Algorithm Let Eik = Si1 , Si2 , ..., Sik be the ith extract that has k sentences, and generated from a document D with n sentences D = S1 , S2 , . . . , Sn . Further, let len(Sj ) give the number of words in sentence Sj . We enforce that Eik satisfies the following constraints: len(Eik ) = len(Si1 ) + . . . + len(Sik ) ≥ L len(Eik−1 ) = len(Si1 ) + . . . + len(Sik−1 ) &lt; L where L is the length constraint on all the extracts of document D. We note that for any Eik , the order of the sentences in Eik−1 does not affect the ROUGE score"
N10-1133,W04-3252,1,\N,Missing
N10-1133,W97-0704,0,\N,Missing
N15-3024,E99-1042,0,0.366733,"learning tools developed to date is to provide assistance to those with limited language abilities, including students learning a second or a foreign language or people suffering from disabilities such as aphasia. These tools draw on research in education, which found that text adaptation can improve 116 Rada Mihalcea University of Michigan mihalcea@umich.edu the reading comprehension skills for learners of English (Yano et al., 1994; Carlo et al., 2004). The language learning technology often consists of methods for text simplification and adaptation, which is performed either at syntactic (Carroll et al., 1999; Siddharthan et al., 2004) or lexical level (Carroll et al., 1998; Devlin et al., 2000; Canning and Tait, 1999; Burstein et al., 2007). Work has also been carried out on the prediction and simplification of difficult technical text (Elhadad, 2006a; Elhadad, 2006b) and on the use of syntactic constraints for translations in context (Grefenstette and Segond, 2003). In this paper, we describe an interface developed with the goal of assisting ESL students in their English reading activities. The interface builds upon a lexical substitution system that we developed, which provides synonyms and def"
N15-3024,hokamp-etal-2014-modeling,1,0.828686,"h the tool. We also used this interface with English middle school students whose primary language is English. The students had to read short excerpts of a book that was a part of their curriculum. Students were allowed to click on only one highlighted word per excerpt. In this experiment, supplementary information was provided from WordNet. There was a postreading quiz to evaluate the students understanding of the words. By training a regression model on the interaction features collected during the reading exercises, we were able to accurately predict students’ performance on the post-quiz (Hokamp et al., 2014). We have now enabled the S A LSA interface to provide feedback on arbitrary English content from the web. By implementing the tool as a browser extension, we are able to show inline additional information about text on any web page, even when the content is dynamically generated. The interface also collects both explicit and implicit feedback. The explicit feedback is collected via upvotes and downvotes on feedback items. The implicit feedback is based on the user interactions with the system while they are reading. Currently, we collect several kinds of interactions. These interactions inclu"
N15-3024,S07-1009,0,0.0261252,"face developed with the goal of assisting ESL students in their English reading activities. The interface builds upon a lexical substitution system that we developed, which provides synonyms and definitions for target words in context. We first give a brief overview of the lexical substitution task, and then present our system S A LSA (Sinha and Mihalcea, 2014) and (Sinha and Mihalcea, 2012). We then describe the functionality of the interface, and the interaction that a user can have with this interface. 2 Lexical Substitution Lexical substitution, also known as contextual synonym expansion (McCarthy and Navigli, 2007), involves replacing a certain word in a given context with another, suitable word, such that the overall meaning of the word and the sentence are unchanged. As an example, see the four sentences in Table 1, drawn from the development data from the S EMEVAL -2007 lexical substitution task. In the first sentence, for instance, assuming we choose bright as Proceedings of NAACL-HLT 2015, pages 116–120, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics the target word, a suitable substitute could be brilliant, which would both maintain the meaning of the tar"
N15-3024,C04-1129,0,0.0110127,"ped to date is to provide assistance to those with limited language abilities, including students learning a second or a foreign language or people suffering from disabilities such as aphasia. These tools draw on research in education, which found that text adaptation can improve 116 Rada Mihalcea University of Michigan mihalcea@umich.edu the reading comprehension skills for learners of English (Yano et al., 1994; Carlo et al., 2004). The language learning technology often consists of methods for text simplification and adaptation, which is performed either at syntactic (Carroll et al., 1999; Siddharthan et al., 2004) or lexical level (Carroll et al., 1998; Devlin et al., 2000; Canning and Tait, 1999; Burstein et al., 2007). Work has also been carried out on the prediction and simplification of difficult technical text (Elhadad, 2006a; Elhadad, 2006b) and on the use of syntactic constraints for translations in context (Grefenstette and Segond, 2003). In this paper, we describe an interface developed with the goal of assisting ESL students in their English reading activities. The interface builds upon a lexical substitution system that we developed, which provides synonyms and definitions for target words i"
N18-1190,Q18-1008,0,0.0850707,"a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stability of word embeddings. We also explore the stability of embeddings, but focus on a broader range of factors, and consider the effect of stability on downstream tasks. In contrast, Antoniak and Mimno focus on using word embeddings to analyze language (e.g., Garg et al., 2018), rather than to perform tasks. At a higher level of granularity, Tan et al. (2015) analyze word embedding spaces by comparing two spaces. They do this by linearly transforming one space into another space, and they show that words have different usage properties in diffe"
N18-1190,N15-1184,0,0.077191,"Missing"
N18-1190,D17-1242,1,0.835173,"roughout the rest of the paper. 1.0 1024 9998 2 32 20 Frequency in Training Corpus (approx. log-based scale) 1 0.4 0.2 0.0 Figure 2: Stability of GloVe on the PTB. Stability is measured across ten randomized embedding spaces trained on the training data of the PTB (determined using language modeling splits (Mikolov et al., 2010)). Each word is placed in a frequency bucket (left y-axis) and stability is determined using a varying number of nearest neighbors for each frequency bucket (right yaxis). Each row is normalized, and boxes with more than 0.01 of the row’s mass are outlined. tion (e.g., Garimella et al., 2017), where the top ten results are considered in the final evaluation metric. To give some intuition for how changing the number of nearest neighbors affects our stability metric, consider Figure 2. This graph shows how the stability of GloVe changes with the frequency of the word and the number of neighbors used to calculate stability; please see the figure caption for a more detailed explanation of how this graph is structured. Within each frequency bucket, the stability is consistent across varying numbers of neighbors. Ten nearest neighbors performs approximately as well as a higher number of"
N18-1190,C16-1262,0,0.132868,"–2102 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics word2vec (Mikolov et al., 2013b), and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the effect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stability of word embeddings. We also explore the stability of embeddings, but focus on a broader range of factors, and consider the effect of stability on downstream tasks. In contrast, Antoniak and Mimno focus on using word embeddings"
N18-1190,2005.mtsummit-papers.11,0,0.00718392,", we consider the number of different POS present. For a finer-grained representation, we use the number of different WordNet senses associated with the word (Miller, 1995; Fellbaum, 1998). We also consider the number of syllables in a word, determined using the CMU Pronuncing Dictionary (Weide, 1998). If the word is not present in the dictionary, then this is set to zero. 4.3 Data Properties Data features capture properties of the training data (and the word in relation to the training data). For this model, we gather data from two sources: New York Times (NYT) (Sandhaus, 2008) and Europarl (Koehn, 2005). Overall, we consider seven domains of data: (1) NYT - U.S., (2) NYT - New York and Region, (3) NYT - Business, (4) NYT Arts, (5) NYT - Sports, (6) All of the data from 2095 Dataset NYT US NYT NY NYT Business NYT Arts NYT Sports All NYT Europarl Sentences 13,923 36,792 21,048 28,161 21,610 121,534 2,297,621 Vocab. Size 5,787 11,182 7,212 10,508 5,967 24,144 43,888 Num. Tokens / Vocab. Size 64.37 80.41 75.96 65.29 77.85 117.98 1,394.28 Table 3: Dataset statistics. domains 1-5 (denoted “All NYT”), and (7) All of English Europarl. Table 3 shows statistics about these datasets. The first five dom"
N18-1190,Q15-1016,0,0.0577765,"well as a small, but growing, amount of work analyzing the properties of word embeddings. Here, we explore three different embedding methods: PPMI (Bullinaria and Levy, 2007), 2092 Proceedings of NAACL-HLT 2018, pages 2092–2102 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics word2vec (Mikolov et al., 2013b), and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the effect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stabili"
N18-1190,J93-2004,0,0.0610169,"Missing"
N18-1190,S07-1009,0,0.032384,"Missing"
N18-1190,D17-1308,0,0.0646003,"nd Levy, 2007), 2092 Proceedings of NAACL-HLT 2018, pages 2092–2102 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics word2vec (Mikolov et al., 2013b), and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the effect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stability of word embeddings. We also explore the stability of embeddings, but focus on a broader range of factors, and consider the effect of stability on downstream tasks. I"
N18-1190,P16-1013,0,0.12326,"es) are often unstable. We provide empirical evidence for how various factors contribute to the stability of word embeddings, and we analyze the effects of stability on downstream tasks. 100 75 50 1 1e-01 25 1e-02 10 1e-03 5 2 20 24 28 212 216 Frequency of Word in PTB (log scale) Introduction Word embeddings are low-dimensional, dense vector representations that capture semantic properties of words. Recently, they have gained tremendous popularity in Natural Language Processing (NLP) and have been used in tasks as diverse as text similarity (Kenter and De Rijke, 2015), part-of-speech tagging (Tsvetkov et al., 2016), sentiment analysis (Faruqui et al., 2015), and machine translation (Mikolov et al., 2013a). Although word embeddings are widely used across NLP, their stability has not yet been fully evaluated and understood. In this paper, we explore the factors that play a role in the stability of word embeddings, including properties of the data, properties of the algorithm, and properties of the words. We find that word embeddings exhibit substantial instabilities, which can have implications for downstream tasks. Using the overlap between nearest neighbors in an embedding space as a measure of stabilit"
N18-1190,D14-1162,0,0.10588,"at word embeddings are commonly used for. To our knowledge, this is the first study comprehensively examining the factors behind instability. 2 Related Work There has been much recent interest in the applications of word embeddings, as well as a small, but growing, amount of work analyzing the properties of word embeddings. Here, we explore three different embedding methods: PPMI (Bullinaria and Levy, 2007), 2092 Proceedings of NAACL-HLT 2018, pages 2092–2102 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics word2vec (Mikolov et al., 2013b), and GloVe (Pennington et al., 2014). Various aspects of the embedding spaces produced by these algorithms have been previously studied. Particularly, the effect of parameter choices has a large impact on how all three of these algorithms behave (Levy et al., 2015). Further work shows that the parameters of the embedding algorithm word2vec influence the geometry of word vectors and their context vectors (Mimno and Thompson, 2017). These parameters can be optimized; Hellrich and Hahn (2016) posit optimal parameters for negative sampling and the number of epochs to train for. They also demonstrate that in addition to parameter set"
N18-1190,petrov-etal-2012-universal,0,0.041853,"S) of the word. Both of these are represented as bags-of-words of all possible POS, and are determined by looking at the primary (most frequent) and secondary (second most frequent) POS of the word in the Brown corpus3 (Francis and Kucera, 1979). If the word is not present in the Brown corpus, then all of these POS features are set to zero. To get a coarse-grained representation of the 3 Here, we use the universal tagset, which consists of twelve possible POS: adjective, adposition, adverb, conjunction, determiner / article, noun, numeral, particle, pronoun, verb, punctuation mark, and other (Petrov et al., 2012). Word Properties Primary part-of-speech Secondary part-of-speech # Parts-of-speech # WordNet senses Syllables Data Properties Raw frequency in corpus A Raw frequency in corpus B Diff. in raw frequency Vocab. size of corpus A Vocab. size of corpus B Diff. in vocab. size Overlap in corpora vocab. Domains present Do the domains match? Training position in A Training position in B Diff. in training position Algorithm Properties Algorithms present Do the algorithms match? Embedding dimension of A Embedding dimension of B Diff. in dimension Do the dimensions match? Adjective Noun 2 3 5 106 669 563"
N18-1190,P15-2108,0,0.0226935,"t in addition to parameter settings, word properties, such as word ambiguity, affect embedding quality. In addition to exploring word and algorithmic parameters, concurrent work by Antoniak and Mimno (2018) evaluates how document properties affect the stability of word embeddings. We also explore the stability of embeddings, but focus on a broader range of factors, and consider the effect of stability on downstream tasks. In contrast, Antoniak and Mimno focus on using word embeddings to analyze language (e.g., Garg et al., 2018), rather than to perform tasks. At a higher level of granularity, Tan et al. (2015) analyze word embedding spaces by comparing two spaces. They do this by linearly transforming one space into another space, and they show that words have different usage properties in different domains (in their case, Twitter and Wikipedia). Finally, embeddings can be analyzed using second-order properties of embeddings (e.g., how a word relates to the words around it). NewmanGriffis and Fosler-Lussier (2017) validate the usefulness of second-order properties, by demonstrating that embeddings based on second-order properties perform as well as the typical first-order embeddings. Here, we use s"
N18-1200,P14-5010,0,0.0024798,"multiple cues extracted from both textual and multimedia information. A unified learning framework is proposed to enable the joint optimization over the automatically labeled and unlabeled data, along with multiple semantic cues. 5.1 Character Identification and Extraction In this work, we do not consider the set of character names as given because we want to build a model that can be generalized to unseen movies. This strict setting adds to the problem’s complexity. To extract the list of characters from the subtitles, we use the Named Entity Recognizer (NER) in the Stanford CoreNLP toolkit (Manning et al., 2014). The output is a long list of person names that are mentioned in the dialogue. This list is prone to errors including, but not limited to, nouns that are misclassified by the NER as person’s name such as “Dad” and “Aye”, names that are irrelevant to the movie such as “Superman” or named animals, or uncaptured character names. To clean the extracted names list of each movie, we cluster these names based on string minimum edit distance and their gender. From each cluster, we then pick a name to represent it based on its frequency in the dialogue. The result of this step consists of name cluster"
N19-1175,W16-0806,0,0.0472112,"show promising results by successfully classifying truthful and deceptive behaviors. Currently, the dataset provides feature sets drawn from three modalities (verbal and non-verbal, as well as dialogue) but can be further analyzed to extract additional features from other modalities such as speech. Specifically, the dialogue has the potential to add many more layers of information by systematically analyzing verbal, nonverbal, and speech patterns between the participants. These patterns can lead to detectable differences between actions and reactions within a dialogue (Tsunomori et al., 2015; Levitan et al., 2016). We consider analyzing such patterns as a future research venue to expand the analyses on our dataset. A challenge while using this data is the current imbalance of truthful and deceptive feature sets, which can have a detrimental effect on classification performance. However, there are several other possible ways to address this issue other than down-sampling as we did during our experiments. For instance, other computational methods could be explored, such as one-class classification tasks. Such models train on a dataset from the same distribution and classify new data as being similar or d"
N19-1175,N18-1176,0,0.0257506,"Missing"
N19-1175,P09-2078,1,0.851961,"Missing"
N19-1175,wittenburg-etal-2006-elan,0,0.0203714,"nsists of 2 hours and 24 minutes of video. The average length of a video is six minutes and contains around three rounds of the game (this varies depending on the score and on whether additional time was available for extra rounds). Each video features a different guest and Jimmy Fallon, resulting in 26 unique participants, with 6 of them being males and 20 females. 3.2 Annotation of Multimodal Communication Behaviors To capture the non-verbal behavior of the participants, each video is initially segmented based on the conversation turn-taking and annotated with the help of the ELAN software (Wittenburg et al., 2006). ELAN provides a multimodal annotation platform on which audiovisual recordings are annotated in a multi-level tier structure. In our case, we defined the following structure to annotate both types of behavior: host verbal, host non-verbal, guest verbal, and guest non-verbal. 3.2.1 Non-verbal Behaviors To annotate facial and communication behaviors, we use MUMIN, a multimodal coding scheme that is used to study gestures and facial displays in interpersonal communication with a focus on the role played by multimodal expressions for feedback, turn management, and sequencing (Allwood 2 The video"
N19-1175,N13-1053,0,0.0426335,"Missing"
N19-1175,P11-1032,0,0.405302,"Missing"
N19-1175,D15-1281,1,0.850436,"Missing"
N19-1175,perez-rosas-etal-2014-multimodal,1,0.750624,"Missing"
P01-1037,A00-1023,0,\N,Missing
P01-1037,C00-1043,1,\N,Missing
P01-1037,H94-1052,0,\N,Missing
P01-1037,P00-1071,1,\N,Missing
P01-1037,A00-1025,0,\N,Missing
P01-1037,P96-1025,0,\N,Missing
P01-1037,A00-1021,0,\N,Missing
P01-1037,P95-1037,0,\N,Missing
P01-1037,A00-1041,0,\N,Missing
P04-3020,N03-1020,0,0.688299,"15 [0.84] 0.55 8 [0.70] 20 0.35 0.30 19 [0.15] 9 [1.83] 0.59 0.15 4 Evaluation The TextRank sentence extraction algorithm is evaluated in the context of a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002). For each article, TextRank generates a 100-words summary — the task undertaken by other systems participating in this single document summarization task. For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003a). Two manually produced reference summaries are provided, and used in the evaluation process4 . 2 In single documents, sentences with highly similar content are very rarely if at all encountered, and therefore sentence redundancy does not have a significant impact on the summarization of individual texts. This may not be however the case with multiple document summarization, where a redundancy removal technique – such as a maximum threshold imposed on the sentence similarity – needs to be implemented. 3 Weights are listed to the right or above the edge they correspond to. Similar weights are"
P04-3020,W03-0510,0,0.0222306,"15 [0.84] 0.55 8 [0.70] 20 0.35 0.30 19 [0.15] 9 [1.83] 0.59 0.15 4 Evaluation The TextRank sentence extraction algorithm is evaluated in the context of a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002). For each article, TextRank generates a 100-words summary — the task undertaken by other systems participating in this single document summarization task. For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003a). Two manually produced reference summaries are provided, and used in the evaluation process4 . 2 In single documents, sentences with highly similar content are very rarely if at all encountered, and therefore sentence redundancy does not have a significant impact on the summarization of individual texts. This may not be however the case with multiple document summarization, where a redundancy removal technique – such as a maximum threshold imposed on the sentence similarity – needs to be implemented. 3 Weights are listed to the right or above the edge they correspond to. Similar weights are"
P04-3020,C04-1162,1,0.456599,"ather than relying only on local vertex-specific information. A similar line of thinking can be applied to lexical or semantic graphs extracted from natural language documents, resulting in a graph-based ranking model called TextRank (Mihalcea and Tarau, 2004), which can be used for a variety of natural language processing applications where knowledge drawn from an entire text is used in making local ranking/selection decisions. Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004). In this paper, we investigate a range of graphbased ranking algorithms, and evaluate their application to automatic unsupervised sentence extraction in the context of a text summarization task. We show that the results obtained with this new unsupervised method are competitive with previously developed state-of-the-art systems. 2 Graph-Based Ranking Algorithms Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on information drawn from the graph structure. In this section, we present three graph-based ranking algorithms – previou"
P04-3020,W97-0710,0,0.0822621,"rding to the reference summaries for this text). Another important advantage of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries, or longer more explicative summaries, consisting of more than 100 words. 5 Related Work Sentence extraction is considered to be an important first step for automatic text summarization. As a consequence, there is a large body of work on algorithms 5 for sentence extraction undertaken as part of the DUC evaluation exercises. Previous approaches include supervised learning (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, or intra-document similarities (Salton et al., 1997). It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. Notice that rows two and four in Table 1 are in fact redundant, since the “hub” (“weakness”) variations of the HITS (Positional) algorithms can be derived from their “"
P04-3020,W04-3252,1,\N,Missing
P05-3013,N03-1020,0,0.411193,"Missing"
P05-3013,P04-3020,1,0.511881,"most important for the understanding of a given document. Some of the most successful approaches to extractive summarization consist of supervised algorithms that attempt to learn what makes a good summary by training on collections of summaries built for a relatively large number of training documents, e.g. (Hirao et al., 2002), (Teufel and Moens, 1997). However, the price paid for the high performance of such supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new type of data. TextRank (Mihalcea and Tarau, 2004), (Mihalcea, 2004) is specifiRanking algorithms, such as Kleinberg’s HIT S algorithm (Kleinberg, 1999) or Google’s P ageRank (Brin and Page, 1998) have been traditionally and successfully used in Web-link analysis, social networks, and more recently in text processing applications. In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information. The basic idea implemented by the ranking model is that of voting or recommen"
P05-3013,W97-0710,0,0.0309651,"e, techniques for efficient automatic text summarization are essential to improve the access to such information. Algorithms for extractive summarization are typically based on techniques for sentence extraction, and attempt to identify the set of sentences that are most important for the understanding of a given document. Some of the most successful approaches to extractive summarization consist of supervised algorithms that attempt to learn what makes a good summary by training on collections of summaries built for a relatively large number of training documents, e.g. (Hirao et al., 2002), (Teufel and Moens, 1997). However, the price paid for the high performance of such supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new type of data. TextRank (Mihalcea and Tarau, 2004), (Mihalcea, 2004) is specifiRanking algorithms, such as Kleinberg’s HIT S algorithm (Kleinberg, 1999) or Google’s P ageRank (Brin and Page, 1998) have been traditionally and successfully used in Web-link analysis, social networks, and more recently in text processing applications. In short, a graph-based ranking algorithm is a way of deciding on the import"
P05-3013,W04-3252,1,\N,Missing
P05-3014,W04-0827,0,0.0580145,"Missing"
P05-3014,W02-0814,0,0.120494,"Missing"
P05-3014,W04-0838,1,0.850014,"Missing"
P05-3014,C02-1039,1,0.904454,"Missing"
P05-3014,H93-1061,0,0.347444,"Missing"
P05-3014,W04-0864,0,0.0376515,"Missing"
P05-3014,S01-1031,1,\N,Missing
P06-1134,P97-1023,0,0.531349,"es to speech (or writing) events expressing private states: UCC/Disciples leaders roundly condemned the Iranian President’s verbal assault on Israel. The editors of the left-leaning paper attacked the new House Speaker. (3) expressive subjective elements: He would be quite a catch. What’s the catch? That doctor is a quack. Work on automatic subjectivity analysis falls into three main areas. The first is identifying words and phrases that are associated with subjectivity, for example, that think is associated with private states and that beautiful is associated with positive sentiments (e.g., (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kamps and Marx, 2002; Turney, 2002; Esuli and Sebastiani, 2005)). Such judgments are made for words. In contrast, our end task (in Section 4) is to assign subjectivity labels to word senses. The second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g.,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al., 2003; Hu and Liu, 2004)). The third exploits automatic subjectivity analysis in app"
P06-1134,O97-1002,0,0.0146293,". Starting with a given ambiguous word w, we first find the distributionally similar words using the method of (Lin, 1998) applied to the automatically parsed texts of the British National Corpus. Let DSW = dsw1 , dsw2 , ..., dswn be the list of top-ranked distributionally similar words, sorted in decreasing order of their similarity. Next, for each sense wsi of the word w, we determine the similarity with each of the words in the list DSW , using a WordNet-based measure of semantic similarity (wnss). Although a large number of such word-to-word similarity measures exist, we chose to use the (Jiang and Conrath, 1997) measure, since it was found both to be efficient and to provide the best results in previous experiments involving word sense ranking (McCarthy et al., 2004)5 . For distributionally similar words 5 Note that unlike the above measure of distributional simAlgorithm 1 Word Sense Subjectivity Score Input: Word sense wi Input: Distributionally similar words DSW = {dswj |j = 1..n} Output: Subjectivity score subj(wi ) 1: subj(wi ) = 0 2: totalsim = 0 3: for j = 1 to n do 4: Instsj = all instances of dswj in the MPQA corpus 5: for k in Instsj do 6: if k is in a subj. expr. in MPQA corpus then 7: subj"
P06-1134,C04-1200,0,0.866003,"econd is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g.,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al., 2003; Hu and Liu, 2004)). The third exploits automatic subjectivity analysis in applications such as review classification (e.g., (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g., (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g., (Kim and Hovy, 2004)), information extraction (e.g., (Riloff et al., 2005)), 1 Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation. 2 These distinctions are not strictly needed for this paper, but may help the reader appreciate the examples given below. and question answering (e.g., (Yu and Hatzivassiloglou, 2003; Stoyanov et al., 2005)). Most manual subjectivity annotation research has focused on annotating words, out of context (e.g., (Heise, 2001)), or sentences and phrases in the context of a tex"
P06-1134,W02-1006,0,0.0253206,"gh sense-specific keywords determined as a list of at most five words occurring at least three times in the contexts defining a certain word sense. This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of S ENSEVAL systems. The parameters for sense-specific keyword selection were determined through cross-fold validation on the training set. The features are integrated in a Naive Bayes classifier, which was selected mainly for its performance in previous work showing that it can lead to a state-of-the-art disambiguation system given the features we consider (Lee and Ng, 2002). The experiments are performed on the set of ambiguous nouns from the S ENSEVAL -3 English lexical sample evaluation (Mihalcea et al., 2004). We use the rule-based subjective sentence classifier of (Riloff and Wiebe, 2003) to assign an S, O, or B label to all the training and test examples pertaining to these ambiguous words. This subjectivity annotation tool targets sentences, rather than words or paragraphs, and therefore the tool is fed with sentences. We also include a surrounding context of two additional sentences, because the classifier considers some contextual information. Our hypoth"
P06-1134,P98-2127,0,0.0185016,"e can derive information about a word sense based on information drawn from words that are distributionally similar to the given word sense. This idea relates to the unsupervised word sense ranking algorithm described in (McCarthy et al., 2004). Note, however, that (McCarthy et al., 2004) used the information about distributionally similar words to approximate corpus frequencies for word senses, whereas we target the estimation of a property of a given word sense (the “subjectivity”). Starting with a given ambiguous word w, we first find the distributionally similar words using the method of (Lin, 1998) applied to the automatically parsed texts of the British National Corpus. Let DSW = dsw1 , dsw2 , ..., dswn be the list of top-ranked distributionally similar words, sorted in decreasing order of their similarity. Next, for each sense wsi of the word w, we determine the similarity with each of the words in the list DSW , using a WordNet-based measure of semantic similarity (wnss). Although a large number of such word-to-word similarity measures exist, we chose to use the (Jiang and Conrath, 1997) measure, since it was found both to be efficient and to provide the best results in previous expe"
P06-1134,P04-1036,0,0.104809,"ve reactions to the condition. One annotator judged only the sense (giving tag O), while the second considered the hypernym as well (giving tag UB). 4 Automatic Assessment of Word Sense Subjectivity Encouraged by the results of the agreement study, we devised a method targeting the automatic annotation of word senses for subjectivity. The main idea behind our method is that we can derive information about a word sense based on information drawn from words that are distributionally similar to the given word sense. This idea relates to the unsupervised word sense ranking algorithm described in (McCarthy et al., 2004). Note, however, that (McCarthy et al., 2004) used the information about distributionally similar words to approximate corpus frequencies for word senses, whereas we target the estimation of a property of a given word sense (the “subjectivity”). Starting with a given ambiguous word w, we first find the distributionally similar words using the method of (Lin, 1998) applied to the automatically parsed texts of the British National Corpus. Let DSW = dsw1 , dsw2 , ..., dswn be the list of top-ranked distributionally similar words, sorted in decreasing order of their similarity. Next, for each sens"
P06-1134,W04-0807,1,0.308658,"word sense. This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of S ENSEVAL systems. The parameters for sense-specific keyword selection were determined through cross-fold validation on the training set. The features are integrated in a Naive Bayes classifier, which was selected mainly for its performance in previous work showing that it can lead to a state-of-the-art disambiguation system given the features we consider (Lee and Ng, 2002). The experiments are performed on the set of ambiguous nouns from the S ENSEVAL -3 English lexical sample evaluation (Mihalcea et al., 2004). We use the rule-based subjective sentence classifier of (Riloff and Wiebe, 2003) to assign an S, O, or B label to all the training and test examples pertaining to these ambiguous words. This subjectivity annotation tool targets sentences, rather than words or paragraphs, and therefore the tool is fed with sentences. We also include a surrounding context of two additional sentences, because the classifier considers some contextual information. Our hypothesis motivating the use of a sentence-level subjectivity classifier is that instances of subjective senses are more likely to be in subjectiv"
P06-1134,P96-1006,0,0.0609969,"res. 9 The break-even point (Lewis, 1992) is a standard measure used in conjunction with precision-recall evaluations. It represents the value where precision and recall become equal. Specifically, we use the current word and its partof-speech, a local context of three words to the left and right of the ambiguous word, the parts-ofspeech of the surrounding words, and a global context implemented through sense-specific keywords determined as a list of at most five words occurring at least three times in the contexts defining a certain word sense. This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of S ENSEVAL systems. The parameters for sense-specific keyword selection were determined through cross-fold validation on the training set. The features are integrated in a Naive Bayes classifier, which was selected mainly for its performance in previous work showing that it can lead to a state-of-the-art disambiguation system given the features we consider (Lee and Ng, 2002). The experiments are performed on the set of ambiguous nouns from the S ENSEVAL -3 English lexical sample evaluation (Mihalcea et al., 2004). We use the rule-based subjective sentence classifier"
P06-1134,P04-1035,0,0.205261,"ebastiani, 2005)). Such judgments are made for words. In contrast, our end task (in Section 4) is to assign subjectivity labels to word senses. The second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g.,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al., 2003; Hu and Liu, 2004)). The third exploits automatic subjectivity analysis in applications such as review classification (e.g., (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g., (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g., (Kim and Hovy, 2004)), information extraction (e.g., (Riloff et al., 2005)), 1 Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation. 2 These distinctions are not strictly needed for this paper, but may help the reader appreciate the examples given below. and question answering (e.g., (Yu and Hatzivassiloglou, 2003; Stoyanov et al., 2005)). Most manual sub"
P06-1134,H05-1043,0,0.0577035,"to assign subjectivity labels to word senses. The second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g.,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al., 2003; Hu and Liu, 2004)). The third exploits automatic subjectivity analysis in applications such as review classification (e.g., (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g., (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g., (Kim and Hovy, 2004)), information extraction (e.g., (Riloff et al., 2005)), 1 Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation. 2 These distinctions are not strictly needed for this paper, but may help the reader appreciate the examples given below. and question answering (e.g., (Yu and Hatzivassiloglou, 2003; Stoyanov et al., 2005)). Most manual subjectivity annotation research has focused on annotating words, out of context (e.g., (Heise, 2001)), or s"
P06-1134,W03-1014,1,0.344348,"mple, that think is associated with private states and that beautiful is associated with positive sentiments (e.g., (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kamps and Marx, 2002; Turney, 2002; Esuli and Sebastiani, 2005)). Such judgments are made for words. In contrast, our end task (in Section 4) is to assign subjectivity labels to word senses. The second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g.,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al., 2003; Hu and Liu, 2004)). The third exploits automatic subjectivity analysis in applications such as review classification (e.g., (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g., (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g., (Kim and Hovy, 2004)), information extraction (e.g., (Riloff et al., 2005)), 1 Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation. 2 These distin"
P06-1134,H05-1116,1,0.754642,"e.g., (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g., (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g., (Kim and Hovy, 2004)), information extraction (e.g., (Riloff et al., 2005)), 1 Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation. 2 These distinctions are not strictly needed for this paper, but may help the reader appreciate the examples given below. and question answering (e.g., (Yu and Hatzivassiloglou, 2003; Stoyanov et al., 2005)). Most manual subjectivity annotation research has focused on annotating words, out of context (e.g., (Heise, 2001)), or sentences and phrases in the context of a text or conversation (e.g., (Wiebe et al., 2005)). The new annotations in this paper are instead targeting the annotation of word senses. 3 Human Judgment of Word Sense Subjectivity To explore our hypothesis that subjectivity may be associated with word senses, we developed a manual annotation scheme for assigning subjectivity labels to WordNet senses,3 and performed an inter-annotator agreement study to assess its reliability. Sens"
P06-1134,P02-1053,0,0.0172673,"es leaders roundly condemned the Iranian President’s verbal assault on Israel. The editors of the left-leaning paper attacked the new House Speaker. (3) expressive subjective elements: He would be quite a catch. What’s the catch? That doctor is a quack. Work on automatic subjectivity analysis falls into three main areas. The first is identifying words and phrases that are associated with subjectivity, for example, that think is associated with private states and that beautiful is associated with positive sentiments (e.g., (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kamps and Marx, 2002; Turney, 2002; Esuli and Sebastiani, 2005)). Such judgments are made for words. In contrast, our end task (in Section 4) is to assign subjectivity labels to word senses. The second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g.,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al., 2003; Hu and Liu, 2004)). The third exploits automatic subjectivity analysis in applications such as review classification (e.g., (T"
P06-1134,W03-1017,0,0.893463,"ciated with private states and that beautiful is associated with positive sentiments (e.g., (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kamps and Marx, 2002; Turney, 2002; Esuli and Sebastiani, 2005)). Such judgments are made for words. In contrast, our end task (in Section 4) is to assign subjectivity labels to word senses. The second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g.,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al., 2003; Hu and Liu, 2004)). The third exploits automatic subjectivity analysis in applications such as review classification (e.g., (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g., (Yi et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g., (Kim and Hovy, 2004)), information extraction (e.g., (Riloff et al., 2005)), 1 Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation. 2 These distinctions are not strictly needed"
P06-1134,H05-2017,0,\N,Missing
P06-1134,C98-2122,0,\N,Missing
P07-1123,E06-1027,0,0.0591854,"are further classified according to polarity (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). In fact, the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification. This is reported in studies of manual annotation of phrases (Takamura et al., 2006), recognizing contextual polarity of expressions (Wilson et al., 2005), and sentiment tagging of words and word senses (Andreevskaia and Bergler, 2006; Esuli and Sebastiani, 2006). Second, an NLP application may seek a wide range of types of subjectivity attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. For instance, the opinion tracking system Lydia (Lloyd et al., 2005) gives separate ratings for subjectivity and sentiment. These can be detected with subjectivity analysis but not by a method focused only on sentiment. There is world-wide interest in text analysis applications. While work on subjectivity analysis in other languages is growing (e.g., Japanese"
P07-1123,H05-1073,0,0.33515,"Missing"
P07-1123,E06-2031,0,0.0863299,"Missing"
P07-1123,E06-1025,0,0.568579,"nd uses an English subjectivity classifier and a parallel corpus to create target-language training data for developing a statistical classifier. 2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news (Lloyd et al., 2005; Balog et al., 2006), review classification (Turney, 2002; Pang et al., 2002), mining opinions from product reviews (Hu and Liu, 2004), automatic expressive text-to-speech synthesis (Alm et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), and question answering (Yu and Hatzivassiloglou, 2003). 976 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976–983, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons. First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage appro"
P07-1123,I05-1001,0,0.040085,"tivity attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. For instance, the opinion tracking system Lydia (Lloyd et al., 2005) gives separate ratings for subjectivity and sentiment. These can be detected with subjectivity analysis but not by a method focused only on sentiment. There is world-wide interest in text analysis applications. While work on subjectivity analysis in other languages is growing (e.g., Japanese data are used in (Takamura et al., 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al., 2005), and German data are used in (Kim and Hovy, 2006)), much of the work in subjectivity analysis has been applied to English data. Creating corpora and lexical resources for a new language is very time consuming. In general, we would like to leverage resources already developed for one language to more rapidly create subjectivity analysis tools for a new one. This motivates our exploration and use of cross-lingual lexicon translations and annotation projections. Most if not all work on subjectivity analysis has been carried out in a monolingual framework. We 977 are not aware of multi-lingual wo"
P07-1123,W06-1642,0,0.099884,"NLP application may seek a wide range of types of subjectivity attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. For instance, the opinion tracking system Lydia (Lloyd et al., 2005) gives separate ratings for subjectivity and sentiment. These can be detected with subjectivity analysis but not by a method focused only on sentiment. There is world-wide interest in text analysis applications. While work on subjectivity analysis in other languages is growing (e.g., Japanese data are used in (Takamura et al., 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al., 2005), and German data are used in (Kim and Hovy, 2006)), much of the work in subjectivity analysis has been applied to English data. Creating corpora and lexical resources for a new language is very time consuming. In general, we would like to leverage resources already developed for one language to more rapidly create subjectivity analysis tools for a new one. This motivates our exploration and use of cross-lingual lexicon translations and annotation projections. Most if not all work on subjectivity analysis has been carried out in a monolingual framewo"
P07-1123,N06-1026,0,0.452624,"cs While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons. First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage approach is often beneficial, in which subjective instances are distinguished from objective ones, and then the subjective instances are further classified according to polarity (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). In fact, the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification. This is reported in studies of manual annotation of phrases (Takamura et al., 2006), recognizing contextual polarity of expressions (Wilson et al., 2005), and sentiment tagging of words and word senses (Andreevskaia and Bergler, 2006; Esuli and Sebastiani, 2006). Second, an NLP application may seek a wide range of types of subjectivity at"
P07-1123,H93-1061,0,0.0228724,"bjectivity or sentiment classification, e.g., (Pang et al., 2002; Yu and Hatzivassiloglou, 2003)). The hypothesis is that we can eliminate some of the ambiguities (and consequent loss of subjectivity) observed during the lexicon translation by accounting for the context of the ambiguous words, which is possible in a corpus-based approach. Additionally, we also hope to improve the recall of the classifier, by addressing those cases not covered by the lexicon-based approach. In the experiments reported in this section, we use a parallel corpus consisting of 107 documents from the SemCor corpus (Miller et al., 1993) and their manual translations into Romanian.3 The corpus consists of roughly 11,000 sentences, with approximately 250,000 tokens on each side. It is a balanced corpus covering a number of topics in sports, politics, fashion, education, and others. 3 The translation was carried out by a Romanian native speaker, student in a department of “Foreign Languages and Translations” in Romania. 980 Annotation Study. We start by performing an agreement study meant to determine the extent to which subjectivity is preserved by the cross-lingual projections. In the study, three annotators – one native Engl"
P07-1123,P04-1035,0,0.124897,"7 Association for Computational Linguistics While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons. First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage approach is often beneficial, in which subjective instances are distinguished from objective ones, and then the subjective instances are further classified according to polarity (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). In fact, the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification. This is reported in studies of manual annotation of phrases (Takamura et al., 2006), recognizing contextual polarity of expressions (Wilson et al., 2005), and sentiment tagging of words and word senses (Andreevskaia and Bergler, 2006; Esuli and Sebastiani, 2006). Second, an NLP application may see"
P07-1123,W02-1011,0,0.024939,"Missing"
P07-1123,W03-1014,1,0.425604,"s for a new one. This motivates our exploration and use of cross-lingual lexicon translations and annotation projections. Most if not all work on subjectivity analysis has been carried out in a monolingual framework. We 977 are not aware of multi-lingual work in subjectivity analysis such as that proposed here, in which subjectivity analysis resources developed for one language are used to support developing resources in another. 3 A Lexicon-Based Approach Many subjectivity and sentiment analysis tools rely on manually or semi-automatically constructed lexicons (Yu and Hatzivassiloglou, 2003; Riloff and Wiebe, 2003; Kim and Hovy, 2006). Given the success of such techniques, the first approach we take to generating a target-language subjectivity classifier is to create a subjectivity lexicon by translating an existing source language lexicon, and then build a classifier that relies on the resulting lexicon. Below, we describe the translation process and discuss the results of an annotation study to assess the quality of the translated lexicon. We then describe and evaluate a lexicon-based target-language classifier. 3.1 Translating a Subjectivity Lexicon The subjectivity lexicon we use is from OpinionFin"
P07-1123,E06-1026,0,0.0307306,"wo-stage approach is often beneficial, in which subjective instances are distinguished from objective ones, and then the subjective instances are further classified according to polarity (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006). In fact, the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification. This is reported in studies of manual annotation of phrases (Takamura et al., 2006), recognizing contextual polarity of expressions (Wilson et al., 2005), and sentiment tagging of words and word senses (Andreevskaia and Bergler, 2006; Esuli and Sebastiani, 2006). Second, an NLP application may seek a wide range of types of subjectivity attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments. For instance, the opinion tracking system Lydia (Lloyd et al., 2005) gives separate ratings for subjectivity and sentiment. These can be detected with subjectivity analysis but not by a method focused only on se"
P07-1123,P02-1053,0,0.0118121,"roviding motivations, we present two approaches to developing sentence-level subjectivity classifiers for a new target language. The first uses a subjectivity lexicon translated from an English one. The second uses an English subjectivity classifier and a parallel corpus to create target-language training data for developing a statistical classifier. 2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news (Lloyd et al., 2005; Balog et al., 2006), review classification (Turney, 2002; Pang et al., 2002), mining opinions from product reviews (Hu and Liu, 2004), automatic expressive text-to-speech synthesis (Alm et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), and question answering (Yu and Hatzivassiloglou, 2003). 976 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976–983, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotio"
P07-1123,P06-1134,1,0.726219,"m an English one. The second uses an English subjectivity classifier and a parallel corpus to create target-language training data for developing a statistical classifier. 2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news (Lloyd et al., 2005; Balog et al., 2006), review classification (Turney, 2002; Pang et al., 2002), mining opinions from product reviews (Hu and Liu, 2004), automatic expressive text-to-speech synthesis (Alm et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), and question answering (Yu and Hatzivassiloglou, 2003). 976 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976–983, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons. First, even when sentiment is the desired focus, researchers in sentiment analysis have"
P07-1123,H05-1044,1,0.132207,"Missing"
P07-1123,W03-1017,0,0.862182,"llel corpus to create target-language training data for developing a statistical classifier. 2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news (Lloyd et al., 2005; Balog et al., 2006), review classification (Turney, 2002; Pang et al., 2002), mining opinions from product reviews (Hu and Liu, 2004), automatic expressive text-to-speech synthesis (Alm et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), and question answering (Yu and Hatzivassiloglou, 2003). 976 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976–983, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons. First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage approach is often beneficial, in which subjective instances a"
P08-1106,W03-1028,0,0.0895043,"(CI normalized), which is the same as CI shortterm, except that it is normalized by the frequency of the phrase. Through this normalization, we hope to enhance the effect of the semantic relatedness of the phrase to subsequent sentences. • CI maximum score (CI maxscore), which measures the maximum centrality score the phrase achieves across the entire book. This can be thought of as a measure of the importance of the phrase in a smaller coherent segment of the document. 3.3 Syntactic Features Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). The construction integration model described before is already making use of syntactic patterns to some extent, through the use of a shallow parser to identify noun phrases. However, that approach does not cover patterns other than noun phrases. To address this limitation, we introduce a new feature that captures the part-of-speech of the words composing a candidate phrase. There are multiple ways to represent such a feature. The simplest is to create a string feature consisting of the concatenation of the part-of-speech tags. However, this representation imposes limitations on the machine l"
P08-1106,W99-0621,0,0.0174844,"word extraction, namely the sole dependence on term frequency. Even if a phrase appears only once, the construction integration process ensures the presence of the phrase in the short term memory as long as it is relevant to the current topic, thus being a good indicator of the phrase importance. The construction integration model is not directly applicable to keyword extraction due to a number of practical difficulties. The first implementation problem was the lack of a propositional parser. We solve this problem by using a shallow parser to extract noun phrase chunks from the original text (Munoz et al., 1999). Second, since spreading activation is a process difficult to control, with several parameters that require fine tuning, we use instead a different graph centrality measure, namely PageRank (Brin and Page, 1998). Finally, to represent the relations inside the long term semantic memory, we use a variant of latent semantic analysis (LSA) (Landauer et al., 1998) as implemented in the InfoMap package,2 trained on a corpus consisting of the British National Corpus, the English Wikipedia, and the books in our collection. To alleviate the data sparsity problem, we also use the pointwise mutual infor"
P08-1106,P06-2084,0,0.0221021,"calculated from the marginal probabilities (the sum of the values of a column or a row) converted into proportions by dividing them with the total number of observed events (N ): N = O1,1 + O1,2 + O2,1 + O2,2 Then the expected count for seeing the phrase in the document is: O1,1 + O1,2 O1,1 + O2,1 × ×N E1,1 = N N To measure the phraseness of a candidate phrase we use a technique based on the χ2 independence test. We measure the independence of the events of seeing the components of the phrase in the text. This method was found to be one of the best performing models in collocation discovery (Pecina and Schlesinger, 2006). For n-grams where N &gt; 2 we apply the χ2 independence test by splitting the phrase in two (e.g. for a 4-gram, we measure the independence of the composing bigrams). 3.2 Discourse Comprehension Features Very few existing keyword extraction methods look beyond word frequency. Except for (Turney and Littman, 2003), who uses pointwise mutual information to improve the coherence of the keyword set, we are not aware of any other work that attempts to use the semantics of the text to extract keywords. The fact that most systems rely heavily on term frequency properties poses serious difficulties, si"
P09-2078,D08-1027,0,0.0303132,"Missing"
P11-1076,W07-1427,0,0.0853929,"Missing"
P11-1076,W02-1001,0,0.0158251,"to produce these features. The remaining 32 features are lexicosyntactic features3 defined only for Nx3 and are described in more detail in Table 2. We use φ(xi , xs ) to denote the feature vector associated with a pair of nodes hxi , xs i, where xi is a node from the instructor answer Ai and xs is a node from the student answer As . A matching score can then be computed for any pair hxi , xs i ∈ Ai × As through a linear scoring function f (xi , xs ) = wT φ(xi , xs ). In order to learn the parameter vector w, we use the averaged version of the perceptron algorithm (Freund and Schapire, 1999; Collins, 2002). As training data, we randomly select a subset of the student answers in such a way that our set was roughly balanced between good scores, mediocre scores, and poor scores. We then manually annotate each node pair hxi , xs i as matching, i.e. A(xi , xs ) = +1, or not matching, i.e. A(xi , xs ) = −1. Overall, 32 student answers in response to 21 questions with a total of 7303 node pairs (656 matches, 6647 nonmatches) are manually annotated. The pseudocode for the learning algorithm is shown in Table 1. After training the perceptron, these 32 student answers are removed from the dataset, not us"
P11-1076,de-marneffe-etal-2006-generating,0,0.00543042,"Missing"
P11-1076,H05-1049,0,0.171233,"pendency types, word count, and other features to attempt to learn how best to classify an answer/facet pair. Closely related to the task of short answer grading is the task of textual entailment (Dagan et al., 2005), which targets the identification of a directional inferential relation between texts. Given a pair of two texts as input, typically referred to as text and hypothesis, a textual entailment system automatically finds if the hypothesis is entailed by the text. In particular, the entailment-related works that are most similar to our own are the graph matching techniques proposed by Haghighi et al. (2005) and Rus et al. (2007). Both input texts are converted into a graph by using the dependency relations obtained from a parser. Next, a matching score is calculated, by combining separate vertex- and edge-matching scores. The vertex matching functions use wordlevel lexical and semantic features to determine the quality of the match while the the edge matching functions take into account the types of relations and the difference in lengths between the aligned paths. Following the same line of work in the textual entailment world are (Raina et al., 2005), (MacCartney et al., 2006), (de Marneffe et"
P11-1076,N04-1024,0,0.0188438,"ct, and so, we must turn to computerassisted assessment (CAA). While some forms of CAA do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers made up of free text that may require 752 Rada Mihalcea Dept. of Computer Science University of North Texas Denton, TX rada@cs.unt.edu textual analysis. Research to date has concentrated on two subtasks of CAA: grading essay responses, which includes checking the style, grammaticality, and coherence of the essay (Higgins et al., 2004), and the assessment of short student answers (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Mohler and Mihalcea, 2009), which is the focus of this work. An automatic short answer grading system is one that automatically assigns a grade to an answer provided by a student, usually by comparing it to one or more correct answers. Note that this is different from the related tasks of paraphrase detection and textual entailment, since a common requirement in student answer grading is to provide a grade on a certain scale rather than make a simple yes/no decision. In this paper, we explore"
P11-1076,O97-1002,0,0.165198,"ill have very similar structures (which indicates a good alignment). However, the sentences will have opposite meanings. Further information is necessary to arrive at an appropriate score. In order to address this, we combine the graph alignment scores, which encode syntactic knowledge, with the scores obtained from semantic similarity measures. Following Mihalcea et al. (2006) and Mohler and Mihalcea (2009), we use eight knowledgebased measures of semantic similarity: shortest path [PATH], Leacock & Chodorow (1998) [LCH], Lesk (1986), Wu & Palmer(1994) [WUP], Resnik (1995) [RES], Lin (1998), Jiang & Conrath (1997) [JCN], Hirst & St. Onge (1998) [HSO], and two corpusbased measures: Latent Semantic Analysis [LSA] (Landauer and Dumais, 1997) and Explicit Seman756 tic Analysis [ESA] (Gabrilovich and Markovitch, 2007). Briefly, for the knowledge-based measures, we use the maximum semantic similarity – for each open-class word – that can be obtained by pairing it up with individual open-class words in the second input text. We base our implementation on the WordNet::Similarity package provided by Pedersen et al. (2004). For the corpus-based measures, we create a vector for each answer by summing the vectors"
P11-1076,N06-1006,0,0.0548063,"Missing"
P11-1076,E09-1065,1,0.836391,"erstanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers made up of free text that may require 752 Rada Mihalcea Dept. of Computer Science University of North Texas Denton, TX rada@cs.unt.edu textual analysis. Research to date has concentrated on two subtasks of CAA: grading essay responses, which includes checking the style, grammaticality, and coherence of the essay (Higgins et al., 2004), and the assessment of short student answers (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Mohler and Mihalcea, 2009), which is the focus of this work. An automatic short answer grading system is one that automatically assigns a grade to an answer provided by a student, usually by comparing it to one or more correct answers. Note that this is different from the related tasks of paraphrase detection and textual entailment, since a common requirement in student answer grading is to provide a grade on a certain scale rather than make a simple yes/no decision. In this paper, we explore the possibility of improving upon existing bag-of-words (BOW) approaches to short answer grading by utilizing machine learning t"
P11-1076,P94-1019,0,0.0224592,"Missing"
P11-1076,W05-0202,0,\N,Missing
P11-4018,W10-1505,0,0.0281464,"Missing"
P11-4018,S07-1029,0,0.0426442,"Missing"
P11-4018,U07-1008,0,0.0478393,"Missing"
P11-4018,S07-1044,0,0.0426832,"Missing"
P11-4018,P08-2036,0,0.0654292,"dexing and retrieval of large N-gram datasets, such as the Web1T 5-gram corpus. Our tool indexes the entire Web1T dataset with an index size of only 100 MB and performs a retrieval of any N-gram with a single disk access. With an increased index size of 420 MB and duplicate data, it also allows users to issue wild card queries provided that the wild cards in the query are contiguous. Furthermore, we also implement some of the smoothing algorithms that are designed specifically for large datasets and are shown to yield better language models than the traditional ones on the Web1T 5gram corpus (Yuret, 2008). We demonstrate the effectiveness of our tool and the smoothing algorithms on the English Lexical Substitution task by a simple implementation that gives considerable improvement over a basic language model. 1 Introduction The goal of statistical language modeling is to capture the properties of a language through a probability distribution so that the probabilities of word sequences can be estimated. Since the probability distribution is built from a corpus of the language by computing the frequencies of the N-grams found in the corpus, the data sparsity is always an issue with the language"
P11-4018,S07-1009,0,\N,Missing
P12-2051,W02-1006,0,0.0354087,"Missing"
P12-2051,P96-1006,0,0.0446905,"Missing"
P12-2051,N03-1033,0,0.00583767,"pets returned by a search on Google Books for each of the three epochs we consider. 2 For each open class word we create ranked lists of words, where the ranking score is an adjusted tfidf score – the epochs correspond to documents. To choose words frequent only in one epoch, we choose the top words in the list, for words frequent in all epochs we choose the bottom words in this list. 3 A minimum of 30 total examples was required for a word to be considered in the dataset. All the extracted snippets are then processed: the text is tokenized and part-of-speech tagged using the Stanford tagger (Toutanova et al., 2003), and contexts that do not include the target word with the specified part-of-speech are removed. The position of the target word is also identified and recorded as an offset along with the example. For illustration, we show below an example drawn from each epoch for two different words, dinner: 1800: On reaching Mr. Crane’s house, dinner was set before us ; but as is usual here in many places on the Sabbath, it was both dinner and tea combined into a single meal. 1900: The average dinner of today consists of relishes; of soup, either a consomme (clear soup) or a thick soup. 2000: Preparing di"
P12-4004,banea-etal-2008-bootstrapping,1,0.892304,"Missing"
P13-1096,H05-1073,0,0.0105201,"on we provide a brief overview of related work in text-based sentiment analysis, as well as audio-visual emotion analysis. 2.1 Text-based Subjectivity and Sentiment Analysis The techniques developed so far for subjectivity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more l"
P13-1096,N12-1073,0,0.0127022,"processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two pol"
P13-1096,E06-2031,0,0.0557996,"Missing"
P13-1096,P07-1056,0,0.0412454,"1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being sai"
P13-1096,P08-1041,0,0.0103924,"vity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opi"
P13-1096,P11-2099,0,0.0596938,"ext-based Subjectivity and Sentiment Analysis The techniques developed so far for subjectivity and sentiment analysis have focused primarily on the processing of text, and consist of either rulebased classifiers that make use of opinion lexicons, or data-driven methods that assume the availability of a large dataset annotated for polarity. These tools and resources have been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corp"
P13-1096,P12-1043,0,0.0135778,"u and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly"
P13-1096,P11-1015,0,0.157096,"ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years. Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer’s emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language. Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In this paper, we explore the addition of speech and visual modalities to text ana"
P13-1096,P12-1060,0,0.0247281,"nguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling the content at uttera"
P13-1096,P07-1123,1,0.29988,"11), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by"
P13-1096,esuli-sebastiani-2006-sentiwordnet,0,0.0378227,"ular web platforms such as YouTube, Amazon, Facebook, and ExpoTV have reported a significant increase in the number of consumer reviews in video format over the past five years. Compared to traditional text reviews, video reviews provide a more natural experience as they allow the viewer to better sense the reviewer’s emotions, beliefs, and intentions through richer channels such as intonations, facial expressions, and body language. Much of the work to date on opinion analysis has focused on textual data, and a number of resources have been created including lexicons (Wiebe and Riloff, 2005; Esuli and Sebastiani, 2006) or large annotated datasets (Maas et al., 2011). Given the accelerated growth of other media on the Web and elsewhere, which includes massive collections of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa), audio clips (e.g., podcasts), the ability to address the identification of opinions in the presence of diverse modalities is becoming increasingly important. This has motivated researchers to start exploring multimodal clues for the detection of sentiment and emotions in video content (Morency et al., 2011; Wagner et al., 2011). In this paper, we explore the addi"
P13-1096,D12-1034,0,0.0150462,"Missing"
P13-1096,P97-1023,0,0.057328,"ve been already used in a large number of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitz"
P13-1096,P04-1035,0,0.0155733,"t analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical state"
P13-1096,D08-1049,0,0.0192736,"esents our multimodal sen973 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973–982, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics timent analysis approach, including details about our linguistic, acoustic, and visual features. Our experiments and results on multimodal sentiment classification are presented in Section 5, with a detailed discussion and analysis in Section 6. 2 such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. Related Work In this section we provide a brief overview of related work in text-based"
P13-1096,W06-0607,0,0.0283006,"t annotations. Section 4 presents our multimodal sen973 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973–982, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics timent analysis approach, including details about our linguistic, acoustic, and visual features. Our experiments and results on multimodal sentiment classification are presented in Section 5, with a detailed discussion and analysis in Section 6. 2 such as speech or facial expressions. The only exceptions that we are aware of are the findings reported in (Somasundaran et al., 2006; Raaijmakers et al., 2008; Mairesse et al., 2012; Metze et al., 2009), where speech and text have been analyzed jointly for the purpose of subjectivity or sentiment identification, without, however, addressing other modalities such as visual cues; and the work reported in (Morency et al., 2011; Perez-Rosas et al., 2013), where multimodal cues have been used for the analysis of sentiment in product reviews, but where the analysis was done at the much coarser level of full videos rather than individual utterances as we do in our work. Related Work In this section we provide a brief overview of"
P13-1096,S07-1013,1,0.787519,"tically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognit"
P13-1096,J11-2001,0,0.00592148,"pressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mih"
P13-1096,P02-1053,0,0.0366184,"er of applications, including expressive textto-speech synthesis (Alm et al., 2005), tracking sentiment timelines in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 200"
P13-1096,P09-1027,0,0.0107132,"and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed, not much has been done in terms of extending the applicability of sentiment analysis to other modalities, 2.2 Audio-Visual Emotion Analysis. Also related to our work is the research done on emotion analysis. Emotion analysis of speech signals aims to identify the emotional or physical states of a person by analyzing his or her voice (Ververidis and Kotropoulos, 2006). Proposed methods for emotion recognition from speech focus both on what is being said and how is being said, and rely mainly on the analysis of the speech signal by sampling t"
P13-1096,D12-1122,0,0.0128718,"in on-line forums and news (Balog et al., 2006), analysis of political debates (Carvalho et al., 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012). One of the first lexicons used in sentiment analysis is the General Inquirer (Stone, 1968). Since then, many methods have been developed to automatically identify opinion words and their polarity (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Hu and Liu, 2004; Taboada et al., 2011), as well as n-gram and more linguistically complex phrases (Yang and Cardie, 2012). For data-driven methods, one of the most widely used datasets is the MPQA corpus (Wiebe et al., 2005), which is a collection of news articles manually annotated for opinions. Other datasets are also available, including two polarity datasets consisting of movie reviews (Pang and Lee, 2004; Maas et al., 2011), and a collection of newspaper headlines annotated for polarity (Strapparava and Mihalcea, 2007). While difficult problems such as cross-domain (Blitzer et al., 2007; Li et al., 2012) or crosslanguage (Mihalcea et al., 2007; Wan, 2009; Meng et al., 2012) portability have been addressed,"
P14-2072,W12-0403,0,0.0426744,"Missing"
P14-2072,P12-2034,0,0.371006,"eception markers. An important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform linguistic profiling of true tellers and liars (Zhou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and Poesio, 2013) identified deception in Italian by analyzing court cases. The authors explored several strategies for identifying deceptive clues, such as utterance length, LIWC features, lemmas and part of speech"
P14-2072,P09-2078,1,0.868116,"n we use information drawn from one culture to build a deception classifier for another culture? Finally, what are the psycholinguistic classes most strongly associated with deception/truth, and are there commonalities or differences among languages? In all our experiments, we formulate the deception detection task in a machine learning framework, where we use an SVM classifier to discriminate between deceptive and truthful statements.1 4.1 Datasets We collect three datasets for three different cultures: United States (English-US), India (EnglishIndia), and Mexico (Spanish-Mexico). Following (Mihalcea and Strapparava, 2009), we collect short deceptive and truthful essays for three topics: opinions on Abortion, opinions on Death Penalty, and feelings about a Best Friend. For English-US and English-India, we use Amazon Mechanical Turk with a location restriction, so that all the contributors are from the country of interest (US and India). We collect 100 deceptive and 100 truthful statements for each of the three topics. To avoid spam, each contribution is manually verified by one of the authors of this paper.For SpanishMexico, while we initially attempted to collect data also using Mechanical Turk, we were not ab"
P14-2072,P11-1032,0,0.0556471,"Missing"
P14-2072,C12-2131,0,0.0241707,"n important resource that has been used to represent semantic information for the deception task is the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker and Francis, 1999). LIWC provides words grouped into semantic categories relevant to psychological processes, which have been used successfully to perform linguistic profiling of true tellers and liars (Zhou et al., 2003; Newman et al., 2003; Rubin, 2010). In addition to this, features derived from syntactic Context Free Grammar parse trees, and part of speech have also been found to aid the deceit detection (Feng et al., 2012; Xu and Zhao, 2012). 440 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440–445, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics While most of the studies have focused on English, there is a growing interest in studying deception for other languages. For instance, (Fornaciari and Poesio, 2013) identified deception in Italian by analyzing court cases. The authors explored several strategies for identifying deceptive clues, such as utterance length, LIWC features, lemmas and part of speech patterns. (Almela e"
P17-1131,P03-1054,0,0.0535306,"ized and unlexicalized production rules from the Context Free Grammar parse trees9 of each transcript. The final linguistic features set consists of 13,648 features. Raw acoustic features: This feature set includes a large number of speech features extracted with the OpenEar toolkit (Eyben et al., 2009). We use a predefined feature set, EmoLarge, which consists of a set of 6,552 features used for emotion recognition tasks. The features are derived from 25 lowlevel speech descriptors including intensity, loudness, 12 Mel frequency coefficients, pitch (F0), 9 Extracted with the Stanford parser (Klein and Manning, 2003). Table 4: Overall prediction results and F-scores for high empathy (HE) and low empathy (LE) using linguistic and acoustic feature sets. probability of voicing, F0 envelope, zero-crossing rate, and 8 line spectral frequencies. 7.2 Classification Results Classification results for each feature set are shown in Table 4. For the linguistic and acoustic modalities, almost all the feature sets provide classification accuracies above the baseline, with good F-scores for both high and low empathy. The only exception are the nonverbal accommodation features, which have an accuracy comparable to the b"
P17-1131,W16-0305,1,0.882998,"Missing"
P19-1050,L18-1252,0,0.1728,"et. To accomplish this, we crawl through the subtitles of all the episodes and heuristically extract the respective timestamps. In particular, we enforce the following constraints: The remainder of the paper is organized as follows: Section 2 illustrates the EmotionLines dataset; we then present MELD in Section 3; strong baselines and experiments are elaborated in Section 4; future directions and applications of MELD are covered in Section 5 and 6, respectively; finally, Section 7 concludes the paper. EmotionLines Dataset The MELD dataset has evolved from the EmotionLines dataset developed by Chen et al. (2018). EmotionLines contains dialogues from the popular sitcom Friends, where each dialogue contains utterances from multiple speakers. EmotionLines was created by crawling the dialogues from each episode and then grouping them based on the number of utterances in a dialogue into four groups of [5, 9], [10, 14], [15, 19], and [20, 24] utterances respectively. Finally, 250 dialogues were sampled randomly from each of these groups, resulting in the final dataset of 1,000 dialogues. 2.1 Annotation The utterances in each dialogue were annotated with the most appropriate emotion category. For this purpo"
P19-1050,N18-1193,1,0.736465,"ant research work has been carried out on multimodal emotion recognition using audio, visual, and text modalities (Zadeh et al., 2016a; Wollmer et al., 2013), significantly less work has been devoted to emotion recognition in conversations (ERC). One main reason for this is the lack of a large multimodal conversational dataset. According to Poria et al. (2019), ERC presents several challenges such as conversational context modeling, emotion shift of the interlocutors, and others, which make the task more difficult to address. Recent work proposes solutions based on multimodal memory networks (Hazarika et al., 2018). However, they are mostly limited to dyadic conversations, and thus not scalable to ERC with multiple interlocutors. This calls for a multi-party conversational data resource that can encourage research in this direction. In a conversation, the participants’ utterances generally depend on their conversational context. This is also true for their associated emotions. In other words, the context acts as a set of parameters that may influence a person to speak an utterance while expressing a certain emotion. Modeling this context can be done in different ways, e.g., by using recurrent neural net"
P19-1050,D14-1181,0,0.0131143,"ection with sparse estimators, such as SVMs, to get a dense representation of the overall audio segment. For the baselines, we do not use visual features, as videobased speaker identification and localization is an open problem. Bimodal features are obtained by concatenating audio and textual features. 4.2 Baseline Models To provide strong benchmarks for MELD, we perform experiments with multiple baselines. Hyperparameter details for each baseline can be found at http://github.com/senticnet/meld. text-CNN applies CNN to the input utterances without considering the context of the conversation (Kim, 2014). This model represents the simplest baseline which does not leverage context or multimodality in its approach. bcLSTM is a strong baseline proposed by Poria et al. (2017), which represents context using a bi-directional RNN. It follows a two-step hierarchical process that models uni-modal context first and then bi-modal context features. For unimodal text, a CNN-LSTM model extracts contextual representations for each utterance taking the GloVe emDataset IEMOCAP SEMAINE MELD Type acted acted acted # dialogues train dev test 120 31 58 22 1039 114 280 # utterances train dev test 5810 1623 4386 1"
P19-1050,P13-1096,1,0.763598,"Missing"
P19-1050,P17-1081,1,0.829939,"s, and thus not scalable to ERC with multiple interlocutors. This calls for a multi-party conversational data resource that can encourage research in this direction. In a conversation, the participants’ utterances generally depend on their conversational context. This is also true for their associated emotions. In other words, the context acts as a set of parameters that may influence a person to speak an utterance while expressing a certain emotion. Modeling this context can be done in different ways, e.g., by using recurrent neural networks (RNNs) and memory networks (Hazarika et al., 2018; Poria et al., 2017; Serban et al., 2017). Figure 1 shows an example where the speakers change their emotions (emotion shifts) as the dialogue develops. The emotional dynamics here depend on both the previous utterances and their associated emotions. For example, the emotion shift in utterance eight (in the figure) is hard to determine unless cues are taken from the facial expressions and the conversational history of both speakers. Modeling such complex inter-speaker dependencies is one of the major challenges in conversational modeling. Conversation in its natural form is multimodal. In dialogues, we rely on o"
P19-1050,P16-1094,0,0.0406793,"persons are present is very challenging. This is the case for MELD too as it is a multi-party 1200 Distance between test utterance and 1000 2nd highest attention We believe there is room for further improvement using other more advanced fusion methods such as MARN (Zadeh et al., 2018). 6 MELD has multiple use-cases. It can be used to train emotion classifiers to be further used as emotional receptors in generative dialogue systems. These systems can be used to generate empathetic responses (Zhou et al., 2017). It can also be used for emotion and personality modeling of users in conversations (Li et al., 2016). By being multimodal, MELD can also be used to train multimodal dialogue systems. Although by itself it is not large enough to train an end-to-end dialogue system (Table 1), the procedures used to create MELD can be adopted to generate a largescale corpus from any multimodal source such as popular sitcoms. We define multimodal dialogue system as a platform where the system has access to the speaker’s voice and facial expressions which it exploits to generate responses. Multimodal dialogue systems can be very useful for real time personal assistants such as Siri, Google Assistant where the use"
P19-1050,P18-1208,1,0.932673,"ld guide models for better classification. We also provide evidence for these claims through our experiments. The development of conversational AI thus depends on the use of both contextual and multimodal information. The publicly available datasets for multimodal emotion recognition in conversations – IEMOCAP and SEMAINE – have facilitated a significant number of research projects, but also have limitations due to their relatively small number of total utterances and the lack of multi-party conversations. There are also other multimodal emotion and sentiment analysis datasets, such as MOSEI (Zadeh et al., 2018), MOSI (Zadeh et al., 2016b), and MOUD (P´erez-Rosas et al., 2013), but they contain individual narratives instead of dialogues. On the other hand, EmotionLines (Chen et al., 2018) is a dataset that contains dialogues from the popular TV-series Friends with more than two speakers. However, EmotionLines can only be used for textual analysis as it does not provide data from other modalities. In this work, we extend, improve, and further develop the EmotionLines dataset for the multimodal scenario. We propose the Multimodal EmotionLines Dataset (MELD), which includes not only textual dialogues, b"
P19-1050,D14-1162,0,0.080961,"Missing"
P19-1088,Q16-1033,0,0.142279,"study aspects such as language mirroring, empathy, and reflective listening. Tanana et al. (2015) addressed the identification of counselor statements that discuss client change talk using recursive neural networks to model sequences of counselor and client verbal exchanges. Lord et al. (2015) analyzed the language style synchrony between counselors and clients. Their approach relies on the psycholinguistic categories from the Linguistic Inquiry and Word Count (LIWC) lexicon (Tausczik and Pennebaker, 2010) to measure the degree in which counselors match their clients’ language. More recently, Althoff et al. (2016) explored language style and symmetry in counseling interactions by analyzing a large sample of textmessage-based counseling. Their main findings suggest that the counselors who are more successful act with more control in the conversations and show lower levels of verbal coordination (mirroring) than their less successful counterparts. Following this line of work, this paper presents the development of a counseling dataset that can be used to implement data-driven methods for the automatic evaluation of counseling quality. Specifically, we conduct several linguistically inspired analyses on h"
P19-1088,W15-1209,0,0.0282035,"port, and empathy. Methods that combine acoustic and linguistic datastreams have also been proposed to evaluate the quality of counseling interactions. Xiao et al. (2014) presented a study on the automatic evaluation of counselor empathy based on analyzing correlations between prosody patterns and empathy showed by the counselor during the counseling interactions. Second, aiming to improve the understanding of counseling interactions, researchers have started to explore Natural Language Processing (NLP) approaches to study aspects such as language mirroring, empathy, and reflective listening. Tanana et al. (2015) addressed the identification of counselor statements that discuss client change talk using recursive neural networks to model sequences of counselor and client verbal exchanges. Lord et al. (2015) analyzed the language style synchrony between counselors and clients. Their approach relies on the psycholinguistic categories from the Linguistic Inquiry and Word Count (LIWC) lexicon (Tausczik and Pennebaker, 2010) to measure the degree in which counselors match their clients’ language. More recently, Althoff et al. (2016) explored language style and symmetry in counseling interactions by analyzin"
P19-1088,P14-5010,0,0.00275707,"re positive and friendly at the beginning of the conversation and ending the conversation with positive remarks. Sentiment Trends The sentiment expressed by counselors during the conversation can provide important insights into whether counselors focus on positive or negative aspects of client communication. We thus analyze the sentiment expressed across the conversation in relation to conversation quality. Given the effort required to manually annotate the sentiment in each conversation turn, we opt for using an automatic off-the-shelf sentiment classifier from the Stanford Core NLP package (Manning et al., 2014). Using this tool, we obtain the sentiment score for each conversation turn. The score ranges from very negative to very positive −−, −, 0, + and ++, representing five sentiment categories in the order of increasing positiveness. Since −− and ++ rarely occur in our dataset we treat both −− and − as negative, 0 as neutral, and + and ++ as positive. Figure 5 shows the distribution of the three sentiment categories in low- and high-quality counseling respectively, where we observe that neutral sentiment occurs most frequently while the 5 Linguistic Alignment The degree of language coordination th"
P19-1088,H05-2018,0,0.253625,"Missing"
P19-1088,strapparava-valitutti-2004-wordnet,0,0.626376,"Missing"
P19-1245,D16-1091,0,0.0215157,"e a relatively narrow set of behaviors compared to the huge range of things that people do on a day-to-day basis. On the other hand, publicly available social media data provide us with information about an extremely rich and diverse set of human activities, but the data are rarely structured or categorized, and they mostly exist in the form of natural language. Recently, however, natural language processing research has provided several examples of methodologies for extracting and representing human activities from text (Fast et al., 2016; Wilson and Mihalcea, 2017) and even multimodal data (Agrawal et al., 2016). The activities we do are linked to our interests, personality, political preferences, and decisions we make about the future. In this paper, we explore the task of predicting human activities from user-generated content. We collect a dataset containing instances of social media users writing about a range of everyday activities. We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities. We train a neural network model to make predictions about which clusters contain activities th"
P19-1245,D14-1162,0,0.0820687,"h the user. Then, a prediction is made for each of the kact clusters, first applying softmax in order to obtain a probability distribution. We refer to the dimension of the output as dimo . Figure 1: Predictive model architecture. represented as a sequence of tokens. For each user, we populate the profile input using the plain text user description associated with their account, which often contains terms which express selfidentity such as “republican” or “athiest.” We represent the tokens in both the user’s history and profile with the pretrained 100dimensional GloVe-Twitter word embeddings (Pennington et al., 2014), and preprocess all text with the script included with these embeddings.7 Finally, our model allows the inclusion of any additional attributes that might be known or inferred in order to aid the prediction task, which can be passed to the model as a dima dimensional real-valued vector. For instance, we can use personal values as a set of attributes, as described in Section 3.3. We train a deep neural model, summarized in Figure 1, to take a user’s history, profile, and attributes, and output a probability distribution over the set of kact clusters of human activities, indicating the likelihoo"
P19-1245,P18-1043,0,0.0622711,"Missing"
P19-1245,D17-1070,0,0.103548,"the match appears. By doing this clustering, our models will be able to make a prediction about the likelihood that a user has mentioned activities from each cluster, rather than only making predictions about a single point in the semantic space of human activities. In order to cluster our activity phrase instances, we need to define a notion of distance between any pair of instances. For this, we turn to prior work on models to determine semantic similarity between human activity phrases (Zhang et al., 2018) in which the authors utilized transfer learning in order to fine-tune the Infersent (Conneau et al., 2017) sentence similarity model to specifically capture relationships between human activity phrases. We use the authors’ BiLSTM-max sentence encoder trained to capture the relatedness dimension of human activity phrases5 to obtain vector representations of each of our activity phrases. The measure of distance between vectors produced by this model was shown to be strongly correlated with human judgments of general activity relatedness (Spearman’s ρ = .722 between the model and human ratings, while inter-annotator agreement is .768). While the relationship between two activity phrases can be define"
P19-1245,I17-1067,1,0.920912,"eing done by the involved users, these methods capture a relatively narrow set of behaviors compared to the huge range of things that people do on a day-to-day basis. On the other hand, publicly available social media data provide us with information about an extremely rich and diverse set of human activities, but the data are rarely structured or categorized, and they mostly exist in the form of natural language. Recently, however, natural language processing research has provided several examples of methodologies for extracting and representing human activities from text (Fast et al., 2016; Wilson and Mihalcea, 2017) and even multimodal data (Agrawal et al., 2016). The activities we do are linked to our interests, personality, political preferences, and decisions we make about the future. In this paper, we explore the task of predicting human activities from user-generated content. We collect a dataset containing instances of social media users writing about a range of everyday activities. We then use a state-of-the-art sentence embedding framework tailored to recognize the semantics of human activities and perform an automatic clustering of these activities. We train a neural network model to make predic"
P19-1339,P16-1231,0,0.0306712,"Missing"
P19-1339,D14-1082,0,0.0336313,"butions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1,814 are found in ProQuest and their metadata is retrieved. 556 articles with an empty Author field are removed, resulting in 1,258 WSJ articles with"
P19-1339,N10-1093,0,0.0285274,"dinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in most training sets. In order to address gender bias in part-of-speech (POS) tagging and dependency parsing, we first require an"
P19-1339,P15-1073,1,0.823085,"al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in most training sets. In order to address gender bias in part-of-speech (POS) tagging and dependency parsing, we first require an adequate size data set labeled for a) syntax along with b) gender information of the authors. However, existing data sets fail to meet both criteria: data sets with gender information are either too small to train on, lack syntactic information, or are restricted to social media; sufficiently large syntactic dat"
P19-1339,P15-2079,1,0.885205,"(2015). 1 We exemplify in parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount"
P19-1339,P16-2096,1,0.880471,"n parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in m"
P19-1339,K15-1011,1,0.849754,"se grammatical features to signal the speakers’ membership in a demographic group, with a focus on gender (Vigliocco and Franck, 1999; Mondorf, 2002; Eckert and McConnell-Ginet, 2013). Mondorf (2002) shows systemic differences in the usage of various types of clauses and their positions for men and women, stating that women have a higher usage of adverbial (accordingly, consequently1 ), causal (since, because), conditional (if, when) and purpose (so, in order that) clauses, while men tend to use more concessive clauses (but, although, whereas). Similar results hold across various languages in Johannsen et al. (2015). 1 We exemplify in parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy a"
P19-1339,W15-4302,1,0.891018,"ges in Johannsen et al. (2015). 1 We exemplify in parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little unde"
P19-1339,P03-1054,0,0.0541134,"tional Linguistics Contributions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1,814 are found in ProQuest and their metadata is retrieved. 556 articles with an empty Author field a"
P19-1339,D17-1119,0,0.0122243,"ted demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in most training sets. In order to address gender bias in part-of-speech (POS) tagging and dependency parsing, we first require an adequate size data set labeled for a) syntax along with b) gender information of the authors. However, existing data sets fail to meet both criteria: data sets with gender information are either too small to train on, lack syntactic information, or are restricted to social media; sufficiently large syntactic data sets are not labeled with gender information and rely (at least in part) on news g"
P19-1339,J93-2004,0,0.0746908,"r gender. 3493 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3493–3498 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Contributions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 192"
P19-1339,W04-2407,0,0.159302,"Missing"
P19-1339,C04-1010,0,0.0778337,"butions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1,814 are found in ProQuest and their metadata is retrieved. 556 articles with an empty Author field are removed, resulting in"
P19-1339,W96-0213,0,0.827935,"tics, pages 3493–3498 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Contributions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1"
P19-1339,D13-1170,0,0.00622069,"important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015; Bolukbasi et al., 2016; Benton et al., 2017; Zhao et al., 2017; Lynn et al., 2017), there is still little understanding on the amount and sources of bias in most training sets. In order to address gender bias in part-of-speech (POS) tagging and dependency parsing, we first require an adequate size data set labeled for a) syntax along with b) gender informatio"
P19-1339,N03-1033,0,0.0766919,"- August 2, 2019. 2019 Association for Computational Linguistics Contributions. The main contributions of this paper are as follows: • We annotate a standard POS-tagging and dependency parsing data set with gender information. • We conduct experiments and show the role played by gender information in POS-tagging and syntactic parsing. • We analyze POS and syntactic differences related to author gender. 2 Annotating PTB for Gender The Penn Treebank (Marcus et al., 1993) is the de facto data set used to train many of the POS taggers (Brill, 1994; Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003) and syntactic parsers (Klein and Manning, 2003; Nivre and Scholz, 2004; Chen and Manning, 2014). It contains articles published in the WSJ in 1989, as well as a small sample of ATIS-3 material, totalling over one million tokens, and manually annotated with POS tags and syntactic parse trees. We supplement the WSJ articles with metadata from the ProQuest Historical Newspapers database, which indexes, among others, WSJ articles released between 1923 and 2000, and provides fields such as author names. Out of the original 2,499 WSJ articles, 1,814 are found in ProQuest and their metadata is retri"
P19-1339,W00-1308,0,0.425141,"Missing"
P19-1339,D13-1187,0,0.08357,"Missing"
P19-1339,D17-1323,0,0.0751563,"men tend to use more concessive clauses (but, although, whereas). Similar results hold across various languages in Johannsen et al. (2015). 1 We exemplify in parentheses conjunctions or conjunctive adverbs that introduce and link in a subordinating relationship the given type of subordinate clause. This correlation between grammatical features and gender has important ramifications for statistical models of syntax: if the training sample is unbalanced, these differences inadvertently introduce a strong gender bias into the training data. Such demographic imbalances are amplified by the model (Zhao et al., 2017), which in turn can be detrimental to members of the underrepresented demographic groups (Jørgensen et al., 2015; Hovy and Søgaard, 2015; Hovy and Spruit, 2016). Since several works use syntactic analysis to improve tasks ranging from data-driven dependency parsing (Gadde et al., 2010) to sentiment classification (Moilanen and Pulman, 2007; Socher et al., 2013), underlying model biases end up affecting the performance of a wide range of applications. While data bias can be overcome by accounting for demographics, and can even improve classification performance (Volkova et al., 2013; Hovy, 2015"
P19-1455,P16-3016,0,0.0349892,"approaches for detecting sarcasm in text have considered rule-based techniques (Veale and Hao, 2010), lexical and pragmatic features (Carvalho et al., 2009), stylistic features (Davidov et al., 2010), situational disparity (Riloff et al., 2013), incongruity (Joshi et al., 2015), or user-provided annotations such as hashtags (Liebrecht et al., 2013). Resources in this domain are collected using Twitter as a primary data source and are annotated using two main strategies: manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in online platforms (Rajadesingan et al., 2015), embeddings of expressed sentiment and speaker’s personality traits (Poria et al., 2016), learning of user-specific representations (Wallace et al., 2016; Kolchinski and Potts, 2018), user-community features (Wallace et al., 2015), as well as stylistic and discourse features (Hazarika et al., 2018). In our"
P19-1455,C18-1156,1,0.842654,"0; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in online platforms (Rajadesingan et al., 2015), embeddings of expressed sentiment and speaker’s personality traits (Poria et al., 2016), learning of user-specific representations (Wallace et al., 2016; Kolchinski and Potts, 2018), user-community features (Wallace et al., 2015), as well as stylistic and discourse features (Hazarika et al., 2018). In our dataset, we capitalize on the conversational format and provide context by including preceding utterances along with speaker identities. To the best of our knowledge, there is no prior work which deals Multimodal Sarcasm: Contextual information for sarcasm in text can be included from other modalities. These modalities help in providing additional cues in the form of both common or contrasting patterns. Prior work mainly considers multimodal learning for the readers’ ability to perceive sarcasm. Such research couples textual features with cognitive features such as the gazebehavior of"
P19-1455,W16-2111,0,0.148292,"alities, such as text, speech, and visual data streams. Sarcasm in Text: Traditional approaches for detecting sarcasm in text have considered rule-based techniques (Veale and Hao, 2010), lexical and pragmatic features (Carvalho et al., 2009), stylistic features (Davidov et al., 2010), situational disparity (Riloff et al., 2013), incongruity (Joshi et al., 2015), or user-provided annotations such as hashtags (Liebrecht et al., 2013). Resources in this domain are collected using Twitter as a primary data source and are annotated using two main strategies: manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in online platforms (Rajadesingan et al., 2015), embeddings of expressed sentiment and speaker’s personality traits (Poria et al., 2016), learning of user-specific representations (Wallace et al., 2016; Kolchinski and Potts, 2018), user-community features (Wall"
P19-1455,W10-2914,0,0.32777,"s in our dataset and baseline experiments. Related Work Automated sarcasm detection has gained increased interest in recent years. It is a widely studied linguistic device whose significance is seen in sentiment analysis and human-machine interaction research. Various research projects have approached this problem through different modalities, such as text, speech, and visual data streams. Sarcasm in Text: Traditional approaches for detecting sarcasm in text have considered rule-based techniques (Veale and Hao, 2010), lexical and pragmatic features (Carvalho et al., 2009), stylistic features (Davidov et al., 2010), situational disparity (Riloff et al., 2013), incongruity (Joshi et al., 2015), or user-provided annotations such as hashtags (Liebrecht et al., 2013). Resources in this domain are collected using Twitter as a primary data source and are annotated using two main strategies: manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual"
P19-1455,P15-2124,0,0.328915,"on has gained increased interest in recent years. It is a widely studied linguistic device whose significance is seen in sentiment analysis and human-machine interaction research. Various research projects have approached this problem through different modalities, such as text, speech, and visual data streams. Sarcasm in Text: Traditional approaches for detecting sarcasm in text have considered rule-based techniques (Veale and Hao, 2010), lexical and pragmatic features (Carvalho et al., 2009), stylistic features (Davidov et al., 2010), situational disparity (Riloff et al., 2013), incongruity (Joshi et al., 2015), or user-provided annotations such as hashtags (Liebrecht et al., 2013). Resources in this domain are collected using Twitter as a primary data source and are annotated using two main strategies: manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in onl"
P19-1455,K16-1015,0,0.162118,"alities, such as text, speech, and visual data streams. Sarcasm in Text: Traditional approaches for detecting sarcasm in text have considered rule-based techniques (Veale and Hao, 2010), lexical and pragmatic features (Carvalho et al., 2009), stylistic features (Davidov et al., 2010), situational disparity (Riloff et al., 2013), incongruity (Joshi et al., 2015), or user-provided annotations such as hashtags (Liebrecht et al., 2013). Resources in this domain are collected using Twitter as a primary data source and are annotated using two main strategies: manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in online platforms (Rajadesingan et al., 2015), embeddings of expressed sentiment and speaker’s personality traits (Poria et al., 2016), learning of user-specific representations (Wallace et al., 2016; Kolchinski and Potts, 2018), user-community features (Wall"
P19-1455,D18-1140,0,0.0115584,"manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in online platforms (Rajadesingan et al., 2015), embeddings of expressed sentiment and speaker’s personality traits (Poria et al., 2016), learning of user-specific representations (Wallace et al., 2016; Kolchinski and Potts, 2018), user-community features (Wallace et al., 2015), as well as stylistic and discourse features (Hazarika et al., 2018). In our dataset, we capitalize on the conversational format and provide context by including preceding utterances along with speaker identities. To the best of our knowledge, there is no prior work which deals Multimodal Sarcasm: Contextual information for sarcasm in text can be included from other modalities. These modalities help in providing additional cues in the form of both common or contrasting patterns. Prior work mainly considers multimodal learning for the readers’ ab"
P19-1455,W13-1605,0,0.295518,"Missing"
P19-1455,D13-1066,0,0.460898,"Missing"
P19-1455,P16-1104,0,0.701841,"of Singapore, Singapore ι Information Systems Technology and Design, SUTD, Singapore {sacastro,vrncapr,mihalcea}@umich.edu, {hazarika,rogerz}@comp.nus.edu.sg, sporia@sutd.edu.sg Abstract can reveal the speaker’s intentions. For instance, sarcasm can be expressed using a combination of verbal and non-verbal cues, such as a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. Moreover, sarcasm detection involves finding linguistic or contextual incongruity, which in turn requires further information, either from multiple modalities (Schifanella et al., 2016; Mishra et al., 2016a) or from the context history in a dialogue. Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. As a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD1 ), compiled"
P19-1455,D14-1162,0,0.0819831,"xtraction Ross : diﬀerent. It’s Full spectrum 512 128 0 0:00 0:10 0:20 0:30 0:40 0:50 1:00 1:10 lowed to extract each of them is described below: Text Features: We represent the textual utterances in the dataset using BERT (Devlin et al., 2018), which provides a sentence representation ut ∈ Rdt for every utterance u. In particular, we average the last four transformer layers of the first token ([CLS]) in the utterance – using the BERTBase model – to get a unique utterance representation of size dt = 768. We also considered averaging Common Crawl pre-trained 300 dimensional GloVe word vectors (Pennington et al., 2014) for each token; however, it resulted in lower performance as compared to BERT-based features. Speech Features: To leverage information from the audio modality, we obtain low-level features from the audio data stream for each utterance in the dataset. Through these features, we intend to provide information related to pitch, intonation, and other tonal-specific details of the speaker (Tepperman et al., 2006). We utilize the popular speechprocessing library Librosa (McFee et al., 2018) and perform the processing pipeline described next. First, we load the audio sample for an utterance as a time"
P19-1455,C16-1151,1,0.902299,"d using Twitter as a primary data source and are annotated using two main strategies: manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in online platforms (Rajadesingan et al., 2015), embeddings of expressed sentiment and speaker’s personality traits (Poria et al., 2016), learning of user-specific representations (Wallace et al., 2016; Kolchinski and Potts, 2018), user-community features (Wallace et al., 2015), as well as stylistic and discourse features (Hazarika et al., 2018). In our dataset, we capitalize on the conversational format and provide context by including preceding utterances along with speaker identities. To the best of our knowledge, there is no prior work which deals Multimodal Sarcasm: Contextual information for sarcasm in text can be included from other modalities. These modalities help in providing additional cues in the form of both commo"
P19-1455,P15-1100,0,0.0358586,"Missing"
P19-1455,P14-2084,0,0.121953,"ures (Carvalho et al., 2009), stylistic features (Davidov et al., 2010), situational disparity (Riloff et al., 2013), incongruity (Joshi et al., 2015), or user-provided annotations such as hashtags (Liebrecht et al., 2013). Resources in this domain are collected using Twitter as a primary data source and are annotated using two main strategies: manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in online platforms (Rajadesingan et al., 2015), embeddings of expressed sentiment and speaker’s personality traits (Poria et al., 2016), learning of user-specific representations (Wallace et al., 2016; Kolchinski and Potts, 2018), user-community features (Wallace et al., 2015), as well as stylistic and discourse features (Hazarika et al., 2018). In our dataset, we capitalize on the conversational format and provide context by including preceding utterances along with spe"
P19-1455,K16-1017,0,0.177658,"two main strategies: manual annotation (Riloff et al., 2013; Joshi et al., 2016a) and distant supervision through hashtags (Davidov et al., 2010; Abercrombie and Hovy, 2016). Other research leverages context to acquire shared knowledge between the speaker and the audience (Wallace et al., 2014; Bamman and Smith, 2015). A variety of contextual features have been explored, including speaker’s background and behavior in online platforms (Rajadesingan et al., 2015), embeddings of expressed sentiment and speaker’s personality traits (Poria et al., 2016), learning of user-specific representations (Wallace et al., 2016; Kolchinski and Potts, 2018), user-community features (Wallace et al., 2015), as well as stylistic and discourse features (Hazarika et al., 2018). In our dataset, we capitalize on the conversational format and provide context by including preceding utterances along with speaker identities. To the best of our knowledge, there is no prior work which deals Multimodal Sarcasm: Contextual information for sarcasm in text can be included from other modalities. These modalities help in providing additional cues in the form of both common or contrasting patterns. Prior work mainly considers multimodal"
P19-1455,P17-1035,0,\N,Missing
P19-1643,D14-1082,0,0.0370601,"ylist, we randomly download ten videos. The following data processing steps are applied: Transcript Filtering. Transcripts are automatically generated by YouTube. We filter out videos that do not contain any transcripts or that contain transcripts with an average (over the entire video) of less than 0.5 words per second. These videos do not contain detailed action descriptions so we cannot effectively leverage textual information. 6408 Extract Candidate Actions from Transcript. Starting with the transcript, we generate a noisy list of potential actions. This is done using the Stanford parser (Chen and Manning, 2014) to split the transcript into sentences and identify verb phrases, augmented by a set of hand-crafted rules to eliminate some parsing errors. The resulting actions are noisy, containing phrases such as “found it helpful if you” and “created before up the top you.” Segment Videos into Miniclips. The length of our collected videos varies from two minutes to twenty minutes. To ease the annotation process, we split each video into miniclips (short video sequences of maximum one minute). Miniclips are split to minimize the chance that the same action is shown across multiple miniclips. This is done"
P19-1643,D18-1167,0,0.0201247,"l., 2016). Specifically, we use an architecture similar to (Jang et al., 2017), where an LSTM (Hochreiter and Schmidhuber, 1997) is used together with frame-level visual features such as Inception (Szegedy et al., 2016), and sequence-level features such as C3D (Tran et al., 2015). However, unlike (Jang et al., 2017) who encode the textual information (question-answers pairs) using an LSTM, we chose instead to encode our textual information (action descriptions and their contexts) using a large-scale language model ELMo (Peters et al., 2018). Similar to previous research on multimodal methods (Lei et al., 2018; Xu et al., 2015; Wu et al., 2013; Jang et al., 2017), we also perform feature ablation to determine the role played by each modality in solving the task. Consistent with earlier work, we observe that the textual modality leads to the highest performance across individual modalities, and that the multimodal model combining textual and visual clues has the best overall performance. Query Results my morning routine my after school routine my workout routine my cleaning routine DIY 28M+ 13M+ 23M+ 13M+ 78M+ Table 2: Approximate number of videos found when searching for routine and do-it-yourself"
P19-1643,D14-1162,0,0.0853228,"if actions mentioned in the transcript of a video are visually represented in the video. We develop a multimodal model that leverages both visual and textual information, and we compare its performance with several singlemodality baselines. 4.1 Data Processing and Representations Starting with our annotated dataset, which includes miniclips paired with transcripts and candidate actions drawn from the transcript, we extract several layers of information, which we then use to develop our multimodal model, as well as several baselines. Action Embeddings. To encode each action, we use both GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. When using GloVe embeddings, we represent the action as the average of all its individual word embeddings. We use embeddings with dimension 50. When using ELMo, we represent the action as a list of words which we feed into the default ELMo embedding layer.2 This performs a fixed mean pooling of all the contextualized word representations in each action. Part-of-speech (POS). We use POS information for each action. Similar to word embeddings (Pennington et al., 2014), we train POS embeddings. We run the Stanford POS Tagger (Toutanova et al., 2003) on"
P19-1643,N18-1202,0,0.143737,"t descriptions to video content (Karpathy and Fei-Fei, 2015; Rohrbach et al., 2016). Specifically, we use an architecture similar to (Jang et al., 2017), where an LSTM (Hochreiter and Schmidhuber, 1997) is used together with frame-level visual features such as Inception (Szegedy et al., 2016), and sequence-level features such as C3D (Tran et al., 2015). However, unlike (Jang et al., 2017) who encode the textual information (question-answers pairs) using an LSTM, we chose instead to encode our textual information (action descriptions and their contexts) using a large-scale language model ELMo (Peters et al., 2018). Similar to previous research on multimodal methods (Lei et al., 2018; Xu et al., 2015; Wu et al., 2013; Jang et al., 2017), we also perform feature ablation to determine the role played by each modality in solving the task. Consistent with earlier work, we observe that the textual modality leads to the highest performance across individual modalities, and that the multimodal model combining textual and visual clues has the best overall performance. Query Results my morning routine my after school routine my workout routine my cleaning routine DIY 28M+ 13M+ 23M+ 13M+ 78M+ Table 2: Approximate"
P19-1643,N03-1033,0,0.0307407,"e (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. When using GloVe embeddings, we represent the action as the average of all its individual word embeddings. We use embeddings with dimension 50. When using ELMo, we represent the action as a list of words which we feed into the default ELMo embedding layer.2 This performs a fixed mean pooling of all the contextualized word representations in each action. Part-of-speech (POS). We use POS information for each action. Similar to word embeddings (Pennington et al., 2014), we train POS embeddings. We run the Stanford POS Tagger (Toutanova et al., 2003) on the transcripts and assign a POS to each word in an action. To obtain the POS embeddings, we train GloVe on the Google N-gram corpus3 using POS information from the five-grams. Finally, for each action, we average together the POS embeddings for all the words in the action to form a POS embedding vector. Context Embeddings. Context can be helpful to determine if an action is visible or not. We use two types of context information, action-level and sentence-level. Action-level context takes into account the previous action and the next action; we denote it as ContextA . These are each calcu"
P19-1643,N03-1000,0,0.14318,"Missing"
P99-1020,P94-1020,0,0.0605572,"1 Introduction Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et al., 1992), (Miller et al., 1994), (Agirre and Rigau, 1995), (Li et al., 1995), (McRoy, 1992); There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. A possible solution for automatic acquisition of sense tagged corpora has been presented in (Mihalcea and Moldovan, 1999), but the corpora acquired with this method has not been yet tested for statistical disambiguation of words. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. None of the sta"
P99-1020,H92-1046,0,0.0370666,"Missing"
P99-1020,H92-1045,0,0.0109888,"he sentence context. The words are paired and an attempt is made to disambiguate one word within the context of the other word. This is done by searching on Internet with queries formed using different senses of one word, while keeping the other word fixed. The senses are ranked simply by the order provided by the number of hits. A good accuracy is obtained, perhaps because the number of texts on the Internet is so large. In this way, all the words are 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale et al., 1992), (Ng and Lee, 1996); 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky, 1995) (Resnik, 1997). 152 processed and the senses axe ranked. We use the ranking of senses to curb the computational complexity in the step that follows. Only the most promising senses are kept. The next step is to refine the ordering of senses by using a completely different method, namely the semantic density. This is measured by the number of common words that are within a semantic distance of two or more words. The closer the semantic relationship between two words the hi"
P99-1020,J92-1001,0,0.0850231,"Missing"
P99-1020,H94-1046,0,0.00891521,"Missing"
P99-1020,P96-1006,0,0.458527,"e Disambiguation (WSD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et al., 1992), (Miller et al., 1994), (Agirre and Rigau, 1995), (Li et al., 1995), (McRoy, 1992); There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. A possible solution for automatic acquisition of sense tagged corpora has been presented in (Mihalcea and Moldovan, 1999), but the corpora acquired with this method has not been yet tested for statistical disambiguation of words. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. None of the statistical methods di"
P99-1020,W97-0213,0,0.0121105,"the use of glosses is that they are not part-of-speech tagged, like some corpora are (i.e. Treebank). For this reason, when determining the nouns from the verb glosses, an error rate is introduced, as some verbs (like make, have, go, do) are lexically ambiguous having a noun representation in WordNet as well. We believe that future work on part-of-speech tagging the glosses of WordNet will improve our results. 2. The determination of senses in SemCor was done of course within a larger context, the context of sentence and discourse. By working 156 Comparison with other methods As indicated in (Resnik and Yarowsky, 1997), it is difficult to compare the WSD methods, as long as distinctions reside in the approach considered (MRD based methods, supervised or unsupervised statistical methods), and in the words that are disambiguated. A method that disambiguates unrestricted nouns, verbs, adverbs and adjectives in texts is presented in (Stetina et al., 1998); it attempts to exploit sentential and discourse contexts and is based on the idea of semantic distance between words, and lexical relations. It uses WordNet and it was tested on SemCor. Table 4 presents the accuracy obtained by other WSD methods. The baseline"
P99-1020,W97-0209,0,0.0270838,"searching on Internet with queries formed using different senses of one word, while keeping the other word fixed. The senses are ranked simply by the order provided by the number of hits. A good accuracy is obtained, perhaps because the number of texts on the Internet is so large. In this way, all the words are 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale et al., 1992), (Ng and Lee, 1996); 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky, 1995) (Resnik, 1997). 152 processed and the senses axe ranked. We use the ranking of senses to curb the computational complexity in the step that follows. Only the most promising senses are kept. The next step is to refine the ordering of senses by using a completely different method, namely the semantic density. This is measured by the number of common words that are within a semantic distance of two or more words. The closer the semantic relationship between two words the higher the semantic density between them. We introduce the semantic density because it is relatively easy to measure it on a MRD like WordNet"
P99-1020,P97-1007,0,0.0160918,"SD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et al., 1992), (Miller et al., 1994), (Agirre and Rigau, 1995), (Li et al., 1995), (McRoy, 1992); There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. A possible solution for automatic acquisition of sense tagged corpora has been presented in (Mihalcea and Moldovan, 1999), but the corpora acquired with this method has not been yet tested for statistical disambiguation of words. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. None of the statistical methods disambiguate adjectives"
P99-1020,W98-0701,0,0.0221786,"ut the corpora acquired with this method has not been yet tested for statistical disambiguation of words. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. None of the statistical methods disambiguate adjectives or adverbs so far. In this paper, we introduce a method that attempts to disambiguate all the nouns, verbs, adjectives and adverbs in a text, using the senses provided in WordNet (Fellbaum, 1998). To our knowledge, there is only one other method, recently reported, that disambiguates unrestricted words in texts (Stetina et al., 1998). 2 A word-word approach dependency The method presented here takes advantage of the sentence context. The words are paired and an attempt is made to disambiguate one word within the context of the other word. This is done by searching on Internet with queries formed using different senses of one word, while keeping the other word fixed. The senses are ranked simply by the order provided by the number of hits. A good accuracy is obtained, perhaps because the number of texts on the Internet is so large. In this way, all the words are 2. WSD that use information gathered from training on a corpu"
P99-1020,P95-1026,0,0.0407558,"This is done by searching on Internet with queries formed using different senses of one word, while keeping the other word fixed. The senses are ranked simply by the order provided by the number of hits. A good accuracy is obtained, perhaps because the number of texts on the Internet is so large. In this way, all the words are 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale et al., 1992), (Ng and Lee, 1996); 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky, 1995) (Resnik, 1997). 152 processed and the senses axe ranked. We use the ranking of senses to curb the computational complexity in the step that follows. Only the most promising senses are kept. The next step is to refine the ordering of senses by using a completely different method, namely the semantic density. This is measured by the number of common words that are within a semantic distance of two or more words. The closer the semantic relationship between two words the higher the semantic density between them. We introduce the semantic density because it is relatively easy to measure it on a M"
perez-rosas-etal-2012-learning,E06-1026,0,\N,Missing
perez-rosas-etal-2012-learning,I05-1001,0,\N,Missing
perez-rosas-etal-2012-learning,banea-etal-2008-bootstrapping,1,\N,Missing
perez-rosas-etal-2012-learning,E06-2031,0,\N,Missing
perez-rosas-etal-2012-learning,W06-1639,0,\N,Missing
perez-rosas-etal-2012-learning,N06-1026,0,\N,Missing
perez-rosas-etal-2012-learning,C08-1135,0,\N,Missing
perez-rosas-etal-2012-learning,D08-1058,0,\N,Missing
perez-rosas-etal-2012-learning,W03-1017,0,\N,Missing
perez-rosas-etal-2012-learning,D08-1014,1,\N,Missing
perez-rosas-etal-2012-learning,W06-1642,0,\N,Missing
perez-rosas-etal-2012-learning,E09-1077,0,\N,Missing
perez-rosas-etal-2012-learning,P06-1134,1,\N,Missing
perez-rosas-etal-2012-learning,P11-1015,0,\N,Missing
perez-rosas-etal-2012-learning,P11-2103,0,\N,Missing
perez-rosas-etal-2012-learning,D07-1115,0,\N,Missing
perez-rosas-etal-2012-learning,P11-2099,0,\N,Missing
perez-rosas-etal-2012-learning,P07-1123,1,\N,Missing
perez-rosas-etal-2012-learning,C10-1136,0,\N,Missing
perez-rosas-etal-2012-learning,P08-1041,0,\N,Missing
perez-rosas-etal-2012-learning,H05-1073,0,\N,Missing
perez-rosas-etal-2012-learning,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
R09-1024,westerhout-monachesi-2008-creating,0,0.101359,"terms can be ambiguous, e.g., “tree” or “list,” and thus we explicitly disambiguate the query by adding the phrase “data structure.” By performing this explicit disambiguation, we can focus on the educativeness property of the documents returned by the search, rather than on the differences that could arise from ambiguities of meaning. 3.1 Properties of Educational Materials We define a set of features largely based on the properties associated with learning objects, as defined in standards such as IEEE LOM [2]. Some of the features are also motivated by previous work on educational metadata [11]. The following features are associated with each document in the data set. Educativeness To be able to capture the educativeness of a resource, the annotators had to score each page on its overall educative value. This feature serves as the major class of the documents in the data set. The annotators were instructed to evaluate the resource as a necessary asset for a student to understand the topic, and score each document on a four point scale ranging from ”noneducative” to ”strongly-educative.” 2 From the top 60 documents, some had to be removed prior to any further processing, because they"
R09-1073,W04-0906,0,0.0172485,"feature film debut won awards. The market is tight right now. Keywords lexical semantics, synonym expansion, lexical substitution Target bright bright film tight Synonym brilliant intelligent movie pressured Table 1: Examples of synonym expansion in context 1 Introduction Word meanings are central to the semantic interpretation of texts. The understanding of the meaning of words is important for a large number of natural language processing applications, including information retrieval [11, 10, 19], machine translation [4, 3], knowledge acquisition [7], text simplification, question answering [1], cross-language information retrieval [18, 5]. In this paper, we experiment with contextual synonym expansion as a way to represent word meanings in context. We combine the benefits of multiple lexical resources in order to define flexible word meanings that can be adapted to the context at hand. The task, also referred to as lexical substitution, has been officially introduced during Semeval-2007 [16], where participating systems were asked to provide lists of synonyms that were appropriate for selected target words in a given context. Although it may sound simple at first, the task is remar"
R09-1073,D07-1007,0,0.0335256,"e context. Sentence The sun was bright. He was bright and independent. His feature film debut won awards. The market is tight right now. Keywords lexical semantics, synonym expansion, lexical substitution Target bright bright film tight Synonym brilliant intelligent movie pressured Table 1: Examples of synonym expansion in context 1 Introduction Word meanings are central to the semantic interpretation of texts. The understanding of the meaning of words is important for a large number of natural language processing applications, including information retrieval [11, 10, 19], machine translation [4, 3], knowledge acquisition [7], text simplification, question answering [1], cross-language information retrieval [18, 5]. In this paper, we experiment with contextual synonym expansion as a way to represent word meanings in context. We combine the benefits of multiple lexical resources in order to define flexible word meanings that can be adapted to the context at hand. The task, also referred to as lexical substitution, has been officially introduced during Semeval-2007 [16], where participating systems were asked to provide lists of synonyms that were appropriate for selected target words in a"
R09-1073,P07-1005,0,0.0366659,"e context. Sentence The sun was bright. He was bright and independent. His feature film debut won awards. The market is tight right now. Keywords lexical semantics, synonym expansion, lexical substitution Target bright bright film tight Synonym brilliant intelligent movie pressured Table 1: Examples of synonym expansion in context 1 Introduction Word meanings are central to the semantic interpretation of texts. The understanding of the meaning of words is important for a large number of natural language processing applications, including information retrieval [11, 10, 19], machine translation [4, 3], knowledge acquisition [7], text simplification, question answering [1], cross-language information retrieval [18, 5]. In this paper, we experiment with contextual synonym expansion as a way to represent word meanings in context. We combine the benefits of multiple lexical resources in order to define flexible word meanings that can be adapted to the context at hand. The task, also referred to as lexical substitution, has been officially introduced during Semeval-2007 [16], where participating systems were asked to provide lists of synonyms that were appropriate for selected target words in a"
R09-1073,2007.mtsummit-papers.24,0,0.111334,"is tight right now. Keywords lexical semantics, synonym expansion, lexical substitution Target bright bright film tight Synonym brilliant intelligent movie pressured Table 1: Examples of synonym expansion in context 1 Introduction Word meanings are central to the semantic interpretation of texts. The understanding of the meaning of words is important for a large number of natural language processing applications, including information retrieval [11, 10, 19], machine translation [4, 3], knowledge acquisition [7], text simplification, question answering [1], cross-language information retrieval [18, 5]. In this paper, we experiment with contextual synonym expansion as a way to represent word meanings in context. We combine the benefits of multiple lexical resources in order to define flexible word meanings that can be adapted to the context at hand. The task, also referred to as lexical substitution, has been officially introduced during Semeval-2007 [16], where participating systems were asked to provide lists of synonyms that were appropriate for selected target words in a given context. Although it may sound simple at first, the task is remarkably difficult, as evidenced by the accuracie"
R09-1073,N03-1011,0,0.0269372,"s bright. He was bright and independent. His feature film debut won awards. The market is tight right now. Keywords lexical semantics, synonym expansion, lexical substitution Target bright bright film tight Synonym brilliant intelligent movie pressured Table 1: Examples of synonym expansion in context 1 Introduction Word meanings are central to the semantic interpretation of texts. The understanding of the meaning of words is important for a large number of natural language processing applications, including information retrieval [11, 10, 19], machine translation [4, 3], knowledge acquisition [7], text simplification, question answering [1], cross-language information retrieval [18, 5]. In this paper, we experiment with contextual synonym expansion as a way to represent word meanings in context. We combine the benefits of multiple lexical resources in order to define flexible word meanings that can be adapted to the context at hand. The task, also referred to as lexical substitution, has been officially introduced during Semeval-2007 [16], where participating systems were asked to provide lists of synonyms that were appropriate for selected target words in a given context. Although it"
R09-1073,S07-1029,0,0.106601,"Missing"
R09-1073,S07-1091,1,0.779828,"two systems also experimented with two different lexical resources. Also, several systems used Web queries or Google N-gram data to obtain counts for contextual fitness. We describe below the top five performing systems. KU [20] is the highest ranking system for the best normal metric. It uses a statistical language model based on the Google Web 1T five-grams dataset to calculate the probabilities of all the synonyms. In the development phase, it compares two of the resources that we use in our work, namely WordNet and Roget’s Thesaurus. In the test phase, it only uses the Roget resource. UNT [9] is the best system for both the best mode and the oot mode mode. As lexical resources, it uses WordNet and Encarta, along with back-and-forth translations collected from commercial translation engines, and N-gram-based models calculated on the Google Web 1T corpus. 409 Conclusions In this paper, we experimented with the task of synonym expansion, and compared the benefits of combining multiple lexical resources, by using several contextual fitness models integrated into both unsupervised and supervised approaches. The experiments provided us with several insights into the most useful resource"
R09-1073,P97-1010,0,0.0607212,"t word and at the same time fit the context. Sentence The sun was bright. He was bright and independent. His feature film debut won awards. The market is tight right now. Keywords lexical semantics, synonym expansion, lexical substitution Target bright bright film tight Synonym brilliant intelligent movie pressured Table 1: Examples of synonym expansion in context 1 Introduction Word meanings are central to the semantic interpretation of texts. The understanding of the meaning of words is important for a large number of natural language processing applications, including information retrieval [11, 10, 19], machine translation [4, 3], knowledge acquisition [7], text simplification, question answering [1], cross-language information retrieval [18, 5]. In this paper, we experiment with contextual synonym expansion as a way to represent word meanings in context. We combine the benefits of multiple lexical resources in order to define flexible word meanings that can be adapted to the context at hand. The task, also referred to as lexical substitution, has been officially introduced during Semeval-2007 [16], where participating systems were asked to provide lists of synonyms that were appropriate fo"
R09-1073,S07-1050,0,0.0853712,"Missing"
R09-1073,S07-1009,0,0.457982,"a@my.unt.edu, rada@cs.unt.edu 2 Abstract In this paper, we experiment with the task of contextual synonym expansion, and compare the benefits of combining multiple lexical resources using both unsupervised and supervised approaches. Overall, the results obtained through the combination of several resources exceed the current state-of-the-art when selecting the best synonym for a given target word, and place second when selecting the top ten synonyms, thus demonstrating the usefulness of the approach. Synonym expansion in context Contextual synonym expansion, also known as lexical substitution [16], is the task of replacing a certain word in a given context with another, suitable word. See for example the four sentences from table 1, drawn from the development data from the Semeval-2007 lexical substitution task. In the first sentence, for instance, assuming we choose bright as the target word, a suitable substitute could be brilliant, which would both maintain the meaning of the target word and at the same time fit the context. Sentence The sun was bright. He was bright and independent. His feature film debut won awards. The market is tight right now. Keywords lexical semantics, synony"
R09-1073,H05-1051,0,0.0226915,"t word and at the same time fit the context. Sentence The sun was bright. He was bright and independent. His feature film debut won awards. The market is tight right now. Keywords lexical semantics, synonym expansion, lexical substitution Target bright bright film tight Synonym brilliant intelligent movie pressured Table 1: Examples of synonym expansion in context 1 Introduction Word meanings are central to the semantic interpretation of texts. The understanding of the meaning of words is important for a large number of natural language processing applications, including information retrieval [11, 10, 19], machine translation [4, 3], knowledge acquisition [7], text simplification, question answering [1], cross-language information retrieval [18, 5]. In this paper, we experiment with contextual synonym expansion as a way to represent word meanings in context. We combine the benefits of multiple lexical resources in order to define flexible word meanings that can be adapted to the context at hand. The task, also referred to as lexical substitution, has been officially introduced during Semeval-2007 [16], where participating systems were asked to provide lists of synonyms that were appropriate fo"
R09-1073,S07-1044,0,0.0320436,"all the other systems for the best normal and best mode metrics, and ranks the second for the oot normal and oot mode metrics, demonstrating the usefulness of our combined approach. 7 Related work There are several systems for synonym expansion that participated in the Semeval-2007 lexical substitution task [16]. Most of the systems used only one lexical resource, although two systems also experimented with two different lexical resources. Also, several systems used Web queries or Google N-gram data to obtain counts for contextual fitness. We describe below the top five performing systems. KU [20] is the highest ranking system for the best normal metric. It uses a statistical language model based on the Google Web 1T five-grams dataset to calculate the probabilities of all the synonyms. In the development phase, it compares two of the resources that we use in our work, namely WordNet and Roget’s Thesaurus. In the test phase, it only uses the Roget resource. UNT [9] is the best system for both the best mode and the oot mode mode. As lexical resources, it uses WordNet and Encarta, along with back-and-forth translations collected from commercial translation engines, and N-gram-based model"
R09-1073,S07-1036,0,0.065008,"Missing"
R13-1022,N07-1025,1,0.919321,"hio University University of North Texas bunescu@ohio.edu bharathdandala@gmail.com chris.hokamp@gmail.com rada@cs.unt.edu Abstract Moldovan, 2001) or the Oxford dictionary (Navigli, 2006), newer user-contributed sense inventories such as Wikipedia or Wiktionary are also quickly expanding and refining the senses defined for a word, thus pointing to the need of sense clustering for coarser word sense distinctions. In this paper, we specifically focus on the task of sense clustering over Wikipedia senses. Wikipedia has been recently recognized as a rich resource for WSD (Bunescu and Pasca, 2006; Mihalcea, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008), offering a significantly increased coverage of word meanings relative to established repositories such as WordNet or Roget. At the same time, WSD systems using Wikipedia have been shown to obtain comparable or even increased disambiguation precision. While earlier work on WSD using the 2007 version of Wikipedia reported an average of three senses per word for a dataset of 30 nouns (Mihalcea, 2007), more recent work on the same dataset using the 2012 version of Wikipedia has shown a significant increase to an average of nine senses per word"
R13-1022,E06-1002,1,0.831518,"Missing"
R13-1022,P07-1005,0,0.0376024,"ed in improved WSD performance. The OntoNotes project, a large-scale effort to cluster and supplement word senses in WordNet in order to produce a high-quality dataset for automatic WSD (Hovy et al., 2006), has also been beneficial for other language processing tasks such as discourse analysis, coreference resolution, and semantic parsing. Coarser sense inventories also make it easier to identify synonyms or translations of selected words in context, which can lead to improvements in information retrieval (Zhong and Ng, 2012), semantic indexing (Gonzalo et al., 1998), and machine translation (Chan et al., 2007). In this paper, we address two main research questions. First, can we build an accurate method to automatically cluster the fine-grained senses in Wikipedia? We describe a set of structural and content features that are integrated in a machine learning framework in order to automatically predict when two Wikipedia senses are close in meaning and should be clustered together. Second, can we use the multilingual links in Wikipedia to derive additional multilingual features to enhance this clustering? We rely upon the interlingua links in Wikipedia, and upon features that can be obtained from se"
R13-1022,W02-0817,1,0.693612,"Missing"
R13-1022,P06-1014,0,0.183215,"ar indicator of very finegrained word senses that are difficult to differentiate, even for humans. To achieve the sense granularity appropriate for WSD, word senses that are closely related in meaning are grouped together in a sense clustering step. While this task was originally defined in relation to more traditional sense inventories, such as WordNet (Hovy et al., 2006; Mihalcea and 164 Proceedings of Recent Advances in Natural Language Processing, pages 164–171, Hissar, Bulgaria, 7-13 September 2013. clustering. For example, work on mapping WordNet senses to the coarser Oxford dictionary (Navigli, 2006; Navigli et al., 2007) has resulted in improved WSD performance. The OntoNotes project, a large-scale effort to cluster and supplement word senses in WordNet in order to produce a high-quality dataset for automatic WSD (Hovy et al., 2006), has also been beneficial for other language processing tasks such as discourse analysis, coreference resolution, and semantic parsing. Coarser sense inventories also make it easier to identify synonyms or translations of selected words in context, which can lead to improvements in information retrieval (Zhong and Ng, 2012), semantic indexing (Gonzalo et al."
R13-1022,S12-1004,1,0.876057,"Missing"
R13-1022,D07-1107,0,0.131263,"Missing"
R13-1022,D09-1046,0,0.0134123,"onstraints that arise when merging senses in a hierarchical structures. Another closely related work is that of (Pedersen et al., 2005), which describes an unsupervised method for discriminating ambiguous names by clustering contexts, and relies upon features found in corpora obtained for a language with more resources. The major aim of the coarse-grained all-words WSD task at Semeval-2007 was to determine whether a more accurate WSD system can enable sense-aware applications, such as information retrieval, question answering, or machine translation. Finally, in recent work, Erk and McCarthy (Erk and McCarthy, 2009) also considered the sense granularity issue, and introduced the idea of graded WSD, in which they relax the single sense assignment and allow for multiple sense assignments for a particular target word. from other languages. Even for English, which is a major language with significant resources, we observe improvements when multilingual features are added.These results support our hypothesis that multilingual features can improve the accuracy of sense clustering, even in a more realistic setting where we do not have corresponding sense pairs in all languages. In such cases, when trying to clu"
R13-1022,W98-0705,0,0.0592352,"(Navigli, 2006; Navigli et al., 2007) has resulted in improved WSD performance. The OntoNotes project, a large-scale effort to cluster and supplement word senses in WordNet in order to produce a high-quality dataset for automatic WSD (Hovy et al., 2006), has also been beneficial for other language processing tasks such as discourse analysis, coreference resolution, and semantic parsing. Coarser sense inventories also make it easier to identify synonyms or translations of selected words in context, which can lead to improvements in information retrieval (Zhong and Ng, 2012), semantic indexing (Gonzalo et al., 1998), and machine translation (Chan et al., 2007). In this paper, we address two main research questions. First, can we build an accurate method to automatically cluster the fine-grained senses in Wikipedia? We describe a set of structural and content features that are integrated in a machine learning framework in order to automatically predict when two Wikipedia senses are close in meaning and should be clustered together. Second, can we use the multilingual links in Wikipedia to derive additional multilingual features to enhance this clustering? We rely upon the interlingua links in Wikipedia, a"
R13-1022,W04-0811,0,0.0937958,"Missing"
R13-1022,N06-2015,0,0.127219,"Missing"
R13-1022,P12-1029,0,0.0131253,"senses to the coarser Oxford dictionary (Navigli, 2006; Navigli et al., 2007) has resulted in improved WSD performance. The OntoNotes project, a large-scale effort to cluster and supplement word senses in WordNet in order to produce a high-quality dataset for automatic WSD (Hovy et al., 2006), has also been beneficial for other language processing tasks such as discourse analysis, coreference resolution, and semantic parsing. Coarser sense inventories also make it easier to identify synonyms or translations of selected words in context, which can lead to improvements in information retrieval (Zhong and Ng, 2012), semantic indexing (Gonzalo et al., 1998), and machine translation (Chan et al., 2007). In this paper, we address two main research questions. First, can we build an accurate method to automatically cluster the fine-grained senses in Wikipedia? We describe a set of structural and content features that are integrated in a machine learning framework in order to automatically predict when two Wikipedia senses are close in meaning and should be clustered together. Second, can we use the multilingual links in Wikipedia to derive additional multilingual features to enhance this clustering? We rely"
R13-1022,W06-2503,0,0.0671667,"Missing"
R13-1022,S07-1006,0,\N,Missing
S01-1031,J95-4004,0,0.0137324,"ning from available sense tagged corpora and dictionary definitions and instance based learning with active feature selection. The two modules are preceded by a preprocessing phase which includes compound concept identification, and followed by a default phase that assigns the most frequent sense as a last resort, when no other previous methods could be applied. The shaded areas in Figure 1 are specific for the case when larger training data sets are available. During the preprocessing stage, SGML tags are eliminated, the text is tokenized, part of speech tags are assigned using Brill tagger (Brill, 1995), and Named Entities (NE) are identified with an in-house implementation of an NE recognizer. To identify collocations, we determine sequences of words that form compound concept,s defined in WordNet. In the second step, patterns 3 are learned from WordNet, SemCor and GenCor, which is a large 2 I.e. in addition to the publicly available sense tagged corpora 3 We alternatively call them rules as they basically specify the sense triggered by a given local context, using rules like ""if the word before is X then sense is Y"" FREE UNTAGGED TEXT ,'~REPROCESSING , '' - eliminate SGMLtags - tokenizatio"
S07-1013,esuli-sebastiani-2006-sentiwordnet,0,0.258438,"d to identify what is being said about the main subject by exploiting the dependency graph obtained from the parser. Each word was first rated separately for each emotion (the six emotions plus Compassion) and for valence. Next, the main subject rating was boosted. Contrasts and accentuations between “good” or “bad” were detected, making it possible to identify surprising good or bad news. The system also takes into account: human will (as opposed to illness or natural disasters); negation and modals; high-tech context; celebrities. The lexical resource used was a combination of SentiWordNet (Esuli and Sebastiani, 2006) and WordNetAffect (Strapparava and Valitutti, 2004), which were semi-automatically enriched on the basis of the original trial data. SICS: The SICS team used a very simple approach for valence annotation based on a word-space model and a set of seed words. The idea was to create two points in a high-dimensional word space one representing positive valence, the other representing negative valence - and then projecting each headline into this space, choosing the valence whose point was closer to the headline. The word space was produced from a lemmatized and stop list filtered version of the LA"
S07-1013,P97-1023,0,0.0190084,"a knowledgebased domain-independent unsupervised approach to headline valence detection and scoring. The system uses three main kinds of knowledge: a list of sentiment-bearing words, a list of valence shifters and a set of rules that define the scope and the result of the combination of sentiment-bearing words and valence shifters. The unigrams used for sentence/headline classification were learned from WordNet dictionary entries. In order to take advantage of the special properties of WordNet glosses and relations, we developed a system that used the list of human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and learned additional unigrams from WordNet synsets and glosses. The list was then expanded by adding to it all the words annotated with Positive or Negative tags in the General Inquirer. Each unigram in the resulting list had the degree of membership in the category of positive or negative sentiment assigned to it using the fuzzy Net Overlap Score method described in the team’s earlier work (Andreevskaia and Bergler, 2006). Only words with fuzzy membership score not equal to zero were retained in the list. The resulting list contained 10,809 sentimentbearing words of differen"
S07-1013,P04-1035,0,0.0278626,"e developed a Webbased annotation interface that displayed one headline at a time, together with six slide bars for emotions and one slide bar for valence. The interval for the emotion annotations was set to [0, 100], where 0 means the emotion is missing from the given headline, and 100 represents maximum emotional load. The interval for the valence annotations was set to [−100, 100], where 0 represents a neutral headline, −100 represents a highly negative headline, and 100 corresponds to a highly positive headline. Unlike previous annotations of sentiment or subjectivity (Wiebe et al., 2005; Pang and Lee, 2004), which typically relied on binary 0/1 annotations, we decided to use a finer-grained scale, hence allowing the annotators to select different degrees of emotional load. The test data set was independently labeled by six annotators. The annotators were instructed to select the appropriate emotions for each headline based on the presence of words or phrases with emotional content, as well as the overall feeling invoked by the headline. Annotation examples were also provided, including examples of headlines bearing two or more emotions to illustrate the case where several emotions were jointly a"
S07-1013,strapparava-valitutti-2004-wordnet,1,0.728742,"am was able to participate in one or both tasks. The task was carried out in an unsupervised setting, and consequently no training was provided. The reason behind this decision is that we wanted to emphasize the study of emotion lexical semantics, and avoid biasing the participants toward simple “text categorization” approaches. Nonetheless supervised systems were not precluded from participation, and in such cases the teams were allowed to create their own supervised training sets. Participants were free to use any resources they wanted. We provided a set words extracted from WordNet Affect (Strapparava and Valitutti, 2004), relevant to the six emotions of interest. However, the use of this list was entirely optional. 71 2.1 Data Set The data set consisted of news headlines drawn from major newspapers such as New York Times, CNN, and BBC News, as well as from the Google News search engine. We decided to focus our attention on headlines for two main reasons. First, news have typically a high load of emotional content, as they describe major national or worldwide events, and are written in a style meant to attract the attention of the readers. Second, the structure of headlines was appropriate for our goal of cond"
S07-1013,strapparava-etal-2006-affective,1,0.465856,"ches to emotion recognition. The task is not easy. Indeed, as (Ortony et al., 1987) indicates, besides words directly referring to emotional states (e.g., “fear”, “cheerful”) and for which an appropriate lexicon would help, there are words that act only as an indirect reference to 70 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 70–74, c Prague, June 2007. 2007 Association for Computational Linguistics emotions depending on the context (e.g. “monster”, “ghost”). We can call the former direct affective words and the latter indirect affective words (Strapparava et al., 2006). 2 Task Definition We proposed to focus on the emotion classification of news headlines extracted from news web sites. Headlines typically consist of a few words and are often written by creative people with the intention to “provoke” emotions, and consequently to attract the readers’ attention. These characteristics make this type of text particularly suitable for use in an automatic emotion recognition setting, as the affective/emotional features (if present) are guaranteed to appear in these short sentences. The structure of the task was as follows: Corpus: News titles, extracted from news"
S07-1090,W06-1670,1,0.847894,"training and evaluation data using three sequential taggers, one for each tagset. The tagger is a Hidden Markov Model trained with the perceptron algorithm introduced in (Collins, 2002), which applies Viterbi decoding and is regularized using averaging. Label to label dependencies are limited to the previous tag (first order HMM). We use a generic feature set for NER based on words, lemmas, POS tags, and word shape features, in addition we use as a feature of each token the supersense of a first (super)sense baseline. A detailed description of the features used and the tagger can be found in (Ciaramita and Altun, 2006). The supersense tagger is trained on the Brown sections one and two of SemCor. The BBN tagger is trained on sections 221 of the BBN corpus. The ACE tagger is trained 1 BBN Corpus documentation. 408 5 Feature Combination For the final system we create a combined feature set for each target word, consisting of the lemma, the part of speech, the collocational S ENSE L EARNER features, and the three coarse grained semantic tags of the target word. Note that the semantic features are represented as lemma TAG to avoid overgeneralization. In the training stage, a feature vector is constructed for ea"
S07-1090,W02-1001,0,0.017145,"er words, the tagger predicts a labeled bracketing of the tokens in each sentence. As an example, the supersense tagger annotates the tokens in the phrase “substance abuse” as “substanceB−noun.act ” and “abuseI−noun.act ”, although the gold standard segmentation of the data does not identify the phrase as one lemma. We use the labels generated in this way as features of each token to disambiguate. 4.2 Taggers We annotate the training and evaluation data using three sequential taggers, one for each tagset. The tagger is a Hidden Markov Model trained with the perceptron algorithm introduced in (Collins, 2002), which applies Viterbi decoding and is regularized using averaging. Label to label dependencies are limited to the previous tag (first order HMM). We use a generic feature set for NER based on words, lemmas, POS tags, and word shape features, in addition we use as a feature of each token the supersense of a first (super)sense baseline. A detailed description of the features used and the tagger can be found in (Ciaramita and Altun, 2006). The supersense tagger is trained on the Brown sections one and two of SemCor. The BBN tagger is trained on sections 221 of the BBN corpus. The ACE tagger is"
S07-1090,W04-0827,0,0.417342,"Missing"
S07-1090,W02-0814,0,0.0722626,"Missing"
S07-1090,P05-3014,1,0.781889,"Missing"
S07-1090,C02-1039,1,0.728802,"arning, we are using the Timbl memory based learning algorithm (Daelemode noun verb all Training size 89052 48936 137988 R ESULTS Precision Recall 0.658 0.228 0.539 0.353 0.583 0.583 References Table 1: Precision and recall for the S UPER S ENSE L EARNER semantic models. mode noun verb all Training size 89052 48936 137988 R ESULTS Precision Recall 0.666 0.233 0.554 0.360 0.593 0.593 Table 2: Precision and recall for the S UPER S ENSE L EARNER semantic models - without U labels. mans et al., 2001), which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002; Mihalcea, 2002). Following the learning stage, each vector in the test data set is labeled with a predicted word and sense. If the word predicted by the learning algorithm coincides with the target word in the test feature vector, then the predicted sense is used to annotate the test instance. Otherwise, if the predicted word is different from the target word, no annotation is produced, and the word is left for annotation in a later stage (e.g., using the most frequent sense back-off method). 6 Results The S UPER S ENSE L EARNER system participated in the S EM E VAL all-words word sense disambiguation task."
S07-1090,H93-1061,0,0.0904109,"le the performance of such methods is usually exceeded by their supervised lexical-sample al2 Learning for All-Words Word Sense Disambiguation Our goal is to use as little annotated data as possible, and at the same time make the algorithm general enough to be able to disambiguate as many content words as possible in a text, and efficient enough so that large amounts of text can be annotated in real time. S UPER S ENSE L EARNER is attempting to learn general semantic models for various word categories, starting with a relatively small sense-annotated corpus. We base our experiments on SemCor (Miller et al., 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers. The input to the disambiguation algorithm consists of raw text. The output is a text with word meaning annotations for all open-class words. The algorithm starts with a preprocessing stage, where the text is tokenized and annotated with part406 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 406–409, c Prague, June 2007. 2007 Association for Computational Linguistics of-speech tags; collocations are identified using a sliding window approac"
S07-1091,P06-1057,0,0.0321602,"words available in the given context. Similar to the previous models, the target word in the context is replaced by all the generated inflections of the selected candidate and then queried using a web search engine. The resulting rank represents the sum of the total number of pages in which the candidate or any of its inflections occur together with the context. This also reflects the semantic relatedness or the relevance of the candidate to the context. Word Sense Disambiguation (WSD): Since previous work indicated the usefulness of word sense disambiguation systems in lexical substitution (Dagan et al., 2006), we use the SenseLearner word sense disambiguation tool (Mihalcea and Csomai, 2005) to disambiguate the target word and, accordingly, to propose its synonyms as candidates. 412 Final System: Our candidate ranking methods are aimed at different aspects of what constitutes a good candidate. On one hand, we measure the semantic relatedness of a candidate with the original context (the LSA and WSD methods fall under this category). On the other hand, we also want to ensure that the candidate fits the context and leads to a well formed English sentence (e.g., the language model method). Given that"
S07-1091,S07-1009,0,0.220485,"make up to 10 guesses, without penalizing, and without being of any benefit if less than 10 substitutes are provided. The ordering of guesses in the oot metric is unimportant. For both tracks, the evaluation is carried out using precision and recall, calculated based on the number of matching responses between the system and the human annotators, respectively. A “mode” evaluation is also conducted, which measures the ability of the systems to capture the most frequent response (the “mode”) from the gold standard annotations. For details, please refer to the official task description document (McCarthy and Navigli, 2007). Tables 2 and 3 show the results obtained by S UB F INDER in the best and oot tracks respectively. The tables also show a breakdown of the results based on: only target words that were not identified as multiwords (NMWT); only substitutes that were not identified as multiwords (NMWS); only items with sentences randomly selected from the Internet corpus (RAND); only items with sentences manually selected from the Internet corpus (MAN). best oot WSD 34 6 LSA 2 82 IR 64 7 LB 63 28 MCS 56 46 MTA 69 14 MTG 38 32 LM 97 68 Table 1: Weights of the individual ranking methods OVERALL NMWT NMWS RAND MAN"
S07-1091,W02-0816,0,0.061027,"h the input target word and the context. This paper describes the University of North Texas S UB F INDER system. The system is able to provide the most likely set of substitutes for a word in a given context, by combining several techniques and knowledge sources. S UB F INDER has successfully participated in the best and out of ten (oot) tracks in the S EM E VAL lexical substitution task, consistently ranking in the first or second place. 1 Introduction 3 Lexical substitution is defined as the task of identifying the most likely alternatives (substitutes) for a target word, given its context (McCarthy, 2002). Many natural language processing applications can benefit from the availability of such alternative words, including word sense disambiguation, lexical acquisition, machine translation, information retrieval, question answering, text simplification, and others. The task is closely related to the problem of word sense disambiguation, with the substitutes acting as synonyms for the input word meaning. Unlike word sense disambiguation however, lexical substitution is not performed with respect to a given sense inventory, but instead candidate synonyms are generated “on the fly” for a given word"
S07-1091,P05-3014,1,0.826746,"rget word in the context is replaced by all the generated inflections of the selected candidate and then queried using a web search engine. The resulting rank represents the sum of the total number of pages in which the candidate or any of its inflections occur together with the context. This also reflects the semantic relatedness or the relevance of the candidate to the context. Word Sense Disambiguation (WSD): Since previous work indicated the usefulness of word sense disambiguation systems in lexical substitution (Dagan et al., 2006), we use the SenseLearner word sense disambiguation tool (Mihalcea and Csomai, 2005) to disambiguate the target word and, accordingly, to propose its synonyms as candidates. 412 Final System: Our candidate ranking methods are aimed at different aspects of what constitutes a good candidate. On one hand, we measure the semantic relatedness of a candidate with the original context (the LSA and WSD methods fall under this category). On the other hand, we also want to ensure that the candidate fits the context and leads to a well formed English sentence (e.g., the language model method). Given that the methods described earlier aim at orthogonal aspects of the problem, it is expec"
S10-1002,D07-1007,0,0.0762143,"16 July 2010. 2010 Association for Computational Linguistics 4 The Cross-Lingual Lexical Substitution Task conducted experiments using words in context, rather than a predefined sense-inventory however in these experiments the annotators were asked for a single preferred translation. In our case, we allowed annotators to supply as many translations as they felt were equally valid. This allows us to examine more subtle relationships between usages and to allow partial credit to systems that get a close approximation to the annotators’ translations. Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems are not required to translate the whole context but just the target word. The Cross-Lingual Lexical Substitution task follows LEXSUB except that the annotations are translations rather than paraphrases. Given a target word in context, the task is to provide several correct translations for that word in a given language. We used English as the source language and Spanish as the target language. We provided both development and test sets, but no training data. As for LEXSUB, any systems requiring training data had to obtain it from other sources. We included nouns, verbs"
S10-1002,S01-1009,0,0.110328,"Missing"
S10-1002,S10-1003,0,0.148904,"D tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan, 2007). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not (Sinha et al., 2009). In our task, we collected a dataset which allows instances of the same word to have some translations in common, while not necessitating a clustering of translations from a specific resource into senses (in comparison to Lefever and Hoste (2010)). 1 Resnik and Yarowsky (2000) also In this paper we describe the SemEval2010 Cross-Lingual Lexical Substitution task, where given an English target word in context, participating systems had to find an alternative substitute word or phrase in Spanish. The task is based on the English Lexical Substitution task run at SemEval-2007. In this paper we provide background and motivation for the task, we describe the data annotation process and the scoring system, and present the results of the participating systems. 1 Introduction In the Cross-Lingual Lexical Substitution task, annotators and syste"
S10-1002,S07-1009,1,0.504538,"nt and test sets, but no training data. As for LEXSUB, any systems requiring training data had to obtain it from other sources. We included nouns, verbs, adjectives and adverbs in both development and test data. We used the same set of 30 development words as in LEXSUB , and a subset of 100 words from the LEX SUB test set, selected so that they exhibit a wide variety of substitutes. For each word, the same example sentences were used as in LEXSUB. 3 Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval2007 (McCarthy and Navigli, 2007; McCarthy and Navigli, 2009). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning, such as those proposed by Sch¨utze (1998) and Pantel and Lin (2002). 4.1 Annotation We used four annotators for the task, all native Spanish speakers from Mexico, with a high level of proficiency in English. As in LEXSUB, the annotators were allowed to use any resources they wanted to, and were required to provide as many substitutes as they could think of. The i"
S10-1002,S07-1010,0,0.0605658,"lation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, in our task we provided actual translations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit evaluation of new systems after the competition. Previous standalone WSD tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan, 2007). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not (Sinha et al., 2009). In our task, we collected a dataset which allows instances of the same word to have some translations in common, while not necessitating a clustering of translations from a specific resource into senses (in comparison to Lefever and Hoste (2010)). 1 Resnik and Yarowsky (2000) also In this paper we describe the SemEval2010 Cross-Lingual Lexica"
S10-1002,J98-1004,0,0.385291,"Missing"
S10-1002,W02-0816,1,\N,Missing
S10-1002,W09-2413,0,\N,Missing
S10-1002,H05-1051,0,\N,Missing
S10-1002,W09-2412,1,\N,Missing
S12-1003,W11-0104,1,0.709389,"useful for languages with scarce resources. (Davidov and Rappoport, 2009) experiment with the use of multiple languages to enhance an existing lexicon. In their experiments, using three source languages and 45 intermediate languages, they find that the multilingual resources can lead to significant improvements in concept expansion. (Banea et al., 2010) explore the use of parallel multilingual corpora to improve subjectivity classification in a target language, finding that the use of multilingual representations for subjectivity analysis improves over the monolingual classifiers. Similarly, (Banea and Mihalcea, 2011) investigate the use of multilingual contexts for word sense disambiguation. By leveraging on the translations of the annotated contexts in multiple languages, a multilingual thematic space emerges that better disambiguates target words. Finally, there are two lines of work that explore semantic distances in a multilingual space. First, (Besanc¸on and Rajman, 2002) examine the notion that the distances between document vectors within a language correlate with the distances between their corresponding vectors in a parallel corpus. These findings provide clues about the possibility of reliable s"
S12-1003,C10-1004,1,0.829845,"gulation for machine translation, where multiple translation models are learned using multilingual parallel corpora. The model was found especially beneficial for languages where the training dataset was small, thus suggesting that this method may be particularly useful for languages with scarce resources. (Davidov and Rappoport, 2009) experiment with the use of multiple languages to enhance an existing lexicon. In their experiments, using three source languages and 45 intermediate languages, they find that the multilingual resources can lead to significant improvements in concept expansion. (Banea et al., 2010) explore the use of parallel multilingual corpora to improve subjectivity classification in a target language, finding that the use of multilingual representations for subjectivity analysis improves over the monolingual classifiers. Similarly, (Banea and Mihalcea, 2011) investigate the use of multilingual contexts for word sense disambiguation. By leveraging on the translations of the annotated contexts in multiple languages, a multilingual thematic space emerges that better disambiguates target words. Finally, there are two lines of work that explore semantic distances in a multilingual space"
S12-1003,besancon-rajman-2002-evaluation,0,0.0222953,"Missing"
S12-1003,J90-1003,0,0.0315694,"ilability of the lexical resource in that language; furthermore, even though taxonomies such as WordNet (Miller, 1995) are available in a number of languages1 , their coverage is still limited, and often times they are not publicly available. For these reasons, in multilingual settings, these measures often become untractable. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1991), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (HAL) (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words, and thus they can be easily transferred 1 http://www.illc.uva.nl/EuroWordNet/ 21 to a new language provided that a large corpus in that language is available. Multilingual natural language processing. Also relevant"
S12-1003,P07-1092,0,0.0320035,"the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words, and thus they can be easily transferred 1 http://www.illc.uva.nl/EuroWordNet/ 21 to a new language provided that a large corpus in that language is available. Multilingual natural language processing. Also relevant is the work done on multilingual text processing, which attempts to improve the performance of different natural language processing tasks by integrating information drawn from multiple languages. For instance, (Cohn and Lapata, 2007) explore the use of triangulation for machine translation, where multiple translation models are learned using multilingual parallel corpora. The model was found especially beneficial for languages where the training dataset was small, thus suggesting that this method may be particularly useful for languages with scarce resources. (Davidov and Rappoport, 2009) experiment with the use of multiple languages to enhance an existing lexicon. In their experiments, using three source languages and 45 intermediate languages, they find that the multilingual resources can lead to significant improvement"
S12-1003,D09-1089,0,0.0143492,"guage processing. Also relevant is the work done on multilingual text processing, which attempts to improve the performance of different natural language processing tasks by integrating information drawn from multiple languages. For instance, (Cohn and Lapata, 2007) explore the use of triangulation for machine translation, where multiple translation models are learned using multilingual parallel corpora. The model was found especially beneficial for languages where the training dataset was small, thus suggesting that this method may be particularly useful for languages with scarce resources. (Davidov and Rappoport, 2009) experiment with the use of multiple languages to enhance an existing lexicon. In their experiments, using three source languages and 45 intermediate languages, they find that the multilingual resources can lead to significant improvements in concept expansion. (Banea et al., 2010) explore the use of parallel multilingual corpora to improve subjectivity classification in a target language, finding that the use of multilingual representations for subjectivity analysis improves over the monolingual classifiers. Similarly, (Banea and Mihalcea, 2011) investigate the use of multilingual contexts fo"
S12-1003,D09-1124,1,0.847759,"ambiguation. By leveraging on the translations of the annotated contexts in multiple languages, a multilingual thematic space emerges that better disambiguates target words. Finally, there are two lines of work that explore semantic distances in a multilingual space. First, (Besanc¸on and Rajman, 2002) examine the notion that the distances between document vectors within a language correlate with the distances between their corresponding vectors in a parallel corpus. These findings provide clues about the possibility of reliable semantic knowledge transfer across language boundaries. Second, (Hassan and Mihalcea, 2009) propose a framework to compute semantic relatedness between two words in different languages, by considering Wikipedia articles in multiple languages. The method differs from the one proposed here, as we aggregate relatedness over monolingual spaces rather than measuring cross-lingual relatedness, and we do not specifically use the inter-wiki links between Wikipedia pages. 3 Measures of Text Relatedness In this work, we focus on corpus-based metrics because of their unsupervised nature, their flexibility, scalability, and portability to different languages. Specifically, we utilize three popu"
S12-1003,islam-inkpen-2006-second,0,0.0153712,"ven though taxonomies such as WordNet (Miller, 1995) are available in a number of languages1 , their coverage is still limited, and often times they are not publicly available. For these reasons, in multilingual settings, these measures often become untractable. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1991), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (HAL) (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words, and thus they can be easily transferred 1 http://www.illc.uva.nl/EuroWordNet/ 21 to a new language provided that a large corpus in that language is available. Multilingual natural language processing. Also relevant is the work done on multilingual text processing, which attempts"
S12-1003,P10-1026,0,0.065541,"Missing"
S12-1003,W09-3009,1,0.842165,"fying the strength of the semantic connection between textual units, be they words, sentences, or documents. For instance, one may want to determine how semantically related are two words such as car and automobile, or two pieces of text such as I love animals and I own a pet. It is one of the main tasks explored in the field of natural language processing, as it lies at the core of a large number of applications such as information retrieval (Ponte and Croft, 1998), query reformulation (Metzler et al., 2007; Yih and Meek, 2007; Sahami and Heilman, 2006; Broder et al., 2008), image retrieval (Leong and Mihalcea, 2009; Goodrum, 2000), plagiarism detection (Hoad and Zobel, 2003; Shivakumar and GarciaMolina, 1995; Broder et al., 1997; Heintze, 1996; Brin et al., 1995; Manber, 1994), information flow (Metzler et al., 2005), sponsored search (Broder et al., 2008), short answer grading (Mohler and Mihalcea, 2009a; Pulman and Sukkarieh, 2005; Mitchell et al., 2002), and textual entailment (Dagan et al., 2005). The typical approach to semantic relatedness is to either measure the distance between the constituent words by using a knowledge base such as WordNet or Roget (e.g., (Leacock and Chodorow, 1998; Lesk, 198"
S12-1003,O05-2006,0,0.0222163,"her measure the distance between the constituent words by using a knowledge base such as WordNet or Roget (e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Jarmasz and Szpakowicz, 2003; Pedersen et al., 2004)), or to calculate the similarity between the word distributions in very large corpora (e.g., (Landauer et al., 1991; Lin, 1998; Gabrilovich and Markovitch, 2007)). With almost no exception, these methods have been applied on one language at a time – English, most of the time, although measures of relatedness have also been explored on languages such as German (Zesch et al., 2007), Chinese (Li et al., 2005), Japanese (Kazama et al., 2010), and others. In this paper, we take a step further and explore a joint multilingual semantic relatedness metric, which aggregates semantic relatedness scores measured on several different languages. Specifically, in our method, in order to measure the relatedness of two textual units, we first determine their relatedness in multiple languages, and consequently infer a final relatedness score by averaging the scores calculated in the individual languages. Our hypothesis is that a multilingual representation can enrich the relatedness space and address relevant i"
S12-1003,E09-1065,1,0.899118,"main tasks explored in the field of natural language processing, as it lies at the core of a large number of applications such as information retrieval (Ponte and Croft, 1998), query reformulation (Metzler et al., 2007; Yih and Meek, 2007; Sahami and Heilman, 2006; Broder et al., 2008), image retrieval (Leong and Mihalcea, 2009; Goodrum, 2000), plagiarism detection (Hoad and Zobel, 2003; Shivakumar and GarciaMolina, 1995; Broder et al., 1997; Heintze, 1996; Brin et al., 1995; Manber, 1994), information flow (Metzler et al., 2005), sponsored search (Broder et al., 2008), short answer grading (Mohler and Mihalcea, 2009a; Pulman and Sukkarieh, 2005; Mitchell et al., 2002), and textual entailment (Dagan et al., 2005). The typical approach to semantic relatedness is to either measure the distance between the constituent words by using a knowledge base such as WordNet or Roget (e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Jarmasz and Szpakowicz, 2003; Pedersen et al., 2004)), or to calculate the similarity between the word distributions in very large corpora (e.g., (Landauer et al., 1991; Lin, 1998; Gabrilovich and Markovitch, 2007)). With almost no exception, these methods have been applied on one language a"
S12-1003,N04-3012,0,0.0279573,"ction (Hoad and Zobel, 2003; Shivakumar and GarciaMolina, 1995; Broder et al., 1997; Heintze, 1996; Brin et al., 1995; Manber, 1994), information flow (Metzler et al., 2005), sponsored search (Broder et al., 2008), short answer grading (Mohler and Mihalcea, 2009a; Pulman and Sukkarieh, 2005; Mitchell et al., 2002), and textual entailment (Dagan et al., 2005). The typical approach to semantic relatedness is to either measure the distance between the constituent words by using a knowledge base such as WordNet or Roget (e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Jarmasz and Szpakowicz, 2003; Pedersen et al., 2004)), or to calculate the similarity between the word distributions in very large corpora (e.g., (Landauer et al., 1991; Lin, 1998; Gabrilovich and Markovitch, 2007)). With almost no exception, these methods have been applied on one language at a time – English, most of the time, although measures of relatedness have also been explored on languages such as German (Zesch et al., 2007), Chinese (Li et al., 2005), Japanese (Kazama et al., 2010), and others. In this paper, we take a step further and explore a joint multilingual semantic relatedness metric, which aggregates semantic relatedness scores"
S12-1003,W05-0202,0,0.0141163,"field of natural language processing, as it lies at the core of a large number of applications such as information retrieval (Ponte and Croft, 1998), query reformulation (Metzler et al., 2007; Yih and Meek, 2007; Sahami and Heilman, 2006; Broder et al., 2008), image retrieval (Leong and Mihalcea, 2009; Goodrum, 2000), plagiarism detection (Hoad and Zobel, 2003; Shivakumar and GarciaMolina, 1995; Broder et al., 1997; Heintze, 1996; Brin et al., 1995; Manber, 1994), information flow (Metzler et al., 2005), sponsored search (Broder et al., 2008), short answer grading (Mohler and Mihalcea, 2009a; Pulman and Sukkarieh, 2005; Mitchell et al., 2002), and textual entailment (Dagan et al., 2005). The typical approach to semantic relatedness is to either measure the distance between the constituent words by using a knowledge base such as WordNet or Roget (e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Jarmasz and Szpakowicz, 2003; Pedersen et al., 2004)), or to calculate the similarity between the word distributions in very large corpora (e.g., (Landauer et al., 1991; Lin, 1998; Gabrilovich and Markovitch, 2007)). With almost no exception, these methods have been applied on one language at a time – English, most of t"
S12-1003,N07-2052,0,0.0605046,"Missing"
S12-1003,W07-1401,0,\N,Missing
S12-1003,P94-1019,0,\N,Missing
S12-1004,P10-1087,0,0.0708981,"Missing"
S12-1004,D09-1124,1,0.830767,"For instance, (Ponzetto and Strube, 2007) derived a large scale taxonomy from the existing Wikipedia. In related work, (de Melo and Weikum, 2010a) worked on a similar problem in which they combined all the existing multilingual Wikipedias to build a stable, large multilingual taxonomy. The interlingual links have also been used for cross-lingual information retrieval (Nguyen et al., 2009) or to generate bilingual parallel corpora (Mohammadi and QasemAghaee, 2010). (Ni et al., 2011) used multilingual editions of Wikipedia to mine topics for the task of cross lingual text classification, while (Hassan and Mihalcea, 2009) used Wikipedias in different languages to measure crosslingual semantic relatedness between concepts and texts in different languages. (Bharadwaj et al., 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages. They developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and abstracts, which they then be used to extract a bilingual dictionary. (Navigli and Ponzetto, 2010) explored the connections that can be drawn between Wikipedia and WordNet. While no attempts were made to complete"
S12-1004,P10-1023,0,0.0402266,"1) used multilingual editions of Wikipedia to mine topics for the task of cross lingual text classification, while (Hassan and Mihalcea, 2009) used Wikipedias in different languages to measure crosslingual semantic relatedness between concepts and texts in different languages. (Bharadwaj et al., 2010) explored the use of the multilingual links to mine dictionaries for under-resourced languages. They developed an iterative approach to construct a parallel corpus, using the interlingual links, info boxes, category pages, and abstracts, which they then be used to extract a bilingual dictionary. (Navigli and Ponzetto, 2010) explored the connections that can be drawn between Wikipedia and WordNet. While no attempts were made to complete the existing link structure of Wikipedia, the authors made use of machine translation to enrich the resource. The two previous works most closely related to ours are the systems introduced in (Sorg and Cimiano, 2008) and (de Melo and Weikum, 2010a; de Melo and Weikum, 2010b). (Sorg and Cimiano, 36 2008) designed a system that predicts new interlingual links by using a classification based approach. They extract certain types of links from bilingual Wikipedias, which are then used"
S12-1046,S12-1067,0,0.0303565,"yntactic complexity of documents that are similar to the given context, candidate length, and letter-wise recognizability of candidate as measured by a trigram LM. The first feature sets for co-training combines the syntactic complexity, character trigram LM and basic word length features, resulting in 29 features against the remaining 21. EMNLPCPH-ORD2: This is a variant of the EMNLPCPH-ORD1 system where the first feature set pools all syntactic complexity features and Wikipedia-based features (28 features) against all the remaining 22 features in the second group. SB-mmSystem: The approach (Amoia and Romanelli, 2012) builds on the baseline definition of simplicity using word frequencies but attempt at defining a more linguistically motivated notion of simplicity based on lexical semantics considerations. It adopts different strategies depending on the syntactic complexity of the substitute. For one-word substitutes or common collocations, the system uses its frequency from Wordnet as a metric. In the case of multi-words substitutes the system uses “relevance” rules that apply (de)compositional semantic criteria and attempts to identify a unique content word in the substitute that might better approximate"
S12-1046,P11-2087,0,0.397661,"iolation, but enormity should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. The SemEval-2012 share"
S12-1046,W11-2103,0,0.0278906,"varies from individual to individual, we carefully chose a group of annotators in an attempt to capture as much of a common notion of simplicity as possible. For practical reasons, we selected annotators with high proficiency levels in English as second language learners - all with a university first degree in different subjects. The Trial dataset was annotated by four people while the Test dataset was annotated by five people. In both cases each annotator tagged the complete dataset. Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al., 2011). This is also the primary evaluation metric for participating systems in the shared task, and it is covered in more detail in Section 4. The inter-annotator agreement was computed for each pair of annotators and averaged over all possible pairs for a final agreement score. On the Trial dataset, a kappa index of 0.386 was found, while for the Test dataset, a kappa index of 0.398 was found. It may be noted that certain annotators disagreed considerably with all others. For example, on the Test set, if annotations from one judge are removed, the average inter-annotator agreement rises to 0.443."
S12-1046,W09-2105,1,0.505463,"Missing"
S12-1046,S12-1066,1,0.513793,"tensive and nonfree resource such as the Web1T corpus makes a difference over other free and lightweight resources. UNT-SaLSA: The only resource SaLSA depends on is the Web1T data, and in particular only 3-grams from this corpus. It leverages the context provided with the dataset by replacing the target placeholder one by one with each of the substitutes and their inflections thus building sets of 3-grams for each substitute in a given instance. The score of any substitute is then the sum of the 3-gram frequencies of all the generated 3-grams for that substitute. UOW-SHEF-SimpLex: The system (Jauhar and Specia, 2012) uses a linear weighted ranking function composed of three features to produce a ranking. These include a context sensitive n-gram frequency model, a bag-of-words model and a feature composed of simplicity oriented psycholinguistic features. These three features are combined using an SVM ranker that is trained and tuned on the Trial dataset. 6.2 Pairwise kappa The official task results and the ranking of the systems are shown in Table 3. Firstly, it is worthwhile to note that all the top ranking systems include features that use frequency as a surrogate measure for lexical simplicity. This ind"
S12-1046,S12-1068,0,0.0205478,"he other hand, performs very strongly, in spite of its simplistic approach, which is entirely agnostic to context. In fact it surpasses the average inter-annotator agreement on both Trial and Test datasets. Indeed, the scores on the Test set approach the best inter-annotator agreement scores between any two annotators. L-Sub Gold Random Simple Freq. Trial 0.050 0.016 0.397 Test 0.106 0.012 0.471 Table 2: Baseline kappa scores on trial and test sets 6 6.1 Results and Discussion Participants Five sites submitted one or more systems to the task, totaling nine systems: ANNLOR-lmbing: This system (Ligozat et al., 2012) relies on language models probabilities, and builds on the principle of the Simple Frequency baseline. While the baseline uses Google n-grams to rank substitutes, this approach uses Microsoft Web n-grams in the same way. Additionally characteristics, such as the contexts of each term to be substituted, were integrated into the system. Microsoft Web N-gram Service was used to obtain log likelihood probabilities for text units, composed of the lexical item and 4 words to the left and right from the surrounding context. ANNLOR-simple: The system (Ligozat et al., 2012) is based on Simple English"
S12-1046,S07-1009,0,0.309906,"regardless of its context. 1 Introduction Lexical Simplification is a subtask of Text Simplification (Siddharthan, 2006) concerned with replacing words or short phrases by simpler variants in a context aware fashion (generally synonyms), which can be understood by a wider range of readers. It generally envisages a certain human target audience that may find it difficult or impossible to understand complex words or phrases, e.g., children, people with poor literacy levels or cognitive disabilities, or second language learners. It is similar in many respects to the task of Lexical Substitution (McCarthy and Navigli, 2007) in that it involves determining adequate substitutes in context, but in this case on the basis of a predefined criterion: simplicity. As an example take the sentence: “Hitler committed terrible atrocities during the second World War.” The system would first identify complex words, e.g. atrocities, then search for substitutes that might adequately replace it. A thesaurus lookup would yield the following synonyms: abomination, cruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these su"
S12-1046,S12-1069,0,0.0648761,"requency from Wordnet as a metric. In the case of multi-words substitutes the system uses “relevance” rules that apply (de)compositional semantic criteria and attempts to identify a unique content word in the substitute that might better approximate the whole expression. The expression is then assigned the frequency associated to this content word for the ranking. After POS tagging and sense disambiguating all substitutes, hand-written rules are used to decompose the meaning of a complex phrase and identify the most relevant word conveying the semantics of the whole. UNT-SimpRank: The system (Sinha, 2012) uses external resources, including the Simple English Wikipedia corpus, a set of Spoken English dialogues, transcribed into machine readable form, WordNet, and unigram frequencies (Google Web1T data). SimpRank scores each substitute by a sum of its unigram frequency, its Rank 1 frequency in the Simple English Wikipedia, its frequency in the spoken corpus, the inverse of its length, and the number of senses the substitute has in WordNet. For a given context, the substitutes are then reverse-ranked based on their simplicity scores. UNT-SimpRankLight: This is a variant of SimpRank which does not"
S12-1046,N10-1056,0,0.284082,"ruelty, enormity and violation, but enormity should be dropped as it does not fit the context appropriately. Finally, the system would determine the simplest of these substitutes, e.g., cruelty, and use it to replace the complex word, yielding the sentence: “Hitler committed terrible cruelties during the second World War.”. Different from other subtasks of Text Simplification like Syntactic Simplification, which have been relatively well studied, Lexical Simplification has received less attention. Although a few recent attempts explicitly address dependency on context (de Belder et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Specia, 2010), most approaches are contextindependent (Candido et al., 2009; Devlin and Tait, 1998). In addition, a general deeper understanding 347 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347–355, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of the problem is yet to be gained. As a first attempt to address this problem in the shape of a shared task, the English Simplification task at SemEval-2012 focuses on the third component, which we believe is the core of the Lexical Simplification problem. Th"
S12-1094,S12-1051,0,0.057714,"r data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), or Salient Semantic Analysis (Hassan and Mihalcea, 2011). In this paper, we describe the system with which 635 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635–642, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics we participated in the S EMEVAL 2012 task on semantic text similarity (Agirre et al., 2012). The system builds upon our earlier work on corpus-based and knowledge-based methods of text semantic similarity (Mihalcea et al., 2006; Hassan and Mihalcea, 2011; Mohler et al., 2011), and combines all these previous methods into a meta-system by using machine learning. The framework provided by the task organizers also enabled us to perform an indepth analysis of the various components used in our system, and draw conclusions concerning the role played by the different resources, features, and algorithms in building a state-of-the-art semantic text similarity system. 2 Related Work Over the"
S12-1094,J90-1003,0,0.109517,"wledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 636"
S12-1094,W02-1001,0,0.0151093,"algorithm. We define a total of 64 features5 to be used to train a machine learning system to compute subgraphsubgraph similarity. Of these, 32 are based upon the bag-of-words semantic similarity of the subgraphs using the metrics described in Section 3.3.1 as well as a Wikipedia-trained LSA model. The remaining 32 features are lexico-syntactic features associated with the parent nodes of the subgraphs and are described in more detail in our earlier paper. We then calculate weights associated with these features using an averaged version of the perceptron algorithm (Freund and Schapire, 1999; Collins, 2002) trained on a set of 32 manually annotated instructor/student answer pairs selected from the short-answer grading corpus (MM2011). These pairs contain 7303 node pairs (656 matches, 6647 non-matches). Once the weights are calculated, a similarity score for each pair of nodes can be computed by taking the dot product of the feature vector with the weights. In the second stage, the node similarity scores calculated in the previous step are used to find an optimal alignment for the pair of dependency graphs. We begin with a bipartite graph where each node in one graph is represented by a node on t"
S12-1094,islam-inkpen-2006-second,0,0.0187451,"lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 636 3 Semantic Textual Similarity System The system we proposed for th"
S12-1094,O97-1002,0,0.890282,"also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as Latent Semantic Analysis (Landauer et al., 1997), Explicit Semantic A"
S12-1094,N03-1020,0,0.0161119,"s of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea"
S12-1094,E09-1065,1,0.872766,"proposed by Lesk (1986) as a solution for word sense disambiguation. The Wu & Palmer (Wu and Palmer, 1994) (W U P ) similarity metric measures the depth of two given concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: Simwup = 2 ∗ depth(LCS) depth(concept1 ) + depth(concept2 ) (3) The measure introduced by Resnik (Resnik, 1995) (RES) returns the information content (IC) of the LCS of two concepts: Simres = IC(LCS) 3.3.1 (1) (4) where IC is defined as: Following prior work from our group (Mihalcea et al., 2006; Mohler and Mihalcea, 2009), we employ several WordNet-based similarity metrics for the task of sentence-level similarity. Briefly, for each open-class word in one of the input texts, we compute the maximum semantic similarity (using the WordNet::Similarity package (Pedersen et al., 2004)) that can be obtained by pairing it with any open-class word in the other input text. All the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts. We provide below a short description for each of the similarity metrics employed by this 637 IC(c) = − log P (c) (5) and P (c)"
S12-1094,J98-1004,0,0.218088,"the individual parts. 1 Introduction Measures of text similarity have been used for a long time in applications in natural language processing and related areas. One of the earliest applications of text similarity is perhaps the vectorspace model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and more recently for extractive summarization (Salton et al., 1997), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). Measures of text similarity were also found useful for the evaluation of text coherence (Lapata and Barzilay, 2005). More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for en"
S12-1094,P02-1040,0,\N,Missing
S12-1094,P11-1076,1,\N,Missing
S12-1094,P94-1019,0,\N,Missing
S13-1003,E06-1002,1,0.726372,"duction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to Since many words and names mentioned in Wikipedia articles are inherently ambiguous, their corresponding links can be seen as a useful source of supervision for training named entity and word sense disambiguation systems. For example, Wikipedia con"
S13-1003,D07-1074,0,0.0926912,"e vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to Since many words and names mentioned in Wikipedia articles are inherently ambiguous, their corresponding links can be seen as a useful source of supervision for training named entity and word sense disambiguation systems. For example, Wikipedia contains articles th"
S13-1003,D09-1120,0,0.0264316,"hed explicitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Rada Mihalcea Department of CSE University of North Texas Denton, TX 76203, USA rada@cs.unt.edu Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding"
S13-1003,P08-4009,0,0.0298522,"tion ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Rada Mihalcea Department of CSE University of North Texas Denton, TX 76203, USA rada@cs.unt.edu Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to Since many words and names mentioned in Wikipedi"
S13-1003,N07-1025,1,0.865824,"ikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors mention an existing Wikipedia entity inside an article, they are required to link at least its first mention to Since many words and names mentioned in Wikipedia articles are inherently ambiguous, their corresponding links can be seen as a useful source of supervision for training named entity and word sense disambiguation systems. For example, Wikipedia contains articles that describe possible senses of the word “capit"
S13-1003,P96-1006,0,0.248372,"ation 4. The two classifiers g and f are trained using SVMs and a linear kernel. Platt scaling is used with the first classifier to obtain the probability estimates g(x) = p(s = 1|x), which are then converted into weights following Equations 4 and 5, and used during the training of the second classifier. 4 Experimental Evaluation We ran disambiguation experiments on the 6 ambiguous words atmosphere, president, dollar, game, diamond and Corinth. The corresponding Wikipedia sense repositories have been summarized in Tables 1 and 2. All WSD classifiers used the same set of standard WSD features (Ng and Lee, 1996; Stevenson and Wilks, 2001), such as words and their part-ofspeech tags in a window of 3 words around the ambiguous keyword, the unigram and bigram content words that are within 2 sentences of the current sentence, the syntactic governor of the keyword, and its chains of syntactic dependencies of lengths up to two. Furthermore, for each example, a Wikipedia 28 specific feature was computed as the cosine similarity between the context of the ambiguous word and the text of the article for the target sense or reference. The Level1 and Level3 classifiers were trained using the SVMmulti component"
S13-1003,P10-1154,0,0.398656,"Missing"
S13-1003,N06-1025,0,0.0395786,"s, without being distinguished explicitly. We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Rada Mihalcea Department of CSE University of North Texas Denton, TX 76203, USA rada@cs.unt.edu Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that al"
S13-1003,P11-1082,0,0.0206578,"ining coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies. Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation. 1 Rada Mihalcea Department of CSE University of North Texas Denton, TX 76203, USA rada@cs.unt.edu Introduction and Motivation The vast amount of world knowledge available in Wikipedia has been shown to benefit many types of text processing tasks, such as coreference resolution (Ponzetto and Strube, 2006; Haghighi and Klein, 2009; Bryl et al., 2010; Rahman and Ng, 2011), information retrieval (Milne, 2007; Li et al., 2007; Potthast et al., 2008; Cimiano et al., 2009), or question answering (Ahn et al., 2004; Kaisser, 2008; Ferrucci et al., 2010). In particular, the user contributed link structure of Wikipedia has been shown to provide useful supervision for training named entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007) and word sense disambiguation (Mihalcea, 2007; Ponzetto and Navigli, 2010) systems. Articles in Wikipedia often contain mentions of concepts or entities that already have a corresponding article. When contributing authors menti"
S13-1003,J01-3001,0,0.0607857,"lassifiers g and f are trained using SVMs and a linear kernel. Platt scaling is used with the first classifier to obtain the probability estimates g(x) = p(s = 1|x), which are then converted into weights following Equations 4 and 5, and used during the training of the second classifier. 4 Experimental Evaluation We ran disambiguation experiments on the 6 ambiguous words atmosphere, president, dollar, game, diamond and Corinth. The corresponding Wikipedia sense repositories have been summarized in Tables 1 and 2. All WSD classifiers used the same set of standard WSD features (Ng and Lee, 1996; Stevenson and Wilks, 2001), such as words and their part-ofspeech tags in a window of 3 words around the ambiguous keyword, the unigram and bigram content words that are within 2 sentences of the current sentence, the syntactic governor of the keyword, and its chains of syntactic dependencies of lengths up to two. Furthermore, for each example, a Wikipedia 28 specific feature was computed as the cosine similarity between the context of the ambiguous word and the text of the article for the target sense or reference. The Level1 and Level3 classifiers were trained using the SVMmulti component of the SVMlight package.1 Th"
S13-1032,S12-1051,0,0.042451,"verage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training. 3 Semantic Textual Similarity System 3.1 Task Setup The STS task consists of labeling one sentence pair at a time, based on the semantic similarity existent between its two component sentences. Human assigned similarity scores range from 0 (no relation) to 5 (semantivally equivalent). The *S EM 2013 STS task did not provide additional labeled data to the training and testing sets released as part of the STS task hosted at S EM E VAL 2012 (Agirre et al., 2012); our system variations were trained on S EM E VAL 2012 data. The test sets (Agirre et al., 2013) consist of text pairs extracted from headlines (headlines, 750 pairs), sense definitions from WordNet and OntoNotes (OnWN, 561 pairs), sense definitions from WordNet and FrameNet (FNWN, 189 pairs), and data used in the evaluation of machine translation systems (SMT, 750 pairs). 3.2 Resources Various subparts of our framework use several resources that are described in more detail below. Wikipedia1 is the most comprehensive encyclopedia to date, and it is an open collaborative effort hosted on-line"
S13-1032,S12-1094,1,0.46486,"Missing"
S13-1032,J90-1003,0,0.122907,"wledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. We explore several of these measures in depth in Section 3.3.1. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al., 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam 222 and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledgebased methods, which suffer from limited coverage, corpus-based measures are able to induce a similarity between any given two words, as long as they appear in the very large corpus used as training."
S13-1032,islam-inkpen-2006-second,0,0.0423081,"Missing"
S13-1032,O97-1002,0,0.820124,"he given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and extractive summarization (Salton et al., 1997), in the automatic evaluation of machine translation (Papineni et al., 2002), ∗ † Google Inc. Mountain View, CA √ Language Computer Corp. Richardson, TX Abstract 1 § carmen.banea@gmail.com rada@cs.unt.edu More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as latent semantic analysis (Landauer et al., 1997), explicit semantic a"
S13-1032,N03-1020,0,0.295196,"Missing"
S13-1032,E09-1065,1,0.829997,"e relationships are employed by various knowledge-based methods to derive semantic similarity. The MPQA corpus (Wiebe and Riloff, 2005) is a newswire data set that was manually annotated at the expression level for opinion-related content. Some of the features derived by our opinion extraction models were based on training on this corpus. 3.3 Features Our system variations derive the similarity score of a given sentence-pair by integrating information from knowledge, corpus, and opinion-based sources3 . 3.3.1 Knowledge-Based Features Following prior work from our group (Mihalcea et al., 2006; Mohler and Mihalcea, 2009), we employ several WordNet-based similarity metrics for the task of sentence-level similarity. Briefly, for each open-class word in one of the input texts, we compute the maximum semantic similarity4 that can be obtained by pairing it with any open-class word in the other input text. All the word-to-word similarity scores obtained in this way are summed and normalized to the length of the two input texts. We provide below a short description for each of the similarity metrics employed by this system. The shortest path (P ath) similarity is equal to: Simpath = 1 length length 2∗D Simwup = 2 ∗"
S13-1032,P11-1076,1,0.829995,"X Abstract 1 § carmen.banea@gmail.com rada@cs.unt.edu More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score (Mihalcea et al., 2006; Islam and Inkpen, 2009; Hassan and Mihalcea, 2011; Mohler et al., 2011), or data-driven methods that perform component-wise additions of semantic vector representations as obtained with corpus measures such as latent semantic analysis (Landauer et al., 1997), explicit semantic analysis (Gabrilovich and Markovitch, 2007), or salient semantic analysis 221 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 221–228, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics (Hassan and Mihalcea, 2011). In this paper, we describe the system variations"
S13-1032,P02-1040,0,0.0966848,"ge processing and related areas. One of the earliest applications of text similarity is perhaps the vector-space model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their angular distance with the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and extractive summarization (Salton et al., 1997), in the automatic evaluation of machine translation (Papineni et al., 2002), ∗ † Google Inc. Mountain View, CA √ Language Computer Corp. Richardson, TX Abstract 1 § carmen.banea@gmail.com rada@cs.unt.edu More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction mainly consist of either bipartite-graph matching strategies that aggregate word-to-word similarity into a text similarity score"
S13-1032,J98-1004,0,0.0156291,"o identify it. Introduction Measures of text similarity have been used for a long time in applications in natural language processing and related areas. One of the earliest applications of text similarity is perhaps the vector-space model used in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their angular distance with the given query (Salton and Lesk, 1971). Text similarity has also been used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), and extractive summarization (Salton et al., 1997), in the automatic evaluation of machine translation (Papineni et al., 2002), ∗ † Google Inc. Mountain View, CA √ Language Computer Corp. Richardson, TX Abstract 1 § carmen.banea@gmail.com rada@cs.unt.edu More recently, researchers have started to consider the possibility of combining the large number of word-to-word semantic similarity measures (e.g., (Jiang and Conrath, 1997; Leacock and Chodorow, 1998; Lin, 1998; Resnik, 1995)) within a semantic similarity method that works for entire texts. The methods proposed to date in this direction m"
S13-1032,H05-2018,1,0.725452,"ating a list ϕ which holds the strongest semantic pairings between the fragments’ terms, such that each term can only belong to one and only one pair. Sim(Ta , Tb ) = (ω + P|ϕ| ϕi ) × (2ab) a+b i=1 (8) where ϕi is the similarity score for the ith pairing. 3.3.3 Opinion Aware Features We design opinion-aware features to capture sentence similarity on the subjectivity level based on the output of three subjectivity analysis systems. Intuitively, two sentences are similar in terms of subjectivity if there exists similar opinion expressions which also share similar opinion holders. OpinionFinder (Wilson et al., 2005) is a publicly available opinion extraction model that annotates the subjectivity of new text based on the presence (or absence) of words or phrases in a large lexicon. The system consists of a two step process, by feeding the sentences identified as subjective or objective by a rule-based high-precision classifier to a highrecall classifier that iteratively learns from the remaining corpus. For each sentence in a STS pair, the two classifiers provide two predictions; a subjectivity similarity score (SUBJSL) is computed as follows. If both sentences are classified as subjective or objective, t"
S13-1032,D12-1122,1,0.841706,"er. We first record how many expressions the two sentences have: feature NUMEX1 and NUMEX2. Then we compare how many tokens these expressions share and we normalize by the total number of expressions (feature EXPR). We compute the difference between the probabilities of the two sentences being subjective (SUBJDIFF), by employing a logistic regression classifier using LIBLINEAR (Fan et al., 2008) trained on the MPQA corpus. The smaller the difference, the more similar the sentences are in terms of subjectivity. We also employ features produced by the opinionextraction model of Yang and Cardie (Yang and Cardie, 2012), which is better suited to process expressions of arbitrary length. Specifically, for each sentence, we extract subjective expressions and generate the following features. SUBJCNT is a binary feature which is equal to 1 if both sentences contain a subjective expression. DSEALGN marks the number of shared words between subjective expressions in two sentences, while DSESIM represents their similarity beyond the word level. We represent the subjective expressions in each sentence as a feature vector, containing unigrams extracted from the expressions, their part-of-speech, their WordNet hypernym"
S13-1032,P94-1019,0,\N,Missing
S14-2010,S14-2085,0,0.0392393,"Missing"
S14-2010,S14-2069,0,0.0326403,"Missing"
S14-2010,S14-2128,0,0.0328229,"Missing"
S14-2010,P13-1024,1,0.0618966,"data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the news title, while the other one represents a Twitter comment on that particular news. They are evenly sampled from string similarity values between 0.5 and 1. Table 1 shows the explanations and values associated with each score between 5 and 0. As in prior years, we used Amazon Mechanical Turk (AMT)3 to crowdsource the annotation of the English pairs.4 Annotators are presented with the Table 2: English subtask: Summary of train (2012 and 2013) and test (2014) datasets. a DARPA sponsored workshop at Columbia University.1 In 2013, STS wa"
S14-2010,S14-2112,0,0.0317369,"Missing"
S14-2010,N06-2015,0,0.0715746,"ferent similarity ranges, hence we built two sets of headline pairs: (i) a set where the pairs come from the same EMM cluster, (ii) and another set where the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs"
S14-2010,S12-1051,1,0.623306,"r as both tasks have been defined to date in the literature) in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for a myriad of NLP tasks such as MT evaluation, information extraction, question answering, summarization, etc. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline map"
S14-2010,S14-2131,0,0.0336314,"Missing"
S14-2010,S14-2072,0,0.0817436,"Missing"
S14-2010,Q14-1018,0,0.0731,"Missing"
S14-2010,S14-2039,0,0.0993932,"Missing"
S14-2010,S14-2078,0,0.101731,"Missing"
S14-2010,S14-2022,0,0.0306687,"Missing"
S14-2010,S14-2046,0,0.022257,"Missing"
S14-2010,D13-1179,0,0.0175763,"Missing"
S14-2010,S12-1060,0,0.0199826,"Missing"
S14-2010,S14-2093,0,0.0281526,"Missing"
S14-2010,W10-0721,0,0.050451,"d 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the new"
S14-2010,W10-0707,0,\N,Missing
S14-2010,P94-1019,0,\N,Missing
S14-2010,Q14-1017,0,\N,Missing
S14-2010,S14-2138,0,\N,Missing
S14-2098,O97-1002,0,0.055036,"Abstract 2 Related Work Over the past years, the research community has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. T"
S14-2098,S12-1051,0,0.0213281,"l language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (Jurgens et al., 2014). 3 ∗ {carmennb,chenditc,mihalcea}@umich.edu This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 3.1 System De"
S14-2098,S14-2003,0,0.0210175,"), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (Jurgens et al., 2014). 3 ∗ {carmennb,chenditc,mihalcea}@umich.edu This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 3.1 System Description Generic Features Our system employs both knowledge and corpusbased measures as detailed below. 560 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 560–565, Dublin, Ireland, August 23-24, 2014. Knowledge-based features Knowledge-based metrics were shown to provide high correlat"
S14-2098,P13-4021,0,0.0236756,"Missing"
S14-2098,N03-1020,0,0.0569777,"ovel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Janyce Wiebe University of Pittsburgh Pittsburgh, PA Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this l"
S14-2098,J90-1003,0,0.0840197,"rm distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. This article pres"
S14-2098,D07-1061,0,0.00911978,"ty has focused on computing semantic relatedness using methods that are either knowledge-based or corpus-based. Knowledge-based methods derive a measure of relatedness by utilizing lexical resources and ontologies such as WordNet (Miller, 1995) or Roget (Rog, 1995) to measure definitional overlap, term distance within a graphical taxonomy, or term depth in the taxonomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual inf"
S14-2098,N13-1090,0,0.00492088,"and obtain metrics W T V 1 (by applying Align) and W T V 2 (using VectorSum). paragraph2sentence. At this level, due to the long context that entails one-to-many mappings between the words in the sentence and those in the paragraph, we use a text clustering technique prior to calculating the features’ weights. Corpus based features Our corpus based features are derived from a deep learning vector space model that is able to “understand” word meaning without human input. Distributed word embeddings are learned using a skip-gram recurrent neural net architecture running over a large raw corpus (Mikolov et al., 2013b; Mikolov et al., 2013a). A primary advantage of such a model is that, by breaking away from the typical n-gram model that sees individual units with no relationship to each other, it is able to generalize and produce word vectors that are similar for related words, thus encoding linguistic regularities and patterns (Mikolov et al., 2013b). For example, vec(Madrid)-vec(Spain)+vec(France) is closer to vec(Paris) than any other word vector (Mikolov et al., 2013a). We used the pretrained Google News word2vec model (W T V ) built over a 100 billion words corpus, and containing 3 million 300-dimen"
S14-2098,islam-inkpen-2006-second,0,0.0105473,"nomy as a measure of specificity. There are many knowledge-based measures that were proposed in the past, e.g., (Leacock and Chodorow, 1998; Lesk, 1986; Resnik, 1995; Jiang and Conrath, 1997; Lin, 1998; Jarmasz and Szpakowicz, 2003; Hughes and Ramage, 2007). On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islam and Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al., 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. They consist of unsupervised methods that utilize the contextual information and patterns observed in raw text to build semantic profiles of words. Unlike knowledge-based methods, which suffer from limited coverage, corpus-based measures are able to induce the similarity between any two words, as long as they appear in the corpus used for training. This article presents our team’s participating system at SemEval-2014 Task 3. Using"
S14-2098,P02-1040,0,0.0914881,"with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Janyce Wiebe University of Pittsburgh Pittsburgh, PA Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEval Cross-level semantic similarity task is aimed at this latter scenario, and is described in more details in the task paper (J"
S14-2098,J98-1004,0,0.0351393,"ith traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems. 1 Janyce Wiebe University of Pittsburgh Pittsburgh, PA Introduction Semantic textual similarity is one of the key components behind a multitude of natural language processing applications, such as information retrieval (Salton and Lesk, 1971), relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986; Schutze, 1998), summarization (Salton et al., 1997; Lin and Hovy, 2003), automatic evaluation of machine translation (Papineni et al., 2002), plagiarism detection (Nawab et al., 2011), and more. To date, semantic similarity research has primarily focused on comparing text snippets of similar length (see the semantic textual similarity tasks organized during *Sem 2013 (Agirre et al., 2013) and SemEval 2012 (Agirre et al., 2012)). Yet, as new challenges emerge, such as augmenting a knowledge-base with textual evidence, assessing similarity across different context granularities is gaining traction. The SemEva"
S14-2098,S13-1004,0,\N,Missing
S15-2045,agerri-etal-2014-ixa,1,0.57453,"onal and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple regular expressions. Additionally, this step collects chunk regions coming either from gold standard or from the chunking done by ixa-pipes-chunk (Agerri et al., 2014). This is followed by a lowercased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus ena"
S15-2045,S12-1051,1,0.519741,"TS also differs from both TE and paraphrasing (in as far as both tasks have been defined to date in the literature) in that rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for many NLP tasks such as MT evaluation, information extraction, question answering, summarization. In 2012, we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success (Agirre et al., 2012). In addition, we 252 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity"
S15-2045,S14-2010,1,0.800447,"f the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity in a new language, namely Spanish (Agirre et al., 2014). This year we presented three subtasks: the English subtask, the Spanish subtask and the interpretable pilot subtask. The English subtask comprised pairs from headlines and image descriptions, and it also introduced new genres, including answer pairs from a tutorial dialogue system and from Q&A websites, and pairs from a dataset tagged with committed belief annotations. For the Spanish subtask, additional pairs from news and Wikipedia articles were selected. The annotations for both tasks leveraged crowdsourcing. Finally, with the interpretable STS pilot subtask, we wanted to start exploring"
S15-2045,P14-1023,0,0.0115737,"7070 0.7251 0.7311 0.7250 0.7422 0.6364 0.7775 0.7032 0.7130 0.7189 0.4616 0.7533 0.6111 0.5379 0.5424 0.5672 0.6558 0.4919 0.5912 0.6964 0.7114 0.6364 Table 3: Task 2a: English evaluation results in terms of Pearson correlation. 259 Rank 61 42 29 63 56 57 70 69 71 62 44 49 43 74 72 73 34 28 26 1 3 5 19 18 16 8 9 2 12 13 23 41 59 20 47 22 11 15 33 45 55 24 10 17 50 51 52 4 7 6 46 36 27 39 31 30 32 25 53 14 40 37 35 68 21 58 66 65 64 48 67 60 42 38 54 approach for the top three participants (DLS@CU, ExBThemis, Samsung). They use WordNet (Miller, 1995), Mikolov Embeddings (Mikolov et al., 2013; Baroni et al., 2014) and PPDB (Ganitkevitch et al., 2013). In general, generic NLP tools such as lemmatization, PoS tagging, distributional word embeddings, distributional and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple"
S15-2045,N13-1092,0,0.0796576,"the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,P13-2080,0,0.0179598,"Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,W10-0721,0,0.0157602,"ns student answers Q&A forum answers commited belief Table 2: English subtask: Summary of train (2012, 2013, 2014) and test (2015) datasets. lines come from a different EMM cluster. Then, we computed the string similarity between those pairs. Accordingly, we sampled 1000 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity as a metric. We sampled another 1000 pairs from the different EMM cluster in the same manner. The Images dataset is a subset of the PASCAL VOC-2008 dataset (Rashtchian et al., 2010), which consists of 1000 images with around 10 descriptions each, and has been used by a number of image description systems. It was also sampled using string similarity, discarding those that had been used in previous years. We organized two bins with 1000 pairs each: one with pairs of descriptions from the same image, and the other one with pairs of descriptions from different images. The source of the Answers-student pairs is the BEETLE corpus (Dzikovska et al., 2010), which is a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system."
S15-2045,W00-0726,0,0.313421,"Missing"
S15-2045,S12-1060,0,0.211502,"Missing"
S15-2045,W10-0707,0,\N,Missing
S16-1081,S16-1103,0,0.0432732,"n of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The nex"
S16-1081,S12-1051,1,0.454212,"for replicating human judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translat"
S16-1081,S13-1004,1,0.536348,"n judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techni"
S16-1081,S14-2010,1,0.564132,"g the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fl"
S16-1081,S16-1101,1,0.859309,"Missing"
S16-1081,S16-1086,0,0.0343004,"Missing"
S16-1081,P98-1013,0,0.0704238,"E498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translation"
S16-1081,S16-1117,0,0.0317832,"Missing"
S16-1081,S16-1089,0,0.463664,"STS. The overall winner, Samsung Poland NLP Team, proposes a textual similarity model that is a novel hybrid of recursive auto-encoders from deep learning with penalty and reward signals extracted from WordNet (Rychalska et al., 2016). To obtain even better performance, this model is combined in an ensemble with a number of other similarity models including a version of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structure"
S16-1081,D15-1181,0,0.0283646,"nik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased approach for textual similarity that incorporates word alignment information (Itoh, 2016). 6"
S16-1081,S16-1170,0,0.023392,"s such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased"
S16-1081,N06-2015,0,0.0113871,"ual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translations. We also introduce an evaluat"
S16-1081,S16-1106,0,0.0956972,"ween the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time systems without corrections are: ALL 0.68923; plagiarism 0.78949; answer-answer 0.48018; postediting 0.81241; headlines 0.76439; question-question 0.57140. Team Run ALL Ans.-Ans. HDL Plagiarism Postediting Ques.-Ques. Samsung Poland NLP Team UWB MayoNLPTeam Samsung Poland NLP Team NaCTeM ECNU UMD-TTIC"
S16-1081,P14-2124,0,0.0245172,"roximately 0.25 drop in correlation on the news data as compare to the multi-source setting; 2) systems performing evenly on both data sets. 6.5.1 Methods In terms of approaches, most runs rely on a monolingual framework. They automatically translate the Spanish member of a sentence pair into English and then compute monolingual semantic similarity using a system developed for English. In contrast, the CNRC team (Lo et al., 2016) provides a true crosslingual system that makes use of embedding space phrase similarity, the score from XMEANT, a crosslingual machine translation evaluation metric (Lo et al., 2014), and precision and recall features for material filling aligned cross-lingual semantic roles (e.g., action, agent, patient). The FBK HLT team (Ataman et al., 2016) proposes a model combining cross-lingual word embeddings with features from QuEst (Specia et al., 2013), a tool for machine translation quality estimation. The RTM system (Bic¸ici, 2016) also builds on methods developed for machine translation quality estimation and is applicable to both cross-lingual and monolingual similarity. The GWU NLP team (Aldarmaki and Diab, 2016) uses a shared cross-lingual vector space to directly assess"
S16-1081,S16-1102,0,0.0363344,"Missing"
S16-1081,P14-5010,0,0.0120217,"data sources we use for the evaluation sets. 3.1.1 Selection Heuristics Unless otherwise noted, pairs are heuristically selected using a combination of lexical surface form and word embedding similarity between a candidate pair of text snippets. The heuristics are used to find pairs sharing some minimal level of either surface or embedding space similarity. An approximately equal number of candidate sentence pairs are produced using our lexical surface form and word embedding selection heuristics. Both heuristics make use of a Penn Treebank style tokenization of the text provided by CoreNLP (Manning et al., 2014). 500 year 2016 2016 2016 dataset Trial News Multi-source pairs 103 301 294 source Sampled ≤ 2015 STS en-es news articles en news headlines, short-answer plag., MT postedits, Q&A forum answers, Q&A forum questions Table 3: Spanish-English subtask: Trial and test data sets. Surface Lexical Similarity Our surface form selection heuristic uses an information theoretic measure based on unigram overlap (Lin, 1998). As shown in equation (1), surface level lexical similarity between two snippets s1 and s2 is computed as a log probability weighted sum of the words common to both snippets divided by a"
S16-1081,D14-1162,0,0.109685,"Missing"
S16-1081,S16-1093,0,0.0144173,"ased model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engin"
S16-1081,S16-1091,0,0.0291873,"representations of the two snippets. 6.4 English Subtask The rankings for the English STS subtask are given in Tables 4 and 5. The baseline system ranked 100th. Table 6 provides the best and median scores for each of the individual evaluation sets as well as overall.15 The table also provides the difference between the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time sy"
S16-1081,P13-4014,0,0.0272301,"Missing"
S16-1081,2011.eamt-1.12,0,0.00782041,"swers. This corpus provides a collection of short answers to computer science questions that exhibit varying degrees of plagiarism from related Wikipedia articles.4 The short answers include text that was constructed by each of the following four strategies: 1) copying and pasting individual sentences from Wikipedia; 2) light revision of material copied from Wikipedia; 3) heavy revision of material from Wikipedia; 4) non-plagiarised answers produced without even looking at Wikipedia. This corpus is segmented into individual sentences using CoreNLP (Manning et al., 2014). 3.1.4 Postediting The Specia (2011) EAMT 2011 corpus provides machine translations of French news data using the Moses machine translation system (Koehn et al., 2007) paired with postedited corrections of those translations.5 The corrections were provided by human translators instructed to perform the minimum useful for finding semantically similar text snippets that differ in surface form. 4 Questions: A. What is inheritance in object orientated programming?, B. Explain the PageRank algorithm that is used by the Google search engine, C. Explain the Vector Space Model that is used for Information Retrieval., D. Explain Bayes Th"
S16-1081,S15-2027,0,0.0111195,"r to be significantly worse than the monolingual submissions even though the systems are being asked to perform the more challenging problem of evaluating crosslingual sentence pairs. While the correlations are not directly comparable, they do seem to motivate a more direct comparison between cross-lingual and monolingual STS systems. In terms of performance on the manually culled news data set, the highest overall rank is achieved by an unsupervised system submitted by team UWB (Brychcin and Svoboda, 2016). The unsupervised UWB system builds on the word alignment based STS method proposed by Sultan et al. (2015). However, when calculating the final similarity score, it weights both the aligned and unaligned words by their inverse document frequency. This system is able to attain a 0.912 correlation on the news data, while ranking second on the multi-source data set. For the multi-source test set, the highest scoring submission is a supervised system from the UWB team that combines multiple signals originating from lexical, syntactic and semantic similarity approaches in a regression-based model, achieving a 0.819 correlation. This is modestly better than the second place unsupervised approach that ac"
S16-1081,S16-1094,0,0.00995156,"th a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without"
S16-1081,C98-1013,0,\N,Missing
S16-1081,P07-2045,0,\N,Missing
S19-1005,S17-2001,0,0.0323139,"Missing"
S19-1005,S13-1018,0,0.180431,"Human Activity Phrase (Wilson and Mihalcea, 2017): a collection of pairs of phrases regarding human activities, annotated with the following four different relations. • Similarity (SIM): The degree to which the two activity phrases describe the same thing, semantic similarity in a strict sense. Example of high similarity phrases: to watch a film and to see a movie. • Relatedness (REL): The degree to which the activities are related to one another, a general semantic association between two phrases. Example of strongly related phrases: to give a gift and to receive a present. Typed-Similarity (Agirre et al., 2013b): a collection of meta-data describing books, paintings, films, museum objects and archival records taken from Europeana,4 presented as the pilot track in the SemEval 2013 STS shared task. Typically, the items consist of title, subject, description, and so on, describing a cultural heritage item and, sometimes, a thumbnail of the item itself. For the purpose of measuring semantic similarity, we concatenate all the textual entries such as title, creator, subject and description into a short paragraph that is used as input, although the annotations might be informed of the image aspects of the"
S19-1005,D17-1070,0,0.123045,"t. However, we hypothesize that each relation may contain useful information about the others, and training on only one relation inevitably neglects some relevant information. Thus, training jointly on multiple relations may improve performance on one or more relations. We propose a joint multi-label transfer learning setting based on LSTM, and show that it can be an effective solution for the multi-relational semantic similarity tasks. Due to the small size of multirelational semantic similarity datasets and the recent success of LSTM-based sentence representations (Wieting and Gimpel, 2018; Conneau et al., 2017), the model is pre-trained on a large corpus and transfer learning is applied using fine-tuning. In our setting, the network is jointly trained on multiple relations by outputting multiple predictions (one for each relation) and aggregating the losses during back-propagation. This is different from the traditional multi-task learning setting where the model makes one prediction at a time, switching between the tasks. We treat the multi-task setting and the single-task setting (i.e., where a separate model is learned for each relation) as baselines, and show that the multi-label setting outperf"
S19-1005,S14-2001,0,0.0217535,"previously reported. Label R, only calculate Loss R, and all parameters except those of dense layer dL are updated. A multi-label learning model (our model) would pick a batch of sentences pairs, consider both Label L and Label R, calculate Loss L and Loss R, aggregate them as the total loss, and update all parameters. 3 Experiments To show the effectiveness of the multi-label transfer learning setting, we experiment on three semantic similarity datasets with multiple relations annotated, and use one LSTM-based sentence encoder that has been very successful in many downstream tasks. 3.1 SICK (Marelli et al., 2014b,a): the Sentences Involving Compositional Knowledge benchmark, which includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena. Each pair of sentences is annotated in two dimensions: relatedness and entailment. The relatedness score ranges from 1 to 5, and Pearson’s r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral. There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split. The sentence pairs are generated from image and vid"
S19-1005,S16-1081,1,0.906556,"Missing"
S19-1005,marelli-etal-2014-sick,0,0.0253454,"previously reported. Label R, only calculate Loss R, and all parameters except those of dense layer dL are updated. A multi-label learning model (our model) would pick a batch of sentences pairs, consider both Label L and Label R, calculate Loss L and Loss R, aggregate them as the total loss, and update all parameters. 3 Experiments To show the effectiveness of the multi-label transfer learning setting, we experiment on three semantic similarity datasets with multiple relations annotated, and use one LSTM-based sentence encoder that has been very successful in many downstream tasks. 3.1 SICK (Marelli et al., 2014b,a): the Sentences Involving Compositional Knowledge benchmark, which includes a large number of sentence pairs that are rich in the lexical, syntactic and semantic phenomena. Each pair of sentences is annotated in two dimensions: relatedness and entailment. The relatedness score ranges from 1 to 5, and Pearson’s r is used for evaluation; the entailment relation is categorical, consisting of entailment, contradiction, and neutral. There are 4439 pairs in the train split, 495 in the trial split used for development and 4906 in the test split. The sentence pairs are generated from image and vid"
S19-1005,S12-1051,0,0.0489425,", or relating short texts or sentences1 in a semantic space – be those phrases, sentences or short paragraphs – is a task that requires systems to determine the degree of equivalence between the underlying semantics of the two sentences. Although relatively easy for humans, this task remains one of the most difficult natural language understanding problems. The task has been receiving significant interest from the research community. For instance, from 2012 to 2017, the International Workshop on Semantic Evaluation (SemEval) has been holding the Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012, 2013b, 2015, 2016; Cer et al., 2017), dedicated to tackling this problem, with close to 100 team submissions each year. In some semantic similarity datasets, an example consists of a sentence pair and a single annotated similarity score, while in others, each pair 1 In this work, we do not consider word level similarity. 44 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 44–50 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics pair are first mapped to word vector sequences and then encoded as sentence embeddings. Up t"
S19-1005,P15-1150,0,0.139992,"Missing"
S19-1005,S13-1004,0,0.141514,"Human Activity Phrase (Wilson and Mihalcea, 2017): a collection of pairs of phrases regarding human activities, annotated with the following four different relations. • Similarity (SIM): The degree to which the two activity phrases describe the same thing, semantic similarity in a strict sense. Example of high similarity phrases: to watch a film and to see a movie. • Relatedness (REL): The degree to which the activities are related to one another, a general semantic association between two phrases. Example of strongly related phrases: to give a gift and to receive a present. Typed-Similarity (Agirre et al., 2013b): a collection of meta-data describing books, paintings, films, museum objects and archival records taken from Europeana,4 presented as the pilot track in the SemEval 2013 STS shared task. Typically, the items consist of title, subject, description, and so on, describing a cultural heritage item and, sometimes, a thumbnail of the item itself. For the purpose of measuring semantic similarity, we concatenate all the textual entries such as title, creator, subject and description into a short paragraph that is used as input, although the annotations might be informed of the image aspects of the"
S19-1005,P18-1042,0,0.0119043,"ime while ignoring the rest. However, we hypothesize that each relation may contain useful information about the others, and training on only one relation inevitably neglects some relevant information. Thus, training jointly on multiple relations may improve performance on one or more relations. We propose a joint multi-label transfer learning setting based on LSTM, and show that it can be an effective solution for the multi-relational semantic similarity tasks. Due to the small size of multirelational semantic similarity datasets and the recent success of LSTM-based sentence representations (Wieting and Gimpel, 2018; Conneau et al., 2017), the model is pre-trained on a large corpus and transfer learning is applied using fine-tuning. In our setting, the network is jointly trained on multiple relations by outputting multiple predictions (one for each relation) and aggregating the losses during back-propagation. This is different from the traditional multi-task learning setting where the model makes one prediction at a time, switching between the tasks. We treat the multi-task setting and the single-task setting (i.e., where a separate model is learned for each relation) as baselines, and show that the mult"
S19-1005,D15-1075,0,0.0307022,"possible scores. The output dense layers follow the methods of Tai et al. (2015). With two such dense output layers, two losses are calculated, one for each relation. The total loss is calculated as the sum of the two losses for backpropagation which updates all parameters in the end-to-end network. 2.2 Model We use InferSent (Conneau et al., 2017) as the sentence encoder due to its outstanding performances reported on various semantic similarity tasks. Due to the small sizes of the evaluation datasets, we use the sentence encoder pre-trained on the Stanford Natural Language Inference corpus (Bowman et al., 2015) and Multi-Genre Natural Language Inference corpus (Williams et al., 2018), and transfer to the semantic similarity tasks using fine-tuning. In this process, the output layers for multi-label learning discussed above are stacked on top of the InferSent network, forming an end-to-end model for training and testing on semantic similarity tasks. Figure 1: Overview of the multi-label architecture. 2.3 Neither multi-task nor multi-label learning have been used for multi-relational semantic similarity datasets. For these datasets, either multi-task or multi-label learning can be achieved by treating"
S19-1005,N18-1101,0,0.0197845,". (2015). With two such dense output layers, two losses are calculated, one for each relation. The total loss is calculated as the sum of the two losses for backpropagation which updates all parameters in the end-to-end network. 2.2 Model We use InferSent (Conneau et al., 2017) as the sentence encoder due to its outstanding performances reported on various semantic similarity tasks. Due to the small sizes of the evaluation datasets, we use the sentence encoder pre-trained on the Stanford Natural Language Inference corpus (Bowman et al., 2015) and Multi-Genre Natural Language Inference corpus (Williams et al., 2018), and transfer to the semantic similarity tasks using fine-tuning. In this process, the output layers for multi-label learning discussed above are stacked on top of the InferSent network, forming an end-to-end model for training and testing on semantic similarity tasks. Figure 1: Overview of the multi-label architecture. 2.3 Neither multi-task nor multi-label learning have been used for multi-relational semantic similarity datasets. For these datasets, either multi-task or multi-label learning can be achieved by treating each relation as a “task.” The key differences between the two are the re"
S19-1005,I17-1067,1,0.854066,"in a semantic space – be those phrases, sentences or short paragraphs – is a task that requires systems to determine the degree of equivalence between the underlying semantics of the two sentences. Although relatively easy for humans, this task remains one of the most difficult natural language understanding problems. The task has been receiving significant interest from the research community. For instance, from 2012 to 2017, the International Workshop on Semantic Evaluation (SemEval) has been holding the Semantic Textual Similarity (STS) shared tasks (Agirre et al., 2012, 2013b, 2015, 2016; Cer et al., 2017), dedicated to tackling this problem, with close to 100 team submissions each year. In some semantic similarity datasets, an example consists of a sentence pair and a single annotated similarity score, while in others, each pair 1 In this work, we do not consider word level similarity. 44 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 44–50 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics pair are first mapped to word vector sequences and then encoded as sentence embeddings. Up to this step, the choice of the word em"
strapparava-etal-2012-parallel,D08-1027,0,\N,Missing
W00-1104,P97-1010,0,0.0493456,"Missing"
W00-1104,H92-1022,0,0.0172453,"Missing"
W00-1104,H93-1061,0,0.0411975,"peech and Offset is the offset of the WordNet synset in which this word occurs. In the case when no sense is assigned by the WSD module or if the word cannot be found in WordNet, the last field is left empty. 2. I n d e x i n g module, which indexes the documents, after they are processed by the WSD module. From the new format of a word, as returned by the WSD function, the Stem and, separately, the Offset{POS are added to the index. This 38 be part of these pairs. Then, we extract all the occurrences of these pairs found within the semantic tagged corpus formed with the 179 texts from SemCor(Miller et al., 1993). If, in all the occurrences, the word Wi has only one sense # k , and the number of occurrences of this sense is larger t h a n 3, then mark the word Wi as having sense # k . Example. Consider t h e word a p p r o v a l in the text fragment ' ' c o m m i t t e e a p p r o v a l o f ' '. The pairs formed are ' ~cown-ittee a p p r o v a l ' ' and ' ~a p p r o v a l o f ' '. No occurrences of the first pair are found in the corpus. Instead, there are four occurrences of the second pair, and in all these occurrences the sense of a p p r o v a l is sense # 1 . Thus, a p p r o v a l is marked with"
W00-1104,H93-1052,0,0.0946179,"Missing"
W02-0817,P94-1020,0,0.015778,"Missing"
W02-0817,W97-0201,0,0.181252,"Missing"
W02-0817,J93-2004,0,0.0326151,"er users are not presented so as to not bias the contributor’s decisions. Based on early feedback from both researchers and contributors, a future version of Open Mind Word Expert may allow contributors to specify more than one sense for any word. A prototype of the system has been implemented and is available at http://www.teachcomputers.org. Figure 1 shows a screen shot from the system interface, illustrating the screen presented to users when tagging the noun “child”. 3.1 Data The starting corpus we use is formed by a mix of three different sources of data, namely the Penn Treebank corpus (Marcus et al., 1993), the Los Angeles Times collection, as provided during TREC conferences1 , and Open Mind Common Sense2 , a collection of about 400,000 commonsense assertions in English as contributed by volunteers over the Web. A mix of several sources, each covering a different spectrum of usage, is 1 2 http://trec.nist.gov http://commonsense.media.mit.edu Figure 1: Screen shot from Open Mind Word Expert used to increase the coverage of word senses and writing styles. While the first two sources are well known to the NLP community, the Open Mind Common Sense constitutes a fairly new textual corpus. It consis"
W02-0817,mihalcea-2002-bootstrapping,1,0.347992,"ous words, STAFS builds a separate feature space for each individual word. The features are selected from a pool of eighteen different features that have been previously acknowledged as good indicators of word sense, including: part of speech of the ambiguous word itself, surrounding words and their parts of speech, keywords in context, noun before and after, verb before and after, and others. An iterative forward search algorithm identifies at each step the feature that leads to the highest cross-validation precision computed on the training data. More details on this system can be found in (Mihalcea, 2002b). The second classifier is a COnstraint-BAsed Language Tagger (COBALT). The system treats every training example as a set of soft constraints on the sense of the word of interest. WordNet glosses, hyponyms, hyponym glosses and other WordNet data is also used to create soft constraints. Currently, only “keywords in context” type of constraint is implemented, with weights accounting for the distance from the target word. The tagging is performed by finding the sense that minimizes the violation of constraints in the instance being tagged. COBALT generates confidences in its tagging of a given"
W02-0817,C02-1039,1,0.709348,"ous words, STAFS builds a separate feature space for each individual word. The features are selected from a pool of eighteen different features that have been previously acknowledged as good indicators of word sense, including: part of speech of the ambiguous word itself, surrounding words and their parts of speech, keywords in context, noun before and after, verb before and after, and others. An iterative forward search algorithm identifies at each step the feature that leads to the highest cross-validation precision computed on the training data. More details on this system can be found in (Mihalcea, 2002b). The second classifier is a COnstraint-BAsed Language Tagger (COBALT). The system treats every training example as a set of soft constraints on the sense of the word of interest. WordNet glosses, hyponyms, hyponym glosses and other WordNet data is also used to create soft constraints. Currently, only “keywords in context” type of constraint is implemented, with weights accounting for the distance from the target word. The tagging is performed by finding the sense that minimizes the violation of constraints in the instance being tagged. COBALT generates confidences in its tagging of a given"
W02-0817,H93-1061,0,0.443122,"Missing"
W02-0817,P96-1006,0,0.303025,"Missing"
W02-0817,S01-1004,0,\N,Missing
W02-2021,W98-1504,0,\N,Missing
W02-2021,P94-1013,0,\N,Missing
W02-2021,P01-1005,0,\N,Missing
W03-0301,P01-1005,0,0.0158626,"Missing"
W03-0301,J93-2003,0,0.0943816,"total of 27 submissions from the seven teams, where 14 sets of results were submitted for the English-French subtask, and 13 for the Romanian-English subtask. Of the 27 total submissions, there were 17 in the Limited resources subtask, and 10 in the Unlimited resources subtask. Tables 2 and 3 show all of the submissions for each team in the two subtasks, and provide a brief description of their approaches. While each participating system was unique, there were a few unifying themes. Four teams had approaches that relied (to varying degrees) on an IBM model of statistical machine translation (Brown et al., 1993). UMD was a straightforward implementation of IBM Model 2, BiBr employed a boosting procedure in deriving an IBM Model 1 lexicon, Ralign used IBM Model 2 as a foundation for their recursive splitting procedure, and XRCE used IBM Model 4 as a base for alignment with lemmatized text and bilingual lexicons. Two teams made use of syntactic structure in the text to be aligned. ProAlign satisfies constraints derived from a dependency tree parse of the English sentence being 3 The two teams that did not participate in English-French were Fourday and RACAI. aligned. BiBr also employs syntactic constra"
W03-0301,W03-0305,0,0.0844355,"Missing"
W03-0301,W03-0306,0,0.0345882,"Missing"
W03-0301,W03-0302,0,0.0687715,"Missing"
W03-0301,C00-2163,0,0.0833749,"xts, provided together with manually determined word alignments. The main purpose of these data was to enable participants to better understand the format required for the word alignment result files. Trial sets consisted of 37 English-French, and 17 Romanian-English aligned sentences. guaranteed to be a subset of the Probable alignment set. The annotators did not produce any NULL alignments. Instead, we assigned NULL alignments as a default backup mechanism, which forced each word to belong to at least one alignment. The English-French aligned data were produced by Franz Och and Hermann Ney (Och and Ney, 2000). For Romanian-English, annotators were instructed to assign an alignment to all words, with specific instructions as to when to assign a NULL alignment. Annotators were not asked to assign a Sure or Probable label. Instead, we had an arbitration phase, where a third annotator judged the cases where the first two annotators disagreed. Since an inter-annotator agreement was reached for all word alignments, the final resulting alignments were considered to be Sure alignments. 3 Evaluation Measures Evaluations were performed with respect to four different measures. Three of them – precision, reca"
W03-0301,W03-0304,0,0.0515658,"Missing"
W03-0301,W03-0309,1,0.824107,"Missing"
W03-0301,W03-0308,0,0.039329,"Missing"
W03-0301,C02-1002,0,0.0524515,"Missing"
W03-0301,W03-0303,0,0.0445012,"Missing"
W03-2408,S01-1004,0,\N,Missing
W03-2408,J93-2004,0,\N,Missing
W03-2408,C02-1039,1,\N,Missing
W03-2408,H93-1061,0,\N,Missing
W03-2408,E99-1046,0,\N,Missing
W03-2408,P94-1020,0,\N,Missing
W03-2408,J96-2004,0,\N,Missing
W03-2408,P96-1006,0,\N,Missing
W03-2408,P01-1005,0,\N,Missing
W04-0802,S01-1014,0,0.0294776,"HKUST me ts systems are maximum entropy classifiers. The HKUST comb t and HKUST comb ts systems are voted classifiers that combine a new Kernel PCA model with a maximum entropy model and a boosting–based model. The HKUST comb2 t and HKUST comb2 ts are voted classifiers that combine a new Kernel PCA model with a maximum entropy model, a boosting–based model, and a Naive Bayesian model. 5.4 UMD The UMD team from the University of Maryland entered (UMD–SST) in the t task. UMD–SST is a supervised sense tagger based on the Support Vector Machine learning algorithm, and is described more fully in (Cabezas et al., 2001). 5.5 Duluth The Duluth team from the University of Minnesota, Duluth had one system (Duluth-ELSS) that participated in the t task. This system is an ensemble of three bagged decision trees, each based on a different type of lexical feature. This system was known as Duluth3 in S ENSEVAL -2, and it is described more fully in (Pedersen, 2001). 6 Results All systems attempted all of the test instances, so precision and recall are identical, hence we report Table 2: t Subtask Results System nusmlst HKUST comb t HKUST comb2 t HKUST me t FL-MIX FC-MIX UMD-SST Duluth-ELSS Baseline (majority) Accuracy"
W04-0802,W02-0817,1,0.825136,"her than using the sense inventory from a dictionary we follow the suggestion of (Resnik and Yarowsky, 1999) and use the translations of the target words into a second language. In this task for S ENSEVAL -3, the contexts are in English, and the “sense tags” for the English target words are their translations in Hindi. This paper outlines some of the major issues that arose in the creation of this task, and then describes the participating systems and summarizes their results. 2 Open Mind Word Expert The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations 1 . To overcome the current lack of tagged data and the limitations imposed by the creation of such data using trained lexicographers, the Open Mind Word 1 Multilingual Open Mind Word Expert can be accessed at http://teach-computers.org/word-expert/english-hindi Expert system enables the collection of semantically annotated corpora over the Web. Tagged examples are collected using a Web-based application that allows contributors to annotate words with their meanings. The tagging exercise proceeds as follows. For each target word the system extracts a set"
W04-0802,S01-1034,1,0.83931,"based model, and a Naive Bayesian model. 5.4 UMD The UMD team from the University of Maryland entered (UMD–SST) in the t task. UMD–SST is a supervised sense tagger based on the Support Vector Machine learning algorithm, and is described more fully in (Cabezas et al., 2001). 5.5 Duluth The Duluth team from the University of Minnesota, Duluth had one system (Duluth-ELSS) that participated in the t task. This system is an ensemble of three bagged decision trees, each based on a different type of lexical feature. This system was known as Duluth3 in S ENSEVAL -2, and it is described more fully in (Pedersen, 2001). 6 Results All systems attempted all of the test instances, so precision and recall are identical, hence we report Table 2: t Subtask Results System nusmlst HKUST comb t HKUST comb2 t HKUST me t FL-MIX FC-MIX UMD-SST Duluth-ELSS Baseline (majority) Accuracy 63.4 62.0 61.4 60.6 60.3 60.3 59.4 58.2 51.9 Table 3: ts Subtask Results System nusmlsts FL-MIX FC-MIX HKUST comb ts HKUST comb2 ts HKUST me ts Baseline (majority) Accuracy 67.3 64.1 64.1 63.8 63.8 60.8 55.8 the single Accuracy figure. Tables 2 and 3 show results for the t and ts subtasks, respectively. We note that the participating syste"
W04-0807,J96-2004,0,0.0387814,"Missing"
W04-0807,W02-0817,1,0.693609,"Sense Disambiguation. This task is a follow-up to similar tasks organized during the S ENSEVAL -1 (Kilgarriff and Palmer, 2000) and S ENSEVAL -2 (Preiss and Yarowsky, 2001) evaluations. The main changes in this year’s evaluation consist of a new methodology for collecting annotated data (with contributions from Web users, as opposed to trained lexicographers), and a new sense inventory used for verb entries (Wordsmyth). 2 Building a Sense Tagged Corpus with Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002) 1 . To overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trained lexicographers, the OMWE system enables the collection of semantically annotated corpora over the Web. Sense tagged examples are collected using 1 Open Mind Word Expert can be accessed at http://teachcomputers.org/ a Web-based application that allows contributors to annotate words with their meanings. The tagging exercise proceeds as follows. For each target word the system extracts a set of sentences from a large textual corpus. These examples are presented to the cont"
W04-0808,A00-1031,0,0.0521672,"Missing"
W04-0808,W02-0817,1,0.825635,"ber of languages were added to the original set of tasks. Having the WSD task prepared for several languages provides the opportunity to test the generality of WSD systems, and to detect differences with respect to word senses in various languages. This year we have proposed a Romanian WSD task. Five teams with a total of seven systems have tackled this task. We present in this paper the data used and how it was obtained, and the performance of the participating systems. 2 Open Mind Word Expert The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian1 . To overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trained lexicographers, the Open Mind Word Expert system enables the collection of semantically annotated corpora over the Web. Sense tagged examples are collected using a Webbased application that allows contributors to annotate words with their meanings. The tagging exercise proceeds as follows. For each target word the system extracts a set of sentences from a large textual corpus. These examples are presented to the contributors, who are asked to select"
W04-0838,C02-1039,1,\N,Missing
W04-0838,N01-1012,0,\N,Missing
W04-0838,H93-1061,0,\N,Missing
W04-0838,P97-1009,0,\N,Missing
W04-0838,W02-0814,0,\N,Missing
W04-2008,W03-1007,0,0.119436,"s and adverbs may modify different attributes in different situations and sometimes the resolution requires high level understanding using commonsense knowledge and context. These interfaces make the semantic parser more flexible, robust, and easier to integrate into other systems that achieve high level meaning processing and understanding. 7 Related Work There are several statistical approaches for automatic semantic role labeling based on PropBank and FrameNet. (Gildea and Jurafsky, 2000) proposed a statistical approach based on FrameNet I data for annotation of semantic roles. Fleischman (Fleischman et al., 2003) used FrameNet annotations in a maximum entropy framework. A more flexible generative model is proposed in (Thompson et al., 2003), where null-instantiated roles can be also identified, and frames are not assumed to be known a-priori. These approaches exclusively focus on semantic roles labeling based on statistical methods, rather than analysis of the full structure of sentence semantics. However, a rule-based approach is closer to the way humans interpret the semantic structure of a sentence. Moreover, as mentioned earlier, the FrameNet data is not meant to be “statistically representative”"
W04-2008,P00-1065,0,0.0237321,"interface for add-ons that can resolve polysemy of descriptive adjectives and adverbs. Due to polysemy, some descriptive adjectives and adverbs may modify different attributes in different situations and sometimes the resolution requires high level understanding using commonsense knowledge and context. These interfaces make the semantic parser more flexible, robust, and easier to integrate into other systems that achieve high level meaning processing and understanding. 7 Related Work There are several statistical approaches for automatic semantic role labeling based on PropBank and FrameNet. (Gildea and Jurafsky, 2000) proposed a statistical approach based on FrameNet I data for annotation of semantic roles. Fleischman (Fleischman et al., 2003) used FrameNet annotations in a maximum entropy framework. A more flexible generative model is proposed in (Thompson et al., 2003), where null-instantiated roles can be also identified, and frames are not assumed to be known a-priori. These approaches exclusively focus on semantic roles labeling based on statistical methods, rather than analysis of the full structure of sentence semantics. However, a rule-based approach is closer to the way humans interpret the semant"
W04-2405,W03-0407,0,0.402147,"Mitchell, 1998) attempt to increase the amount of annotated data using some (large) amounts of unlabeled data. Shortly, co-training algorithms work by generating several classifiers trained on the input labeled data, which are then used to tag new unlabeled data. From this newly annotated data, the most confident predictions are sought, and subsequently added to the set of labeled data. The process may continue for several iterations. In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. One important aspect of co-training consists in the relation between the views used in learning. In the original definition of co-training, (Blum and Mitchell, 1998) state conditional independence of the views as a required criterion for co-training to work. In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. He is proposing a greedy algorithm to maximize agreement on unla"
W04-2405,W02-1006,0,0.0430693,"ees of success, including Bayesian learning, decision trees, decision lists, memory based learning, and others. (Yarowsky and Florian, 2002) give a comprehensive examination of learning methods and their combination. 3.4 Basic Classifiers for Word Sense Disambiguation Several basic word sense disambiguation classifiers can be implemented using feature combinations from Table 1, and feature vectors can be plugged into any learning algorithm. We use Naive Bayes, since it was previously shown that in combination with the features we consider, can lead to a state-of-the-art disambiguation system (Lee and Ng, 2002). Moreover, Naive Bayes is particularly suitable for co-training and self-training, since it provides confidence scores and is efficient in terms of training and testing time. The two separate views required for co-training are defined using a local versus topical feature split. For selftraining, a global classifier with no feature split is defined. A local classifier A local classifier was implemented using all local features listed in Table 1. A topical classifier The topical classifier relies on features extracted from a large context, in particular keywords specific to each individual sens"
W04-2405,C02-1039,1,0.268105,"is indicated as local (L) or topical (T). 3.2 Features that are good indicators of word sense Previous work on word sense disambiguation has acknowledged several local and topical features as good indicators of word sense. These include surrounding words and their part of speech tags, collocations, keywords in contexts. More recently, other possible features have been investigated: bigrams, named entities, syntactic features, semantic relations with other words in context. Table 1 lists commonly used features in word sense disambiguation (list drawn from a larger set of features compiled by (Mihalcea, 2002)). 3.3 Supervised learning for word sense disambiguation Related work in supervised word sense disambiguations includes experiments with a variety of learning algorithms, with varying degrees of success, including Bayesian learning, decision trees, decision lists, memory based learning, and others. (Yarowsky and Florian, 2002) give a comprehensive examination of learning methods and their combination. 3.4 Basic Classifiers for Word Sense Disambiguation Several basic word sense disambiguation classifiers can be implemented using feature combinations from Table 1, and feature vectors can be plug"
W04-2405,N03-1023,0,0.599485,"abeled data, co-training algorithms (Blum and Mitchell, 1998) attempt to increase the amount of annotated data using some (large) amounts of unlabeled data. Shortly, co-training algorithms work by generating several classifiers trained on the input labeled data, which are then used to tag new unlabeled data. From this newly annotated data, the most confident predictions are sought, and subsequently added to the set of labeled data. The process may continue for several iterations. In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. One important aspect of co-training consists in the relation between the views used in learning. In the original definition of co-training, (Blum and Mitchell, 1998) state conditional independence of the views as a required criterion for co-training to work. In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. He is proposing a g"
W04-2405,W01-0501,0,0.0262514,"Missing"
W04-2405,N01-1023,0,0.121193,"Co-training Starting with a set of labeled data, co-training algorithms (Blum and Mitchell, 1998) attempt to increase the amount of annotated data using some (large) amounts of unlabeled data. Shortly, co-training algorithms work by generating several classifiers trained on the input labeled data, which are then used to tag new unlabeled data. From this newly annotated data, the most confident predictions are sought, and subsequently added to the set of labeled data. The process may continue for several iterations. In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. One important aspect of co-training consists in the relation between the views used in learning. In the original definition of co-training, (Blum and Mitchell, 1998) state conditional independence of the views as a required criterion for co-training to work. In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker"
W04-2405,S01-1027,0,\N,Missing
W04-2405,P02-1046,0,\N,Missing
W04-3252,W03-1028,0,\N,Missing
W04-3252,P04-3020,1,\N,Missing
W04-3252,N03-1020,0,\N,Missing
W04-3252,C04-1162,1,\N,Missing
W05-0809,C00-2163,0,\N,Missing
W05-0809,J93-2003,0,\N,Missing
W05-0809,W05-0817,0,\N,Missing
W05-0809,W05-0814,0,\N,Missing
W05-0809,W05-0808,0,\N,Missing
W05-0809,W05-0811,0,\N,Missing
W05-0809,W05-0810,0,\N,Missing
W05-0809,W05-0801,0,\N,Missing
W05-0809,W05-0819,0,\N,Missing
W05-0809,W05-0818,0,\N,Missing
W05-0809,W05-0812,0,\N,Missing
W05-0809,W03-0301,1,\N,Missing
W05-0809,J03-1002,0,\N,Missing
W05-0809,W05-0815,0,\N,Missing
W05-0809,W03-0320,1,\N,Missing
W05-0809,W05-0813,0,\N,Missing
W05-1203,C04-1051,0,0.0213715,"or fill which is related to complete. Unlike traditional similarity measures based on lexical matching, our metric takes into account the semantic similarity of these words, resulting in a more precise measure of text similarity. IDF 5.80 5.23 3.57 0.85 0.09 0.28 0.45 1.29 0.06 1.39 Table 1: Wu & Palmer word similarity scores for computing text similarity with respect to text 1 16 Evaluation To test the effectiveness of the text semantic similarity metric, we use this measure to automatically identify if two text segments are paraphrases of each other. We use the Microsoft paraphrase corpus (Dolan et al., 2004), consisting of 4,076 training pairs and 1,725 test pairs, and determine the number of correctly identified paraphrase pairs in the corpus using the text semantic similarity measure as the only indicator of paraphrasing. In addition, we also evaluate the measure using the PASCAL corpus (Dagan et al., 2005), consisting of 1,380 test–hypothesis pairs with a directional entailment (580 development pairs and 800 test pairs). For each of the two data sets, we conduct two evaluations, under two different settings: (1) An unsupervised setting, where the decision on what constitutes a paraphrase (enta"
W05-1203,O97-1002,0,0.473534,"esnik (Resnik, 1995) returns the information content (IC) of the LCS of two concepts: Simres = IC(LCS) (3) where IC is defined as: IC(c) = − log P (c) (4) and P (c) is the probability of encountering an instance of concept c in a large corpus. The next measure we use in our experiments is the metric introduced by Lin (Lin, 1998), which builds on Resnik’s measure of similarity, and adds a normalization factor consisting of the information content of the two input concepts: Simlin = 2 ∗ IC(LCS) IC(concept1 ) + IC(concept2 ) (5) Finally, the last similarity metric we consider is Jiang & Conrath (Jiang and Conrath, 1997), which returns a score determined by: Simjnc = 1 IC(concept1 ) + IC(concept2 ) − 2 ∗ IC(LCS) (6) 2.2 Language Models In addition to the semantic similarity of words, we also want to take into account the specificity of words, so that we can give a higher weight to a semantic matching identified between two very specific words (e.g. collie and sheepdog), and give less importance to the similarity score measured between generic concepts (e.g. go and be). While the specificity of words is already measured to some extent by their depth in the semantic hierarchy, we are reinforcing this factor wit"
W05-1203,N03-1020,0,0.0146667,"earliest applications of text similarity is perhaps the vectorial model in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has been also used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986), and more recently for extractive summarization (Salton et al., 1997b), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). The typical approach to finding the similarity between two text segments is to use a simple lexical matching method, and produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longest subsequence matching, as well as various weighting and normalization factors (Salton et al., 1997a). While successful to a certain degree, these lexical matching similarity methods fail to identify the semantic similarity of texts. For instance, there is an obvious"
W05-1203,P02-1040,0,0.0908423,"guage processing and related areas. One of the earliest applications of text similarity is perhaps the vectorial model in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971). Text similarity has been also used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986), and more recently for extractive summarization (Salton et al., 1997b), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003). The typical approach to finding the similarity between two text segments is to use a simple lexical matching method, and produce a similarity score based on the number of lexical units that occur in both input segments. Improvements to this simple method have considered stemming, stop-word removal, part-of-speech tagging, longest subsequence matching, as well as various weighting and normalization factors (Salton et al., 1997a). While successful to a certain degree, these lexical matching similarity methods fail to identify the semantic similarity o"
W05-1203,P94-1019,0,0.134382,"ead to the highest concept-to-concept similarity. We use the WordNet-based implementation of these metrics, as available in the WordNet::Similarity package (Patwardhan et al., 2003). 14 length 2∗D and (1) where length is the length of the shortest path between two concepts using node-counting, and D is the maximum depth of the taxonomy. The Lesk similarity of two concepts is defined as a function of the overlap between the corresponding definitions, as provided by a dictionary. It is based on an algorithm proposed in (Lesk, 1986) as a solution for word sense disambiguation. The Wu and Palmer (Wu and Palmer, 1994) similarity metric measures the depth of the two concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score: Simwup = 2 ∗ depth(LCS) depth(concept1 ) + depth(concept2 ) (2) The measure introduced by Resnik (Resnik, 1995) returns the information content (IC) of the LCS of two concepts: Simres = IC(LCS) (3) where IC is defined as: IC(c) = − log P (c) (4) and P (c) is the probability of encountering an instance of concept c in a large corpus. The next measure we use in our experiments is the metric introduced by Lin (Lin,"
W09-1126,D07-1061,0,0.00630853,"ely large, and while it was found useful for the task of text classification with a relatively small number of categories, it would be difficult to adapt for topic identification when the number of possible topics grows beyond the approximately 390,000 under consideration. In a similar line of work, (Bodo et al., 2007) examined the use of Wikipedia and latent semantic analysis for the purposes of text categorization, but reported negative results when used for the categorization of the Reuters-21578 dataset. Others are exploring the use of graph propagation for deriving semantic information. (Hughes and Ramage, 2007) described the use of a biased PageRank over the WordNet graph to compute word pair semantic relatedness using the divergence of the probability values over the graph created by each word. (Ollivier and Senellart, 2007) describes a method to determine related Wikipedia article using a Markov chain derived value called the green measure. Differences exist between the PageRank based methods used as a baseline in their work and the method proposed here, since our system can use the content 217 6 Conclusions and Future Work In this paper, we introduced a system for automatic topic identification,"
W09-2412,D07-1007,0,0.11801,"efined sense inventory. Resnik and Yarowsky (2000) also conducted their experiments using words in context, rather than a predefined sense-inventory as in (Ng and Chan, 2007; Chan and Ng, 2005), however in these experiments the annotators were asked for a single preferred translation. We intend to allow annotators to supply as many translations as they feel are equally valid. This will allow us to examine more subtle relationships between usages and to allow partial credit to systems which get a close approximation to the annotators’ translations. Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems will not be required to translate the whole context but just the target word. 4 The Cross-Lingual Lexical Substitution Task Here we discuss our proposal for a Cross-Lingual Lexical Substitution task. The task will follow LEX SUB except that the annotations will be translations rather than paraphrases. Given a target word in context, the task is to provide several correct translations for that word in a given language. We will use English as the source language and Spanish as the target language. Multiwords are ‘part and parcel’ of natural language. For this reason, rat"
W09-2412,S01-1009,0,0.0946596,"llowed up to 10 attempts. 1 The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). 1 The details are available at http://nlp.cs.swarthmore.edu/semeval/tasks/ task10/task10documentation.pdf. 77 3 Motivation and Related Work While there has been a lot of discussion on the relevant sense distinctions for monolingual WSD systems, for machine translation applications there is a consensus that the relevant sense distinctions are those that reflect different translations. One early and notable work was the S ENSEVAL-2 Japanese Translation task (Kurohashi, 2001) that obtained alternative translation records of typical usages of a test word, also referred to as a translation memory. Systems could either select the most appropriate translation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, we propose to provide actual translations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit e"
W09-2412,S07-1009,1,0.708115,"ta. Any system that relied on training data, such as sense annotated corpora, had to use resources available from other sources. The task had eight participating teams. Teams were allowed to submit up to two systems and there were a total of ten different systems. The scoring was conducted using recall and precision measures using: • the frequency distribution of responses from the annotators and • the mode of the annotators (the most frequent response). The systems were scored using their best guess as well as an out-of-ten score which allowed up to 10 attempts. 1 The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). 1 The details are available at http://nlp.cs.swarthmore.edu/semeval/tasks/ task10/task10documentation.pdf. 77 3 Motivation and Related Work While there has been a lot of discussion on the relevant sense distinctions for monolingual WSD systems, for machine translation applications there is a consensus that the relevant sense distinctions are those that reflect different translations. One early and notable work was the S ENSEVAL-2 Japanese Translation task (Kurohashi, 2001) that obtained alternative translation records of typical usages of"
W09-2412,W02-0816,1,0.911884,"be in English, but the substitutes will be in Spanish. An automatic system for cross-lingual lexical substitution would be useful for a number of applications. For instance, such a system could be used to assist human translators in their work, by providing a number of correct translations that the human translator can choose from. Similarly, the system Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval-2007 following earlier ideas on a method of testing WSD systems without predetermining the inventory (McCarthy, 2002). The issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1"
W09-2412,S07-1010,0,0.0538714,"nslation memory record for each instance and were scored against a gold-standard set of annotations, or they could provide a translation that was scored by translation experts after the results were submitted. In contrast to this work, we propose to provide actual translations for target instances in advance, rather than predetermine translations using lexicographers or rely on post-hoc evaluation, which does not permit evaluation of new systems after the competition. Previous standalone WSD tasks based on parallel data have obtained distinct translations for senses as listed in a dictionary (Ng and Chan, 2007). In this way fine-grained senses with the same translations can be lumped together, however this does not fully allow for the fact that some senses for the same words may have some translations in common but also others that are not. An example from Resnik and Yarowsky (2000) (table 4 in that paper) is the first two senses from WordNet for the noun interest: WordNet sense monetary e.g. on loan stake/share Spanish Translation inter´es, r´edito inter´es,participaci´on For WSD tasks, a decision can be made to lump senses with such overlap, or split them using the distinctive translation and then"
W09-2412,J98-1004,0,0.130057,"Missing"
W09-2412,H05-1051,0,0.0572677,"Similarly, the system Background: The English Lexical Substitution Task The English Lexical substitution task (hereafter referred to as LEXSUB) was run at SemEval-2007 following earlier ideas on a method of testing WSD systems without predetermining the inventory (McCarthy, 2002). The issue of which inventory is appropriate for the task has been a long standing issue for debate, and while there is hope that coarsegrained inventories will allow for increased system performance (Ide and Wilks, 2006) we do not yet know if these will make the distinctions that will most benefit practical systems (Stokoe, 2005) or reflect cognitive processes (Kilgarriff, 2006). LEXSUB was proposed as a task which, while requiring contextual disambiguation, did not presuppose a specific sense inventory. In fact, it is quite possible to use alternative representations of meaning (Sch¨utze, 1998; Pantel and Lin, 2002). The motivation for a substitution task was that it would reflect capabilities that might be useful for natural language processing tasks such as paraphrasing and textual entailment, while only focusing on one aspect of the problem and therefore not requiring a complete system that might mask system capab"
W09-2412,S10-1002,1,\N,Missing
W09-3009,P08-1032,0,0.0730754,"ter Science & Engineering University of North Texas cheeweeleong@my.unt.edu Rada Mihalcea Computer Science & Engineering University of North Texas rada@cs.unt.edu Abstract there are only a few efforts that leverage on the multitude of resources available for natural language processing to derive robust linguistic based image annotation models. Most of the work has posed the annotation task as a classification problem, such as (Li and Wang, 2008), where images are annotated using semantic labels associated to a semantic class. The most recent work on image annotation using linguistic features (Feng and Lapata, 2008) involves implementing an extended version of the continuous relevance model that is proposed in (Jeon et al., 2003). The basic idea underlying their work is to perform annotation of a test image by using keywords shared by similar training images. Evaluation of their system performance is based on a dataset collected from the news domain (BBC). Unlike them, in this paper, we attempt to perform image annotation on datasets from unrestricted domains. We are also interested in extending the work pursued in (Deschacht and Moens, 2007), where visualness and salience are proposed as important textu"
W09-3009,W04-3252,1,\N,Missing
W09-3009,P07-1126,0,\N,Missing
W10-0731,D09-1020,1,0.782035,"P systems subject to a so-called knowledge acquisition bottleneck. For example, (Ng, 1997) estimates an effort of 16 person years to construct training data for a highaccuracy domain independent Word Sense Disambiguation (WSD) system. Recently researchers have been investigating Amazon Mechanical Turk (MTurk) as a source of non-expert natural language annotation, which is a cheap and quick alternative to expert annotations (Kaisser and Lowe, 2008; Mrozinski et al., 2008). In this paper, we utilize MTurk to obtain training data for Subjectivity Word Sense Disambiguation (SWSD) as described in (Akkaya et al., 2009). The goal of SWSD is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. SWSD is a new task which suffers from the absence of a substantial amount of annotated data and thus can only be applied on a small scale. SWSD has strong connections to WSD. Like supervised WSD, it requires training data where target word instances – words which need to be disambiguated by the system – are labeled as having an objective sense or a subjective sense. (Akkaya et al., 2009) show that SWSD may bring substantial imp"
W10-0731,D09-1030,0,0.447101,"scale. The good news is that training data for 80 selected keywords is enough to make a substantial difference (Akkaya et al., 2009). Thus, large scale SWSD is feasible. We hypothesize that annotations for SWSD can be provided by non-experts reliably if the annotation task is presented in a simple way. The annotations obtained from MTurk workers are noisy by nature, because MTurk workers are not trained for the underlying annotation task. That is why previous work explored methods to assess annotation quality and to aggregate multiple noisy annotations for high reliability (Snow et al., 2008; Callison-Burch, 2009). It is understandable that not every worker will provide high-quality annotations, 195 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 195–203, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics depending on their background and interest. Unfortunately, some MTurk workers do not follow the annotation guidelines and carelessly submit annotations in order to gain economic benefits with only minimal effort. We define this group of workers as spammers. We believe it is essential to distinguish b"
W10-0731,W09-1904,0,0.0326961,"ing data (in our case for SWSD). Several studies have concentrated specifically on the quality aspect of the MTurk annotations. They investigated methods to assess annotation quality and to aggregate multiple noisy annotations for high reliability. (Snow et al., 2008) report MTurk annotation quality on various NLP tasks (e.g. WSD, Textual Entailment, Word Similarity) and define a bias correction method for non-expert annotators. (Callison-Burch, 2009) uses MTurk workers for manual evaluation of automatic translation quality and experiments with weighed voting to combine multiple annotations. (Hsueh et al., 2009) define various annotation quality measures and show that they are useful for selecting annotations leading to more accurate classifiers. Our work investigates the effect of built-in qualifications on the quality of MTurk annotations. (Hsueh et al., 2009) applies MTurk to get sentiment annotations on political blog snippets. (Snow et al., 2008) utilizes MTurk for affective text annotation task. In both works, MTurk workers annotated larger entities but on a more detailed scale than we 202 do. (Snow et al., 2008) also provides a WSD annotation task which is similar to our annotation task. The d"
W10-0731,kaisser-lowe-2008-creating,0,0.0933232,"ounts of manually annotated data that is collected from domain experts. The annotation process to obtain this data is very laborious and expensive. This makes supervised NLP systems subject to a so-called knowledge acquisition bottleneck. For example, (Ng, 1997) estimates an effort of 16 person years to construct training data for a highaccuracy domain independent Word Sense Disambiguation (WSD) system. Recently researchers have been investigating Amazon Mechanical Turk (MTurk) as a source of non-expert natural language annotation, which is a cheap and quick alternative to expert annotations (Kaisser and Lowe, 2008; Mrozinski et al., 2008). In this paper, we utilize MTurk to obtain training data for Subjectivity Word Sense Disambiguation (SWSD) as described in (Akkaya et al., 2009). The goal of SWSD is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. SWSD is a new task which suffers from the absence of a substantial amount of annotated data and thus can only be applied on a small scale. SWSD has strong connections to WSD. Like supervised WSD, it requires training data where target word instances – words whi"
W10-0731,P08-1051,0,0.121127,"ted data that is collected from domain experts. The annotation process to obtain this data is very laborious and expensive. This makes supervised NLP systems subject to a so-called knowledge acquisition bottleneck. For example, (Ng, 1997) estimates an effort of 16 person years to construct training data for a highaccuracy domain independent Word Sense Disambiguation (WSD) system. Recently researchers have been investigating Amazon Mechanical Turk (MTurk) as a source of non-expert natural language annotation, which is a cheap and quick alternative to expert annotations (Kaisser and Lowe, 2008; Mrozinski et al., 2008). In this paper, we utilize MTurk to obtain training data for Subjectivity Word Sense Disambiguation (SWSD) as described in (Akkaya et al., 2009). The goal of SWSD is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. SWSD is a new task which suffers from the absence of a substantial amount of annotated data and thus can only be applied on a small scale. SWSD has strong connections to WSD. Like supervised WSD, it requires training data where target word instances – words which need to be disambiguat"
W10-0731,W97-0201,0,0.110822,"conclusive, we are able to obtain high-quality annotations for the SWSD task. These results suggest a greater role for MTurk with respect to constructing a large scale SWSD system in the future, promising substantial improvement in subjectivity and sentiment analysis. 1 Introduction Many Natural Language Processing (NLP) systems rely on large amounts of manually annotated data that is collected from domain experts. The annotation process to obtain this data is very laborious and expensive. This makes supervised NLP systems subject to a so-called knowledge acquisition bottleneck. For example, (Ng, 1997) estimates an effort of 16 person years to construct training data for a highaccuracy domain independent Word Sense Disambiguation (WSD) system. Recently researchers have been investigating Amazon Mechanical Turk (MTurk) as a source of non-expert natural language annotation, which is a cheap and quick alternative to expert annotations (Kaisser and Lowe, 2008; Mrozinski et al., 2008). In this paper, we utilize MTurk to obtain training data for Subjectivity Word Sense Disambiguation (SWSD) as described in (Akkaya et al., 2009). The goal of SWSD is to automatically determine which word instances"
W10-0731,D08-1027,0,0.567301,"Missing"
W10-0731,P06-1134,1,0.203751,"small enough to wear on your wrist”; “a device intended to conserve water”) He sold his catch at the market. catch, haul – (the quantity that was caught; “the catch was only 10 fish”) =&gt; indefinite quantity – (an estimated quantity) Figure 1: Subjective and objective word sense examples. techniques such as majority voting among the submissions can be used to aggregate the results for some types of HITs, resulting in a higher-quality final answer. Previous work (Snow et al., 2008) demonstrates that aggregating worker submissions often leads to an increase in quality. 3 Word Sense Subjectivity (Wiebe and Mihalcea, 2006) define subjective expressions as words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs. Many approaches to sentiment and subjectivity analysis rely on lexicons of such words (subjectivity clues). However, such clues often have both subjective and objective senses, as illustrated by (Wiebe and Mihalcea, 2006). Figure 1 provides subjective and objective examples of senses. (Akkaya et al., 2009) points out that most subjectivity lexicons are compiled as lists of keywords, rather than word meanings (senses). Thus, subjectiv"
W10-0731,passonneau-etal-2006-inter,0,\N,Missing
W11-0104,C10-1004,1,0.657683,"that has been proposed for cross-lingual lexical substitution, where the word sense disambiguation task was more flexibly formulated as the identification of crosslingual lexical substitutes in context (Mihalcea et al., 2010). A number of different approaches have been proposed by the teams participating in the task, and although several of them involved the translation of contexts or substitutes from one language to another, none of them attempted to make simultaneous use of the information available in the two languages. Finally, although the multilingual subjectivity classifier proposed in Banea et al. (2010) is not directly applicable to the disambiguation task we address in this paper, their findings are similar to ours. In that paper, the authors showed how a natural language task can benefit from the use of features drawn from multiple languages, thus supporting the hypothesis that multilingual features can be effectively used to improve the accuracy of a monolingual classifier. 3 Motivation Our work seeks to explore the expansion of a monolingual feature set with features drawn from multiple languages in order to generate a more robust and more effective vector-space representation that can b"
W11-0104,J94-4003,0,0.127475,"of the disambiguation algorithm increases with the number of languages used. There have also been a number of attempts to exploit parallel corpora for word sense disambiguation (Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al., 2003), but in that line of work the parallel 25 texts were mainly used as a way to induce word senses or to create sense-tagged corpora, rather than as a source of additional multilingual views for the disambiguation features. Another related technique is concerned with the selection of correct word senses in context using large corpora in a second language (Dagan and Itai, 1994), but as before, the additional language is used to help distinguishing between the word senses in the original language, and not as a source of additional information for the disambiguation context. Also related is the recent S EMEVAL task that has been proposed for cross-lingual lexical substitution, where the word sense disambiguation task was more flexibly formulated as the identification of crosslingual lexical substitutes in context (Mihalcea et al., 2010). A number of different approaches have been proposed by the teams participating in the task, and although several of them involved th"
W11-0104,P02-1033,0,0.361346,"nslations are automatically disambiguated using information iteratively drawn from two languages. Unlike that approach, which iterates between two languages to select the correct translation for a given target word, in our method we simultaneously use the features extracted from several languages. In fact, our method can handle more than two languages at a time, and we show that the accuracy of the disambiguation algorithm increases with the number of languages used. There have also been a number of attempts to exploit parallel corpora for word sense disambiguation (Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al., 2003), but in that line of work the parallel 25 texts were mainly used as a way to induce word senses or to create sense-tagged corpora, rather than as a source of additional multilingual views for the disambiguation features. Another related technique is concerned with the selection of correct word senses in context using large corpora in a second language (Dagan and Itai, 1994), but as before, the additional language is used to help distinguishing between the word senses in the original language, and not as a source of additional information for the disambiguation context. Also"
W11-0104,P02-1044,0,0.0217485,"that by representing the features in a multilingual space, we are able to improve the performance of a word sense disambiguation system by a significant margin, as compared to a traditional system that uses only monolingual features. 2 Related Work Despite the large number of word sense disambiguation methods that have been proposed so far, targeting the resolution of word ambiguity in different languages, there are only a few methods that try to explore more than one language at a time. The work that is perhaps most closely related to ours is the bilingual bootstrapping method introduced in (Li and Li, 2002), where word translations are automatically disambiguated using information iteratively drawn from two languages. Unlike that approach, which iterates between two languages to select the correct translation for a given target word, in our method we simultaneously use the features extracted from several languages. In fact, our method can handle more than two languages at a time, and we show that the accuracy of the disambiguation algorithm increases with the number of languages used. There have also been a number of attempts to exploit parallel corpora for word sense disambiguation (Resnik and"
W11-0104,S10-1002,1,0.142455,"Another related technique is concerned with the selection of correct word senses in context using large corpora in a second language (Dagan and Itai, 1994), but as before, the additional language is used to help distinguishing between the word senses in the original language, and not as a source of additional information for the disambiguation context. Also related is the recent S EMEVAL task that has been proposed for cross-lingual lexical substitution, where the word sense disambiguation task was more flexibly formulated as the identification of crosslingual lexical substitutes in context (Mihalcea et al., 2010). A number of different approaches have been proposed by the teams participating in the task, and although several of them involved the translation of contexts or substitutes from one language to another, none of them attempted to make simultaneous use of the information available in the two languages. Finally, although the multilingual subjectivity classifier proposed in Banea et al. (2010) is not directly applicable to the disambiguation task we address in this paper, their findings are similar to ours. In that paper, the authors showed how a natural language task can benefit from the use of"
W11-0104,C04-1162,1,0.0771701,"word sense ambiguity is prevalent in all natural languages, with a large number of the words in any given language carrying more than one meaning. For instance, the English noun plant can mean green plant or factory; similarly the French word feuille can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Among the various knowledge-based (Lesk, 1986; Mihalcea et al., 2004) and data-driven (Yarowsky, 1995; Ng and Lee, 1996) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance. In these systems, the sense disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then used in an automatic learning process. One of the main drawbacks associated with these methods is the fact that their performance is closely connected to the amount of labeled data available"
W11-0104,P96-1006,0,0.0294364,"es, with a large number of the words in any given language carrying more than one meaning. For instance, the English noun plant can mean green plant or factory; similarly the French word feuille can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Among the various knowledge-based (Lesk, 1986; Mihalcea et al., 2004) and data-driven (Yarowsky, 1995; Ng and Lee, 1996) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance. In these systems, the sense disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then used in an automatic learning process. One of the main drawbacks associated with these methods is the fact that their performance is closely connected to the amount of labeled data available at hand. In this paper, we investigate a new super"
W11-0104,P03-1058,0,0.0400333,"ally disambiguated using information iteratively drawn from two languages. Unlike that approach, which iterates between two languages to select the correct translation for a given target word, in our method we simultaneously use the features extracted from several languages. In fact, our method can handle more than two languages at a time, and we show that the accuracy of the disambiguation algorithm increases with the number of languages used. There have also been a number of attempts to exploit parallel corpora for word sense disambiguation (Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al., 2003), but in that line of work the parallel 25 texts were mainly used as a way to induce word senses or to create sense-tagged corpora, rather than as a source of additional multilingual views for the disambiguation features. Another related technique is concerned with the selection of correct word senses in context using large corpora in a second language (Dagan and Itai, 1994), but as before, the additional language is used to help distinguishing between the word senses in the original language, and not as a source of additional information for the disambiguation context. Also related is the rec"
W11-0104,S07-1016,0,0.0607284,"Missing"
W11-0104,P95-1026,0,0.590682,"natural languages, with a large number of the words in any given language carrying more than one meaning. For instance, the English noun plant can mean green plant or factory; similarly the French word feuille can mean leaf or paper. The correct sense of an ambiguous word can be selected based on the context where it occurs, and correspondingly the problem of word sense disambiguation is defined as the task of automatically assigning the most appropriate meaning to a polysemous word within a given context. Among the various knowledge-based (Lesk, 1986; Mihalcea et al., 2004) and data-driven (Yarowsky, 1995; Ng and Lee, 1996) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance. In these systems, the sense disambiguation problem is formulated as a supervised learning task, where each sense-tagged occurrence of a particular word is transformed into a feature vector which is then used in an automatic learning process. One of the main drawbacks associated with these methods is the fact that their performance is closely connected to the amount of labeled data available at hand. In this paper, we inve"
W11-0104,N06-2015,0,\N,Missing
W11-0104,W09-2412,1,\N,Missing
W11-0104,J04-1001,0,\N,Missing
W11-0120,D09-1046,0,0.0440929,"Missing"
W11-0120,N10-1011,0,0.0528731,"ilistic LSA to construct joint semantic spaces in order to study their effects on automatic image annotation and semantic image retrieval, but their evaluation was restricted exclusively to the Corel dataset, which is somewhat idealistic and not reflective of the challenges presented by real-world, noisy images. Another related line of work by Barnard and Forsyth (2001) used a generative hierarchical model to learn the associative semantics of words and images for improving information retrieval tasks. Their approach was supervised and evaluated again only on the Corel dataset. More recently, Feng and Lapata (2010) showed that it is possible to combine visual representations of word meanings into a joint bimodal representation constructed by using latent topics. While their work focused on unifying meanings from visual and textual data via supervised techniques, no effort was made to compare the semantic relatedness between arbitrary pairs of word and image. 3 Bag of Visual Codewords Inspired by the bag-of-words approach employed in information retrieval, the “bag of visual codewords” is a similar technique used mainly for scene classification (Yang et al., 2007). Starting with an image collection, visu"
W11-0120,C10-2074,1,0.702947,"ncerned with semantic relatedness, which is a more general concept than semantic similarity. Similarity is concerned with entities related by virtues of their likeness, e.g., bank-trust company, but dissimilar entities may also be related, e.g., hot-cold. A full treatment of the topic can be found in Budanitsky and Hirst (2005). 185 1987). Despite this challenge, we believe this is a worthy research direction, as many important problems can benefit from the association of image content in relation to word meanings, such as automatic image annotation, image retrieval and classification (e.g., (Leong et al., 2010)) as well as tasks in the domains of of text-to-image synthesis, image harvesting and augmentative and alternative communication. 2 Related Work Despite the large amount of work in computing semantic relatedness between words or similarity between images, there are only a few studies in the literature that associate the meaning of words and pictures in a joint semantic space. The work most similar to ours was done by Westerveld (2000), who employed LSA to combine textual words with simple visual features extracted from news images using colors and textures. Although it was concluded that such"
W11-0120,widdows-ferraro-2008-semantic,0,0.0283068,"ttle language-specific requirements as long as texts can be reliably tokenized. Furthermore, various studies (Kanerva, 1998) have shown that by using collaborative, distributive memory units to represent semantic vectors, a closer correspondence to human cognition can be achieved. While vector-space models typically require nontrivial algebraic machinery, reducing dimensions is often key to uncover the hidden (latent) features of the terms distribution in the corpus, and to circumvent the sparseness issue. There are a number of methods that have been developed to reduce dimensions – see e.g., Widdows and Ferraro (2008) for an overview. Here, we briefly describe one commonly used 187 technique, namely the Latent Semantic Analysis (LSA), noted for its effectiveness in previous works for reducing dimensions. In LSA, term co-occurrences in a corpus are captured by means of a dimensionality reduction operated by a Singular Value Decomposition (SVD) on the term-by-document matrix T representing the corpus. SVD is a well-known operation in linear algebra, which can be applied to any rectangular matrix in order to find correlations among its rows and columns. SVD decomposes the term-by-document matrix T into three"
W11-0120,J06-1003,0,\N,Missing
W11-0311,E09-1004,0,0.109842,"a annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis. 1 (2) Introduction Often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective (opinion-carrying) words (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)). Examples of such words are the following (in bold): (1) Rada Mihalcea University of North Texas Denton TX, 76207, USA rada@cs.unt.edu He is a disease to every team he has gone to. Converting to SMF is a headache. The concert left me cold. That guy is such a pain. Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting. Recently, in (Akkaya et al., 2009), we introduced the task of subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, a"
W11-0311,D09-1020,1,0.811735,"Missing"
W11-0311,W10-0731,1,0.946719,"tive) { attack } – begin to injure; ”The cancer cells are attacking his liver”; ”Rust is attacking the metal” { attack, aggress } – take the initiative and go on the offensive; ”The visiting team started to attack” Figure 1: Sense sets for target word “attack” (abridged). of the subjectivity lexicon of (Wilson et al., 2005; Wilson, 2007).3 There are 39 such words. (Akkaya et al., 2009) chose words from a subjectivity lexicon because such words are known to have subjective usages. For this paper, subjectivity sense-tagged data was obtained from the MTurk workers using the annotation scheme of (Akkaya et al., 2010). A goal is to keep the annotation task as simple as possible. Thus, the workers are not directly asked if the instance of a target word has a subjective or an objective sense, because the concept of subjectivity would be difficult to explain in this setting. Instead the workers are shown two sets of senses – one subjective set and one objective set – for a specific target word and a text passage in which the target word appears. Their job is to select the set that best reflects the meaning of the target word in the text passage. The set they choose gives us the subjectivity label of the insta"
W11-0311,E06-1027,0,0.014381,"tion is statistically significant at the p &lt; .01 level with McNemar’s test. More importantly, the F-measure for all the labels improves. This indicates that non-expert MTurk annotations can replace expert annotations for our end-goal – improving contextual opinion analysis – while reducing time and cost requirements by a large margin. Moreover, we see that the improvements in (Akkaya et al., 2009) scale up to new subjectivity clues. 4 Related Work One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009). In contrast, the task in our paper is to automatically assign labels to word instances in a corpus. Recently, some researchers have exploited full word sense disambiguation in methods for opinionrelated tasks. For example, (Mart´ın-Wanton et al., 2010) exploit WSD for recognizing quotation polarities, and (Rentoumi et al., 2009; Mart´ın-Wanton et al., 2010) exploit WSD for recognizing headline polarities. None of this previous work investigates performing a coarse-grained variation of WSD such as SWSD to improve the"
W11-0311,P08-1034,0,0.122996,"e, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis. 1 (2) Introduction Often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective (opinion-carrying) words (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)). Examples of such words are the following (in bold): (1) Rada Mihalcea University of North Texas Denton TX, 76207, USA rada@cs.unt.edu He is a disease to every team he has gone to. Converting to SMF is a headache. The concert left me cold. That guy is such a pain. Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting. Recently, in (Akkaya et al., 2009), we introduced the task of subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used wi"
W11-0311,N07-1039,0,0.21355,"ements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis. 1 (2) Introduction Often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective (opinion-carrying) words (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)). Examples of such words are the following (in bold): (1) Rada Mihalcea University of North Texas Denton TX, 76207, USA rada@cs.unt.edu He is a disease to every team he has gone to. Converting to SMF is a headache. The concert left me cold. That guy is such a pain. Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting. Recently, in (Akkaya et al., 2009), we introduced the task of subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instanc"
W11-0311,P07-1054,0,0.0630713,"h McNemar’s test. More importantly, the F-measure for all the labels improves. This indicates that non-expert MTurk annotations can replace expert annotations for our end-goal – improving contextual opinion analysis – while reducing time and cost requirements by a large margin. Moreover, we see that the improvements in (Akkaya et al., 2009) scale up to new subjectivity clues. 4 Related Work One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009). In contrast, the task in our paper is to automatically assign labels to word instances in a corpus. Recently, some researchers have exploited full word sense disambiguation in methods for opinionrelated tasks. For example, (Mart´ın-Wanton et al., 2010) exploit WSD for recognizing quotation polarities, and (Rentoumi et al., 2009; Mart´ın-Wanton et al., 2010) exploit WSD for recognizing headline polarities. None of this previous work investigates performing a coarse-grained variation of WSD such as SWSD to improve their application results, as we do in this work. A notab"
W11-0311,N09-1002,1,0.91279,"Missing"
W11-0311,N06-2015,0,0.0415888,"SWSD to improve the performance on a contextual NLP task, as we do. While the task in our paper is subjectivity and sentiment analysis, their task is English-Chinese lexical substitution. As (Akkaya et al., 2009) did, they anno94 tated word senses, and exploited SENSEVAL data as training data for SWSD. They did not directly annotate words in context with S/O labels, as we do in our work. Further, they did not separately evaluate a SWSD system component. Many researchers work on reducing the granularity of sense inventories for WSD (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)). Their criteria for grouping senses are syntactic and semantic similarities, while the groupings in work on SWSD are driven by the goals to improve contextual subjectivity and sentiment analysis. 5 Conclusions and Future Work In this paper, we utilized a large pool of non-expert annotators (MTurk) to collect subjectivity sensetagged data for SWSD. We showed that non-expert annotations are as good as expert annotations for training SWSD classifiers. Moreover, we demonstrated that SWSD classifiers trained on non-expert annotations can be exploited to improve contextual opinion analysis. The ad"
W11-0311,C04-1200,0,0.759913,"still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis. 1 (2) Introduction Often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective (opinion-carrying) words (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)). Examples of such words are the following (in bold): (1) Rada Mihalcea University of North Texas Denton TX, 76207, USA rada@cs.unt.edu He is a disease to every team he has gone to. Converting to SMF is a headache. The concert left me cold. That guy is such a pain. Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting. Recently, in (Akkaya et al., 2009), we introduced the task of subjectivity word sense disambiguation (SWSD), which is to automatically determin"
W11-0311,P06-1014,0,0.0335572,"u and Markert, 2010), who exploit SWSD to improve the performance on a contextual NLP task, as we do. While the task in our paper is subjectivity and sentiment analysis, their task is English-Chinese lexical substitution. As (Akkaya et al., 2009) did, they anno94 tated word senses, and exploited SENSEVAL data as training data for SWSD. They did not directly annotate words in context with S/O labels, as we do in our work. Further, they did not separately evaluate a SWSD system component. Many researchers work on reducing the granularity of sense inventories for WSD (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)). Their criteria for grouping senses are syntactic and semantic similarities, while the groupings in work on SWSD are driven by the goals to improve contextual subjectivity and sentiment analysis. 5 Conclusions and Future Work In this paper, we utilized a large pool of non-expert annotators (MTurk) to collect subjectivity sensetagged data for SWSD. We showed that non-expert annotations are as good as expert annotations for training SWSD classifiers. Moreover, we demonstrated that SWSD classifiers trained on non-expert annotations can be exploited to impr"
W11-0311,W04-2807,0,0.0327808,"table exception is (Su and Markert, 2010), who exploit SWSD to improve the performance on a contextual NLP task, as we do. While the task in our paper is subjectivity and sentiment analysis, their task is English-Chinese lexical substitution. As (Akkaya et al., 2009) did, they anno94 tated word senses, and exploited SENSEVAL data as training data for SWSD. They did not directly annotate words in context with S/O labels, as we do in our work. Further, they did not separately evaluate a SWSD system component. Many researchers work on reducing the granularity of sense inventories for WSD (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)). Their criteria for grouping senses are syntactic and semantic similarities, while the groupings in work on SWSD are driven by the goals to improve contextual subjectivity and sentiment analysis. 5 Conclusions and Future Work In this paper, we utilized a large pool of non-expert annotators (MTurk) to collect subjectivity sensetagged data for SWSD. We showed that non-expert annotations are as good as expert annotations for training SWSD classifiers. Moreover, we demonstrated that SWSD classifiers trained on non-expert annotations can be ex"
W11-0311,R09-1067,0,0.0625514,"subjectivity clues. 4 Related Work One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009). In contrast, the task in our paper is to automatically assign labels to word instances in a corpus. Recently, some researchers have exploited full word sense disambiguation in methods for opinionrelated tasks. For example, (Mart´ın-Wanton et al., 2010) exploit WSD for recognizing quotation polarities, and (Rentoumi et al., 2009; Mart´ın-Wanton et al., 2010) exploit WSD for recognizing headline polarities. None of this previous work investigates performing a coarse-grained variation of WSD such as SWSD to improve their application results, as we do in this work. A notable exception is (Su and Markert, 2010), who exploit SWSD to improve the performance on a contextual NLP task, as we do. While the task in our paper is subjectivity and sentiment analysis, their task is English-Chinese lexical substitution. As (Akkaya et al., 2009) did, they anno94 tated word senses, and exploited SENSEVAL data as training data for SWSD"
W11-0311,W03-1014,1,0.777946,"ntegration of SWSD into contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis. 1 (2) Introduction Often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective (opinion-carrying) words (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)). Examples of such words are the following (in bold): (1) Rada Mihalcea University of North Texas Denton TX, 76207, USA rada@cs.unt.edu He is a disease to every team he has gone to. Converting to SMF is a headache. The concert left me cold. That guy is such a pain. Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting. Recently, in (Akkaya et al., 2009), we introduced the task of subjectivity word sense disamb"
W11-0311,D07-1107,0,0.0197717,"2010), who exploit SWSD to improve the performance on a contextual NLP task, as we do. While the task in our paper is subjectivity and sentiment analysis, their task is English-Chinese lexical substitution. As (Akkaya et al., 2009) did, they anno94 tated word senses, and exploited SENSEVAL data as training data for SWSD. They did not directly annotate words in context with S/O labels, as we do in our work. Further, they did not separately evaluate a SWSD system component. Many researchers work on reducing the granularity of sense inventories for WSD (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)). Their criteria for grouping senses are syntactic and semantic similarities, while the groupings in work on SWSD are driven by the goals to improve contextual subjectivity and sentiment analysis. 5 Conclusions and Future Work In this paper, we utilized a large pool of non-expert annotators (MTurk) to collect subjectivity sensetagged data for SWSD. We showed that non-expert annotations are as good as expert annotations for training SWSD classifiers. Moreover, we demonstrated that SWSD classifiers trained on non-expert annotations can be exploited to improve contextual opin"
W11-0311,N09-1001,0,0.0772355,"tantly, the F-measure for all the labels improves. This indicates that non-expert MTurk annotations can replace expert annotations for our end-goal – improving contextual opinion analysis – while reducing time and cost requirements by a large margin. Moreover, we see that the improvements in (Akkaya et al., 2009) scale up to new subjectivity clues. 4 Related Work One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009). In contrast, the task in our paper is to automatically assign labels to word instances in a corpus. Recently, some researchers have exploited full word sense disambiguation in methods for opinionrelated tasks. For example, (Mart´ın-Wanton et al., 2010) exploit WSD for recognizing quotation polarities, and (Rentoumi et al., 2009; Mart´ın-Wanton et al., 2010) exploit WSD for recognizing headline polarities. None of this previous work investigates performing a coarse-grained variation of WSD such as SWSD to improve their application results, as we do in this work. A notable exception is (Su and"
W11-0311,N10-1054,0,0.0167085,"2009). In contrast, the task in our paper is to automatically assign labels to word instances in a corpus. Recently, some researchers have exploited full word sense disambiguation in methods for opinionrelated tasks. For example, (Mart´ın-Wanton et al., 2010) exploit WSD for recognizing quotation polarities, and (Rentoumi et al., 2009; Mart´ın-Wanton et al., 2010) exploit WSD for recognizing headline polarities. None of this previous work investigates performing a coarse-grained variation of WSD such as SWSD to improve their application results, as we do in this work. A notable exception is (Su and Markert, 2010), who exploit SWSD to improve the performance on a contextual NLP task, as we do. While the task in our paper is subjectivity and sentiment analysis, their task is English-Chinese lexical substitution. As (Akkaya et al., 2009) did, they anno94 tated word senses, and exploited SENSEVAL data as training data for SWSD. They did not directly annotate words in context with S/O labels, as we do in our work. Further, they did not separately evaluate a SWSD system component. Many researchers work on reducing the granularity of sense inventories for WSD (e.g., (Palmer et al., 2004; Navigli, 2006; Snow"
W11-0311,P02-1053,0,0.00653987,"les. In this paper, we scale up the integration of SWSD into contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis. 1 (2) Introduction Often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective (opinion-carrying) words (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)). Examples of such words are the following (in bold): (1) Rada Mihalcea University of North Texas Denton TX, 76207, USA rada@cs.unt.edu He is a disease to every team he has gone to. Converting to SMF is a headache. The concert left me cold. That guy is such a pain. Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting. Recently, in (Akkaya et al., 2009), we introd"
W11-0311,P06-1134,1,0.959197,"omings, in this paper, we investigate (1) the feasibility of obtaining a substantial amount of annotated data, (2) whether performance improvements on contextual opinion analysis can be realized on a larger scale, and (3) whether those improvements can be realized with subjectivity sense tagged data that is not built on expert fullinventory sense annotations. In addition, we explore better methods for applying SWSD to contextual opinion analysis. 2 Subjectivity Word Sense Disambiguation 2.1 Annotation Tasks We adopt the definitions of subjective (S) and objective (O) from (Wiebe et al., 2005; Wiebe and Mihalcea, 2006; Wilson, 2007). Subjective expressions are words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs. A general covering term for such states is private state (Quirk et al., 1985), an internal state that cannot be directly observed or verified by others. Objective expressions instead are words and phrases that lack subjectivity. The contextual opinion analysis experiments described in Section 3 include both S/O and polarity (positive,negative, neutral) classifications. The opinion-annotated data used in those experiments is"
W11-0311,H05-1044,1,0.626848,"p://www.cs.pitt.edu/mpqa 88 Sense Set1 (Subjective) { attack, round, assail, lash out, snipe, assault } – attack in speech or writing; ”The editors attacked the House Speaker” { assail, assault, set on, attack } – attack someone emotionally; ”Nightmares assailed him regularly” Sense Set2 (Objective) { attack } – begin to injure; ”The cancer cells are attacking his liver”; ”Rust is attacking the metal” { attack, aggress } – take the initiative and go on the offensive; ”The visiting team started to attack” Figure 1: Sense sets for target word “attack” (abridged). of the subjectivity lexicon of (Wilson et al., 2005; Wilson, 2007).3 There are 39 such words. (Akkaya et al., 2009) chose words from a subjectivity lexicon because such words are known to have subjective usages. For this paper, subjectivity sense-tagged data was obtained from the MTurk workers using the annotation scheme of (Akkaya et al., 2010). A goal is to keep the annotation task as simple as possible. Thus, the workers are not directly asked if the instance of a target word has a subjective or an objective sense, because the concept of subjectivity would be difficult to explain in this setting. Instead the workers are shown two sets of se"
W11-0311,W03-1017,0,0.119389,"contextual opinion analysis and still obtain improvements in performance, by successfully gathering data annotated by non-expert annotators. Further, by improving the method for integrating SWSD into contextual opinion analysis, even greater benefits from SWSD are achieved than in previous work. We thus more firmly demonstrate the potential of SWSD to improve contextual opinion analysis. 1 (2) Introduction Often, methods for opinion, sentiment, and subjectivity analysis rely on lexicons of subjective (opinion-carrying) words (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Andreevskaia and Bergler, 2008; Agarwal et al., 2009)). Examples of such words are the following (in bold): (1) Rada Mihalcea University of North Texas Denton TX, 76207, USA rada@cs.unt.edu He is a disease to every team he has gone to. Converting to SMF is a headache. The concert left me cold. That guy is such a pain. Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting. Recently, in (Akkaya et al., 2009), we introduced the task of subjectivity word sense disambiguation (SWSD), which is to au"
W11-0311,C08-1104,0,\N,Missing
W11-1513,P05-1045,0,0.0101579,"Missing"
W11-1513,D08-1038,0,0.209833,"Missing"
W11-1513,D10-1024,0,0.0607051,"Missing"
W11-3707,D09-1020,1,0.880556,"Missing"
W11-3707,E06-2031,0,0.0438856,"Missing"
W11-3707,banea-etal-2008-bootstrapping,1,0.841995,"10) in Dutch. Sentiment and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjected to experiments in Spanish, Romanian, or Chinese upon automatic translation by (Banea et al., 2008b; Wan, 2009). Furthermore, tools developed for English were used to determine sentiment 44 Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCNLP 2011, pages 44–50, Chiang Mai, Thailand, November 13, 2011. or subjectivity labeling for a given target language by transferring the text to English and applying an English classifier on the resulting data. The labels were then transferred back into the target language (Bautin et al., 2008; Banea et al., 2008b). These experiments are carried out in Arabic, Chinese, French, German, Japanese, Spanish, Romanian. We"
W11-3707,D08-1014,1,0.83571,"10) in Dutch. Sentiment and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjected to experiments in Spanish, Romanian, or Chinese upon automatic translation by (Banea et al., 2008b; Wan, 2009). Furthermore, tools developed for English were used to determine sentiment 44 Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCNLP 2011, pages 44–50, Chiang Mai, Thailand, November 13, 2011. or subjectivity labeling for a given target language by transferring the text to English and applying an English classifier on the resulting data. The labels were then transferred back into the target language (Bautin et al., 2008; Banea et al., 2008b). These experiments are carried out in Arabic, Chinese, French, German, Japanese, Spanish, Romanian. We"
W11-3707,C10-1004,1,0.647963,"Missing"
W11-3707,P07-1056,0,0.00609238,"rried out by (Esuli et al., 2008) when annotating expressions of private state in Italian or by (Maks and Vossen, 2010) in Dutch. Sentiment and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjected to experiments in Spanish, Romanian, or Chinese upon automatic translation by (Banea et al., 2008b; Wan, 2009). Furthermore, tools developed for English were used to determine sentiment 44 Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCNLP 2011, pages 44–50, Chiang Mai, Thailand, November 13, 2011. or subjectivity labeling for a given target language by transferring the text to English and applying an English classifier on the resulting data. The labels were then transferred back into the target language (Bautin et al., 2008; Banea et al"
W11-3707,P08-1041,0,0.0183951,"chniques for automatic sentiment and subjectivity analysis, including automatic expressive text-to-speech synthesis (Alm et al., 1990), tracking sentiment timelines in on-line forums and news (Balog et al., 2006; Lloyd et al., 2005), and mining opinions from product reviews (Hu and Liu, 2004). In many natural language processing tasks, subjectivity and sentiment classification has been used as a first phase filtering to generate more viable data. Research that benefited from this additional layering ranges from question answering (Yu and Hatzivassiloglou, 2003), to conversation summarization (Carenini et al., 2008), text semantic analysis (Wiebe and MihalRelated Work Recently, resources and tools for sentiment analysis developed for English have been used as a starting point to build resources in other languages, via cross-lingual projections or monolingual and multilingual bootstrapping. Several directions were followed, focused on leveraging annotation schemes, lexicons, corpora and automated annotation systems. English annotation schemes developed for opinionated text lays the groundwork for research carried out by (Esuli et al., 2008) when annotating expressions of private state in Italian or by (Ma"
W11-3707,E06-1025,0,0.0226646,"al projections or monolingual and multilingual bootstrapping. Several directions were followed, focused on leveraging annotation schemes, lexicons, corpora and automated annotation systems. English annotation schemes developed for opinionated text lays the groundwork for research carried out by (Esuli et al., 2008) when annotating expressions of private state in Italian or by (Maks and Vossen, 2010) in Dutch. Sentiment and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjected to experiments in Spanish, Romanian, or Chinese upon automatic translation by (Banea et al., 2008b; Wan, 2009). Furthermore, tools developed for English were used to determine sentiment 44 Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCNLP 2011, pages 44–50, Chiang"
W11-3707,esuli-sebastiani-2006-sentiwordnet,0,0.0203499,"al projections or monolingual and multilingual bootstrapping. Several directions were followed, focused on leveraging annotation schemes, lexicons, corpora and automated annotation systems. English annotation schemes developed for opinionated text lays the groundwork for research carried out by (Esuli et al., 2008) when annotating expressions of private state in Italian or by (Maks and Vossen, 2010) in Dutch. Sentiment and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjected to experiments in Spanish, Romanian, or Chinese upon automatic translation by (Banea et al., 2008b; Wan, 2009). Furthermore, tools developed for English were used to determine sentiment 44 Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCNLP 2011, pages 44–50, Chiang"
W11-3707,esuli-etal-2008-annotating,0,0.025217,"Yu and Hatzivassiloglou, 2003), to conversation summarization (Carenini et al., 2008), text semantic analysis (Wiebe and MihalRelated Work Recently, resources and tools for sentiment analysis developed for English have been used as a starting point to build resources in other languages, via cross-lingual projections or monolingual and multilingual bootstrapping. Several directions were followed, focused on leveraging annotation schemes, lexicons, corpora and automated annotation systems. English annotation schemes developed for opinionated text lays the groundwork for research carried out by (Esuli et al., 2008) when annotating expressions of private state in Italian or by (Maks and Vossen, 2010) in Dutch. Sentiment and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjec"
W11-3707,maks-vossen-2010-annotation,0,0.0146337,"8), text semantic analysis (Wiebe and MihalRelated Work Recently, resources and tools for sentiment analysis developed for English have been used as a starting point to build resources in other languages, via cross-lingual projections or monolingual and multilingual bootstrapping. Several directions were followed, focused on leveraging annotation schemes, lexicons, corpora and automated annotation systems. English annotation schemes developed for opinionated text lays the groundwork for research carried out by (Esuli et al., 2008) when annotating expressions of private state in Italian or by (Maks and Vossen, 2010) in Dutch. Sentiment and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjected to experiments in Spanish, Romanian, or Chinese upon automatic translation by (Ban"
W11-3707,P07-1123,1,0.831231,"leveraging annotation schemes, lexicons, corpora and automated annotation systems. English annotation schemes developed for opinionated text lays the groundwork for research carried out by (Esuli et al., 2008) when annotating expressions of private state in Italian or by (Maks and Vossen, 2010) in Dutch. Sentiment and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjected to experiments in Spanish, Romanian, or Chinese upon automatic translation by (Banea et al., 2008b; Wan, 2009). Furthermore, tools developed for English were used to determine sentiment 44 Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCNLP 2011, pages 44–50, Chiang Mai, Thailand, November 13, 2011. or subjectivity labeling for a given target language by transferrin"
W11-3707,N10-1054,0,0.0382252,"Missing"
W11-3707,P09-1027,0,0.378731,"nt and subjectivity lexicons such as the one included with the OpinionFinder distribution (Wiebe and Riloff, 2005), the General Inquirer (Stone et al., 1967), or the SentiWordNet (Esuli and Sebastiani, 2006b) were transferred into Chinese (Ku et al., 2006; Wu, 2008) and into Romanian (Mihalcea et al., 2007). English corpora manually annotated for subjectivity or sentiment such as MPQA (Wiebe et al., 2005), or the multi-domain sentiment classification corpus (Blitzer et al., 2007) were subjected to experiments in Spanish, Romanian, or Chinese upon automatic translation by (Banea et al., 2008b; Wan, 2009). Furthermore, tools developed for English were used to determine sentiment 44 Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), IJCNLP 2011, pages 44–50, Chiang Mai, Thailand, November 13, 2011. or subjectivity labeling for a given target language by transferring the text to English and applying an English classifier on the resulting data. The labels were then transferred back into the target language (Bautin et al., 2008; Banea et al., 2008b). These experiments are carried out in Arabic, Chinese, French, German, Japanese, Spanish, Romanian. We are not aware"
W11-3707,P06-1134,1,0.839338,"Missing"
W11-3707,W03-1017,0,0.0337717,"te, a large number of text processing applications have used techniques for automatic sentiment and subjectivity analysis, including automatic expressive text-to-speech synthesis (Alm et al., 1990), tracking sentiment timelines in on-line forums and news (Balog et al., 2006; Lloyd et al., 2005), and mining opinions from product reviews (Hu and Liu, 2004). In many natural language processing tasks, subjectivity and sentiment classification has been used as a first phase filtering to generate more viable data. Research that benefited from this additional layering ranges from question answering (Yu and Hatzivassiloglou, 2003), to conversation summarization (Carenini et al., 2008), text semantic analysis (Wiebe and MihalRelated Work Recently, resources and tools for sentiment analysis developed for English have been used as a starting point to build resources in other languages, via cross-lingual projections or monolingual and multilingual bootstrapping. Several directions were followed, focused on leveraging annotation schemes, lexicons, corpora and automated annotation systems. English annotation schemes developed for opinionated text lays the groundwork for research carried out by (Esuli et al., 2008) when annot"
W11-3707,W11-0104,1,\N,Missing
W11-3707,W08-1207,0,\N,Missing
W11-3707,H05-1073,0,\N,Missing
W11-3707,andreevskaia-bergler-2006-semantic,0,\N,Missing
W13-1732,N12-1033,0,0.0539126,"Missing"
W13-1732,brooke-hirst-2012-measuring,0,0.0733001,"ed as training and test data. The NLI problem has recently seen a big surge in interest, sparked in part by three influential early papers on this problem (Tomokiyo and Jones, 2001; van Halteren and Oostdijk, 2004; Koppel et al., 2005). Apart from shedding light on the way nonnative learners (also called “L2 learners”) learn a new language, the NLI task allows constrastive analysis (Wong and Dras, 2009), study of different types of errors that people make while learning a new language (Kochmar, 2011; Bestgen et al., 2012; Jarvis et al., 2012), and identification of language transfer patterns (Brooke and Hirst, 2012a; Jarvis and Crossley, 2012), thereby helping L2-students improve their writing styles and expediting the learning process. It also helps L2 educators to concentrate their efforts on particular areas of a language that cause the most learning difficulty for different L1s. The NLI task is closely related to traditional NLP problems of authorship attribution (Juola, 2006; Stamatatos, 2009; Koppel et al., 2009) and author profiling (Keˇselj et al., 2003; Estival et al., 2007a; Estival et al., 2007b; Bergsma et al., 2012), and shares many of the same features. Like authorship attribution, NLI is"
W13-1732,C12-1025,0,0.742379,"ed as training and test data. The NLI problem has recently seen a big surge in interest, sparked in part by three influential early papers on this problem (Tomokiyo and Jones, 2001; van Halteren and Oostdijk, 2004; Koppel et al., 2005). Apart from shedding light on the way nonnative learners (also called “L2 learners”) learn a new language, the NLI task allows constrastive analysis (Wong and Dras, 2009), study of different types of errors that people make while learning a new language (Kochmar, 2011; Bestgen et al., 2012; Jarvis et al., 2012), and identification of language transfer patterns (Brooke and Hirst, 2012a; Jarvis and Crossley, 2012), thereby helping L2-students improve their writing styles and expediting the learning process. It also helps L2 educators to concentrate their efforts on particular areas of a language that cause the most learning difficulty for different L1s. The NLI task is closely related to traditional NLP problems of authorship attribution (Juola, 2006; Stamatatos, 2009; Koppel et al., 2009) and author profiling (Keˇselj et al., 2003; Estival et al., 2007a; Estival et al., 2007b; Bergsma et al., 2012), and shares many of the same features. Like authorship attribution, NLI is"
W13-1732,C12-1027,0,0.0243769,"Missing"
W13-1732,U07-1006,0,0.0195742,"w language (Kochmar, 2011; Bestgen et al., 2012; Jarvis et al., 2012), and identification of language transfer patterns (Brooke and Hirst, 2012a; Jarvis and Crossley, 2012), thereby helping L2-students improve their writing styles and expediting the learning process. It also helps L2 educators to concentrate their efforts on particular areas of a language that cause the most learning difficulty for different L1s. The NLI task is closely related to traditional NLP problems of authorship attribution (Juola, 2006; Stamatatos, 2009; Koppel et al., 2009) and author profiling (Keˇselj et al., 2003; Estival et al., 2007a; Estival et al., 2007b; Bergsma et al., 2012), and shares many of the same features. Like authorship attribution, NLI is greatly benefitted by having function words and character n-grams as features (Brooke and Hirst, 2011; Brooke and Hirst, 2012b). Native languages form a part of an author’s socio-cultural and psychological profiles, thereby being related to author profiling (van Halteren and Oostdijk, 2004; Torney et al., 2012). Researchers have used different types of features for the NLI problem, including but not limited to function words (Brooke and Hirst, 2012b); character, word and P"
W13-1732,P12-2038,0,0.044812,"Missing"
W13-1732,W13-1706,0,0.172083,"size and difficulty of data, etc. presence/absence indicator on top 100, 200, 500 and 1000 n-grams:2 2 We experimented with punctuation because previous research indicates that punctuation is helpful (Wong and Dras, 2009; Kochmar, 2011). In total, there are 216 types of n-gram feature vectors (with dimensions 100, 200, 500 and 1000) for a particular document. Because of size restrictions (e.g., some ngram dictionaries are smaller than the specified feature vector dimensions), we ended up with 168 types of feature vectors per document (cf. Tables 2 to 4). Our Approach The NLI 2013 Shared Task (Tetreault et al., 2013) marks an effort in bringing together the NLI research community to share and compare their results and evaluations on a common dataset - TOEFL11 (Blanchard et al., 2013) - consisting of 12,100 unique English essays written by non-native learners of eleven different languages.1 The dataset has 9,900 essays for training, 1,100 essays for test, and 1,100 essays for development. Each of the three sets is balanced across different L1s. Inspired by previous work in NLI, in our different NLI systems submissions we used several different types of character, word, and POS n-gram features (cf. Section"
W13-1732,N01-1031,0,0.0333152,"task. 1 Introduction Native Language Identification (NLI) is a wellestablished problem in NLP, where the goal is to identify a writer’s native language (L1) from his/her writing in a second language (L2), usually English. NLI is generally framed as a multi-class classification problem (Koppel et al., 2005; Brooke and Hirst, 2011; Wong and Dras, 2011), where native languages (L1) are considered class labels, and writing samples in L2 are used as training and test data. The NLI problem has recently seen a big surge in interest, sparked in part by three influential early papers on this problem (Tomokiyo and Jones, 2001; van Halteren and Oostdijk, 2004; Koppel et al., 2005). Apart from shedding light on the way nonnative learners (also called “L2 learners”) learn a new language, the NLI task allows constrastive analysis (Wong and Dras, 2009), study of different types of errors that people make while learning a new language (Kochmar, 2011; Bestgen et al., 2012; Jarvis et al., 2012), and identification of language transfer patterns (Brooke and Hirst, 2012a; Jarvis and Crossley, 2012), thereby helping L2-students improve their writing styles and expediting the learning process. It also helps L2 educators to con"
W13-1732,C04-1139,0,0.0526222,"Missing"
W13-1732,U09-1008,0,0.260742,"generally framed as a multi-class classification problem (Koppel et al., 2005; Brooke and Hirst, 2011; Wong and Dras, 2011), where native languages (L1) are considered class labels, and writing samples in L2 are used as training and test data. The NLI problem has recently seen a big surge in interest, sparked in part by three influential early papers on this problem (Tomokiyo and Jones, 2001; van Halteren and Oostdijk, 2004; Koppel et al., 2005). Apart from shedding light on the way nonnative learners (also called “L2 learners”) learn a new language, the NLI task allows constrastive analysis (Wong and Dras, 2009), study of different types of errors that people make while learning a new language (Kochmar, 2011; Bestgen et al., 2012; Jarvis et al., 2012), and identification of language transfer patterns (Brooke and Hirst, 2012a; Jarvis and Crossley, 2012), thereby helping L2-students improve their writing styles and expediting the learning process. It also helps L2 educators to concentrate their efforts on particular areas of a language that cause the most learning difficulty for different L1s. The NLI task is closely related to traditional NLP problems of authorship attribution (Juola, 2006; Stamatatos"
W13-1732,D11-1148,0,0.0264547,"petitive performance against the baseline feature set, which is a promising result. We also present a discussion of feature analysis based on information gain, and an overview on the performance of different word network features in the Native Language Identification task. 1 Introduction Native Language Identification (NLI) is a wellestablished problem in NLP, where the goal is to identify a writer’s native language (L1) from his/her writing in a second language (L2), usually English. NLI is generally framed as a multi-class classification problem (Koppel et al., 2005; Brooke and Hirst, 2011; Wong and Dras, 2011), where native languages (L1) are considered class labels, and writing samples in L2 are used as training and test data. The NLI problem has recently seen a big surge in interest, sparked in part by three influential early papers on this problem (Tomokiyo and Jones, 2001; van Halteren and Oostdijk, 2004; Koppel et al., 2005). Apart from shedding light on the way nonnative learners (also called “L2 learners”) learn a new language, the NLI task allows constrastive analysis (Wong and Dras, 2009), study of different types of errors that people make while learning a new language (Kochmar, 2011; Bes"
W13-1732,U11-1015,0,0.039596,"Missing"
W13-1732,D12-1064,0,0.0385927,"Missing"
W13-1732,W04-3252,1,\N,Missing
W16-0305,W15-1209,0,0.0749336,"ist and client during MI encounters. In this work, authors relied in the psycholinguistic categories from the Linguistic Inquiry and Word Count lexicon to measure the degree in which counselor matches the client language. (Xiao et al., 2014) presents a study on the automatic evaluation of counselor empathy by analyzing correlations between prosody patterns and empathy showed by the therapist during the counseling interaction. Although most of the work on coding of MI within session language has focused on modeling the counselor language, there is also work that addresses the client language. (Tanana et al., 2015) used recursive neural networks (RNN) to identify client change and sustain talk in MI transcripts, i.e., language that indicates commitment towards and away behavioral change. In this work, authors combined both therapist and client utterances in a single sequence model using Maximum Entropy Markov Models, NRR, and n-grams features. (Gupta et al., 2014) analyzed the valence of client’s attitude towards the target behavior by using n-grams and conditional maximum entropy models. In this paper authors also present an exploration of the role laughter of both counselor and client’s during the MI"
W16-4301,P05-1054,0,0.0476497,"l., 2011) used pattern-based feature creation approach in combination with word classes to classify author’s gender from blog posts. Also of interest is the work by (Prabhakaran et al., 2014), who use topic segments to predict the behavioral patterns of political leaders in election campaigns. Our work is to some extent related to that research, as we also seek to understand and model behaviors from text, however we do this for men and women rather than political figures. In speech, an analysis of the most frequently used words by males and females in telephone conversations was presented in (Boulis and Ostendorf, 2005), who found that swear words are more often used by males (bullshit, sucks, damn), whereas family-relation terms are more often used by females (children, marriage, boyfriend). One exception from the general theme of previous work on surface-level gender classification is the work by (Sarawgi et al., 2011), where topic bias is explicitly avoided, with the goal of identifying stylistic differences between men and women writings. The authors use blogs addressing predefined topics (e.g., education, travel) and scientific publications, and show that differences can be found even when the data sour"
W16-4301,D11-1120,0,0.386761,"e typical surface-level text classification approach, by (1) identifying semantic and psycholinguistic word classes that reflect systematic differences between men and women and (2) finding differences between genders in the ways they use the same words. We describe several experiments and report results on a large collection of blogs authored by men and women. 1 Introduction Previous work on understanding gender differences has mainly focused on the authorship detection facet, trying to identify the gender of the author of a certain writing, be that a blog (Mukherjee and Liu, 2010), a tweet (Burger et al., 2011), or other works of fiction or non-fiction (Koppel et al., 2002). In this paper, we depart from this earlier research and attempt to move beyond the surface level of word occurrences and counts. We instead use semantic analysis to identify broad semantic classes that are specific to each gender, and also find differences that exist between genders in how they use certain concepts. Specifically, the paper addresses the following two main questions. First, can we identify broad semantic and psycholinguistic classes that are predominantly used by men and women? We use linguistic ethnography in co"
W16-4301,I13-1057,1,0.833562,"of the ambiguous word; the parts-of-speech of the surrounding words; the first noun before and after the target word; the first verb before and after the target word. The topical features are determined from the global context and are implemented through class-specific keywords, which are determined as a list of at most five words occurring at least three times in the contexts defining a certain word class (or epoch). The features are then integrated in a Naive Bayes classifier. The final disambiguation system is similar to several word sense disambiguation systems described in previous work (Dandala et al., 2013). For evaluation, we calculate the average accuracy obtained through ten-fold cross-validations applied on the data collected for each word. To place results in perspective, we also calculate a simple baseline, which assigns the most frequent class by default. the more emotional nature of women. 3 A minimum of 100 total examples was required for a word to be considered in the dataset. 6 4.4 Results and Discussion Table 6 summarizes the results obtained for the 364 words.4 Overall, we find that there are indeed differences between the ways men and women use predefined target words, with an aver"
W16-4301,W11-2606,0,0.0458334,"Missing"
W16-4301,H93-1061,0,0.29415,"C was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999; Pennebaker and King, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., emotion, cognition). The LIWC lexicon has been validated by showing significant correlation between human ratings of a large number of written texts and the rating obtained through LIWC-based analyses of the same texts. WordNet Affect (WA). WA (Strapparava and Valitutti, 2004) is a resource that was created starting with WordNet (Miller et al., 1993), by annotating synsets with several emotions. It uses several resources for affective information, including the emotion classification of Ortony (Ortony et al., 1987). We build an affective lexicon by extracting the words corresponding to the six basic emotions defined by (Ortony et al., 1987), namely anger, disgust, fear, joy, sadness, and surprise. 3.3 Gender Dominant Word Classes Applying the word class saliency metric on the blog dataset using the three resources described before results in a score associated with each class. The following word classes were found to be dominant in either"
W16-4301,D10-1021,0,0.257321,"ation and attempt to move beyond the typical surface-level text classification approach, by (1) identifying semantic and psycholinguistic word classes that reflect systematic differences between men and women and (2) finding differences between genders in the ways they use the same words. We describe several experiments and report results on a large collection of blogs authored by men and women. 1 Introduction Previous work on understanding gender differences has mainly focused on the authorship detection facet, trying to identify the gender of the author of a certain writing, be that a blog (Mukherjee and Liu, 2010), a tweet (Burger et al., 2011), or other works of fiction or non-fiction (Koppel et al., 2002). In this paper, we depart from this earlier research and attempt to move beyond the surface level of word occurrences and counts. We instead use semantic analysis to identify broad semantic classes that are specific to each gender, and also find differences that exist between genders in how they use certain concepts. Specifically, the paper addresses the following two main questions. First, can we identify broad semantic and psycholinguistic classes that are predominantly used by men and women? We u"
W16-4301,C14-1184,0,0.0362443,"Missing"
W16-4301,D14-1157,0,0.0309472,"use of automatic classification to distinguish between men and women writings, and also on finding words that are specific to each gender by performing statistical analysis on large amounts of data. Other related work includes recently published research by (Nguyen et al., 2014), who showed how a person’s gender identity can be constructed by using various linguistic aspects of male and female speech in language. (Gianfortoni et al., 2011) used pattern-based feature creation approach in combination with word classes to classify author’s gender from blog posts. Also of interest is the work by (Prabhakaran et al., 2014), who use topic segments to predict the behavioral patterns of political leaders in election campaigns. Our work is to some extent related to that research, as we also seek to understand and model behaviors from text, however we do this for men and women rather than political figures. In speech, an analysis of the most frequently used words by males and females in telephone conversations was presented in (Boulis and Ostendorf, 2005), who found that swear words are more often used by males (bullshit, sucks, damn), whereas family-relation terms are more often used by females (children, marriage,"
W16-4301,W11-0310,0,0.0181195,"e extent related to that research, as we also seek to understand and model behaviors from text, however we do this for men and women rather than political figures. In speech, an analysis of the most frequently used words by males and females in telephone conversations was presented in (Boulis and Ostendorf, 2005), who found that swear words are more often used by males (bullshit, sucks, damn), whereas family-relation terms are more often used by females (children, marriage, boyfriend). One exception from the general theme of previous work on surface-level gender classification is the work by (Sarawgi et al., 2011), where topic bias is explicitly avoided, with the goal of identifying stylistic differences between men and women writings. The authors use blogs addressing predefined topics (e.g., education, travel) and scientific publications, and show that differences can be found even when the data sources are controlled for topic. In our research, we zoom in even deeper, and try to identify semantic and psycholinguistic word classes that characterize gender differences, and also find the distinctive ways in which men and women use certain words. 2.1 Data We use a large corpus of blogposts annotated for"
W16-4301,strapparava-valitutti-2004-wordnet,0,0.0128742,"s grouped into nearly 1,000 head classes. Linguistic Inquiry and Word Count (LIWC). LIWC was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999; Pennebaker and King, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., emotion, cognition). The LIWC lexicon has been validated by showing significant correlation between human ratings of a large number of written texts and the rating obtained through LIWC-based analyses of the same texts. WordNet Affect (WA). WA (Strapparava and Valitutti, 2004) is a resource that was created starting with WordNet (Miller et al., 1993), by annotating synsets with several emotions. It uses several resources for affective information, including the emotion classification of Ortony (Ortony et al., 1987). We build an affective lexicon by extracting the words corresponding to the six basic emotions defined by (Ortony et al., 1987), namely anger, disgust, fear, joy, sadness, and surprise. 3.3 Gender Dominant Word Classes Applying the word class saliency metric on the blog dataset using the three resources described before results in a score associated with"
W16-4301,N03-1033,0,0.0157061,"the 4 parts of speech, uniformly distributed over multiple-sense/unique sense words, and with the frequency based sample as described above. From this initial set of words, we could not identify enough examples for 36,3 which left us with a final set of 364 words. 4.2 Data Preprocessing For each target word in our dataset, we collect 300 examples from each gender, for a maximum of 600 examples per target word. The average number of examples is 492 examples per target word. All the extracted snippets are then processed: the text is tokenized and part-of-speech tagged using the Stanford tagger (Toutanova et al., 2003), and contexts that do not include the target word with the specified part-of-speech are removed. The position of the target word is also identified and recorded as an offset along with the example. 4.3 Gender Disambiguation Algorithm The classification algorithm we use is inspired by previous work on data-driven word sense disambiguation. Specifically, we use a system that integrates both local and topical features. The local features include: the current word and its part-of-speech; a local context of three words to the left and right of the ambiguous word; the parts-of-speech of the surroun"
W98-0703,H92-1046,0,0.0314179,"Missing"
W98-0703,H93-1061,0,0.231041,"Missing"
W98-0703,P96-1006,0,0.059827,"Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et a1.1992), (Miller et a1.1994), (Agirre and Rigau, 1995), (Li et a1.1995), (McRoy, 1992); 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996}; 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. On the other hand, the disambiguation using unsupervised methods"
W98-0703,W97-0209,0,0.0851351,"as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et a1.1992), (Miller et a1.1994), (Agirre and Rigau, 1995), (Li et a1.1995), (McRoy, 1992); 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996}; 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. To our knowledge, none of the statistical methods disambigu"
W98-0703,W97-0213,0,0.16071,"Missing"
W98-0703,P97-1007,0,0.0214995,"Missing"
W98-0703,P95-1026,0,0.12922,"ther tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et a1.1992), (Miller et a1.1994), (Agirre and Rigau, 1995), (Li et a1.1995), (McRoy, 1992); 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996}; 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. To our knowledge, none of the statistical met"
W98-0703,P94-1020,0,0.0277235,"rovided by machine readable dictionaries (Cowie et a1.1992), (Miller et a1.1994), (Agirre and Rigau, 1995), (Li et a1.1995), (McRoy, 1992); 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996}; 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. To our knowledge, none of the statistical methods disambiguate adjectives or adverbs so far. One approach to WSD is to determine the conceptual distance between words, that is to measure the semantic closeness of the words within a sem"
W98-0703,W95-0105,0,0.0417132,"Missing"
W98-0703,W95-0100,0,0.232285,"Missing"
W98-0703,C90-2067,0,0.0943322,"Missing"
W98-0703,C92-2070,0,0.268935,"Missing"
W98-0703,H94-1046,0,\N,Missing
