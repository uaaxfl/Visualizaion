2020.aacl-main.40,P15-1166,0,0.022422,"h and r¯omaji for Japanese. In addition, ACC-ACT can be extended to alphabetical languages by, for instance, constructing the Alternating Sub-word Table which stores lists of interchangeable subsequences. Another possible future work is to redesign the objective function by treating λ as a trainable parameter or including the correlation information (Papasarantopoulos et al., 2019). 8 Related Work Previous work has demonstrated the effectiveness of using MTL on models through joint learning of various NLP tasks such as machine translation, syntactic and dependency parsing (Luong et al., 2016; Dong et al., 2015; Li et al., 2014). In most of this work, underlies a similar idea to create a unified training setting for several tasks by sharing the core parameters. Besides, machine transliteration has a long history of using phonetic information, for example, by mapping a phrase to its pronunciation in the source language and then convert the sound to the target word (Knight and Graehl, 1997). There is also relevant work that uses both graphemes and phonemes to various extents for transliteration, such as the correspondence-based (Oh et al., 2006) and G2P-based (Le and Sadat, 2018) approaches. Our work"
2020.aacl-main.40,W18-2413,0,0.270008,"group of corresponding subsequences in different representations. Introduction Transliteration, the act of mapping a name from the orthographic system of one language to another, is directed by the pronunciation in the source and target languages, and often by historical reasons or conventions. It plays an important role in tasks like information retrieval and machine translation (Marton and Zitouni, 2014; Hermjakob et al., 2008). Over the recent years, many have addressed transliteration using sequence-to-sequence (seq2seq) deep learning models (Rosca and Breuel, 2016; Merhav and Ash, 2018; Grundkiewicz and Heafield, 2018), enhanced with several NMT techniques (Grundkiewicz and Heafield, 2018). However, this recent work neglects the most crucial feature for transliteration, i.e. pronunciation. To * Work done at The University of Edinburgh. Our code and data are available at https://github. com/Lawhy/Multi-task-NMTransliteration. 1 bridge this gap, we define a phonetic auxiliary task that shares the sound information with the main transliteration task under the multi-task learning (MTL) setting. Depending on the specific language, the written form of a word reveals its pronunciation to various extents. For alpha"
2020.aacl-main.40,P08-1045,0,0.107898,"Missing"
2020.aacl-main.40,W09-3521,0,0.058394,"Missing"
2020.aacl-main.40,P97-1017,0,0.631685,"2019). 8 Related Work Previous work has demonstrated the effectiveness of using MTL on models through joint learning of various NLP tasks such as machine translation, syntactic and dependency parsing (Luong et al., 2016; Dong et al., 2015; Li et al., 2014). In most of this work, underlies a similar idea to create a unified training setting for several tasks by sharing the core parameters. Besides, machine transliteration has a long history of using phonetic information, for example, by mapping a phrase to its pronunciation in the source language and then convert the sound to the target word (Knight and Graehl, 1997). There is also relevant work that uses both graphemes and phonemes to various extents for transliteration, such as the correspondence-based (Oh et al., 2006) and G2P-based (Le and Sadat, 2018) approaches. Our work is inspired by the intu385 itive understanding that pronunciation is essential for transliteration, and the success of incorporating phonetic information such as Pinyin (Jiang et al., 2009) and IPA (Salam et al., 2011), in the model design. 9 Conclusion We argue in this paper that language-specific features should be used when solving transliteration in a neural setting, and we exem"
2020.aacl-main.40,W18-2414,0,0.0218335,"ng et al., 2016; Dong et al., 2015; Li et al., 2014). In most of this work, underlies a similar idea to create a unified training setting for several tasks by sharing the core parameters. Besides, machine transliteration has a long history of using phonetic information, for example, by mapping a phrase to its pronunciation in the source language and then convert the sound to the target word (Knight and Graehl, 1997). There is also relevant work that uses both graphemes and phonemes to various extents for transliteration, such as the correspondence-based (Oh et al., 2006) and G2P-based (Le and Sadat, 2018) approaches. Our work is inspired by the intu385 itive understanding that pronunciation is essential for transliteration, and the success of incorporating phonetic information such as Pinyin (Jiang et al., 2009) and IPA (Salam et al., 2011), in the model design. 9 Conclusion We argue in this paper that language-specific features should be used when solving transliteration in a neural setting, and we exemplify a way of using phonetic information as the transferred knowledge to improve a neural machine transliteration system. Our results demonstrate that the main transliteration task and the aux"
2020.aacl-main.40,C18-1053,0,0.0274322,"米. Each row presents a group of corresponding subsequences in different representations. Introduction Transliteration, the act of mapping a name from the orthographic system of one language to another, is directed by the pronunciation in the source and target languages, and often by historical reasons or conventions. It plays an important role in tasks like information retrieval and machine translation (Marton and Zitouni, 2014; Hermjakob et al., 2008). Over the recent years, many have addressed transliteration using sequence-to-sequence (seq2seq) deep learning models (Rosca and Breuel, 2016; Merhav and Ash, 2018; Grundkiewicz and Heafield, 2018), enhanced with several NMT techniques (Grundkiewicz and Heafield, 2018). However, this recent work neglects the most crucial feature for transliteration, i.e. pronunciation. To * Work done at The University of Edinburgh. Our code and data are available at https://github. com/Lawhy/Multi-task-NMTransliteration. 1 bridge this gap, we define a phonetic auxiliary task that shares the sound information with the main transliteration task under the multi-task learning (MTL) setting. Depending on the specific language, the written form of a word reveals its pronuncia"
2020.aacl-main.40,W17-4710,0,0.0122169,"sly. 6.4 Test-set Results and System Comparison We submit our 1-best transliteration results on the NEWS official test set through the CodaLab link provided by the Shared Task’s Committee and we present the leaderboard partially in Table 9. Note that in addition to ACC+, the leaderboard also records mean F-score16 on which we rank first. We report the test-set performance of our best multi-task model on NEWS in Table 7 and DICT in Table 8, in comparison to the system built by Grundkiewicz and Heafield (2018). The baseline model of their work employs the RNN-based BiDeep17 architecture (Miceli Barone et al., 2017) which consists of 4 bidirectional alternating stacked encoder, each with a 2-layer transition RNN cell, and 4 stacked decoders with base RNN of depth 2 and higher RNN of depth 4 (Zhou et al., 2016; Pascanu et al., 2014; Wu et al., 2016). Besides, they strengthen the model by applying layer normalization (Ba et al., 2016), skip connections (Zhang et al., 2016) and parameter tying (Press and Wolf, 2017). We reproduce their model without changing any configurations in their paper (Grundkiewicz and Heafield, 2018), and train it on both tasks separately. In Table 7, we can see that the multi-task"
2020.aacl-main.40,D19-1212,1,0.82711,"伊 does not have any source-word correspondence if we consider the pronunciation of the source name. Finally, our model can be generalized to other transliteration tasks by replacing Pinyin with other phonetic representations such as IPA for English and r¯omaji for Japanese. In addition, ACC-ACT can be extended to alphabetical languages by, for instance, constructing the Alternating Sub-word Table which stores lists of interchangeable subsequences. Another possible future work is to redesign the objective function by treating λ as a trainable parameter or including the correlation information (Papasarantopoulos et al., 2019). 8 Related Work Previous work has demonstrated the effectiveness of using MTL on models through joint learning of various NLP tasks such as machine translation, syntactic and dependency parsing (Luong et al., 2016; Dong et al., 2015; Li et al., 2014). In most of this work, underlies a similar idea to create a unified training setting for several tasks by sharing the core parameters. Besides, machine transliteration has a long history of using phonetic information, for example, by mapping a phrase to its pronunciation in the source language and then convert the sound to the target word (Knight"
2020.aacl-main.40,E17-2025,0,0.029792,"on NEWS in Table 7 and DICT in Table 8, in comparison to the system built by Grundkiewicz and Heafield (2018). The baseline model of their work employs the RNN-based BiDeep17 architecture (Miceli Barone et al., 2017) which consists of 4 bidirectional alternating stacked encoder, each with a 2-layer transition RNN cell, and 4 stacked decoders with base RNN of depth 2 and higher RNN of depth 4 (Zhou et al., 2016; Pascanu et al., 2014; Wu et al., 2016). Besides, they strengthen the model by applying layer normalization (Ba et al., 2016), skip connections (Zhang et al., 2016) and parameter tying (Press and Wolf, 2017). We reproduce their model without changing any configurations in their paper (Grundkiewicz and Heafield, 2018), and train it on both tasks separately. In Table 7, we can see that the multi-task model performs significantly better than both the singletask baseline and the BiDeep model in all metrics on NEWS. Note that the BiDeep model we reproduce achieves the same ACC+ as reported in the work of Grundkiewicz and Heafield (2018) and ACC+ is the only evaluation metric used in their paper. “BiDeep+” in the third row refers to the final system they submitted to the Shared Task, on which they adop"
2020.aacl-main.40,E17-3017,0,0.0347676,"Missing"
2020.aacl-main.40,P06-1010,0,0.0899042,"presentation before transliterating a Chinese name to English. In contrast, our idea is to build a model with a shared encoder and dual decoders, that can learn the mapping from English to Chinese and Pinyin simultaneously. By jointly learning source-to-target and source-to-sound mappings, the encoder is expected to generalize better (Ruder, 2017) and pass more refined information to the decoders. Transliteration datasets are often extracted from dictionaries, or aligned corpus generated from applying named entity recognition (NER) system to parallel newspaper articles in different languages (Sproat et al., 2006). We use two datasets for our experiments, one taken from NEWS Machine Transliteration Shared Task (Chen et al., 2018) and the other extracted from a large dictionary. We evaluate the transliteration system using both the conventional word accuracy and a novel metric designed for English-to-Chinese transliteration (see Section 5). Our contributions are as follows: Target (y) Pinyin (p) Caleigh 凯莉 kai li Table 2: An example data point under our multi-task learning setting. the same model sizes, we report accuracy of 0.729 as compared to their 0.732. 2 Problem Formulation We use the word vocabul"
2020.aacl-main.40,Q16-1027,0,0.0263159,"nt the leaderboard partially in Table 9. Note that in addition to ACC+, the leaderboard also records mean F-score16 on which we rank first. We report the test-set performance of our best multi-task model on NEWS in Table 7 and DICT in Table 8, in comparison to the system built by Grundkiewicz and Heafield (2018). The baseline model of their work employs the RNN-based BiDeep17 architecture (Miceli Barone et al., 2017) which consists of 4 bidirectional alternating stacked encoder, each with a 2-layer transition RNN cell, and 4 stacked decoders with base RNN of depth 2 and higher RNN of depth 4 (Zhou et al., 2016; Pascanu et al., 2014; Wu et al., 2016). Besides, they strengthen the model by applying layer normalization (Ba et al., 2016), skip connections (Zhang et al., 2016) and parameter tying (Press and Wolf, 2017). We reproduce their model without changing any configurations in their paper (Grundkiewicz and Heafield, 2018), and train it on both tasks separately. In Table 7, we can see that the multi-task model performs significantly better than both the singletask baseline and the BiDeep model in all metrics on NEWS. Note that the BiDeep model we reproduce achieves the same ACC+ as reported in the"
2020.acl-main.129,W03-2123,0,0.121142,"ced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment. Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on outof-domain data. 1 Introduction The dialog manager (DM) is the brain of a taskoriented dialog system. Given the information it has received or gleaned from a user, it decides how to respond. Typically, this module is composed of an extensive set of hand-crafted rules covering the decision tree of a dialog (Litman and Allen, 1987; Bos et al., 2003). To circumvent the high development cost of writing and maintaining these rules there have been efforts to automatically learn a dialog manager using reinforcement learning (RL; Walker 2000; Young et al. 2013). RL solves problems of optimal control – where past predictions affect future states – making it well-suited to dialog management, in which a misstep by the agent can throw the whole dialog off course. But using RL to train a dialog manager is not straightforward, and is often hindered by large dialog state spaces and sparse rewards (Gao et al., 2019). *equal contribution Figure 1: Illu"
2020.acl-main.129,D18-1547,0,0.0515071,"Missing"
2020.acl-main.129,W14-4012,0,0.0978127,"Missing"
2020.acl-main.129,W17-5526,0,0.0398844,"Missing"
2020.acl-main.129,W16-3613,0,0.400672,"Missing"
2020.acl-main.129,I17-1074,0,0.0309965,"hrow the whole dialog off course. But using RL to train a dialog manager is not straightforward, and is often hindered by large dialog state spaces and sparse rewards (Gao et al., 2019). *equal contribution Figure 1: Illustration of reinforcement learning for dialog management. The agent (top right) interacts with the environment (left) by taking actions, and observing the resulting new state and reward. DQfD and RoFL RL agents are guided by an expert demonstrator (bottom right). Neural network-based deep RL (Mnih et al., 2015) mitigates the problem of large state spaces (Fatemi et al., 2016; Li et al., 2017) but it still struggles when the DM has to choose a response – or action – across multiple domains (e.g. hotel and flight booking). In addition, deep RL performs poorly without regular feedback – or reward – on the correctness of its decisions. In a dialog there is no obvious way to automatically quantify the appropriateness of each response, so RL training environments for dialog managers usually wait until conversation-end before assigning a reward based on whether the user’s task, or goal, was completed. An established way to deal with these difficulties is to guide the dialog manager with"
2020.acl-main.129,N18-1187,0,0.0167763,"dialog transitions generated during training to update the expert’s weights, adapting the previously learned knowledge to the learning environment. Our experiments show that RoFL training improves demonstrations gathered from the employed experts, giving a boost in RL performance and hastening convergence. 2 porate external expertise into the dialog manager. One alternative is to use supervised learning to train a neural network policy on an in-domain dialog dataset, and then fine-tune it with policygradient RL on a user-simulator (Su et al., 2016; Williams et al., 2017; Liu and Lane, 2017). Liu et al. (2018) fine-tune their RL policy on human rather than simulated users. Another, parallel, approach to RL-based DMs aims to increase the frequency of meaningful rewards. Takanobu et al. (2019) use inverse RL to learn a dense reward based on a dialog corpus, while Lu et al. (2019) decompose the task into subgoals that can be regularly assessed. Weak demonstrations have been used outside of dialog system research to tackle RL environments with large state spaces and sparse rewards. Aytar et al. (2018) train an expert to imitate YouTube videos of people playing challenging Atari games and exceed human-l"
2020.acl-main.129,D14-1162,0,0.0834923,"vie, restaurant and taxi booking domains – and Maluuba Frames (El Asri et al., 2017) which is made up of 1,369 dialogs from the flight and hotel booking domains. While three of these domains are also in MultiWOZ, the specifics of the conversations are different. Our Full Label Expert is a feedforward neural network (FFN) with one 150 dimensional hidden layer, ReLU activation function and 0.1 dropout which takes the current dialog state as input. The Reduced Label Expert uses the last utterance in the conversation as context, which is embedded with 300 dimensional pre-trained GloVe embeddings (Pennington et al., 2014), then passed through a uni-directional 128 dimensional hidden layer GRU (Cho et al., 2014) from which the last hidden state is used to make a multi-label prediction. Finally, our No Label Expert uses pre-trained BERTbase-uncased (Devlin et al., 2018) to embed and concatenate user and agent utterances into 1536dimensional input vectors, and employs a feedforward neural network with SELU activations (Klambauer et al., 2017) to predict whether the agent’s response is an appropriate answer to the last user utterance. Note that the RLE and NLE both take natural language as input yet use different"
2020.acl-main.129,N07-2038,0,0.0695334,". RoFL is agnostic to the expert in question and we apply it to each of our methods described above. We evaluate our weak experts in ConvLab (Lee et al., 2019), a multi-domain dialog framework based on the MultiWOZ dataset (Budzianowski et al., 2018). In ConvLab, the dialog manager’s task is to help a user plan and book a trip around a city, a problem that spans multiple domains ranging from recommending attractions for sightseeing, to booking transportation (taxi and train) and hotel accommodation. ConvLab supports RL training with an environment that includes an agenda-based user-simulator (Schatzmann et al., 2007) and a database. The agent has a binary dialog state that encodes the task-relevant information that the environment has provided so far. This state has 392 elements yielding a state space of size 2392 . In each state there are 300 actions that the DM can choose between, corresponding to different system responses when verbalized by the Natural Language Generation (NLG) module. These actions are composite and can consist of several individual informs and requests. For example, [attraction-inform-name, Experimental Setup attraction-request-area] is one action. We train our DMs on the exact dial"
2020.acl-main.129,D19-1010,0,0.216092,"training improves demonstrations gathered from the employed experts, giving a boost in RL performance and hastening convergence. 2 porate external expertise into the dialog manager. One alternative is to use supervised learning to train a neural network policy on an in-domain dialog dataset, and then fine-tune it with policygradient RL on a user-simulator (Su et al., 2016; Williams et al., 2017; Liu and Lane, 2017). Liu et al. (2018) fine-tune their RL policy on human rather than simulated users. Another, parallel, approach to RL-based DMs aims to increase the frequency of meaningful rewards. Takanobu et al. (2019) use inverse RL to learn a dense reward based on a dialog corpus, while Lu et al. (2019) decompose the task into subgoals that can be regularly assessed. Weak demonstrations have been used outside of dialog system research to tackle RL environments with large state spaces and sparse rewards. Aytar et al. (2018) train an expert to imitate YouTube videos of people playing challenging Atari games and exceed human-level performance. Salimans and Chen (2018) beat their score on Montezuma’s Revenge using only a single human demonstration, resetting the environment to different states from the expert"
2020.acl-main.129,P17-1062,0,\N,Missing
2020.acl-main.416,E17-2039,0,0.0421148,"Missing"
2020.acl-main.416,W13-2322,0,0.0171379,"cross sentences. The basic meaning-carrying units in DRT are Discourse Representation Structures (DRSs). They consist of discourse referents (e.g., x1 , x2 ) representing entities in the discourse and conditions (e.g., male.n.02(x1 ), Agent(e1 , x1 )) representing information about discourse referents. Every variable and condition are bounded by a box label (e.g., b1 ) which implies that the variable or condition are interpreted in that box. DRSs are constructed recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b"
2020.acl-main.416,W13-2101,0,0.0301587,"more consistent with human evaluation than S MATCH (Cai and Knight, 2013b), an AMR metric which is the basis of C OUNTER. S EM B LEU cannot be directly used on DRS graphs due to the large amount of indexed variables and the fact that the graphs are not explicitly given; moreover, our metric outputs F1 scores instead of precision only. Opitz et al. (2020) propose a set of principles for AMR-related metrics, showing the advantages and drawbacks of alignment- and BLEU-based AMR metrics. However, efficiency of the metric is crucial for the development of document-level models of semantic parsing. Basile and Bos (2013) propose to represent DRSs via Discourse Representation Graphs (DRGs) which are acyclic and directed. However, DRGs are similar to flattened trees, and not able to capture clause-level information (e.g., b1 Agent e1 x1 ) required for evaluation (van Noord et al., 2018a). 5 Conclusions In this work we proposed D SCORER, as a DRS evaluation metric alternative to C OUNTER. Our metric is significantly more efficient than C OUNTER and considers high-order DRSs. D SCORER allows to speed up model selection and development removing the bottleneck of evaluation time. Acknowledgments We thank the anonym"
2020.acl-main.416,W15-1841,0,0.0243534,"hboring nodes and connecting edges. For example, the two E nodes are differExperiments In our experiments, we investigate the correlation between D SCORER and C OUNTER, and the efficiency of the two metrics. We present results on two datasets, namely the Groningen Meaning Bank (GMB; Bos et al. 2017) and the Parallel Meaning Bank (PMB; Abzianidze et al. 2017). We compare two published systems on the GMB: DRTS-sent which is a sentence-level parser (Liu et al., 2018) and DRTS-doc which is a documentlevel parser (Liu et al., 2019a). On the PMB, we compare seven systems: Boxer, a CCG-based parser (Bos, 2015), AMR2DRS, a rule-based parser that converts AMRs to DRSs, SIM-SPAR giving the DRS in the training set most similar to the current DRS, SPAR giving a fixed DRS for each sentence, seq2seq-char, a character-based sequence-tosequence clause parser (van Noord et al., 2018b), seq2seq-word, a word-based sequence-to-sequence clause parser, and a transformer-based clause parser (Liu et al., 2019b). 3.1 Metric Settings C OUNTER takes 100 hill-climbing restarts to search for the best variable mappings on PMB and 10 restarts on GMB. Both D SCORER and C OUNTER are computed on one CPU (2.10GHz). The weight"
2020.acl-main.416,P13-2131,0,0.317625,"automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this alignment is NP-complete, similar to other metrics such as S MATCH (Cai and Knight, 2013a) for Abstract Meaning Representation. C OUNTER uses a greedy hill-climbing algorithm to obtain one-to-one variable mappings, and then computes precision, recall, and F1 scores according to the overlap of clauses between two DRSs. To get around the problem of search errors, the hill-climbing search implementation applies several random restarts. This incurs unacceptable runtime, especially when evaluating document-level DRSs with a large number of variables. Another problem with the current evaluation is that C OUNTER only considers local clauses without taking larger window sizes into accoun"
2020.acl-main.416,W19-1202,0,0.0120492,"ions (e.g., male.n.02(x1 ), Agent(e1 , x1 )) representing information about discourse referents. Every variable and condition are bounded by a box label (e.g., b1 ) which implies that the variable or condition are interpreted in that box. DRSs are constructed recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this ali"
2020.acl-main.416,P18-1040,1,0.931081,"ucted recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this alignment is NP-complete, similar to other metrics such as S MATCH (Cai and Knight, 2013a) for Abstract Meaning Representation. C OUNTER uses a greedy hill-climbing algorithm to obtain one-to-one variable mappings, and then computes precision, recall, and F1 sc"
2020.acl-main.416,P19-1629,1,0.800877,"ale.n.02(x1 ), Agent(e1 , x1 )) representing information about discourse referents. Every variable and condition are bounded by a box label (e.g., b1 ) which implies that the variable or condition are interpreted in that box. DRSs are constructed recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this alignment is NP-compl"
2020.acl-main.416,W19-1203,1,0.905421,"ale.n.02(x1 ), Agent(e1 , x1 )) representing information about discourse referents. Every variable and condition are bounded by a box label (e.g., b1 ) which implies that the variable or condition are interpreted in that box. DRSs are constructed recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this alignment is NP-compl"
2020.acl-main.416,W19-1204,0,0.0208265,"Missing"
2020.acl-main.416,L18-1267,0,0.0327133,"Missing"
2020.acl-main.416,Q18-1043,0,0.32164,"Missing"
2020.acl-main.416,2020.tacl-1.34,0,0.0135934,"as well. And the mismatch of the second path reduces the final score. 4 Related Work The metric S EM B LEU (Song and Gildea, 2019) is most closely related to ours. It evaluates AMR graphs by calculating precision based on n-gram overlap. S EM B LEU yields scores more consistent with human evaluation than S MATCH (Cai and Knight, 2013b), an AMR metric which is the basis of C OUNTER. S EM B LEU cannot be directly used on DRS graphs due to the large amount of indexed variables and the fact that the graphs are not explicitly given; moreover, our metric outputs F1 scores instead of precision only. Opitz et al. (2020) propose a set of principles for AMR-related metrics, showing the advantages and drawbacks of alignment- and BLEU-based AMR metrics. However, efficiency of the metric is crucial for the development of document-level models of semantic parsing. Basile and Bos (2013) propose to represent DRSs via Discourse Representation Graphs (DRGs) which are acyclic and directed. However, DRGs are similar to flattened trees, and not able to capture clause-level information (e.g., b1 Agent e1 x1 ) required for evaluation (van Noord et al., 2018a). 5 Conclusions In this work we proposed D SCORER, as a DRS evalu"
2020.acl-main.416,P02-1040,0,0.107989,"ntire documents. In order to address the above issues, we propose D SCORER, a highly efficient metric for the evalu4547 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4547–4554 c July 5 - 10, 2020. 2020 Association for Computational Linguistics He didn’t play the piano. But she sang. ation of DRS parsing on texts of arbitrary length. D SCORER converts DRSs (predicted and gold) to graphs from which it extracts n-grams, and then computes precision, recall and F1 scores between them. The algorithm operates over n-grams in a fashion similar to BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which are metrics widely used for evaluating the output of machine translation and summarization systems. While BLEU only calculates precision with a brevity penalty (it is not straightforward to define recall given the wide range of possible translations for a given input), ROUGE is a recall-oriented metric since the summary length is typically constrained by a prespecified budget.1 However, in DRS parsing, there is a single correct semantic representation (goldstandard reference) and no limit on the maximum size of DRSs. Our proposed metric, D SCORER, converts box-sty"
2020.acl-main.416,P19-1446,0,0.0367455,"Missing"
2020.emnlp-main.169,W13-2322,0,0.159262,": The concept (join-01) in vanilla GCNs is that it only captures information from its immediate neighbors (first-order), while in LDGCNs it can integrate information from neighbors of different order (e.g., second-order and third-order). In SANs, the node collects information from all other nodes, while in structured SANs it is aware of its connected nodes in the original graph. Introduction Graph structures play a pivotal role in NLP because they are able to capture particularly rich structural information. For example, Figure 1 shows a directed, labeled Abstract Meaning Representation (AMR; Banarescu et al. 2013) graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016"
2020.emnlp-main.169,P18-1026,0,0.626864,"the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlo"
2020.emnlp-main.169,D19-1378,0,0.0186021,"l., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their constructive comments. We would also like t"
2020.emnlp-main.169,P19-1448,0,0.0224765,"l., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their constructive comments. We would also like t"
2020.emnlp-main.169,2020.acl-main.640,0,0.307027,"aph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alternative to capture"
2020.emnlp-main.169,N19-1223,0,0.143753,"al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020) GT GRU (Cai and Lam, 2020) GT SAN (Zhu et al., 2019) Single Single Single Single LDGCN WT LDGCN GC Single Single C M AMR2017 #P 23.5 23.3 24.4 23.6 25.7 0 54.5‡ 031.5‡ 018.6M‡ 24.3 053.8‡ 30.5 060.3M‡ - - B M #P 26.8 24.9 23.3 50.4 28.3M 2"
2020.emnlp-main.169,W14-3348,0,0.00949318,"and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020) GT GRU (Cai and Lam, 2020) GT SAN (Zhu et al., 2019)"
2020.emnlp-main.169,E17-3017,0,0.0226855,"max( (hu WQ )(hv WK )T √ ) d (2) where WQ and WK are projection parameters. The adjacency matrix A in GCNs is given by the input AMR graph, while in SANs A is computed based on H, which neglects the structural information of the input AMR. The number of operations required by graph convolutions scales is found linearly in the input length, whereas they are quadratic for SANs. Structured SANs Zhu et al. (2019) and Cai and Lam (2020) extend SAN s by incorporating the relation ruv between node u and node v in the 1 Our implementation is based on MXNET (Chen et al., 2015) and the Sockeye toolkit (Felix et al., 2017). 2163 Hl A1 X Wl Hl+1 X Vanilla GCN Layer Hl A1 Wl X Hl A2 X X Wl X Dynamic Fusion Hl+1 LDGCN Layer Figure 2: Comparison between vanilla GCNs and LDGCNs. Hl denotes the representation of l-th layer. Wl denotes the trainable weights and × denotes matrix multiplication. Vanilla GCNs take the 1st-order adjacency matrix A1 as the input, which only captures information from one-hop neighbors. LDGCNs take k number of k-order adjacency matrix Ak as inputs, Wl is shared for all Ak . k is set to 2 here for simplification. A dynamic fusion mechanism is applied to integrate the information from 1- to k-"
2020.emnlp-main.169,N16-1087,0,0.0142735,"f the sentence, i.e., “rather than let them get even worse”, but it fails to capture the meaning of word “early” in its output, which is a critical part. DeepGCN parses both “early” and “get even worse” in the results. However, the readability of the generated sentence is not satisfactory. Compared to baselines, LDGCN is able to produce the best result, which has a correct starting phrase and captures the semantic meaning of critical words such as “early” and “get even worse” while also attaining good readability. 6 Early efforts for AMR-to-text generation mainly include grammar-based models (Flanigan et al., 2016; Song et al., 2017) and sequence-based models (Pourdamghani et al., 2016; Konstas et al., 2017; Cao and Clark, 2019), discarding crucial structural information when linearising the input AMR graph. To solve that, various GNNs including graph recurrent networks (Song et al., 2018; Ribeiro et al., 2019) and graph convolutional networks (Damonte and Cohen, 2019; Guo et al., 2019b) have been used to encode the AMR structure. Though GNNs are able to operate directly on graphs, the locality nature of them precludes efficient information propagation (Abu-El-Haija et al., 2018, 2019; Luan et al., 201"
2020.emnlp-main.169,P19-1024,1,0.166026,"information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswa"
2020.emnlp-main.169,Q19-1019,1,0.13775,"information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswa"
2020.emnlp-main.169,N19-1366,1,0.811253,"onding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et"
2020.emnlp-main.169,W04-3250,0,0.196269,"Missing"
2020.emnlp-main.169,2020.findings-emnlp.199,1,0.492865,"erly. As shown in Table 6, LDGCN GC has better human rankings in terms of both meaning similarity and readability than the state-of-the art GNN-based (DualGraph) and SAN-based model (GT SAN). DeepGCN without dynamic fusion mechanism obtains lower scores than GT SAN, which further confirms that synthesizing higher order information helps in learning better graph representations. 5.5 0-20 Additional Analysis To further reveal the source of performance gains, we perform additional analysis based on the characteristics of AMR graphs, i.e., graph size and graph reentrancy (Damonte and Cohen, 2019; Damonte et al., 2020). All experiments are conducted on the AMR2.0 test set and CHRF++ scores are reported. CHRF++ 5.4 58 64 58 0-1 2-3 4-5 Graph Reentrancies &gt;5 Figure 6: Performance against graph re-entrancies. Graph Size. As shown in Figure 5, the size of AMR graphs is partitioned into four categories ((0, 20], (20, 30], (30, 40], &gt; 40), Overall, LDGCN GC outperforms the best-reported GT SAN model across all graph sizes, and the performance gap becomes more profound with the increase of graph sizes. Although both models have sharp performance degradation for extremely large graphs (&gt; 40), the performance of LDG"
2020.emnlp-main.169,P17-1014,0,0.450535,"raph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps captur"
2020.emnlp-main.169,N19-1240,0,0.0608384,"Missing"
2020.emnlp-main.169,D19-1282,0,0.0218573,"mplex non-local interactions (Xu et al., 2018; Li et al., 2019a). More recently, SANbased models (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) outperform GNN-based models as they are able to capture global dependencies. Unlike previous models, our local, yet efficient model, based solely on graph convolutions, outperforms competitive structured SANs while using a significantly smaller model. Related Work Graph convolutional networks (Kipf and Welling, 2017) have been widely used as the structural encoder in various NLP applications including question answering (De Cao et al., 2019; Lin et al., 2019), semantic parsing (Bogin et al., 2019a,b) and relation extraction (Guo et al., 2019a, 2020). Conclusion In this paper, we propose LDGCNs for AMR-totext generation. Compared with existing GCNs and SANs, LDGCNs maintain a better balance between parameter efficiency and model capacity. LDGCNs outperform state-of-the-art models on AMR-to-text generation. In future work, we would like to investigate methods to stabilize the training of weight tied models and apply our model on other tasks in Natural Language Generation. Acknowledgments We would like to thank the anonymous reviewers for their const"
2020.emnlp-main.169,D19-1548,0,0.0984317,"g it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alt"
2020.emnlp-main.169,P02-1040,0,0.106582,"dynamic fusion mechanism, N =2 for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT"
2020.emnlp-main.169,W17-4770,0,0.024584,"for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damonte and Cohen, 2019). Following Wang et al. (2020), we use a transformer as the decoder for large-scale evaluation. For fair comparisons, we use the same optimization and regularization strategies as in Guo et al. (2019b). All hyperparameters are tuned on the development set2 . For evaluation, we report BLEU scores (Papineni et al., 2002), CHRF++ (Popovic, 2017) scores and METEOR scores (Denkowski and Lavie, 2014) with additional human evaluation results. 2 Hyperparameter search; all hyperparameters are attached in the supplementary material. 2166 Model AMR2015 Type B Seq2Seq (Cao and Clark, 2019) GraphLSTM (Song et al., 2018) GGNNs (Beck et al., 2018) GCNLSTM (Damonte and Cohen, 2019) DCGCN (Guo et al., 2019b) DualGraph (Ribeiro et al., 2019) Single Single Single Single Single Single Seq2Seq (Konstas et al., 2017) GGNNs (Beck et al., 2018) DCGCN (Guo et al., 2019b) Ensemble Ensemble Ensemble Transformer (Zhu et al., 2019) GT Dual (Wang et al., 2020)"
2020.emnlp-main.169,W16-6603,0,0.247764,"R; Banarescu et al. 2013) graph, where each node denotes a semantic concept and each edge denotes a relation between such concepts. Within ∗ Equally Contributed. Work done while Yan Zhang was an intern at DAMO Academy, Alibaba Group and Zhijiang Guo was at the University of Edinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional laye"
2020.emnlp-main.169,D19-1314,0,0.452439,"of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings o"
2020.emnlp-main.169,P16-1162,0,0.0167726,"p convolutional neural networks. Formally, the Hf inal of LDGCNs which have L layer ˆ L , ..., H ˆ 1 ), where is obtained by: Hf inal = F(H F is a linear transformation. 5 5.1 Experiments Setup We evaluate our model on the LDC2015E86 (AMR1.0), LDC2017T10 (AMR2.0) and LDC2020T02 (AMR3.0) datasets, which have 16,833, 36,521 and 55,635 instances for training, respectively. Both AMR1.0 and AMR2.0 have 1,368 instances for development, and 1,371 instances for testing. AMR3.0 has 1,722 instances for development and 1,898 instances for testing. Following Zhu et al. (2019), we use byte pair encodings (Sennrich et al., 2016) to deal with rare words. Following Guo et al. (2019b), we stack 4 LDGCN blocks as the encoder of our model. Each block consists of two sub-blocks where the bottom one contains 6 layers and the top one contains 3 layers. The hidden dimension of LDGCN model is 480. Other model hyperparameters are set as λ=0.7, K=2 for dynamic fusion mechanism, N =2 for depthwise graph convolutions and M =6 and 3 for layerwise graph convolutions for the bottom and top sub-blocks, respectively. For the decoder, we employ the same attention-based LSTM as in previous work (Beck et al., 2018; Guo et al., 2019b; Damo"
2020.emnlp-main.169,P17-2002,0,0.0176185,"rather than let them get even worse”, but it fails to capture the meaning of word “early” in its output, which is a critical part. DeepGCN parses both “early” and “get even worse” in the results. However, the readability of the generated sentence is not satisfactory. Compared to baselines, LDGCN is able to produce the best result, which has a correct starting phrase and captures the semantic meaning of critical words such as “early” and “get even worse” while also attaining good readability. 6 Early efforts for AMR-to-text generation mainly include grammar-based models (Flanigan et al., 2016; Song et al., 2017) and sequence-based models (Pourdamghani et al., 2016; Konstas et al., 2017; Cao and Clark, 2019), discarding crucial structural information when linearising the input AMR graph. To solve that, various GNNs including graph recurrent networks (Song et al., 2018; Ribeiro et al., 2019) and graph convolutional networks (Damonte and Cohen, 2019; Guo et al., 2019b) have been used to encode the AMR structure. Though GNNs are able to operate directly on graphs, the locality nature of them precludes efficient information propagation (Abu-El-Haija et al., 2018, 2019; Luan et al., 2019). Larger and deepe"
2020.emnlp-main.169,P18-1150,0,0.565052,"dinburgh. † Corresponding author. the realm of work on AMR, we focus in this paper on the problem of AMR-to-text generation, i.e. transducing AMR graphs into text that conveys the information in the AMR structure. A key challenge in this task is to efficiently learn useful representations of the AMR graphs. Early efforts (Pourdamghani et al., 2016; Konstas et al., 2017) neglect a significant part of the structural information in the input graph by linearizing it. Recently, Graph Neural Networks (GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information pro"
2020.emnlp-main.169,2020.tacl-1.2,0,0.411099,"(GNNs) have been explored to better encode structural information for this task (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019). One type of such GNNs is Graph Convolutional Networks (GCNs; Kipf and Welling 2017). GCNs follow a local information aggregation scheme, iteratively updating the representations of nodes based on their immediate (first-order) neighbors. Intuitively, stacking more convolutional layers in GCNs helps capture more complex interactions (Xu et al., 2018; Guo et al., 2019b). However, prior efforts (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020) have shown that the locality property of existing GCNs precludes efficient nonlocal information propagation. Abu-El-Haija et al. 2162 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2162–2172, c November 16–20, 2020. 2020 Association for Computational Linguistics (2019) further proved that vanilla GCNs are unable to capture feature differences among neighbors from different orders no matter how many layers are stacked. Therefore, Self-Attention Networks (SANs; Vaswani et al. 2017) have been explored as an alternative to capture global dependencies"
2020.emnlp-main.245,P19-1285,0,0.0491807,"Missing"
2020.emnlp-main.245,N19-1423,0,0.0346298,"bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage (background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension model that performs chai"
2020.emnlp-main.245,N19-1246,1,0.875678,"Missing"
2020.emnlp-main.245,P19-1222,0,0.0170661,"omprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differ"
2020.emnlp-main.245,N16-1181,0,0.225016,"he variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to understand and chain together free-form predicates and logical connectives. The proposed model is inspired by neural module networks (NMNs), which were proposed for visual question answering (Andreas et al., 2016b,a). NMNs assemble a network from a collection of specialized modules where each module performs some learnable function, such as locating a question word in an image, or recognizing relationships between objects in the image. The modules are composed together specific to what is asked in the question, then executed to obtain an answer. We design general modules that are targeted at the reasoning necessary for ROPES and compose them together to answer questions. We design three kinds of basic modules to learn the neuro-symbolic multi-step inference over questions, situations, and background p"
2020.emnlp-main.245,D19-5817,1,0.83692,"19; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to d"
2020.emnlp-main.245,D19-1455,0,0.0745637,"tion, then successively chains them, making a prediction after including the situation in the chaining. ∗ Work done during an internship at Allen Institute for Artificial Intelligence. Prior work addressing this problem has largely 3040 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3040–3050, c November 16–20, 2020. 2020 Association for Computational Linguistics either used symbolic reasoning, such as markov logic networks (Khot et al., 2015) and integer linear programming (Khashabi et al., 2016), or blackbox neural networks (Jiang et al., 2019; Jiang and Bansal, 2019). Symbolic methods give some measure of interpretability and the ability to handle logical operators to track polarity, but they are brittle, unable to handle the variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to understand and chain together fr"
2020.emnlp-main.245,P19-1261,0,0.0780502,"background and question, then successively chains them, making a prediction after including the situation in the chaining. ∗ Work done during an internship at Allen Institute for Artificial Intelligence. Prior work addressing this problem has largely 3040 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3040–3050, c November 16–20, 2020. 2020 Association for Computational Linguistics either used symbolic reasoning, such as markov logic networks (Khot et al., 2015) and integer linear programming (Khashabi et al., 2016), or blackbox neural networks (Jiang et al., 2019; Jiang and Bansal, 2019). Symbolic methods give some measure of interpretability and the ability to handle logical operators to track polarity, but they are brittle, unable to handle the variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to underst"
2020.emnlp-main.245,D15-1080,0,0.0761646,"Missing"
2020.emnlp-main.245,P18-1249,0,0.0998005,"tial experiments, and we performed an analysis of the data to figure out the cause. ROPES used an annotator split to separate the train, dev, and test sets in order to avoid annotator bias (Geva et al., 2019), but we discovered that this led to a large distributional shift between train/dev and test, which we explore in this section. In light of this analysis, we recommend treating the dev set as an in-domain test set, and the original test set as an out-of-domain test. Answer types Our analysis is based on looking at the syntactic category of the answer phrase. We use the syntactic parser of Kitaev and Klein (2018) to obtain constituent trees for the passages in ROPES. We take the constituent label of the lowest subtree that covers the answer span3 as the answer type. The four most frequent answer types in ROPES are noun phrase (NP), verb phrase (VP), adjective phrase (ADJP) and adverb phrase (ADVP). Table 1 shows examples for each type. Most NP answers 3 The passages could have more than one span that matches the answer; we use the last occurrence of the answer span for our analysis. 3043 Type NP VP ADJP ADVP Others Passage ...The child poured two spoonfuls of sugar into cup A and three spoonfuls of su"
2020.emnlp-main.245,D19-5808,1,0.903659,"ion: Which category of flowers would be more likely to have brightly colored petals? Introduction Answer: category B Performing chained inference over natural language text is a long-standing goal in artificial intelligence (Grosz et al., 1986; Reddy, 2003). This kind of inference requires understanding how natural language statements fit together in a way that permits drawing conclusions. This is very challenging without a formal model of the semantics underlying the text, and when polarity needs to be tracked across many statements. For instance, consider the example in Figure 1 from ROPES (Lin et al., 2019), a recently released reading comprehension dataset that requires applying information contained in a background paragraph to a new situation. To answer the question, one must associate each category of flowers with a polarity for having brightly colored petals, which must be done by going through the information about pollinators given in the situation and linking it to what was said about pollinators and brightly colored petals in the background paragraph, along with tracking the polarity of those statements. (a) background question situation S ELECT C HAIN C HAIN P REDICT category B (b) Fig"
2020.emnlp-main.245,2021.ccl-1.108,0,0.180833,"Missing"
2020.emnlp-main.245,P19-1613,0,0.0172757,"several reading comprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker"
2020.emnlp-main.245,N18-1202,1,0.510184,", where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage (background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension mod"
2020.emnlp-main.245,D16-1264,0,0.121792,"Missing"
2020.emnlp-main.245,D19-1374,0,0.01434,"background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension model that performs chained inference over natural language text. We have demonstrated that our model substantially outperforms prior work on ROPES, a challenging new reading comprehension dataset. We have additionally presented some analysis of ROPES that should inform future work on this dataset. While our model is not a neural module network, as our model uses"
2020.emnlp-main.245,P19-1260,0,0.0183596,"ize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage"
2020.emnlp-main.245,W18-5446,0,0.0697258,"Missing"
2020.emnlp-main.245,Q18-1021,0,0.0301516,"Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning"
2020.emnlp-main.245,D18-1259,0,0.0516665,"to say, the crash rate per cyclist goes down as the cycle volume increases... Situation: ...Day 1 had 500 cyclists left. Day 2 had Related Work 400 cyclists left. Day 3 had 300 cyclists left. Day 4 had Neural Module Networks were originally proposed for visual question answering tasks (Andreas et al., 2016b,a), and recently have been used on several reading comprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the docu"
2020.emnlp-main.245,P19-1009,0,0.0506588,"Missing"
2020.emnlp-main.245,P19-1218,0,0.021383,"20), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the eff"
2020.findings-emnlp.199,W13-2322,0,0.325217,"speak receive-01 :ARG0 I :ARG1 :ARG1 instruct-01 :ARG2 :ARG0 Verbalization I received instructions to act act-02 Table 1: Several linguistic phenomena causing reentrancies in AMR. 2200 in which the node it has two incoming edges, creating a reentrancy. As mentioned before, one would expect relative clauses to be one of the syntactic reentrancy triggers, because the noun involved has a semantic role in both the main and relative clause: (8) I saw the womani who i won. In the example above, the woman is the object of seeing and the subject of winning. However, according to the AMR guidelines (Banarescu et al., 2013) relative clauses should be annotated as attaching to the noun with an inverse role, thereby avoiding a reentrancy (see Table 1). Pragmatic triggers Human annotators resolve coreferences even in the absence of definite syntactic clues, giving rise to pragmatically triggered reentrancies. To this class belong for instance the cases of pronominal anaphora resolution where the anaphora is not syntactically bound (unlike in 1). While coreference is, in general, a discourse phenomenon (Hobbs, 1979), it is also applicable to individual sentences such as those in the AMR corpora: (9) The coach of FC"
2020.findings-emnlp.199,P02-1018,0,0.387757,"Missing"
2020.findings-emnlp.199,P17-4012,0,0.0136849,".00) O RACLE +10.3 (0.00) R ANDOM SEQ 2 SEQ -4.2 (0.06) -0.1 (0.25) Table 4: Relative improvements in reentrancy prediction scores on the test set of LDC2017T10, obtained by the oracle and the proposed baselines. VANILLA are the scores obtained by Lyu and Titov (2018). 5 Automatic Error Correction 7 We further provide baseline systems that learn when to apply A DD, the most impactful action. First, we experiment with a system that randomly selects two nodes in the predicted graph that are not connected by any edge and add an edge with ARG0, the most frequent label. We also train a OpenNMT-py (Klein et al., 2017) sequence-tosequence model (Bahdanau et al., 2015) with a copy mechanism (Gulcehre et al., 2016). The input sequence is the predicted graph and the output sequence is the sequence of edges to add. For each edge, the output contains three tokens: the parent node, the child node, and the edge label. Table 4 shows that the baselines do not improve the predictions of the original parsers (VANILLA). While sequence modeling of the output is convenient, other options can be attempted. We are also only exploiting the input AMR parse but not the input sentence. We leave it to future work to address the"
2020.findings-emnlp.199,Q15-1040,0,0.029067,", where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detailed discussion of the types and linguistic causes of reentrant structures. We aim to fill the gap by describing the phenomena causing reentrancies and quantifying their prevalence in the AMR corpus. We identify sources of reentrancy which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragmatics. AMR parsers are evaluated using Smatch (Cai and Knight, 2013), which however does not explicit"
2020.findings-emnlp.199,J95-4004,0,0.586869,"g good AMR parsing performance. Related Work Our classification of phenomena causing reentrancies extends previous work in this direction (Groschwitz et al., 2017). van Noord and Bos (2017) previously attempted to improve the prediction of reentrancies in a neural parser. They experiment with several pre- and post-processing techniques and showed that co-indexing reentrancies nodes in the AMR annotations yields the best results. Transformation-based learning (Brill, 1993) inspired the idea of correcting existing parses. This approach has been mostly used for tagging (Ramshaw and Marcus, 1999; Brill, 1995; Nguyen et al., 2016) but it has also shown promises for semantic parsing (Jurˇc´ıcˇ ek et al., 2009). A similar approach has been also used to add empty nodes in constituent parses (Johnson, Conclusions Building upon previous observations that AMR parsers do not perform well at recovering reentrancies, we analyzed the linguistic phenomena responsible for reentrancies in AMR. We found sources of reentrancies which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragmatics. The inclusion of reentrancies due to pragmatics is controversial; we hope th"
2020.findings-emnlp.199,J16-4009,0,0.0341884,"Missing"
2020.findings-emnlp.199,2020.acl-main.119,0,0.181779,"s. Our heuristics fail to detect the causes of many reentrancies. For a more precise estimate of the most common causes of reentrancies, it is necessary to manually annotate the reentrancies in the AMR corpora. Our oracle experiments show that there is room for improvement in predicting reentrancies, which in turn can translate to better parsing results. Stronger baselines that can learn how to correct the errors automatically are left to future work. While the parser we experimented with no longer gives state-of-the-art results (but also not far from them), newer parsers (Zhang et al., 2019; Cai and Lam, 2020) also report relatively low accuracy on reentrancies (using the metrics from Damonte et al. 2017), and as such we believe our work is relevant to these parsers. Acknowledgments The authors would like to thank anoymous reviewers, Adam Lopez, Bonnie Webber, Nathan Schneider, Sameer Bansal, and Yevgen Matusevych for their help and comments. This research was supported by a grant from Bloomberg as well as by the European Union H2020 project SUMMA, under grant agreement 688139 and the project SEMANTAX, which has received funding from the European Research Council (ERC) under the European 2206 Union"
2020.findings-emnlp.199,J13-2005,0,0.0275263,"ence of nodes with multiple parents, called reentrancies, as demonstrated in Figure 1, where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detailed discussion of the types and linguistic causes of reentrant structures. We aim to fill the gap by describing the phenomena causing reentrancies and quantifying their prevalence in the AMR corpus. We identify sources of reentrancy which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragm"
2020.findings-emnlp.199,P13-2131,0,0.0877441,"t an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detailed discussion of the types and linguistic causes of reentrant structures. We aim to fill the gap by describing the phenomena causing reentrancies and quantifying their prevalence in the AMR corpus. We identify sources of reentrancy which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragmatics. AMR parsers are evaluated using Smatch (Cai and Knight, 2013), which however does not explicitly assess the parsers’ ability to recover reentrancies. Damonte et al. (2017) introduced a measure of reentrancy prediction, which computes the Smatch score of the AMR subgraphs containing reentrancies. It was observed that the performance of parsers at recovering reentrancy structures is generally poor. We analyze errors made by the parsers and use an oracle to demonstrate that correcting reentrancy-related errors leads to parsing score improvement. Our contributions are as follows: Equal contribution Work done while at University of Edinburgh • We classify th"
2020.findings-emnlp.199,P18-1037,0,0.556171,"05.7 (3.21) 281.3 (5.51) 572.3 (4.04) 224.7 (3.06) +1.7 +0.7 +0.4 +0.2 +10.3 +3.1 -0.1 +0.8 M ERGE M ERGE - RMN S PLIT S PLIT- ADDN 187.3 (1.53) 94.3 (1.15) 574.7 (3.21) 333.0 (1.00) +0.4 +0.3 +1.2 +0.9 +1.6 +1.0 +1.8 -0.2 193.3 (3.06) 84.0 (2.00) 541.3 (4.16) 347.3 (3.79) +0.4 +0.2 +1.1 +0.9 +1.7 +0.9 +1.7 -0.0 A DD - SIB A DD - SIB - ADDN R M - SIB R M - SIB - RMN 128.0 (1.00) 99.7 (3.06) 69.3 (0.58) 0.0 (0.00) +0.2 +0.1 +0.1 +0.0 +1.3 -0.1 +0.2 -0.1 119.7 (1.15) 104.3 (1.53) 89.3 (0.58) 0.0 (0.00) +0.1 +0.1 +0.0 +0.0 +1.2 -0.0 +0.2 +0.0 Table 3: Relative Smatch improvements with respect to Lyu and Titov (2018) of all actions on the test split of LDC2015E86 and LDC2017T10. Freq. is the number of times the action could be applied, Smatch is the parsing score and Reent. is the reentrancies prediction score. A LL is the combination of all actions. VANILLA are the scores obtained by the original parsers. In parentheses, we report the standard deviation of the actions’ frequency. The standard deviation for the Smatch and reentrancy prediction scores is less or equal than 0.12. sa a) sb sc sa b) tb tb sa c) sb sc sb tc tb tc ta R EMOVE - SIB - RMN sc Results are shown in Table 3.9 While the largest improv"
2020.findings-emnlp.199,P17-1005,0,0.0210633,"with multiple parents, called reentrancies, as demonstrated in Figure 1, where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detailed discussion of the types and linguistic causes of reentrant structures. We aim to fill the gap by describing the phenomena causing reentrancies and quantifying their prevalence in the AMR corpus. We identify sources of reentrancy which have not been acknowledged in the AMR literature such as adjunct control, verbalization, and pragmatics. AMR parsers ar"
2020.findings-emnlp.199,E17-1051,1,0.942412,"graphs (Banarescu et al., 2013) — rooted and directed acyclic graphs where nodes represent concepts and edges represent semantic relations between them. The AMR for the sentence I want you to believe me is shown in Figure 1. One of the main properties of AMR, and the reason why sentences are represented as graphs rather than trees, is the presence of nodes with multiple parents, called reentrancies, as demonstrated in Figure 1, where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR literature lacks any detail"
2020.findings-emnlp.199,P14-1134,0,0.123863,"ism rather than by what is actually expressed by the sentence. There are other conventions besides verbalization which introduce reentrancies, in particular if inverse roles were normalized2 . Our choice to nor normalize edge direcionality was partially motivated by a desire to avoid including those phenomena in our analysis. 3 Quantifying Reentrancy Causes In order to assess the prevalence of the various reentrancy triggers, we designed heuristics to assign each reentrancy in the AMR corpus to one of the above phenomena. We automatically align AMR graphs to their source sentences using JAMR (Flanigan et al., 2014) and identify the spans of words associated with re-entrant nodes.3 Heuristics based on Universal Dependency (UD) parses (Manning et al., 2014) and automatic coreference resolution are applied to the spans and the AMR subgraphs containing the reentrancy to classify the cause.4 We use the NeuralCoref project for coreference resolution.5 We recognize syntactic reentrancy triggers primarily with UD-based heuristics. For prototypical cases of control we look for common con2 representation of ”-er” nouns with their corresponding predicate and a person node; the convention for representing governmen"
2020.findings-emnlp.199,W17-6810,0,0.281805,"s or ”-er” nouns), with relative clauses admittedly being an exception. With that in mind, we classify reentrancy triggers into three broad types: syntactic, pragmatic, and AMR-specific. Syntactic triggers We consider a reentrancy as syntactically triggered if the syntactic structure of a sentence forces an interpretation in which one entity performs more than one semantic role. Below we illustrate the syntactic triggers which are commonly discussed in the AMR literature: some types of pronominal anaphora resolution (1), prototypical subject and object control (3 and 4), and coordination (2) (Groschwitz et al., 2017; van Noord and Bos, 2017). (1) The mani saw himselfi in the mirror. (2) Shei ate and i drank. (3) Theyi want i to believe. (4) I asked youi i to sing. In addition to those, our inspection of the AMR data revealed that other kinds of control structures, primarily adjunct control, are frequent reentrancy triggers. In adjunct control, the clause which lacks a subject is an adjunct of the main clause, as in the following examples: (5) Ii went home before i eating. (6) Shei left the room i crying. Such adjuncts express various additional information regarding the main clause, for example the"
2020.findings-emnlp.199,P16-1014,0,0.0208376,"ments in reentrancy prediction scores on the test set of LDC2017T10, obtained by the oracle and the proposed baselines. VANILLA are the scores obtained by Lyu and Titov (2018). 5 Automatic Error Correction 7 We further provide baseline systems that learn when to apply A DD, the most impactful action. First, we experiment with a system that randomly selects two nodes in the predicted graph that are not connected by any edge and add an edge with ARG0, the most frequent label. We also train a OpenNMT-py (Klein et al., 2017) sequence-tosequence model (Bahdanau et al., 2015) with a copy mechanism (Gulcehre et al., 2016). The input sequence is the predicted graph and the output sequence is the sequence of edges to add. For each edge, the output contains three tokens: the parent node, the child node, and the edge label. Table 4 shows that the baselines do not improve the predictions of the original parsers (VANILLA). While sequence modeling of the output is convenient, other options can be attempted. We are also only exploiting the input AMR parse but not the input sentence. We leave it to future work to address these issues and achieve better results. 6 2002), with considerable success. The SEQ 2 SEQ baseline"
2020.findings-emnlp.199,P14-5010,0,0.00301277,"n particular if inverse roles were normalized2 . Our choice to nor normalize edge direcionality was partially motivated by a desire to avoid including those phenomena in our analysis. 3 Quantifying Reentrancy Causes In order to assess the prevalence of the various reentrancy triggers, we designed heuristics to assign each reentrancy in the AMR corpus to one of the above phenomena. We automatically align AMR graphs to their source sentences using JAMR (Flanigan et al., 2014) and identify the spans of words associated with re-entrant nodes.3 Heuristics based on Universal Dependency (UD) parses (Manning et al., 2014) and automatic coreference resolution are applied to the spans and the AMR subgraphs containing the reentrancy to classify the cause.4 We use the NeuralCoref project for coreference resolution.5 We recognize syntactic reentrancy triggers primarily with UD-based heuristics. For prototypical cases of control we look for common con2 representation of ”-er” nouns with their corresponding predicate and a person node; the convention for representing government; special frames for roles 3 https://github.com/jflanigan/jamr 4 https://stanfordnlp.github.io/CoreNLP 5 https://github.com/huggingface/neural"
2020.findings-emnlp.199,W17-7306,0,0.176461,"Missing"
2020.findings-emnlp.199,P15-2141,0,0.0444994,"sentences into AMR graphs (Banarescu et al., 2013) — rooted and directed acyclic graphs where nodes represent concepts and edges represent semantic relations between them. The AMR for the sentence I want you to believe me is shown in Figure 1. One of the main properties of AMR, and the reason why sentences are represented as graphs rather than trees, is the presence of nodes with multiple parents, called reentrancies, as demonstrated in Figure 1, where the node I has two parents. Reentrancies complicate AMR parsing and require the addition of specific transitions in transition-based parsing (Wang et al., 2015; Damonte et al., 2017) or of pre- and post-processing steps in sequence-to-sequence parsing (van Noord and Bos, 2017). Enabling AMR parsers to predict † you :ARG1 Introduction ∗ :ARG1 :ARG0 reentrancy structures correctly is of particular importance because it separates AMR parsing from semantic parsing based on tree structures (Steedman, 2000; Liang, 2013; Cheng et al., 2017). Reentrancy is however not an AMR-specific problem (Kuhlmann and Jonsson, 2015), and other formalisms can benefit from a better understanding of how to parse such structures. Nevertheless, to our knowledge, the AMR lite"
2020.findings-emnlp.199,P19-1009,0,0.0732168,"Missing"
2020.findings-emnlp.203,D15-1075,0,0.0116155,"by leveraging Open Information Extraction (Banko et al., 2007) along with parsed dependency trees of the input text. Zhang et al. (2019) developed a framework to evaluate the factual correctness of generated summaries by employing an information extraction module to check facts against the source document, and proposed a training strategy that optimizes the model using reinforcement learning with factual correctness as a reward policy. Falke et al. (2019) proposed a re-ranking approach to improve factual consistency of summarization models. Their approach used natural language inference (NLI; Bowman et al. 2015) models to score candidate summaries obtained in beam search by averaging the entailment probability between all sentence pairs of source document and summary. The summary with the highest score is up-ranked and used as final output of the summarization system. After evaluating their approach using summaries generated by summarization systems trained on the CNN-DailyMail corpus (Hermann et al., 2015), they concluded that out-of-the-box NLI models transfer poorly to the task of evaluating factual correctness, limiting the effectiveness of re-ranking. 3 Methodology Let X be the article and S be"
2020.findings-emnlp.203,P19-1213,0,0.292762,"Missing"
2020.findings-emnlp.203,W18-2501,0,0.0230247,"Missing"
2020.findings-emnlp.203,N18-1065,0,0.0259571,"accuracy is defined as Precision between claims made in the source document and the generated summary, where claims are represented as subject-relation-object triplets. Durmus et al. (2020) proposed an automatic question answering based metric for evaluating faithfulness. The metric has high correlation with human evaluations, especially for highly abstractive summaries. Several studies have focused on tackling the problem of factual inconsistencies between inputs and outputs of summarization models by exploring different model architectures and methods for training and inference. Cao et al. (2018) attempted to solve the problem by encoding extracted facts as additional inputs to the system. The fact descriptions are obtained by leveraging Open Information Extraction (Banko et al., 2007) along with parsed dependency trees of the input text. Zhang et al. (2019) developed a framework to evaluate the factual correctness of generated summaries by employing an information extraction module to check facts against the source document, and proposed a training strategy that optimizes the model using reinforcement learning with factual correctness as a reward policy. Falke et al. (2019) proposed"
2020.findings-emnlp.203,N19-1423,0,0.00690062,"instances which cannot be perturbed to obtain an UNVERIFIED summary. MAN , 5 2 https://spacy.io/api/annotation# named-entities Recall 78.13 71.28 85.63 83.93 100.0 F1 76.63 73.14 85.20 83.89 100.0 Table 4: Results of H ERMAN on the test set using GloVe word embedding. Label B-V B-U I-V I-U O Experiments For all experiments, we set the hidden dimensions to 256, the word embeddings to 100, and the vocabulary size to 50k. The word embeddings are initialized using pre-trained GloVe (Pennington et al., 2014) vectors (6B tokens, uncased). We also experimented using a pre-trained, base-uncased BERT (Devlin et al., 2019) for word embedding initialization. Our training used the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. We also use gradient clipping with a maximum gradient norm of 5 and we do not use any kind of regularization. We use loss on the validation set to perform early stopping. We set α to 0.66, suggesting local verification is more important than global verification. Our model was trained on a single GeForce GTX 1080 Ti GPU with a batch size of 32. We use PyTorch (Paszke et al., 2019) for our model implementation. For CRF, we used the AllenNLP library (Gardner et al., 2018)"
2020.findings-emnlp.203,2020.acl-main.454,0,0.21577,"ther witnesses are sought at this time” . . . Several people have been injured in a three-car collision on ... B-V O O O O O O B-V O O ... 1 0 0 0 0 0 0 1 0 0 ... VERIFIED Table 2: An example of a VERIFIED summary with its labels from our dataset. Cyan text highlights the support in the source document for the quantity token highlighted green in the summary. metric for estimating the factual accuracy of generated text. Factual accuracy is defined as Precision between claims made in the source document and the generated summary, where claims are represented as subject-relation-object triplets. Durmus et al. (2020) proposed an automatic question answering based metric for evaluating faithfulness. The metric has high correlation with human evaluations, especially for highly abstractive summaries. Several studies have focused on tackling the problem of factual inconsistencies between inputs and outputs of summarization models by exploring different model architectures and methods for training and inference. Cao et al. (2018) attempted to solve the problem by encoding extracted facts as additional inputs to the system. The fact descriptions are obtained by leveraging Open Information Extraction (Banko et a"
2020.findings-emnlp.203,D19-1051,0,0.0378765,"Missing"
2020.findings-emnlp.203,W04-1013,0,0.0793426,"Missing"
2020.findings-emnlp.203,P11-1052,0,0.0500705,"nstrate that the ROUGE scores of such up-ranked summaries have a higher Precision than summaries that have not been upranked, without a comparable loss in Recall, resulting in higher F1 . Preliminary human evaluation of up-ranked vs. original summaries shows people’s preference for the former. 1 Introduction Automatic summarization is the task of compressing a lengthy text to a more concise version that preserves the information of the original text. Common approaches are either extractive, selecting and assembling salient words, phrases and sentences from the source text to form the summary (Lin and Bilmes, 2011; Nallapati et al., 2017; Narayan et al., 2018b), or abstractive, generating the summary from scratch, containing novel words and phrases that are paraphrased from important parts of the original text (Clarke and Lapata, 2008; Rush et al., 2015; Wang et al., 2019). The latter is more challenging as it involves human-like capabilities, e.g., paraphrasing, generalizing, inferring and including Table 1: Examples of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phras"
2020.findings-emnlp.203,D19-1387,0,0.0154075,"ion. We use loss on the validation set to perform early stopping. We set α to 0.66, suggesting local verification is more important than global verification. Our model was trained on a single GeForce GTX 1080 Ti GPU with a batch size of 32. We use PyTorch (Paszke et al., 2019) for our model implementation. For CRF, we used the AllenNLP library (Gardner et al., 2018) with constrained decoding for the BIO scheme. To evaluate our verification model, we need outputs from abstractive summarization systems. We obtain those from three selected systems: TC ONV S2S (Narayan et al., 2018a), B ERT S UM (Liu and Lapata, 2019), and BART (Lewis et al., 2019) using pre-trained checkpoints provided by the authors. Precision 75.18 75.11 84.78 83.86 100.0 Precision 72.83 75.73 84.58 85.03 100.0 Recall 81.24 69.28 87.27 83.47 100.0 F1 76.81 72.37 85.90 84.24 100.0 Table 5: Results of H ERMAN on the test set using BERT word embedding. 6 Results Automatic Evaluation We first present results in Table 4 from our verification model using GloVe on the test set. On the binary classification task of determining whether a summary is VERIFIED or UNVERIFIED, the model achieved accuracy of 80.12 and F1 of 80.94. The results using BE"
2020.findings-emnlp.203,W11-1605,0,0.0779032,"Missing"
2020.findings-emnlp.203,2020.acl-main.173,0,0.211366,"Missing"
2020.findings-emnlp.203,D18-1206,1,0.928767,"d summaries have a higher Precision than summaries that have not been upranked, without a comparable loss in Recall, resulting in higher F1 . Preliminary human evaluation of up-ranked vs. original summaries shows people’s preference for the former. 1 Introduction Automatic summarization is the task of compressing a lengthy text to a more concise version that preserves the information of the original text. Common approaches are either extractive, selecting and assembling salient words, phrases and sentences from the source text to form the summary (Lin and Bilmes, 2011; Nallapati et al., 2017; Narayan et al., 2018b), or abstractive, generating the summary from scratch, containing novel words and phrases that are paraphrased from important parts of the original text (Clarke and Lapata, 2008; Rush et al., 2015; Wang et al., 2019). The latter is more challenging as it involves human-like capabilities, e.g., paraphrasing, generalizing, inferring and including Table 1: Examples of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in the summaries highlighted in green are c"
2020.findings-emnlp.203,N18-1158,1,0.91391,"d summaries have a higher Precision than summaries that have not been upranked, without a comparable loss in Recall, resulting in higher F1 . Preliminary human evaluation of up-ranked vs. original summaries shows people’s preference for the former. 1 Introduction Automatic summarization is the task of compressing a lengthy text to a more concise version that preserves the information of the original text. Common approaches are either extractive, selecting and assembling salient words, phrases and sentences from the source text to form the summary (Lin and Bilmes, 2011; Nallapati et al., 2017; Narayan et al., 2018b), or abstractive, generating the summary from scratch, containing novel words and phrases that are paraphrased from important parts of the original text (Clarke and Lapata, 2008; Rush et al., 2015; Wang et al., 2019). The latter is more challenging as it involves human-like capabilities, e.g., paraphrasing, generalizing, inferring and including Table 1: Examples of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in the summaries highlighted in green are c"
2020.findings-emnlp.203,D14-1162,0,0.0830157,"Missing"
2020.findings-emnlp.203,D15-1044,0,0.0166339,"aries shows people’s preference for the former. 1 Introduction Automatic summarization is the task of compressing a lengthy text to a more concise version that preserves the information of the original text. Common approaches are either extractive, selecting and assembling salient words, phrases and sentences from the source text to form the summary (Lin and Bilmes, 2011; Nallapati et al., 2017; Narayan et al., 2018b), or abstractive, generating the summary from scratch, containing novel words and phrases that are paraphrased from important parts of the original text (Clarke and Lapata, 2008; Rush et al., 2015; Wang et al., 2019). The latter is more challenging as it involves human-like capabilities, e.g., paraphrasing, generalizing, inferring and including Table 1: Examples of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in the summaries highlighted in green are correct with respect to the article, whereas red highlighting indicates hallucinations. Note that the first article describes both a new eruption and a previous one in 2014. It was in the previous er"
2020.findings-emnlp.203,P17-1099,0,0.0345211,"of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in the summaries highlighted in green are correct with respect to the article, whereas red highlighting indicates hallucinations. Note that the first article describes both a new eruption and a previous one in 2014. It was in the previous eruption that more than a dozen people were killed, hence a hallucination of at least 11 people killed and at least 20 injured in the new eruption. real-world knowledge (See et al., 2017). Abstractive summarization has attracted increasing attention recently, thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018a) and advances on neural architectures (Sutskever et al., 2014; Bahdanau et al., 2015a; Vinyals et al., 2015; Vaswani et al., 2017). Although modern abstractive summarization systems generate relatively fluent summaries, recent work has called attention to the problem they have with 2237 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2237–2249 c November 16 - 20,"
2020.findings-emnlp.203,P19-1207,0,0.0536199,"Missing"
2020.iwpt-1.7,D18-1001,1,0.933625,"t to a server which performs potentially computationally intensive operations and returns a new data, still encrypted, which only the client can decipher. All of this is done without the server itself ever being exposed to the actual content of the encrypted input data. While solutions for generic homomorphic encryption have been discovered, they are either computationally inefficient (Gentry, 2010) or have strong limitations in regards to the depth and complexity of computation they permit (Bos et al., 2013). x y server eavesdropper Figure 2: General setting illustration (figure adapted from Coavoux et al. 2018). An NLP client encrypts an x into y through obfuscation and y is sent to an NLP server. The NLP server (potentially even a legacy one) does not need to be modified to de-obfuscate y. An eavesdropper (a possibly malicious channel listener) only has access to y which is needed to be deobfuscated to gain any information about x. In this paper, we consider a softer version of homomorphic encryption in the form of obfuscation for natural language. Our goal is to identify an efficient function that stochastically transforms a given natural language input (such as a sentence) into another input whic"
2020.iwpt-1.7,N19-1423,0,0.02218,"ttacker experiments, we assume that it is known which words in the sentence are obfuscated. As such, the results we provide for attacking our obfuscation are an upper bound. In practice, an attacker would also have to identify which words were substituted for new words, which may lead to a small decrease in its accuracy. 5.1 6 In this section, we describe our experiments with our obfuscation model. We first describe the experimental setting and then turn to the results.6 6.1 Trained Attacker Pretrained Attacker In addition to a trained attacker, we also use a conditional language model, BERT (Devlin et al., 2019).5 BERT is based on the Transformer model of Vaswani et al. (2017), and uses a bidirectional encoder to obtain “contextual” embeddings for each word in a given sentence. We use the BERT model by masking out each obfuscated word, and then predicting the masked word similar to the “masked language task” that is mentioned by Devlin et al. (2019). This means that the embeddings in each position are fed into a softmax function to predict the missing word. We use the 5 We use the implementation available https://github.com/huggingface/ pytorch-pretrained-BERT. Experimental Setting In our experiments"
2020.iwpt-1.7,D18-1002,0,0.0210957,"rlap is 1. There was a stark difference between the two averages of the overlap sizes. For the random baseline model, the average was 1.46 (over 5,680 tokens) and for the neural model the average was 1.80. The difference between these two averages is statistically significant with p-value < 0.05 in a one-sided t-test. 7 Related Work There has been a significant increase in interest in the topic of privacy in the NLP community in recent years. For example, Reddy and Knight (2016) focused on obfuscation of gender features from social media text, while Li et al. (2018), Coavoux et al. (2018) and Elazar and Goldberg (2018) focused on the removal of private information from neural representations such as named entities and demographic information. Unlike the latter work, we are interested in preserving the privacy of the inputs themselves, while requiring no extra work from deployed NLP software which processes these 12 The verbs were lemmatized first using the WordNet lemmatizer available in NLTK. 69 inputs. Marujo et al. (2015), for example, perform multi-document summarization on an approximate version of the original documents. Differential privacy (Dwork, 2008) which aims to protect the privacy of informati"
2020.iwpt-1.7,kingsbury-palmer-2002-treebank,0,0.307791,"on NN . . . . . . . . . . . . strength professionalism direction NN elsewhere near even RB . . . . Table 3: Example of five sentences obfuscated with the random and neural models. Words in italics are the ones being substituted (or the substitutes). The obfuscated terms are named entities, nouns, adjectives, verbs and adverbs. and its substituted version beyond them having been seen in the training data with the same partof-speech tag. To further test whether the neural model preserves other syntactic similarities between the original and obfuscated sentences, we took all verbs from Propbank (Kingsbury and Palmer, 2002) and created a signature for each one: the list of argument types it can appear with. For example, the signature for yield is 01,012, which means that “yield” appears with two frames in Propbank, one with two arguments and the other with three arguments. We then calculated for each verb12 that appears in the original sentence the overlap between its signature and the signature of the verb in the obfuscated sentence (neural or random). This overlap is counted as the size of the intersection of the frame signatures of the two verbs. For example, the signature of advocate might be 012 while the s"
2020.iwpt-1.7,P18-2005,0,0.124044,"test the efficiency of our obfuscation model by developing two independent attacker models. Their goal is to recover the original words by inspecting only the obfuscated sentence. The attacker models may have access to all data that the parser and the obfuscator models were trained and developed on. This is perhaps unlike other scenarios in which the training set is assumed to be inaccessible to any attacker. We note that ideally, we would want to show that our obfuscation model retains privacy universally for any attacker. However, this is quite a difficult task, and we follow Coavoux et al. (2018) in presenting two strong attackers which may represent possible universal attackers. In our attacker experiments, we assume that it is known which words in the sentence are obfuscated. As such, the results we provide for attacking our obfuscation are an upper bound. In practice, an attacker would also have to identify which words were substituted for new words, which may lead to a small decrease in its accuracy. 5.1 6 In this section, we describe our experiments with our obfuscation model. We first describe the experimental setting and then turn to the results.6 6.1 Trained Attacker Pretraine"
2020.iwpt-1.7,W18-2501,0,0.0221199,"Missing"
2020.iwpt-1.7,J93-2004,0,0.0702364,"training, the constituency parser that is included in the AllenNLP software package (Gardner et al., 2018) was used.7 For our dependency parser, we follow the canonical setting of using pre-trained word embedding, 1D convolutional character level embedding and POS tag embedding, each of 100 dimensions as the input feature. We also use a three-layer bidirectional LSTM with Bayesian dropout (Gal and Ghahramani, 2016) as the encoder. We use the biaffine attention mechanism to obtain the prediction for each head, and also the prediction for the edge labels. We use the English Penn Treebank (PTB; Marcus et al. 1993) version 3.0 converted using Stanford dependencies for training the dependency parser. We follow the standard parsing split for training (sections 01–21), development (section 22) and test sets (section 23). The training set portion of the PTB data is also used to train our neural obfuscator model. We also create a spectrum over the POS tags to decide on the set P for each of our experiments (see Section 3.1). This spectrum is described in Table 1. Our first attacker works by first encoding the obfuscated sentence with a BiLSTM network. We then try to predict original words by using a feedforw"
2020.iwpt-1.7,W16-5603,0,0.0277543,"gnatures of the two verbs. For example, the signature of advocate might be 012 while the signature of affect is 012,01. Therefore, their overlap is 1. There was a stark difference between the two averages of the overlap sizes. For the random baseline model, the average was 1.46 (over 5,680 tokens) and for the neural model the average was 1.80. The difference between these two averages is statistically significant with p-value < 0.05 in a one-sided t-test. 7 Related Work There has been a significant increase in interest in the topic of privacy in the NLP community in recent years. For example, Reddy and Knight (2016) focused on obfuscation of gender features from social media text, while Li et al. (2018), Coavoux et al. (2018) and Elazar and Goldberg (2018) focused on the removal of private information from neural representations such as named entities and demographic information. Unlike the latter work, we are interested in preserving the privacy of the inputs themselves, while requiring no extra work from deployed NLP software which processes these 12 The verbs were lemmatized first using the WordNet lemmatizer available in NLTK. 69 inputs. Marujo et al. (2015), for example, perform multi-document summa"
2020.iwpt-1.8,P83-1021,0,0.781842,"is relies on the ability of a WLP to decompose the value of a proof to a combination of the values of the sub-proofs. Specifically, given a derivation tree, a WLP description automatically provides algorithms for calculating the inside and outside values. We provide analogous algorithms for calculating the inside and outside values for partial-semiring WLPs. Our outside formulation addresses the noncommutative nature of tensors themselves, and could be extended to cases where the underlying semiring is non-commutative using the techniques presented by Goodman (1998). 2 “Parsing as deduction” (Pereira and Warren, 1983) is an established framework that allows a number of parsing algorithms to be written as declarative rules and deductive systems (Shieber et al., 1995), and their correctness to be rigorously stated (Sikkel, 1998). Goodman (1999) has extended the parsing as deduction framework to arbitrary semirings and showed that various different values of interest could be computed using the same algorithm by changing the semiring. This led to the development of Dyna, a toolkit for declaratively specifying weighted logic programs, allowing concise implementation of a number of NLP algorithms (Eisner et al."
2020.iwpt-1.8,H05-1036,0,0.300308,"tensors over semirings is no longer a semiring, we prove that if the set of tensors have certain matching dimensions for the set of grammar rules they are assigned to, then they fulfill all the desirable properties relevant for the semiring parsing framework. This paves the way to use WLPs with latent variables, naturally improving the expressivity of the statistical model represented by the underlying WLP. Introducing a semiring framework like ours makes it easier to seamlessly incorporate latent variables into any execution model for dynamic programming algorithms (or software such as Dyna, Eisner et al. 2005, and other Prolog-like/WLP-like solvers). We focus on CFG parsing, however the same latent variable techniques can be applied to any weighted deduction system, including systems for parsing TAG, CCG and LCFRS, and systems for Machine Translation (Lopez, 2009). The methods we present for inside and outside computation can be used to learn latent refinements of a specified grammar for any of these tasks with EM (Dempster et al., 1977; Matsuzaki et al., 2005), or used as a backbone to create spectral learning algorithms (Hsu et al., 2012; Bailly et al., 2009; Cohen et al., 2014). valid derivatio"
2020.iwpt-1.8,C18-1258,0,0.0480097,"Missing"
2020.iwpt-1.8,E09-1037,0,0.0286376,"). These utilize the algebraic structure to efficiently track quantities needed by the expectationmaximization algorithm for parameter estimation. Their framework allows working with parameters in the form of vectors in Rn for a fixed n, coupled with a scalar in R≥0 . The semiring value of a path is roughly calculated by the multiplication of the scalars and (appropriately weighted) addition of the vectors. This is in contrast with our framework where weights could be tensors of arbitrary rank rather than only vectors, and the values of paths are calculated via tensor multiplication. Finally, Gimpel and Smith (2009) extended the semiring framework to a more general algebraic structure with the purpose of incorporating nonlocal features. Their extension comes at the cost that the new algebraic structure does not obey all the semiring axioms. Our framework differs from theirs in that under reasonable conditions, tensors of semirings do behave fully like regular semirings. 4 CFG derivations can naturally be represented as trees. We will use the notation hr : T1 . . . Tk i to represent a tree that has the node r as its root and T1 , . . . , Tk as its direct subtrees. We will use DG to denote the set of all d"
2020.iwpt-1.8,J99-4004,0,0.12792,"ariables, and demonstrates their applicability on discontinuous constituent parsing. Given the usefulness of latent variables, it would be desirable to have a generic inference mechanism for any latent variable grammar. WLPs can represent inference algorithms for probabilistic grammars effectively. However, this does not trivially extend to latent-variable models because latent variables are often represented as vectors, matrices and higher-order tensors, and these taken together no longer form a semiring. This is because in the semiring framework, values for deduction items Semiring parsing (Goodman, 1999) is an elegant framework for describing parsers by using semiring weighted logic programs. In this paper we present a generalization of this concept: latent-variable semiring parsing. With our framework, any semiring weighted logic program can be latentified by transforming weights from scalar values of a semiring to rank-n arrays, or tensors, of semiring values, allowing the modeling of latent variables within the semiring parsing framework. Semiring is too strong a notion when dealing with tensors, and we have to resort to a weaker structure: a partial semiring.1 We prove that this generaliz"
2020.iwpt-1.8,D09-1005,0,0.0425864,"erminals, and denote this as A = ⇒ σ. We will denote the language that a grammar G defines by ∗ L(G) = {σ|S = ⇒ σ}. The semiring characterization of possible values to assign to WLPs gave rise to the formulation of a number of novel semirings. One novel semiring of interest for purposes of learning parameters is the generalized entropy semiring (Cohen et al., 2008) which can be used to calculate the KL-divergence between the distribution of derivations induced by two weighted logic programs. Other two semirings of interest are expectation and variance semirings introduced by Eisner (2002) and Li and Eisner (2009). These utilize the algebraic structure to efficiently track quantities needed by the expectationmaximization algorithm for parameter estimation. Their framework allows working with parameters in the form of vectors in Rn for a fixed n, coupled with a scalar in R≥0 . The semiring value of a path is roughly calculated by the multiplication of the scalars and (appropriately weighted) addition of the vectors. This is in contrast with our framework where weights could be tensors of arbitrary rank rather than only vectors, and the values of paths are calculated via tensor multiplication. Finally, G"
2020.iwpt-1.8,E09-1061,0,0.0383367,"aves the way to use WLPs with latent variables, naturally improving the expressivity of the statistical model represented by the underlying WLP. Introducing a semiring framework like ours makes it easier to seamlessly incorporate latent variables into any execution model for dynamic programming algorithms (or software such as Dyna, Eisner et al. 2005, and other Prolog-like/WLP-like solvers). We focus on CFG parsing, however the same latent variable techniques can be applied to any weighted deduction system, including systems for parsing TAG, CCG and LCFRS, and systems for Machine Translation (Lopez, 2009). The methods we present for inside and outside computation can be used to learn latent refinements of a specified grammar for any of these tasks with EM (Dempster et al., 1977; Matsuzaki et al., 2005), or used as a backbone to create spectral learning algorithms (Hsu et al., 2012; Bailly et al., 2009; Cohen et al., 2014). valid derivation should correspond to a sequence of well defined semiring operations. For CFGs, we give a straightforward condition that ensures this is the case. This essentially boils down to making sure that each non-terminal corresponds to a fixed vector space dimension."
2020.iwpt-1.8,P05-1010,0,0.0358787,"kes it easier to seamlessly incorporate latent variables into any execution model for dynamic programming algorithms (or software such as Dyna, Eisner et al. 2005, and other Prolog-like/WLP-like solvers). We focus on CFG parsing, however the same latent variable techniques can be applied to any weighted deduction system, including systems for parsing TAG, CCG and LCFRS, and systems for Machine Translation (Lopez, 2009). The methods we present for inside and outside computation can be used to learn latent refinements of a specified grammar for any of these tasks with EM (Dempster et al., 1977; Matsuzaki et al., 2005), or used as a backbone to create spectral learning algorithms (Hsu et al., 2012; Bailly et al., 2009; Cohen et al., 2014). valid derivation should correspond to a sequence of well defined semiring operations. For CFGs, we give a straightforward condition that ensures this is the case. This essentially boils down to making sure that each non-terminal corresponds to a fixed vector space dimension. For example, if A corresponds to a space of d1 dimensions, B to d2 and C to d3 , then a rule A → B C would have a tensor weight in d2 × d3 × d1 . As long as the weights are well defined, the standard"
2020.iwpt-1.8,J03-1006,0,0.174783,"ed descriptions of these semirings see Goodman (1999). Context-free Grammars Formally, a Context-Free Grammar (CFG) is a 4tuple hN, Σ, R, Si. The set of N denotes the nonterminals which will be denoted by uppercase letters A, B etc., and S is a non-terminal that is the special start symbol. The set of Σ denotes the terminals which will be denoted by lowercase letters a, b etc. R is the set of rules of the form A → α consisting of one non-terminal on the left hand side 2 Note that given a grammar G in a formalism F and a string α, it is possible to construct a CFG grammar c(G, w) from G and α (Nederhof, 2003). This construction is possible even for range concatenation grammars (Boullier, 2004) which span all languages that could be parsed in poly-time. 75 4.3 Weighted Logic Programming 4.4 In the context of parsing, Goodman (1999) presents a framework where a grammar G comes equipped with a function w that maps each rule in G to a semiring value. Then, a grammar derivation string E consisting of the successive applications of rules e1 ,Q . . . , en is defined to have the value n VG (E) = of a seni=1 w(ei ), and the value P tence σ ∈ L(G) is defined as VG = kj=1 VG (Ej ) where E1 , E2 , . . . , Ek"
2021.emnlp-main.714,W13-2322,0,0.13195,"of its semantic role ARG1. Intuitively, this subgraph needs to be aligned to the token ‘opinion’. Similarly, ‘(b / boy)’ should be aligned to the token ‘boy’. Given such an alignment and segmentation, it is straightforward to construct a simple parser: parsing can be framed as tagging input tokens with subgraphs (including empty subgraphs), followed by predicting relations between the subgraphs. The key obstacle to training such an AMR parser is that the segmentation and alignment between AMR subgraphs and words are latent, i.e. not annotated in the data. Abstract Meaning Representation (AMR; Banarescu et al. 2013) is a broad-coverage semantic Most previous work adopts a pipeline approach formalism which represents sentence meaning as to handling this obstacle. They rely on a prerooted labeled directed acyclic graphs. The rep- learned aligner (e.g., (Pourdamghani et al., 2014)) resentations have been exploited in a wide range to produce the alignment, and apply a rule sysof tasks, including text summarization (Liu et al., tem to segment the AMR subgraph (Flanigan et al., 2015; Dohare and Karnick, 2017; Hardy and Vla- 2014; Werling et al., 2015; Damonte et al., 2017; chos, 2018), machine translation (Jon"
2021.emnlp-main.714,L18-1266,0,0.0222361,"Missing"
2021.emnlp-main.714,2020.lrec-1.86,0,0.161565,"., 9075 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9075–9091 c November 7–11, 2021. 2021 Association for Computational Linguistics 2013; Damonte and Cohen, 2018) and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one (Migueles-Abraira et al., 2018). As new AMR sembanks in languages other than English are being developed (Anchiêta and Pardo, 2018; Song et al., 2020), domain-specific AMR extensions get developed (Bonn et al., 2020; Bonial et al., 2020), and extra constructions are getting introduced to AMRs (Bonial et al., 2018), eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem to solve. We propose to optimize a graph-based parser that treats the alignment and graph segmentation as latent variables. The graph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components rely on latent alignment"
2021.emnlp-main.714,2020.lrec-1.601,0,0.0162014,"ua (Banarescu et al., 9075 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9075–9091 c November 7–11, 2021. 2021 Association for Computational Linguistics 2013; Damonte and Cohen, 2018) and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one (Migueles-Abraira et al., 2018). As new AMR sembanks in languages other than English are being developed (Anchiêta and Pardo, 2018; Song et al., 2020), domain-specific AMR extensions get developed (Bonn et al., 2020; Bonial et al., 2020), and extra constructions are getting introduced to AMRs (Bonial et al., 2018), eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem to solve. We propose to optimize a graph-based parser that treats the alignment and graph segmentation as latent variables. The graph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components re"
2021.emnlp-main.714,D19-1393,0,0.0242465,"Missing"
2021.emnlp-main.714,2020.acl-main.119,0,0.568672,"identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components rely on latent alignment and segmentation, which are being induced simultaneously. Importantly, at test time, the parser simply tags the input with the subgraphs and predicts the relations, so there is no test-time overhead from using the latent-structure apparatus. An extra benefit of this approach, in contrast to encoder-decoder AMR models (Konstas et al., 2017; van Noord and Bos, 2017; Cai and Lam, 2020) is its transparency, as one can readily see which input token triggers each subgraph.1 To develop our parser, we frame the alignment and segmentation problems as choosing a generation order of concept nodes, as we explain in Section 2.2. As marginalization over the latent generation orders is infeasible, we adopt the variational auto-encoder (VAE) framework (Kingma and Welling, 2014). Intuitively, a trainable neural module (an encoder in the VAE) is used to sample a plausible generation order (i.e., a segmentation plus an alignment), which is then used to train the parser (a decoder in the VA"
2021.emnlp-main.714,P13-2131,0,0.333006,"Missing"
2021.emnlp-main.714,N18-1104,1,0.853323,"on rules are relvidually aligned to sentence tokens (Flanigan et al., atively complex — e.g., the rules of Lyu and Titov 2014). In Figure 1, each dashed box represents (2018) targeted 40 different AMR subgraph types — the boundary of a single semantic subgraph. Red and language-dependent. AMR has never been inarrows represent the alignment between subgraphs tended to be used as an interlingua (Banarescu et al., 9075 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9075–9091 c November 7–11, 2021. 2021 Association for Computational Linguistics 2013; Damonte and Cohen, 2018) and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one (Migueles-Abraira et al., 2018). As new AMR sembanks in languages other than English are being developed (Anchiêta and Pardo, 2018; Song et al., 2020), domain-specific AMR extensions get developed (Bonn et al., 2020; Bonial et al., 2020), and extra constructions are getting introduced to AMRs (Bonial et al., 2018), eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem t"
2021.emnlp-main.714,E17-1051,1,0.936262,"bstract Meaning Representation (AMR; Banarescu et al. 2013) is a broad-coverage semantic Most previous work adopts a pipeline approach formalism which represents sentence meaning as to handling this obstacle. They rely on a prerooted labeled directed acyclic graphs. The rep- learned aligner (e.g., (Pourdamghani et al., 2014)) resentations have been exploited in a wide range to produce the alignment, and apply a rule sysof tasks, including text summarization (Liu et al., tem to segment the AMR subgraph (Flanigan et al., 2015; Dohare and Karnick, 2017; Hardy and Vla- 2014; Werling et al., 2015; Damonte et al., 2017; chos, 2018), machine translation (Jones et al., 2012; Ballesteros and Al-Onaizan, 2017; Peng et al., Song et al., 2019), paraphrase detection (Issa et al., 2015; Artzi et al., 2015; Groschwitz et al., 2018). 2018) and question answering (Mitra and Baral, While Lyu and Titov (2018) jointly optimize the 2016). parser and the alignment model, the rules handling An AMR graph can be regarded as consisting specific constructions still needed to be crafted to of multiple concept subgraphs, which can be indi- segment the graph. The segmentation rules are relvidually aligned to sentence tokens (Flani"
2021.emnlp-main.714,P14-1134,0,0.0233221,".8 71 72 78.2 Smatch 71.0 74.4 77.0 75.5 73.2 76.8 80.2 + 88.1 74.2 80.2 88.1 74.5 78.7 87.4 78.9 80.2 - 87.5 ± 0.1 71.3 ±0.1 75.2 ± 0.1 + 88.7 ± 0.2 73.6± 0.2 76.8 ± 0.4 - 88.3 ± 0.3 73.0 ± 0.2 76.1 ± 0.2 rized in Table 3. As expected, the full model performs the best, demonstrating that it is important to learn both alignments and segmentation. Interestingly, both ‘segmentation learned’ and ‘alignment learned’ obtain reasonable performance, but the ‘nothing learned’ model fails badly. 7 Related Work A wide range of approaches for AMR parsing have been explored, including graph-based models (Flanigan et al., 2014; Werling et al., 2015; Lyu and Titov, 2018; Zhang et al., 2019a), transitionbased models (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017), grammar-based modTable 1: Scores with standard deviation on the AMR els (Peng et al., 2015; Artzi et al., 2015; Groschwitz 2.0 test set. digits. The columns ’R’ indicate if handet al., 2018; Lindemann et al., 2020) and neural crafted rules are used for segmentation, ♦ indicates that autoregressive models (Konstas et al., 2017; van the system used specialized pretraining or self-training. Noord and Bos, 2017; Zhang et al., 2019b; Cai and Our results"
2021.emnlp-main.714,W18-2501,0,0.0242931,"Missing"
2021.emnlp-main.714,W17-6810,0,0.0189296,"system used specialized pretraining or self-training. Noord and Bos, 2017; Zhang et al., 2019b; Cai and Our results are averaged over 4 runs. Lam, 2020; Xu et al., 2020b). The majority of strong parsers rely on explicit Metric Concept SRL Smatch graph segmentation in training. Typically, the seggreedy 87.0 71.5 74.8 mentation is dealt with hand-crafted rules, with rule 88.0 72.6 75.8 rule templates developed by studying training set full 87.8 72.9 75.6 statistics and ensuring the necessary level of coverTable 2: AMR 3.0 test set, averaged over 2 runs. age. Alternatively, Artzi et al. (2015); Groschwitz et al. (2017, 2018); Lindemann et al. (2020); Peng Concept SRL Smatch et al. (2015) using existing grammar formalisms to nothing learned 81.7 62.6 61.9 segment the AMR graphs. Astudillo et al. (2020); segmentation learned 86.0 69.1 70.5 Lee et al. (2020) - while not not relying on graph alignment learned 87.6 71.1 74.4 recategorization rules - use a rule system to ‘pack’ full (all learned) 88.3 73.0 76.1 and ‘unpack’ nodes. In recent work, strong results were obtained without using any explicit segmentaTable 3: Scores with different versions of latent segtion and alignment, relying on sequence-sequence me"
2021.emnlp-main.714,P18-1170,0,0.0597585,"cle. They rely on a prerooted labeled directed acyclic graphs. The rep- learned aligner (e.g., (Pourdamghani et al., 2014)) resentations have been exploited in a wide range to produce the alignment, and apply a rule sysof tasks, including text summarization (Liu et al., tem to segment the AMR subgraph (Flanigan et al., 2015; Dohare and Karnick, 2017; Hardy and Vla- 2014; Werling et al., 2015; Damonte et al., 2017; chos, 2018), machine translation (Jones et al., 2012; Ballesteros and Al-Onaizan, 2017; Peng et al., Song et al., 2019), paraphrase detection (Issa et al., 2015; Artzi et al., 2015; Groschwitz et al., 2018). 2018) and question answering (Mitra and Baral, While Lyu and Titov (2018) jointly optimize the 2016). parser and the alignment model, the rules handling An AMR graph can be regarded as consisting specific constructions still needed to be crafted to of multiple concept subgraphs, which can be indi- segment the graph. The segmentation rules are relvidually aligned to sentence tokens (Flanigan et al., atively complex — e.g., the rules of Lyu and Titov 2014). In Figure 1, each dashed box represents (2018) targeted 40 different AMR subgraph types — the boundary of a single semantic subgraph. Red"
2021.emnlp-main.714,D18-1086,0,0.480202,"Missing"
2021.emnlp-main.714,N18-1041,1,0.871721,"Missing"
2021.emnlp-main.714,P17-1014,0,0.153553,"aph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component decides on the labeled edges. During training, both components rely on latent alignment and segmentation, which are being induced simultaneously. Importantly, at test time, the parser simply tags the input with the subgraphs and predicts the relations, so there is no test-time overhead from using the latent-structure apparatus. An extra benefit of this approach, in contrast to encoder-decoder AMR models (Konstas et al., 2017; van Noord and Bos, 2017; Cai and Lam, 2020) is its transparency, as one can readily see which input token triggers each subgraph.1 To develop our parser, we frame the alignment and segmentation problems as choosing a generation order of concept nodes, as we explain in Section 2.2. As marginalization over the latent generation orders is infeasible, we adopt the variational auto-encoder (VAE) framework (Kingma and Welling, 2014). Intuitively, a trainable neural module (an encoder in the VAE) is used to sample a plausible generation order (i.e., a segmentation plus an alignment), which is then"
2021.emnlp-main.714,2020.emnlp-main.323,0,0.628777,"n system of Lyu and Titov (2018), or, more specifically, its re-implementation by Zhang et al. (2019a). Arguably, this can be thought of as an upper bound for how well an induction method can do. This fixed segmentation can be incorporated into our latent-generation-order framework, so that the alignment between concept nodes and the tokens will still be induced. This is achieved by fixing S, while still inducing A. details about the strategy. Results In Table 1, we compare our models with recent AMR parsers (Xu et al., 2020a; Cai and Lam, 2020, 2019; Zhang et al., 2019a; Naseem et al., 2019; Lindemann et al., 2020; Lee et al., 2020), as well as (Lyu and Titov, 2018), which we build on, and (van Noord and Bos, 2017), the earliest model which does not exploit any rules. Overall, our model (‘full’) performs competitively, but lags behind scores reported by some of the very recent parsers.7 However, except for a no-rule version of Cai and Lam (2020), all these models either use rules (Lee et al., 2020) (see Section 7) or specialized pretraining (Xu et al., 2020a). Both our VAE model and the rule-based segmentation achieve high concept identification scores (Damonte et al., 2017). The relation identificatio"
2021.emnlp-main.714,N15-1114,0,0.0647233,"Missing"
2021.emnlp-main.714,Q18-1005,0,0.0244773,"ying on sequence-sequence mentation on the AMR 2.0 test set, averaged over 2 models (Xu et al., 2020b; Cai and Lam, 2020), still runs the rules appear useful even with these strong modmethod is able to induce relatively accurate align- els (Cai and Lam, 2020). ments, and joint induction of alignments with segMore generally, outside of AMR parsing, difmentation may be beneficial, or, at the very least, ferentiable relaxations of latent structure reprenot detrimental to alignment quality. sentations have received attention in NLP (Kim Ablations To reconfirm that it is important to et al., 2017; Liu and Lapata, 2018), including prelearn the segmentation and alignment, rather than vious applications of the perturb-and-MAP frameto sample it randomly, we perform further ablations. work (Corro and Titov, 2019). From a more genIn our parameterization, discussed in Section 4.2.2, eral goal perspective – inducing a segmentation it is possible to set Araw = 0 and/or Sraw = 0, of a linguistic structure – our work is related to which corresponds to sampling from the prior in tree-substitution grammar induction (Sima’an et al., training (i.e. quasi-uniformly while respecting the 1995; Cohn et al., 2010), the DOP par"
2021.emnlp-main.714,2021.ccl-1.108,0,0.0234037,"Missing"
2021.emnlp-main.714,D19-1099,1,0.882105,"Missing"
2021.emnlp-main.714,P18-1037,1,0.377359,". In contrast, we treat both alignment and segmentation as latent variables in our model and induce them as part of end-to-end training. As marginalizing over the structured latent variables is infeasible, we use the variational autoencoding framework. To ensure end-to-end differentiable optimization, we introduce a differentiable relaxation of the segmentation and alignment problems. We observe that inducing segmentation yields substantial gains over using a ‘greedy’ segmentation heuristic. The performance of our method also approaches that of a model that relies on the segmentation rules of Lyu and Titov (2018), which were hand-crafted to handle individual AMR constructions. 1 Introduction 1 thing 0 The ARG1 opinion 0 opine-01 1 of 2 ARG0 2 boy the 3 4 boy Figure 1: An example of AMR, the dashed red arrows mark latent alignment. Dashed blue boxes represent the latent segmentation. and tokens. For example, ‘(o / opine-01: ARG1 (t / thing))’ refers to a combination of the predicate ‘opine-01’ and a filler of its semantic role ARG1. Intuitively, this subgraph needs to be aligned to the token ‘opinion’. Similarly, ‘(b / boy)’ should be aligned to the token ‘boy’. Given such an alignment and segmentation"
2021.emnlp-main.714,K15-1004,0,0.0190254,"ull model performs the best, demonstrating that it is important to learn both alignments and segmentation. Interestingly, both ‘segmentation learned’ and ‘alignment learned’ obtain reasonable performance, but the ‘nothing learned’ model fails badly. 7 Related Work A wide range of approaches for AMR parsing have been explored, including graph-based models (Flanigan et al., 2014; Werling et al., 2015; Lyu and Titov, 2018; Zhang et al., 2019a), transitionbased models (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017), grammar-based modTable 1: Scores with standard deviation on the AMR els (Peng et al., 2015; Artzi et al., 2015; Groschwitz 2.0 test set. digits. The columns ’R’ indicate if handet al., 2018; Lindemann et al., 2020) and neural crafted rules are used for segmentation, ♦ indicates that autoregressive models (Konstas et al., 2017; van the system used specialized pretraining or self-training. Noord and Bos, 2017; Zhang et al., 2019b; Cai and Our results are averaged over 4 runs. Lam, 2020; Xu et al., 2020b). The majority of strong parsers rely on explicit Metric Concept SRL Smatch graph segmentation in training. Typically, the seggreedy 87.0 71.5 74.8 mentation is dealt with hand-crafte"
2021.emnlp-main.714,D14-1162,0,0.0861259,"Missing"
2021.emnlp-main.714,2020.lrec-1.362,0,0.0353183,"the alignment between subgraphs tended to be used as an interlingua (Banarescu et al., 9075 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9075–9091 c November 7–11, 2021. 2021 Association for Computational Linguistics 2013; Damonte and Cohen, 2018) and AMR banks for individual languages substantially diverge from English AMR. For example, Spanish AMR represents pronouns and ellipsis differently from the English one (Migueles-Abraira et al., 2018). As new AMR sembanks in languages other than English are being developed (Anchiêta and Pardo, 2018; Song et al., 2020), domain-specific AMR extensions get developed (Bonn et al., 2020; Bonial et al., 2020), and extra constructions are getting introduced to AMRs (Bonial et al., 2018), eliminating the need for rules while learning graph segmentation from scratch is becoming an important problem to solve. We propose to optimize a graph-based parser that treats the alignment and graph segmentation as latent variables. The graph-based parser consists of two parts: concept identification and relation identification. The concept identification model generates the AMR nodes, and the relation identification component"
2021.emnlp-main.714,Q19-1002,0,0.0543064,"ne approach formalism which represents sentence meaning as to handling this obstacle. They rely on a prerooted labeled directed acyclic graphs. The rep- learned aligner (e.g., (Pourdamghani et al., 2014)) resentations have been exploited in a wide range to produce the alignment, and apply a rule sysof tasks, including text summarization (Liu et al., tem to segment the AMR subgraph (Flanigan et al., 2015; Dohare and Karnick, 2017; Hardy and Vla- 2014; Werling et al., 2015; Damonte et al., 2017; chos, 2018), machine translation (Jones et al., 2012; Ballesteros and Al-Onaizan, 2017; Peng et al., Song et al., 2019), paraphrase detection (Issa et al., 2015; Artzi et al., 2015; Groschwitz et al., 2018). 2018) and question answering (Mitra and Baral, While Lyu and Titov (2018) jointly optimize the 2016). parser and the alignment model, the rules handling An AMR graph can be regarded as consisting specific constructions still needed to be crafted to of multiple concept subgraphs, which can be indi- segment the graph. The segmentation rules are relvidually aligned to sentence tokens (Flanigan et al., atively complex — e.g., the rules of Lyu and Titov 2014). In Figure 1, each dashed box represents (2018) targ"
2021.emnlp-main.714,2020.findings-emnlp.199,1,0.83745,"Missing"
2021.emnlp-main.823,2020.iwpt-1.2,0,0.0425494,"i et al., 2020) without compromising at all on accuracy. Graph-based dependency parsers work in two steps. The first step forms a complete weighted directed graph of words and a special ROOT token by computing the weights using a trained statistical model.2 The second step then executes the main inference procedure: it identifies a directed spanning tree (often referred to as arborescence) in this graph, aiming to maximize its weight, and retaining ROOT as the root node of the arborescence. While some of the previous work to optimize the speed of graph-based parsers focused on the first step (Anderson and Gómez-Rodríguez, 2020), we demonstrate in Figure 1 that most of the parsing time is actually spent on the spanning tree inference routine. As sentence length increases, the gap between the spanning tree inference time and time spent on constructing the weighted graph increases significantly.3 MST search is often done using the Chu-LiuEdmonds (CLE) algorithm (Chu and Liu, 1965; Edmonds, 1967) that runs in O(n3 ) where n is the sentence length. Tarjan (1977) presents a relatively complicated way of implementing the CLE algorithm in O(n2 ). Tarjan’s algorithm is often cited in NLP literature, but to the best of our kn"
2021.findings-acl.207,D18-1316,0,0.0174526,"n return x(t) else return None end if This helps to determine the word substituting order in the proposed method. In this work, we use a combination of the changes found in the unlabelled attachment score (UAS) and in the labelled attachment score (LAS) to measure word importance. Specifically, the importance of a word xi in sentence x is computed as 2.3.1 Generating Process We collect substitutes from the following methods: BERT-Based Method: We use BERT to generate candidates for each target word from its context. This method generates only single subwords. Embedding-Based Method: Following Alzantot et al. (2018), we use word embeddings of Mrkˇsi´c et al. (2016)2 to compute the N nearest neighbours of each target word according to their cosine similarity and use them as candidates. Sememe-Based Method: The sememes of a word represent its core meaning (Dong and Dong, 2006). Following Zang et al. (2020), we collect the substitutes of the target word x based on the rule that one of the substitutes the senses of x∗ must have the same sememe annotations as one of senses of x. Synonym-Based Method: We use WordNet3 to extract synonyms of each target word as candidates. 2.3.2 Filtering Process Generation of S"
2021.findings-acl.207,N19-1423,0,0.0124486,"Preserving the syntactic structure enables us to use the gold syntactic structure of the original sentence in the evaluation process. While preserving the fluency ensures that ungrammatical adversarial examples, which not only fool the target model but also confuse humans, will not be considered valid. Therefore in this paper, we evaluate the quality of an adversarial example in two aspects, namely the fluency and syntactic structure preservation. Recently, Zheng et al. (2020) proposed the first dependency parser attacking algorithm based on word-substitution which depended entirely on BERT (Devlin et al., 2019) to generate candidate substitutes. The rational was that the use of the pre-trained language model will ensure fluency of the adversarial examples. However, we find that using BERT alone is far from enough to preserve fluency. Therefore, in this paper, we propose a method to generate better adversarial examples for dependency parsing with four types of candidate generators and filters. Specifically, our method consists of three steps: (i) determining the substitution order, (ii) generating and filtering candidate substitutes for each word, (iii) searching for the best possible combination of"
2021.findings-acl.207,2020.emnlp-main.182,0,0.356771,"operties of adversarial attacks. We find that (i) the introduction of out-of-vocabulary (OOV, words not in the embedding’s vocabulary) and out-of-training (OOT, words not in the training set of the parser) words in adversarial examples are two main factors that harm models’ performance; (ii) adversarial examples generated against a parser strongly depend on the type of the parser, the token embeddings and even the random seed. Adversarial training (Goodfellow et al., 2015), where adversarial examples are added in the training stage, has been commonly used in previous work (Zheng et al., 2020; Han et al., 2020) to improve a parser’s robustness. Only a limited number of adversarial examples have been used in such cases, and Zheng et al. (2020) argued that overuse of them may lead to a performance drop on the clean data. However, we show that with improvement in the quality of adversarial examples produced in our method, more adversarial examples can be used in the training stage to further improve the parsing models’ robustness without producing any apparent harm in their performance on the clean data. Inspired by our second finding, we propose to improve the parsers’ robustness by combining models t"
2021.findings-acl.207,2020.iwpt-1.7,1,0.754643,"or, and we propose to generate better examples by using more generators and stricter filters. Han et al. (2020) proposed an approach to attack structured prediction models with a seq2seq model (Wang et al., 2016) and evaluated this model on dependency parsing. They used two reference parsers in addition to the victim parser to supervise the training of the adversarial example generator, and found that the three parsers produce better results when they have different inductive biases embedded to make the attack successful. This finding is quite close in spirit to our conclusion in Section 4.5. Hu et al. (2020) also put forth efforts to modify the text in syntactic tasks while preserving the original syntactic structure. However, their goal is to preserve privacy via the modification of words that could disclose sensitive information. 6 Conclusion In this paper, we propose a method for generating high-quality adversarial examples for dependency parsing and show its effectiveness based on automatic and human evaluation. We investigate This work was supported by the National Key R&D Program of China via grant 2020AAA0106501 and the National Natural Science Foundation of China (NSFC) via grant 61976072"
2021.findings-acl.207,N16-1082,0,0.0336606,"ii) the true dependency tree of x∗ should be the same as that of x. In this paper, these two constraints are ensured through the use of various filters (see Section 2.3) and are used to evaluate the quality of adversarial examples (see details on fluency and syntactic structure preservation in Section 3.3). 2.2 Word Importance Ranking Word importance ranking in our model is based on the observation that some words have a stronger influence on model prediction than others. Such word importance is typically computed by setting each word to unknown and examining the changes in their predictions (Li et al., 2016; Ren et al., 2019). 2345 Algorithm 1 Dependency Parsing Attack generation methods, then apply filters to discard inappropriate substitutes, ensuring both diversity and quality of the generated candidates. Input: Sentence example x(0) = {x1 , x2 , . . . , xN }, maximum percentage of words allowed to be modified γ Output: Adversarial example x(i) 1: for i = 1 to N do 2: Compute word importance I(x(0) , xi ) via Eq. 1 3: end for 4: Create a set W of all words xi ∈ x(0) sorted by the descending order of their importance I(x(0) , xi ). 5: t = 0 6: for each word xj in W do 7: Build candidate set Cj"
2021.findings-acl.207,2021.ccl-1.108,0,0.0377807,"Missing"
2021.findings-acl.207,P18-1130,0,0.0126623,"rds in the sentence exceeds a threshold γ, we stop the process. Otherwise, we search for a substitute for the next target word. 3 3.1 Experimental Setup Target Parsers and Token Embeddings We choose the following two strong and commonly used English parsers, one graph-based, the other transition-based, as target models, both of which achieve performance close to the state-of-the-art. Deep Biaffine Parser (Dozat and Manning, 2017) is a graph-based parser that scores each candidate arc independently and relies on a decoding algorithm to search for the highest-scoring tree. Stack-Pointer Parser (Ma et al., 2018) is a transition-based parser that incrementally builds the dependency tree with pre-defined operations. We used the following four types of token embeddings to study their influence on each parsers’ robustness. To focus on the influence of the embeddings, we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece emb"
2021.findings-acl.207,de-marneffe-etal-2006-generating,0,0.0517672,"Missing"
2021.findings-acl.207,2020.findings-emnlp.341,0,0.0200122,"model (i.e., a random seed). We use these insights to improve the robustness of English parsing models, relying on adversarial training and model ensembling.1 1 Introduction Neural network-based models have achieved great successes in a wide range of NLP tasks. However, recent work has shown that their performance can be easily undermined with adversarial examples that would pose no confusion for humans (Zhang et al., 2020). As an increasing number of successful adversarial attackers have been developed for NLP tasks, the quality of the adversarial examples they generate has been questioned (Morris et al., 2020). The definition of a valid successful adversarial example differs across target tasks. In semantic tasks such as sentiment analysis (Zhang et al., 2019) and textual entailment (Jin et al., 2020), a valid successful adversarial example needs to be able to alter the prediction of the target model while ∗ Work partially done while at the University of Edinburgh. 1 Our code is available at: https://github.com/ WangYuxuan93/DepAttacker.git preserving the semantic content and fluency of the original text. In contrast, in the less explored field of attacking syntactic tasks, the syntactic structure,"
2021.findings-acl.207,N16-1018,0,0.0609948,"Missing"
2021.findings-acl.207,D14-1162,0,0.0855441,"ich achieve performance close to the state-of-the-art. Deep Biaffine Parser (Dozat and Manning, 2017) is a graph-based parser that scores each candidate arc independently and relies on a decoding algorithm to search for the highest-scoring tree. Stack-Pointer Parser (Ma et al., 2018) is a transition-based parser that incrementally builds the dependency tree with pre-defined operations. We used the following four types of token embeddings to study their influence on each parsers’ robustness. To focus on the influence of the embeddings, we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece embeddings. ELECTRA (Clark et al., 2020) is a pre-trained language model based on a replaced token detection object, which learns to predict whether each token in the corrupted input has been replaced. It produces contextualised word piece embeddings. ELMo (Peters et al., 2018) is a pre-trained language representation model based on characte"
2021.findings-acl.207,N18-1202,0,0.0174184,", we use only the embeddings as input to the parsers: GloVe (Pennington et al., 2014) is a frequently used static word embedding. RoBERTa (Liu et al., 2019) is a pre-trained language model based on a masked language modelling object, which learns to predict a randomly masked token based on its context. It produces contextualised word piece embeddings. ELECTRA (Clark et al., 2020) is a pre-trained language model based on a replaced token detection object, which learns to predict whether each token in the corrupted input has been replaced. It produces contextualised word piece embeddings. ELMo (Peters et al., 2018) is a pre-trained language representation model based on character embeddings and bidirectional language modelling. 3.2 Datasets and Experimental Settings We train the target parsers and evaluate the proposed method on the English Penn Treebank (PTB) dataset,7 converted into Stanford dependencies using version 3.3.0 of the Stanford dependency converter (de Marneffe et al., 2006) (PTB-SD-3.3.0). We follow the standard PTB split, using section 2-21 for training, section 22 as a development set and 23 as a test set. It is important to note that when converting PTB into Stanford dependencies, Zhen"
2021.findings-acl.207,P19-1103,1,0.805871,"ndency tree of x∗ should be the same as that of x. In this paper, these two constraints are ensured through the use of various filters (see Section 2.3) and are used to evaluate the quality of adversarial examples (see details on fluency and syntactic structure preservation in Section 3.3). 2.2 Word Importance Ranking Word importance ranking in our model is based on the observation that some words have a stronger influence on model prediction than others. Such word importance is typically computed by setting each word to unknown and examining the changes in their predictions (Li et al., 2016; Ren et al., 2019). 2345 Algorithm 1 Dependency Parsing Attack generation methods, then apply filters to discard inappropriate substitutes, ensuring both diversity and quality of the generated candidates. Input: Sentence example x(0) = {x1 , x2 , . . . , xN }, maximum percentage of words allowed to be modified γ Output: Adversarial example x(i) 1: for i = 1 to N do 2: Compute word importance I(x(0) , xi ) via Eq. 1 3: end for 4: Create a set W of all words xi ∈ x(0) sorted by the descending order of their importance I(x(0) , xi ). 5: t = 0 6: for each word xj in W do 7: Build candidate set Cj for xj following t"
2021.findings-acl.207,D16-1058,0,0.0368721,"Morris et al. (2020) reported that quite a number of these techniques introduce grammatical errors. In syntactic tasks, Zheng et al. (2020) recently proposed the first dependency parser attacking method which depends entirely on BERT to generate candidates. However, we show that the quality of adversarial examples generated by their method is relatively low due to the limitation of the BERTbased generator, and we propose to generate better examples by using more generators and stricter filters. Han et al. (2020) proposed an approach to attack structured prediction models with a seq2seq model (Wang et al., 2016) and evaluated this model on dependency parsing. They used two reference parsers in addition to the victim parser to supervise the training of the adversarial example generator, and found that the three parsers produce better results when they have different inductive biases embedded to make the attack successful. This finding is quite close in spirit to our conclusion in Section 4.5. Hu et al. (2020) also put forth efforts to modify the text in syntactic tasks while preserving the original syntactic structure. However, their goal is to preserve privacy via the modification of words that could"
2021.findings-acl.207,2020.acl-main.540,0,0.0323383,"e importance of a word xi in sentence x is computed as 2.3.1 Generating Process We collect substitutes from the following methods: BERT-Based Method: We use BERT to generate candidates for each target word from its context. This method generates only single subwords. Embedding-Based Method: Following Alzantot et al. (2018), we use word embeddings of Mrkˇsi´c et al. (2016)2 to compute the N nearest neighbours of each target word according to their cosine similarity and use them as candidates. Sememe-Based Method: The sememes of a word represent its core meaning (Dong and Dong, 2006). Following Zang et al. (2020), we collect the substitutes of the target word x based on the rule that one of the substitutes the senses of x∗ must have the same sememe annotations as one of senses of x. Synonym-Based Method: We use WordNet3 to extract synonyms of each target word as candidates. 2.3.2 Filtering Process Generation of Substitute Candidates We apply the following four types of filters to discard candidates which are likely inappropriate, either in terms of syntactic preservation or fluency. POS Filter: We first filter out substitutes with different part-of-speech (POS) tags from the original word.4 This filte"
2021.findings-acl.207,P19-1559,0,0.0216978,"ing.1 1 Introduction Neural network-based models have achieved great successes in a wide range of NLP tasks. However, recent work has shown that their performance can be easily undermined with adversarial examples that would pose no confusion for humans (Zhang et al., 2020). As an increasing number of successful adversarial attackers have been developed for NLP tasks, the quality of the adversarial examples they generate has been questioned (Morris et al., 2020). The definition of a valid successful adversarial example differs across target tasks. In semantic tasks such as sentiment analysis (Zhang et al., 2019) and textual entailment (Jin et al., 2020), a valid successful adversarial example needs to be able to alter the prediction of the target model while ∗ Work partially done while at the University of Edinburgh. 1 Our code is available at: https://github.com/ WangYuxuan93/DepAttacker.git preserving the semantic content and fluency of the original text. In contrast, in the less explored field of attacking syntactic tasks, the syntactic structure, rather than the semantic content, must be preserved while also maintaining the fluency. Preserving the syntactic structure enables us to use the gold sy"
2021.findings-acl.207,2020.acl-main.590,0,0.151193,"tacking syntactic tasks, the syntactic structure, rather than the semantic content, must be preserved while also maintaining the fluency. Preserving the syntactic structure enables us to use the gold syntactic structure of the original sentence in the evaluation process. While preserving the fluency ensures that ungrammatical adversarial examples, which not only fool the target model but also confuse humans, will not be considered valid. Therefore in this paper, we evaluate the quality of an adversarial example in two aspects, namely the fluency and syntactic structure preservation. Recently, Zheng et al. (2020) proposed the first dependency parser attacking algorithm based on word-substitution which depended entirely on BERT (Devlin et al., 2019) to generate candidate substitutes. The rational was that the use of the pre-trained language model will ensure fluency of the adversarial examples. However, we find that using BERT alone is far from enough to preserve fluency. Therefore, in this paper, we propose a method to generate better adversarial examples for dependency parsing with four types of candidate generators and filters. Specifically, our method consists of three steps: (i) determining the su"
2021.findings-emnlp.238,D19-1522,0,0.13997,"ph because the corresponding triple was not found in the text (Hosseini et al., 2019; Broscheit et al., 2020). Figure 1a shows part of an example open-domain KG, in which the triple (Apple, own, Beats) is missing, but can be inferred using link prediction over all entities in the complete KG. 2790 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2790–2802 November 7–11, 2021. ©2021 Association for Computational Linguistics 's pu Previous work has applied standard link prediction methods such as TransE (Bordes et al., 2013), ConvE (Dettmers et al., 2018), or TuckER (Balazevic et al., 2019) to open-domain triples. These methods have been shown to be effective in learning the KG structure, but they are sub-optimal for open-domain link prediction because they ignore the textual context of the triples. Since the triples are extracted from text, they can be automatically grounded back to their contexts. Hence, in addition to the KG structure, the triple contexts can be used as input to the link prediction task. Figure 1b shows the context sentences that have given rise to the partial KG in Figure 1a.1 There are multiple clues in the contexts such as deal, $, cash, stock, and Financi"
2021.findings-emnlp.238,J15-2003,0,0.0282748,"Missing"
2021.findings-emnlp.238,P10-1124,0,0.0462528,"dd missing relations to the KG (e.g., own in in Figure 1a) by predicting the relations that hold between the entities of triple mentions in context (e.g., the context c1 in Figure 1b). Our experiments show that the proposed model for the contextual link prediction task significantly outperforms standard link prediction in open-domain KG completion. In addition, we investigate the interplay between contextual link prediction and context-independent entailments between relations, in the form of entailment graphs (EG). An EG has typed relations as nodes and entailment relation as directed edges (Berant et al., 2010, 2011, 2015; Hosseini et al., 2018; Hosseini, 2021). The type of each relation is determined by the types of its two entities. EGs are by definition context-independent, but they use relation types as a proxy of the context. Figure 1c shows a fragment of an EG showing that for example acquire entails own. Similar to opendomain KGs, EGs are constructed based on extracted triples from text. The entailment between two relations is predicted by computing a directional entailment score between them. It has been recently shown that the two tasks of open-domain link prediction and EG learning are co"
2021.findings-emnlp.238,P11-1062,0,0.0326858,"r training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts take a local approach and predict entailment relations independently from each other (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Berant et al. (2011, 2015) and Hosseini et al. (2018) propose a global approach where the dependencies between the entailment relations are taken into account. They first build a local typed EG for any plausible type pair. They then build global EGs that satisfy soft or hard constraints such as the transitivity of entailment. The constraints consider the structures both across typed EGs and inside each graph. In this work, we improve the local entailment scores, which in turn improves the global EGs. Hosseini et al. (2019) perform standard link prediction to add more coverage to the EGs by adding 3 Contextual Li"
2021.findings-emnlp.238,D19-1651,0,0.0538611,"Missing"
2021.findings-emnlp.238,2020.acl-main.209,0,0.0364414,"knowledge graph (KG) is constituted by a set of (subject, relation, object) triples such as (Apple, acquire, Beats). KGs have entities (subjects and objects) as nodes and relations as labeled edges. Manually-built KGs such as Freebase (Bollacker et al., 2008), Wikidata (Vrandeˇci´c and Krötzsch, 2014), or DBPedia (Lehmann et al., 2015) have a known set of hand-built relations. In contrast, the relation-labels of open-domain KGs are obtained from text rather than fixed. Open-domain KGs can be constructed by applying parsers or openinformation extraction methods to text (Hosseini et al., 2019; Broscheit et al., 2020). ∗ (a) Figure 1: a) Part of an example KG. The relation own is missing, but can be predicted from the rest of the KG and the triple contexts using contextual link prediction. b) The contexts c1 and c2 from which we have extracted the KG triples. The relation tokens are boldfaced and entities are italic. The contextual link prediction task predicts relations that hold between the entitypair in a grounded triple. For example, we predict that the relation own should be added between Apple and Beats. c) An example EG of type Organization, Organization. The contextual link prediction and EG learni"
2021.findings-emnlp.238,N19-1423,0,0.0397277,"is because these clues could have been seen around occurrences of other entity-pairs of the same type (e.g., Facebook and Whatsapp) that are connected by acquire, ’s purchase of, and own relations. In this paper, we propose the new task of contextual link prediction for such open-domain graphs: Given a triple (e1 , r, e2 ) grounded in context with the relation r holding between the entities e1 and e2 , our goal is to predict all the other relations that hold between the two entities. We present a model that uses contextualized relation embeddings to predict new relations. We start with BERT (Devlin et al., 2019) pre-trained embeddings and fine-tune them with a novel unsupervised contextual link prediction objective function. After training the contextual link prediction model, we can add missing relations to the KG (e.g., own in in Figure 1a) by predicting the relations that hold between the entities of triple mentions in context (e.g., the context c1 in Figure 1b). Our experiments show that the proposed model for the contextual link prediction task significantly outperforms standard link prediction in open-domain KG completion. In addition, we investigate the interplay between contextual link predic"
2021.findings-emnlp.238,2020.textgraphs-1.7,1,0.829589,"Missing"
2021.findings-emnlp.238,2021.eacl-main.316,0,0.0236904,"Missing"
2021.findings-emnlp.238,P19-1468,1,0.820203,"arch. headphone Apple A knowledge graph (KG) is constituted by a set of (subject, relation, object) triples such as (Apple, acquire, Beats). KGs have entities (subjects and objects) as nodes and relations as labeled edges. Manually-built KGs such as Freebase (Bollacker et al., 2008), Wikidata (Vrandeˇci´c and Krötzsch, 2014), or DBPedia (Lehmann et al., 2015) have a known set of hand-built relations. In contrast, the relation-labels of open-domain KGs are obtained from text rather than fixed. Open-domain KGs can be constructed by applying parsers or openinformation extraction methods to text (Hosseini et al., 2019; Broscheit et al., 2020). ∗ (a) Figure 1: a) Part of an example KG. The relation own is missing, but can be predicted from the rest of the KG and the triple contexts using contextual link prediction. b) The contexts c1 and c2 from which we have extracted the KG triples. The relation tokens are boldfaced and entities are italic. The contextual link prediction task predicts relations that hold between the entitypair in a grounded triple. For example, we predict that the relation own should be added between Apple and Beats. c) An example EG of type Organization, Organization. The contextual link"
2021.findings-emnlp.238,2020.tacl-1.28,0,0.0471479,"kens, whereas we use with unequal types, we do not need the flag as the the natural text associated with the triples. order is obvious and set o(r) = 0. For relations Extracting Factual Knowledge from Prewith identical types, we set o(r) = 0 if the entities Trained Language Models. These works form are in the original order and o(r) = 1, otherwise. a prompt where an entity is missing (e.g., Apple A triple mention is a triple grounded in its texacquire [MASK] ), and ask the language models tual context. We define a triple mention as a tuto predict the masked entity (Petroni et al., 2019, 2020; Jiang et al., 2020; Bouraoui et al., 2020; Ha- 3 For brevity, we drop the types when they are obvious. 2792 Model ple m = (e1 , r, e2 , c, s), where r ∈ R is a relation and e1 , e2 ∈ E are entities. The sub-word token sequence c = [c0 , . . . , cn ] is the textual context of the triple including the surface form of the relation and entity-pair.4 The pair s = (s1 , s2 ) indicates the indices of the first and last relation tokens. An example triple mention in Figure 1b is (Apple,acquire,Beats,c   2 ,[9, 11]). We denote by D= (ei,1 , ri , ei,2 , ci , si ) i∈{1,...,N } the set of all triple mentions. We define th"
2021.findings-emnlp.238,W19-4002,0,0.0628762,"Missing"
2021.findings-emnlp.238,2021.eacl-main.108,0,0.0439871,"Missing"
2021.findings-emnlp.238,P16-2041,0,0.0517452,"Missing"
2021.findings-emnlp.238,P19-1279,0,0.0277664,"es as well as predicted ones from contextual link prediction. We build state-of-the-art EGs. • We show that EGs in turn improve contextual link prediction. • We release a dataset containing the extracted triples grounded in context, for future research. 2791 Our code and data are available at https://github. com/mjhosseini/open_contextual_link_ pred. 2 Related Work viv et al., 2021). These models do not probe for relations because a) They face technical challenges in processing multi-token relations; and b) Relations can be expressed in many different ways. The matching-the-blank (MTB) model (Soares et al., 2019) learns relation embeddings by encouraging relations that share the same entity-pairs to have similar embeddings. This is similar to our training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts"
2021.findings-emnlp.238,N15-1098,0,0.0298638,"upervised R(t1 , t2 ) as the set of relations with types t1 , t2 , model that fine-tunes pre-trained LMs directly on a or t2 , t1 . For example, R(Person, Location) intraining portion of entailment datasets. They report better results than EGs, but our focus is different. cludes born in(Person,Location), birthplace of (Location,Person), etc. Similarly, we define R(e1 , e2 ) Unlike their method, our approach is unsupervised as the set of relations r ∈ R such that (e1 , r, e2 ) is and is not capable of learning potential artifacts a valid (extracted) triple. For example, R(Barack from datasets (Levy et al., 2015). In addition, we Obama, Hawaii) includes born in3 , visit, etc. explicitly build EGs by doing machine-reading over Link prediction and entailment can hold between large text corpora, and hence can explain the basis relations with the same entity order or the reverse for the beliefs captured in them. order. When the two entity types are identical, we Pre-trained LMs for Link Prediction. KGkeep two copies of the relations one for each entity BERT (Yao et al., 2019) uses contextual represenorder. For example, acquire(Org1 ,Org2 ) predicts tations for KG completion. However, they form be part of"
2021.findings-emnlp.238,P98-2127,0,0.0947191,"Missing"
2021.findings-emnlp.238,2021.ccl-1.108,0,0.0396328,"Missing"
2021.findings-emnlp.238,2021.emnlp-main.840,1,0.762734,"Missing"
2021.findings-emnlp.238,C08-1107,0,0.294776,"ings. This is similar to our training, but has two main differences: First, our contextual link prediction model outputs a directional score between relations in context (e.g., acquire in Figure 1) and hypothesis relations (e.g., own), while MTB learns a symmetric similarity score. Second, we can predict a score for any hypothesis relation as long as the relation is previously observed somewhere in the corpus with any other entities (§3.2). Relational Entailment Graphs. Earlier attempts take a local approach and predict entailment relations independently from each other (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Berant et al. (2011, 2015) and Hosseini et al. (2018) propose a global approach where the dependencies between the entailment relations are taken into account. They first build a local typed EG for any plausible type pair. They then build global EGs that satisfy soft or hard constraints such as the transitivity of entailment. The constraints consider the structures both across typed EGs and inside each graph. In this work, we improve the local entailment scores, which in turn improves the global EGs. Hosseini et al. (2019) perform standard link prediction to add more coverage to the EGs by a"
2021.findings-emnlp.238,D13-1183,0,0.0319067,"Missing"
2021.findings-emnlp.238,D19-1250,0,0.0573115,"Missing"
2021.naacl-main.35,P14-1041,0,0.589919,") b1 b9 : x 2 , b2 : e 9 , b2 : x3 , b8 : t4 b2 b9 : Pred(x2 , male.n.02) b2 : Pred(e1 , play.v.03) b2 : Agent(e9 , x2 ) b2 : Theme(e9 , x3 ) b2 : Pred(x3 , piano.n.01) b8 : Pred(t1 , now.n.01) b2 : temp after(e9 , t1 ) (b) b3 b3 :  : b4 : x1 , b4 : e1 b4 b4 : Named(x1 , “tom”) b4 : Pred(e1 , stop.v.05) b4 : Agent(e1 , x1 ) b2 : Pred(x2 , male.n.02) b4 : Patient(e1 , x2 ) b4 : temp before(e1 , e9 ) CONTRAST(b2 , b3 ) Figure 2: DRS from Figure 1 with (a) shuffled conditions and (b) different variable names. to generate text from DRSs have been few and far between (however see Basile 2015 and Narayan and Gardent 2014 for notable exceptions). This is primarily due to two properties of DRS-based semantic representations which render generation from them challenging. Firstly, DRS conditions are unordered representing a set (rather than a list).2 A hypothetical generator would have to produce the same output text for any DRSs which convey the same meaning but appear different due to their conditions having a different order (see Figures 1 and 2a which are otherwise identical but the order of conditions in boxes b1 and b4 varies). The second challenge concerns variables and their prominent status in DRSs. Vari"
2021.naacl-main.35,P17-1014,0,0.0463635,"odels which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of t"
2021.naacl-main.35,P03-1069,1,0.608649,"n, and scope marked in brown. first performs sentence splitting and deletion operations over DRSs and then uses a phrase-based machine translation model for surface realization. Our work is closest to Basile (2015); we share the same goal of generating from DRSs, however, our model is trained end-to-end and can perform long-form generation for documents and sentences alike. We also adopt an ordering component, but we order DRS conditions rather than lexical items, and propose a model capable of inferring a global order. There has been long-standing interest in information ordering within NLP (Lapata, 2003; Abend et al., 2015; Chen et al., 2016; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Yin et al., 2019; Honovich et al., 2020). Our innovation lies in conceptualizing ordering as a graph scoring task which can be further realized with graph neural network models (Wu et al., 2020). 5 Conclusions the generation task. We have introduced a novel sibling treeLSTM for encoding DRSs rendered as trees and shown it is particularly suited to trees with wide branches. We have experimentally demonstrated that our encoder coupled with a graph-based condition ordering model outperforms stro"
2021.naacl-main.35,P18-1040,1,0.501651,"Pred(E0 , step.v.01) B0 : Agent(E0 , X0 ) B−2 : Pred(X−2 , male.n.01) B0 : Patient(E0 , X−2 ) B0 : temp before(E0 , E−1 ) Figure 4: DRSs with relative variables. merging variables in the top layer with variables in the bottom layer via introducing special conditions. We collect variables in top layers of DRS boxes to construct a dictionary d = {v : b}, where v denotes a variable and b is a presupposition box label (e.g., x1 : b1 ). We then move variables 2.1 DRS-to-Tree Conversion from the top to the bottom layer by expressing them as special conditions b : Ref(v) and placing The algorithm of Liu et al. (2018) renders DRSs them before conditions on variable v. For example, in a tree-style format. It constructs trees based on DRS conditions in the bottom box layers, with- b6 : x1 in Figure 1 becomes special condition b6 : Ref(x1 ) and is placed before condition b6 : Pred(x1 , out considering variables in the top layer. This male.n.02) in Figure 3(a). results in oversimplified semantic representations Once top variables have been rewritten as speand information loss (e.g., presuppositions cannot cial conditions, the resulting DRSs are converted be handled). We improve upon their approach by into tree"
2021.naacl-main.35,P19-1629,1,0.791072,"Boxes b1 and b2 are DRSs, the top layers contain variables (e.g., x1 , x2 ) indicating discourse referents and the bottom layers contain conditions (e.g., Named(x3 , “tom”)) representing information about discourse referents. Variables and conditions have pointers (denoted by b in the figure) pointing to the boxes where they should be interpreted.1 Predicates are disambiguated to their Wordnet (Fellbaum, 1998) senses (e.g., male.n.02 and play.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et"
2021.naacl-main.35,W19-1203,1,0.826837,"Boxes b1 and b2 are DRSs, the top layers contain variables (e.g., x1 , x2 ) indicating discourse referents and the bottom layers contain conditions (e.g., Named(x3 , “tom”)) representing information about discourse referents. Variables and conditions have pointers (denoted by b in the figure) pointing to the boxes where they should be interpreted.1 Predicates are disambiguated to their Wordnet (Fellbaum, 1998) senses (e.g., male.n.02 and play.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et"
2021.naacl-main.35,2020.acl-main.416,1,0.776526,"s a key role in the generation: both Seq and Sibling models improve when ordering of conditions is explicitly incorporated (either with Counting or GraphOrder). We observe that the combination of Sibling with GraphOrder achieves the best results (58.73 BLEU). Table 5 presents our results on the test set. We compare our Sibling encoder against a sequential one. Both models are interfaced with GraphOrder. We also compare to a previous graph-to-text model (Song et al., 2018; Damonte and Cohen, 2019) which has been used for generating from AMRs. We converted DRSs to graphs following the method of Liu et al. (2020); graphs were encoded with a GCRN (Seo et al., 2018) and decoded with an LSTM. As can be seen, Sibling+GraphOrder outperforms all comparison systems achieving a BLEU of 59.26. However, compared to ideal-world generation (see Table 3) there is still considerable room for improvement. somewhat problematic in our case as it merely calculates word overlap between generated and goldstandard text without assessing whether model output is faithful to the semantics of the input (i.e., the DRS meaning representations). To this effect, we present examples of text generated by our model, demonstrating ho"
2021.naacl-main.35,D11-1149,0,0.0397722,"y.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositi"
2021.naacl-main.35,D19-1314,0,0.0943767,"t al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of the man in the context of the two-sentence discourse. 397 Proceedings of the 2021"
2021.naacl-main.35,P17-1099,0,0.046218,"d generation (dev set); improvements compared to Seq shown in parentheses. 3.2 [h0 , h1 , ..., hn−1 ] = BiLSTM([x0 , x1 , ..., xn ]) In addition, we included various models with treebased encoders: ChildSum, is the bidirectional childsum-treeLSTM encoder of Tai et al. (2015); it operates over right-branch binarized trees; Nary, is the bidirectional Nary-TreeLSTM of Tai et al. (2015), again over right-branch binarized trees;5 and Sibling is our bidirectional sibling-TreeLSTM. All models were equipped with the same LSTM decoder, global attention (Bahdanau et al., 2015), and the copy strategy of See et al. (2017). The embedding dimension was 300 and the hidden dimension 512. All encoders and decoders have 2 layers. The detailed settings are shown in BLEU Seq+Naive Seq+Random Seq+Counting Seq+GraphOrder 4.61 24.34 (16.77) 45.17 55.57 Sibling+Naive Sibling+Random Sibling+Counting Sibling+GraphOrder 6.98 43.43 (0.26) 49.54 58.73 Table 4: Real-world generation (dev set). For Random, we report average results after shuffling 5 times (variance shown in parentheses). Models BLEU Parameters Graph Seq+GraphOrder Sibling+GraphOrder 45.72 55.28 59.26 30.1M 32.4M + 6.1M 34.5M + 6.1M Ideal-World Generation Models"
2021.naacl-main.35,P18-1150,0,0.441532,"ext in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of the man in the cont"
2021.naacl-main.35,P15-1150,0,0.0972604,"Missing"
2021.naacl-main.35,Q18-1043,0,0.127145,"Missing"
2021.naacl-main.35,W19-0504,0,0.040473,"Missing"
2021.naacl-main.35,D18-1112,0,0.0212898,"(P (x) → Q(x))). 4 Related Work Much previous work has focused on text generation from formal representations of meaning focusing exclusively on isolated sentences or queries. The literature offers a collection of approaches to generating from AMRs most of which employ neural models and structured encoders (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020). Other work generates text from structured query language (SQL) adopting either sequence-to-sequence (Iyer et al., 2016) or graph-to-sequence models (Xu et al., 2018). Basile (2015) was the first to attempt generation from DRT-based meaning representations. He 3.4 Analysis proposes a pipeline system which operates over Figure 7 shows model performance on test set graphs and consists of three components: an alignagainst DRS size (i.e., the number of nodes in a ment module learns the correspondence between DRS tree). Perhaps unsurprisingly, we see that gen- surface text and DRS structure, an ordering moderation quality deteriorates with bigger DRSs (i.e., ule determines the relative position of words and with &gt;1,600 nodes). phrases in the surface form and a"
2021.naacl-main.35,D19-1548,0,0.211358,"ancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of the man in the context of the two-sentence discourse. 397 Proceedings of the 2021 Conference of the"
2021.naacl-main.35,C80-1061,0,0.616058,"preted.1 Predicates are disambiguated to their Wordnet (Fellbaum, 1998) senses (e.g., male.n.02 and play.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Re"
2021.naacl-main.35,2020.tacl-1.2,0,0.197659,"is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of the man in the context of the two-sentence discourse. 397 Proceedings of the 2021 Conference of the North American Chapter of the Associ"
2021.naacl-main.35,D19-1098,0,0.0181705,"the encoder- fjs (Equation (3)) and fjp (Equation (4)) for its decoder framework, where an encoder is used to neighbor cell and the last child cell, respectively. encode input DRS trees and a decoder outputs a The memory of the current cell cj (Equation (5)) is sequence of words. A limitation of sequential en- updated by the gated sum of its cell input represencoders is that they only allow sequential informa- tation and the memories of its neighbor and child tion propagation without considering the structure cells. The hidden representation of current node hj of the input (Tai et al., 2015; Wang et al., 2019). In is computed with its output gate oj (Equation (6)). 400 Pred “male.n.02” a0 - of Agent of a1 temp after a0-of a0 f (b) a 1-o R∗ = arg max S COREK (R|Rset ), Pred “now.n.01” Pred “play.v.03” a0 a1 As discussed previously, DRSs at test time may exhibit an arbitrary order of conditions, which our model should be able to handle. Our solution is to to reorder conditions prior to generation by learning a latent canonical order from training data (e.g., to recover boxes b1 and b3 in Figure 1 from boxes b1 and b3 in Figure 2). More formally, given a set of conditions Rset , we obtain an optimal o"
2021.naacl-main.35,2007.mtsummit-ucnlg.4,0,0.095697,"lbaum, 1998) senses (e.g., male.n.02 and play.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carryin"
C18-1115,P17-1183,0,0.113652,"models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertion requires identifying both the point of insertion and the character that needs to be inserted, while deletion is a “binary” decision – either the decoding algorithm chooses to delete a specific symbol or not to. A similar observation was made by Schnober et al. (2016), R"
C18-1115,P05-1022,0,0.0769671,"n placeholders that are deleted during decoding. In contrast to the other datasets, where perhaps the assumption of monotonicity is too strong, seq2seq models with an attention mechanism designed to handle monotonic alignments (Aharoni and Goldberg, 2017) perform quite well on this task, even better than the vanilla seq2seq models. Finally, we also consider the case in which we use an ensemble method, combining several spectral models together (the top 50 performing models on the development set from the hyperparameter sweep). We combine the models using a MaxEnt reranker such as described by Charniak and Johnson (2005) and Narayan and Cohen (2015). We find that the ensemble approach does improve the results significantly for the 13SIA dataset, and also for the 2PKE-z dataset. 5 Conclusion We presented a technique to frame general string transduction problems as sequence labeling. Our technique works by adding to the string to be transduced additional insertion markers, which are later potentially deleted during the sequence labeling process. Our approach is general and works with any sequence labeling algorithm. We developed our technique with conditional random fields, refinement hidden Markov models and n"
C18-1115,P12-1024,1,0.823958,"ng and then remove the D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recognition (OCR) correction and morphological inflection. We believe that this array of datasets represents a broad set of problems in which string transduction as sequence 3 We use the code from https://github.com/shashiongithub/Rainbow-Parser. 1363 labeling can be tested. We apply the insertion technique to a set of sequence labelers: conditional random fields, refinement HMMs with expectation-maximization, spectral algor"
C18-1115,N13-1015,1,0.800426,"he D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recognition (OCR) correction and morphological inflection. We believe that this array of datasets represents a broad set of problems in which string transduction as sequence 3 We use the code from https://github.com/shashiongithub/Rainbow-Parser. 1363 labeling can be tested. We apply the insertion technique to a set of sequence labelers: conditional random fields, refinement HMMs with expectation-maximization, spectral algorithms, and neural net"
C18-1115,P14-2102,0,0.0217992,"orks best for problems in which string transduction imposes a strong notion of locality (no long range dependencies). We experiment with spelling correction for social media, OCR correction, and morphological inflection, and we see that it behaves better than seq2seq models and yields state-of-the-art results in several cases. 1 Introduction String transduction (mapping one string to another) is an essential ingredient in the natural language processing (NLP) toolkit that helps solve problems ranging from morphological inflection (Dreyer et al., 2008) and lemmatization to spelling correction (Cotterell et al., 2014), text normalization (Porta and Sancho, 2013) and machine translation (Kumar et al., 2006). Finite-state technology is often used with string transduction (Allauzen et al., 2007), especially when there is a strong notion of locality in the transduction problem, i.e., when there are no long range dependencies between the predicted characters. More recently, neural network methods have become more common for such problems using the seq2seq models that were introduced for machine translation (Bahdanau et al., 2015). Similarly, sequence labeling algorithms have been the mainstay for an array of pr"
C18-1115,D08-1113,0,0.0414885,"roach can be used with any sequence labeling algorithm and it works best for problems in which string transduction imposes a strong notion of locality (no long range dependencies). We experiment with spelling correction for social media, OCR correction, and morphological inflection, and we see that it behaves better than seq2seq models and yields state-of-the-art results in several cases. 1 Introduction String transduction (mapping one string to another) is an essential ingredient in the natural language processing (NLP) toolkit that helps solve problems ranging from morphological inflection (Dreyer et al., 2008) and lemmatization to spelling correction (Cotterell et al., 2014), text normalization (Porta and Sancho, 2013) and machine translation (Kumar et al., 2006). Finite-state technology is often used with string transduction (Allauzen et al., 2007), especially when there is a strong notion of locality in the transduction problem, i.e., when there are no long range dependencies between the predicted characters. More recently, neural network methods have become more common for such problems using the seq2seq models that were introduced for machine translation (Bahdanau et al., 2015). Similarly, sequ"
C18-1115,P02-1001,0,0.112997,"e between the two, as string transduction can re-write a string into a completely different string, while sequence labeling has a stronger notion of locality. As such, sequence labeling is considered an easier problem than general string transduction. Yet, we show in this paper how to exploit sequence labeling algorithms, with their flexibility and efficiency, to do general string transduction. 1361 3 Transduction as Insertion and Labeling Most approaches to string transduction involve inducing an alignment between symbols in the input and output strings (Knight and Graehl, 1998; Clark, 2001; Eisner, 2002; Azawi et al., 2013; Bailly et al., 2013). In an alignment, unaligned input symbols are called deletions, while unaligned output symbols are called insertions. It is challenging to jointly induce alignments and learn a transduction model. A second challenge is that, at prediction time, it is difficult to predict the insertions, as there can be an arbitrary number of them between any two input symbols. The prediction problem would be much simpler if the insertion positions were in place, because the model would only need to decide which symbol goes in each position. Our approach is based on th"
C18-1115,P16-2090,0,0.0418981,"Missing"
C18-1115,D15-1214,1,0.833557,"during decoding. In contrast to the other datasets, where perhaps the assumption of monotonicity is too strong, seq2seq models with an attention mechanism designed to handle monotonic alignments (Aharoni and Goldberg, 2017) perform quite well on this task, even better than the vanilla seq2seq models. Finally, we also consider the case in which we use an ensemble method, combining several spectral models together (the top 50 performing models on the development set from the hyperparameter sweep). We combine the models using a MaxEnt reranker such as described by Charniak and Johnson (2005) and Narayan and Cohen (2015). We find that the ensemble approach does improve the results significantly for the 13SIA dataset, and also for the 2PKE-z dataset. 5 Conclusion We presented a technique to frame general string transduction problems as sequence labeling. Our technique works by adding to the string to be transduced additional insertion markers, which are later potentially deleted during the sequence labeling process. Our approach is general and works with any sequence labeling algorithm. We developed our technique with conditional random fields, refinement hidden Markov models and neural networks. We tested our"
C18-1115,N16-1076,0,0.0888006,"he sentence in the target language can be shorter, longer, and contain a significant amount of re-ordering. Indeed, re-ordering is a challenge with string transduction. A sequence labeling model usually maintains higher order of monotonicity (such as with Markovian models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertio"
C18-1115,C16-1160,0,0.34387,"with string transduction. A sequence labeling model usually maintains higher order of monotonicity (such as with Markovian models), while for general string transduction problems, one needs models such as encoder-decoders or grammatical models. Such expressive models, on the other hand, are not a good fit for string transduction problems with a strong notion of locality, as they are too complex and lead to weaker generalization power for such problems (Rastogi et al., 2016). We show this weakness in our experiments with seq2seq models supporting similar conclusions in previous work such as by Schnober et al. (2016). Our experiments show that even when seq2seq models are incorporated with hard monotonic attention (Aharoni and Goldberg, 2017), our reduction to sequence labeling outperforms such models on certain problems with a strong notion of locality. Our approach to reduce string transduction to sequence labeling relies on a simple observation: in most cases in transduction, it is easier to delete a symbol than to insert a symbol. This is true because insertion requires identifying both the point of insertion and the character that needs to be inserted, while deletion is a “binary” decision – either t"
C18-1115,W16-2406,0,0.274929,"Missing"
C18-1115,W13-3507,1,0.894402,"nd the transformation of the final y to i to accommodate the new suffix). For example, the past tense of may is also may, and in this case, if the string may was also in the training set, may should retain its form and be transduced to may. With the context function mentioned above we would have the pair mayε (input sequence), mayD (output sequence) to learn from. During decoding, we apply the σ function on the input string and then remove the D symbols from the string. 3.2 Sequence Labeling Models We experiment with three models for sequence labeling: refinement hidden Markov models (R-HMMs; Stratos et al., 2013), conditional random fields (CRFs; Lafferty et al., 2001) and bidirectional Long Short Term Memory neural networks (biLSTMs; Graves and Schmidhuber, 2005). A graphical depiction of the models that we experiment with is given in Figure 1. For R-HMMs, we experiment with two learning algorithms, the expectation-maximization algorithm (EM; Dempster et al., 1977) and an L-PCFG spectral learning algorithm (Cohen et al., 2012; Cohen et al., 2013).3 4 Experiments We describe in this section a set of experiments on datasets from three problems: social media spelling correction, optical character recogn"
D07-1022,P06-1084,0,0.244988,"lready (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing (see, for example, Cowan et al., 2005, and Buchholz et al., 2006). Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation become"
D07-1022,W05-0706,0,0.208385,"Missing"
D07-1022,H91-1060,0,0.172336,"izes each violation (by a morpheme) of a one-to-one correspondence,6 and each character edit required to transform one side of a correspondence into the other (without whitespace). Word boundaries are (here) known and included as index positions. In the case where m ~ˆ = m ~ ∗ (or equal up to whitespace) the method is identical to Parseval (and also to Tsarfaty, 2006). POS tag accuracy is evaluated the same way, for the same reasons; we report F1 -accuracy for tagging and parsing. 5.2 Experimental Comparison In our experiment we vary four settings: Evaluation Measures The “Parseval” measures (Black et al., 1991) are used to evaluate a parser’s phrase-structure trees against a gold standard. They compute precision and recall of constituents, each indexed by a label and two endpoints. As pointed out by Tsarfaty (2006), joint parsing of morphology and syntax renders this indexing inappropriate, since it assumes the yields of the trees are identical—that assumption is violated if there are any errors in the hypothesized m. ~ Tsarfaty (2006) instead indexed by non-whitespace character positions, to deal with segmentation mismatches. In general (and in this work) that is still insufficient, since L(~x) may"
D07-1022,W06-2920,0,0.0913054,"Missing"
D07-1022,A00-2018,0,0.280817,"Missing"
D07-1022,H05-1100,0,0.149941,"Missing"
D07-1022,W04-3246,0,0.0306848,"nput to the parser was already (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing (see, for example, Cowan et al., 2005, and Buchholz et al., 2006). Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of amb"
D07-1022,J01-2001,0,0.00963688,"of 88,747 words (4,783 sentences) and parsed it using a probabilistic model. However, they assumed that the input to the parser was already (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing (see, for example, Cowan et al., 2005, and Buchholz et al., 2006). Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morph"
D07-1022,P96-1024,0,0.0674228,"ate the full distribution over morphological analyses for the sentence by a simpler, sentence-specific unigram model that assumes each word’s analysis is to be chosen independently of the others. Note that our model (pL ) does not make such an assumption, only the approximate model p0L does, and the approximation is per-sentence. The idea resembles a mean-field variational approximation for graphical models. Turning to implementation, we can solve for pL (m ~ i |~x) exactly using the forward-backward algorithm. We will call this method fvari,α (see Eq. 5). A closely related method, applied by Goodman (1996) is called minimum-risk decoding. Goodman called it “maximum expected recall” when applying it to parsing. In the HMM community it 4 In prior work involving factored syntax models— lexicalized (Klein and Manning, 2003b) and bilingual (Smith and Smith, 2004)—fpoe,1 was applied, and the asymptotic runtime went to O(n5 ) and O(n7 ). fpoe,α (~x) = argmax log pG (τ, m) ~ + α log pL (m ~ |~x) (4) hm,τ ~ i∈GEN(~ x) fvari,α (~x) = argmax log pG (τ, m) ~ +α Pn log pG (τ, m) ~ +α Pn hm,τ ~ i∈GEN(~ x) frisk,α (~x) = argmax hm,τ ~ i∈GEN(~ x) is sometimes called “posterior decoding.” Minimum risk decoding"
D07-1022,P05-1071,0,0.0273247,"ilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approximation of these models with two parsers: one a pipeline (segmentation → tagging →"
D07-1022,P01-1035,0,0.108436,"Missing"
D07-1022,C00-1042,0,0.128821,"Missing"
D07-1022,J98-4004,0,0.0164249,"Missing"
D07-1022,P03-1054,0,0.0360985,"rs inherent in pipelines. 3 Joint Inference of Morphology and Syntax GEN(~x) = {hm, ~ τi |m ~ ∈ L(~x), τ ∈ DG (m)} ~ 3.2 Product of Experts Our mapping f (~x) is based on a joint probability model p(τ, m ~ |~x) which combines two probability models pG (τ, m) ~ (a PCFG built on the grammar G) and pL (m ~ |~x) (a morphological disambiguation model built on the lexicon L). Factoring the joint model into sub-models simplifies training, since we can train each model separately, and inference (parsing), as we will see later in this section. Factored estimation has been quite popular in NLP of late (Klein and Manning, 2003b; Smith and Smith, 2004; Smith et al., 2005a, inter alia). The most obvious joint parser uses pG as a conditional model over trees given morphemes and maximizes the joint likelihood: flik (~x) We now formalize the problem and supply the necessary framework for performing joint morphological disambiguation and syntactic parsing. 3.1 classifier. We use DG (m) ~ ⊆ T to denote the set of valid trees under a grammar G (here, a PCFG with terminal alphabet M) for morpheme sequence m. ~ To be precise, f (~x) selects a mutually consistent morphological and syntactic analysis from = argmax pG (τ |m) ~"
D07-1022,W04-3230,0,0.0168627,"terature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approx"
D07-1022,2004.tmi-1.1,0,0.0251319,"τ ~ i∈GEN(~ x) pL (m ~ 0 , ~x) pG (τ 0 , m) ~ argmax τ0 Notation and Morphological Sausages Let X be the language’s word vocabulary and M be its morpheme inventory. The set of valid analyses for a surface word is defined using a morphological lexicon L, which defines L(x) ⊆ M+ . L(~x) ⊆ (M+ )+ (sequence of sequences) is the set of wholesentence analyses for sentence ~x = hx1 , x2 , ..., xn i, produced by concatenating elements of L(xi ) in order. L(~x) can be represented as an acyclic lattice with a “sausage” shape familiar from speech recognition (Mangu et al., 1999) and machine translation (Lavie et al., 2004). Fig. 1h shows a sausage lattice for a sentence in Hebrew. We use m ~ to denote an element of L(~x) and m ~ i to denote an element of L(xi ); in general, m ~ = hm ~ 1, m ~ 2 , ..., m ~ n i. We are interested in a function f : X+ → (M+ )+ × T, where T is the set of syntactic trees for the language. f can be viewed as a structured 210 (1) hm,τ ~ i∈GEN(~ x) m ~0 This is not straightforward, because it involves summing up the trees for each m ~ to compute pG (m), ~ which calls for the O(|m| ~ 3 )-Inside algorithm to be called on each m. ~ Instead, we use the joint, pG (τ, m), ~ which, strictly sp"
D07-1022,J95-3004,0,0.104992,"they assumed that the input to the parser was already (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing (see, for example, Cowan et al., 2005, and Buchholz et al., 2006). Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involv"
D07-1022,P05-1010,0,0.00730791,"n the grammar constant (at best, quadratic in the number of nonterminals), parsing with pI is not computationally attractive.4 fpoe,α is not, then, a scalable solution when we wish to use a morphology model pL that can make interdependent decisions about different words in ~x in context. We propose two new, efficient dynamic programming solutions for joint parsing. In the first, we approximate the distribution ~ |~x) using a unigram-factored model of the pL (M form in Eq. 3: Q ~i = m (7) ~ |~x) = ni=1 pL (M ~ |~x) p0L (m {z i } | posterior, depends on all of ~x Similar methods were applied by Matsuzaki et al. (2005) and Petrov and Klein (2007) for parsing under a PCFG with nonterminals with latent annotations. Their approach was variational, approximating the true posterior over coarse parses using a sentence-specific PCFG on the coarse nonterminals, created directly out of the true fine-grained PCFG. In our case, we approximate the full distribution over morphological analyses for the sentence by a simpler, sentence-specific unigram model that assumes each word’s analysis is to be chosen independently of the others. Note that our model (pL ) does not make such an assumption, only the approximate model p"
D07-1022,N07-1051,0,0.0350744,"est, quadratic in the number of nonterminals), parsing with pI is not computationally attractive.4 fpoe,α is not, then, a scalable solution when we wish to use a morphology model pL that can make interdependent decisions about different words in ~x in context. We propose two new, efficient dynamic programming solutions for joint parsing. In the first, we approximate the distribution ~ |~x) using a unigram-factored model of the pL (M form in Eq. 3: Q ~i = m (7) ~ |~x) = ni=1 pL (M ~ |~x) p0L (m {z i } | posterior, depends on all of ~x Similar methods were applied by Matsuzaki et al. (2005) and Petrov and Klein (2007) for parsing under a PCFG with nonterminals with latent annotations. Their approach was variational, approximating the true posterior over coarse parses using a sentence-specific PCFG on the coarse nonterminals, created directly out of the true fine-grained PCFG. In our case, we approximate the full distribution over morphological analyses for the sentence by a simpler, sentence-specific unigram model that assumes each word’s analysis is to be chosen independently of the others. Note that our model (pL ) does not make such an assumption, only the approximate model p0L does, and the approximati"
D07-1022,roark-etal-2006-sparseval,0,0.0212971,"old standard. They compute precision and recall of constituents, each indexed by a label and two endpoints. As pointed out by Tsarfaty (2006), joint parsing of morphology and syntax renders this indexing inappropriate, since it assumes the yields of the trees are identical—that assumption is violated if there are any errors in the hypothesized m. ~ Tsarfaty (2006) instead indexed by non-whitespace character positions, to deal with segmentation mismatches. In general (and in this work) that is still insufficient, since L(~x) may include m ~ that are not simply segmentations of ~x (see §4.2.1). Roark et al. (2006) propose an evaluation metric for comparing a parse tree over a sentence generated by a speech recognizer to a gold-standard parse. As in our case, the hypothesized tree could have a different yield than the original gold-standard 214 • Decoding algorithm: fpoe,α , frisk,α , or fvari,α (§3.3). • Syntax model: Gvan or Gv=2 (§4.1). crf • Morphology model: puni L or pL (§4.2). In the latter case, we can use the scores over morpheme sequences only (summing out tags before lattice parsing; denoted m.-pcrf L ) or the full model over 7 morphemes and tags, denoted t.-pcrf L . • α, the relative strengt"
D07-1022,W04-3207,1,0.811606,"3 Joint Inference of Morphology and Syntax GEN(~x) = {hm, ~ τi |m ~ ∈ L(~x), τ ∈ DG (m)} ~ 3.2 Product of Experts Our mapping f (~x) is based on a joint probability model p(τ, m ~ |~x) which combines two probability models pG (τ, m) ~ (a PCFG built on the grammar G) and pL (m ~ |~x) (a morphological disambiguation model built on the lexicon L). Factoring the joint model into sub-models simplifies training, since we can train each model separately, and inference (parsing), as we will see later in this section. Factored estimation has been quite popular in NLP of late (Klein and Manning, 2003b; Smith and Smith, 2004; Smith et al., 2005a, inter alia). The most obvious joint parser uses pG as a conditional model over trees given morphemes and maximizes the joint likelihood: flik (~x) We now formalize the problem and supply the necessary framework for performing joint morphological disambiguation and syntactic parsing. 3.1 classifier. We use DG (m) ~ ⊆ T to denote the set of valid trees under a grammar G (here, a PCFG with terminal alphabet M) for morpheme sequence m. ~ To be precise, f (~x) selects a mutually consistent morphological and syntactic analysis from = argmax pG (τ |m) ~ · pL (m ~ |~x) = pL (m,"
D07-1022,P05-1003,0,0.100974,"ogical processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approximation of these mod"
D07-1022,H05-1060,1,0.902295,"ogical processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approximation of these mod"
D07-1022,P06-3009,0,0.484611,"uation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approximation of these models with two parsers: one a pipeline (segmentation → tagging → parsing), the other involved joint inference of segmentation and tagging, with the result piped to the parser. The latter was slightly more accurate. Tsarfaty discussed but did not carry out joint inference. In a morphologically rich language, the different morphemes that make up a word can play a variety of different syntactic roles. A reasonable linguistic analysis might"
D07-1022,W05-0702,0,0.0679741,"Missing"
D07-1022,J03-4003,0,\N,Missing
D11-1005,P10-1131,0,0.76042,"sed parameter estimation. Some previous multilingual research, such as Bayesian parameter tying across languages (Cohen and Smith, 2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem et al. (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. Our experiments demonstrate that the presented method outperforms strong state-of-the-art unsupervised baselines for both tasks. Our approach can be applied to other problems in which a"
D11-1005,N10-1083,0,0.0976671,"Missing"
D11-1005,W06-2920,0,0.0501669,"l during unsupervised learning, and are initialized using standard methods. 6 Experiments and Results In this section, we describe the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared"
D11-1005,D08-1092,0,0.0368004,"such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to lear"
D11-1005,D07-1022,1,0.760276,"escribed in §6.4. All tag induction uses a dictionary as specified in §6.3. The last row in this table indicates the best results using multilingual guidance taken from our methods in Table 3. “Avg” denotes macro-average across the ten languages. 2. Apply the fine-grained tagger to the words in the training data for the dependency parser. We consider two variants: the most probable assignment of tags to words (denoted “Pipeline”), and the posterior distribution over tags for each word, represented as a weighted “sausage” lattice (denoted “Joint”). This idea was explored for joint inference by Cohen and Smith (2007). 3. We apply the Mixture+EM unsupervised parser learning method from §6.4 to the automatically tagged sentences, or the lattices. 4. Given the two models, we infer POS tags on the test data using DG or Mixture+DG to get a lattice (Joint) or a sequence (Pipeline) and then parse using the model from the previous step.10 The resulting dependency trees are evaluated against the gold standard. Results are reported in Table 4. In almost all cases, joint decoding of tags and trees performs better than the pipeline. Even though our part-of-speech tagger with multilingual guidance outperforms the comp"
D11-1005,N09-1009,1,0.852683,"begin with supervised maximum likelihood estimates for models of the helper languages. In the second stage, we learn a model for the target language using unannotated data, maximizing likelihood over interpolations of the helper language models’ distributions. The tying is performed at the parameter level, through coarse, nearly-universal syntactic categories (POS tags). The resulting model is then used to initialize learning of the target language’s model using standard unsupervised parameter estimation. Some previous multilingual research, such as Bayesian parameter tying across languages (Cohen and Smith, 2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem"
D11-1005,P11-1061,1,0.909688,"and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annot"
D11-1005,A94-1009,0,0.0200607,"most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essentially equating to a"
D11-1005,P10-2036,0,0.0191573,"Missing"
D11-1005,N06-1041,0,0.0979683,"described in §5. We call this model “Uniform+DG.” The second version estimates the mixture coefficients to maximize likelihood, then expands the POS tags (§5), using the result to initialize training of the final model. We call this model “Mixture+DG.” No Tag Dictionary For each of the above configurations, we ran purely unsupervised training without a tag dictionary, and evaluated using one-to-one mapping accuracy constraining at most one HMM state to map to a unique treebank tag in the test data, using maximum bipartite matching. This is a variant of the greedy one-to-one mapping scheme of Haghighi and Klein (2006).8 With a Tag Dictionary We also ran a second version of each experimental configuration, where we used a tag dictionary to restrict the possible path sequences of the HMM during both learning and inference. This tag dictionary was constructed only from the training section of a given language’s treebank. It is widely known that such knowledge improves the quality of the model, though it is an open debate whether such knowledge is realistic to assume. For this experiment we removed punctuation from the training and test data, enabling direct use within the dependency grammar induction experime"
D11-1005,N09-1012,0,0.0439172,"dency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10 The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. 59 directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dependencies which performs better than a pipeline. Future work might consider exploiting a larger number of treebanks, and more powerful techniques for combining models than sim"
D11-1005,N07-1018,0,0.0117058,"d data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al.,"
D11-1005,P04-1061,0,0.550547,"available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2"
D11-1005,J93-2004,0,0.0374901,"tandard methods. 6 Experiments and Results In this section, we describe the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007)."
D11-1005,D11-1006,0,0.610872,"nson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target la"
D11-1005,J94-2001,0,0.0620279,"bset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essential"
D11-1005,D10-1120,0,0.401513,"2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem et al. (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. Our experiments demonstrate that the presented method outperforms strong state-of-the-art unsupervised baselines for both tasks. Our approach can be applied to other problems in which a subset of the model parameters can be linked across languages. We also experiment with unsupervised learning of depen"
D11-1005,P07-1049,0,0.0401133,"tilingual guidance outperforms the completely unsupervised baseline, there is not always an advantage of using this multilingually guided partof-speech tagger for dependency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10 The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. 59 directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dep"
D11-1005,P05-1044,1,0.488925,"rts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essentially equating to a traditional multinomial"
D11-1005,D09-1086,0,0.10063,"and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another lan"
D11-1005,W04-3207,1,0.820928,"t al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work men"
D11-1005,P08-1084,0,0.0323582,"ts purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work mentioned above, our framework does not rely on parallel da"
D11-1005,P09-1009,0,0.126357,"Missing"
D11-1005,N10-1116,0,0.0267179,"fficients are learned automatically fares better than uniform weighting. With a tag dictionary, the multilingual variants outperform the baseline in seven out of ten cases, and the learned mixture outperforms or matches the uniform mixture in five of those seven (Table 2b). 6.4 Dependency Grammar Induction We next describe experiments for dependency grammar induction. As the basic grammatical model, we adopt the dependency model with valence (Klein and Manning, 2004), which forms the basis for stateof-the-art results for dependency grammar induction in various settings (Cohen and Smith, 2009; Spitkovsky et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010). As shown in Table 3, DMV obtains much higher accuracy in the supervised setting than the unsupervised setting, suggesting that more can be achieved with this model family.9 For this reason, and because DMV is easily interpreted as a PCFG, it is our starting point and baseline. We consider four conditions. The independent variables are (1) whether we use uniform β (all set to 1 4 ) or estimate them using EM (as described in §4), and (2) whether we simply use the mixture model to decode the test data, or to initialize EM for the DMV."
D11-1005,H05-1107,0,0.0956265,"s involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data f"
D11-1005,N01-1026,0,0.0625168,"of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to usi"
D11-1005,D07-1096,0,\N,Missing
D11-1114,W06-2922,0,0.201718,"s. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial"
D11-1114,P89-1018,0,0.0432038,"ng algorithm for simulating the computations of the system from §2–3. Given an input string w, our algorithm produces a compact representation of the set Γ (w), defined as the set of all possible computations of the model when processing w. In combination with the appropriate semirings, this method can provide for instance the highest probability computation in Γ (w), or else the probability of w, defined as the sum of all probabilities of computations in Γ (w). We follow a standard approach in the literature on dynamic programming simulation of stack-based automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989). More recently, this approach has also been applied by Huang and Sagae (2010) and by 1239 c1 σ h1 minimum stack length in c1 , . . . , cm cm i i+1 i σ i+1 buﬀer size  p(la2 |b3 , b2 , b1 ) = θbrd1 · θbrd22,b1 · θbla32,b2 ,b1 , h1  p(ra1 |b2 , b1 ) = θbrd1 · θbra21,b1 , σ buﬀer p(shb |b1 ) = θbsh1 b , ∀b ∈ Σ , c0 stack distributions p(t |σ) as follows: stack size h2 h3 j Figure 2: Schematic representation of the computations γ associated with item [h1 , i, h2 h3 , j]. Kuhlmann et al. (2011) to the simulation of projective transition-based parsers. The basic idea in this approach is to de"
D11-1114,W06-2920,0,0.0594122,"g able to handle only projective dependencies. This formulation permits parsing a subset of the non-projective trees, where this subset depends on the degree of the transitions. The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three. For instance, on training data for Czech containing 28,934 non-projective relations, 27,181 can be handled by degree two transitions, and 1,668 additional dependencies can be handled by degree three transitions. Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two. Our results are based on such a restriction. It is not difficult to extend our algorithms (§4) to higher degree transitions, but this comes at the expense of higher complexity. See §6 for more discussion on this issue. Let w = a0 · · · an−1 be an input string over Σ defined as in §2.1, with a0 = $. Our transition system for non-projective dependency parsing is (np) S (np) = (C, T (np) , I (np) , Ct ), 1236 Deg. 2 180 961 27181 876"
D11-1114,P99-1059,1,0.728094,"been applied in the computation of each item by the above algorithm, by encoding each application of a rule as a reference to the pair of items that were taken as antecedent in the inference. In this way, we obtain a parse forest structure that can be viewed as a hypergraph or as a non-recursive context-free grammar, similar to the case of parsing based on contextfree grammars. See for instance Klein and Manning 1241 (2001) or Nederhof (2003). Such a parse forest encodes all valid computations in Γ (w), as desired. The algorithm runs in O(n8 ) time. Using methods similar to those specified in Eisner and Satta (1999), we can reduce the running time to O(n7 ). However, we do not further pursue this idea here, and proceed with the discussion of exact inference, found in the next section. 5 Inference We turn next to specify exact inference with our model, for computing feature expectations. Such inference enables, for example, the derivation of an expectation-maximization algorithm for unsupervised parsing. Here, a feature is a function over computations, providing the count of a pattern related to a parameter. We denote by fbla32,b2 ,b1 (γ), for instance, the number of occurrences of transition la2 within γ"
D11-1114,H05-1036,0,0.0155602,"the items in the tabular algorithm. More specifically, given a string w, we associate each item [h1 , i, h2 h3 , j] defined as in §4 with two quantities: I([h1 , i, h2 h3 , j]) = X p(γ) ; (5) γ=([h1 ],βi ),...,([h2 ,h3 ],βj ) O([h1 , i, h2 h3 , j]) = X p(γ) · p(γ 0 ) . (6) σ,γ=([¢],β0 ),...,(σ|h1 ,βi ) γ 0 =(σ|h2 |h3 ,βj ),...,([¢,0],βn ) I([h1 , i, h2 h3 , j]) and O([h1 , i, h2 h3 , j]) are called the inside and the outside probabilities, respectively, of item [h1 , i, h2 h3 , j]. The tabular algorithm of §4 can be used to compute the inside probabilities. Using the gradient transformation (Eisner et al., 2005), a technique for deriving outside probabilities from a set of inference rules, we can also compute O([h1 , i, h2 h3 , j]). The use of the gradient transformation is valid in our case because the tabular algorithm is unambiguous (see §4). Using the inside and outside probabilities, we can now efficiently compute feature expectations for our Ep(γ|w) [fbla32,b2 ,b1 (γ)] = X p(γ |w) · fbla32,b2 ,b1 (γ) = γ∈Γ (w) X 1 · p(w) = X p(γ0 ) · p(γ1 ) · p(γ2 ) · p(la2 |b3 , b2 , b1 ) · p(γ3 ) γ1 =(σ|h1 ,βi ),...,(σ|h2 |h3 ,βk ), h1 ,h2 ,h3 ,h4 ,h5 , γ2 =(σ|h2 |h3 ,βk ),...,(σ|h2 |h4 |h5 ,βj ), s.t. ah2 =b"
D11-1114,P10-1151,1,0.917041,"Missing"
D11-1114,E09-1034,1,0.886882,"Missing"
D11-1114,J11-3004,1,0.87406,"Missing"
D11-1114,J99-4004,0,0.0436778,"vre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of eleme"
D11-1114,P10-1110,0,0.56228,"mith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlm"
D11-1114,P98-1106,0,0.150218,"ures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To the best of our knowledge, this paper is the first to Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the transition"
D11-1114,W01-1812,0,0.020146,"Missing"
D11-1114,D07-1015,0,0.0189125,"ed for unsupervised learning of non-projective dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been suc"
D11-1114,E09-1055,1,0.906123,"ional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To the best of our knowledge, this paper is the first to Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the transition-based model we use, together with a probabilistic generative i"
D11-1114,P11-1068,1,0.834515,"Missing"
D11-1114,D09-1005,0,0.0203863,"e recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent po"
D11-1114,W07-2216,1,0.910657,"d learning of non-projective dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for project"
D11-1114,H05-1066,0,0.196007,"Missing"
D11-1114,J03-1006,0,0.0175004,"e not larger than the size of the stack associated with the initial configuration. As a final remark, we observe that we can keep track of all inference rules that have been applied in the computation of each item by the above algorithm, by encoding each application of a rule as a reference to the pair of items that were taken as antecedent in the inference. In this way, we obtain a parse forest structure that can be viewed as a hypergraph or as a non-recursive context-free grammar, similar to the case of parsing based on contextfree grammars. See for instance Klein and Manning 1241 (2001) or Nederhof (2003). Such a parse forest encodes all valid computations in Γ (w), as desired. The algorithm runs in O(n8 ) time. Using methods similar to those specified in Eisner and Satta (1999), we can reduce the running time to O(n7 ). However, we do not further pursue this idea here, and proceed with the discussion of exact inference, found in the next section. 5 Inference We turn next to specify exact inference with our model, for computing feature expectations. Such inference enables, for example, the derivation of an expectation-maximization algorithm for unsupervised parsing. Here, a feature is a functi"
D11-1114,P05-1013,0,0.0605722,"community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) al"
D11-1114,W04-0308,0,0.121591,"are called reduce transitions, i.e., transitions that consume nodes from the stack. Notice that in the transition system at hand all the reduce transitions decrease the size of the stack by one element. Transition la1 creates a new arc with the topmost node on the stack as the head and the secondtopmost node as the dependent, and removes the latter from the stack. Transition ra1 is symmetric with respect to la1 . Transitions la1 and ra1 have degree one, as already explained. When restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of Nivre (2004). Transition la2 and transition ra2 are very similar to la1 and ra1 , respectively, but with the difference that they create a new arc between the topmost node in the stack and a node which is two positions below the topmost node. Hence, these transitions have degree two, and are the key components in parsing of non-projective dependencies. We turn next to describe the equivalence between our system and the system in Attardi (2006). The transition-based parser presented by Attardi pushes back into the buffer elements that are in the top position of the stack. However, a careful analysis shows"
D11-1114,J08-4003,0,0.65713,"spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1"
D11-1114,P09-1040,0,0.435291,"tical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space represe"
D11-1114,C96-2215,0,0.177144,"Missing"
D11-1114,D07-1014,0,0.0205805,"ve dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and S"
D11-1114,W03-3023,0,0.462207,"n in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as c"
D11-1114,C98-1102,0,\N,Missing
D14-1210,N10-1083,0,0.0851391,"Missing"
D14-1210,J07-2003,0,0.171693,"moments. We evaluate their performance on a Chinese–English translation task. The results indicate that we can achieve significant gains over the baseline with both approaches, but in particular the momentsbased estimator is both faster and performs better than EM. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) treat the translation problem as a context-free parsing problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to p"
D14-1210,N13-1015,1,0.849514,"milar, but in lieu of sparse feature functions in the spectral case, EM uses partial counts estimated with the current set of parameters. The nature of EM allows it to be susceptible to local optima, while the spectral approach comes with guarantees on obtaining the global optimum (Cohen et al., 2014). Lastly, computing the SVD and estimating parameters in the low-rank space is a one-shot operation, as opposed to the iterative procedure of EM, and therefore is much more computationally efficient. 4.1 Estimation with Spectral Method We generalize the parameter estimation algorithm presented in Cohen et al. (2013) to the syn5 We filtered rules with arity 3 and above (i.e., containing more than 3 NTs on the RHS). While the L-SCFG formalism is perfectly capable of handling such cases, it would have resulted in higher order tensors for our parameter structures. 1956 Inputs: Training examples (r(i) , t(i,1) , t(i,2) , t(i,3) , o(i) , b(i) ) for i ∈ {1 . . . M }, where r(i) is a context free rule; t(i,1) , t(i,2) , and t(i,3) are inside trees; o(i) is an outside tree; and b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function φ that maps inside trees t to feature-vectors φ(t) ∈ Rd . A function"
D14-1210,N13-1029,0,0.0166761,"thm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima’an (2011) select among linguistically motivated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over different derivations therefore their inside-outside algorithm is O(n6 ). In the syntax-directed translation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote parameter sharing (Venugopal et al., 2009; Hanneman and Lavie, 2013), which is the complement of our aim in this paper. The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables. Matsuzaki et al. (2005) look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while Petrov et al. (2006) learn a variable number of sub-categories per NT. The latter’s extension may be useful for finding the optimal number of latent states from the data in our case. The question of whether we can incorpor"
D14-1210,2006.amta-papers.8,0,0.0329779,"r1 , . . . , rN together with values h1 , . . . , hN for every NT in the tree. An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category grammar introduced by Chiang (2007) for HPBT in order to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended treeto-string transducer (xRs) formalism (Huang et al., 2006; Graehl et al., 2008) commonly used in syntax-directed translation, and phrase-based MT (Koehn et al., 2003). Marginal Inference with L-SCFGs. For a parameter t of rule r, the latent state h1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are associated with the inside trees of that NT. Since we do not assume conditional independence of these states, we need to consider all possible interactions, which can be compactly represented as a • Arity 2 (binary) rules (R2 ): 3rd -order tensor"
D14-1210,D10-1014,0,0.0162481,"unsom et al. (2008) present a Bayesian model for synchronous grammar induction, and place an appropriate nonparametric prior on the parameters. However, their starting point is to estimate a synchronous grammar with multiple categories from parallel data (using the word alignments as a prior), while we aim to refine a fixed grammar with additional latent states. Furthermore, their estimation procedure is extremely expensive and is restricted to learning up to five NT categories, via a series of mean-field approximations. Another approach is to explicitly attach a realvalued vector to each NT: Huang et al. (2010) use an external source-language parser for this purpose and score rules based on the similarity between a source sentence parse and the information contained in this vector, which explicitly requires the integration of a good-quality source-language parser. The EM-based algorithm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima’an (2011) select among linguistically motivated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over different d"
D14-1210,W01-1812,0,0.0708033,"he right and ×0 when multiplying on the left of the matrix. The decoder computes marginal probabilities for each skeletal rule in the 1 This operation is sometimes called a contraction. parse forest of a source sentence by marginalizing over the latent states, which in practice corresponds to simple tensor-vector products. This operation is not dependent on the manner in which the parameters were estimated. Figure 1 presents the tensor version of the inside-outside algorithm for decoding L-SCFGs. The algorithm takes as input the parse forest of the source sentence represented as a hypergraph (Klein and Manning, 2001), which is computed using a bottom-up parser with Earley-style rules similar to the algorithm in Chiang (2007). Hypergraphs are a compact way to represent a forest of multiple parse trees. Each node in the hypergraph corresponds to an NT span, and can have multiple incoming and outgoing hyperedges. Hyperedges, which connect one or more tail nodes to a single head node, correspond exactly to rules, and tail or head nodes correspond to children (RHS NTs) or parent (LHS NT). The function B(q) returns all incoming hyperedges to a node q, i.e., all rules such that the LHS NT of the rule corresponds"
D14-1210,N03-1017,0,0.0451428,"n-generalizable rules are utilized. Hence, the hope is that this work pro1953 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1964, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics motes the move towards translation models that directly model the conditional likelihood of translation rules via (potentially feature-rich) latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies (Koehn et al., 2003) from non-hierarchical phrase-based translation. 2 Latent-Variable SCFGs Before discussing parameter learning, we introduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference algorithm for marginalizing over latent states. We extend the definition of L-PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006) to synchronous grammars as used in machine translation (Chiang, 2007). A latent-variable SCFG (LSCFG) is a 6-tuple (N , m, ns , nt , π, t) where: • N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based translation (HPBT), the set"
D14-1210,D12-1021,1,0.84686,"ng problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petro"
D14-1210,D07-1072,0,0.0262621,"owever, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petrov et al., 2006; Liang et al., 2007; Cohen et al., 2014), where it reliably yields stateof-the-art phrase structure parsers based on generative models, but we are the first to apply it to translation. We first generalize the concept of latent PCFGs to latent-variable SCFGs (§2). We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states. Subsequently, two methods for parameter estimation are presented (§4): a spectral approach based on the method of moments, and an EM-based likelihood maximization. Results on a Chinese–English"
D14-1210,J06-4004,0,0.0963096,"Missing"
D14-1210,P11-1105,0,0.0177486,"0 ln(sum) 2 ln(sum)at word: Span starting 1 I ’ll bring it . Span starting at word: 0 I go away . I ’d like a shampoo and style . I ’d like a shampoo and style . I ’d like a shampoo and style . (d) MLE (e) Spectral m = 16 RI (f) EM m = 16 Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE, spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based pa"
D14-1210,P05-1010,0,0.835872,"ls that directly model the conditional likelihood of translation rules via (potentially feature-rich) latentvariable models which leverage information contained in the synchronous tree structure, instead of relying on a heuristic set of features based on empirical relative frequencies (Koehn et al., 2003) from non-hierarchical phrase-based translation. 2 Latent-Variable SCFGs Before discussing parameter learning, we introduce latent-variable synchronous context-free grammars (L-SCFGs) and discuss an inference algorithm for marginalizing over latent states. We extend the definition of L-PCFGs (Matsuzaki et al., 2005; Petrov et al., 2006) to synchronous grammars as used in machine translation (Chiang, 2007). A latent-variable SCFG (LSCFG) is a 6-tuple (N , m, ns , nt , π, t) where: • N is a set of non-terminal (NT) symbols in the grammar. For hierarchical phrase-based translation (HPBT), the set consists of only two symbols, X and a goal symbol S. • [m] is the set of possible hidden states associated with NTs. Aligned pairs of NTs across the source and target languages share the same hidden state. • [ns ] is the set of source side words, i.e., the source-side vocabulary, with [ns ] ∩ N = ∅. • [nt ] is the"
D14-1210,P10-4002,1,0.799568,"e upon a minimal grammar baseline with only a single category, but the spectral approach does better. In fact, it matches the performance of the standard H I ERO baseline, despite learning on top of a minimal grammar. 5.1 TRAIN (SRC) TRAIN (TGT) DEV (SRC) DEV (TGT) TEST (SRC) TEST (TGT) Data and Baselines The ZH-EN data is the BTEC parallel corpus (Paul, 2009); we combine the first and second development sets in one, and evaluate on the third development set. The development and test sets are evaluated with 16 references. Statistics for the data are shown in Table 1. We used the CDEC decoder (Dyer et al., 2010) to extract word alignments and the baseline hierarchical grammars, MERT tuning, and decoding. We used a 4-gram language model built from the target-side of the parallel training data. The Python-based implementation of the tensor-based decoder, as well as the parameter estimation algorithms is available at github.com/asaluja/spectral-scfg/. The baseline HIERO system uses a grammar extracted by applying the commonly used heurisZH-EN 334K 366K 7K 7.6K 3.8K 3.9K Table 1: Corpus statistics (in words). For the target DEV and TEST statistics, we take the first reference. tics (Chiang, 2007). Each r"
D14-1210,P11-1065,0,0.0309594,"Missing"
D14-1210,P13-1033,0,0.0115376,"tral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov models. This idea was extended to L-PCFGs (Cohen et al., 2014), and our approach can be seen as a bilingual or synchronous generalization. 7 Conclusion In this work, we presented an approach to refine synchronous grammars used in MT by inferring the laten"
D14-1210,N04-1035,0,0.144233,"Missing"
D14-1210,P06-1121,0,0.490662,"face statistics associated with the phrase pairs or rules. The weights of these features are then learned using a discriminative training algorithm (Och, 2003; Chiang, 2012, inter alia). In contrast, in this work we restrict the number of possible synchronous derivations for each sentence pair to just one; thus, derivation forests do not have to be considered, making parameter estimation more tractable.3 To achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules consistent with the word alignments, as opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees consistent with the alignments for a given sentence pair, and thus the total number of applicable rules can be combinatorially larger than if we just consider the set of rules that cannot be formed from other rules, namely the minimal rules. The rule types across all sentence pairs are combined to form a minimal grammar.4 To extract a set of minimal rules, we use the linear-time extraction algorithm of Zhang et al. (2008). We give a rough description of their m"
D14-1210,J08-3004,0,0.013094,"ether with values h1 , . . . , hN for every NT in the tree. An important point to keep in mind in comparison to L-PCFGs is that the right-hand side (RHS) non-terminals of synchronous rules are aligned pairs across the source and target languages. In this work, we refine the one-category grammar introduced by Chiang (2007) for HPBT in order to learn additional latent NT categories. Thus, the following discussion is restricted to these kinds of grammars, although the method is equally applicable in other scenarios, e.g., the extended treeto-string transducer (xRs) formalism (Huang et al., 2006; Graehl et al., 2008) commonly used in syntax-directed translation, and phrase-based MT (Koehn et al., 2003). Marginal Inference with L-SCFGs. For a parameter t of rule r, the latent state h1 attached to the left-hand side (LHS) NT of r is associated with the outside tree for the sub-tree rooted at the LHS, and the states attached to the RHS NTs are associated with the inside trees of that NT. Since we do not assume conditional independence of these states, we need to consider all possible interactions, which can be compactly represented as a • Arity 2 (binary) rules (R2 ): 3rd -order tensor in the case of a binar"
D14-1210,P03-1021,0,0.131506,"composed of synchronous s-trees, which can be acquired from word alignments. Normally in phrase-based translation models, we consider all possible phrase 2 In practice, the term m3 |G |can be replaced with a smaller term, which separates the rules in G by the number of NTs on the RHS. This idea relates to the notion of “effective grammar size” which we discuss in §5. 1955 pairs consistent with the word alignments and estimate features based on surface statistics associated with the phrase pairs or rules. The weights of these features are then learned using a discriminative training algorithm (Och, 2003; Chiang, 2012, inter alia). In contrast, in this work we restrict the number of possible synchronous derivations for each sentence pair to just one; thus, derivation forests do not have to be considered, making parameter estimation more tractable.3 To achieve this objective, for each sentence in the training data we extract the minimal set of synchronous rules consistent with the word alignments, as opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees con"
D14-1210,P02-1040,0,0.091384,"me described in Matsuzaki et al. (2005), but found that it provided little benefit. 1958 procedure (Berg-Kirkpatrick et al., 2010) and we leave this extension for future work. 5 Experiments The goal of the experimental section is to evaluate the performance of the latent-variable SCFG in comparison to a baseline without any additional NT annotations (M IN -G RAMMAR), and to compare the performance of the two parameter estimation algorithms. We also compare L-SCFGs to a H IERO baseline (Chiang, 2007). The language pair of evaluation is Chinese–English (ZH-EN). We score translations using BLEU (Papineni et al., 2002). The latent-variable model is integrated into the standard MT pipeline by computing marginal probabilities for each rule in the parse forest of a source sentence using the algorithm in Figure 1 with the parameters estimated through the algorithms in Figure 2, and is added as a feature for the rule during MERT (Och, 2003). These probabilities are conditioned on the LHS (X), and are thus joint probabilities for a source-target RHS pair. We also write out as features the conditional relative frequencies Pˆ (e|f ) and Pˆ (f |e) as estimated by our latent-variable model, i.e., conditioned on the s"
D14-1210,P06-1055,0,0.894834,"2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique has a long history in monolingual parsing (Petrov et al., 2006; Liang et al., 2007; Cohen et al., 2014), where it reliably yields stateof-the-art phrase structure parsers based on generative models, but we are the first to apply it to translation. We first generalize the concept of latent PCFGs to latent-variable SCFGs (§2). We then follow by a presentation of the tensor-based formulation for our parameters, a representation that makes it convenient to marginalize over latent states. Subsequently, two methods for parameter estimation are presented (§4): a spectral approach based on the method of moments, and an EM-based likelihood maximization. Results o"
D14-1210,P11-1086,0,0.0149769,"a shampoo and style . I ’d like a shampoo and style . I ’d like a shampoo and style . (d) MLE (e) Spectral m = 16 RI (f) EM m = 16 Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans µ(X, i, j) for the MLE, spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue, lower likelihoods in red. The hypotheses produced by each setup are below the heat maps. els (Mari˜no et al., 2006; Durrani et al., 2011) seek to model long-distance dependencies and reorderings through n-grams. Similarly, Vaswani et al. (2011) use a Markov model in the context of tree-to-string translation, where the parameters are smoothed with absolute discounting (Ney et al., 1994), while in our instance we capture this smoothing effect through low rank or latent states. Feng and Cohn (2013) also utilize a Markov model for MT, but learn the parameters through a more sophisticated estimation technique that makes use of Pitman-Yor hierarchical priors. Hsu et al. (2009) presented one of the initial efforts at spectral-based parameter estimation (using SVD) of observed moments for latent-variable models, in the case of Hidden Markov"
D14-1210,N09-1027,0,0.0192305,"ser. The EM-based algorithm that we propose here is similar to what they propose, except that we need to handle tensor structures. Mylonakis and Sima’an (2011) select among linguistically motivated non-terminal labels with a cross-validated version of EM. Although they consider a restricted hypothesis space, they do marginalize over different derivations therefore their inside-outside algorithm is O(n6 ). In the syntax-directed translation literature, there have been efforts to relax or coarsen the hard labels provided by a syntactic parser in an automatic manner to promote parameter sharing (Venugopal et al., 2009; Hanneman and Lavie, 2013), which is the complement of our aim in this paper. The idea of automatically learned grammar refinements comes from the monolingual parsing literature, where phenomena like head lexicalization can be modeled through latent variables. Matsuzaki et al. (2005) look at a likelihood-based method to split the NT categories of a grammar into a fixed number of sub-categories, while Petrov et al. (2006) learn a variable number of sub-categories per NT. The latter’s extension may be useful for finding the optimal number of latent states from the data in our case. The question"
D14-1210,J97-3002,0,0.377768,"problem as a context-free parsing problem. A parser constructs trees over the input sentence by parsing with the source language projection of a synchronous CFG, and each derivation induces translations in the target language (Chiang, 2007). However, in contrast to syntactic parsing, where linguistic intuitions can help elucidate the “right” tree structure for a grammatical sentence, no such intuitions are available for synchronous derivations, and so learning the “right” grammars is a central challenge. Of course, learning synchronous grammars from parallel data is a widely studied problem (Wu, 1997; Blunsom et al., 2008; Levenberg et al., 2012, inter alia). However, there has been less exploration of learning rich non-terminal categories, largely because previous efforts to learn In this work, we take a different approach to previous work in synchronous grammar induction by assuming that reasonable tree structures for a parallel corpus can be chosen heuristically, and then, fixing the trees (thereby enabling us to sidestep the worst of the computational issues), we learn non-terminal categories as latent variables to explain the distribution of these synchronous trees. This technique ha"
D14-1210,zhang-etal-2004-interpreting,0,0.0152996,"IERO baseline, but it only uses rules extracted from a minimal grammar, whose size is a fraction of the HIERO grammar. The gains seem to level off at this rank; additional ranks seem to add noise to the parameters. Feature-wise, additional lexical and length features add little, prob1960 ably because much of this information is encapsulated in the rule indicator features. For EM, m = 16 outperforms the minimal grammar baseline, but is not at the level of the spectral results. All EM, spectral, and MLE results are statistically significant (p < 0.01) with respect to the M IN G RAMMAR baseline (Zhang et al., 2004), and the improvement over the H IERO baseline achieved by the m = 16 rule indicator configuration is also statistically significant. The two estimation algorithms differ significantly in their estimation time. Given a feature covariance matrix, the spectral algorithm (SVD, which was done with Matlab, and correlation computation steps) for m = 16 took 7 minutes, while the EM algorithm took 5 minutes for each iteration with this rank. 5.4 Analysis Figure 3 presents a comparison of the nonterminal span marginals for two sentences in the development set. We visualize these differences through a h"
D14-1210,C08-1136,0,0.128119,"opposed to the composed set of rules (Galley et al., 2006). Composed rules are ones that can be formed from smaller rules in the grammar; with these rules, there are multiple synchronous trees consistent with the alignments for a given sentence pair, and thus the total number of applicable rules can be combinatorially larger than if we just consider the set of rules that cannot be formed from other rules, namely the minimal rules. The rule types across all sentence pairs are combined to form a minimal grammar.4 To extract a set of minimal rules, we use the linear-time extraction algorithm of Zhang et al. (2008). We give a rough description of their method below, and refer the reader to the original paper for additional details. The algorithm returns a complete minimal derivation tree for each word-aligned sentence pair, and generalizes an approach for finding all common intervals (pairs of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase) between two permutations (Uno and Yagiura, 2000) to sequences with many-to-many alignment links between the two sides, as in word alignment. The key idea is to encode all phrase pairs of a sentence alignmen"
D14-1210,W06-3119,0,0.0804889,"Missing"
D14-1210,2009.iwslt-evaluation.1,0,\N,Missing
D15-1178,N13-1015,1,0.81284,", we compute p(T [j] → pi |T [j]) as the probability under a unigram language model Lj which is trained on the collection of the posts from the training corpus which are dominated by Q Np T [j] nodes. p(T [j] → pi |T [j]) = k=1i Lj (wki ) i are the tokens in post pi . where w1i , w2i ...wN pi The rest of the production probabilities are learned using MLE on the training trees. In the case of LCFRS rules, the gap information is also obtained during the extraction. 6.1 6.3 6 Parsers for Conversation Trees Refining the non-terminals We use a clustering approach, akin to the spectral algorithm of Cohen et al. (2013) and Narayan and Cohen (2015),3 to create finer grained categories corresponding to GB ’s non-terminals: S, X, C and T . Each node in each tree in the training data is associated with a feature vector, which is a function of the tree and the anchor node. These vectors are clustered (for each of the non-terminals separately) and then, each node is annotated with the corresponding cluster. This process gives us the non-terminals S[i], X[j], T [k] and C[l] of GE . The features for a node nl are: depth of nl in the tree, root is at depth 0; maximum depth of the subtree under nl ; number of sibling"
D15-1178,D08-1035,0,0.143178,"identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur"
D15-1178,N09-1040,0,0.0185533,"ast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to"
D15-1178,J10-3004,0,0.669708,"nk annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein et al., 2008), and automatic summarization (Nenkova and Bagga, 2003) can be carried out on a finegrained level using topic information. Conversation disentanglement. A related problem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conversations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversations (Shen et al., 2006; Adams and Martell, 2008; Elsner and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama"
D15-1178,P03-1071,0,0.0606101,"typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly re"
D15-1178,D10-1084,0,0.0230044,"perform other tree generation baselines by a significant margin especially on short threads. 2 Related work The ideas in this paper are related to three areas of prior research. Forum thread analysis. Finding structure in forum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time (Wang et al., 2008; Cong et al., 2008). Reply links are sometimes augmented with a dialog act label indicating whether the child post is a question, answer, or confirmation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1 Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality pr"
D15-1178,W12-4615,0,0.0337959,"Missing"
D15-1178,P06-1004,0,0.0348871,"r the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and tha"
D15-1178,P05-1010,0,0.0878193,"→ T C |T , T → p and C → X |X X |X C. The C nodes can be collapsed and its daughters attached to the parent of C to revert back to the non-binary tree. While this CFG defines the structure of conversation trees, by itself this grammar is insufficient for our task. In particular, it contains a single nonterminal of each type (S, X, T , C) and so does not distinguish between topics. We extend this grammar to create GE which has a set of nonterminals corresponding to each non-terminal in GB , these fine-grained non-terminals correspond to different topics. GE is created using latent annotations (Matsuzaki et al., 2005) on the X, T , S and C non-terminals from GB . The resulting non-terminals for GE are S[i], X[j], T [k] and C[l], such that 1 ≤ i ≤ NS , 1 ≤ j ≤ NX , 1 ≤ k ≤ NT , 1 ≤ l ≤ NC . i, j, k and l identify specific topics attached to a particular node type. Our output trees are created with GE to depict the topic segmentation of the thread and are nonbinary. The binary trees produced by our algorithms are converted by collapsing the C. As a result, conversation trees have S[i], X[j] and T [k] 2 Any context-free grammar can be converted to an equivalent CNF grammar. Our algorithms support unary rules."
D15-1178,D15-1214,1,0.77639,"i |T [j]) as the probability under a unigram language model Lj which is trained on the collection of the posts from the training corpus which are dominated by Q Np T [j] nodes. p(T [j] → pi |T [j]) = k=1i Lj (wki ) i are the tokens in post pi . where w1i , w2i ...wN pi The rest of the production probabilities are learned using MLE on the training trees. In the case of LCFRS rules, the gap information is also obtained during the extraction. 6.1 6.3 6 Parsers for Conversation Trees Refining the non-terminals We use a clustering approach, akin to the spectral algorithm of Cohen et al. (2013) and Narayan and Cohen (2015),3 to create finer grained categories corresponding to GB ’s non-terminals: S, X, C and T . Each node in each tree in the training data is associated with a feature vector, which is a function of the tree and the anchor node. These vectors are clustered (for each of the non-terminals separately) and then, each node is annotated with the corresponding cluster. This process gives us the non-terminals S[i], X[j], T [k] and C[l] of GE . The features for a node nl are: depth of nl in the tree, root is at depth 0; maximum depth of the subtree under nl ; number of siblings of nl ; number of children"
D15-1178,J03-1006,0,0.0288771,"r to yield a single span of the LHS non-terminal. A Probabilistic LCFRS (PLCFRS) (Levy, 2005) also contains D, a function which assigns ~ x)) to conditional probabilities p(A(~x) → φ|A(~ the rules. The probabilities conditioned on a particular non-terminal and span configuration, A(~x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules. 1545 To find the most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is of the form [A, ρ ~] where A is a non-terminal and ρ~ is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log |of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth’s generalization (Knuth, 1977) of the shortest paths algorithm, the most likely tree can be found without exhaustive parsing as in Viterbi parsing of CFGs. The complexity of parsing is O(n3k ) where k is the fa"
D15-1178,J86-3001,0,0.597124,"level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree, a requirement posed by many prior seg1544 mentation algorithms. The second limitation of prior studies is assuming that topics do not recur in the same document. But linguistic theories allow for non-adjacent utterances to belong to the same topic segment (Grosz and Sidner, 1986) and this fact is empirically true in chat and forum conversations (Elsner and Charniak, 2010; Wang et al., 2011). Our mo"
D15-1178,P94-1002,0,0.546745,"r and Charniak, 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics"
D15-1178,P10-2028,0,0.0164992,"s perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular consensus order, and that the same topic does not recur in the same document. Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure (Grosz and Sidner, 1986) and our grammar model is an ideal framework for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversation tree,"
D15-1178,P01-1064,0,0.15201,", 2010). Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly focuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear segmentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments (Hearst, 1994; Utiyama and Isahara, 2001; Galley et al., 2003; Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008). Very few methods recursively combine smaller segments into larger ones. Such hierarchical models (Eisenstein, 2009) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear segmentation for documents within the same domain and having a regular structure (Chen et al., 2009; Jeong and Titov, 2010; Du et al., 2015). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are dis"
D15-1178,P87-1015,0,0.840849,"Missing"
D15-1178,J13-1006,0,0.0515534,"productions of a CFG take the single spans of each non-terminal on the RHS and concatenate them in the same order to yield a single span of the LHS non-terminal. A Probabilistic LCFRS (PLCFRS) (Levy, 2005) also contains D, a function which assigns ~ x)) to conditional probabilities p(A(~x) → φ|A(~ the rules. The probabilities conditioned on a particular non-terminal and span configuration, A(~x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules. 1545 To find the most likely parse tree, we use the parsing algorithm proposed by Kallmeyer and Maier (2013) for binary PLCFRS. The approach uses weighted deduction rules (Shieber et al., 1995; Nederhof, 2003), which specify how to compute a new item from other existing items. Each item is of the form [A, ρ ~] where A is a non-terminal and ρ~ is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log |of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth’s generalization (Knuth, 1977) of the shortest paths algorithm, the most likely tree can be found without exh"
D15-1178,D11-1002,0,0.458409,"generation baselines by a significant margin especially on short threads. 2 Related work The ideas in this paper are related to three areas of prior research. Forum thread analysis. Finding structure in forum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time (Wang et al., 2008; Cong et al., 2008). Reply links are sometimes augmented with a dialog act label indicating whether the child post is a question, answer, or confirmation to the parent post (Kim et al., 2010; Wang et al., 2011). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags (Joty et al., 2013). We focus on producing rich hierarchical segmentation going beyond clusters which do not contain any cluster-internal structure. We also be1 Our corpus is available from http:// kinloch.inf.ed.ac.uk/public/CTREES/ ConversationTrees.html lieve that topic structure is complementary to dialog act and reply link annotations. Tasks on forum data such as user expertise (Lui and Baldwin, 2009) and post quality prediction (Agichtein"
D15-1178,C10-1061,0,\N,Missing
D15-1178,U10-1009,0,\N,Missing
D15-1214,H91-1060,0,0.233311,"Missing"
D15-1214,W08-2102,0,0.0721588,"Missing"
D15-1214,P05-1022,0,0.702038,"PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all"
D15-1214,D15-1160,0,0.0117897,"German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diver"
D15-1214,P14-1099,1,0.867448,"e foot of the outside tree to the root of the tree which is different from the head node of the foot node. • The width of the spans to the left and to the right of the foot node, paired with the label of the foot node. Other Spectral Algorithms The SVD step on the Ωa matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010). Cohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums. In our preliminary experiments, we found out that the clusterin"
D15-1214,P12-1024,1,0.948709,"arsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement. This algorithm has value for readers who are interested in learning more about spectral algorithms – it demonstrates some of the core ideas in spectral learning in a rather intuitive way. In addition, this algorithm leads to sparse grammar estimates and compact models. Second, we describe how a diverse set of predictors can be used with spectral learning techniques. 1868 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1868–1878, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational L"
D15-1214,N13-1015,1,0.916617,"in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petr"
D15-1214,J03-4003,0,0.671112,"Missing"
D15-1214,P05-1039,0,0.0765088,"Missing"
D15-1214,N09-2064,0,0.141107,"experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More"
D15-1214,P96-1024,0,0.477791,",” and rely on the marginals that each model gives. Decoding with Multiple Models Let G1 , . . . , Gp be a set of L-PCFG grammars. In §6, we create such models using the noising techniques described above. The question that remains is how to combine these models together to get a single best output parse tree given an input sentence. With L-PCFGs, decoding a single sentence requires marginalizing out the latent states to find the best skeletal tree2 for a given string. Let s be a sentence. We define t(Gi , s) to be the output tree according to minimum Bayes risk decoding. This means we follow Goodman (1996), who uses dynamic programming to compute the tree that maximizes the sum of all marginals of all nonterminals in the output tree. Each marginal, for each span ha, i, ji (where a is a nonterminal and i and j are endpoints in the sentence), is computed by using the inside-outside algorithm. p X X t∗ = arg max MaxEnt reranking: We train a MaxEnt reranker on a training set that includes outputs from multiple models, and then, during testing time, decode with each of the models, and use the trained reranker to select one of the parses. We use the reranker of Charniak and Johnson (2005).3 As we see"
D15-1214,W99-0623,0,0.464086,"ree ways to decode with multiple models. In addition, we describe a simple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information fro"
D15-1214,D10-1004,0,0.10846,"Missing"
D15-1214,P05-1010,0,0.573618,"techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used toget"
D15-1214,N15-1076,0,0.0159453,"ormation from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single m"
D15-1214,N07-1051,0,0.0308175,"2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for estimating L-PCFGs, akin to the spectral algorithm of Cohen et al. (2012), but simpler to understand and implement. This algorithm has value for readers who are interested in learning more a"
D15-1214,P06-1055,0,0.293158,"nts have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate multiple models, where the underlying features are perturbed with several perturbation schemes. Each such model, during test time, yields a single parse, and all parses are then used together in several ways to select a single best parse. The main contributions of this paper are twofold. First, we present an algorithm for"
D15-1214,N10-1003,0,0.763531,"r English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniq"
D15-1214,N15-1058,0,0.0639382,"Missing"
D15-1214,N06-2033,0,0.0654012,"to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made fo"
D15-1214,P12-1046,0,0.0877184,"Missing"
D15-1214,D15-1178,1,0.77639,"bel of the foot node. Other Spectral Algorithms The SVD step on the Ωa matrix is pivotal to many algorithms, and has been used in the past for other L-PCFG estimation algorithms. Cohen et al. (2012) used it for developing a spectral algorithm that identifies the parameters of the L-PCFG up to a linear transformation. Their algorithm generalizes the work of Hsu et al. (2009) and Bailly et al. (2010). Cohen and Collins (2014) also developed an algorithm that makes use of an SVD step on the inside-outside. It relies on the idea of “pivot features” – features that uniquely identify latent states. Louis and Cohen (2015) used a clustering algorithm that resembles ours but does not separate inside trees from outside trees or follows up with a singular value decomposition step. Their algorithm was applied to both L-PCFGs and linear context-free rewriting systems. Their application was the analysis of hierarchical structure of conversations in online forums. In our preliminary experiments, we found out that the clustering algorithm by itself performs worse than the spectral algorithm of Cohen et al. (2013). We believe that the reason is two-fold: (a) k-means finds a local maximum during clustering; (b) we do har"
D15-1214,A97-1014,0,0.175074,"Missing"
D15-1214,N15-1028,0,0.0242867,"utions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berkeley parser from Petrov and Klein (2007). Instead of exploiting diversity from a k-best list from a single model, we estimate"
D15-1214,E12-1042,0,0.0554628,"anking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we describe techniques to exploit diversity with spectral learning algorithms for natural language parsing. Spectral techniques and the method of moments have been recently used for various problems in natural language processing, including parsing, topic modeling and the derivation of word embeddings (Luque et al., 2012; Cohen et al., 2013; Stratos et al., 2014; Dhillon et al., 2015; Rastogi et al., 2015; Nguyen et al., 2015; Lu et al., 2015). Cohen et al. (2013) showed how to estimate an L-PCFG using spectral techniques, and showed that such estimation outperforms the expectationmaximization algorithm (Matsuzaki et al., 2005). Their result still lags behind state of the art in natural language parsing, with methods such as coarseto-fine (Petrov et al., 2006). We further advance the accuracy of natural language parsing with spectral techniques and LPCFGs, yielding a result that outperforms the original Berke"
D15-1214,D07-1105,0,0.0158895,"ple variant of the spectral algorithm for L-PCFGs that is fast and leads to compact models. Our experiments for natural language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use"
D15-1214,J93-2004,0,0.050059,"Missing"
D15-1214,J01-2002,0,0.0406511,"Missing"
D15-1214,D13-1117,0,0.105259,"spectral learning in a rather intuitive way. In addition, this algorithm leads to sparse grammar estimates and compact models. Second, we describe how a diverse set of predictors can be used with spectral learning techniques. 1868 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1868–1878, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Our approach relies on adding noise to the feature functions that help the spectral algorithm compute the latent states. Our noise schemes are similar to those described by Wang et al. (2013). We add noise to the whole training data, then train a model using our algorithm (or other spectral algorithms; Cohen et al., 2013), and repeat this process multiple times. We then use the set of parses we get from all models in a recombination step. The rest of the paper is organized as follows. In §2 we describe notation and background about L-PCFG parsing. In §3 we describe our new spectral algorithm for estimating L-PCFGs. It is based on similar intuitions as older spectral algorithms for L-PCFGs. In §4 we describe the various noise schemes we use with our spectral algorithm and the spect"
D15-1214,D09-1161,0,0.0772842,"language parsing, for English and German, show that we get a significant improvement over baselines comparable to state of the art. For English, we achieve the F1 score of 90.18, and for German we achieve the F1 score of 83.38. 1 Introduction It has been long identified in NLP that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems (Henderson and Brill, 1999). Such problems include machine translation (Macherey and Och, 2007), syntactic parsing (Charniak and Johnson, 2005; Sagae and Lavie, 2006; Fossum and Knight, 2009; Zhang et al., 2009; Petrov, 2010; Choe et al., 2015) and others (Van Halteren et al., 2001). The main argument behind the use of such a diverse set of solutions (such as k-best list of parses for a natural language sentence) is the hope that each solution in the set is mostly correct. Therefore, recombination or reranking of solutions in that set will further optimize the choice of a solution, combining together the information from all solutions. In this paper, we explore another angle for the use of a set of parse tree predictions, where all predictions are made for the same sentence. More specifically, we de"
D16-1028,D13-1059,0,0.0334481,"Missing"
D16-1028,N10-1083,0,0.0606133,"Missing"
D16-1028,J92-4003,0,0.620219,"Missing"
D16-1028,P14-1099,1,0.91772,"rvised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset o"
D16-1028,N13-1015,1,0.928849,"w), we can estimate the emission probabilities O by direct application of Bayes rule: Therefore, given the set of anchor words A(h), the bw,h = p(H = h |X = w) × p(X = w) (15) O hth column of R can be estimated in a single pass p(H = h) over the unlabeled data, as follows: Eq. 7 z}|{ P γ bw,c × pbw ψ (z)1(x ∈ A(h)) c =P . (16) bc,h = x,z∈D P U R (12) bw0 ,c × pbw0 w0 γ 1(x ∈ A(h)) x,z∈DU 290 These parameters are guaranteed to lie in the probability simplex, avoiding the need of heuristics for dealing with “negative” and “unnormalized” probabilities required by prior work in spectral learning (Cohen et al., 2013). 3.5 Transition Distributions It remains to estimate the transition matrix T. For the problems tackled in this paper, the number of labels K is small, compared to the vocabulary size V . The transition matrix has only O(K 2 ) degrees of freedom, and we found it effective to estimate it using the labeled sequences in DL alone, without any refinement. This was done by smoothed maximum likelihood estimation on the labeled data, which boils down to counting occurrences of consecutive labels, applying add-one smoothing to avoid zero probabilities for unobserved transitions, and normalizing. For pr"
D16-1028,P13-1057,0,0.224936,"rameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised le"
D16-1028,P11-2008,1,0.79776,"Missing"
D16-1028,Q15-1016,0,0.0308812,"s the matrix Q ∈ RC×V , defined as: Qc,w := E[ψc (Z) |X = w]. (3) Expectations here are taken with respect to the probabilistic model in Eq. 1 that generates the data. The following quantities will also be necessary: Figure 1: HMM, context (green) conditionally independent of present (red) w` given state h` . and will be formally defined in §3.1. Such cooccurrence matrices are often collected in NLP, for various problems, ranging from dimensionality reduction of documents using latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998), distributional semantics (Sch¨utze, 1998; Levy et al., 2015) and word embedding generation (Dhillon et al., 2015; Osborne et al., 2016). We can build such a moment matrix entirely from the unlabeled data DU . The same unlabeled data is used to build an estimate of a context-label moment matrix R ∈ RC×K , as explained in §3.3. This is done by first identifying words that are unambiguously associated with each label h, called anchor words, with the aid of a few labeled data; this is outlined in §3.2. Finally, given empirical estimates of Q and R, we estimate the emission matrix O by solving a small optimization problem independently per word (§3.4). The"
D16-1028,P12-3005,0,0.0268494,"ter experiments, we also evaluated a stacked architecture in which we derived features from our model’s predictions to improve a state-ofthe-art POS tagger (MEMM).4 6.1 Twitter POS Tagging For the Twitter experiment, we used the Oct27 dataset of Gimpel et al. (2011), with the provided partitions (1,000 tweets for training and 328 for validation), and tested on the Daily547 dataset (547 tweets). Anchor words were selected from the training partition as described in §5. We used 2.7M unlabeled tweets (O’Connor et al., 2010) to train the semi-supervised methods, filtering the English tweets as in Lui and Baldwin (2012), tokenizing them as in Owoputi et al. (2013), and normalizing at-mentions, URLs, and emoticons. We used as word features φ(X) the word iself, as well as binary features for capitalization, titles, and digits (Berg-Kirkpatrick et al., 2010), the word shape, and the Unicode class of each character. Similarly to Owoputi et al. (2013), we also used suffixes and prefixes (up to length 3), and Twitter4 http://www.ark.cs.cmu.edu/TweetNLP/ 293 0.95 Tagging accuracy (0/1 loss) select the anchors on the validation set, using steps of 0.1 in the unit interval, and making sure that all tags have at least"
D16-1028,J94-2001,0,0.254846,"collect moment statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and"
D16-1028,N15-1076,0,0.0174388,"ultiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, the estimation of the model parameters can be made efficient b"
D16-1028,Q16-1030,1,0.819216,"ations here are taken with respect to the probabilistic model in Eq. 1 that generates the data. The following quantities will also be necessary: Figure 1: HMM, context (green) conditionally independent of present (red) w` given state h` . and will be formally defined in §3.1. Such cooccurrence matrices are often collected in NLP, for various problems, ranging from dimensionality reduction of documents using latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998), distributional semantics (Sch¨utze, 1998; Levy et al., 2015) and word embedding generation (Dhillon et al., 2015; Osborne et al., 2016). We can build such a moment matrix entirely from the unlabeled data DU . The same unlabeled data is used to build an estimate of a context-label moment matrix R ∈ RC×K , as explained in §3.3. This is done by first identifying words that are unambiguously associated with each label h, called anchor words, with the aid of a few labeled data; this is outlined in §3.2. Finally, given empirical estimates of Q and R, we estimate the emission matrix O by solving a small optimization problem independently per word (§3.4). The transition matrix T is obtained directly from the labeled dataset DL by max"
D16-1028,N13-1039,1,0.813851,"Missing"
D16-1028,petrov-etal-2012-universal,0,0.0129586,"mpute a mapping from mean parameters µh 3 As shown by Xiaojin Zhu (1999) and Yasemin Altun to canonical parameters θ h , we use the well-known (2006), this regularization is equivalent, in the dual, to a “soft” Fenchel-Legendre duality between the entropy and constraint kEθh [φ(X) |H = h] − µh k2 ≤ , as opposed to a the log-partition function (Wainwright and Jordan, strict equality. 292 6 Experiments We evaluated our method on two tasks: POS tagging of Twitter text (in English), and POS tagging for a low-resource language (Malagasy). For all the experiments, we used the universal POS tagset (Petrov et al., 2012), which consists of K = 12 tags. We compared our method against supervised baselines (HMM and FHMM), which use the labeled data only, and two semi-supervised baselines that exploit the unlabeled data: self-training and EM. For the Twitter experiments, we also evaluated a stacked architecture in which we derived features from our model’s predictions to improve a state-ofthe-art POS tagger (MEMM).4 6.1 Twitter POS Tagging For the Twitter experiment, we used the Oct27 dataset of Gimpel et al. (2011), with the provided partitions (1,000 tweets for training and 328 for validation), and tested on th"
D16-1028,J98-1004,0,0.524216,"Missing"
D16-1028,P05-1044,1,0.844823,"statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et a"
D16-1028,W13-3507,1,0.860689,"oblems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, t"
D16-1028,Q16-1018,0,0.0111305,"), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, the estimation of the model parameters can be made efficient by collecting moment statistics from unlabeled data, then solving a small quadratic program for each word. Our contributions are as follows: • We adapt anchor methods to semi-supervised learning of generative sequence models. • We show how our method can also handle loglinear feature-based emissions. • We apply this model to POS tagging. Our experiments on the Twitter dataset introduced by Gimpel et al. (2011) and on the dataset introduced by"
D17-1064,C10-1037,0,0.0810355,"Missing"
D17-1064,D15-1042,0,0.0886682,"Missing"
D17-1064,P05-1074,0,0.0497637,"B, UK ‡ CNRS, LORIA, UMR 7503, Vandoeuvre-l`es-Nancy, F-54500, France shashi.narayan@ed.ac.uk claire.gardent@loria.fr scohen@inf.ed.ac.uk anastasia.shimorina@loria.fr Abstract Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sent"
D17-1064,W08-1105,0,0.143307,"Missing"
D17-1064,P01-1008,0,0.114122,"ton Street, Edinburgh EH8 9AB, UK ‡ CNRS, LORIA, UMR 7503, Vandoeuvre-l`es-Nancy, F-54500, France shashi.narayan@ed.ac.uk claire.gardent@loria.fr scohen@inf.ed.ac.uk anastasia.shimorina@loria.fr Abstract Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operatio"
D17-1064,D11-1108,0,0.0392016,"Missing"
D17-1064,P16-2055,0,0.0228674,"Missing"
D17-1064,E99-1042,0,0.232209,"(Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression and simplification) mentioned above, only sentence simplification involves sentence splitting. Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014) from the parallel dataset"
D17-1064,P17-1017,1,0.850809,"Missing"
D17-1064,C96-2183,0,0.729533,"kes the input complex sentence into account when generating while the other does not. Table 1: Similarities and differences between sentence rewriting tasks with respect to splitting (Split), deletion (Delete), rephrasing (Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression and simplification) mentioned above, only sentence simplification involves sent"
D17-1064,W03-1602,0,0.291741,"itting (Split), deletion (Delete), rephrasing (Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression and simplification) mentioned above, only sentence simplification involves sentence splitting. Most simplification methods learn a statistical model (Zhu et al., 2010; Coster and Kauchak, 2011; Woodsend and Lapata, 2011; Wubben et al., 2012; Naray"
D17-1064,C08-1018,0,0.0473474,"e and make available a benchmark consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning.1 Second, we propose five models (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task. 1 Introduction Several sentence rewriting operations have been extensively discussed in the literature: sentence compression, multi-sentence fusion, sentence paraphrasing and sentence simplification. Sentence compression rewrites an input sentence into a shorter paraphrase (Knight and Marcu, 2000; Cohn and Lapata, 2008; Filippova and 1 The Split-and-Rephrase dataset is available here: https://github.com/shashiongithub/ Split-and-Rephrase. 606 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 606–616 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Compression Fusion Paraphrasing Simplification Split-and-Rephrase Split N N N Y Y Delete Y Y N Y N Rephr. ?Y Y Y Y Y MPre. N ?Y Y N Y ing. This allows for the learning of semanticallyinformed models (cf. Section 5). Our second contribution is to provide five models to understand"
D17-1064,jelinek-2014-improvements,0,0.112961,"Missing"
D17-1064,W11-1601,0,0.0946978,"Missing"
D17-1064,W04-1016,0,0.0257937,"Missing"
D17-1064,D15-1166,0,0.0302707,"Missing"
D17-1064,E17-1083,0,0.0153724,"-54500, France shashi.narayan@ed.ac.uk claire.gardent@loria.fr scohen@inf.ed.ac.uk anastasia.shimorina@loria.fr Abstract Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new"
D17-1064,N06-2009,0,0.107081,"Missing"
D17-1064,P14-5010,0,0.00544315,"Missing"
D17-1064,J11-1007,0,0.0187072,"les) of the complex sentence into smaller units and then generate a text for each RDF subset in that partition. One model is multi-source and takes the input complex sentence into account when generating while the other does not. Table 1: Similarities and differences between sentence rewriting tasks with respect to splitting (Split), deletion (Delete), rephrasing (Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of th"
D17-1064,I13-1198,0,0.0149176,"i Narayan† Claire Gardent‡ Shay B. Cohen† Anastasia Shimorina‡ School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK ‡ CNRS, LORIA, UMR 7503, Vandoeuvre-l`es-Nancy, F-54500, France shashi.narayan@ed.ac.uk claire.gardent@loria.fr scohen@inf.ed.ac.uk anastasia.shimorina@loria.fr Abstract Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all f"
D17-1064,N10-1044,0,0.0303032,"Missing"
D17-1064,P14-1041,1,0.893088,"consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting"
D17-1064,W16-6620,1,0.862186,"with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting and rephrasing. There is no deletion and no"
D17-1064,D16-1033,0,0.0899926,"Missing"
D17-1064,W16-6625,1,0.901333,"Missing"
D17-1064,P08-1040,0,0.140872,"text for each RDF subset in that partition. One model is multi-source and takes the input complex sentence into account when generating while the other does not. Table 1: Similarities and differences between sentence rewriting tasks with respect to splitting (Split), deletion (Delete), rephrasing (Rephr.) and meaning preserving (MPre.) operations (Y: yes, N: No, ?Y: should do but most existing approaches do not). ing step which facilitates and improves the performance of parsers (Tomita, 1985; Chandrasekar and Srinivas, 1997; McDonald and Nivre, 2011; Jel´ınek, 2014), semantic role labelers (Vickrey and Koller, 2008) and statistical machine translation (SMT) systems (Chandrasekar et al., 1996). In addition, because it allows the conversion of longer sentences into shorter ones, it should also be of use for people with reading disabilities (Inui et al., 2003) such as aphasia patients (Carroll et al., 1999), low-literacy readers (Watanabe et al., 2009), language learners (Siddharthan, 2002) and children (De Belder and Moens, 2010). 2 Related Work We briefly review previous work on sentence splitting and rephrasing. Sentence Splitting. Of the four sentence rewriting tasks (paraphrasing, fusion, compression a"
D17-1064,P02-1040,0,0.0989634,"Missing"
D17-1064,W04-3219,0,0.196979,"Missing"
D17-1064,D11-1038,0,0.13372,", 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In"
D17-1064,P02-1006,0,0.0227466,"Missing"
D17-1064,P12-1107,0,0.137343,"Missing"
D17-1064,D15-1044,0,0.0384958,"Missing"
D17-1064,W10-4223,0,0.0690832,"Missing"
D17-1064,P15-1152,0,0.0512972,"Missing"
D17-1064,Q15-1021,0,0.06186,"or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting and rephrasing."
D17-1064,Q16-1029,0,0.184709,"Missing"
D17-1064,W10-4213,0,0.073206,"Missing"
D17-1064,W11-2802,0,0.0930787,"Missing"
D17-1064,D17-1062,0,0.0548473,"on content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences while preserving meaning. In that task, the emphasis is on sentence splitting and rephrasing. There is no deletion and no lexical or phrasal simpl"
D17-1064,E14-1076,0,0.254892,"Missing"
D17-1064,P08-1089,0,0.0467264,"Missing"
D17-1064,C04-1129,0,0.059261,"Strube, 2008; Pitler, 2010; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into sh"
D17-1064,C10-1152,0,0.335065,"; Filippova et al., 2015; Toutanova et al., 2016). Sentence fusion consists of combining two or more sentences with overlapping information content, preserving common information and deleting irrelevant details (McKeown et al., 2010; Filippova, 2010; Thadani and McKeown, 2013). Sentence paraphrasing aims to rewrite a sentence while preserving its meaning (Dras, 1999; Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Wubben et al., 2010; Mallinson et al., 2017). Finally, sentence (or text) simplification aims to produce a text that is easier to understand (Siddharthan et al., 2004; Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2015; Narayan and Gardent, 2016; Zhang and Lapata, 2017). Because the vocabulary used, the length of the sentences and the syntactic structures occurring in a text are all factors known to affect readability, simplification systems mostly focus on modelling three main text rewriting operations: simplifying paraphrasing, sentence splitting and deletion. We propose a new sentence simplification task, which we dub Split-and-Rephrase, where the goal is to split a complex input sentence into shorter sentences wh"
D17-1064,N16-1004,0,0.034648,"Missing"
D17-1064,P07-2009,0,\N,Missing
D18-1001,P15-1073,0,0.036556,"Missing"
D18-1001,P16-2096,0,0.0202405,"privacy (Dwork, 2006) provides privacy guarantees for the problem of releasing information without compromising confidential data, and usually involves adding noise in the released information. It has been applied to the training of deep learning models (Abadi et al., 2016; Papernot et al., 2016; Papernot et al., 2018), and Bayesian topic models (Schein et al., 2018). The notion of privacy is particularly crucial to NLP, since it deals with textual data, oftentimes user-generated data, that contain a lot of private information. For example, textual data contain a lot of signal about authors (Hovy and Spruit, 2016). and can be leveraged to predict demographic variables (Rosenthal and McKeown, 2011; Preot¸iucPietro et al., 2015). Oftentimes, this information is not explicit in the text but latent and related to the usage of various linguistic traits. Our work is based on a stronger hypothesis: this latent information is still present in vectorial representations of texts, even if the representations have not been supervised by these latent variables. Li et al. (2017) study the privacy of unsupervised representations of images, and measures their privacy with the peak signal to noise ratio between an orig"
D18-1001,P11-1077,0,0.180648,"type of attack on neural representations: an attacker eavesdrops on the hidden representations of novel input examples (that are not in the training set) and tries to recover information about the content of the input text (Figure 1). A typical scenario where such attacks would occur is when the computation of a deep neural net Private information can take the form of key phrases explicitly contained in the text. However, it can also be implicit. For example, demographic information about the author of a text can be predicted with above chance accuracy from linguistic cues in the text itself (Rosenthal and McKeown, 2011; Preot¸iuc-Pietro et al., 2015). Independently of its explicitness, some of this private information correlates with the output labels, and therefore will be learned by the network. In such a case, there is a tradeoff between the utility of the representation (measured by the accuracy of the network) and its privacy. It might be 1 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1–10 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics latent representation of x used by the main classifier. We illustrat"
D18-1001,P18-2005,0,0.0566168,", since it is also 8 References If an attacker has access to e.g. a trained language model, they are likely to be able to generate sentences from the training set, since the language model is trained to assign high probabilities to those sentences. Such memorization is problematic when the training data contains private information and personal data. The experimental setting we explore is different from these works: we assume that the attacker has access to a hidden layer of the network and tries to recover information about an input example that is not in the training set. In a recent study, Li et al. (2018) proposed a method based on GAN designed to improve the robustness and privacy of neural representations, applied to part-of-speech tagging and sentiment analysis. They use a training scheme with two agents similar to our multidetasking strategy (Section 3.1.1), and found that it made neural representations more robust and accurate. However, they only use a single adversary to alter the training of the main model and to evaluate the privacy of the representations, with the risk of overestimating privacy. In contrast, once the parameters of our main model are fixed, we train a new classifier fr"
D18-1001,D16-1058,0,0.0389437,"ase 2. Generation of a dataset of pairs (r(x), z) for the attacker, r is the representation function of the main classifier (r is defined in Section 2.1); Phase 3. Training of the attacker’s network and evaluation of its performance for measuring privacy. In the remainder of this section, we describe the main classifier (Section 2.1), and the attacker’s model (Section 2.2). 2.1 As our base model, we chose a standard LSTM architecture (Hochreiter and Schmidhuber, 1997) for sequence classification. LSTM-based architectures have been applied to many NLP tasks, including sentiment classification (Wang et al., 2016) and text classification (Zhou et al., 2016). First, an LSTM encoder computes a fixed-size representation r(x) from a sequence of tokens x = (x1 , x2 , . . . , xn ) projected to an embedding space. We use θ r to denote the parameters used to construct r. They include the parameters of the LSTM, as well as the word embeddings. Then, the encoder output r(x) is fed as input to a feedforward network with parameters θ p that predicts the label y of the text, with a softmax output activation. In the standard setting, the model is trained to minimize the negative log-likelihood of y labels: • We prop"
D18-1001,C16-1329,0,0.021223,"), z) for the attacker, r is the representation function of the main classifier (r is defined in Section 2.1); Phase 3. Training of the attacker’s network and evaluation of its performance for measuring privacy. In the remainder of this section, we describe the main classifier (Section 2.1), and the attacker’s model (Section 2.2). 2.1 As our base model, we chose a standard LSTM architecture (Hochreiter and Schmidhuber, 1997) for sequence classification. LSTM-based architectures have been applied to many NLP tasks, including sentiment classification (Wang et al., 2016) and text classification (Zhou et al., 2016). First, an LSTM encoder computes a fixed-size representation r(x) from a sequence of tokens x = (x1 , x2 , . . . , xn ) projected to an embedding space. We use θ r to denote the parameters used to construct r. They include the parameters of the LSTM, as well as the word embeddings. Then, the encoder output r(x) is fed as input to a feedforward network with parameters θ p that predicts the label y of the text, with a softmax output activation. In the standard setting, the model is trained to minimize the negative log-likelihood of y labels: • We propose a metric to measure the privacy of the n"
D18-1001,I17-1102,0,0.0259719,"Missing"
D18-1001,P15-1169,0,0.120266,"Missing"
D18-1206,N18-1150,0,0.418261,"osing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor e"
D18-1206,P16-1046,1,0.86461,"tem and state-of-the-art abstractive approaches when evaluated automatically and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing to"
D18-1206,P17-1012,0,0.256314,"h Broadcasting Corporation (BBC) that often include a firstsentence summary. We further propose a novel deep learning model which we argue is well-suited to the extreme summarization task. Unlike most existing abstractive approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018) which rely on an encoder-decoder architecture modeled by recurrent neural networks (RNNs), we present a topic-conditioned neural model which is based entirely on convolutional neural networks (Gehring et al., 2017b). Convolution layers capture longrange dependencies between words in the document more effectively compared to RNNs, allowing to perform document-level inference, abstraction, and paraphrasing. Our convolutional encoder associates each word with a topic vector capturing whether it is representative of the document’s content, while our convolutional decoder conditions each word prediction on a document topic vector. Experimental results show that when evaluated automatically (in terms of ROUGE) our topicaware convolutional model outperforms an oracle extractive system and state-of-the-art RNN"
D18-1206,N18-1065,0,0.359024,", 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which create a summary by identifying (and subsequently concatenating) the most important sentences in a document (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018b). Abstractive approaches, despite being more faithful to the actual summarization task, either lag behind extractive ones or are mostly extractive, exhibiting a small degree of abstraction (See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018). In this paper we introduce extreme summariza1797 Proceedi"
D18-1206,J10-3005,1,0.862457,"Missing"
D18-1206,P16-1188,0,0.0555743,"abstractive approaches when evaluated automatically and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic anno"
D18-1206,W18-2706,0,0.0695067,"Missing"
D18-1206,P18-1082,0,0.0248868,"extremely 1799 [P A D . w on [P A D ] ] En gl an d conditioning each word prediction on the document topic vector. t0i ⊗ tD xi + p i e Convolutions f GLU f ⊗ f ⊗ ⊗ zu Attention ⊕ w w  P P P c` hL h` ⊗ f GLU ⊗ f ⊕ ⊗ f Convolutions . po rt re t ch po r re ] at M D [P A D ] [P A D ] [P A M at ch x0i + p0i tD g Figure 2: Topic-conditioned convolutional model for extreme summarization. high, and pertinent content can be easily missed. Recently, a convolutional alternative to sequence modeling has been proposed showing promise for machine translation (Gehring et al., 2017a,b) and story generation (Fan et al., 2018). We believe that convolutional architectures are attractive for our summarization task for at least two reasons. Firstly, contrary to recurrent networks which view the input as a chain structure, convolutional networks can be stacked to represent large context sizes. Secondly, hierarchical features can be extracted over larger and larger contents, allowing to represent long-range dependencies efficiently through shorter paths. Our model builds on the work of Gehring et al. (2017b) who develop an encoder-decoder architecture for machine translation with an attention mechanism (Sukhbaatar et al"
D18-1206,N03-1020,0,0.627338,"Missing"
D18-1206,P18-1188,1,0.923319,"elating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which"
D18-1206,N18-1158,1,0.9104,"elating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which"
D18-1206,D15-1044,0,0.353547,"ery different from a headline whose aim is to encourage readers to read the story; it draws on information interspersed in various parts of the document (not only the beginning) and displays multiple levels of abstraction including paraphrasing, fusion, synthesis, and inference. We build a dataset for the proposed task by harvesting online articles from the British Broadcasting Corporation (BBC) that often include a firstsentence summary. We further propose a novel deep learning model which we argue is well-suited to the extreme summarization task. Unlike most existing abstractive approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018) which rely on an encoder-decoder architecture modeled by recurrent neural networks (RNNs), we present a topic-conditioned neural model which is based entirely on convolutional neural networks (Gehring et al., 2017b). Convolution layers capture longrange dependencies between words in the document more effectively compared to RNNs, allowing to perform document-level inference, abstraction, and paraphrasing. Our convolutional encoder associate"
D18-1206,E17-2007,0,0.157128,"Missing"
D18-1206,P17-1099,0,0.185195,"and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-sca"
D18-1206,D16-1248,0,0.0229063,"Let D denote a document consisting of a sequence of words (w1 , . . . , wm ); we embed D into a distributional space x = (x1 , . . . , xm ) where xi ∈ Rf is a column in embedding matrix M ∈ RV ×f (where V is the vocabulary size). We also embed the absolute word positions in the document p = (p1 , . . . , pm ) where pi ∈ Rf is a column in position matrix 1800 P ∈ RN ×f , and N is the maximum number of positions. Position embeddings have proved useful for convolutional sequence modeling (Gehring et al., 2017b), because, in contrast to RNNs, they do not observe the temporal positions of words 0 (Shi et al., 2016). Let tD ∈ Rf be the topic distribution of document D and t0 = (t01 , . . . , t0m ) the topic distributions of words in the document 0 (where t0i ∈ Rf ). During encoding, we represent document D via e = (e1 , . . . , em ), where ei is: 0 ei = [(xi + pi ); (t0i ⊗ tD )] ∈ Rf +f , and ⊗ denotes point-wise multiplication. The topic distribution t0i of word wi essentially captures how topical the word is in itself (local context), whereas the topic distribution tD represents the overall theme of the document (global context). The encoder essentially enriches the context of the word with its topical"
D18-1206,P17-1108,0,0.49684,"Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summari"
D18-1206,N18-2102,0,0.377679,"Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, th"
D18-1483,P18-4017,1,0.8749,"Missing"
D18-1483,P13-2109,0,0.0772629,"Missing"
D18-1483,P16-4025,0,0.0624954,"Missing"
D19-1099,Q17-1010,0,0.00637774,"y loss. 6 Experiments Datasets We conduct experiments on CoNLL2009 (Hajiˇc et al., 2009) data set for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). We use the predicted part-of-speech tags, dependency labels, and pre-identified predicate, provided with the dataset. The statistics of datasets are shown in Table 2. 4 A more canonical way of controlling stochasticity is to use the temperature but we prefer not to scale the gradient. Hyperparameters We use ELMo (Peters et al., 2018) for English, and FastText embeddings (Bojanowski et al., 2017; Grave et al., 2018) for all other languages. We train and run the refinement networks for two iterations. All other hyperparameters are the same for all languages, except BiLSTMs for English is larger than others. Training Details Training the refinement network takes roughly 2 times more time than the baseline models, as it requires running BiLSTMs. The extra computation for the structured refinement network is minimal. For English, training the iterative refinement model for 1 epoch takes about 6 minutes on one 1080ti GPU. Adam is used as the optimizer (Kingma and Ba, 2015), with the learn"
D19-1099,C18-1233,0,0.455788,"Missing"
D19-1099,D19-1544,1,0.609629,"ll A0 A1 A2 Baseline predicated role Figure 2: Confusion matrix for the baseline model, and a correction matrix where the errors were corrected by the refinement network. Only Null, A0, A1, A2 are presented here. can take an inspiration from either declarative constraints used in the previous work (Punyakanok et al., 2008) or from literature on lexical semantics of verbs, studying patterns of event and argument realization (e.g., Levin 1993). Indeed, the unique role constraint as a declarative constraint is one of the motivation for the concurrent work on modeling argument interaction in SRL (Chen et al., 2019). That work relies on capsule networks (Sabour et al., 2017) and focuses primarily on enforcing the role uniqueness constraint. The framework can be extended to other tasks. For example, in syntactic dependency parsing: the refinement network can rely on representations of grandparent nodes, siblings and children to propose a correction. In general, structure refinement networks should allow domain experts to incorporate prior knowledge about output dependencies and improve model performance. Acknowledgments We thank the anonymous reviewers for their suggestions. The project was supported by t"
D19-1099,S12-1029,0,0.0238771,"ated over the entire sentences but not the information what the other arguments are. It is a coarse compressed representation of the prediction, yet it represents long-distance information not readily available within the factorized base model. While this is not the only possible design, we believe that the empirical gains from using this simple refinement network, demonstrate the viability of our general framework of iterative refinement with restricted inference networks. They also suggest that intuitions underlying declarative constraints used in early work on SRL (Punyakanok et al., 2008; Das et al., 2012) can be revisited but now encoded in a flexible soft way to provide induction biases for the refinement networks. We leave this for future work. We consider the CoNLL-2009 dataset (Hajiˇc et al., 2009). We start with a strong factorized baseline model, which already achieves state-ofthe-art results on a subset of the languages. Then, using our structure refinement network, we improve on this baseline on all 7 CoNLL-2009 languages. The model achieves best-reported results in 5 languages, including English. We also observe improvements on out-of-domain test sets, confirming the robustness of our"
D19-1099,P18-2077,0,0.0596169,"ze n × r such that each row sums to 1, corresponding to a probability distribution over roles. In particular Ri,0 is the probability of i-th word not being an argument of the predicate. We index role label and sense predictions from different refinement iterations (‘time steps’) with t, i.e. Pt and Rt . The index t ranges from 0 to T , and P0 and R0 denotes the predictions from the factorized baseline model. Details (e.g., hyperparameters) are provided in the appendix. 1073 3.2 Factorized Model Similarly to recent approaches to SRL and semantic graph parsing (He et al., 2017; Li et al., 2018; Dozat and Manning, 2018), our factorized baseline model starts with concatenated embeddings x. Then, we encode the sentence with a BiLSTM, further extract features with an MLP (multilayer perceptron) and apply a bi-affine classifier to the resulting features to label the words with roles. We also use a predicate-dependent linear layer for sense disambiguation. More formally, we start with getting a sentence representation by concatenating embeddings. We have xw ∈ Rn×dw , xdep ∈ Rn×dδ , xpos ∈ Rn×dp for words, dependency labels and part-of-speech tags, respectively. We concatenate them to form a sentence representatio"
D19-1099,D15-1112,0,0.101519,"Missing"
D19-1099,P17-1044,0,0.202681,"s are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL methods relied on modeling high-order interactions in the output space (e.g., between arguments or arguments and predicates) (Watanabe et al., 2010; Toutanova et al., 2008). Earlier neural methods can model such output interactions through a transition system, and achieve competitive performance (Henderson et al., 2013). However, current state-ofthe-art SRL systems use powerful sentence encoders (e.g., layers of LSTMs (Li et al., 2018; He et al., 2017) or multi-head self-attention (Strubell et al., 2018)) and factorize over small fragments of the predicted structures. Specifically, most modern models process individual arguments and perform predicate disambiguation independently. The trend towards more factorizable models is not unique to dependency-based SRL but common for most structured prediction tasks in NLP (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, 2018). The only major exception is language generation tasks, especially machine translation and language modeling, where larger amounts of text are typically used in traini"
D19-1099,J13-4006,1,0.927424,"ans (see the graph in red in Figure 1). Edges in the dependency graphs are annotated with semantic roles (e.g., A0: PLEASER) and the predicates are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL methods relied on modeling high-order interactions in the output space (e.g., between arguments or arguments and predicates) (Watanabe et al., 2010; Toutanova et al., 2008). Earlier neural methods can model such output interactions through a transition system, and achieve competitive performance (Henderson et al., 2013). However, current state-ofthe-art SRL systems use powerful sentence encoders (e.g., layers of LSTMs (Li et al., 2018; He et al., 2017) or multi-head self-attention (Strubell et al., 2018)) and factorize over small fragments of the predicted structures. Specifically, most modern models process individual arguments and perform predicate disambiguation independently. The trend towards more factorizable models is not unique to dependency-based SRL but common for most structured prediction tasks in NLP (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, 2018). The only major exception is lan"
D19-1099,W18-2501,0,0.0389846,"Missing"
D19-1099,P00-1065,0,0.337018,", University of Edinburgh 2 ILLC, University of Amsterdam chunchuan.lv@gmail.com scohen@inf.ed.ac.uk ititov@inf.ed.ac.uk entity fulfilled (A1) satisfy.01 pleaser (A0) An easier standard for An An easier standard for easier standard for a state to satisfy to to satisfy satisfy satisfy.01 entity fulfilled (A1) a a state state ... Figure 1: An example of structured refinement, the sentence fragment is from CoNLL-2009: the initial prediction by the factorized model in blue, the refined one (identical to the gold standard) in red. Introduction Semantic role labeling (SRL), originally introduced by Gildea and Jurafsky (2000), involves the prediction of predicate-argument structure, i.e., identification of arguments and their assignment to underlying semantic roles. Semantic-role representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2011) and machine translation (Marcheggiani et al., 2018). In this work, we focus on dependency-based SRL (Hajiˇc et al., 2009), a popular version of the task which involves identifying syntactic heads of arguments rather than marking entire argument spans (see the gra"
D19-1099,N10-1112,0,0.0412181,"is drawn from the standard Gumbel distribution (Maddison et al., 2017; Jang et al., 2017), and λg is a hyper-parameter controlling decoding stochasticity.4 5.3 Loss for Iterative Refinement Let us denote gold-standard labels for roles and predicates as R∗ and P∗ . We use two separate losses Lbase (R∗ , P∗ , x) and Lrefine (R∗ , P∗ , x) for our two-stage training. We define losses for predictions from each refinement iteration and sum them up: Lbase (R∗ , P∗ , x) = L(R∗ , R0 ) + L(P∗ , P0 ) (31) Lrefine (R∗ , P∗ , x) = T X L(R∗ , Rt ) + L(P∗ , Pt ) t=1 (32) We adopt the Softmax-Margin loss (Gimpel and Smith, 2010; Blondel et al., 2019) for individual L. Effectively, we subtract 1 from the logit of the gold label, and apply the cross entropy loss. 6 Experiments Datasets We conduct experiments on CoNLL2009 (Hajiˇc et al., 2009) data set for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). We use the predicted part-of-speech tags, dependency labels, and pre-identified predicate, provided with the dataset. The statistics of datasets are shown in Table 2. 4 A more canonical way of controlling stochasticity is to use the temperature"
D19-1099,L18-1550,0,0.0145407,"asets We conduct experiments on CoNLL2009 (Hajiˇc et al., 2009) data set for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). We use the predicted part-of-speech tags, dependency labels, and pre-identified predicate, provided with the dataset. The statistics of datasets are shown in Table 2. 4 A more canonical way of controlling stochasticity is to use the temperature but we prefer not to scale the gradient. Hyperparameters We use ELMo (Peters et al., 2018) for English, and FastText embeddings (Bojanowski et al., 2017; Grave et al., 2018) for all other languages. We train and run the refinement networks for two iterations. All other hyperparameters are the same for all languages, except BiLSTMs for English is larger than others. Training Details Training the refinement network takes roughly 2 times more time than the baseline models, as it requires running BiLSTMs. The extra computation for the structured refinement network is minimal. For English, training the iterative refinement model for 1 epoch takes about 6 minutes on one 1080ti GPU. Adam is used as the optimizer (Kingma and Ba, 2015), with the learning rate of 3e-4. We"
D19-1099,Q16-1023,0,0.0202311,"tions through a transition system, and achieve competitive performance (Henderson et al., 2013). However, current state-ofthe-art SRL systems use powerful sentence encoders (e.g., layers of LSTMs (Li et al., 2018; He et al., 2017) or multi-head self-attention (Strubell et al., 2018)) and factorize over small fragments of the predicted structures. Specifically, most modern models process individual arguments and perform predicate disambiguation independently. The trend towards more factorizable models is not unique to dependency-based SRL but common for most structured prediction tasks in NLP (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, 2018). The only major exception is language generation tasks, especially machine translation and language modeling, where larger amounts of text are typically used in training. Powerful encoders, in principle, can capture long-distance dependencies and hence alleviate the need for modeling high-order interactions in 1071 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1071–1082, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computatio"
D19-1099,D18-1149,0,0.149851,"LFILLED, as in ‘a sweet tooth to satisfy’), instrument (A2: METHOD, as in ‘a little dessert to satisfy your sweet tooth’) and agent (A0: PLEASER, as in our actual example). The basic factorized model got it wrong, assigning A1 to the argument ‘state’. However, taking into account other arguments, the model can correct the label. The configuration ‘A1 to satisfy’ is more likely when an agent (A0) is present in the sentence. The lack of an agent boosts the score for the correct configuration ‘A0 to satisfy’. Our iterative refinement approach encodes the above intuition. In iterative refinement (Lee et al., 2018), a refinement network repeatedly takes previous output as input and produces its refined version. Formally, we have y t+1 = Refine(x, y t ). Naturally, such refinement strategy also requires an initial prediction y 0 , which is produced by a (‘base’) factorized model. Refinement strategies have been successful in machine translation (Lee et al., 2018; Novak et al., 2017; Xia et al., 2017; Hassan et al., 2018), but their effectiveness in other NLP tasks is yet to be demonstrated.1 We conjecture that this discrepancy is due to differences in data availability. Given larger amounts of training d"
D19-1099,N18-2078,1,0.775886,"e fragment is from CoNLL-2009: the initial prediction by the factorized model in blue, the refined one (identical to the gold standard) in red. Introduction Semantic role labeling (SRL), originally introduced by Gildea and Jurafsky (2000), involves the prediction of predicate-argument structure, i.e., identification of arguments and their assignment to underlying semantic roles. Semantic-role representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2011) and machine translation (Marcheggiani et al., 2018). In this work, we focus on dependency-based SRL (Hajiˇc et al., 2009), a popular version of the task which involves identifying syntactic heads of arguments rather than marking entire argument spans (see the graph in red in Figure 1). Edges in the dependency graphs are annotated with semantic roles (e.g., A0: PLEASER) and the predicates are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL methods relied on modeling high-order interactions in the output space (e.g., between arguments or arg"
D19-1099,K17-1041,1,0.824981,"Missing"
D19-1099,D07-1002,0,0.0748708,"ntity fulfilled (A1) a a state state ... Figure 1: An example of structured refinement, the sentence fragment is from CoNLL-2009: the initial prediction by the factorized model in blue, the refined one (identical to the gold standard) in red. Introduction Semantic role labeling (SRL), originally introduced by Gildea and Jurafsky (2000), involves the prediction of predicate-argument structure, i.e., identification of arguments and their assignment to underlying semantic roles. Semantic-role representations have been shown to be beneficial in many NLP applications, including question answering (Shen and Lapata, 2007), information extraction (Christensen et al., 2011) and machine translation (Marcheggiani et al., 2018). In this work, we focus on dependency-based SRL (Hajiˇc et al., 2009), a popular version of the task which involves identifying syntactic heads of arguments rather than marking entire argument spans (see the graph in red in Figure 1). Edges in the dependency graphs are annotated with semantic roles (e.g., A0: PLEASER) and the predicates are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL"
D19-1099,P18-2106,0,0.039378,"Missing"
D19-1099,S15-2153,0,0.217243,"Missing"
D19-1099,N18-1202,0,0.0516561,"rom the logit of the gold label, and apply the cross entropy loss. 6 Experiments Datasets We conduct experiments on CoNLL2009 (Hajiˇc et al., 2009) data set for all languages, including Catalan (Ca), Chinese (Zh), Czech (Cz), English (En), German (De), Japanese (Jp) and Spanish (Es). We use the predicted part-of-speech tags, dependency labels, and pre-identified predicate, provided with the dataset. The statistics of datasets are shown in Table 2. 4 A more canonical way of controlling stochasticity is to use the temperature but we prefer not to scale the gradient. Hyperparameters We use ELMo (Peters et al., 2018) for English, and FastText embeddings (Bojanowski et al., 2017; Grave et al., 2018) for all other languages. We train and run the refinement networks for two iterations. All other hyperparameters are the same for all languages, except BiLSTMs for English is larger than others. Training Details Training the refinement network takes roughly 2 times more time than the baseline models, as it requires running BiLSTMs. The extra computation for the structured refinement network is minimal. For English, training the iterative refinement model for 1 epoch takes about 6 minutes on one 1080ti GPU. Adam"
D19-1099,J08-2005,0,0.664079,"previous iteration aggregated over the entire sentences but not the information what the other arguments are. It is a coarse compressed representation of the prediction, yet it represents long-distance information not readily available within the factorized base model. While this is not the only possible design, we believe that the empirical gains from using this simple refinement network, demonstrate the viability of our general framework of iterative refinement with restricted inference networks. They also suggest that intuitions underlying declarative constraints used in early work on SRL (Punyakanok et al., 2008; Das et al., 2012) can be revisited but now encoded in a flexible soft way to provide induction biases for the refinement networks. We leave this for future work. We consider the CoNLL-2009 dataset (Hajiˇc et al., 2009). We start with a strong factorized baseline model, which already achieves state-ofthe-art results on a subset of the languages. Then, using our structure refinement network, we improve on this baseline on all 7 CoNLL-2009 languages. The model achieves best-reported results in 5 languages, including English. We also observe improvements on out-of-domain test sets, confirming th"
D19-1099,P16-1113,0,0.151582,"., 2018). 6.1 Results and Discussions Test Results Results for all CoNLL-2009 languages on the standard (in-domain) datasets are presented in Table 1. We compare our best model to the best previous single model for the corresponding language (excluding ensemble ones). Most research has focused on English, but we include results of recent models which were evaluated on at least 3 languages. When compared to the previous models, both our models are very competitive, with the exception of German. On the German dataset, Mulcaire et al. (2018) also report a relatively weak result, when compared to Roth and Lapata (2016). The German dataset is the smallest one in terms of the number of predicates. Syntactic information used by Roth and Lapata (2016) may be very beneficial in this setting and may be the reason for this discrepancy. Our structured refinement approach improves over the best previous results on 5 out of 7 languages. Note that hyper-parameters of the refinement network are not tuned for individual languages, suggesting that the proposed method is robust and may be easy to apply to new languages and/or new base models. The only case where the refinement network was not effective is Chinese, where i"
D19-1099,D18-1548,0,0.173638,"Missing"
D19-1099,J08-2002,0,0.124802,"Missing"
D19-1099,N19-1335,0,0.0308413,"his work. While they improve over their baseline model, their baseline model used multilayer perceptron to encode local factors, thus the encoder power is limited. Moreover their refined model performs worse in the out-of-domain setting than their baseline model, indicating overfitting (Belanger et al., 2017). In the follow-up work, Tu and Gimpel (2018, 2019) introduce inference networks to replace gradient descent. Their inference networks directly refine the output. Improvements over competitive baselines are reported on part-of-speech tagging, named entity recognition and CCG supertagging (Tu and Gimpel, 2019). However, their inference networks are distilling knowledge from a tractable linear-chain conditional random field (CRF) model. Thus, these methods do not provide direct performance gains. More importantly, the interactions captured in these models are likely local, as they learn to mimic Markov CRFs. Denoising autoencoders (Vincent et al., 2008) can also be used to refine structure. Indeed, image segmentation can be improved through iterative inference with denoising autoencoders (Romero et al., 2017; Drozdzal et al., 2018). Their framework is very similar to ours, albeit we are working in a"
D19-1099,P19-1454,0,0.0388933,"zal et al., 2018). Their framework is very similar to ours, albeit we are working in a discrete domain. One other difference is that by using a convolutional architecture in the refinement network, they are still modeling only local interactions. At a more conceptual level, Bengio et al. (2013) argued that a denoising autoencoder should not be too robust to the input variations as to ignore the input. This indicates that we should not expect refinement networks to correct all the errors, even in theory, and hence, the refinement networks do not need to be particularly powerful. Very recently, Wang et al. (2019) used high order statistical model for Semantic Dependency Parsing (Oepen et al., 2015), and obtain improvements over strong baseline using BiLSTM. They attempted loopy belief propagation and mean field variational inference for inference, and train the model end to end. Such inference steps are well motivated. This work is similar to energy network approach (Belanger and McCallum, 2016), while a global score function is provided, and approximate inference steps are used. Comparing to ours, the inference can also be regarded as iterative structure refinement. Yet, we do not provide a global sc"
D19-1099,P10-2018,0,0.200576,"on dependency-based SRL (Hajiˇc et al., 2009), a popular version of the task which involves identifying syntactic heads of arguments rather than marking entire argument spans (see the graph in red in Figure 1). Edges in the dependency graphs are annotated with semantic roles (e.g., A0: PLEASER) and the predicates are labeled with their senses from a given sense inventory (e.g., SATISFY.01 in the example). Before the rise of deep learning methods, the most accurate SRL methods relied on modeling high-order interactions in the output space (e.g., between arguments or arguments and predicates) (Watanabe et al., 2010; Toutanova et al., 2008). Earlier neural methods can model such output interactions through a transition system, and achieve competitive performance (Henderson et al., 2013). However, current state-ofthe-art SRL systems use powerful sentence encoders (e.g., layers of LSTMs (Li et al., 2018; He et al., 2017) or multi-head self-attention (Strubell et al., 2018)) and factorize over small fragments of the predicted structures. Specifically, most modern models process individual arguments and perform predicate disambiguation independently. The trend towards more factorizable models is not unique t"
D19-1099,W09-1209,0,0.412489,"t. Out-of-Domain Results on the out-of-domain 1076 Model Ca Cz De En Ja Es Zh Avg. Roth and Lapata (2016) Marcheggiani et al. (2017) Mulcaire et al. (2018)* Previous best single model Baseline model Structured refinement 79.45 80.32 80.69 80.91 86.00 85.14 86.02 87.30 87.62 80.10 69.97 80.10 75.06 75.87 86.7 87.7 87.24 90.40 90.65 90.99 76.00 78.69 81.97 82.54 80.20 80.30 77.32 80.50 79.87 80.53 79.4 81.2 81.89 84.30 83.26 83.31 79.57 82.90 82.69 83.11 Table 1: Labeled F1 score (including senses) for all languages on the CoNLL-2009 in-domain test set. For previous best result, Catalan is from Zhao et al. (2009), Japanese is from Watanabe et al. (2010), Czech is from Henderson et al. (2013), German and Spanish are from Roth and Lapata (2016), English is from Li et al. (2018) and Chinese is from Cai et al. (2018). We report the best testing results from Mulcaire et al. (2018). Ca Cz De En Ja Es Zh #sent 13200 38727 36020 39279 4393 14329 22277 #pred 37444 414133 17400 179014 25712 43828 102827 #pred/#sent 2.84 10.69 0.48 4.56 5.85 3.06 4.62 accurate predictions. A potential alternative explanation is that our refinement network is restricted to simple interactions, resulting in the fixed point reachab"
D19-1212,W19-4301,0,0.0550614,"Missing"
D19-1212,P17-1175,0,0.0691405,"Missing"
D19-1212,N12-1048,0,0.0227948,"an be seen that the multi-view model outperforms all other models. Incremental Inference Incremental sequence labeling refers to making predictions on an incoming sequence when “streamed” in an online fashion. For example, if the sequence is a sentence, we are not allowed to encode the whole sentence first, but instead have to output a relevant label for each word as it arrives in the sequence. Incrementality underlies fundamental human cognition and is essential for scaling systems to large datasets and real-time inference, necessary, for example, in simultaneous translation (interpretation; Bangalore et al., 2012; Yarmohammadi et al., 2013; Cho and Esipova, 2016). 2061 N ONE B I -D IR U NI -D IR MODEL G RISSOM Grissom doesn’t look worried. He takes his You ever been to the gloves off and puts theater, Peter? pr 42.8 39.4 41.3 40.0 43.6 49.6 re 51.2 60.4 63.4 62.7 58.1 49.4 f1 46.6 47.7 50.0 48.8 49.8 49.5 Table 1: Precision (pr), recall (re) and F1 scores for detecting the minority class (perpetrator mentioned) on the held-out dataset. EF stands for early-fusion, while MV for multi-view. The first section of the table reports scores for unidirectional (incremental) models and the second for bidirectio"
D19-1212,Q18-1001,1,0.852946,"available modalities. We demonstrate the effectiveness of our model in an incremental inference setup (Figure 1), where it makes predictions on the fly without encoding the sequence in full, a more realistic scenario of interacting with data. This is a critical feature for online applications such as simultaneous translation (interpretation) and also a desirable behavior for movie processing models that mimic a human viewer watching a movie for the first time. We evaluate our architecture on three tasks pertaining to movie understanding. Specifically, we use the recently introduced dataset of Frermann et al. (2018), which consists of episodes of the Crime Series Investigation (CSI) television series, segmented and aligned for three different modali2057 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2057–2067, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics y1 y2 yt h1 h2 ht ... h1 x11 x21 x31 h2 x12 x22 x32 ht−1 ht x1t x2t x3t Figure 1: General unfolded overview of our model for a multimodal example x with three modalities: at each time step t ∈"
D19-1212,P17-1164,0,0.0160079,"”. Information learned by the attention mechanism may have a distinct conceptual importance (e.g. alignments in machine translation) or simply indicate which elements of the input contribute more to the final output representation. Attention mechanisms can be learned along with the rest of the network in an end-to-end fashion, or can be explicitly supervised by providing the model with pre-calculated attention scores. Supervised attention has been shown to boost the performance of models for machine translation (Mi et al., 2016), constituency parsing (Kamigaito et al., 2017), event detection (Liu et al., 2017b) and aspect-based sentiment analysis (Cheng et al., 2017). Furthermore, image attention mechanisms guided by weak or direct supervision have been proposed for the tasks of image (Liu et al., 2017a) and video captioning (Yu et al., 2017). Multi-head attention mechanisms (Vaswani et al., 2017) employ more than one, independent, attention mechanisms, boasting multiple areas of focus on the input sequence. The main idea behind them is that a single attention head may not prove adequate to capture all the different types and positions of information that are important to the end task. Our attenti"
D19-1212,I17-2002,0,0.01426,"ments of the sequence to a specific “query”. Information learned by the attention mechanism may have a distinct conceptual importance (e.g. alignments in machine translation) or simply indicate which elements of the input contribute more to the final output representation. Attention mechanisms can be learned along with the rest of the network in an end-to-end fashion, or can be explicitly supervised by providing the model with pre-calculated attention scores. Supervised attention has been shown to boost the performance of models for machine translation (Mi et al., 2016), constituency parsing (Kamigaito et al., 2017), event detection (Liu et al., 2017b) and aspect-based sentiment analysis (Cheng et al., 2017). Furthermore, image attention mechanisms guided by weak or direct supervision have been proposed for the tasks of image (Liu et al., 2017a) and video captioning (Yu et al., 2017). Multi-head attention mechanisms (Vaswani et al., 2017) employ more than one, independent, attention mechanisms, boasting multiple areas of focus on the input sequence. The main idea behind them is that a single attention head may not prove adequate to capture all the different types and positions of information that are imp"
D19-1212,D16-1249,0,0.0642349,"Missing"
D19-1212,L18-1602,0,0.0176944,"work and strong baselines. Moreover, we introduce two tasks, crime case and speaker type tagging, that contribute to movie understanding and demonstrate the effectiveness of our model on them.1 1 Introduction While many natural language processing (NLP) problems concern exclusively textual or speech data, the integration of multimodal information (such as images, video or audio) is beneficial for a variety of problems. For example, visual information has been used in affect analysis (Kahou et al., 2016), sentiment analysis (Morency et al., 2011) and machine translation (Calixto et al., 2017; Lala and Specia, 2018). This is also the case for problems which are sequential in nature, such as video summarization (Smith and Kanade, 1998), contin1 Our code is available at https://github.com/ papagandalf/multiview_csi. uous prediction of affect (Nicolaou et al., 2011) or engagement level prediction (Rehg et al., 2013). Most existing multi-view representation learning approaches are tested in an unsupervised setup where the multi-view representations are learned separately from the task, and are designed to accommodate the learning of representation for monolithic (albeit multi-view) data points, not sequences"
D19-1212,N16-1030,0,0.0549442,"example of the labels on which the model operates can be found in Figure 3. We modify the architecture of our model, so that the output of the sequence model cell is fed to different output layers, one for each task. Training proceeds by summing the loss terms for both tasks. In the case of our multiview model, the loss consists of two cross-entropy terms and one correlation term. Moreover, we experiment with adding a Conditional Random Field (CRF) on top of the sequence models, based on recent work that achieves state-of-the-art performance in tagging tasks, such as Named Entity Recognition (Lample et al., 2016). The results for speaker type and case tagging can be found in Table 4, where our model (MV) is compared with an LSTM early fusion model. We use a variant of the evaluation script used for the CoNLL shared tasks5 and report average scores. Our multi-view model consistently outperforms early fusion models. Interestingly, the multi-task MV+CRF model trained exhibits the best performance, suggesting that jointly solving the two tasks improves the capabilities of the model. 4 http://www.conll.org/previous-tasks https://github.com/spyysalo/ conlleval.py 5 5 Related Work Inference on multimodal seq"
D19-1212,D14-1162,0,0.0820859,"abels (binary) and the name of the speaker that uttered them (“None” for scene descriptions). Each speaker belongs to one of the types detective, perpetrator, suspect, extra (none for scene descriptions). An annotated example ex3 Speech has been stripped from the audio track, leaving it only with audio effects and music (so that the text modality will not be deemed redundant and the dataset does not contain overlapping information). Experimental Setup For all experiments, we adopted an experimental setup similar to that of Frermann et al. (2018). For text, we use 50-dimensional GloVe vectors (Pennington et al., 2014) and a convolutional text encoder with maxpooling (filters of sizes 3, 4 and 5, each returning a 75-dimension output). Image features are generated by the final hidden layer of the inception-v4 Szegedy et al., 2017 model (dimensionality of 1,546). Audio features are constructed by concatenating five 13-dimensional Mel-Frequency Cepstral Coefficient (MFCC) feature vectors for each interval. For perpetrator mention identification, we use the case level splits, whereas the speaker and case tasks are performed on the episode level. All LSTM and GRU variants have one layer of length 128 and a dropo"
D19-1212,D18-1548,0,0.0257923,"hich reconstructs the original representations of each view, while our model does not include autoencoding. Casting correlation maximization between three or more variables as the maximization of the sum of the correlation between all pairs of available variables has been previously used in extensions of CCA for more than two views (Benton et al., 2019), or other multi-view learning works (Kumar et al., 2011). Yang et al. (2017) mention it in their paper, although they do not experiment with it. The multi-head attention component of our model bears similarities in spirit to the recent work of Strubell et al. (2018), where an attention head is replaced by a model trained to predict syntac2064 tic dependencies (Dozat and Manning, 2017). In contrast, our model uses explicit supervision for all self-attention heads and is trained to predict the correct attention scores in a multi-task fashion. 6 Conclusions We describe a neural multi-view sequential architecture, paired with a novel objective that takes advantage of supervision, while at the same time, maximizes the correlation between views. We test our approach on the task of perpetrator mention identification of the CSI dataset, on which we show that it"
D19-1212,N16-1174,0,0.0317202,"ld create if only the cross-entropy loss was used. Multi-head Attention Each of the videos of the dataset is assumed to be divided to snippets, with the sequence model operating and making predictions on the snippet level. For each snippet, the text modality may contain a different number of tokens and a fixed-length text representation is generated by a text encoder. We use an RNN as text encoder and the encoded text representation for each snippet is weighted by attention scores calculated over its tokens. Both “query” and “token” representations come from the same sequence (self-attention; Yang et al. 2016). In cases where the dataset contains, apart from snippetlevel, also token-level annotations, the attention module can be directly supervised: we add a term 2060 yt cerpt is shown in Figure 3. 4.1 ht ... ... ht−1 ht x3t x2t a b c ... wt,1 wt,2 wt,k Figure 2: Hierarchical multi-view recurrent model with multi-head attention. Each sentence is encoded with an RNN and three attention heads (a, b and c) calculate attention scores for each of the tokens wti of the t-th sentence of the script. to the model’s loss that minimizes the error of the attention scores, with respect to token-level annotation"
D19-1421,J96-1002,0,0.132643,"dels that aim to predict the next token in a sequence: they can be applied to basic units ranging from individual characters to full words, each approach coming with its own benefits and limitations (Merity et al., 2018a). Wordlevel language models have traditionally been based on n-gram counts, obtaining good performance with smoothing techniques (Kneser and Ney, 1995; Goodman, 2001). Recently, neural networks have shown strong results in language modeling (Bengio et al., 2001), especially recurrent neural networks (Mikolov et al., 2011b). As previous approaches, like maximum entropy models (Berger et al., 1996), neural language models are trained via Maximum Likelihood Estimation (MLE). Thus, their training cost grows linearly with the number of words in the vocabulary, often making it prohibitively slow. This motivated a large amount of research work, bringing a variety of solutions (Chen et al., 2016). The large vocabulary sizes encountered in training corpora arguably stem from the fact that the frequency distribution of words in a corpus of natural language follows Zipf’s law (Powers, 1998). This also implies that the discrepancy between counts of high-frequency and lowfrequency words increases"
D19-1421,D18-1405,0,0.0138012,"g to note that the power transformations will here be applied on the posterior classification probabilities pC θ instead of categorical probabilities pθ . 4.3 Working With Positive Measures The three divergences presentend in Section 3 are defined on positive measures: in theory, we can simply use the exp function on the scores sθ and do not need to normalize them: Obj(θ) = D(pD ||exp (sθ )) where pn is an auxiliary distribution chosen to reflect the training data while still being easy to sample from, and k  |Y |is the number of samples drawn. 8 The same objective is called ‘Ranking NCE’ by Ma and Collins (2018). However, neither the α and β divergences are scale invariant (see right column of Table 1 and Cichocki and Amari 2010). We can show that working with an un-normalized model distribution will, in both those cases, give an objective proportional to the scale of the model, allowing the divergence 4107 9 The full derivation is given in Appendix A.2. Expression Scaling properties n   P 1 pαi qi1−α − αpi + (α − 1)qi α(α−1) i=1 i n h P β β β−1 1 p + (β − 1)q − βp q i i i i β(β−1) i=1 n n n P P P 1 1 pγi + γ1 log qiγ − γ−1 log pi qiγ−1 γ(γ−1) log i=1 i=1 i=1 α ∈ R  {0, 1} β ∈ R  {0, 1} γ ∈ R  {"
D19-1421,W98-1218,0,0.3158,"rrent neural networks (Mikolov et al., 2011b). As previous approaches, like maximum entropy models (Berger et al., 1996), neural language models are trained via Maximum Likelihood Estimation (MLE). Thus, their training cost grows linearly with the number of words in the vocabulary, often making it prohibitively slow. This motivated a large amount of research work, bringing a variety of solutions (Chen et al., 2016). The large vocabulary sizes encountered in training corpora arguably stem from the fact that the frequency distribution of words in a corpus of natural language follows Zipf’s law (Powers, 1998). This also implies that the discrepancy between counts of high-frequency and lowfrequency words increases with the size of the corpus, as well as the number of those low-frequency words. As a consequence, distributed word representations of low-frequency words are difficult to learn. Numerous approaches use decomposition of words with various sub-word units (Sennrich et al., 2016; Kim et al., 2016), but the same phenomenon exists for low-frequency subwords. In order to deal with this issue and to accelerate training, Grave et al. (2017a) implement a dependency between embedding sizes and word"
D19-1421,P16-1162,0,0.0180465,"a variety of solutions (Chen et al., 2016). The large vocabulary sizes encountered in training corpora arguably stem from the fact that the frequency distribution of words in a corpus of natural language follows Zipf’s law (Powers, 1998). This also implies that the discrepancy between counts of high-frequency and lowfrequency words increases with the size of the corpus, as well as the number of those low-frequency words. As a consequence, distributed word representations of low-frequency words are difficult to learn. Numerous approaches use decomposition of words with various sub-word units (Sennrich et al., 2016; Kim et al., 2016), but the same phenomenon exists for low-frequency subwords. In order to deal with this issue and to accelerate training, Grave et al. (2017a) implement a dependency between embedding sizes and word frequencies in the output layer, while Baevski and Auli (2019) apply it to the input layer, comparing the possible choices of which units to model. Using a different approach, Gong et al. (2018) attempt to learn word representations that are less affected by these large discrepancies in word frequencies, with an adver4104 Proceedings of the 2019 Conference on Empirical Methods in"
D19-1421,W10-2605,0,0.0843445,"Missing"
E17-1051,W04-0308,0,0.582249,"a set of configurations and transitions between them. The basic components of a configuration are a stack of partially processed words and a buffer of unseen input words. Starting from an initial configuration, the system applies transitions until a terminal configuration is reached. The sentence is scanned left to right, with linear time complexity for dependency parsing. This is made possible by the use of a greedy classifier that chooses the transition to be applied at each step. In this paper we introduce a parser for AMR that is inspired by the A RC E AGER dependency transition system of Nivre (2004). The main difference between our system and A RC E AGER is that we need to account for the mapping from word tokens to AMR nodes, non-projectivity of AMR structures and reentrant nodes (multiple incoming edges). Our AMR parser brings closer dependency parsing and AMR parsing by showing that dependency parsing algorithms, with some modAbstract Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition, semantic role labeling, word sense disambiguation and co-reference resolution. We descri"
E17-1051,J08-4003,0,0.18503,"B. Cohen Giorgio Satta School of Informatics Dept. of Information Engineering University of Edinburgh University of Padua scohen@inf.ed.ac.uk Abstract ied, such as CCG (Steedman, 1996; Steedman, 2000) and UCCA (Abend and Rappoport, 2013). Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015a; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Zhou et al., 2016). This line of research is new and current results suggest a large room for improvement. Greedy transitionbased methods (Nivre, 2008) are one of the most popular choices for dependency parsing, because of their good balance between efficiency and accuracy. These methods seem promising also for AMR, due to the similarity between dependency trees and AMR structures, i.e., both representations use graphs with nodes that have lexical content and edges that represent linguistic relations. A transition system is an abstract machine characterized by a set of configurations and transitions between them. The basic components of a configuration are a stack of partially processed words and a buffer of unseen input words. Starting from"
E17-1051,K15-1004,0,0.465629,"Missing"
E17-1051,P13-1023,0,0.195614,"Missing"
E17-1051,D15-1136,0,0.256154,"Missing"
E17-1051,D15-1198,0,0.515494,"Missing"
E17-1051,W13-2322,0,0.217613,"ses sentences leftto-right, in linear time. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our parser is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity. 1 satta@dei.unipd.it Introduction Semantic parsing aims to solve the problem of canonicalizing language and representing its meaning: given an input sentence, it aims to extract a semantic representation of that sentence. Abstract meaning representation (Banarescu et al., 2013), or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and coreference resolution. AMR is partially motivated by the need to provide the NLP community with a single dataset that includes basic disambiguation information, instead of having to rely on different datasets for each disambiguation problem. The annotation process is straightforward, enabling the development of large datasets. Alternative semantic representations h"
E17-1051,P13-2131,0,0.538359,"Missing"
E17-1051,D14-1082,0,0.00349826,"or named entities and additional sparse features, extracted from the current configuration of the transition system; this is reported in more details in Table 3. The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion charOracle Training our system from data requires an oracle—an algorithm that given a gold-standard AMR graph and a sentence returns transition sequences that maximize the overlap between the gold-standard graph and the graph dictated by the sequence of transitions. We adopt a shortest stack, static oracle similar to Chen and Manning (2014). Informally, static means that if the actual configuration of the parser has no mistakes, the oracle provides a transition that does not introduce any mistake. Shortest stack means that the oracle prefers transitions where the number of items in the stack is minimized. Given the current configuration (σ, β, A) and the gold540 action Shift Shift Shift LArc RArc Shift Shift RArc Reduce Reduce stack [◦] [◦] [◦, boy] [◦, boy, and ] [◦, and ] [◦, and ] [◦, and ] [◦, and, girl ] [◦, and, girl ] [◦, and ] [◦] buffer [the,boy,and,the,girl] [boy,and,the,girl] [and,the,girl] [the,girl] [the,girl] [the,"
E17-1051,S14-2080,0,0.0372974,"t al. (2015a), called CAMR, also defines a transition system. It differs from ours because we process the sentence left-toright while they first acquire the entire dependency tree and then process it bottom-up. More recently Zhou et al. (2016) presented a non-greedy transition system for AMR parsing, based on A RC S TANDARD (Nivre, 2004). Our transition system is also related to an adaptation of A RC E AGER for directed acyclic graphs (DAGs), introduced by Sagae and Tsujii (2008). This is also the basis for Ribeyre et al. (2015), a transition system used to parse dependency graphs. Similarly, Du et al. (2014) also address dependency graph parsing by means of transition systems. Analogously to dependency trees, dependency graphs have the property that their nodes consist of the word tokens, which is not true for AMR. As such, these transition systems are more closely related to traditional transition systems for dependency parsing. Our contributions in this paper are as follows: Figure 1: Annotation for the sentence “I beg you to excuse me.” Variables are in boldface and concepts and edge labels are in italics. :top beg-01 :ARG0 i you :ARG0 :ARG2 excuse-01 :ARG1 Figure 2: AMR graph representation f"
E17-1051,P15-2140,0,0.0126748,"ument structure. For instance, we may be interested in knowing whether two events or entities are related to each other, while not being concerned with the precise type of relation holding between them. No WSD gives a score that does not take into account word sense disambiguation errors. By ignoring the sense specified by the Propbank frame used (e.g., duck-01 vs duck-02) we have a score that does not take into account this additional complexity in the parsing procedure. To compute this score, we simply strip off the suffixes from all Propbank frames and calculate the Smatch score. Following Sawai et al. (2015), we also evaluate the parsers using the Smatch score on noun phrases only (NP-only), by extracting from the AMR dataset all noun phrases that do not include further NPs. As we previously discussed, reentrancy is a very important characteristic of AMR graphs and it is not trivial to handle. We therefore implement a test for it (Reentrancy), where we compute the Smatch score only on reentrant edges. Concept identification is another critical component of the parsing process and we therefore compute the F-score on the list of predicted concepts (Concepts) too. Identifying the correct concepts is"
E17-1051,P14-1134,0,0.57426,"entity recognizer to make the correct predictions. In order to alleviate the problem of wrong automatic alignments with respect to polarity and better detect negation, we performed a post-processing step on the aligner output where we align the AMR constant - (minus) with words bearing negative polarity such as not, illegitimate and asymmetry. Table 6: Results on test split of LDC2015E86 for JAMR, CAMR and our A MR E AGER. J stands for JAMR and C for CAMR (followed by the year of publication). Best systems are in bold. 6 Experiments We compare our parser11 against two available parsers: JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015b; Wang et al., 2015a), using the LDC2015E86 dataset for evaluation. Both parsers are available online12 and were recently updated for SemEval-2016 Task 8 (Flanigan et al., 2016; Wang et al., 2016). However, CAMR’s SemEval system, which reports a Smatch score of 67, is not publicly available. CAMR has a quadratic worst-case complexity (although linear in practice). In JAMR, the concept identification step is quadratic and the relation identification step is O(|V |2 log |V |), with |V |being the set of nodes in the AMR graph. Table 6 shows the results obtained by the"
E17-1051,S16-1186,0,0.165267,"the aligner output where we align the AMR constant - (minus) with words bearing negative polarity such as not, illegitimate and asymmetry. Table 6: Results on test split of LDC2015E86 for JAMR, CAMR and our A MR E AGER. J stands for JAMR and C for CAMR (followed by the year of publication). Best systems are in bold. 6 Experiments We compare our parser11 against two available parsers: JAMR (Flanigan et al., 2014) and CAMR (Wang et al., 2015b; Wang et al., 2015a), using the LDC2015E86 dataset for evaluation. Both parsers are available online12 and were recently updated for SemEval-2016 Task 8 (Flanigan et al., 2016; Wang et al., 2016). However, CAMR’s SemEval system, which reports a Smatch score of 67, is not publicly available. CAMR has a quadratic worst-case complexity (although linear in practice). In JAMR, the concept identification step is quadratic and the relation identification step is O(|V |2 log |V |), with |V |being the set of nodes in the AMR graph. Table 6 shows the results obtained by the parsers on all metrics previously introduced. On Smatch, our system does not give state-of-the-art results. However, we do obtain the best results for Unlabeled and Concept and outperform the other parses"
E17-1051,N15-3006,0,0.0865352,"Missing"
E17-1051,P16-1001,0,0.0614125,"Missing"
E17-1051,P15-2141,0,0.706086,", enabling the development of large datasets. Alternative semantic representations have been developed and stud536 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 536–546, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ( b / beg-01 :ARG0 ( i / i :ARG1 ( y / you) :ARG2 ( e / excuse-01 :ARG0 y :ARG1 i ) ) ifications, can be used for AMR. Key properties such as working left-to-right, incrementality1 and linear complexity further strengthen its relevance. The AMR parser of Wang et al. (2015a), called CAMR, also defines a transition system. It differs from ours because we process the sentence left-toright while they first acquire the entire dependency tree and then process it bottom-up. More recently Zhou et al. (2016) presented a non-greedy transition system for AMR parsing, based on A RC S TANDARD (Nivre, 2004). Our transition system is also related to an adaptation of A RC E AGER for directed acyclic graphs (DAGs), introduced by Sagae and Tsujii (2008). This is also the basis for Ribeyre et al. (2015), a transition system used to parse dependency graphs. Similarly, Du et al. ("
E17-1051,Q15-1040,0,0.0467316,"u is aligned with xi and v is aligned with xj . The spanning set for e, written S(e), is the set of all nodes w such that π(w) = k and i < k < j if i < j or j < k < i if j < i. We say that e is projective if, for every node w ∈ S(e), all of its parent and child nodes are in S(e) ∪ {u, v}; otherwise, we say that e is non-projective. An AMR is projective if all of its edges are projective, and is non-projective otherwise. This corresponds to the intuitive definition of projectivity for DAGs introduced in Sagae and Tsujii (2008) and is closely related to the definition of non-crossing graphs of Kuhlmann and Jonsson (2015). Table 1 demonstrates that a relatively small percentage of all AMR edges are non-projective. Yet, a large fraction of the sentences contain at least one non-projective edge. Our parser is able to construct non-projective edges, as described in §3. since it is neither injective nor surjective. For each i ∈ [n], we let π −1 (i) = {v |v ∈ V, π(v) = i} be the pre-image of i under π (this set can be empty for some i), which means that we map a token in the sentence to a set of nodes in the AMR. In this way we can align each index i for x to the induced subgraph of G. More formally, we define ← −("
E17-1051,N15-1040,0,0.715322,", enabling the development of large datasets. Alternative semantic representations have been developed and stud536 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 536–546, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ( b / beg-01 :ARG0 ( i / i :ARG1 ( y / you) :ARG2 ( e / excuse-01 :ARG0 y :ARG1 i ) ) ifications, can be used for AMR. Key properties such as working left-to-right, incrementality1 and linear complexity further strengthen its relevance. The AMR parser of Wang et al. (2015a), called CAMR, also defines a transition system. It differs from ours because we process the sentence left-toright while they first acquire the entire dependency tree and then process it bottom-up. More recently Zhou et al. (2016) presented a non-greedy transition system for AMR parsing, based on A RC S TANDARD (Nivre, 2004). Our transition system is also related to an adaptation of A RC E AGER for directed acyclic graphs (DAGs), introduced by Sagae and Tsujii (2008). This is also the basis for Ribeyre et al. (2015), a transition system used to parse dependency graphs. Similarly, Du et al. ("
E17-1051,P11-2033,0,0.0466942,"Missing"
E17-1051,D16-1065,0,0.800083,"remental Parser for Abstract Meaning Representation Marco Damonte School of Informatics University of Edinburgh m.damonte@sms.ed.ac.uk Shay B. Cohen Giorgio Satta School of Informatics Dept. of Information Engineering University of Edinburgh University of Padua scohen@inf.ed.ac.uk Abstract ied, such as CCG (Steedman, 1996; Steedman, 2000) and UCCA (Abend and Rappoport, 2013). Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015a; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Zhou et al., 2016). This line of research is new and current results suggest a large room for improvement. Greedy transitionbased methods (Nivre, 2008) are one of the most popular choices for dependency parsing, because of their good balance between efficiency and accuracy. These methods seem promising also for AMR, due to the similarity between dependency trees and AMR structures, i.e., both representations use graphs with nodes that have lexical content and edges that represent linguistic relations. A transition system is an abstract machine characterized by a set of configurations and transitions between the"
E17-1051,P14-5010,0,\N,Missing
E17-1051,C08-1095,0,\N,Missing
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
J12-3003,P05-1022,0,0.0220149,"he relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent s"
J12-3003,J99-1004,0,0.675922,"µ(k,i−1),2 = 1 − µ(k,i−1),2 . G , µ is a weighted context-free grammar such that the µ(k,i),1 corresponds to the ith event in the k multinomial of the original grammar. Let z be a derivation in G and z = ΥG →G (z). Then, from Utility Lemma 1 and the construction of g , we have that: p(z |θ, G) = Nk K   ψ (z) θk,ik,i k =1 i =1 Nk ψk,i (z) K    = θk,i k =1 i =1 l =1 Nk ψk,i (z) K    = k =1 i =1 l =1 Nk K   = k =1 i =1     i− 1  j =1 i− 1   µ(k,j),2  µ(k,i),1  ψk,i (z)  µψk,i (z) µ(k,j),2 (k,i),1 j=1 Nk 2 K    = ψ (z ) k,j µ(k,j),i k =1 j =1 i =1 = p(z |µ, G ) From Chi (1999), we know that the weighted grammar G , µ can be converted to a probabilistic context-free grammar G , θ , through a construction of θ based on µ, such that p(z |µ, G ) = p(z |θ , G ).  The proof for Theorem 1 gives a construction the parameters θ of G such that G, θ is equivalent to G , θ . The construction of θ can also be reversed: Given θ for G , we can construct θ for G so that again we have equivalence between G, θ and G , θ . In this section, we focused on presenting parametrized, empirically justiﬁed distributional assumptions about language data that will make the a"
J12-3003,P10-1152,1,0.932275,"note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justiﬁed assumptions about the distributions th"
J12-3003,J03-4003,0,0.0604102,"nnotate, and the relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars i"
J12-3003,N06-1043,0,0.099011,"s optimization problem is   θk,i = min  1 − γ, max     γ,  n  j=1  &quot; ψˆ j,k,i   n  2  j=1 i  =1    ψˆ j,k,i   (22) where ψˆ j,k,i is the number of times that ψk,i ﬁres in Example j. (We include a full derivation of this result in Appendix B.) The interpretation of Equation (22) is simple: We count the number of times a rule appears in the samples and then normalize this value by the total number of times rules associated with the same multinomial appear in the samples. This frequency count is the maximum likelihood solution with respect to the full hypothesis class H (Corazza and Satta 2006; see Appendix B). Because we constrain ourselves to obtain a value away from 0 or 1 by a margin of γ, we need to truncate this solution, as done in Equation (22). This truncation to a margin γ can be thought of as a smoothing factor that enables us to compute sample complexity bounds. We explore this connection to smoothing with a Dirichlet prior in a Maximum a posteriori (MAP) Bayesian setting in Section 7.2. 6.2 Unsupervised Case Similarly to the supervised case, minimizing the empirical log-loss in the unsupervised setting requires minimizing (with respect to θ) the following:     p˜n"
J12-3003,N10-1118,0,0.0265415,"of G as deﬁned earlier. Then, there exists θ for G such that for any z ∈ D(G) we have p(z |θ, G) = p(ΥG →G (z) |θ , G ). 6 We note that this notion of binarization is different from previous types of binarization appearing in computational linguistics for grammars. Typically in previous work about binarized grammars such as CFGs, the grammars are constrained to have at most two nonterminals in the right side in Chomsky normal form. Another form of binarization for linear context-free rewriting systems is restriction of the ´ fan-out of the rules to two (Gomez-Rodr´ ıguez and Satta 2009; Gildea 2010). We, however, limit the number of rules for each nonterminal (or more generally, the number of elements in each multinomial). 491 Computational Linguistics Volume 38, Number 3 Proof For the grammar G, index the set {1, ..., K} with nonterminals ranging from A1 to AK . Deﬁne G as before. We need to deﬁne θ . Index the multinomials in G by (k, i), each having two events. Let µ(k,i),1 = θk,i , µ(k,i),2 = 1 − θk,i for i = 1 and set µk,i,1 = θk,i /µ(k,i−1),2 , and µ(k,i−1),2 = 1 − µ(k,i−1),2 . G , µ is a weighted context-free grammar such that the µ(k,i),1 corresponds to the ith event in the k"
J12-3003,P09-1111,0,0.0605962,"Missing"
J12-3003,P04-1061,0,0.0656274,"o language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justiﬁed assumptions abou"
J12-3003,P89-1017,0,0.445798,"ticle, we limit the degree of the grammar by making the assumption that all Nk ≤ 2. This assumption may seem, at ﬁrst glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence, other formalisms), this assumption does not limit the total generative capacity that we can have across all context-free grammars. We ﬁrst show that any context-free grammar with arbitrary degree can be mapped to a corresponding grammar with all Nk ≤ 2 that generates derivations equivalent to derivations in the original grammar. Such a grammar is also called a “covering grammar” (Nijholt 1980; Leermakers 1989). Let G be a CFG. Let A be the kth nonterminal. Consider the rules A → αi for i ≤ Nk where A appears on the left side. For each rule 490 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars Figure 2 Example of a context-free grammar and its equivalent binarized form. A → αi , i < Nk , we create a new nonterminal in G such that Ai has two rewrite rules: Ai → αi and Ai → Ai+1 . In addition, we create rules A → A1 and ANk → αNk . Figure 2 demonstrates an example of this transformation on a small context-free grammar. It is easy to verify that the resulting grammar G has an equ"
J12-3003,P92-1017,0,0.382509,"fer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically j"
J16-3003,N13-1052,1,0.884109,"Missing"
J16-3003,P81-1022,0,0.746306,"e parsed in time O(n4.76 ). It also shows that inversion transduction grammars can be parsed in time O(n5.76 ). In addition, binary LCFRS subsumes many other formalisms and types of grammars, for some of which we also improve the asymptotic complexity of parsing. 1. Introduction The problem of grammar recognition is a decision problem of determining whether a string belongs to a language induced by a grammar. For context-free grammars (CFGs), recognition can be done using parsing algorithms such as the CKY algorithm (Kasami 1965; Younger 1967; Cocke and Schwartz 1970) or the Earley algorithm (Earley 1970). The asymptotic complexity of these chart-parsing algorithms is cubic in the length of the sentence. In a major breakthrough, Valiant (1975) showed that context-free grammar recognition is no more complex than Boolean matrix multiplication for a matrix of size m × m where m is linear in the length of the sentence, n. With current state-of-the-art results in matrix multiplication, this means that CFG recognition can be done with an asymptotic complexity of O(n2.38 ). ∗ School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, United Kingdom. E-mail: scohen@inf.ed.ac.uk. ∗∗ Department"
J16-3003,H05-1036,0,0.091604,"Missing"
J16-3003,P99-1059,0,0.174207,"d it is a condition on the set of LCFRS grammar rules that is satisfied with many practical grammars. In cases where the grammar is balanced, our algorithm can be used as a subroutine so that it parses the binary LCFRS in time O(nωd+1 ). A similar procedure was applied by Nakanishi et al. (1998) for multiple component context-free grammars. See more discussion of this in Section 7.5. Our results focus on the asymptotic complexity as a function of string length. We do not give explicit grammar constants. For other work that focuses on reducing the grammar constant in parsing, see, for example, Eisner and Satta (1999), Dunlop, Bodenstab, and Roark (2010), and Cohen, Satta, and Collins (2013). For a discussion of the optimality of the grammar constants in Valiant’s algorithm, see, for example, Abboud, Backurs, and Williams (2015). 2. Background and Notation This section provides background on LCFRS, and establishes notation used in the remainder of the paper. A reference table of notation is also provided in Appendix A. For an integer n, let [n] denote the set of integers {1, . . . , n}. Let [n]0 = [n] ∪ {0}. For a set X, we denote by X+ the set of all sequences of length 1 or more of elements from X. A spa"
J16-3003,J11-1008,1,0.84548,"nonterminals and the rank of the rules. With tabular parsing, we can actually refer to the parsing complexity of a specific rule in the grammar. Its complexity is O(np ), where the parsing complexity p is the total fan-out of all nonterminals in the rule. For binary rules of the form A → B C, p = ϕ(A) + ϕ(B) + ϕ(C). To optimize the tabular algorithm time complexity of parsing with a binary LCFRS, equivalent to another non-binary LCFRS, we would want to minimize the time complexity it takes to parse each rule. As such, our goal is to minimize ϕ(A) + ϕ(B) + ϕ(C) in the resulting binary grammar. Gildea (2011) has shown that this metric corresponds to the tree width of a dependency graph that is constructed from the grammar. It is not known whether finding the optimal binarization of an LCFRS is an NP-complete problem, but Gildea shows that a polynomial time algorithm would imply improved approximation algorithms for the treewidth of general graphs. In general, the optimal binarization for tabular parsing may not by the same as the optimal binarization for parsing with our algorithm based on matrix multiplication. In order to optimize the complexity of our algorithm, we want to minimize d, which is"
J16-3003,J09-4009,1,0.801115,"re details in Section 7.3. 7.2 General Recognition for Synchronous Parsing Similarly to LCFRS, the rank of an SCFG is the maximal number of nonterminals that appear in the right-hand side of a rule. Any SCFG can be binarized into an LCFRS grammar. However, when the SCFG rank is arbitrary, this means that the fan-out of the LCFRS grammar can be larger than 2. This happens because binarization creates intermediate nonterminals that span several substrings, denoting binarization steps of the rule. These substrings are eventually combined into two spans, to yield the language of the SCFG grammar (Huang et al. 2009). Our algorithm does not always improve the asymptotic complexity of SCFG parsing over tabular methods. For example, Figure 10 shows the combination of spans for the rule [S → A B C D, B D A C], along with a binarization into three simpler LCFRS rules. A na¨ıve tabular algorithm for this rule would have the asymptotic complexity of O(n10 ), but the binarization shown in Figure 10 reduces this to O(n8 ). Our algorithm gives a complexity of O(n9.52 ), as the second step in the binarization shown consists of a rule with d = 4. 7.3 Generalization to Weighted Logic Programs Weighted logic programs"
J16-3003,C10-1061,0,0.0805117,"Missing"
J16-3003,P92-1012,0,0.483558,"widely used in practice. Bened´ı and S´anchez (2007) show speed improvement when parsing natural language sentences using Strassen’s algorithm as the matrix multiplication subroutine for Valiant’s algorithm for CFG parsing. This indicates that similar speed-ups may be possible in practice using our algorithm for LCFRS parsing. 1.2 Main Result Our main result is a matrix multiplication algorithm for unbalanced, single-initial binary LCFRS with asymptotic complexity M(nd ) = O(nωd ) where d is the maximal 1 Without placing a bound on f , the problem of recognition of LCFRS languages is NP-hard (Satta 1992). 422 Cohen and Gildea Parsing LCFRS with Fast Matrix Multiplication number of combination points in all grammar rules. The constant d can be easily determined from the grammar at hand:    ϕ(A) + ϕ(B) − ϕ(C),  d = max max ϕ(A) − ϕ(B) + ϕ(C),   A→B C −ϕ(A) + ϕ(B) + ϕ(C) where A → B C ranges over rules in the grammar and ϕ(A) is the fan-out of nonterminal A. Single-initial grammars are defined in Section 2, and include common formalisms such as tree-adjoining grammars. Any LCFRS can be converted to single-initial form by increasing its fan-out by at most one. The notion of unbalanced gramm"
J16-3003,J94-2002,0,0.44688,"eir (1994) reduced tree-adjoining grammars to combinatory categorial grammars. The TAGs they tackle are in “normal form,” such that the auxiliary trees are binary (all TAGs can be reduced to normal form TAGs). Such TAGs can be converted to weakly equivalent CCG (but not necessarily strongly equivalent), and as such, our algorithm applies to TAGs as well. As mentioned earlier, this finding supports the finding of Rajasekaran and Yooseph (1998), who show that TAG can be recognized in time O(M(n2 )). For an earlier discussion connections between TAG parsing and Boolean matrix multiplication, see Satta (1994). 5 General indexed grammars copy the stack to multiple nonterminals on the right-hand side. 448 Cohen and Gildea Parsing LCFRS with Fast Matrix Multiplication 6.2 Synchronous Context-Free Grammars SCFGs are widely used in machine translation to model the simultaneous derivation of translationally equivalent strings in two natural languages, and are equivalent to the syntax-directed translation schemata of Aho and Ullman (1969). SCFGs are a subclass of LCFRS where each nonterminal has fan-out 2: one span in one language and one span in the other. Because the first span of the l.h.s. nontermina"
J16-3003,C90-3045,0,0.251844,"ve a bound of O(n2ω+1 ) for ITG, which is O(n5.76 ) using the current state of the art for matrix multiplication. We achieve even greater gains for the case of multi-language synchronous parsing. Generalizing ITG to allow two nonterminals on the right-hand side of a rule in each of k languages, we have an LCFRS with fan-out k. Traditional tabular parsing has an asymptotic complexity of O(n3k ), whereas our algorithm has the complexity of O(nωk+1 ). Another interesting case of a synchronous formalism that our algorithm improves the best well-known result for is that of binary synchronous TAGs (Shieber and Schabes 1990)—that is, a TAG in which all auxiliary trees are binary. This formalism can be reduced to a binary LCFRS. A tabular algorithm for such grammar has the asymptotic complexity of O(n12 ). With our algorithm, d = 4 for this formalism, and as such its asymptotic complexity in that case is O(n9.52 ). 7. Discussion and Open Problems In this section, we discuss some extensions to our algorithm and open problems. 7.1 Turning Recognition into Parsing The algorithm we presented focuses on recognition: Given a string and a grammar, it can decide whether the string is in the language of the grammar or not."
J16-3003,J97-3002,0,0.450389,"ning that we reduce parsing complexity from O(n3f ) to O(nωf ), and that, in general, the savings in the exponent is larger for more complex grammars. LCFRS is a broad family of grammars. As such, we are able to support the findings of Rajasekaran and Yooseph (1998), who showed that tree-adjoining grammar (TAG) recognition can be done in time O(M(n2 )) = O(n4.76 ) (TAG can be reduced to LCFRS with d = 2). As a result, combinatory categorial grammars, head grammars, and linear indexed grammars can be recognized in time O(M(n2 )). In addition, we show that inversion transduction grammars (ITGs; Wu 1997) can be parsed in time O(nM(n2 )) = O(n5.76 ), improving the best asymptotic complexity previously known for ITGs. 1.1 Matrix Multiplication State of the Art Our algorithm reduces the problem of LCFRS parsing to Boolean matrix multiplication. Let M(n) be the complexity of multiplying two such n × n matrices. These matrices can be na¨ıvely multiplied in O(n3 ) time by computing for each output cell the dot product between the corresponding row and column in the input matrices (each such product is an O(n) operation). Strassen (1969) discovered a way to do the same multiplication in O(n2.8704 )"
J16-3003,W90-0102,0,\N,Missing
K15-1001,D13-1160,0,0.0277749,"t al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the prediction. 3 3.1 1: I"
K15-1001,P10-4002,0,0.0458185,"Missing"
K15-1001,N13-1073,0,0.0443787,"terations 12000 16000 0.90 scaled; α = 0.1 scaled; α = 0.5 scaled; α = 1.0 scaled; α = 0.1 scaled; α = 0.5 scaled; α = 1.0 0.80 0.70 0.32 0.31 0.60 0.50 TER regret 0.29 20000 iterations 0.40 0.30 0.30 0.20 0.10 0.00 0 4000 8000 12000 16000 0 20000 4000 iterations 8000 12000 16000 0.29 20000 iterations Figure 1: Regret and TER vs. iterations for α-informative feedback ranging from weak (α = 0.1) to strong (α = 1.0) informativeness, with (lower part) and without re-scaling (upper part). Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned with fast align (Dyer et al., 2013). In all experiments, training is started with the Moses default weights. The size of the n-best list, where used, was set to 1,000. Irrespective of the use of re-scaling in perceptron training, a constant learning rate of 10−5 was used for learning from simulated feedback, and 10−4 for learning from surrogate translations. Our experiments on online learning require a random sequence of examples for learning. Following the techniques described in Bertsekas (2011) to generate random sequences for incremental optimization, we compared cyclic order (K epochs of T examples in fixed order), randomi"
K15-1001,W12-3160,0,0.0138411,"eory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user (providing feedbac"
K15-1001,N12-1023,0,0.0909,"t al. (2008) for pioneering work on online computer-assisted translation). Online learning algorithms can be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the decoder search space. For example, the objective function actually optimized in Liang et al.’s (2006) application of Collins’ (2002) structure perceptron has been analyzed by Gimpel and Smith (2012) as a non-convex ramp loss function (McAllester and Keshet, 2011; Do et al., 2008; Collobert et al., 2006). Since online convex optimization does not provide convergence guarantees for the algorithm of Liang et al. (2006), Gimpel and Smith (2012) recommend CCCP (Yuille and Rangarajan, 2003) instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective. The goal of this paper is to present an alternative theoretical analysis of online learning algorithms for SMT from the viewpoint of coactive learning (Shivaswamy and Joach"
K15-1001,D14-1130,0,0.0779747,"et and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et al., 2006) or “oracle” derivations (Sokolov et al., 2 Algorithm 1 Feedback-based Latent Perceptron or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in resp"
K15-1001,D08-1024,0,0.0319351,"an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of co"
K15-1001,N09-1025,0,0.0172291,"nversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of computing surrogate refe"
K15-1001,W11-2123,0,0.015573,"G corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3 http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4 http://statmt.org/wmt10/translation-task.html 2 This condition is too strong for large datasets. However, we believe that a weaker condition based on ideas from the 5 0.90 α = 0.1 α = 0.5 α = 1.0 α = 0.1 α = 0.5 α = 1.0 0.80 0.70 0.31 0.60 0.50 TER regret 0.32 0.40 0.30 0.30 0.20 0.10 0"
K15-1001,P07-2045,0,0.00485873,", respectively. Finally, we denote by DT,K = Tt=1 ∆2t,K , and by wT,K the final weight vector returned after K epochs. We state a condition of convergence2 : 4 Experiments We used the LIG corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3 http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4 http://statmt.org/wmt10/translation-task.html 2 This condition is too strong for large datasets. However,"
K15-1001,W02-1001,0,0.173415,"rithm. p DT,K T The theorem can be interpreted as a bound on the generalization error (lefthand-side) by the empirical error (the first two righthand-side terms) and the variance caused by the finite sample (the third term in the theorem). The result follows directly from McDiarmid’s concentration inequality. Generalization for Online-to-Batch Conversion. In practice, perceptron-type algorithms are often applied in a batch learning scenario, i.e., the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set (Freund and Schapire, 1999; Collins, 2002). The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector wT,K whose expected loss on unseen data we would like to bound. We assume that the algorithm is fed with a sequence of examples x1 , . . . , xT , and at each epoch k = 1, . . . , K it makes a prediction yt,k . The correct label is yt∗ . For k = 1, . . . , K and t = 1, . . . , T , let `t,k = U (xt , yt∗ ) − U (xt , yt,k ), and denote by ∆t,k and ξt,k the distance at epoch k for example t, and the slack at epoch k for example Pt, respective"
K15-1001,D13-1161,0,0.0218824,"ase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter alia). Feedback in response-based learning is given by a user accepting or rejecting system predictions, but not by user corrections. Lastly, feedback in form of numerical utility values for actions is studied in the frameworks of reinforcement learning (Sutton and Barto, 1998) or in online learning with limited feedback, e.g., multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative feedback with immediate qualitative feedback in form of a structured object that improves upon the utility of the"
K15-1001,E14-1042,0,0.0580624,"ang, 2012) minimizes regret and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et al., 2006) or “oracle” derivations (Sokolov et al., 2 Algorithm 1 Feedback-based Latent Perceptron or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and updating the model parameters. Such world interaction consists of database access in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013), inter al"
K15-1001,P06-1096,0,0.0940262,"Missing"
K15-1001,P06-1091,0,0.030103,"additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contri"
K15-1001,P03-1021,0,0.10804,"Missing"
K15-1001,2013.mtsummit-papers.2,1,0.871215,"Missing"
K15-1001,W10-1723,0,0.0209251,"epoch k for example t, and the slack at epoch k for example Pt, respectively. Finally, we denote by DT,K = Tt=1 ∆2t,K , and by wT,K the final weight vector returned after K epochs. We state a condition of convergence2 : 4 Experiments We used the LIG corpus3 which consists of 10,881 tuples of French-English post-edits (Potet et al., 2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare SMT outputs for post-editing, the creators of the corpus used their own WMT10 system (Potet et al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features. We replicated a similar Moses system using the same monolingual and parallel data: a 5-gram language model was estimated with the KenLM toolkit (Heafield, 2011) on news.en data (48.65M sentences, 1.13B tokens), pre-processed with the tools from the cdec toolkit (Dyer et al., 2010). perceptron cycling theorem (Block and Levin, 1970; Gelfand et al., 2010) should suffice to show a similar bound. 3 http://www-clips.imag.fr/geod/User/marion.potet/ index.php?page=download 4 http://statmt.org/wmt10/translation-task.ht"
K15-1001,2006.iwslt-evaluation.14,0,0.0304547,"extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confin"
K15-1001,potet-etal-2012-collection,0,0.0397471,"Missing"
K15-1001,D07-1080,0,0.0262121,"alization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that c"
K15-1001,N12-1026,0,0.0190537,"ts for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user (providing feedback in form of an i"
K15-1001,2012.amta-papers.14,0,0.0403179,"slation) so that we can compare different surrogacy modes as different ways of approximate utility maximization. We show experimentally that learning from surrogate “hope” derivations (Chiang, 2012) minimizes regret and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et al., 2006) or “oracle” derivations (Sokolov et al., 2 Algorithm 1 Feedback-based Latent Perceptron or post-edits on the output from similar SMT systems, are used as for online learning (CesaBianchi et al. (2008), L´opez-Salcedo et al. (2012), Mart´ınez-G´omez et al. (2012), Saluja et al. (2012), Saluja and Zhang (2014), inter alia). Recent approaches extend online parameter updating by online phrase extraction (W¨aschle et al. (2013), Bertoldi et al. (2014), Denkowski et al. (2014), Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to be important in online learning for post-editing, in our theoretical analysis (Denkowski et al., 2014). Learning from weak feedback is related to binary response-based learning where a meaning representation is “tried out” by iteratively generating system outputs, receiving feedback from world interaction, and"
K15-1001,D13-1112,0,0.0167655,"ea of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where in"
K15-1001,N04-1023,0,0.0432168,"Joachims (2012). We extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edit"
K15-1001,P12-1002,1,0.832752,"ov et al., 2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations. Online learning has been applied for discriminative training in SMT, based on perceptron-type algorithms (Shen et al. (2004), Watanabe et al. (2006), Liang et al. (2006), Yu et al. (2013), inter alia), or large-margin approaches (Tillmann and Zhang (2006), Watanabe et al. (2007), Chiang et al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012), Watanabe (2012), Green et al. (2013), inter alia). Most approaches rely on hidden derivation variables, use some form of surrogate references, and involve n-best lists that change after each update. Online learning from post-edits has mostly been confined to “simulated post-editing” where independently created human reference translations, • Our third contribution is to show that certain practices of computing surrogate references actually can be understood as a form of weak feedback. Coactive learning decouples the learner (performing prediction and updates) from the user"
K15-1001,2006.amta-papers.25,0,0.0856923,"essarily optimal, object y¯t with respect to a utility function U . The key asset of coactive learning is the ability of the learner to converge to predictions that are close to optimal structures yt∗ , although the utility function is unknown to the learner, and only weak feedback in form of slightly improved structures y¯t is seen in training. We present a proof-of-concept experiment in which translation feedback of varying grades is chosen from the n-best list of an “optimal” model that has access to full information. We show that weak feedback structures correspond to improvements in TER (Snover et al., 2006) over predicted structures, and that learning from weak feedback minimizes regret and TER. 2 Related Work Our work builds on the framework of coactive learning, introduced by Shivaswamy and Joachims (2012). We extend their algorithms and proofs to the area of SMT where latent variable models are appropriate, and additionally present generalization guarantees and an online-to-batch conversion. Our theoretical analysis is easily extendable to the full information case of Sun et al. (2013). We also extend our own previous work (Sokolov et al., 2015) with theory and experiments for online-tobatch"
N09-1009,D08-1092,0,0.027779,"nese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy on two different language"
N09-1009,P05-1022,0,0.00520607,"vation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilisti"
N09-1009,J03-4003,0,0.0616512,"listic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in di"
N09-1009,P91-1017,0,0.132253,"hieving highest performance with T IE V&N. Performance with Chinese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used t"
N09-1009,W02-1009,0,0.0216379,"over the probability simplex. Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN. The prior encodes information about which grammar rules’ weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.1 The contribution of this paper is two-fold. First, from the modeling perspective, we present a generalization of the LN prior of Cohen et al. (2008), showing how to extend the use of the LN prior to 1 Although the task, underlying model, and weights being tied were different, Eisner (2002) also showed evidence for the efficacy of parameter tying in grammar learning. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 74–82, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tie between any grammar weights in a probabilistic grammar (instead of only allowing weights within the same multinomial distribution to covary). Second, from the experimental perspective, we show how such flexibility in parameter tying can help in unsupervised grammar learning in the well-known monolingual setting and in a new b"
N09-1009,P07-1035,0,0.0663304,"ood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multi"
N09-1009,P96-1024,0,0.0217755,"notated data), and report final results on §23. For Chinese, we train on §1–270, use §301–1151 for development and report testing results on §271–300.3 To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus. This performance measure is also known as attachment accuracy. We considered two parsing methods after extracting a point estimate for the grammar: the most probable “Viterbi” parse (argmaxy p(y |x, θ)) and the minimum Bayes risk (MBR) parse (argminy Ep(y0 |x,θ) [`(y; x, y0 )]) with dependency attachment error as the loss function (Goodman, 1996). Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing. 4.1 Nouns, Verbs, and Adjectives In this paper, we use a few simple heuristics to decide which partition structure S to use. Our heuris3 Unsupervised training for these datasets can be costly, and requires iteratively running a cubic-time inside-outside dynamic programming algorithm, so we follow Klein and Manning (2004) in restricting the training set to sentences of ten or fewer words in length. Short sentences are also less structurally ambiguous and may theref"
N09-1009,P08-1088,0,0.0239793,"ayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results. We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data. Acknowledgments This research was supported by NSF IIS-0836431. The authors thank the"
N09-1009,N09-1012,0,0.527987,"ntire tree is given by p(x, y |θ) = P (y(0) |$, θ). The θ are the multinomial distributions θs (· |·, ·, ·) and θc (· |·, ·). To P (y(i) |xi , θ) = Q D∈{left,right} θs (stop × Q j∈yD (i) θs (¬stop |xi , D, [yD (i) = ∅]) (2) |xi , D, firsty (j)) × θc (xj |xi , D) × P (y(j) |xj , θ) Figure 1: The “dependency model with valence” recursive equation. firsty (j) is a predicate defined to be true iff xj is the closest child (on either side) to its parent xi . The probability of the tree p(x, y |θ) = P (y(0) |$, θ). follow the general setting of Eq. 1, we index these distributions as θ 1 , ..., θ K . Headden et al. (2009) extended DMV so that the distributions θc condition on the valence as well, with smoothing, and showed significant improvements for short sentences. Our experiments found that these improvements do not hold on longer sentences. Here we experiment only with DMV, but note that our techniques are also applicable to richer probabilistic grammars like that of Headden et al. 2.2 Learning DMV Klein and Manning (2004) learned the DMV probabilities θ from a corpus of part-of-speech-tagged sentences using the EM algorithm. EM manipulates θ to locally optimize the likelihood of the observed portion of t"
N09-1009,N07-1018,0,0.152845,"Missing"
N09-1009,D07-1031,0,0.10414,"n an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability. Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to"
N09-1009,P04-1061,0,0.718056,"in the well-known monolingual setting and in a new bilingual setting where grammars for two languages are learned at once (without parallel corpora). Our method is based on a distribution which we call the shared logistic normal distribution, which is a distribution over a collection of multinomials from different probability simplexes. We provide a variational EM algorithm for inference. The rest of this paper is organized as follows. In §2, we give a brief explanation of probabilistic grammars and introduce some notation for the specific type of dependency grammar used in this paper, due to Klein and Manning (2004). In §3, we present our model and a variational inference algorithm for it. In §4, we report on experiments for both monolingual settings and a bilingual setting and discuss them. We discuss future work (§5) and conclude in §6. 2 Probabilistic Grammars and Dependency Grammar Induction A probabilistic grammar defines a probability distribution over grammatical derivations generated through a step-by-step process. HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state. Each “step” of the walk and each symbol"
N09-1009,D07-1072,0,0.149029,"ion methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjug"
N09-1009,J93-2004,0,0.0378494,"Missing"
N09-1009,W04-3207,1,0.631421,"st performance with T IE V&N. Performance with Chinese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve un"
N09-1009,P08-1084,0,0.0387932,"E V&N. Performance with Chinese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy"
N09-1009,P06-1124,0,0.0398998,"T IE V, T IE N, T IE V&N, and T IE A. After running the inference algorithm which learns the two models jointly, we use unseen data to test each learned model separately. Table 1 includes the results for these experiments. The performance on English improved significantly in the bilingual setting, achieving highest performance with T IE V&N. Performance with Chinese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train"
N09-1009,D07-1003,1,0.272145,"lgorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametr"
N09-1009,J97-3002,0,0.0962191,"ar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johns"
N09-1009,D08-1109,0,\N,Missing
N10-1081,N09-1009,1,0.0984104,"ng step (described in detail in §3.3) that defines, for each A ∈ M, a list of strings sA = hsA,1 , . . . , sA,NA i. Then, for q(zA,i |φA ) we use the grammaton G(A, sA,i ) and for q(zi |φ) we use the grammaton G(A, xi ) where xi is the ith observed sentence. We parametrize the grammaton with weights φA (or φ) for each rule in the grammaton. This makes the variational distributions over the trees for strings s (and trees for x) globally normalized weighted grammars. Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al., 2008, and Cohen and Smith, 2009). In practice we do not have to use rewrite rules for all strings t ⊆ s in the grammaton. It suffices to add rewrite rules only for the strings t = sA,i that have some grammaton attached q(v, θ, z) = (2) to them, G(A, s ). A,i ! NA n Y Y Y The variational distribution above yields a variq(θ A ) q(vA,i ) × q(zA,i ) × q(zi ) ational inference algorithm for approximating the i=1 i=1 A∈M posterior by estimating γ A,i , τ A , φA and φ itIt is natural to define the variational distributions eratively, given a fixed set of hyperparameters over θ and v to be Dirichlet distributions with pa- a, b and α"
N10-1081,N09-1019,0,0.00801963,"onds; (ii) each iteration took approximately 204 seconds, with convergence after 40 iterations, leading to 8,160 seconds of pure varia6 We used the code and data available at http://www. cog.brown.edu/˜mj/Software.htm. The machine used for this comparison is a 64-bit machine with 2.6GHz CPU, 4MB of cache memory and 8GB of RAM. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to th"
N10-1081,P07-1094,0,0.00950445,"ethods. To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars"
N10-1081,P96-1024,0,0.027574,"gs in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5 The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and Blei (2009), but we did not achieve better performance and it had an adverse effect on runtime. For completeness, we give these results in §4. 569 and minimum Bayes risk decoding (MBR; Goodman, 1996). To parse a string with Viterbi (or MBR) decoding, we find the tree with highest score for the grammaton which is attached to that string. For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction. 4.1 Word Segmentation We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sa"
N10-1081,N09-1012,0,0.0127891,"Missing"
N10-1081,N09-1036,0,0.566168,"lts for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously s"
N10-1081,N07-1018,0,0.0543151,"epresentation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational"
N10-1081,W08-0704,0,0.0142245,"inally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, bu"
N10-1081,P08-1046,0,0.127259,"inally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, bu"
N10-1081,P04-1061,0,0.00705163,"iterations, leading to 8,160 seconds of pure varia6 We used the code and data available at http://www. cog.brown.edu/˜mj/Software.htm. The machine used for this comparison is a 64-bit machine with 2.6GHz CPU, 4MB of cache memory and 8GB of RAM. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to the test set. The nonterminals that we adapted correspond to nonterminals that define nou"
N10-1081,D07-1072,0,0.042418,"inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously slow to converge, especially with complex hidden structures such as syntactic trees. Johnson (2008b) comments on this, and suggests the use of variational inference as a possible remedy. Variational inference provides a deterministic alternative to sampling. It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al. (2007). With NP Bayes models, variational methods are based on the stick-breaking representation (Sethuraman, 1994). Devising a stick-breaking representation is a central challenge to using variational inference in this setting. The rest of this paper is organized as follows. In §2 we describe a stick-breaking representation of adaptor grammars, which enables variational inference (§3) and a well-defined incorporation of recursion into adaptor grammars. In §4 we give an empirical comparison of the algorithm to MCMC inference and describe a novel application of adaptor grammars to unsupervised depend"
N10-1081,J93-2004,0,0.0334897,"of RAM. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to the test set. The nonterminals that we adapted correspond to nonterminals that define noun constituents. We then use the preprocessing step defined in §3.3 with a uniform grammar and take the top 3,000 strings for each nonterminal of a noun constituent. The results are in Table 4.2. We report attachment accuracy, the fra"
N10-1081,P09-1012,0,0.0327989,"he strings generated by the adaptor grammars that yields an accurate variational estimation. We begin with a weighted context-free grammar Gheur that has the same rules as in G, only the weight for all of its rules is 1. We then compute the quantity: ! n 1 X c(A, s) = EGheur [fi (z; A, s)] − ρ log |s| n i=1 (3) where fi (z; A, s) is a function computing the count of constituents headed by A with yield s in the tree z for the sentence xi . This quantity can be computed by using the IO algorithm on Gheur . The term ρ log |s |is subtracted to avoid preference for shorter constituents, similar to Mochihashi et al. (2009). While computing c(A, s) using the IO algorithm, we sort the set of all substrings of s according to their expected counts (aggregated over all strings s). Then, we use the top NA strings in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5 The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and"
N13-1015,H91-1060,0,0.284786,"e a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the F1 measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8, 16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the model on section 22 (development data) at different iteration numbers. Table 1 shows that a peak level of accuracy is reached for all values of m, other than m = 8, at iteration 20–30, with som"
N13-1015,W08-2102,1,0.366224,"Missing"
N13-1015,P05-1022,0,0.193929,"Missing"
N13-1015,P12-1024,1,0.782914,"iments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the data where parameter values are cal"
N13-1015,J03-4003,1,0.155015,"Missing"
N13-1015,D12-1019,1,0.792489,"Missing"
N13-1015,P96-1024,0,0.481709,", 2, . . . n}. 149 The parsing problem is to take a sentence as input, and produce a skeletal tree as output. A standard method for parsing with L-PCFGs is as follows. First, for a given input sentence x1 . . . xn , for any triple (a, i, j) such that a ∈ N and 1 ≤ i ≤ j ≤ n, the marginal µ(a, i, j) is defined as X p(t) (1) µ(a, i, j) = t:(a,i,j)∈t where the sum is over all skeletal trees t for x1 . . . xn that include non-terminal a spanning words xi . . . xj . A variant of the inside-outside algorithm can be used to calculate marginals. Once marginals have been computed, Goodman’s algorithm (Goodman, 1996) is used to find P arg maxt (a,i,j)∈t µ(a, i, j).3 2.2 The Spectral Learning Algorithm We now give a sketch of the spectral learning algorithm. The training data for the algorithm is a set of skeletal trees. The output from the algorithm is a set of parameter estimates for t, q and π (more precisely, the estimates are estimates of linearly transformed parameters; see Cohen et al. (2012) and section 2.3.1 for more details). The algorithm takes two inputs in addition to the set of skeletal trees. The first is an integer m, specifying the number of latent state values in the model. Typically m is"
N13-1015,P12-1046,0,0.0332165,"Missing"
N13-1015,P03-1054,0,0.0369327,"D∗ N. • The two-level and three-level rule fragments above the foot node. In the above example these features would be VP V S NP D∗ NP N VP V NP D∗ N • The label of the foot node, together with the label of its parent. In the above example this is (D, NP). • The label of the foot node, together with the label of its parent and grandparent. In the above example this is (D, NP, VP). • The part of speech of the first head word along the path from the foot of the outside tree to the root of the tree which is different from the head node of 4 We use the English head rules from the Stanford parser (Klein and Manning, 2003). 152 the foot node. In the above example this is N. • The width of the span to the left of the foot node, paired with the label of the foot node. • The width of the span to the right of the foot node, paired with the label of the foot node. Scaling of features. The features defined above are almost all binary valued features. We scale the features in the following way. For each feature φi (t), define count(i) to be the number of times the feature is equal to 1, and M to be the number of training examples. The feature is then redefined to be s M φi (t) × count(i) + κ where κ is a smoothing ter"
N13-1015,E12-1042,0,0.573193,"Missing"
N13-1015,J93-2004,0,0.0490646,"ds to the lowest probability parse being output under the model). We suspect that this is because in some cases a dominant parameter has had its sign flipped due to sampling error; more theoretical and empirical work is required in fully understanding this issue. 4 Experiments In this section we describe parsing experiments using the L-PCFG estimation method. We give comparisons to the EM algorithm, considering both speed of training, and accuracy of the resulting model; we also give experiments investigating the various choices described in the previous section. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 is used as the final test set. We binarize the trees in training data using the same method as that described in Petrov et al. (2006). For example, the non-binary rule VP → V NP PP SBAR would be converted to the structure [VP [@VP [@VP V NP] PP] SBAR] where @VP is a new symbol in the grammar. Unary rules are removed by collapsing non-terminal chains: for example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the init"
N13-1015,D10-1004,0,0.0243085,"example the unary rule S → VP would be replaced by a single non-terminal S|VP. For the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). For efficiency, we use a coarse-to-fine algorithm for parsing with either the EM or spectral derived grammar: a PCFG without latent states is used to calculate marginals, and dynamic programming items are removed if their marginal probability is lower than some threshold (0.00005 in our experiments). For simplicity the parser takes part-of-speech tagged sentences as input. We use automatically tagged data from Turbo Tagger (Martins et al., 2010). The tagger is used to tag both the development data and the test data. The tagger was retrained on sections 2–21. We use the F1 measure according to the Parseval metric (Black et al., 1991). For the spectral algorithm, we tuned the smoothing parameters using section 0 of the treebank. 4.1 Comparison to EM: Accuracy We compare models trained using EM and the spectral algorithm using values for m in {8, 16, 24, 32}.5 For EM, we found that it was important to use development data to choose the number of iterations of training. We train the models for 100 iterations, then test accuracy of the mo"
N13-1015,P05-1010,0,0.673932,"uarantees of sample complexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a"
N13-1015,N07-1051,0,0.036876,"Missing"
N13-1015,P06-1055,0,0.331212,"lexity). This paper describes experiments using the spectral algorithm. We show that the algorithm provides models with the same accuracy as EM, but is an order of magnitude more efficient. We describe a number of key steps used to obtain this level of performance; these should be relevant to other work on the application of spectral learning algorithms. We view our results as strong empirical evidence for the viability of spectral methods as an alternative to EM. 1 Introduction Latent-variable PCFGS (L-PCFGs) are a highly successful model for natural language parsing (Matsuzaki et al., 2005; Petrov et al., 2006). Recent work (Cohen et al., 2012) has introduced a spectral learning algorithm for L-PCFGs. A crucial property of the algorithm is that it is guaranteed to provide consistent parameter estimates—in fact it has PAC-style guarantees of sample complexity.1 This is in contrast to the EM algorithm, the usual method for parameter estimation in L-PCFGs, which has the weaker guarantee of reaching a local maximum of the likelihood function. The spectral algorithm is relatively simple and efficient, relying on a singular value decomposition of the training examples, followed by a single pass over the d"
N13-1052,H91-1060,0,0.485743,"for z. It is often the case that parsing aims to find the highest scoring tree τ ∗ for z according to the underlying PCFG, also called the “Viterbi parse:” τ ∗ = argmax p(τ ) τ ∈T (z) j∈[m],k∈[m] We also let C(1,2) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,2) (y 1 , y 2 )]k = Ci,j,k yi1 yj2 . i∈[m],j∈[m] Finally, we let C(1,3) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,3) (y 1 , y 2 )]j = Ci,j,k yi1 yk2 . i∈[m],k∈[m] Goodman (1996) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the “Labelled Recall Algorithm,” which aims to fix this issue. Goodman’s algorithm has two phases. In the first phase it computes, for each a ∈ N and for each substring xi · · · xj of z, the marginal µ(a, i, j) defined as: µ(a, i, j) = For two vectors x, y ∈ Rm we denote by x y ∈ the Hadamard product of x and y, i.e., [x y]i = xi yi . Finally, for vectors x, y, z ∈ Rm , xy &gt; z &gt; is the Rm 488 X p(τ ). τ ∈T (z) : (a,i,j)∈τ Here we write (a, i, j) ∈ τ if nonterminal a spans words xi · · · xj in the parse tree τ . Inputs: Sentence x1 · · ·"
N13-1052,W03-3005,0,0.0380165,"s of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in Cohen et al. (2012). We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing. We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods. 2 Preliminaries This section int"
N13-1052,N06-1022,0,0.0230182,"ncluding beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for"
N13-1052,P12-1024,1,0.92836,"approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs, based on a tensor formulation developed for latent-variable PCFGs in Cohen et al. (2012). We combine the method with known techniques for tensor decomposition to approximate the source PCFG, and develop a novel algorithm for approximate PCFG parsing. We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing, and provide error upper bounds on the PCFG approximation, in contrast with existing heuristic methods. 2 Preliminaries This section introduces the special representation for probabilistic context-free grammars that we adopt in this paper, along with the decoding algorithm that we investigate. For an integer i ≥ 1, we let [i] = {1, 2, . . . ,"
N13-1052,W05-1504,0,0.0275876,"ee grammars (PCFGs) has received considerable attention in recent years. Several strategies have been proposed, including beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters are then computed on such a structure, and exploited to filter parsing with the non-approximated grammar. The approach can also be iterated at several levels. In the non-probabilistic setting, a similar filtering approach was also proposed by Boullier (2003), called “guided parsing.” In this paper we rely on"
N13-1052,P96-1024,0,0.442285,"Bayes-Risk Decoding Let z = x1 · · · xN be some input sentence; we write T (z) to denote the set of all possible trees for z. It is often the case that parsing aims to find the highest scoring tree τ ∗ for z according to the underlying PCFG, also called the “Viterbi parse:” τ ∗ = argmax p(τ ) τ ∈T (z) j∈[m],k∈[m] We also let C(1,2) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,2) (y 1 , y 2 )]k = Ci,j,k yi1 yj2 . i∈[m],j∈[m] Finally, we let C(1,3) (y 1 , y 2 ) be the m-dimensional column vector with components: X [C(1,3) (y 1 , y 2 )]j = Ci,j,k yi1 yk2 . i∈[m],k∈[m] Goodman (1996) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation (Black et al., 1991). He suggested an alternative algorithm, which he called the “Labelled Recall Algorithm,” which aims to fix this issue. Goodman’s algorithm has two phases. In the first phase it computes, for each a ∈ N and for each substring xi · · · xj of z, the marginal µ(a, i, j) defined as: µ(a, i, j) = For two vectors x, y ∈ Rm we denote by x y ∈ the Hadamard product of x and y, i.e., [x y]i = xi yi . Finally, for vectors x, y, z ∈ Rm , xy &gt; z &gt; is the Rm 488 X p(τ ). τ ∈T (z) : (a,"
N13-1052,P03-1054,0,0.0240128,"up of a factor of 4.75 for Arabic (r = 140) and 6.5 for English (r = 260) while retaining similar performance. Perhaps more surprising is that using the tensor approximation actually improves performance in several cases. We hypothesize that the cause of this is that the tensor decomposition requires less parameters to express the rule probabilities in the grammar, and therefore leads to better generalization than a vanilla maximum likelihood estimate. We include results for a more complex model for Arabic, which uses horizontal Markovization of order 1 and vertical Markovization of order 2 (Klein and Manning, 2003). This grammar includes 2,188 binary rules. Parsing exhaustively using this grammar takes 1.30 seconds per sentence (on average) with an F1 measure of 64.43. Parsing with tensor decomposition for r = 280 takes 0.62 seconds per sentence (on average) with an F1 measure of 64.05. The probability of Pa sentence z under a PCFG is defined as p(z) = τ ∈T (z) p(τ ), and can be approximated using the algorithm in Section 4.3, running in time O(rN 3 + rmN 2 ). Of theoretical interest, we discuss here a time O(rN 3 + r2 N 2 ) algorithm, which is more convenient when r &lt; m. Observe that in Eq. (3) vector"
N13-1052,J93-2004,0,0.0421226,"Missing"
N13-1052,P05-1010,0,0.121189,"s algorithm does not enforce that the output parse trees are included in the tree language of the PCFG, that is, certain combinations of children and parent nonterminals may violate the rules in the grammar. In our experiments we departed from this, and changed Goodman’s algorithm by incorporating the grammar into the dynamic programming algorithm in Figure 1. The reason this is important for our experiments is that we binarize the grammar prior to parsing, and we need to enforce the links between the split nonterminals (in the binarized grammar) that refer to the same syntactic category. See Matsuzaki et al. (2005) for more details about the binarization scheme we used. This step changes the dynamic programming equation of Goodman to be linear in the size of the grammar (figure 1). However, empirically, it is the insideoutside algorithm which takes most of the time to compute with Goodman’s algorithm. In this paper we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm. 3 Tensor Formulation of the Inside-Outside Algorithm At the core of our approach lies the observation that there is a (multi)linear algebraic formulatio"
N13-1052,J00-1003,0,0.0675288,"mproves time complexity with respect to the input grammar size, and prove upper bounds on the approximation quality. We test our algorithm on two treebanks, and get significant improvements in parsing speed. 1 Introduction The problem of speeding-up parsing algorithms based on probabilistic context-free grammars (PCFGs) has received considerable attention in recent years. Several strategies have been proposed, including beam-search, best-first and A∗ . In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency. Nederhof (2000) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata, on the basis of specialized preprocessing of selfembedding structures. In the probabilistic domain, approximation by means of regular grammars is also exploited by Eisner and Smith (2005), who filter long-distance dependencies on-the-fly. Beyond finite automata approximation, Charniak et al. (2006) propose a coarse-to-fine approach in which an approximated (not necessarily regular) PCFG is used to construct a parse forest for the input sentence. Some statistical parameters"
N13-4005,E12-1042,0,\N,Missing
N13-4005,J92-4003,0,\N,Missing
N13-4005,P06-1055,0,\N,Missing
N13-4005,P05-1010,0,\N,Missing
N13-4005,W99-0613,1,\N,Missing
N13-4005,D12-1019,1,\N,Missing
N13-4005,N13-1015,1,\N,Missing
N13-4005,W97-0309,0,\N,Missing
N13-4005,P96-1024,0,\N,Missing
N15-1122,D13-1178,0,0.0871705,"Missing"
N15-1122,J08-1001,0,0.0391789,"ss the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate planning and reasoning scenarios (see §3), and is useful in itself for tasks such as conceptto-text generation (Reiter et al., 2000), or in validating the correctness of instruction sets. A related idea can be found in modeling sentence coherence (Lapata, 2003; Barzilay and Lapata, 2008, inter alia), although here we focus on lexical relations between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the lates"
N15-1122,J92-4003,0,0.0957719,"rstorder Markovian and use the same (greedy) inference procedure. Lapata’s model differs from G REEDY-L OG L IN in being a generative model, where each event is a tuple of features, and the transition probability between events is defined as the product of transition probabilities between feature pairs. G REEDY-L OG L IN is discriminative, so to be maximally comparable to the presented model. 5 The Feature Set Table 1 presents the complete set of features. We consider three sets of features: Lexical encodes the written forms of the event pair predicates and objects; Brown uses Brown clusters (Brown et al., 1992) to encode similar information, but allows generalization between distributionally similar words; and Frequency encodes the empirical distribution of temporally-related phenomena. The feature definitions make use of several functions. For brevity, we sometimes say that an event e is (a, c1 ) if e’s predicate is a and its first argument is c1 , disregarding its other arguments. Let C be a reference corpus of recipes for collecting statistics. The function B(w) gives the Brown cluster of a word w, as determined by clustering C into 50 clusters {1, . . . , 50}. The function ORD(a, c) returns the"
N15-1122,P14-2082,0,0.0379789,"Missing"
N15-1122,D08-1073,0,0.203506,"h of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously difficult, and evaluation is often"
N15-1122,P08-1090,0,0.373477,"h of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously difficult, and evaluation is often"
N15-1122,P09-1068,0,0.176291,"Missing"
N15-1122,W04-3205,0,0.0471795,", given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously diffic"
N15-1122,W13-2102,0,0.0613741,"Missing"
N15-1122,W02-1001,0,0.0254305,"Missing"
N15-1122,P07-1030,0,0.0179481,"Missing"
N15-1122,W08-1301,0,0.0143165,"Missing"
N15-1122,N13-1112,0,0.0503299,"Missing"
N15-1122,E14-1006,0,0.211782,"Missing"
N15-1122,E12-1034,0,0.229035,"Missing"
N15-1122,P06-1095,0,0.280094,"s in a domain where this order is aligned with their temporal order, namely cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predic"
N15-1122,P03-1069,0,0.236224,"paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate planning and reasoning scenarios (see §3), and is useful in itself for tasks such as conceptto-text generation (Reiter et al., 2000), or in validating the correctness of instruction sets. A related idea can be found in modeling sentence coherence (Lapata, 2003; Barzilay and Lapata, 2008, inter alia), although here we focus on lexical relations between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annot"
N15-1122,J06-4002,0,0.0246016,".edu/software/corenlp.shtml Links to the original recipes, the preprocessed recipes and all extracted events can be found in http://homepages. inf.ed.ac.uk/oabend/event_order.html. 1166 This did not result from an extraction problem, but rather from the recipe text itself being too noisy to interpret. 7 Events are parsed manually so to avoid confounding the results with the parser’s performance. of recipes does not suggest that the textual order is the only order of events that would yield the same outcome. We compute the Kendall’s Tau correlation, a standard measure for information ordering (Lapata, 2006), between the temporal and linear orderings for each recipe. In cases of several events that happen simultaneously (including disjunctions), we take their ordinals to be equal. For instance, for three events where the last two happen at the same time, we take their ordering to be (1,2,2) in our analysis. We find that indeed temporal and textual orderings are in very high agreement, with 6 recipes of the 19 perfectly aligned. The average Kendall’s Tau between the temporal ordering and the linear one is 0.924. 7 Experimental Setup Evaluation. We compute the accuracy of our algorithms by comparin"
N15-1122,W14-2407,0,0.0803724,"Missing"
N15-1122,P05-1012,0,0.0307215,"Missing"
N15-1122,E14-1033,0,0.0122346,"cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate p"
N15-1122,W14-1606,0,0.176512,"Missing"
N15-1122,E14-1024,0,0.163803,"Missing"
N15-1122,P09-2004,0,0.0384851,"Missing"
N15-1122,P10-1100,0,0.312406,"Missing"
N15-1122,Q13-1003,0,0.0758771,"Missing"
N15-1122,D13-1177,0,0.172065,"Missing"
N15-1122,D09-1105,0,0.0757134,"Missing"
N15-1122,S13-2001,0,0.0440954,"ons between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions,"
N15-1122,P09-1046,0,0.0272091,"r is aligned with their temporal order, namely cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of"
N18-1041,D15-1198,0,0.0222556,"fiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambiguity (ability to full disambiguate text into the representation in a way that does not l"
N18-1041,J10-4006,0,0.010306,", this goal is not entirely achieved in practice, and it will take long for AMR parsers to mature and achieve such canonicalization. At the moment, for example, even a simple pair of sentences such as “the boy desires the cake” and the “the boy wants the cake” would not have the same canonical form by state-of-the-art AMR parsers. While some researchers (Fodor, 1975) have doubted the practical possibility of canonicalizing language or finding identical paraphrases in English or otherwise, much work in NLP has been devoted to the problem of paraphrase identification (Mitchell and Lapata, 2010; Baroni and Lenci, 2010; Socher et al., 2011; Guo and Diab, 2012; Ji and Eisenstein, 2013) and more weakly, finding entailment between sentences and phrases (Dagan et al., 2006; Bos and Markert, 2005; Harabagiu and Hickl, 2006; Lewis and Steedman, 2013). In this work, we use the AMRs parsed for given sentences as a mean to extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds"
N18-1041,S16-1176,0,0.0122652,". 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambiguity (ability to full disambiguate text into the representation in a way that does not leave any ambiguity lingeri"
N18-1041,P14-1133,0,0.0674327,"Missing"
N18-1041,H05-1079,0,0.082123,"ir of sentences such as “the boy desires the cake” and the “the boy wants the cake” would not have the same canonical form by state-of-the-art AMR parsers. While some researchers (Fodor, 1975) have doubted the practical possibility of canonicalizing language or finding identical paraphrases in English or otherwise, much work in NLP has been devoted to the problem of paraphrase identification (Mitchell and Lapata, 2010; Baroni and Lenci, 2010; Socher et al., 2011; Guo and Diab, 2012; Ji and Eisenstein, 2013) and more weakly, finding entailment between sentences and phrases (Dagan et al., 2006; Bos and Markert, 2005; Harabagiu and Hickl, 2006; Lewis and Steedman, 2013). In this work, we use the AMRs parsed for given sentences as a mean to extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several pa"
N18-1041,P13-2131,0,0.1817,"Missing"
N18-1041,N15-1114,0,0.0928581,"y means of PageRank (Page et al., 1999). The utility of re-weighting terms in the sentence-term matrix has been previously proved (Turney and Pantel, 2010). PageRank is a method, originally developed for web pages, for ranking nodes in a graph according to their impact on other nodes. The algorithm works iteratively by adjusting at each iteration the score of each node based on the number and scores of nearby nodes that is connected to it, until convergence. Prior to applying PageRank, we merge the two graphs by collapsing the concepts in the two graphs that have the same labels, similarly to Liu et al. (2015), as shown in Figure 2. We then compute the PageRank score for each node in the merged graph and multiply them by the corresponding frequency count of that concept in the sentence-term matrix. The graph merging step is necessary in order to ensure that overlapping concepts obtain high PageRank scores. The PageRank step applied to the merged graph ensures that this importance propagates to nearby nodes. For a given graph G = (V, E), PageRank takes as input a list of edges between nodes: 6.1 Experiments Graph Similarity and Bag of AMR Concepts The Bag of words (BOW) baseline consists of a SVM th"
N18-1041,N18-1104,1,0.788777,"sition-based parser that works by scanning the string left-to-right and building the graph as the scan proceeds. This transition-based system is akin to the dependency parsing transition-system ArcEager of Nivre (2004), only without constraints that ensure that the resulting structure is a tree. In addition, there are operations that make the system create additional non-projective structures by checking after transition step whether siblings should be connected together with an edge. The complexity of AMREager is linear in the length of the sentence. AMREager was extended to other languages (Damonte and Cohen, 2018), and we leave it for future work to test the utility of AMR for paraphrase detection in these languages. 3 where n is the number of training examples, xj ∈ S, j ∈ {1, 2} and b(i) ∈ {0, 1} is a binary indica(i) (i) tor that tells whether x1 is a paraphrase of x2 . The goal is to learn a classifier c : S × S → {0, 1} that tells for unseen instances whether the pair of sentences given as input are paraphrases of each other. We denote by [n] the set {1, . . . , n}. 4 Latent Semantic Analysis for Paraphrase Detection The first step in our approach is the construction of lower-dimensional represent"
N18-1041,P14-5010,0,0.00365335,"rted to an AMR graph (b). Table 1: Baseline results for paraphrase detection with AMR and with bag-of-words (BOW). “linear,” “poly” and “rbf” denote the kernel which is used with a support vector machine classifier. “Smatch” denotes the use of the additional graph similarity feature and “BOC” the use of the additional Jaccard score on the bag of concept. Best result in each column is in bold. to AMR graphs, the same code is used to run the experiments as in the case of AMR parsing, with both the PageRank and TF-IDF reweighting settings. We used the dependency parser from the Stanford CoreNLP (Manning et al., 2014). The results are given in Table 3, under “dep.” As can be seen, these results lag behind the bag-of-words model in the inductive case and the AMR models in the transductive case. This could be attributed to AMR parsers better abstracting away from the surface form than dependency parsers. tive case, the AMRs provided by JAMR are helpful with both TF-IDF and PageRank, while the graphs provided by AMREager give good results only for the PageRank scheme. The best result is achieved with JAMR, PageRank and a linear kernel for the SVM classifier. We wanted to test in our experiments whether the sa"
N18-1041,E17-1051,1,0.933668,"of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambiguity (ability to full disambiguate text into the representation in a way that does not leave any ambiguity lingering), inference (the existence of a calculu"
N18-1041,S16-1166,0,0.0146074,"his task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambiguity (ability to full disambiguate text into the representation in a way that does not leave any ambiguity lingering), inference (the existence of a calculus that can be used to Latent Semantic Analysis Our work falls under the category of distributional methods for paraphrase detection (Turney a"
N18-1041,C04-1051,0,0.331141,"Missing"
N18-1041,S17-2090,0,0.0115356,"beds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambiguity (ability to full disambiguate text into the representation in a way that does not leave any ambiguity lingering), inference (the existence of a calculus that can be used to Latent Semantic Analysis Our work falls under the category of distributional methods for paraphrase detection (Turney and Pantel, 2010; Mihalcea et"
N18-1041,S16-1186,0,0.0434506,"Missing"
N18-1041,P14-1134,0,0.248013,"man, 2013). In this work, we use the AMRs parsed for given sentences as a mean to extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning represe"
N18-1041,W04-0308,0,0.0529025,"gments corresponding to span of words in the sentence, while the latter finds the optimal spanning connected subgraph from the concepts identified in the first step. The concept identification step has quadratic complexity and the relation identification step is O(|V |2 log |V |), with |V |being the set of nodes in the AMR graph. The second is AMREager (Damonte et al., 2017), which is a transition-based parser that works by scanning the string left-to-right and building the graph as the scan proceeds. This transition-based system is akin to the dependency parsing transition-system ArcEager of Nivre (2004), only without constraints that ensure that the resulting structure is a tree. In addition, there are operations that make the system create additional non-projective structures by checking after transition step whether siblings should be connected together with an edge. The complexity of AMREager is linear in the length of the sentence. AMREager was extended to other languages (Damonte and Cohen, 2018), and we leave it for future work to test the utility of AMR for paraphrase detection in these languages. 3 where n is the number of training examples, xj ∈ S, j ∈ {1, 2} and b(i) ∈ {0, 1} is a"
N18-1041,P16-1001,0,0.0118327,"o extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambiguity (ability to full di"
N18-1041,K15-1004,0,0.0377128,"parsed for given sentences as a mean to extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of th"
N18-1041,P12-1091,0,0.438795,"f her: curmudgeon, 3. She was a curmudgeon, according to his description. 442 Proceedings of NAACL-HLT 2018, pages 442–452 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics analysis (LSA, Landauer et al., 1998). The main principle behind this approach is to detect semantic similarity through distributional representations for a given sentence and its potential paraphrase, where these representations are compared against each other according to some similarity metric or used as features with a discriminative classification method (Mihalcea et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013). LSA is indeed one of the main tools in obtaining such distributional representations for the problem of paraphrase detection. Most often, TFIDF weighting has been used for building the sentence-term matrix, but Ji and Eisenstein (2013) have shown that a significant improvement can be achieved in detecting similarity if one re-weights the sentence-term matrix differently. Indeed, this is one of our main contributions: we build on previous work on LSA for paraphrase detection and propose a technique to re-weight a sentenceconcept matrix based on the AMR graphs for the"
N18-1041,D15-1136,0,0.0276564,"ntences as a mean to extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambigui"
N18-1041,P06-1114,0,0.0492562,"“the boy desires the cake” and the “the boy wants the cake” would not have the same canonical form by state-of-the-art AMR parsers. While some researchers (Fodor, 1975) have doubted the practical possibility of canonicalizing language or finding identical paraphrases in English or otherwise, much work in NLP has been devoted to the problem of paraphrase identification (Mitchell and Lapata, 2010; Baroni and Lenci, 2010; Socher et al., 2011; Guo and Diab, 2012; Ji and Eisenstein, 2013) and more weakly, finding entailment between sentences and phrases (Dagan et al., 2006; Bos and Markert, 2005; Harabagiu and Hickl, 2006; Lewis and Steedman, 2013). In this work, we use the AMRs parsed for given sentences as a mean to extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been rec"
N18-1041,D13-1090,0,0.123742,". She was a curmudgeon, according to his description. 442 Proceedings of NAACL-HLT 2018, pages 442–452 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics analysis (LSA, Landauer et al., 1998). The main principle behind this approach is to detect semantic similarity through distributional representations for a given sentence and its potential paraphrase, where these representations are compared against each other according to some similarity metric or used as features with a discriminative classification method (Mihalcea et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013). LSA is indeed one of the main tools in obtaining such distributional representations for the problem of paraphrase detection. Most often, TFIDF weighting has been used for building the sentence-term matrix, but Ji and Eisenstein (2013) have shown that a significant improvement can be achieved in detecting similarity if one re-weights the sentence-term matrix differently. Indeed, this is one of our main contributions: we build on previous work on LSA for paraphrase detection and propose a technique to re-weight a sentenceconcept matrix based on the AMR graphs for the given sentences. More det"
N18-1041,kingsbury-palmer-2002-treebank,0,0.453796,"tection appear in Section 4. as a further benchmark for AMR parsers, highlighting their ability of abstracting away from syntax and representing the core concepts expressed in the sentence. In order to advance research in AMR and its applications, it is important to have metrics that reflect on the ability of AMR graphs to have impact on subsequent tasks. In this work we therefore use two different AMR parsers, comparing them throughout all experiments. 2 Background AMRs are rooted, edge labeled, node labeled, directed graphs. They are biased towards the English language and rely on PropBank (Kingsbury and Palmer, 2002) for the definition of the main events in the sentence. Nodes in an AMR graph represent events and concepts, while edges represent the relationships between them. Banarescu et al. (2013) state that AMR are aimed at canonicalizing multiple ways of expressing the same idea, which could be of great assistance to solve the problem of paraphrase detection. However, this goal is not entirely achieved in practice, and it will take long for AMR parsers to mature and achieve such canonicalization. At the moment, for example, even a simple pair of sentences such as “the boy desires the cake” and the “th"
N18-1041,D11-1014,0,0.00728775,"ely achieved in practice, and it will take long for AMR parsers to mature and achieve such canonicalization. At the moment, for example, even a simple pair of sentences such as “the boy desires the cake” and the “the boy wants the cake” would not have the same canonical form by state-of-the-art AMR parsers. While some researchers (Fodor, 1975) have doubted the practical possibility of canonicalizing language or finding identical paraphrases in English or otherwise, much work in NLP has been devoted to the problem of paraphrase identification (Mitchell and Lapata, 2010; Baroni and Lenci, 2010; Socher et al., 2011; Guo and Diab, 2012; Ji and Eisenstein, 2013) and more weakly, finding entailment between sentences and phrases (Dagan et al., 2006; Bos and Markert, 2005; Harabagiu and Hickl, 2006; Lewis and Steedman, 2013). In this work, we use the AMRs parsed for given sentences as a mean to extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP pr"
N18-1041,P17-1014,0,0.0966477,"graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambiguity (ability to full disambiguate text into the representation in a way that does not leave any ambiguity lingering), inference (the existence of a calculus that can be used to Latent Semantic Analysis Our"
N18-1041,P15-2141,0,0.0328927,"k, we use the AMRs parsed for given sentences as a mean to extract useful information and train paraphrase detection classifiers on top of them. 2.1 2.2 AMR Parsing AMR parsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in"
N18-1041,P15-1095,0,0.0208416,"ional feature for the SVM. The amount of overlap in the AMR nodes of the two graphs can be a good indicator of whether the sentences are paraphrases of each other. To test this hypothesis, we extract the unordered sets of AMR nodes and use the Jaccard similarity coefficient as a feature. This is directly related to the concept identification step of the AMR parsing process, which is concerned with generating and labeling the nodes of the AMR graph. Concept identification is arguably one of the most challenging part of AMR parsing as the mapping between word spans and AMR nodes is not trivial (Werling et al., 2015). It is often considered as the first stage in the AMR parsing pipeline and it is therefore reasonable to attempt using its intermediate results. We choose Jaccard as a metric for bag of concepts overlap following previous work in paraphrase detection (Achananuparp et al., 2008; BeT ≈ U ΣV &gt; where U ∈ Rk×m , V ∈ R`×m and Σ ∈ Rm×m is a diagonal matrix of singular values. The final sentence representations are the rows of the U matrix which range over the sentences and have m dimensions. The output of this process is a function f : S → Rm which attaches to each sentence a representation. The ide"
N18-1041,D16-1065,0,0.0119707,"arsing is the task of converting natural language sentences into AMR graphs, which are Directed Acyclic Graphs (DAGs) in all cases except a few rare controversial cases. This task embeds several common NLP problems together, such as named entity recognition, sentential-level coreference resolution, semantic role labeling and wordsense disambiguation. Several parsers for AMR have been recently developed (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Pust et al., 2015; Goodman et al., 2016; Rao et al., 2015; Vanderwende et al., 2015; Artzi et al., 2015; Barzdins and Gosko, 2016a; Zhou et al., 2016; Damonte et al., 2017; Barzdins and Gosko, 2016b; Konstas et al., 2017). Shared tasks were also organized in order to push forward the state-of-the-art (May, 2016; May and Priyadarshi, 2017). Meaning representations are usually evaluated based on their compositionality (construction of a representation based on parts of the text in a consistent way), verifiability (ability to check whether a meaning representation is true in a given model of the world), unambiguity (ability to full disambiguate text into the representation in a way that does not leave any ambiguity lingering), inference (the"
N18-1104,P15-1039,0,0.08331,"R (Xue et al., 2014; Bojar, 2014), UCCA (Sulem et al., 2015) and Propbank (Van der Plas et al., 2010). Cross-lingual techniques can cope with the lack of labeled data on languages when this data is available in at least one language, usually English. The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis (Yarowsky et al., 2001) but it has also been used for dependency parsing (Hwa et al., 2005), role labeling (Pad´o and Lapata, 2009; Akbik et al., 2015) and semantic parsing (Evang and Bos, 2016). Another common thread of cross-lingual work is model transfer, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen and Smith, 2009; Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). 7 work as it uses a projection mechanism similar to ours for CCG. A crucial difference is that, in order to project CCG parse trees to the target languages, they only make use of literal translation. Previous work has also focused on assessing the stabilConclusions We introduced the problem of parsing AMR structures, annotated for English"
N18-1104,P13-2131,0,0.401077,"Missing"
N18-1104,carreras-etal-2004-freeling,0,0.158374,"Missing"
N18-1104,D11-1005,1,0.824448,"one language, usually English. The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis (Yarowsky et al., 2001) but it has also been used for dependency parsing (Hwa et al., 2005), role labeling (Pad´o and Lapata, 2009; Akbik et al., 2015) and semantic parsing (Evang and Bos, 2016). Another common thread of cross-lingual work is model transfer, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen and Smith, 2009; Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). 7 work as it uses a projection mechanism similar to ours for CCG. A crucial difference is that, in order to project CCG parse trees to the target languages, they only make use of literal translation. Previous work has also focused on assessing the stabilConclusions We introduced the problem of parsing AMR structures, annotated for English, from sentences written in other languages as a way to test the crosslingual properties of AMR. We provided evidence that AMR can be indeed shared across the lan1153 guages tested and that it is possible to overcome tr"
N18-1104,N09-1009,1,0.750714,"available in at least one language, usually English. The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis (Yarowsky et al., 2001) but it has also been used for dependency parsing (Hwa et al., 2005), role labeling (Pad´o and Lapata, 2009; Akbik et al., 2015) and semantic parsing (Evang and Bos, 2016). Another common thread of cross-lingual work is model transfer, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen and Smith, 2009; Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). 7 work as it uses a projection mechanism similar to ours for CCG. A crucial difference is that, in order to project CCG parse trees to the target languages, they only make use of literal translation. Previous work has also focused on assessing the stabilConclusions We introduced the problem of parsing AMR structures, annotated for English, from sentences written in other languages as a way to test the crosslingual properties of AMR. We provided evidence that AMR can be indeed shared across the lan1153 guages tested and that it is pos"
N18-1104,E17-1051,1,0.840978,"h). These are used to train the AMR parsers. The projection approach also requires training the word alignments, for which we use all the remaining sentences from the parallel corpora (Europarl for Spanish/German/Italian and UN Parallel Corpus for Chinese). These are also the sentences we use to train the MT models. The gold AMR dataset is LDC2015E86, containing 16,833 training sentences, 1,368 development sentences, and 1,371 testing sentences. Word alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR (Flanigan et al., 2014). AMREager (Damonte et al., 2017) was chosen as the pre-existing English AMR parser. AMREager is an open-source AMR parser that 1 These datasets are currently available upon request from the authors. 1148 Silver f Gold e Ref FULL-CYCLE Parser e Eval Parser f Eval Ref SILVER Eval Parser e Gold f Ref GOLD Figure 2: Description of SILVER, FULL-CYCLE and GOLD evaluations. e stands for English and f stands for the target (foreign) language. Dashed lines represent the process of transferring learning across languages (e.g. with annotation projection). SILVER uses a parsed parallel corpus as reference (“Ref”), FULL-CYCLE uses the En"
N18-1104,J94-4004,0,0.779951,"Missing"
N18-1104,N13-1073,0,0.0396081,"v/test) sentences for the two step of the annotation projection (English → target and target → English). These are used to train the AMR parsers. The projection approach also requires training the word alignments, for which we use all the remaining sentences from the parallel corpora (Europarl for Spanish/German/Italian and UN Parallel Corpus for Chinese). These are also the sentences we use to train the MT models. The gold AMR dataset is LDC2015E86, containing 16,833 training sentences, 1,368 development sentences, and 1,371 testing sentences. Word alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR (Flanigan et al., 2014). AMREager (Damonte et al., 2017) was chosen as the pre-existing English AMR parser. AMREager is an open-source AMR parser that 1 These datasets are currently available upon request from the authors. 1148 Silver f Gold e Ref FULL-CYCLE Parser e Eval Parser f Eval Ref SILVER Eval Parser e Gold f Ref GOLD Figure 2: Description of SILVER, FULL-CYCLE and GOLD evaluations. e stands for English and f stands for the target (foreign) language. Dashed lines represent the process of transferring learning across languages (e.g. with a"
N18-1104,C16-1056,0,0.416786,"suffice to resolve some of these cases. We extend this line of research by exploring whether divergences among languages can be overcome, i.e., we investigate whether it is possible to maintain the AMR annotated for English as a semantic representation for sentences written in other languages, as in Figure 1. We implement AMR parsers for Italian, Spanish, German and Chinese using annotation projection, where existing annotations are projected from a source language (English) to a target language through a parallel corpus (e.g., Yarowsky et al., 2001; Hwa et al., 2005; Pad´o and Lapata, 2009; Evang and Bos, 2016). By evaluating the parsers and manually analyzing their output, we show that the parsers are able to recover the AMR structures even when there exist structural differences between the languages, i.e., although AMR is not an interlingua it can act as one. This method also provides a quick way to prototype multilingual AMR parsers, assuming that Part-of-speech (POS) taggers, Named Entity Recognition (NER) taggers and dependency parsers are available for the target languages. We also propose an alternative approach, where Machine Translation (MT) is used to translate the input sentences into En"
N18-1104,P14-1134,0,0.16312,"glish → target and target → English). These are used to train the AMR parsers. The projection approach also requires training the word alignments, for which we use all the remaining sentences from the parallel corpora (Europarl for Spanish/German/Italian and UN Parallel Corpus for Chinese). These are also the sentences we use to train the MT models. The gold AMR dataset is LDC2015E86, containing 16,833 training sentences, 1,368 development sentences, and 1,371 testing sentences. Word alignments were generated using fast align (Dyer et al., 2013), while AMR alignments were generated with JAMR (Flanigan et al., 2014). AMREager (Damonte et al., 2017) was chosen as the pre-existing English AMR parser. AMREager is an open-source AMR parser that 1 These datasets are currently available upon request from the authors. 1148 Silver f Gold e Ref FULL-CYCLE Parser e Eval Parser f Eval Ref SILVER Eval Parser e Gold f Ref GOLD Figure 2: Description of SILVER, FULL-CYCLE and GOLD evaluations. e stands for English and f stands for the target (foreign) language. Dashed lines represent the process of transferring learning across languages (e.g. with annotation projection). SILVER uses a parsed parallel corpus as referenc"
N18-1104,W09-1205,0,0.0336805,"dge, the only previous work that attempts to automatically parse AMR graphs for non-English sentences is by Vanderwende et al. (2015). Sentences in several languages (French, German, Spanish and Japanese) are parsed into a logical representation, which is then converted to AMR using a small set of rules. A comparison with this work is difficult, as the authors do not report results for the parsers (due to the lack of an annotated corpus) or release their code. Besides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkov´a et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016). Evang and Bos (2016) is the most closely related to our 1152 envy answer-01 :domain enter-01 :ARG1 :ARG0 I we home (a) ES: Tengo envidia de ti (b) IT: Noi daremo una risposta (c) IT: Lui e` entrato nella casa (I am jealous of you) (We will answer) (He entered the house) eat-01 fear-01 like-01 :ARG0 :ARG0 I :ARG0 I :ARG1 :ARG0 I I grape :ARG0 (d) DE: Ich esse gern (e) IT: Io ho paura (f) ES: Me gustan uvas (I like eating) (I fear) (I like grapes) Figure 4: Parsing examples in several languages involving common translational divergence phenomena: (a) contains a categorica"
N18-1104,P92-1044,0,0.65565,"ng any word. To the best of our knowledge, the only previous work that attempts to automatically parse AMR graphs for non-English sentences is by Vanderwende et al. (2015). Sentences in several languages (French, German, Spanish and Japanese) are parsed into a logical representation, which is then converted to AMR using a small set of rules. A comparison with this work is difficult, as the authors do not report results for the parsers (due to the lack of an annotated corpus) or release their code. Besides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkov´a et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016). Evang and Bos (2016) is the most closely related to our 1152 envy answer-01 :domain enter-01 :ARG1 :ARG0 I we home (a) ES: Tengo envidia de ti (b) IT: Noi daremo una risposta (c) IT: Lui e` entrato nella casa (I am jealous of you) (We will answer) (He entered the house) eat-01 fear-01 like-01 :ARG0 :ARG0 I :ARG0 I :ARG1 :ARG0 I I grape :ARG0 (d) DE: Ich esse gern (e) IT: Io ho paura (f) ES: Me gustan uvas (I like eating) (I fear) (I like grapes) Figure 4: Parsing examples in several languages involving common translational d"
N18-1104,kingsbury-palmer-2002-treebank,0,0.599036,"ed across languages. • We propose two ways to rapidly implement non-English AMR parsers. • We propose a novel method to evaluate nonEnglish AMR parsers when gold annotations in the target languages are missing. This method highly correlates with gold standard evaluation, obtaining a Pearson correlation coefficient of 0.95. • We release human translations of an AMR dataset (LDC2015E86) to Italian, Spanish, German and Chinese. 2 Cross-lingual AMR parsing AMR is a semantic representation heavily biased towards English, where labels for nodes and edges are either English words or Propbank frames (Kingsbury and Palmer, 2002). The goal of AMR is to abstract away from the syntactic realization of the original sentences while maintaining its underlying meaning. As a consequence, different phrasings of one sentence are expected to provide identical AMR representations. This canonicalization does not always hold across languages: two sentences that express the same meaning in two different languages are not guaranteed to produce identical AMR structures (Bojar, 2014; Xue et al., 2014). However, Xue et al. (2014) show that in many cases the unlabeled AMRs are in fact shared across languages. We are encouraged by this f"
N18-1104,2005.mtsummit-papers.11,0,0.122619,"AMR graphs. A diagram summarizing the different evaluation stages is shown in Figure 2. In the case of MTbased systems, the full-cycle corresponds to first translating from English to the target language and then back to English (back-translation), and only then parsing the sentences with the English AMR parser. At the end of this process, a noisy version of the original sentence will be returned and its parsed graph will be a noisy version of the graph parsed from the original sentence. 3 Experiments We run experiments on four languages: Italian, Spanish, German and Chinese. We use Europarl (Koehn, 2005) as the parallel corpus for Italian, Spanish and German, containing around 1.9M sentences for each language pair. For Chinese, we use the first 2M sentences from the United Nations Parallel Corpus (Ziemski et al., 2016). For each target language we extract two parallel datasets of 20,000/2,000/2,000 (train/dev/test) sentences for the two step of the annotation projection (English → target and target → English). These are used to train the AMR parsers. The projection approach also requires training the word alignments, for which we use all the remaining sentences from the parallel corpora (Euro"
N18-1104,P07-2045,0,0.00546664,"Missing"
N18-1104,L16-1561,0,0.0232861,"en back to English (back-translation), and only then parsing the sentences with the English AMR parser. At the end of this process, a noisy version of the original sentence will be returned and its parsed graph will be a noisy version of the graph parsed from the original sentence. 3 Experiments We run experiments on four languages: Italian, Spanish, German and Chinese. We use Europarl (Koehn, 2005) as the parallel corpus for Italian, Spanish and German, containing around 1.9M sentences for each language pair. For Chinese, we use the first 2M sentences from the United Nations Parallel Corpus (Ziemski et al., 2016). For each target language we extract two parallel datasets of 20,000/2,000/2,000 (train/dev/test) sentences for the two step of the annotation projection (English → target and target → English). These are used to train the AMR parsers. The projection approach also requires training the word alignments, for which we use all the remaining sentences from the parallel corpora (Europarl for Spanish/German/Italian and UN Parallel Corpus for Chinese). These are also the sentences we use to train the MT models. The gold AMR dataset is LDC2015E86, containing 16,833 training sentences, 1,368 developmen"
N18-1104,P14-5010,0,0.00947405,"Missing"
N18-1104,D11-1006,0,0.0524414,"y English. The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis (Yarowsky et al., 2001) but it has also been used for dependency parsing (Hwa et al., 2005), role labeling (Pad´o and Lapata, 2009; Akbik et al., 2015) and semantic parsing (Evang and Bos, 2016). Another common thread of cross-lingual work is model transfer, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen and Smith, 2009; Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). 7 work as it uses a projection mechanism similar to ours for CCG. A crucial difference is that, in order to project CCG parse trees to the target languages, they only make use of literal translation. Previous work has also focused on assessing the stabilConclusions We introduced the problem of parsing AMR structures, annotated for English, from sentences written in other languages as a way to test the crosslingual properties of AMR. We provided evidence that AMR can be indeed shared across the lan1153 guages tested and that it is possible to overcome translational divergences"
N18-1104,E17-3017,0,0.0366375,"Missing"
N18-1104,P11-2120,0,0.0280601,"on projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis (Yarowsky et al., 2001) but it has also been used for dependency parsing (Hwa et al., 2005), role labeling (Pad´o and Lapata, 2009; Akbik et al., 2015) and semantic parsing (Evang and Bos, 2016). Another common thread of cross-lingual work is model transfer, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen and Smith, 2009; Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). 7 work as it uses a projection mechanism similar to ours for CCG. A crucial difference is that, in order to project CCG parse trees to the target languages, they only make use of literal translation. Previous work has also focused on assessing the stabilConclusions We introduced the problem of parsing AMR structures, annotated for English, from sentences written in other languages as a way to test the crosslingual properties of AMR. We provided evidence that AMR can be indeed shared across the lan1153 guages tested and that it is possible to overcome translational divergences. We further pro"
N18-1104,W15-3502,0,0.0437433,") IT: Io ho paura (f) ES: Me gustan uvas (I like eating) (I fear) (I like grapes) Figure 4: Parsing examples in several languages involving common translational divergence phenomena: (a) contains a categorical divergence, (b) and (e) conflational divergences, (c) a structural divergence, (d) an head swapping and (f) a thematic divergence. 70 Silver Smatch 60 50 40 40 50 60 Gold Smatch 70 40 50 60 Gold Smatch 70 70 Full-cycle Smatch 60 50 40 Figure 5: Linear regression lines for silver and fullcycle. ity across languages of semantic frameworks such as AMR (Xue et al., 2014; Bojar, 2014), UCCA (Sulem et al., 2015) and Propbank (Van der Plas et al., 2010). Cross-lingual techniques can cope with the lack of labeled data on languages when this data is available in at least one language, usually English. The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis (Yarowsky et al., 2001) but it has also been used for dependency parsing (Hwa et al., 2005), role labeling (Pad´o and Lapata, 2009; Akbik et al., 2015) and semantic parsing (Evang and Bos, 201"
N18-1104,W10-1814,0,0.0721398,"Missing"
N18-1104,N15-3006,0,0.0625015,"s additional noise but it is not as expensive as gold and is more reliable than silver. 6 Related Work AMR parsing for languages other than English has made only a few steps forward. In previous work (Li et al., 2016; Xue et al., 2014; Bojar, 2014), nodes of the target graph were labeled with either English words or with words in the target language. We instead use the AMR annotation used for English for the target language as well, without translating any word. To the best of our knowledge, the only previous work that attempts to automatically parse AMR graphs for non-English sentences is by Vanderwende et al. (2015). Sentences in several languages (French, German, Spanish and Japanese) are parsed into a logical representation, which is then converted to AMR using a small set of rules. A comparison with this work is difficult, as the authors do not report results for the parsers (due to the lack of an annotated corpus) or release their code. Besides AMR, other semantic parsing frameworks for non-English languages have been investigated (Hoffman, 1992; Cinkov´a et al., 2009; Gesmundo et al., 2009; Evang and Bos, 2016). Evang and Bos (2016) is the most closely related to our 1152 envy answer-01 :domain ente"
N18-1104,xue-etal-2014-interlingua,0,0.212505,"g AMR representations (Banarescu et al., 2013). An AMR is a graph with nodes representing the concepts of the sentence and edges representing the semantic relations between them. Most available AMR datasets large enough to train statistical models consist of pairs of English sentences and AMR graphs. The cross-lingual properties of AMR across languages has been the subject of preliminary discussions. The AMR guidelines state that AMR is not an interlingua (Banarescu et al., 2013) and Bojar (2014) categorizes different kinds of divergences in the annotation between English AMRs and Czech AMRs. Xue et al. (2014) show that structurally aligning English AMRs with Czech and Chinese AMRs is not always possible but that refined annotation guidelines suffice to resolve some of these cases. We extend this line of research by exploring whether divergences among languages can be overcome, i.e., we investigate whether it is possible to maintain the AMR annotated for English as a semantic representation for sentences written in other languages, as in Figure 1. We implement AMR parsers for Italian, Spanish, German and Chinese using annotation projection, where existing annotations are projected from a source lan"
N18-1104,H01-1035,0,0.323106,"MRs is not always possible but that refined annotation guidelines suffice to resolve some of these cases. We extend this line of research by exploring whether divergences among languages can be overcome, i.e., we investigate whether it is possible to maintain the AMR annotated for English as a semantic representation for sentences written in other languages, as in Figure 1. We implement AMR parsers for Italian, Spanish, German and Chinese using annotation projection, where existing annotations are projected from a source language (English) to a target language through a parallel corpus (e.g., Yarowsky et al., 2001; Hwa et al., 2005; Pad´o and Lapata, 2009; Evang and Bos, 2016). By evaluating the parsers and manually analyzing their output, we show that the parsers are able to recover the AMR structures even when there exist structural differences between the languages, i.e., although AMR is not an interlingua it can act as one. This method also provides a quick way to prototype multilingual AMR parsers, assuming that Part-of-speech (POS) taggers, Named Entity Recognition (NER) taggers and dependency parsers are available for the target languages. We also propose an alternative approach, where Machine T"
N18-1104,I08-3008,0,0.0447691,"guages when this data is available in at least one language, usually English. The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis (Yarowsky et al., 2001) but it has also been used for dependency parsing (Hwa et al., 2005), role labeling (Pad´o and Lapata, 2009; Akbik et al., 2015) and semantic parsing (Evang and Bos, 2016). Another common thread of cross-lingual work is model transfer, where parameters are shared across languages (Zeman and Resnik, 2008; Cohen and Smith, 2009; Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). 7 work as it uses a projection mechanism similar to ours for CCG. A crucial difference is that, in order to project CCG parse trees to the target languages, they only make use of literal translation. Previous work has also focused on assessing the stabilConclusions We introduced the problem of parsing AMR structures, annotated for English, from sentences written in other languages as a way to test the crosslingual properties of AMR. We provided evidence that AMR can be indeed shared across the lan1153 guages te"
N18-1158,P13-1020,0,0.0258573,"of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Re"
N18-1158,W97-0703,0,0.66836,"e that L EAD is indeed more informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨"
N18-1158,P11-1049,0,0.0913272,"nkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highl"
N18-1158,P16-1046,1,0.107683,"and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptualize extractive summarization as a sequence labeling task in which each label specifies whether each document sentence should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentence"
N18-1158,J10-3005,1,0.920121,"Missing"
N18-1158,W04-1017,0,0.0911027,"EAD is considered better than See et al. (2017) in the QA evaluation, whereas we find the opposite when participants are asked to rank systems. We hypothesize that L EAD is indeed more informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 20"
N18-1158,D15-1042,0,0.163937,"Missing"
N18-1158,P14-1062,0,0.054469,"lecting m sentences with top p(1|si , D, θ) scores. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over th"
N18-1158,D14-1181,0,0.00830447,"ary S by selecting m sentences with top p(1|si , D, θ) scores. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply ma"
N18-1158,W14-1504,0,0.061479,"Missing"
N18-1158,D15-1180,0,0.0182507,"res. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over the feature map f and take the maximum v"
N18-1158,P15-1107,0,0.019581,"plied three times each. Max-pooling over time yields two feature lists f K2 and f K4 ∈ R3 . The final sentence embeddings have six dimensions. Document Encoder The document encoder composes a sequence of sentences to obtain a document representation. We use a recurrent neural network with Long Short-Term Memory (LSTM) cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document D consisting of a sequence of sentences (s1 , s2 , . . . , sn ), we follow common practice and feed sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015; Narayan et al., 2017). This way we make sure that the network also considers the top sentences of the document which are particularly important for summarization (Rush et al., 2015; Nallapati et al., 2016). Sentence Extractor Our sentence extractor sequentially labels each sentence in a document with 1 (relevant for the summary) or 0 (otherwise). 1748 Sentence extractor y5 y4 y3 y2 y1 Candidate summary Gold summary Sentence encoder REWARD s4 Police are still hunting for the driver s3 s5 s2 s1 s4 s3 s2 s1 Document encoder REINFORCE [convolution] [max pooling] Update ag"
N18-1158,D16-1127,0,0.240915,"-entropy loss using ground truth labels and then follows a curriculum learning strategy (Bengio et al., 2015) to gradually teach the model to produce stable predictions on its own. In our experiments MIXER performed worse than the model of Nallapati et al. (2017) just trained on collective labels. We conjecture that this is due to the unbounded nature of our ranking problem. Recall that our model assigns relevance scores to sentences rather than words. The space of sentential representations is vast and fairly unconstrained compared to other prediction tasks operating with fixed vocabularies (Li et al., 2016; Paulus et al., 2017; Zhang and Lapata, 2017). Moreover, our approximation of the gradient allows the model to 1751 In this section we present our experimental setup for assessing the performance of our model which we call R EFRESH as a shorthand for REinFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015)"
N18-1158,N03-1020,0,0.870659,"ce should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentences based on their importance due to the absence of a ranking-based objective. Another discrepancy comes from the mismatch between the learning objective and the evaluation criterion, namely ROUGE (Lin and Hovy, 2003), which takes the entire summary into account. In this paper we argue that cross-entropy training is not optimal for extractive summarization. Models trained this way are prone to generating verbose summaries with unnecessarily long sentences and redundant information. We propose to overcome these difficulties by globally optimizing the ROUGE evaluation metric and learning to rank sentences for summary generation through a reinforcement learning objective. Similar to previous work (Cheng and Lapata, 2016; Narayan et al., 2017; Nallapati et al., 2017), our neural summarization model consists of"
N18-1158,U17-1012,0,0.040784,"Missing"
N18-1158,D17-1061,0,0.0259843,"zation, in our case states are documents (not summaries) and actions are relevance scores which lead to sentence ranking (not sentence-to-sentence transitions). Rather than employing reinforcement learning for sentence selection, our algorithm performs sentence ranking using ROUGE as the reward function. The REINFORCE algorithm (Williams, 1992) has been shown to improve encoder-decoder textrewriting systems by allowing to directly optimize a non-differentiable objective (Ranzato et al., 2015; Li et al., 2016; Paulus et al., 2017) or to inject task-specific constraints (Zhang and Lapata, 2017; Nogueira and Cho, 2017). However, we are not aware of any attempts to use reinforcement learning for training a sentence ranker in the context of extractive summarization. 8 Conclusions In this work we developed an extractive summarization model which is globally trained by optimizing the ROUGE evaluation metric. Our training algorithm explores the space of candidate summaries while learning to optimize a reward function which is relevant for the task at hand. Experimental results show that reinforcement learning offers a great means to steer our model towards generating informative, fluent, and concise summaries ou"
N18-1158,D15-1226,0,0.0592465,"core each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Reinforcement learning ha"
N18-1158,radev-etal-2004-mead,0,0.386489,"Missing"
N18-1158,D14-1075,0,0.0947591,"Missing"
N18-1158,D15-1044,0,0.594398,"f summarization tasks that have been identified over the years (see Nenkova and McKeown, 2011 for a comprehensive overview). Modern approaches to single document summarization are data-driven, taking advantage of the success of neural network architectures and their ability to learn continuous features without recourse to preprocessing tools or linguistic annotations. Abstractive summarization involves various text rewriting operations (e.g., substitution, deletion, reordering) and has been recently framed as a sequence-to-sequence problem (Sutskever et al., 2014). Central in most approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See 1 Our code and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most"
N18-1158,D12-1024,0,0.0672832,"istic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Reinforcement learning has been previously used in the context of traditional multi-document summarization as a means of selecting a sentence or a subset of sentences from a document cluster. Ryang and Abekawa (2012) cast the sentence selection task as a search problem. Their agent observes a state (e.g., a candidate summary), executes an action (a transition operation that produces a new state selecting a not-yet-selected sentence), and then receives a delayed reward based on tf ∗ idf. Follow-on work (Rioux et al., 2014) extends this approach by employing ROUGE as part of the reward function, while Henß et al. (2015) further experiment with Q-learning. Moll´aAliod (2017) has adapt this approach to queryfocused summarization. Our model differs from these approaches both in application and formulation. We"
N18-1158,P08-2052,0,0.0921083,"f L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any"
N18-1158,E17-2007,0,0.0672798,".2 We compared R EFRESH against a baseline which simply selects the first m leading sentences from each document (L EAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss. Cheng and Lapata (2016) train on individual labels, while Nallapati et al. (2017) use collective labels. We also compared our model against the abstractive systems of Chen et al. (2016), Nallapati et al. (2016), See et al. (2017), and Tan and Wan (2017).3 In addition to ROUGE which can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017), we also evaluated system output by eliciting human judgments in two ways. In our first experiment, participants were presented with a news article and summaries generated by three systems: the L EAD baseline, abstracts from See et al. (2017), and extracts from R EFRESH. We also included the human-authored highlights.4 Participants read the articles and were asked to rank the summaries from best (1) to worst (4) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?). We did not allow any ties. W"
N18-1158,P17-1099,0,0.458809,"nFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 documents for CNN and 196,961/12,148/10,397 for DailyMail). We did not anonymize entities or lower case tokens. We followed previous studies (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017) in assuming that the “story highlights” associated with each article are gold-standard abstractive summaries. During training we use these to generate high scoring extracts and to estimate rewards for them, but during testing, they are used as reference summaries to evaluate our models. Implementation Details We generated extracts by selecting three sentences (m = 3) for CNN articles and four sentences (m = 4) for DailyMail articles. These decisions were informed by the fact that gold highlights in the CNN/DailyMail validation sets are 2.6/4.2 sentences long. For both data"
N18-1158,P16-1159,0,0.0290088,"mbeddings with the skip-gram model (Mikolov et al., 2013) using context window size 6, negative sampling size 10, and hierarchical softmax 1. Known words were initialized with pre-trained embeddings of size 200. Embeddings for unknown words were initialized to zero, but estimated during training. L EAD R EFRESH Experimental Setup G OLD 5 See et al. converge much faster to an optimal policy. Advantageously, we do not require an online reward estiˆ which leads to a signifimator, we pre-compute Y, cant speedup during training compared to MIXER (Ranzato et al., 2015) and related training schemes (Shen et al., 2016). Q1 Q2 Q3 • A SkyWest Airlines flight made an emergency landing in Buffalo, New York, on Wednesday after a passenger lost consciousness, officials said. • The passenger received medical attention before being released, according to Marissa Snow, spokeswoman for SkyWest. • She said the airliner expects to accommodate the 75 passengers on another aircraft to their original destination – Hartford, Connecticut – later Wednesday afternoon. • Skywest Airlines flight made an emergency landing in Buffalo, New York, on Wednesday after a passenger lost consciousness. • She said the airliner expects to"
N18-1158,D07-1047,0,0.129055,"Missing"
N18-1158,P17-1108,0,0.457427,"mmarization are data-driven, taking advantage of the success of neural network architectures and their ability to learn continuous features without recourse to preprocessing tools or linguistic annotations. Abstractive summarization involves various text rewriting operations (e.g., substitution, deletion, reordering) and has been recently framed as a sequence-to-sequence problem (Sutskever et al., 2014). Central in most approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See 1 Our code and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptuali"
N18-1158,W97-0710,0,0.602162,"informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin an"
N18-1158,C10-1128,0,0.0490031,"ds compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic"
N18-1158,P10-1058,1,0.933128,"n the task definition and the training objective. While MLE in Equation (1) aims to maximize the likelihood of the ground-truth labels, the model is (a) expected to rank sentences to generate a summary and (b) evaluated using ROUGE at test time. The second discrepancy comes from the reliance on ground-truth labels. Document collections for training summarization systems do not naturally contain labels indicating which sentences should be extracted. Instead, they are typically accompanied by abstractive summaries from which sentence-level labels are extrapolated. Cheng and Lapata (2016) follow Woodsend and Lapata (2010) in adopting a rule-based method which assigns labels to each sentence in the document individually based on their semantic correspondence with the gold summary (see the fourth column in Table 1). An alternative method (Svore et al., 2007; Cao et al., 2016; Nallapati et al., 2017) identifies the set of sentences which collectively gives the highest ROUGE with respect to the gold summary. Sentences in this set are labeled with 1 and 0 otherwise (see the column 5 in Table 1). Labeling sentences individually often generates too many positive labels causing the model to 1749 Collective Oracle ROUG"
N18-1158,D12-1022,1,0.736525,"es, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in h"
N18-1158,K17-1045,0,0.055014,"h. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptualize extractive summarization as a sequence labeling task in which each label specifies whether each document sentence should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentences based on their importance due to the absence of a ranking-based obje"
N18-1158,D17-1062,1,0.913736,"s and then follows a curriculum learning strategy (Bengio et al., 2015) to gradually teach the model to produce stable predictions on its own. In our experiments MIXER performed worse than the model of Nallapati et al. (2017) just trained on collective labels. We conjecture that this is due to the unbounded nature of our ranking problem. Recall that our model assigns relevance scores to sentences rather than words. The space of sentential representations is vast and fairly unconstrained compared to other prediction tasks operating with fixed vocabularies (Li et al., 2016; Paulus et al., 2017; Zhang and Lapata, 2017). Moreover, our approximation of the gradient allows the model to 1751 In this section we present our experimental setup for assessing the performance of our model which we call R EFRESH as a shorthand for REinFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266"
N18-1158,W04-3252,0,\N,Missing
N18-1158,W01-0100,0,\N,Missing
N19-1018,W06-2922,0,0.182159,"Missing"
N19-1018,D16-1211,0,0.028788,"ssuming all previous actions were gold), we optimize the likelihood of the best actions, as computed by the dynamic oracle, from a configuration sampled from the space of all possible configurations. In practice, before each epoch, we sample each sentence from the training corpus with probability p and we use the current (non-averaged) parameters to parse the sentence and generate a sequence of configurations. Instead of selecting the highestscoring action at each parsing step, as in a normal inference step, we sample an action using the softmax distribution computed by the parser, as done by Ballesteros et al. (2016). Then, we use the dynamic oracle to calculate the best action from each of these configurations. In case there are several best actions, we deterministically choose a single action by favoring a COMBINE over a SHIFT (to bias the model towards a small memory), and to COMBINE with the item with the highest right-index (to avoid spurious discontinuity in partial constituents). We train the parser on these sequences of potentially non-gold configurationaction pairs. where FFs is a feedforward network with two hidden layers, a tanh activation and a single output unit. In other words, it outputs a"
N19-1018,C18-1258,0,0.349152,"Missing"
N19-1018,W16-0906,0,0.388132,"present the first experiments in discontinuous constituency parsing using a dynamic oracle. Our parser obtains state-of-the-art results on three English and German discontinuous treebanks. 1 Figure 1: Discontinuous constituency tree from the Discontinuous Penn treebank. A natural alternative to grammar-based chart parsing is transition-based parsing, that usually relies on fast approximate decoding methods such as greedy search or beam search. Transitionbased discontinuous parsers construct discontinuous constituents by reordering terminals with the SWAP action (Versley, 2014a,b; Maier, 2015; Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017), or by using a split stack and the GAP action to combine two non-adjacent constituents (Coavoux and Crabb´e, 2017a; Coavoux et al., 2019). These proposals represent the memory of the parser (i.e. the tree fragments being constructed) with data structures with linear-time sequential access (either a stack, or a stack coupled with a double-ended queue). As a result, these systems need to perform at least n actions to construct a new constituent from two subtrees separated by n intervening subtrees. Our proposal aims at avoiding this cost when constructing"
N19-1018,J93-2004,0,0.0646921,"root symbols and punctuation. 4.1 Datasets Juditsky, 1992) for 100 epochs. Training a single model takes approximately a week with a GPU. We evaluate a model every 4 epochs on the validation set and select the best performing model according to the validation F-score. We refer the reader to Table 5 of Appendix B for the full list of hyperparameters. We perform experiments on three discontinuous constituency corpora. The discontinuous Penn Treebank was introduced by Evang and Kallmeyer (2011) who converted the long distance dependencies encoded by indexed traces in the original Penn treebank (Marcus et al., 1993) to discontinuous constituents. We used the standard split (sections 2-21 for training, 22 for development and 23 for test). The Tiger corpus (Brants et al., 2004) and the Negra corpus (Skut et al., 1997) are both German treebanks natively annotated with discontinuous constituents. We used the SPMRL split for the Tiger corpus (Seddah et al., 2013), and the split of Dubey and Keller (2003) for the Negra corpus. 4.2 We evaluate models with the dedicated module of discodop2 (van Cranenburgh et al., 2016). We use the standard evaluation parameters (proper.prm), that ignore punctuations and root sy"
N19-1018,C12-1059,0,0.0372439,"tuent (X, sg ) ∈ / C is reachable iff both the following properties hold: Dynamic Oracle Parsers are usually trained to predict the gold sequence of actions, using a static oracle. The limitation of this method is that the parser only sees a tiny portion of the search space at train time and only trains on gold input (i.e. configurations obtained after performing gold actions). At test time, it is in a different situation due to error propagation: it must predict what the best actions are in configurations from which the gold tree is probably no longer reachable. To alleviate this limitation, Goldberg and Nivre (2012) proposed to train a parser with a dynamic oracle, an oracle that is defined for any parsing configuration and outputs the set of best actions to perform. In contrast, a static oracle is deterministic and is only defined for gold configurations. Dynamic oracles were proposed for a wide range of dependency parsing transition systems (Goldberg and Nivre, 2013; G´omez-Rodr´ıguez et al., 2014; G´omez-Rodr´ıguez and Fern´andezGonz´alez, 2015), and later adapted to constituency parsing (Coavoux and Crabb´e, 2016; Cross and Huang, 2016b; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2018b,a). 1. max(sf"
N19-1018,Q13-1033,0,0.0278439,"rming gold actions). At test time, it is in a different situation due to error propagation: it must predict what the best actions are in configurations from which the gold tree is probably no longer reachable. To alleviate this limitation, Goldberg and Nivre (2012) proposed to train a parser with a dynamic oracle, an oracle that is defined for any parsing configuration and outputs the set of best actions to perform. In contrast, a static oracle is deterministic and is only defined for gold configurations. Dynamic oracles were proposed for a wide range of dependency parsing transition systems (Goldberg and Nivre, 2013; G´omez-Rodr´ıguez et al., 2014; G´omez-Rodr´ıguez and Fern´andezGonz´alez, 2015), and later adapted to constituency parsing (Coavoux and Crabb´e, 2016; Cross and Huang, 2016b; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2018b,a). 1. max(sf ) ≤ max(sg ); 2. ∀s ∈ S ∪ {sf }, (s ⊆ sg ) or (s ∩ sg = ∅). Condition 1 is necessary because the parser can only construct new constituents (X, s) such that sf  s. Condition 2 makes sure that sg can be constructed from a union of elements from S ∪ {sf }, potentially augmented with terminals from the bufffer: {i, i + 1, . . . , max(sg )}. Following Cross an"
N19-1018,P09-1040,0,0.112128,"to learn parameters with every possible i, and will only learn parameters with the S KIP S HIFT-i actions that are required to derive the training set. In contrast, we use the same parameters to score each possible COMBINE-s action. based strategy, a split-stack strategy, and the use of non-local transitions. Swap-based systems Swap-based transition systems are based on the idea that any discontinuous constituency tree can be transformed into a projective tree by reordering terminals. They reorder terminals by swapping them with a dedicated action (SWAP), commonly used in dependency parsing (Nivre, 2009). The first proposals in transition-based discontinuous constituency parsing used the SWAP action on top of an easy-first parser (Versley, 2014a,b). Subsequent proposals relied on a shift-reduce system (Maier, 2015; Maier and Lichte, 2016) or a shift-promote-adjoin system (Stanojevi´c and Garrido Alhama, 2017). The main limitation of swap-based system is that they tend to require a large number of transitions to derive certain trees. The choice of an oracle that minimizes derivation lengths has a substantially positive effect on parsing (Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama,"
N19-1018,P15-2042,0,0.037804,"Missing"
N19-1018,D14-1099,0,0.14142,"Missing"
N19-1018,P14-1022,0,0.0601175,"from S ∪ {sf }, potentially augmented with terminals from the bufffer: {i, i + 1, . . . , max(sg )}. Following Cross and Huang (2016b), we define next(c, t∗ ) as the smallest reachable gold constituent from a configuration c. Formally: next(c, t∗ ) = argmin reach(c, t∗ ).  207 3.2 Oracle algorithm We first define the oracle o for the odd step of a configuration c = (S, sf , i, C): oodd (c, t∗ ) =  An open issue in neural discontinuous parsing is the representation of discontinuous constituents. In projective constituency parsing, it has become standard to use the boundaries of constituents (Hall et al., 2014; Crabb´e, 2015; Durrett and Klein, 2015), an approach that proved very successful with bi-LSTM token representations (Cross and Huang, 2016b; Stern et al., 2017). Although constituent boundary features improves discontinuous parsing (Coavoux and Crabb´e, 2017a), relying only on the left-index and the right-index of a constituent has the limitation of ignoring gaps inside a constituent. For example, since the two VPs in Figure 1 have the same right-index and left-index, they would have the same representations. It may also happen that constituents with identical right-index and left-index do n"
N19-1018,P17-2018,0,0.143404,"Missing"
N19-1018,Q16-1023,0,0.0377712,"st present an encoder that computes contextaware representations of tokens (Section 3.1). We then discuss how to compute the representation of a set of tokens (Section 3.2). We describe the action scorer (Section 3.3), the POS tagging component (Section 3.4), and the objective function (Section 3.5). 3.1 s = {i |min(s) < i < max(s) and i ∈ / s}. Finally, we extract the corresponding token representations of the 4 indexes and concatenate them to form the vector representation r(s) of s: Token Representations As in recent proposals in dependency and constituency parsing (Cross and Huang, 2016a; Kiperwasser and Goldberg, 2016), our scoring system is based on a sentence transducer that constructs a context-aware representation for each token. Given a sequence of tokens xn1 = (x1 , . . . , xn ), we first run a single-layer character bi-LSTM encoder c to obtain a character-aware embedding c(xi ) for each token. We represent a token xi as the concatenation of a standard word embedding e(xi ) and the character-aware embedding: wxi = [c(xi ); e(xi )]. Then, we run a 2-layer bi-LSTM transducer over the sequence of token representations: (2) (2) (2) (2) r(s) = [hmin(s) ; hmax(s) ; hmin(s) ; hmax(s) ]. For an index set that"
N19-1018,P15-1116,0,0.464529,"system, and present the first experiments in discontinuous constituency parsing using a dynamic oracle. Our parser obtains state-of-the-art results on three English and German discontinuous treebanks. 1 Figure 1: Discontinuous constituency tree from the Discontinuous Penn treebank. A natural alternative to grammar-based chart parsing is transition-based parsing, that usually relies on fast approximate decoding methods such as greedy search or beam search. Transitionbased discontinuous parsers construct discontinuous constituents by reordering terminals with the SWAP action (Versley, 2014a,b; Maier, 2015; Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017), or by using a split stack and the GAP action to combine two non-adjacent constituents (Coavoux and Crabb´e, 2017a; Coavoux et al., 2019). These proposals represent the memory of the parser (i.e. the tree fragments being constructed) with data structures with linear-time sequential access (either a stack, or a stack coupled with a double-ended queue). As a result, these systems need to perform at least n actions to construct a new constituent from two subtrees separated by n intervening subtrees. Our proposal aims at avoiding this"
N19-1018,A97-1014,0,0.709169,"the best performing model according to the validation F-score. We refer the reader to Table 5 of Appendix B for the full list of hyperparameters. We perform experiments on three discontinuous constituency corpora. The discontinuous Penn Treebank was introduced by Evang and Kallmeyer (2011) who converted the long distance dependencies encoded by indexed traces in the original Penn treebank (Marcus et al., 1993) to discontinuous constituents. We used the standard split (sections 2-21 for training, 22 for development and 23 for test). The Tiger corpus (Brants et al., 2004) and the Negra corpus (Skut et al., 1997) are both German treebanks natively annotated with discontinuous constituents. We used the SPMRL split for the Tiger corpus (Seddah et al., 2013), and the split of Dubey and Keller (2003) for the Negra corpus. 4.2 We evaluate models with the dedicated module of discodop2 (van Cranenburgh et al., 2016). We use the standard evaluation parameters (proper.prm), that ignore punctuations and root symbols. We report two evaluation metrics: a standard Fscore (F) and an Fscore computed only on discontinuous constituents (Disc. F), which provides a more qualitative evaluation of the ability of the parse"
N19-1018,D17-1174,0,0.443056,"Missing"
N19-1018,W14-6104,0,0.59616,"the new transition system, and present the first experiments in discontinuous constituency parsing using a dynamic oracle. Our parser obtains state-of-the-art results on three English and German discontinuous treebanks. 1 Figure 1: Discontinuous constituency tree from the Discontinuous Penn treebank. A natural alternative to grammar-based chart parsing is transition-based parsing, that usually relies on fast approximate decoding methods such as greedy search or beam search. Transitionbased discontinuous parsers construct discontinuous constituents by reordering terminals with the SWAP action (Versley, 2014a,b; Maier, 2015; Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017), or by using a split stack and the GAP action to combine two non-adjacent constituents (Coavoux and Crabb´e, 2017a; Coavoux et al., 2019). These proposals represent the memory of the parser (i.e. the tree fragments being constructed) with data structures with linear-time sequential access (either a stack, or a stack coupled with a double-ended queue). As a result, these systems need to perform at least n actions to construct a new constituent from two subtrees separated by n intervening subtrees. Our proposal aims"
N19-1018,W16-0907,0,0.266393,"8.4 83.6 84.0 51.3 54.0 97.9 98.0 Table 3: Results on development corpora. F1 is the Fscore on all constituents, Disc. F1 is an Fscore computed only on discontinuous constituents, POS is the accuracy on part-of-speech tags. Detailed results (including precision and recall) are given in Table 7 of Appendix C. English (DPTB) Model F Disc. F German (Tiger) German (Negra) F Disc. F F Disc. F Predicted POS tags or own tagging This work, dynamic oracle (2019),∗ GAP, Coavoux et al. bi-LSTM Stanojevi´c and Garrido Alhama (2017),∗ SWAP, stack/tree-LSTM Coavoux and Crabb´e (2017a), SR - GAP, perceptron Versley (2016), pseudo-projective, chart-based Corro et al. (2017),∗ bi-LSTM, Maximum Spanning Arborescence van Cranenburgh et al. (2016), DOP, ≤ 40 Fern´andez-Gonz´alez and Martins (2015), dependency-based Gebhardt (2018), LCFRS with latent annotations 90.9 67.3 82.5 55.9 83.2 56.3 91.0 71.3 82.7 77.0 79.3 79.5 55.9 83.2 54.6 89.2 87.0 74.8 77.3 75.1 Gold POS tags (2017),∗ SWAP, Stanojevi´c and Garrido Alhama stack/tree-LSTM Coavoux and Crabb´e (2017a), SR - GAP, perceptron Maier (2015), SWAP, perceptron Corro et al. (2017),∗ bi-LSTM, Maximum Spanning Arborescence Evang and Kallmeyer (2011), PLCFRS, < 25 9"
N19-1018,P87-1015,0,0.689823,"Missing"
N19-1366,W17-7607,0,0.0665839,"Missing"
N19-1366,P17-1177,0,0.0192117,"equence, where the structural encoders are applied to the hidden vectors computed by the BiLSTM. Right side: sequence on top of structure, where the structural encoder is used to create better embeddings which are then fed to the BiLSTM. The dotted lines refer to the process of converting the graph into a sequence or vice-versa. 4 Stacking Encoders We aimed at stacking the explicit source of structural information provided by TreeLSTMs and GCNs with the sequential information which BiLSTMs extract well. This was shown to be effective for other tasks with both TreeLSTMs (Eriguchi et al., 2016; Chen et al., 2017) and GCNs (Marcheggiani and Titov, 2017; Cetoli et al., 2017; Bastings et al., 2017). In previous work, the structural encoders (tree or graph) were used on top of the BiLSTM network: first, the input is passed through the sequential encoder, the output of which is then fed into the structural encoder. While we experiment with this approach, we also propose an alternative solution where the BiLSTM network is used on top of the structural encoder: the input embeddings are refined by exploiting the explicit structural information given by the graph. The refined embeddings are then fed into the B"
N19-1366,E17-1051,1,0.8551,"xplicitly encode reentrant structures present in the AMR graphs. While central to AMR, reentrancies are often hard to treat both in parsing and in generation. Previous work either removed them from the graphs, hence obtaining sequential (Konstas et al., 3649 Proceedings of NAACL-HLT 2019, pages 3649–3658 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2017) or tree-structured (Liu et al., 2015; Takase et al., 2016) data, while other work maintained them but did not analyze their impact on performance (e.g., Song et al., 2018; Beck et al., 2018). Damonte et al. (2017) showed that state-of-the-art parsers do not perform well in predicting reentrant structures, while van Noord and Bos (2017) compared different pre- and post-processing techniques to improve the performance of sequenceto-sequence parsers with respect to reentrancies. It is not yet clear whether explicit encoding of reentrancies is beneficial for generation. In this paper, we compare three types of encoders for AMR: 1) sequential encoders, which reduce AMR graphs to sequences; 2) tree encoders, which ignore reentrancies; and 3) graph encoders. We pay particular attention to two phenomena: reent"
N19-1366,P16-1078,0,0.114023,"akase et al., 2016). TreeLSTMs assume tree-structured input, so AMR graphs must be preprocessed to respect this constraint: reentrancies, which play an essential role in AMR, must be removed, thereby transforming the graphs into trees. We use the Child-Sum variant introduced by Tai et al. (2015), which processes the tree in a bottomup pass. When visiting a node, the hidden states of its children are summed up in a single vector which is then passed into recurrent gates. In order to use information from both incoming and outgoing edges (parents and children), we employ bidirectional TreeLSTMs (Eriguchi et al., 2016), where the bottom-up pass is followed by a top-down pass. The top-down state of the root node is obtained by feeding the bottom-up state of the root node through a feed-forward layer: h↓root = tanh(Wr h↑root + b), where h↑i is the hidden state of node xi ∈ V for the bottom-up pass and h↓i is the hidden state of node xi for the top-down pass. e1:N = h1:N , where ei ∈ Rd , d is the size of the output embeddings. The encoder is related to the TreeLSTM encoder of Takase et al. (2016), which however encodes labeled trees and does not use a top-down pass. 3.3 Graph Convolutional Network Encoders Gr"
N19-1366,P16-1014,0,0.036965,"ges in the AMR are labeled: E0 ⊆ V0 × L × V0 , L = {`1 , `2 , . . . , `n0 }. xi ∈ V. The depth-first traversal of the graph defines the indexing between nodes and tokens in the sequence. For instance, the root node is x1 , its leftmost child is x2 and so on. Nodes with multiple parents are visited more than once. At each visit, their labels are repeated in the sequence, effectively losing reentrancy information, as shown in Figure 1(b). Anonymization removes names and rare words with coarse categories to reduce data sparsity. An alternative to anonymization is to employ a copy 3650 mechanism (Gulcehre et al., 2016), where the models learn to copy rare words from the input itself. In this paper, we follow the anonymization approach. 3 h↓i = LSTM(h↑p(i) , h↑i ), Encoders In this section, we review the encoders adopted as building blocks for our tree and graph encoders. 3.1 The bottom up states for all other nodes are computed with an LSTM, with the cell state given by their parent nodes: Recurrent Neural Network Encoders We reimplement the encoder of Konstas et al. (2017), where the sequential linearization is the input to a bidirectional LSTM (BiLSTM; Graves et al. 2013) network. The hidden state of the"
N19-1366,P17-4012,0,0.0609528,"ehind this approach is that we know that BiLSTMs, given appropriate input embeddings, are very effective at encoding the input sequences. In order to exploit their strength, we do not amend their output but rather provide them with better input embeddings to start with, by explicitly taking the graph relations into account. 5 Experiments We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evaluation metrics.1 We report results on the AMR dataset LDC2015E86 and LDC2017T10. All systems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017). Hyperparameters of each model were tuned on the development set of LDC2015E86. For the GCN components, we use two layers, ReLU activations, and tanh highway layers. We use single layer LSTMs. We train with SGD with the initial learning rate set to 1 and decay to 0.8. Batch size is set to 100.2 We first evaluate the overall performance of the models, after which we focus on two phenomena that we expect to benefit most from structural 1 We used the evaluation script available at https:// github.com/sinantie/NeuralAmr. 2 Our code is available at https://github.com/ mdtux89/OpenNMT-py-AMR-to-tex"
N19-1366,P17-1014,0,0.459958,"on of a sentence, where nodes in the graph represent concepts and edges represent semantic relations between them. AMRs are graphs, rather than trees, because co-references and control structures result in nodes with multiple parents, called reentrancies. For instance, the AMR of Figure 1(a) contains a reentrancy between finger and he, caused by the possessive pronoun his. AMR-to-text generation is the task of automatically generating natural language from AMR graphs. Attentive encoder/decoder architectures, commonly used for Neural Machine Translation (NMT), have been explored for this task (Konstas et al., 2017; Song et al., 2018; Beck et al., 2018). In order to use sequence-to-sequence models, Konstas et al. (2017) reduce the AMR graphs to sequences, while Song et al. (2018) and Beck et al. (2018) directly encode them as graphs. Graph encoding allows the model to explicitly encode reentrant structures present in the AMR graphs. While central to AMR, reentrancies are often hard to treat both in parsing and in generation. Previous work either removed them from the graphs, hence obtaining sequential (Konstas et al., 3649 Proceedings of NAACL-HLT 2019, pages 3649–3658 c Minneapolis, Minnesota, June 2 -"
N19-1366,D17-1209,0,0.114956,"den state of node xi for the top-down pass. e1:N = h1:N , where ei ∈ Rd , d is the size of the output embeddings. The encoder is related to the TreeLSTM encoder of Takase et al. (2016), which however encodes labeled trees and does not use a top-down pass. 3.3 Graph Convolutional Network Encoders Graph Convolutional Network (GCN; Duvenaud et al. 2015; Kipf and Welling 2016) is a neural network architecture that learns embeddings of nodes in a graph by looking at its nearby nodes. In Natural Language Processing, GCNs have been used for Semantic Role Labeling (Marcheggiani and Titov, 2017), NMT (Bastings et al., 2017), Named Entity Recognition (Cetoli et al., 2017) and text generation (Marcheggiani and Perez-Beltrachini, 2018). A graph-to-sequence neural network was first introduced by Xu et al. (2018). The authors review the similarities between their approach, GCN and another approach, based on GRUs (Li et al., 2015). The latter recently inspired a graphto-sequence architecture for AMR-to-text generation (Beck et al., 2018). Simultaneously, Song et al. (2018) proposed a graph encoder based on LSTMs. The architectures of Song et al. (2018) and Beck et al. (2018) are both based on the same core computation"
N19-1366,P18-1026,0,0.228732,"h represent concepts and edges represent semantic relations between them. AMRs are graphs, rather than trees, because co-references and control structures result in nodes with multiple parents, called reentrancies. For instance, the AMR of Figure 1(a) contains a reentrancy between finger and he, caused by the possessive pronoun his. AMR-to-text generation is the task of automatically generating natural language from AMR graphs. Attentive encoder/decoder architectures, commonly used for Neural Machine Translation (NMT), have been explored for this task (Konstas et al., 2017; Song et al., 2018; Beck et al., 2018). In order to use sequence-to-sequence models, Konstas et al. (2017) reduce the AMR graphs to sequences, while Song et al. (2018) and Beck et al. (2018) directly encode them as graphs. Graph encoding allows the model to explicitly encode reentrant structures present in the AMR graphs. While central to AMR, reentrancies are often hard to treat both in parsing and in generation. Previous work either removed them from the graphs, hence obtaining sequential (Konstas et al., 3649 Proceedings of NAACL-HLT 2019, pages 3649–3658 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Com"
N19-1366,Q16-1037,0,0.0311653,"placements and 95 due to gender replacements.5 The results are shown in Table 6. The sequential encoder performs surprisingly well at this task, with better or on par performance with respect to the tree encoder. The graph encoder outperforms the sequential encoder only for pronoun number and gender replacements. Future work is required to more precisely analyze if the different models cope with pronomial mentions in significantly different ways. Other approaches to inspect phenomena of co-reference and control verbs can also be explored, for instance by devising specific training objectives (Linzen et al., 2016). 5.2 Long-range Dependencies When we encode a long sequence, interactions between items that appear distant from each other in the sequence are difficult to capture. The problem of long-range dependencies in natural language is well known for RNN architectures (Bengio et al., 1994). Indeed, the need to solve this problem motivated the introduction of LSTM models, which are known to model long-range dependencies better than traditional RNNs. 5 The generated contrastive examples are available at https://github.com/mdtux89/OpenNMT-py. 3655 (1) REF S EQ T REE G RAPH i dont tell him but he finds o"
N19-1366,N15-1114,0,0.0540396,"nstas et al. (2017) reduce the AMR graphs to sequences, while Song et al. (2018) and Beck et al. (2018) directly encode them as graphs. Graph encoding allows the model to explicitly encode reentrant structures present in the AMR graphs. While central to AMR, reentrancies are often hard to treat both in parsing and in generation. Previous work either removed them from the graphs, hence obtaining sequential (Konstas et al., 3649 Proceedings of NAACL-HLT 2019, pages 3649–3658 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2017) or tree-structured (Liu et al., 2015; Takase et al., 2016) data, while other work maintained them but did not analyze their impact on performance (e.g., Song et al., 2018; Beck et al., 2018). Damonte et al. (2017) showed that state-of-the-art parsers do not perform well in predicting reentrant structures, while van Noord and Bos (2017) compared different pre- and post-processing techniques to improve the performance of sequenceto-sequence parsers with respect to reentrancies. It is not yet clear whether explicit encoding of reentrancies is beneficial for generation. In this paper, we compare three types of encoders for AMR: 1) s"
N19-1366,P14-5010,0,0.0031399,", we use a method to inspect NMT output for specific linguistic analysis based on contrastive pairs (Sennrich, 2017). Given a reference output sentence, a contrastive sentence is generated by introducing a mistake related to the phenomenon we are interested in evaluating. The probability that the model assigns to the reference sentence is then compared to that of the contrastive sentence. The accuracy of a model is determined by the percentage of examples in which the reference sentence has a higher probability than the contrastive sentence. We produce contrastive examples by running CoreNLP (Manning et al., 2014) to identify coreferences, which are the primary cause of reentrancies, and introducing a mistake. When an expression has multiple mentions, the antecedent is repeated in the linearized AMR. For instance, the linearization of Figure 1(b) contains the token he twice, which instead appears only once in the sentence. This repetition may result in generating the token he twice, rather than using a pronoun to refer back to it. To investigate this possible mistake, we replace one of the mentions with the antecedent (e.g., John ate the pizza with his fingers is replaced with John ate the pizza with J"
N19-1366,W18-6501,0,0.0841445,"he output embeddings. The encoder is related to the TreeLSTM encoder of Takase et al. (2016), which however encodes labeled trees and does not use a top-down pass. 3.3 Graph Convolutional Network Encoders Graph Convolutional Network (GCN; Duvenaud et al. 2015; Kipf and Welling 2016) is a neural network architecture that learns embeddings of nodes in a graph by looking at its nearby nodes. In Natural Language Processing, GCNs have been used for Semantic Role Labeling (Marcheggiani and Titov, 2017), NMT (Bastings et al., 2017), Named Entity Recognition (Cetoli et al., 2017) and text generation (Marcheggiani and Perez-Beltrachini, 2018). A graph-to-sequence neural network was first introduced by Xu et al. (2018). The authors review the similarities between their approach, GCN and another approach, based on GRUs (Li et al., 2015). The latter recently inspired a graphto-sequence architecture for AMR-to-text generation (Beck et al., 2018). Simultaneously, Song et al. (2018) proposed a graph encoder based on LSTMs. The architectures of Song et al. (2018) and Beck et al. (2018) are both based on the same core computation of a GCN, which sums over the embeddings of the immediate neighborhood of each 3651 e1 node: e2 ! (k+1) hi =σ"
N19-1366,D17-1159,0,0.427363,"he bottom-up pass and h↓i is the hidden state of node xi for the top-down pass. e1:N = h1:N , where ei ∈ Rd , d is the size of the output embeddings. The encoder is related to the TreeLSTM encoder of Takase et al. (2016), which however encodes labeled trees and does not use a top-down pass. 3.3 Graph Convolutional Network Encoders Graph Convolutional Network (GCN; Duvenaud et al. 2015; Kipf and Welling 2016) is a neural network architecture that learns embeddings of nodes in a graph by looking at its nearby nodes. In Natural Language Processing, GCNs have been used for Semantic Role Labeling (Marcheggiani and Titov, 2017), NMT (Bastings et al., 2017), Named Entity Recognition (Cetoli et al., 2017) and text generation (Marcheggiani and Perez-Beltrachini, 2018). A graph-to-sequence neural network was first introduced by Xu et al. (2018). The authors review the similarities between their approach, GCN and another approach, based on GRUs (Li et al., 2015). The latter recently inspired a graphto-sequence architecture for AMR-to-text generation (Beck et al., 2018). Simultaneously, Song et al. (2018) proposed a graph encoder based on LSTMs. The architectures of Song et al. (2018) and Beck et al. (2018) are both based"
N19-1366,W17-7306,0,0.0700129,"Missing"
N19-1366,P02-1040,0,0.106485,"a way to refine the original word representations. We first apply the structural encoder to the input graphs. The GCN or TreeLSTM representations are then fed into the BiLSTM. We call these models GCNS EQ and T REE LSTMS EQ. The motivation behind this approach is that we know that BiLSTMs, given appropriate input embeddings, are very effective at encoding the input sequences. In order to exploit their strength, we do not amend their output but rather provide them with better input embeddings to start with, by explicitly taking the graph relations into account. 5 Experiments We use both BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005) as evaluation metrics.1 We report results on the AMR dataset LDC2015E86 and LDC2017T10. All systems are implemented in PyTorch (Paszke et al., 2017) using the framework OpenNMT-py (Klein et al., 2017). Hyperparameters of each model were tuned on the development set of LDC2015E86. For the GCN components, we use two layers, ReLU activations, and tanh highway layers. We use single layer LSTMs. We train with SGD with the initial learning rate set to 1 and decay to 0.8. Batch size is set to 100.2 We first evaluate the overall performance of the models, after w"
N19-1366,E17-2060,0,0.0137869,"however, overgenerates a wh-clause. Finally, in Example (4) the tree and graph models deal correctly with the possessive pronoun to generate the phrase tell your ex, while the sequential model does not. Overall, we note that the graph model produces a more accurate output than sequential and tree models by generating the correct pronouns and mentions when control verbs and co-references are involved. 5.1.2 Contrastive Pairs For a quantitative analysis of how the different models handle pronouns, we use a method to inspect NMT output for specific linguistic analysis based on contrastive pairs (Sennrich, 2017). Given a reference output sentence, a contrastive sentence is generated by introducing a mistake related to the phenomenon we are interested in evaluating. The probability that the model assigns to the reference sentence is then compared to that of the contrastive sentence. The accuracy of a model is determined by the percentage of examples in which the reference sentence has a higher probability than the contrastive sentence. We produce contrastive examples by running CoreNLP (Manning et al., 2014) to identify coreferences, which are the primary cause of reentrancies, and introducing a mista"
N19-1366,P18-1150,0,0.481882,"e nodes in the graph represent concepts and edges represent semantic relations between them. AMRs are graphs, rather than trees, because co-references and control structures result in nodes with multiple parents, called reentrancies. For instance, the AMR of Figure 1(a) contains a reentrancy between finger and he, caused by the possessive pronoun his. AMR-to-text generation is the task of automatically generating natural language from AMR graphs. Attentive encoder/decoder architectures, commonly used for Neural Machine Translation (NMT), have been explored for this task (Konstas et al., 2017; Song et al., 2018; Beck et al., 2018). In order to use sequence-to-sequence models, Konstas et al. (2017) reduce the AMR graphs to sequences, while Song et al. (2018) and Beck et al. (2018) directly encode them as graphs. Graph encoding allows the model to explicitly encode reentrant structures present in the AMR graphs. While central to AMR, reentrancies are often hard to treat both in parsing and in generation. Previous work either removed them from the graphs, hence obtaining sequential (Konstas et al., 3649 Proceedings of NAACL-HLT 2019, pages 3649–3658 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019"
N19-1366,P15-1150,0,0.0609182,"1:N ), where p(i) is the parent of node xi in the tree. The final hidden states are obtained by concatenating the states from the bottom-up pass and the topdown pass:   hi = h↓i ; h↑i . The hidden state of the root node is usually used as a representation for the entire tree. In order to use attention over all nodes, as in traditional NMT (Bahdanau et al., 2015), we can however build node embeddings by extracting the hidden states of each node in the tree: where ei ∈ Rd , d is the size of the output embeddings. 3.2 TreeLSTM Encoders Tree-Structured Long Short-Term Memory Networks (TreeLSTM; Tai et al. 2015) have been introduced primarily as a way to encode the hierarchical structure of syntactic trees (Tai et al., 2015), but they have also been applied to AMR for the task of headline generation (Takase et al., 2016). TreeLSTMs assume tree-structured input, so AMR graphs must be preprocessed to respect this constraint: reentrancies, which play an essential role in AMR, must be removed, thereby transforming the graphs into trees. We use the Child-Sum variant introduced by Tai et al. (2015), which processes the tree in a bottomup pass. When visiting a node, the hidden states of its children are sum"
N19-1366,D16-1112,0,0.394699,") reduce the AMR graphs to sequences, while Song et al. (2018) and Beck et al. (2018) directly encode them as graphs. Graph encoding allows the model to explicitly encode reentrant structures present in the AMR graphs. While central to AMR, reentrancies are often hard to treat both in parsing and in generation. Previous work either removed them from the graphs, hence obtaining sequential (Konstas et al., 3649 Proceedings of NAACL-HLT 2019, pages 3649–3658 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2017) or tree-structured (Liu et al., 2015; Takase et al., 2016) data, while other work maintained them but did not analyze their impact on performance (e.g., Song et al., 2018; Beck et al., 2018). Damonte et al. (2017) showed that state-of-the-art parsers do not perform well in predicting reentrant structures, while van Noord and Bos (2017) compared different pre- and post-processing techniques to improve the performance of sequenceto-sequence parsers with respect to reentrancies. It is not yet clear whether explicit encoding of reentrancies is beneficial for generation. In this paper, we compare three types of encoders for AMR: 1) sequential encoders, wh"
N19-1397,P13-1020,1,0.874328,"r methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive in nature than abstractive, since most of the words in the summary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects textual units to include in the summary and compresses them by deleting word spans guided by anaphoric constraints to improve coherence. Recently, Zhang et al. (2018) trained an abstractive sentence compression model using attention-based sequence-to-sequence architecture (Ru"
N19-1397,P11-1049,0,0.175783,"Missing"
N19-1397,P18-1063,0,0.127952,"Missing"
N19-1397,P16-1046,0,0.661102,"nt summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. I"
N19-1397,R13-1027,0,0.0258907,"Missing"
N19-1397,W03-0501,0,0.109136,"Missing"
N19-1397,P16-1188,0,0.084033,"ummary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects textual units to include in the summary and compresses them by deleting word spans guided by anaphoric constraints to improve coherence. Recently, Zhang et al. (2018) trained an abstractive sentence compression model using attention-based sequence-to-sequence architecture (Rush et al., 2015) to map a sentence in the document selected by the extractive model to a sentence in the summary. However, as the sentences in the document and in the summary are not aligned for compression, their compression model is significantly inferior to the extractive model. In thi"
N19-1397,W18-2706,0,0.021044,"ummaries derived automatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from signif"
N19-1397,D15-1042,0,0.0902161,"Missing"
N19-1397,D13-1155,0,0.0222192,"rayan et al., 2018c). To build a compressive oracle, we trained a supervised sentence labeling classifier, adapted from 3959 Oracle Extractive Oracle Compressive Oracle R1 54.67 57.12 R2 30.37 32.59 RL 50.81 53.27 Table 1: Oracle scores obtained for the CNN and DailyMail testsets. We report ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) F1 scores. the Transition-Based Chunking Model (Lample et al., 2016), to annotate spans in every sentence that can be dropped in the final summary. We used the publicly released set of 10,000 sentencecompression pairs from the Google sentence compression dataset (Filippova and Altun, 2013; Filippova et al., 2015) for training. After tagging all sentences in the CNN and DailyMail corpora using this compression model, we generated oracle compressive summaries based on the best average of ROUGE-1 (R1) and ROUGE-2 (R2) F1 scores from the combination of all possible sentences and all removals of the marked compression chunks. To verify the adequacy of our proposed oracles, we show in Table 1 a comparison of their scores. Our compressive oracle achieves much better scores than the extractive oracle, because of its capability to make summaries concise. Moreover, the linguistic qualit"
N19-1397,D18-1443,0,0.100831,". They select sentences based on an LSTM classifier that predicts a binary label for each sentence (Cheng and Lapata, 2016), based on ranking using reinforcement learning (Narayan et al., 2018c), or even by training an extractive latent model (Zhang et al., 2018). Other methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive in nature than abstractive, since most of the words in the summary are copied from the document (Gehrmann et al., 2018). Due to the lack of training corpora, there is almost no work on neural architectures for compressive summarization. Most compressive summarization work has been applied to smaller datasets (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Almeida and Martins, 2013). Other non-neural summarization systems apply this idea to select and compress the summary. Dorr et al. (2003) introduced a method to first extract the first sentence of a news article and then use linguistically-motivated heuristics to iteratively trim parts of it. Durrett et al. (2016) also learns a system that selects tex"
N19-1397,N18-1065,0,0.319626,"Missing"
N19-1397,P18-1013,0,0.0974388,"Missing"
N19-1397,N16-1030,0,0.0472101,"mmaries prior to training using two types of oracles. We used an extractive oracle to identify the set of sentences which collectively gives the highest ROUGE (Lin and Hovy, 2003) with respect to the gold summary (Narayan et al., 2018c). To build a compressive oracle, we trained a supervised sentence labeling classifier, adapted from 3959 Oracle Extractive Oracle Compressive Oracle R1 54.67 57.12 R2 30.37 32.59 RL 50.81 53.27 Table 1: Oracle scores obtained for the CNN and DailyMail testsets. We report ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) F1 scores. the Transition-Based Chunking Model (Lample et al., 2016), to annotate spans in every sentence that can be dropped in the final summary. We used the publicly released set of 10,000 sentencecompression pairs from the Google sentence compression dataset (Filippova and Altun, 2013; Filippova et al., 2015) for training. After tagging all sentences in the CNN and DailyMail corpora using this compression model, we generated oracle compressive summaries based on the best average of ROUGE-1 (R1) and ROUGE-2 (R2) F1 scores from the combination of all possible sentences and all removals of the marked compression chunks. To verify the adequacy of our proposed"
N19-1397,N18-2009,0,0.063515,"Missing"
N19-1397,N03-1020,0,0.659093,"Missing"
N19-1397,W09-1801,1,0.936062,"2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture that attempts to mitigate the problems above via a middle ground, compressive summarization (Martins and Smith, 2009). Our model selects a set of sentences from the input document, and 3955 Proceedings of NAACL-HLT 2019, pages 3955–3966 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics compresses them by removing unnecessary words, while keeping the summaries informative, concise and grammatical. We achieve this by dynamically modeling the generated summary using a Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to produce summary state representations. This state provides crucial information to iteratively increment summaries based on previously"
N19-1397,E06-1038,0,0.169088,"Missing"
N19-1397,K16-1028,0,0.0784861,"Missing"
N19-1397,P18-1188,1,0.119729,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,D18-1206,1,0.135235,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,N18-1158,1,0.111916,"summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive systems are wasteful since they cannot trim the original sentences to fit into the summary, and they lack a mechanism to ensure overall coherence. In contrast, abstractive systems require natural language generation and semantic representation, problems that are inherently harder to solve than just extracting sentences from the original document. In this paper, we present a novel architecture"
N19-1397,N18-2102,0,0.127757,"utomatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods suffer from significant drawbacks: extractive"
N19-1397,D15-1044,0,0.825797,"for each sentence, and a set of compressed oracle summaries (see §4). Experimental results show that when evaluated automatically, both the extractive and compressive variants of our model provide state-of-the-art results. Human evaluation further shows that our model is better than previous state-of-the-art systems at generating informative and concise summaries. 2 Related Work Recent work on neural summarization has mainly focused on sequence-to-sequence (seq2seq) architectures (Sutskever et al., 2014), a formulation particularly suited and initially employed for abstractive summarization (Rush et al., 2015). However, state-of-the-art results have been achieved by RNN-based methods which are extractive. They select sentences based on an LSTM classifier that predicts a binary label for each sentence (Cheng and Lapata, 2016), based on ranking using reinforcement learning (Narayan et al., 2018c), or even by training an extractive latent model (Zhang et al., 2018). Other methods rely on an abstractive approach with strongly conditioned generation on the source document (See et al., 2017). In fact, the best results for abstractive summarization have been achieved with models that are more extractive i"
N19-1397,E17-2007,0,0.0363789,"after every 5,000 batches. We trained with Adam (Kingma and Ba, 2015) with an initial learning rate of 0.001. Our system was implemented using DyNet (Neubig et al., 2017). 4.3 Model Evaluation We evaluated summarization quality using F1 ROUGE (Lin and Hovy, 2003). We report results 4 We show examples of both oracles in Appendix §A.1. in terms of unigram and bigram overlap (R1) and (R2) as a means of assessing informativeness, and the longest common subsequence (RL) as a means 5 of assessing fluency. In addition to ROUGE, which can be misleading when used as the only means to assess summaries (Schluter, 2017), we also conducted a question-answering based human evaluation to assess the informativeness of our summaries in their ability to preserve key informa6 tion from the document (Narayan et al., 2018c). First, questions are written using the gold summary, we then examined how many questions participants were able to answer by reading system 7 summaries alone, without access to the article. Figure 5 shows a set of candidate summaries along with questions used for this evaluation. 4.4 Model and Baselines We evaluated our model E X C ON S UMM in two settings: Extractive (selects sentences to assemb"
N19-1397,P17-1099,0,0.476379,"acle compressive summaries derived automatically from the CNN/DailyMail 1 reference summaries. 1 Figure 1: Summaries produced by our model. For illustration, the compressive summary shows the removed spans strike-through. Introduction Text summarization is an important NLP problem with a wide range of applications in data-driven industries (e.g., news, health, and defense). Single document summarization—the task of generating a short summary of a document preserving its informative content (Sp¨arck Jones, 2007)—has been a highly studied research topic in recent years (Nallapati et al., 2016b; See et al., 2017; Fan et al., 2018; Pasunuru and Bansal, 2018). Modern approaches to single document summarization using neural network architectures 1 Our dataset and code is available at https:// github.com/Priberam/exconsumm. ∗ Now at Google London. have primarily focused on two strategies: extractive and abstractive. The former select a subset of the sentences to assemble a summary (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018a,c). The latter generates sentences that do not appear in the original document (See et al., 2017; Narayan et al., 2018b; Paulus et al., 2018). Both methods"
N19-1397,P17-1108,0,0.102084,"Missing"
N19-1397,D18-1088,0,0.509737,"presses them by removing unnecessary words, while keeping the summaries informative, concise and grammatical. We achieve this by dynamically modeling the generated summary using a Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to produce summary state representations. This state provides crucial information to iteratively increment summaries based on previously extracted information. It also facilitates the generation of variable length summaries as opposed to fixed lengths, in previous extractive systems (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018c; Zhang et al., 2018). Our model can be trained in both extractive (labeling sentences for extraction) or compressive (labeling words for extraction) settings. Figure 1 shows a summary example generated by our model. Our contributions in this paper are three-fold: • we present the first end-to-end neural architecture for EXtractive and COmpressive Neural SUMMarization (dubbed E X C ON S UMM, see §3), • we validate this architecture on the CNN/DailyMail and the Newsroom datasets (Hermann et al., 2015; Grusky et al., 2018), showing that our model generates variablelength summaries which correlate well with gold summ"
P09-2001,P09-2001,1,0.0512899,"Missing"
P09-2001,P04-1061,0,0.0881175,", observed data x, ˜ r : N → 24r annealing schedule 4 Output: learned parameters α and approximate posterior q(θ, y) t ← 1; repeat E-step: repeat (t+1) E-step: forall i ∈ [r] do: qi (y) ← argmax = i=1 q(y)∈Qi P (t) (t) F 0 ( j6=i λj qi (θ)q(y) + λi qi q(y), α(t) ) (t+1) M-step: forall i ∈ [r] do: qi q(θ)∈Qi ˜ r (t) λ∈4 3 until convergence ; M-step: α(t+1) ← P (t+1) (t+1) argmax F 0 ( ri=1 λi qi (θ)qi (y), α) Experiments We tested our method on the unsupervised learning problem of dependency grammar induction. For the generative model, we used the dependency model with valence as it appears in Klein and Manning (2004). We used the data from the Chinese treebank (Xue et al., 2004). Following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure, and sentences of length more than 10 were removed from the set. We experimented with a Dirichlet prior over the parameters and logistic normal priors over the parameters, and found the latter to still be favorable with our method, as in Cohen et al. (2008). We therefore report results with our method only for the logistic normal prior. We do inference on sections 1–270"
P09-2001,P92-1017,0,0.198447,"r by constraining it to be from a mixture family of distributions. We will use x to denote observable random variables, y to denote hidden structure, and θ to denote the to-be-learned parameters of the model (coming from a subset of R` for some `). α will denote the parameters of a prior over θ. The mean-field assumption in the Bayesian setting assumes that the posterior has a factored form: Introduction Learning natural language in an unsupervised way commonly involves the expectation-maximization (EM) algorithm to optimize the parameters of a generative model, often a probabilistic grammar (Pereira and Schabes, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith an"
P09-2001,P06-1072,1,0.948292,"s, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. Grac¸a et al. (2007) added linear constraints on expected values of features of the hidden variables in an alignment task. In this paper, we use posterior mixtures to inject bias or prior knowledge into a Bayesian model. q(θ, y) = q(θ)q(y) (1) Traditionally, variational inference with the meanfield assumption alternates between an E-step which optimizes q(y) and then an M-step which optimizes q(θ).1 The mean-field assumption makes inf"
P10-1152,N09-1009,1,0.873848,"ns and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in"
P10-1152,J03-4003,0,0.549798,"lihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and v"
P10-1152,P08-2007,0,0.39949,"methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P 6= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training. We th"
P10-1152,N07-1018,0,0.0277163,"ions, we showed that solving Viterbi training is hard, and therefore requires an approximation algorithm. Viterbi EM, which is an example of such algorithm, is dependent on an initialization of either θ to start with an E-step or z to start with an M-step. In the absence of a betterinformed initializer, it is reasonable to initialize z using a uniform distribution over D(G, xi ) for each i. If D(G, xi ) is finite, it can be done efficiently by setting θ = 1 (ignoring the normalization constraint), running the inside algorithm, and sampling from the (unnormalized) posterior given by the chart (Johnson et al., 2007). We turn next to an analysis of this initialization technique that suggests it is well-motivated. The sketch of our result is as follows: we first give an asymptotic upper bound for the loglikelihood of derivations and sentences. This bound, which has an information-theoretic interpretation, depends on a parameter λ, which depends on the distribution from which the derivations were chosen. We then show that this bound is minimized when we pick λ such that this distribution is (conditioned on the sentence) a uniform distribution over derivations. Let q(x) be any distribution over L(G) and θ so"
P10-1152,J99-4005,0,0.0622948,"ns++. They show that their initialization is O(log k)-competitive; i.e., it approximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|Lλ2 /n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and λ is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free gramma"
P10-1152,N06-1020,0,0.0609822,"necessary to resort to approximate algorithms like Viterbi EM. Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the Estep finds the mode for each xi ’s derivation, argmaxz∈D(G,xi ) p(xi , z |θ). We will refer to L(θ, z) = n Y p(xi , zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT V Input: A fo"
P10-1152,P06-1043,0,0.0329954,"necessary to resort to approximate algorithms like Viterbi EM. Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the Estep finds the mode for each xi ’s derivation, argmaxz∈D(G,xi ) p(xi , z |θ). We will refer to L(θ, z) = n Y p(xi , zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT V Input: A fo"
P10-1152,W07-2216,0,0.0228016,"clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|Lλ2 /n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and λ is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. We gave motivation for unif"
P10-1152,C96-2215,0,0.167451,"Missing"
P10-1152,J07-4003,1,0.775492,"on 1. Indeed, UniformInit uses θ I to initialize the state of Viterbi EM. We note that if θ I was known for a specific grammar, then we could have used it as a direct initializer. However, Condition 1 only guarantees its existence, and does not give a practical way to identify it. In general, as mentioned above, θ = 1 can be used to obtain a weighted CFG that satisfies p(z |θ, x) = 1/|D(G, x)|. Since we require a uniform posterior distribution, the number of derivations of a fixed length is finite. This means that we can converted the weighted CFG with θ = 1 to a PCFG with the same posterior (Smith and Johnson, 2007), and identify the appropriate θ I . 8 Related Work Viterbi training is closely related to the k-means clustering problem, where the objective is to find k centroids for a given set of d-dimensional points such that the sum of distances between the points and the closest centroid is minimized. The analog for Viterbi EM for the k-means problem is the k-means clustering algorithm (Lloyd, 1982), a coordinate ascent algorithm for solving the k-means problem. It works by iterating between an E-likestep, in which each point is assigned the closest centroid, and an M-like-step, in which the centroids"
P10-1152,W10-2902,0,0.272166,"re based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P 6= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training. We then describe a “competitive"
P10-1152,E06-1004,0,0.0252116,"proximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|Lλ2 /n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and λ is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. W"
P10-1152,D07-1003,1,0.908095,"dels have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P 6= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well"
P10-1152,N07-1009,0,0.15986,"hms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P 6= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this know"
P10-1152,J93-2004,0,\N,Missing
P10-1152,W08-0704,0,\N,Missing
P10-1152,N09-1012,0,\N,Missing
P10-1152,P07-1094,0,\N,Missing
P10-1152,N09-1036,0,\N,Missing
P10-1152,P08-1046,0,\N,Missing
P10-1152,P09-1012,0,\N,Missing
P10-1152,P04-1061,0,\N,Missing
P10-1152,D07-1072,0,\N,Missing
P10-1152,P96-1024,0,\N,Missing
P12-1024,P96-1024,0,0.812379,"rm for calculation of p(r1 . . . rN ). 1. For a given p(r1 . . . rN ). s-tree r1 . . . rN , calculate 2. For a given input sentence x = x1 . . . xN , calculate the marginal probabilities X µ(a, i, j) = p(τ ) τ ∈T (x):(a,i,j)∈τ for each non-terminal a ∈ N , for each (i, j) such that 1 ≤ i ≤ j ≤ N . Here T (x) denotes the set of all possible s-trees for the sentence x, and we write (a, i, j) ∈ τ if nonterminal a spans words xi . . . xj in the parse tree τ . The marginal probabilities have a number of uses. Perhaps most importantly, for a given sentence x = x1 . . . xN , the parsing algorithm of Goodman (1996) can be used to find X arg max µ(a, i, j) τ ∈T (x) (a,i,j)∈τ This is the parsing algorithm used by Petrov et al. (2006), for example. In addition, we can calculate the probability for anPinput sentence, p(x) = P τ ∈T (x) p(τ ), as p(x) = a∈I µ(a, 1, N ). Variants of the inside-outside algorithm can be used for problems 1 and 2. This section introduces a novel form of these algorithms, using tensors. This is the first step in deriving the spectral estimation method. The algorithms are shown in figures 2 and 3. Each algorithm takes the following inputs: 5 Tensor Form of the Inside-Outside Algori"
P12-1024,E12-1042,0,0.119777,"The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation 4 L-PCFGs: Basic Definitions This section gives a definition of the L-PCFG formalism used in this paper. An L-PCFG is a 5-tuple (N , I, P, m, n) where: • N is the set of non-terminal symbols in the grammar. I ⊂ N is a finite set of in-terminals. P ⊂ N is a finite set of pre-terminals. We assume that N = I ∪ P, and I ∩ P = ∅. Hence we have partitioned the set of non-terminals into two subsets. • [m] is the set of possi"
P12-1024,P05-1010,0,0.93,"ar value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of the training examples down"
P12-1024,P92-1017,0,0.508816,"tion for Computational Linguistics, pages 223–231, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation 4 L-PCFGs: Basic Definitions This section gives a definition of the L-"
P12-1024,P06-1055,0,0.878323,"in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/σ, where σ is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of t"
P12-1024,J98-4004,0,\N,Missing
P12-1024,J93-2004,0,\N,Missing
P12-1024,P97-1003,1,\N,Missing
P12-1024,P03-1054,0,\N,Missing
P13-1102,J98-2005,0,0.0546119,"on a mean-field approximation, and Johnson et al. (2007) proposed MCMC samplers for the posterior distribution over rule probabilities and the parse trees of the training data strings. PCFGs have the interesting property (which we expect most linguistically more realistic models to also possess) that the distributions they define are not always properly normalized or “tight”. In a non-tight PCFG the partition function (i.e., sum of the “probabilities” of all the trees generated by the PCFG) is less than one. (Booth and Thompson, 1973, called such non-tight PCFGs “inconsistent”, but we follow Chi and Geman (1998) in calling them “non-tight” to avoid confusion with the consistency of statistical estimators). Chi (1999) showed that renormalized nontight PCFGs (which he called “Gibbs CFGs”) define the same class of distributions over trees as do tight PCFGs with the same rules, and provided an algorithm for mapping any PCFG to a tight PCFG with the same rules that defines the same distribution over trees. An obvious question is then: how does tightness affect the inference of PCFGs? Chi and Geman (1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML estimates are always t"
P13-1102,J99-1004,0,0.0589929,"rule probabilities and the parse trees of the training data strings. PCFGs have the interesting property (which we expect most linguistically more realistic models to also possess) that the distributions they define are not always properly normalized or “tight”. In a non-tight PCFG the partition function (i.e., sum of the “probabilities” of all the trees generated by the PCFG) is less than one. (Booth and Thompson, 1973, called such non-tight PCFGs “inconsistent”, but we follow Chi and Geman (1998) in calling them “non-tight” to avoid confusion with the consistency of statistical estimators). Chi (1999) showed that renormalized nontight PCFGs (which he called “Gibbs CFGs”) define the same class of distributions over trees as do tight PCFGs with the same rules, and provided an algorithm for mapping any PCFG to a tight PCFG with the same rules that defines the same distribution over trees. An obvious question is then: how does tightness affect the inference of PCFGs? Chi and Geman (1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML estimates are always tight for both the supervised case (where the input consists of parse trees) and the unsupervised case (wher"
P13-1102,J12-3003,1,0.849779,"ngineering perspectives. Cognitively it is implausible that children can perceive the parse trees of the language they are learning, but it is more reasonable to assume that they can obtain the terminal strings or yield of these trees. Unsupervised methods for learning a grammar from terminal strings alone is also interesting from an engineering perspective because such training data is cheap and plentiful, while Mark Johnson Department of Computing Macquarie University mark.johnson@mq.edu.au the manually parsed data required by supervised methods are expensive to produce and relatively rare. Cohen and Smith (2012) show that inferring PCFG rule probabilities from strings alone is computationally intractable, so we should not expect to find an efficient, general-purpose algorithm for the unsupervised problem. Instead, approximation algorithms are standardly used. For example, the InsideOutside (IO) algorithm efficiently implements the Expectation-Maximization (EM) procedure for approximating a Maximum Likelihood estimator (Lari and Young, 1990). Bayesian estimators for PCFG rule probabilities have also been attracting attention because they provide a theoretically-principled way of incorporating prior in"
P13-1102,N07-1018,1,0.807749,"Columbia University scohen@cs.columbia.edu Abstract Probabilistic context-free grammars have the unusual property of not always defining tight distributions (i.e., the sum of the “probabilities” of the trees the grammar generates can be less than one). This paper reviews how this non-tightness can arise and discusses its impact on Bayesian estimation of PCFGs. We begin by presenting the notion of “almost everywhere tight grammars” and show that linear CFGs follow it. We then propose three different ways of reinterpreting non-tight PCFGs to make them tight, show that the Bayesian estimators in Johnson et al. (2007) are correct under one of them, and provide MCMC samplers for the other two. We conclude with a discussion of the impact of tightness empirically. 1 Introduction Probabilistic Context-Free Grammars (PCFGs) play a special role in computational linguistics because they are perhaps the simplest probabilistic models of hierarchical structures. Their simplicity enables us to mathematically analyze their properties to a detail that would be difficult with linguistically more accurate models. Such analysis is useful because it is reasonable to expect more complex models to exhibit similar properties"
P13-1102,P04-1061,0,0.0302015,"the state-of-the-art in unsupervised grammar induction, but to try to measure empirical differences in the estimates produced by the three different approaches to tightness just described. The bottom line of our experiments is that we could not detect any significant difference in the estimates produced by samplers for these three different approaches. In our experiments we used the English Penn treebank (Marcus et al., 1993). We use the part-ofspeech tag sequences of sentences shorter than 11 words in sections 2–21. The grammar we use is the PCFG version of the dependency model with valence (Klein and Manning, 2004), as it appears in Smith (2006). We used a symmetric Dirichlet prior with hyperparameter α = 0.1. For each of the three approaches for handling tightness, we ran 100 times the samplers in §7, each for 1,000 iterations. We discarded the first 900 sweeps of each run, and calculated the F1 -scores of the sampled trees every 10th sweep from the last 100 sweeps. For each run we calculated the average F1 -score over the 10 sweeps we evaluated. We thus have 100 average F1 -scores for each of the samplers. Figure 1 plots the density of F1 scores (compared to the gold standard) resulting from the Gibbs"
P13-1102,J93-2004,0,0.0415775,"in unsupervised grammar induction In this section we present experiments using the three samplers just described in an unsupervised grammar induction problem. Our goal here is not to improve the state-of-the-art in unsupervised grammar induction, but to try to measure empirical differences in the estimates produced by the three different approaches to tightness just described. The bottom line of our experiments is that we could not detect any significant difference in the estimates produced by samplers for these three different approaches. In our experiments we used the English Penn treebank (Marcus et al., 1993). We use the part-ofspeech tag sequences of sentences shorter than 11 words in sections 2–21. The grammar we use is the PCFG version of the dependency model with valence (Klein and Manning, 2004), as it appears in Smith (2006). We used a symmetric Dirichlet prior with hyperparameter α = 0.1. For each of the three approaches for handling tightness, we ran 100 times the samplers in §7, each for 1,000 iterations. We discarded the first 900 sweeps of each run, and calculated the F1 -scores of the sampled trees every 10th sweep from the last 100 sweeps. For each run we calculated the average F1 -sc"
P14-1061,W03-1812,0,0.0281587,"to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs"
P14-1061,D10-1115,0,0.0291917,"nes of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of"
P14-1061,D13-1147,0,0.0157785,"ositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of multiple sub-sets of the predicate’s words (LCs). We use it to tackle a supervised prediction task that represents predicates distributionally. Our model assumes a latent distribution over the LCs, and estimates its parameters so to best conf"
P14-1061,P11-1062,0,0.424813,"difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of ar"
P14-1061,Q13-1015,1,0.929241,"between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into account. We present a novel approach to the task that models the selection and relative weighting of the predicate"
P14-1061,W11-1304,0,0.0207168,"fication of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositio"
P14-1061,P98-2127,0,0.0575403,"Missing"
P14-1061,P99-1041,0,0.0817136,"ultiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly b"
P14-1061,W02-0109,0,0.0615685,"Missing"
P14-1061,W03-1810,0,0.0446691,"Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess"
P14-1061,P13-1131,0,0.299606,"applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the r"
P14-1061,C10-2029,0,0.0623497,"Missing"
P14-1061,P13-2051,0,0.0727332,"applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the r"
P14-1061,E09-1025,0,0.0154396,"icate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 200"
P14-1061,D11-1142,0,0.0351763,"This section lists the features used for our experiments. We intentionally select a feature set that relies on either completely unsupervised or shallow processing tools that are available for a wide variety of languages and domains. Given a predicate pair p(i) , a label y ∈ {1, −1} and a latent state h ∈ H (i) , we define their feature vector as Φ(p(i) , y, h) = y · Φ(p(i) , h). The computation of Φ(p(i) , h) requires a reference corpus R that contains triplets of the type (p, x, y) where p is a binary predicate and x and y are its arguments. We use the Reverb corpus as R in our experiments (Fader et al., 2011; see Section 4). We refrain from encoding features that directly reflect the vocabulary of the training set. Such features are not applicable beyond that set’s vocabulary, and as available datasets contain no more than a few thousand examples, these features are unlikely to generalize well. Table 1 presents the set of features we use in our experiments. The features can be divided into two main categories: similarity features between the LHS and the RHS predicates (table’s top), and features that reflect the individual properties of each ∇L = Eh [Φ(pi , yi , h)] − Eh,y [Φ(pi , y, h)] − λ · w"
P14-1061,P06-2075,0,0.0230173,"te inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and"
P14-1061,E06-1043,0,0.0497327,"Missing"
P14-1061,D07-1110,0,0.0218242,"Missing"
P14-1061,D13-1060,0,0.0132378,"tt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWE"
P14-1061,P13-2046,0,0.0173758,"bject of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional"
P14-1061,P02-1006,0,0.00907062,"h addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it gener"
P14-1061,W03-1011,0,0.152252,"pic|hL ) for each of the induced topics. The entropy of the topic distribution P (topic|hL ) Table 1: The feature set used in our experiments. The top part presents the similarity measures based on the DIRT approach. The rest of the listed features apply to the LHS predicate (hL ), and to the first word in it (hA L ). Analogous features are A introduced for the second word, hB L , and for the RHS predicate. The upper-middle part presents the word features for hL . The lower-middle part presents features that apply where hL is of size 2. The bottom part lists the LDA-based features. (1998) and Weeds and Weir (2003) did not yield additional improvements. We encode the similarity of all measures for the A pair hL and hR as well as the pair hA L and hR . The latter feature is an approximation to the similarity between the heads of the predicates, as heads in English tend to be to the left of the predicates. These two features coincide for h values of size 1. Word and Pair Features. These features encode the basic properties of the LC. The motivation behind them is to allow a more accurate leveraging of the similarity features, as well as to better determine the relative weights of h ∈ H (i) . The feature s"
P14-1061,I11-1024,0,0.02046,"rm treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of multiple sub-sets of the predicate’s words (LCs). We use it to tackle a supervised prediction task that represents predicates distributionally. Our model assumes a latent distribution over the"
P14-1061,D12-1018,0,0.153469,"presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a) used topic models to combine typelevel predicate inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtaine"
P14-1061,P10-1044,0,0.0605228,"Missing"
P14-1061,P06-1107,0,0.0264756,"to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation"
P14-1061,D10-1106,0,0.510611,"nd consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally hold"
P14-1061,P12-2031,0,0.683025,"selection and relative weighting of the predicate’s LCs using latent variables. This approach allows the classifier that uses the distributional representations to take into account the most relevant LCs in order to make the prediction. By doing so, we avoid the notoriously difficult problem of defining and identifying MWPs and account for predicates of various sizes and degrees of compositionality. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics. We conduct experiments on the dataset of Zeichner et al. (2012) and compare our methods with analogous ones that select a fixed LC, using stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (200"
P14-1061,I05-5011,0,0.0293266,"main difficult to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said t"
P14-1061,N06-1039,0,0.0171201,"paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distri"
P14-1061,C08-1107,0,0.405935,"ms (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into"
P14-1061,W11-0807,0,0.0965207,"it, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used composi"
P14-1061,P06-4018,0,\N,Missing
P14-1061,P10-1045,0,\N,Missing
P14-1061,C98-2122,0,\N,Missing
P14-1099,J93-2004,0,0.050709,"initializer for the EM algorithm for LPCFGs. Two-step estimation methods such as these are well known in statistics; there are guarantees for example that if the first estimator is consistent, and the second step finds the closest local maxima of the likelihood function, then the resulting estimator is both consistent and efficient (in terms of number of samples required). See for example page 453 or Theorem 4.3 (page 454) of (Lehmann and Casella, 1998). 7 Experiments on Parsing This section describes parsing experiments using the learning algorithm for L-PCFGs. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 was used as the test set. The experimental setup is the same as described by Cohen et al. (2013). The trees are binarized (Petrov et al., 2006) and for the EM algorithm we use the initialization method described m EM Spectral Pivot Pivot+EM 8 86.69 40 85.60 83.56 86.83 2 sec. 16 88.32 30 87.77 86.00 88.14 6 22 24 88.35 30 88.53 86.87 88.64 2 32 88.56 20 88.82 86.40 88.55 2 sec. 23 87.76 88.05 85.83 88.03 Table 1: Results on the development data (section 22) and test d"
P14-1099,P05-1010,0,0.140712,"imated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice. 1 2) Optimization of a convex objective function using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like E"
P14-1099,P06-1055,0,0.708959,"ees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice. 1 2) Optimization of a convex objective function using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like EM, returns estimates o"
P14-1099,W97-0309,0,0.146933,"o latent variable models. We apply the matrix decomposition algorithm to a co-occurrence matrix that can be estimated directly from a training set consisting of parse trees without latent annoThe algorithm provably learns the parameters of an L-PCFG (theorem 1), under an assumption that each latent state has at least one “pivot” feature. This assumption is similar to the “pivot word” assumption used by Arora et al. (2013) and Arora et al. (2012) in the context of learning topic models. We describe experiments on learning of LPCFGs, and also on learning of the latent-variable language model of Saul and Pereira (1997). A hybrid method, which uses our algorithm as an initializer for EM, performs at the same accuracy as EM, but requires significantly fewer iterations for convergence: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in"
P14-1099,P12-1024,1,0.893218,"the algorithm is efficient and effective in practice. 1 2) Optimization of a convex objective function using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like EM, returns estimates of the original parameters of an LPCFG, but, unlike EM, does not suffer from problems of local optima. The algorithm relies on two key ideas: 1) A matrix decomposition algorithm (section 5) which is Papplicable to matrices Q of the form Qf"
P14-1099,N13-1015,1,0.945585,"eloped provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics linear transformation, which cancels in the insideoutside calculations for marginalization over latent states in the L-PCFG. The lack of direct parameter estimates from this method leads to problems with negative or unnormalized probablities; the method does not give parameters"
P14-1099,D12-1019,1,0.844255,"convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Ling"
P14-1100,D13-1059,0,0.0147988,"s to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to provide a first step to bridging this theoryexperiment gap. More specifically, we approach unsupervised constituent parsing from the perspective of structure learning as opposed to parameter learning. We associate each sentence with an undirected latent tree graphical model, which is a tree consisting of both observed variables (corresponding to the words in the sentence) and an additional set of latent variables that a"
P14-1100,J92-4003,0,0.134004,"9 CCM-UB 62.9 23.7 19.1 16.6 15.2 13.8 Table 1: Comparison of different CCM variants on English (training). U stands for universal POS tagset, OB stands for conjoining original POS tags with Brown clusters and UB stands for conjoining universal POS tags with Brown clusters. The best setting is just the vanilla setting, CCM. • Otherwise find the first non-participle verb (say at index j) and return ([0, j − 1], [j, `(x)]). • If no verb exists, return ([0, 1], [1, `(x)]). Word embeddings As mentioned earlier, each wi can be an arbitrary feature vector. For all languages we use Brown clustering (Brown et al., 1992) to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity. For English, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings Turian et al. (2010) of length 50. We also explored two types of CCA embeddings: OSCCA and TSCCA, given in Dhillon et al. (2012). The OSCCA embeddings behaved better, so we only report its results. Choice of kernel For our experiments, we use the kernel Kγ (j, k, j 0 , k 0 |x, x0 )   κ(j, k, j 0 ,"
P14-1100,N09-1009,1,0.888628,"ly annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These"
P14-1100,J12-3003,1,0.833961,"k et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or ma"
P14-1100,P12-1024,1,0.811325,"atrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only restriction is in the form of the above assumption. If wi and zi were discrete, represented as binary vectors, the above assumption would correspond to requiring all conditional probability tables in the latent tree to have rank m. Assumption 1 allows for the wi to be high dimensional features, as long as the expectation requirement above is satisfied. Similar assumptions are made with spectral parameter learning methods e.g. Hsu et al. (2009), Bailly et al. (2009), Parikh et al. (2011), and Cohen et al. (2012). Furthermore, Assumption 1 makes it explicit that regardless of the size of p, the relationships among the variables in the latent tree are restricted to be of rank m, and are thus low rank since p > m. To leverage this low rank structure, we propose using the following additive metric, a normalized variant of that in Anandkumar et al. (2011): dspectral (i, j) = − log Λm (Σx (i, j)) + 12 log Λm (Σx (i, i)) + 12 log Λm (Σx (j, j)) (5) where Λm (A) denotes the product of the top m singular values of A and Σx (i, j) := E[vi vj> |x], i.e. the uncentered cross-covariance matrix. We can then show t"
P14-1100,P99-1059,0,0.414016,"g et al., 2011; Anandkumar et al., 2011; Ishteva et al., 2012). Additive tree metrics can be leveraged by “meta-algorithms” such as neighbor-joining (Saitou and Nei, 1987) and recursive grouping (Choi et al., 2011) to provide consistent learning algorithms for latent trees. Moreover, we show that it is desirable to learn the “minimal” latent tree based on the tree metric (“minimum evolution” in phylogenetics). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community (Zhou et al., 2010; Kolar et al., 2010b; Kolar et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of"
P14-1100,P10-2036,0,0.0410088,"Missing"
P14-1100,N12-1069,0,0.0495455,"likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to prov"
P14-1100,P12-2004,0,0.129699,"unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other"
P14-1100,P13-1044,0,0.089759,"ning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoreti"
P14-1100,N09-1012,0,0.0441815,"red for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while emp"
P14-1100,P02-1017,0,0.54601,"oach is grammarless – we directly learn the bracketing structure of a given sentence without using a grammar model. The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples. Although finding the “minimal” latent tree is NP-hard in general, for the case of projective trees we find that it can be found using bilexical parsing algorithms. Empirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. 1 Introduction Solutions to the problem of grammar induction have been long sought after since the early days of computational linguistics and are interesting both from cognitive and engineering perspectives. Cognitively, it is more plausible to assume that children obtain only terminal strings of parse trees and not the actual parse trees. This means the unsupervised setting is a better model for studying language acquisition. From the engineering perspective, training data for unsupervised parsing exists in abundance (i.e. sentences and part-of-s"
P14-1100,J93-2004,0,0.0460967,"d from Sparse Data We now address the data sparsity problem, in particular that D(x) can be very small, and therefore estimating d for each POS sequence separately can be problematic.3 In order to estimate d from data, we need to estimate the covariance matrices Σx (i, j) (for i, j ∈ {1, . . . , `(x)}) from Eq. 5. To give some motivation to our solution, consider estimating the covariance matrix Σx (1, 2) for the tag sequence x = (DT1 , NN2 , VBD3 , DT4 , NN5 ). D(x) may be insufficient for an accurate empirical es3 This data sparsity problem is quite severe – for example, the Penn treebank (Marcus et al., 1993) has a total number of 43,498 sentences, with 42,246 unique POS tag sequences, averaging |D(x) |to be 1.04. 1067 timate. However, consider another sequence x0 = (RB1 , DT2 , NN3 , VBD4 , DT5 , ADJ6 , NN7 ). Although x and x0 are not identical, it is likely that Σx0 (2, 3) is similar to Σx (1, 2) because the determiner and the noun appear in similar syntactic context. Σx0 (5, 7) also may be somewhat similar, but Σx0 (2, 7) should not be very similar to Σx (1, 2) because the noun and the determiner appear in a different syntactic context. The observation that the covariance matrices depend on lo"
P14-1100,P07-1049,0,0.152566,"et al., 2010a). This allows principled sharing of samples from different but similar underlying distributions. We provide theoretical guarantees on the recovery of the correct underlying latent tree and characterize the associated sample complexity under our technique. Empirically we evaluate our method on data in English, German and Chinese. Our algorithm performs favorably to Klein and Manning’s (2002) constituent-context model (CCM), without the need for careful initialization. In addition, we also analyze CCM’s sensitivity to initialization, and compare our results to Seginer’s algorithm (Seginer, 2007). 2 Learning Setting and Model In this section, we detail the learning setting and a conditional tree model we learn the structure for. 2.1 Learning Setting Let w = (w1 , ..., w` ) be a vector of words corresponding to a sentence of length `. Each wi is represented by a vector in Rp for p ∈ N. The vector is an embedding of the word in some space, choVBD DT NN VBD DT NN Figure 2: Candidate constituent parses for x = (VBD, DT, NN) (left-correct, right -incorrect) sen from a fixed dictionary that maps word types to Rp . In addition, let x = (x1 , ..., x` ) be the associated vector of part-of-spee"
P14-1100,P05-1044,0,0.0210609,"per than the syntactically annotated data required for supervised training. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovs"
P14-1100,N10-1116,0,0.0306316,"ining. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, gener"
P14-1100,W10-2902,0,0.0514068,"ining. Most existing solutions treat the problem of unsupervised parsing by assuming a generative process over parse trees e.g. probabilistic context free grammars (Jelinek et al., 1992), and the constituent context model (Klein and Manning, 2002). Learning then reduces to finding a set of parameters that are estimated by identifying a local maximum of an objective function such as the likeliEric P. Xing School of Computer Science Carnegie Mellon University epxing@cs.cmu.edu hood (Klein and Manning, 2002) or a variant of it (Smith and Eisner, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, gener"
P14-1100,D13-1204,0,0.110681,"er, 2005; Cohen and Smith, 2009; Headden et al., 2009; Spitkovsky et al., 2010b; Gillenwater et al., 2010; Golland et al., 2012). Unfortunately, finding the global maximum for these objective functions is usually intractable (Cohen and Smith, 2012) which often leads to severe local optima problems (but see Gormley and Eisner, 2013). Thus, strong experimental results are often achieved by initialization techniques (Klein and Manning, 2002; Gimpel and Smith, 2012), incremental dataset use (Spitkovsky et al., 2010a) and other specialized techniques to avoid local optima such as count transforms (Spitkovsky et al., 2013). These approaches, while empirically promising, generally lack theoretical justification. On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model (Hsu et al., 2012) or matrix completion (Bailly et al., 2013). These novel perspectives offer strong theoretical guarantees but are not designed to achieve competitive empirical results. In this paper, we suggest a different approach, to provide a first step to bridging this theoryexperiment gap. More specifically, we approach unsupervised constituent parsing from the perspective of structure lear"
P14-1100,P10-1040,0,0.00857564,"g, CCM. • Otherwise find the first non-participle verb (say at index j) and return ([0, j − 1], [j, `(x)]). • If no verb exists, return ([0, 1], [1, `(x)]). Word embeddings As mentioned earlier, each wi can be an arbitrary feature vector. For all languages we use Brown clustering (Brown et al., 1992) to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity. For English, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings Turian et al. (2010) of length 50. We also explored two types of CCA embeddings: OSCCA and TSCCA, given in Dhillon et al. (2012). The OSCCA embeddings behaved better, so we only report its results. Choice of kernel For our experiments, we use the kernel Kγ (j, k, j 0 , k 0 |x, x0 )   κ(j, k, j 0 , k 0 |x, x0 ) = max 0, 1 − γ where γ denotes the user-specified bandwidth, |j − k |− |j 0 − k 0 | and κ(j, k, j 0 , k 0 |x, x0 ) = if |j − k |+ |j 0 − k 0 | x(j) = x(j 0 ) and x(k 0 ) = x(k), and sign(j − k) = sign(j 0 − k 0 ) (and ∞ otherwise). The kernel is non-zero if and only if the tags at position j and k in x ar"
P14-1100,petrov-etal-2012-universal,0,\N,Missing
P16-1146,W14-6110,0,0.141144,"Missing"
P16-1146,H91-1060,0,0.754221,"nterminals and their latent state numbers in the training data. They use the EM algorithm to split and merge nonterminals using the latent states, and optimize the number of latent states for each nonterminal such that it maximizes the likelihood of a training treebank. Their refined grammar successfully splits nonterminals to various degrees to capture their complexity. We take the analogous step with spectral methods. We propose an algorithm where we first compute Ωa on the training data and then we optimize the number of latent states for each nonterminal by optimizing the PARSEVAL metric (Black et al., 1991) on a development set. Our optimization algorithm appears in Figure 2. The input to the algorithm is training and development data in the form of parse trees, a basic spectral estimation algorithm S in its default setting, an upper bound m on the number of latent states that can be used for the different nonterminals and a beam size k which gives a maximal queue size for the beam. The algorithm aims to learn a function f that maps each nonterminal a to the number of latent states. It initializes f by estimating a default grammar GS : (N , I, P, fS , n) using S and setting f = fS . It then iter"
P16-1146,P05-1022,0,0.769903,"van (rep)” are vanilla estimations (i.e., each nonterminal is mapped to fixed number of latent states) replacing rare words by POS or POS+morphological signatures, respectively. The best of these two models is used with our optimization algorithm in “opt”. For Sp, “van” uses the best setting for unknown words as Cl. Best result in each column from the first seven rows is in bold. In addition, our best performing models from rows 3-7 are marked with ∗ . “Bk multiple” shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005). “Cl multiple” gives the results with multiple models generated using the noise induction and decoded using the hierarchical decoding (Narayan and Cohen, 2015). Bk results are not available on the development dataset for German-N. For others, we report Bk results from Bj¨orkelund et al. (2013). We also include results from Hall et al. (2014) and Crabb´e (2015). Sp Cl lang. Bk van opt van opt Bk multiple Cl multiple Hall et al. ’14 F&M ’15 Crabb´e ’15 Basque 74.7 79.6 81.4∗ 79.9 80.5 87.9 83.4 83.4 85.9 84.9 French 80.4 74.3 75.6 78.7 79.1∗ 82.9 80.4 79.7 78.8 80.8 German-N 80.1 76.4 78.0 78.4"
P16-1146,P14-1099,1,0.848693,"e-to-fine fashion: merging and splitting nonterminals using the latent states to optimize the number of latent states for each nonterminal. Cohen et al. (2012) presented a so-called spectral algorithm to estimate L-PCFGs. This algorithm uses linear-algebraic procedures such as singular value decomposition (SVD) during learning. The spectral algorithm of Cohen et al. builds on an estimation algorithm for HMMs by Hsu et al. (2009).1 Cohen et al. (2013) experimented with this spectral algorithm for parsing English. A different variant of a spectral learning algorithm for L-PCFGs was developed by Cohen and Collins (2014). It breaks the problem of L-PCFG estimation into multiple convex optimization problems which are solved using EM. The family of L-PCFG spectral learning algorithms was further extended by Narayan and Cohen (2015). They presented a simplified version of the algorithm of Cohen et al. (2012) that estimates sparse grammars and assigns probabilities (instead of weights) to the rules in the grammar, and as such does not suffer from the problem of negative probabilities that arise with the original spectral algorithm (see discussion in Cohen et al., 2013). In this paper, we use the algorithms by Nar"
P16-1146,P12-1024,1,0.888932,"t the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser – the number of latent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar. It splits and merges nonterminals (with latent state information) with the aim to optimize the likelihood of the training data. Together with other types of fine tuning of the parsing model, this led to state-of-the-art results for English parsing. In more recent work, Cohen et al. (2012) described a different family of estimation algorithms for L-PCFGs. This so-called “spectral” family of learning algorithms is compelling because it offers a rigorous theoretical analysis of statistical convergence, and sidesteps local maxima issues that arise with the EM algorithm. While spectral algorithms for L-PCFGs are compelling from a theoretical perspective, they have been lagging behind in their empirical results on the problem of parsing. In this paper we show that one of the main reasons for that is that spectral algorithms require a more careful tuning procedure for the number of l"
P16-1146,N13-1015,1,0.0708295,"s compelling because it offers a rigorous theoretical analysis of statistical convergence, and sidesteps local maxima issues that arise with the EM algorithm. While spectral algorithms for L-PCFGs are compelling from a theoretical perspective, they have been lagging behind in their empirical results on the problem of parsing. In this paper we show that one of the main reasons for that is that spectral algorithms require a more careful tuning procedure for the number of latent states than that which has been advocated for until now. In a sense, the relationship between our work and the work of Cohen et al. (2013) is analogous to the relationship between the work by Petrov et al. (2006) and the work by Matsuzaki et al. (2005): we suggest a technique for optimizing the number of latent states for spectral algorithms, and test it on eight languages. Our results show that when the number of latent states is optimized using our technique, the parsing models the spectral algorithms yield perform significantly better than the vanilla-estimated models, and for most of the languages – better than the Berkeley parser of Petrov et al. (2006). As such, the contributions of this parser are twofold: • We describe a"
P16-1146,D15-1212,0,0.0326805,"Missing"
P16-1146,P15-1147,0,0.161358,"Missing"
P16-1146,P14-1022,0,0.229121,"rd, and information is lost in this conversion. See also (Rabusseau et al., 2016). 1547 VP S V chased NP NP VP D N D N the cat the mouse Figure 1: The inside tree (left) and outside tree (right) for the nonterminal VP in the parse tree (S (NP (D the) (N mouse)) (VP (V chased) (NP (D the) (N cat)))) for the sentence “the mouse chased the cat.” et al. (2012), and we compare them against stateof-the-art L-PCFG parsers such as the Berkeley parser (Petrov et al., 2006). We also compare our algorithms to other state-of-the-art parsers where elaborate linguistically-motivated feature specifications (Hall et al., 2014), annotations (Crabb´e, 2015) and formalism conversions (Fern´andezGonz´alez and Martins, 2015) are used. 3 Optimizing Spectral Estimation In this section, we describe our optimization algorithm and its motivation. 3.1 Spectral Learning of L-PCFGs and Model Size The family of spectral algorithms for latentvariable PCFGs rely on feature functions that are defined for inside and outside trees. Given a tree, the inside tree for a node contains the entire subtree below that node; the outside tree contains everything in the tree excluding the inside tree. Figure 1 shows an example of inside and out"
P16-1146,D10-1004,0,0.0306015,"for the spectral algorithm of Narayan and Cohen (2015). We experimented with several versions of k-means, and discovered that the version that works best in a set of preliminary experiments is hard k-means.5 Decoding and evaluation For efficiency, we use a base PCFG without latent states to prune marginals which receive a value less than 0.00005 in the dynamic programming chart. This is just a bare-bones PCFG that is estimated using maximum likelihood estimation (with frequency count). The parser takes part-of-speech tagged sentences as input. We tag the German-N data using the Turbo Tagger (Martins et al., 2010). For the languages in the SPMRL data we use the MarMot tagger of M¨ueller et al. (2013) to jointly predict the POS and morphological tags.6 The parser itself can assign different part-of-speech tags to words to avoid parse failure. This is also particularly important for constituency parsing with morphologically rich languages. It helps mitigate the problem of the taggers to assign correct tags when long-distance dependencies are present. For all results, we report the F1 measure of the PARSEVAL metric (Black et al., 1991). We use the EVALB program7 with the parameter file COLLINS.prm (Collin"
P16-1146,P05-1010,0,0.721121,"between the different nonterminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages. 1 Introduction Latent-variable probabilistic context-free grammars (L-PCFGs) have been used in the natural language processing community (NLP) for syntactic parsing for over a decade. They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers. Petrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser – the number of latent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar."
P16-1146,D13-1032,0,0.0855828,"Missing"
P16-1146,D15-1214,1,0.0799359,"L-PCFGs. This algorithm uses linear-algebraic procedures such as singular value decomposition (SVD) during learning. The spectral algorithm of Cohen et al. builds on an estimation algorithm for HMMs by Hsu et al. (2009).1 Cohen et al. (2013) experimented with this spectral algorithm for parsing English. A different variant of a spectral learning algorithm for L-PCFGs was developed by Cohen and Collins (2014). It breaks the problem of L-PCFG estimation into multiple convex optimization problems which are solved using EM. The family of L-PCFG spectral learning algorithms was further extended by Narayan and Cohen (2015). They presented a simplified version of the algorithm of Cohen et al. (2012) that estimates sparse grammars and assigns probabilities (instead of weights) to the rules in the grammar, and as such does not suffer from the problem of negative probabilities that arise with the original spectral algorithm (see discussion in Cohen et al., 2013). In this paper, we use the algorithms by Narayan and Cohen (2015) and Cohen 1 A related algorithm for weighted tree automata (WTA) was developed by Bailly et al. (2010). However, the conversion from L-PCFGs to WTA is not straightforward, and information is"
P16-1146,P06-1055,0,0.168249,"hat our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages. 1 Introduction Latent-variable probabilistic context-free grammars (L-PCFGs) have been used in the natural language processing community (NLP) for syntactic parsing for over a decade. They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers. Petrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser – the number of latent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar. It splits and merges nonterminals (with latent state information) with the aim to optimize the likelihood of the training data. Together with other types of fine tuning of the parsing model, this led to state-of-the-art results for Eng"
P16-1146,N10-1003,0,0.378638,"ohen et al. (2013). In Cl, “van (pos)” and “van (rep)” are vanilla estimations (i.e., each nonterminal is mapped to fixed number of latent states) replacing rare words by POS or POS+morphological signatures, respectively. The best of these two models is used with our optimization algorithm in “opt”. For Sp, “van” uses the best setting for unknown words as Cl. Best result in each column from the first seven rows is in bold. In addition, our best performing models from rows 3-7 are marked with ∗ . “Bk multiple” shows the best results with the multiple models using product-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005). “Cl multiple” gives the results with multiple models generated using the noise induction and decoded using the hierarchical decoding (Narayan and Cohen, 2015). Bk results are not available on the development dataset for German-N. For others, we report Bk results from Bj¨orkelund et al. (2013). We also include results from Hall et al. (2014) and Crabb´e (2015). Sp Cl lang. Bk van opt van opt Bk multiple Cl multiple Hall et al. ’14 F&M ’15 Crabb´e ’15 Basque 74.7 79.6 81.4∗ 79.9 80.5 87.9 83.4 83.4 85.9 84.9 French 80.4 74.3 75.6 78.7 7"
P16-1146,W05-1512,0,0.790877,"rminals. In addition, we contribute an empirical analysis of spectral algorithms on eight morphologically rich languages: Basque, French, German, Hebrew, Hungarian, Korean, Polish and Swedish. Our results show that our estimation consistently performs better or close to coarse-to-fine expectation-maximization techniques for these languages. 1 Introduction Latent-variable probabilistic context-free grammars (L-PCFGs) have been used in the natural language processing community (NLP) for syntactic parsing for over a decade. They were introduced in the NLP community by Matsuzaki et al. (2005) and Prescher (2005), with Matsuzaki et al. using the expectation-maximization (EM) algorithm to estimate them. Their performance on syntactic parsing of English at that stage lagged behind state-of-the-art parsers. Petrov et al. (2006) showed that one of the reasons that the EM algorithm does not estimate state-of-the-art parsing models for English is that the EM algorithm does not control well for the model size used in the parser – the number of latent states associated with the various nonterminals in the grammar. As such, they introduced a coarse-to-fine technique to estimate the grammar. It splits and merge"
P16-1146,A97-1014,0,0.284431,"t sets. Eight out of the nine datasets (Basque, French, German-T, Hebrew, Hungarian, Korean, Polish 2 It has been documented in several papers that the family of spectral estimation algorithms is faster than algorithms such as EM, not just for L-PCFGs. See, for example, Parikh et al. (2012). and Swedish) are taken from the workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL; Seddah et al., 2013). The German corpus in the SPMRL workshop is taken from the TiGer corpus (German-T, Brants et al., 2004). We also experiment with another German corpus, the NEGRA corpus (German-N, Skut et al., 1997), in a standard evaluation split.3 Words in the SPMRL datasets are annotated with their morphological signatures, whereas the NEGRA corpus does not contain any morphological information. Data preprocessing and treatment of rare words We convert all trees in the treebanks to a binary form, train and run the parser in that form, and then transform back the trees when doing evaluation using the PARSEVAL metric. In addition, we collapse unary rules into unary chains, so that our trees are fully binarized. The column “#nts” in Table 1 shows the number of nonterminals after binarization in the vario"
P16-1146,E14-1015,0,0.0383947,"Missing"
P16-1146,J03-4003,0,\N,Missing
P16-1146,W13-4916,0,\N,Missing
P18-1040,P17-1112,0,0.091708,"Missing"
P18-1040,E17-2039,0,0.171166,"Missing"
P18-1040,P13-2131,0,0.160099,"on task, we cannot expect model output to exactly match the gold standard. For instance, the numbering of the referents may be different, but nevertheless valid, or the order of the children of a tree node (e.g., “DRS(india(x1 ) say(e1 ))” and “DRS(say(e1 ) india(x1 ))” are the same). We thus use F1 instead of exact match accuracy. Specifically, we report D-match4 a metric designed to evaluate scoped meaning representations and released as part of the distribution of the Parallel Meaning Bank corpus (Abzianidze et al., 2017). D-match is based on Smatch5 , a metric used to evaluate AMR graphs (Cai and Knight, 2013); it calculates F1 on discourse representation graphs (DRGs), i.e., triples of nodes, arcs, and their referents, applying multiple restarts to obtain a good referent (node) mapping between graphs. We converted DRSs (predicted and goldstandard) into DRGs following the top-down procedure described in Algorithm 1.6 I S C ONDI TION returns true if the child is a condition (e.g., india(x1 )), where three arcs are created, one is connected to a parent node and the other two are connected to arg1 and arg2, respectively (lines 7–12). I S Q UANTIFIER returns true if the child is a quantifier (e.g., π1"
P18-1040,P17-1005,1,0.843668,"hool of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB Jiangming.Liu@ed.ac.uk, scohen@inf.ed.ac.uk, mlap@inf.ed.ac.uk Abstract ear ordering has also prompted efforts to develop recurrent neural network architectures tailored to tree or graph-structured decoding (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017) Most previous work focuses on building semantic parsers for question answering tasks, such as querying a database to retrieve an answer (Zelle and Mooney, 1996; Cheng et al., 2017), or conversing with a flight booking system (Dahl et al., 1994). As a result, parsers trained on query-based datasets work on restricted domains (e.g., restaurants, meetings; Wang et al. 2015), with limited vocabularies, exhibiting limited compositionality, and a small range of syntactic and semantic constructions. In this work, we focus on open-domain semantic parsing and develop a general-purpose system which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). DRT is a popular theory of meaning representation designed to accou"
P18-1040,D15-1198,0,0.020494,"formal meaning representations. Le and Zuidema (2012) were the first to train a semantic parser on an early release of the GMB (2,000 documents; Basile et al. 2012), however, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning representation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see Bos (2016) and Artzi et al. (2015) for translations to first-order logic. 30 sentence length Figure 5: F1 score as a function of sentence length. Figure 5 shows F1 performance for the three parsers on sentences of different length. We observe a similar trend for all models: as sentence length increases, model performance decreases. The baseline and shallow models do not perform well on short sentences which despite containing fewer words, can still represent complex meaning which is challenging to capture sequentially. On the other hand, the performance of the deep model is relatively stable. LSTMs in this case function relati"
P18-1040,J07-4004,0,0.0326455,", (hpvari, hre f i)∗ , (hpvari, hconditioni)∗ k1 : hexpt i, k2 : hexpt i k2 :hexpt i | , coo(k1 , k2 ) sub(k1 , k2 ) (6) hunaryi ::= ¬hexpt i |2hexpt i|3hexpt i|hre f i : hexpt i (1) hbinaryi ::=hexpt i→hexpt i|hexpt i∨hexpt i|hexpt i?hexpt i 3 The Groningen Meaning Bank Corpus Corpus Creation DRSs in GMB were obtained from Boxer (Bos, 2008, 2015), and then refined using expert linguists and crowdsourcing methods. Boxer constructs DRSs based on a pipeline of tools involving POS-tagging, named entity recognition, and parsing. Specifically, it relies on the syntactic analysis of the C&C parser (Clark and Curran, 2007), a general-purpose parser using the framework of Combinatory Categorial Grammar (CCG; Steedman 2001). DRSs are obtained from CCG parses, with semantic composition being guided by the CCG syntactic derivation. Documents in the GMB were collected from a variety of sources including Voice of America (a newspaper published by the US Federal Government), the Open American National Corpus, Aesop’s fables, humorous stories and jokes, and country descriptions from the CIA World Factbook. The dataset consists of 10,000 documents each annotated with a DRS. Various statistics on the GMB are shown in Tab"
P18-1040,H94-1010,0,0.246362,"Edinburgh EH8 9AB Jiangming.Liu@ed.ac.uk, scohen@inf.ed.ac.uk, mlap@inf.ed.ac.uk Abstract ear ordering has also prompted efforts to develop recurrent neural network architectures tailored to tree or graph-structured decoding (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017) Most previous work focuses on building semantic parsers for question answering tasks, such as querying a database to retrieve an answer (Zelle and Mooney, 1996; Cheng et al., 2017), or conversing with a flight booking system (Dahl et al., 1994). As a result, parsers trained on query-based datasets work on restricted domains (e.g., restaurants, meetings; Wang et al. 2015), with limited vocabularies, exhibiting limited compositionality, and a small range of syntactic and semantic constructions. In this work, we focus on open-domain semantic parsing and develop a general-purpose system which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). DRT is a popular theory of meaning representation designed to account for a variety of linguistic phenomena, including the interpre"
P18-1040,W13-2322,0,0.138251,"and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin. 1 Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. A variety of meaning representations have been adopted over the years ranging from functional query language (FunQL; Kate et al. 2005) to dependency-based compositional semantics (λ-DCS; Liang et al. 2011), lambda calculus (Zettlemoyer and Collins, 2005), abstract meaning representations (Banarescu et al., 2013), and minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2"
P18-1040,P16-1004,1,0.933513,"escu et al., 2013), and minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016). The fact that meaning representations do not naturally conform to a lin429 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 429–439 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics x1 , e1 , π1 statement(x1 ), say(e1 ), Cause(e1 , x1 ), Topic(e1 ,π1 ) x7 , s2 , x8 , x9 , e3 x3 , s1 , x3 , x5 , e2 π1 : k1 : x2 thing(x) ⇒ Topic(s1 , x3 ), dead(s1 ), x6 man(x3 ), of(x2 , x3 ), k2 : thing(x6 ) magazine(x4 ), on(x5 ,x4 ) vest(x5 ), wear(e2 ), Agent(e2"
P18-1040,basile-etal-2012-developing,0,0.0502792,"node, we have already obtained the structure of the entire tree. deep shallow baseline 80 60 15 20 25 Wide-coverage Semantic Parsing Our model is trained on the GMB (Bos et al., 2017), a richly annotated resource in the style of DRT which provides a unique opportunity for bootstrapping wide-coverage semantic parsers. Boxer (Bos, 2008) was a precursor to the GMB, the first semantic parser of this kind, which deterministically maps CCG derivations onto formal meaning representations. Le and Zuidema (2012) were the first to train a semantic parser on an early release of the GMB (2,000 documents; Basile et al. 2012), however, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning representation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see Bos (2016) and Artzi et al. (2015) for translations to first-order logic. 30 sentence length Figure 5: F1 score as a function of sentence length. Figure 5 shows F1 performance for"
P18-1040,P82-1020,0,0.828613,"Missing"
P18-1040,P16-1002,0,0.0387247,"minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016). The fact that meaning representations do not naturally conform to a lin429 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 429–439 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics x1 , e1 , π1 statement(x1 ), say(e1 ), Cause(e1 , x1 ), Topic(e1 ,π1 ) x7 , s2 , x8 , x9 , e3 x3 , s1 , x3 , x5 , e2 π1 : k1 : x2 thing(x) ⇒ Topic(s1 , x3 ), dead(s1 ), x6 man(x3 ), of(x2 , x3 ), k2 : thing(x6 ) magazine(x4 ), on(x5 ,x4 ) vest(x5 ), wear(e2 ), Agent(e2 , x2 ), Theme(e2 , x"
P18-1040,W08-2222,0,0.753358,"as temporal order and communicative intentions (see continuation(k1 , k2 ) in Figure 1). More formally, DRSs are expressions of type hexpe i (denoting individuals or discourse referents) and hexpt i (i.e., truth values): hexpe i ::= hre f i, hexpt i ::= hdrsi|hsdrsi, (hpvari, hre f i)∗ , (hpvari, hconditioni)∗ k1 : hexpt i, k2 : hexpt i k2 :hexpt i | , coo(k1 , k2 ) sub(k1 , k2 ) (6) hunaryi ::= ¬hexpt i |2hexpt i|3hexpt i|hre f i : hexpt i (1) hbinaryi ::=hexpt i→hexpt i|hexpt i∨hexpt i|hexpt i?hexpt i 3 The Groningen Meaning Bank Corpus Corpus Creation DRSs in GMB were obtained from Boxer (Bos, 2008, 2015), and then refined using expert linguists and crowdsourcing methods. Boxer constructs DRSs based on a pipeline of tools involving POS-tagging, named entity recognition, and parsing. Specifically, it relies on the syntactic analysis of the C&C parser (Clark and Curran, 2007), a general-purpose parser using the framework of Combinatory Categorial Grammar (CCG; Steedman 2001). DRSs are obtained from CCG parses, with semantic composition being guided by the CCG syntactic derivation. Documents in the GMB were collected from a variety of sources including Voice of America (a newspaper publish"
P18-1040,W15-1841,0,0.230071,"Missing"
P18-1040,J16-3006,0,0.02988,"rivations onto formal meaning representations. Le and Zuidema (2012) were the first to train a semantic parser on an early release of the GMB (2,000 documents; Basile et al. 2012), however, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning representation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see Bos (2016) and Artzi et al. (2015) for translations to first-order logic. 30 sentence length Figure 5: F1 score as a function of sentence length. Figure 5 shows F1 performance for the three parsers on sentences of different length. We observe a similar trend for all models: as sentence length increases, model performance decreases. The baseline and shallow models do not perform well on short sentences which despite containing fewer words, can still represent complex meaning which is challenging to capture sequentially. On the other hand, the performance of the deep model is relatively stable. LSTMs in t"
P18-1040,N06-1056,0,0.061337,"anguage to machine interpretable meaning representations. A variety of meaning representations have been adopted over the years ranging from functional query language (FunQL; Kate et al. 2005) to dependency-based compositional semantics (λ-DCS; Liang et al. 2011), lambda calculus (Zettlemoyer and Collins, 2005), abstract meaning representations (Banarescu et al., 2013), and minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016). The fact that meaning representations do not naturally conform to a lin429 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 429–439 c Melbo"
P18-1040,D16-1116,0,0.0611133,"Missing"
P18-1040,P16-1127,0,0.0186379,"h is progressively refined). We provide examples of model output in the supplementary material. 7 DRG w/o refs & conds P R F1 52.89 71.80 60.91 83.30 62.91 71.68 93.91 88.51 91.13 Table 4: GMB test set. Table 3: GMB development set. 10 DRG w/o refs F1 P R F1 57.69 47.20 58.93 52.42 65.24 66.05 62.93 64.45 77.54 82.87 79.40 81.10 Related Work 8 Tree-structured Decoding A few recent approaches develop structured decoders which make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) generate trees in a top-down fashion, while in other work (Xiao et al., 2016; Krishnamurthy et al., 2017) the decoder generates from a grammar that guarantees that predicted logical forms are well-typed. In a similar vein, Yin and Neubig (2017) generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. Rabinovich et al. (2017) introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explicConclusions We introduced a new end-to-end model for opendomain semantic parsing. Experimental results on the GMB"
P18-1040,D17-1160,0,0.017859,"refined). We provide examples of model output in the supplementary material. 7 DRG w/o refs & conds P R F1 52.89 71.80 60.91 83.30 62.91 71.68 93.91 88.51 91.13 Table 4: GMB test set. Table 3: GMB development set. 10 DRG w/o refs F1 P R F1 57.69 47.20 58.93 52.42 65.24 66.05 62.93 64.45 77.54 82.87 79.40 81.10 Related Work 8 Tree-structured Decoding A few recent approaches develop structured decoders which make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) generate trees in a top-down fashion, while in other work (Xiao et al., 2016; Krishnamurthy et al., 2017) the decoder generates from a grammar that guarantees that predicted logical forms are well-typed. In a similar vein, Yin and Neubig (2017) generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. Rabinovich et al. (2017) introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explicConclusions We introduced a new end-to-end model for opendomain semantic parsing. Experimental results on the GMB show that our decoder is able"
P18-1040,P17-1041,0,0.0115588,".91 88.51 91.13 Table 4: GMB test set. Table 3: GMB development set. 10 DRG w/o refs F1 P R F1 57.69 47.20 58.93 52.42 65.24 66.05 62.93 64.45 77.54 82.87 79.40 81.10 Related Work 8 Tree-structured Decoding A few recent approaches develop structured decoders which make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) generate trees in a top-down fashion, while in other work (Xiao et al., 2016; Krishnamurthy et al., 2017) the decoder generates from a grammar that guarantees that predicted logical forms are well-typed. In a similar vein, Yin and Neubig (2017) generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. Rabinovich et al. (2017) introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explicConclusions We introduced a new end-to-end model for opendomain semantic parsing. Experimental results on the GMB show that our decoder is able to recover discourse representation structures to a good degree (77.54 F1 ), albeit with some simplifications. In the future, we plan to m"
P18-1040,C12-1094,0,0.609827,"trees sequentially, and then expand non-terminal nodes, ensuring that when we generate the children of a node, we have already obtained the structure of the entire tree. deep shallow baseline 80 60 15 20 25 Wide-coverage Semantic Parsing Our model is trained on the GMB (Bos et al., 2017), a richly annotated resource in the style of DRT which provides a unique opportunity for bootstrapping wide-coverage semantic parsers. Boxer (Bos, 2008) was a precursor to the GMB, the first semantic parser of this kind, which deterministically maps CCG derivations onto formal meaning representations. Le and Zuidema (2012) were the first to train a semantic parser on an early release of the GMB (2,000 documents; Basile et al. 2012), however, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning representation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see Bos (2016) and Artzi et al. (2015) for translations to first-order l"
P18-1040,P11-1060,0,0.0240508,"ding process into three stages: basic DRS structure prediction, condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin. 1 Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. A variety of meaning representations have been adopted over the years ranging from functional query language (FunQL; Kate et al. 2005) to dependency-based compositional semantics (λ-DCS; Liang et al. 2011), lambda calculus (Zettlemoyer and Collins, 2005), abstract meaning representations (Banarescu et al., 2013), and minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction"
P18-1040,P17-1049,0,0.0173502,"5 62.93 64.45 77.54 82.87 79.40 81.10 Related Work 8 Tree-structured Decoding A few recent approaches develop structured decoders which make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) generate trees in a top-down fashion, while in other work (Xiao et al., 2016; Krishnamurthy et al., 2017) the decoder generates from a grammar that guarantees that predicted logical forms are well-typed. In a similar vein, Yin and Neubig (2017) generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. Rabinovich et al. (2017) introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explicConclusions We introduced a new end-to-end model for opendomain semantic parsing. Experimental results on the GMB show that our decoder is able to recover discourse representation structures to a good degree (77.54 F1 ), albeit with some simplifications. In the future, we plan to model document-level representations which are more in line with DRT and the GMB annotations. Acknowledgments We thank the anonymous"
P18-1040,W13-0122,0,0.194002,"nd Reyle, 1993). Basic DRSs consist of discourse referents (e.g., x, y) representing entities in the discourse and discourse conditions (e.g., man(x), magazine(y)) representing information about discourse referents. Following conventions in the DRT literature, we visualize DRSs in a box-like format (see Figure 1). GMB adopts a variant of DRT that uses a neoDavidsonian analysis of events (Kipper et al., 2008), i.e., events are first-order entities characterized by one-place predicate symbols (e.g., say(e1 ) in Figure 1). In addition, it follows Projective Discourse Representation Theory (PDRT; Venhuizen et al. 2013) an extension of DRT specifically developed to account for the interpretation of presuppositions and related projection phenomena 1 https://github.com/EdinburghNLP/EncDecDRSparsing 430 sections 00-99 20-99 10-19 00-09 (e.g., conventional implicatures). In PDRT, each basic DRS introduces a label, which can be bound by a pointer indicating the interpretation site of semantic content. To account for the rhetorical structure of texts, GMB adopts Segmented Discourse Representation Theory (SDRT; Asher and Lascarides 2003). In SDRT, discourse segments are linked with rhetorical relations reflecting d"
P18-1040,P15-1129,0,0.0221408,"Missing"
P18-1183,K16-1002,0,0.16083,"∗ |, gT ] + bT ) (26) where WT is a weight matrix and bT is a bias. As to the model objective, we use the Monte Carlo method to approximate the expectation term in Eq. (11) and typically only one sample is used for gradient computation. To incorporate varied temporal importance at the objective level, we first break down the approximated L into a series of temporal objectives f ∈ RT ×1 where ft comprises a likelihood term and a KL term for a trading day t, ft = log pθ (yt |x≤t , z≤t ) 1975 (27) − λDKL [qφ (zt |z&lt;t , x≤t , yt ) k pθ (zt |z&lt;t , x≤t )] where we adopt the KL term annealing trick (Bowman et al., 2016; Semeniuta et al., 2017) and add a linearly-increasing KL term weight λ ∈ (0, 1] to gradually release the KL regularization effect in the training procedure. Then we reuse v ∗ to build the final temporal weight vector v ∈ R1×T , v = [αv ∗ , 1] (28) where 1 is for the main prediction and we adopt the auxiliary weight α ∈ [0, 1] to control the overall auxiliary effects on the model training. α is tuned on the development set and its effects will be discussed at length in Section 6.5. Finally, we write the training objective F by recomposition, N 1 X (n) (n) F (θ, φ; X, y) = v f N n Experiments"
P18-1183,D14-1148,0,0.0989011,"et al., 2007; Bollen et al., 2011; Hu et al., 2018). We present a model to predict stock price movement from tweets and historical stock prices. In natural language processing (NLP), public news and social media are two primary content resources for stock market prediction, and the models that use these sources are often discriminative. Among them, classic research relies heavily on feature engineering (Schumaker and Chen, 2009; Oliveira et al., 2013). With the prevalence of deep neural networks (Le and Mikolov, 2014), eventdriven approaches were studied with structured event representations (Ding et al., 2014, 2015). More recently, Hu et al. (2018) propose to mine news sequence directly from text with hierarchical attention mechanisms for stock trend prediction. However, stock movement prediction is widely considered difficult due to the high stochasticity of the market: stock prices are largely driven by new information, resulting in a random-walk pattern (Malkiel, 1999). Instead of using only deterministic features, generative topic models were extended to jointly learn topics and sentiments for the task (Si et al., 2013; Nguyen and Shirai, 2015). Compared to discriminative models, generative mo"
P18-1183,D17-1222,0,0.023828,"ing the raw price  of directly  vector p˜t = p˜ct , p˜ht , p˜lt comprising of the adjusted closing, highest and lowest price on a trading day t, into the networks, we normalize it with its last adjusted closing price, pt = p˜t /˜ pct−1 − 1. We then concatenate ct with pt to form the final market information input xt for the decoder. 5.2 log pθ (y|X) = T X t=1 (11)  Eqφ (zt |z&lt;t ,x≤t ,yt ) log pθ (yt |x≤t , z≤t ) − DKL [qφ (zt |z&lt;t , x≤t , yt ) k pθ (zt |z&lt;t , x≤t )] ≤ log pθ (y|X) where the likelihood term ( pθ (yt |x≤t , zt ) , if t &lt; T pθ (yt |x≤t , z≤t ) = pθ (yT |X, Z) , if t = T. (12) Li et al. (2017) also provide a lower bound for inferring directly-connected recurrent latent variables in text summarization. In their work, priors are modeled with pθ (zt ) ∼ N (0, I), which, in fact, turns the KL term into a static regularization term encouraging sparsity. In Eq. (11), we provide a more theoretically rigorous lower bound where the KL term with pθ (zt |z&lt;t , x≤t ) plays a dynamic role in inferring dependent latent variables for every different model input and latent history. Decoding As per time series, VMD adopts an RNN with a GRU cell to extract features and decode stock signals recurrent"
P18-1183,P15-1131,0,0.599441,"ches were studied with structured event representations (Ding et al., 2014, 2015). More recently, Hu et al. (2018) propose to mine news sequence directly from text with hierarchical attention mechanisms for stock trend prediction. However, stock movement prediction is widely considered difficult due to the high stochasticity of the market: stock prices are largely driven by new information, resulting in a random-walk pattern (Malkiel, 1999). Instead of using only deterministic features, generative topic models were extended to jointly learn topics and sentiments for the task (Si et al., 2013; Nguyen and Shirai, 2015). Compared to discriminative models, generative models have the natural advantage in depicting the generative process from market information to stock signals and introducing randomness. However, these models underrepresent chaotic social texts with bag-of-words and employ simple discrete latent variables. In essence, stock movement prediction is a time series problem. The significance of the temporal dependency between movement predictions is not addressed in existing NLP research. For instance, when a company suffers from a major scandal on a trading day d1 , generally, its stock price will"
P18-1183,D17-1066,0,0.0823057,"where WT is a weight matrix and bT is a bias. As to the model objective, we use the Monte Carlo method to approximate the expectation term in Eq. (11) and typically only one sample is used for gradient computation. To incorporate varied temporal importance at the objective level, we first break down the approximated L into a series of temporal objectives f ∈ RT ×1 where ft comprises a likelihood term and a KL term for a trading day t, ft = log pθ (yt |x≤t , z≤t ) 1975 (27) − λDKL [qφ (zt |z&lt;t , x≤t , yt ) k pθ (zt |z&lt;t , x≤t )] where we adopt the KL term annealing trick (Bowman et al., 2016; Semeniuta et al., 2017) and add a linearly-increasing KL term weight λ ∈ (0, 1] to gradually release the KL regularization effect in the training procedure. Then we reuse v ∗ to build the final temporal weight vector v ∈ R1×T , v = [αv ∗ , 1] (28) where 1 is for the main prediction and we adopt the auxiliary weight α ∈ [0, 1] to control the overall auxiliary effects on the model training. α is tuned on the development set and its effects will be discussed at length in Section 6.5. Finally, we write the training objective F by recomposition, N 1 X (n) (n) F (θ, φ; X, y) = v f N n Experiments In this section, we detai"
P18-1183,P13-2005,0,0.146229,"ventdriven approaches were studied with structured event representations (Ding et al., 2014, 2015). More recently, Hu et al. (2018) propose to mine news sequence directly from text with hierarchical attention mechanisms for stock trend prediction. However, stock movement prediction is widely considered difficult due to the high stochasticity of the market: stock prices are largely driven by new information, resulting in a random-walk pattern (Malkiel, 1999). Instead of using only deterministic features, generative topic models were extended to jointly learn topics and sentiments for the task (Si et al., 2013; Nguyen and Shirai, 2015). Compared to discriminative models, generative models have the natural advantage in depicting the generative process from market information to stock signals and introducing randomness. However, these models underrepresent chaotic social texts with bag-of-words and employ simple discrete latent variables. In essence, stock movement prediction is a time series problem. The significance of the temporal dependency between movement predictions is not addressed in existing NLP research. For instance, when a company suffers from a major scandal on a trading day d1 , genera"
P18-1183,P13-1086,0,0.0670344,"se and 0 denotes fall,  y = 1 pcd &gt; pcd−1 (1) where pcd denotes the adjusted closing price adjusted for corporate actions affecting stock prices, e.g. dividends and splits.4 The adjusted closing 3 To a fundamentalist, stocks have their intrinsic values that can be derived from the behavior and performance of their company. On the contrary, technical analysis considers only the trends and patterns of the stock price. 4 Technically, d − 1 may not be an eligible trading day and thus has no available price information. In the rest of this price is widely used for predicting stock price movement (Xie et al., 2013) or financial volatility (Rekabsaz et al., 2017). 3 Data Collection In finance, stocks are categorized into 9 industries: Basic Materials, Consumer Goods, Healthcare, Services, Utilities, Conglomerates, Financial, Industrial Goods and Technology.5 Since high-tradevolume-stocks tend to be discussed more on Twitter, we select the two-year price movements from 01/01/2014 to 01/01/2016 of 88 stocks to target, coming from all the 8 stocks in Conglomerates and the top 10 stocks in capital size in each of the other 8 industries (see supplementary material). We observe that there are a number of targe"
P18-1183,D16-1050,0,0.0273464,"-wise product. The noise term  ∼ N (0, I) naturally involves stochastic signals in our model. Similarly, We let the prior pθ (zt |z&lt;t , x≤t ) ∼ N (µ0 , δ 0 2 I). Its calculation is the same as that of the posterior except the absence of yt and independent model parameters, = bθδ g1 g3 information score and a dependency score. Specifically, (18) vi0 = wi |tanh(Wg,i G∗ ) (19) vd0 = gT| ∗ v = hzt 0 = tanh(Wzθ [zt−1 , xt , hst ] + bθz ). g2 Figure 3: The temporal attention in our model. Squares are the non-linear projections of gt and points are scores or normalized weights. where (20) Following Zhang et al. (2016), differently from the posterior, we set the prior zt = µ0t during decoding. Finally, we integrate deterministic features and the final prediction hypothesis is given as gt = tanh(Wg [xt , hst , zt ] + bg ) (21) y˜t = ζ(Wy gt + by ), t &lt; T (22) where Wg , Wy are weight matrices and bg , by are biases. The softmax function ζ(·) outputs the confidence distribution over up and down. As introduced in Section 4, the decoding of the main target yT depends on z&lt;T and thus lies at the interface between VMD and ATA. We will elaborate on it in the next section. 5.3 1 are weight matrices and Since Gaussi"
P18-1183,P17-1157,0,0.23998,"Missing"
P18-1188,P16-1046,1,0.880852,"re large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence"
P18-1188,D15-1042,0,0.0909022,"Missing"
P18-1188,D14-1181,0,0.010422,"y and 0 otherwise. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017). The main components include a sentence encoder, a document encoder, and a novel sentence extractor (see Figure 1) that we describe in more detail below. The novel characteristics of our model are that each sentence is labeled by implicitly estimating its (local and global) relevance to the document and by directly attending to some external information for importance cues. Sentence Encoder A core component of our model is a convolutional sentence encoder (Kim, 2014; Kim et al., 2016) which encodes sentences into continuous representations. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over the feature map f and take the maximum value as the feature corresponding to this particular filter K. We use multiple kernels of various sizes and each kernel multiple times to construct the representation of a se"
P18-1188,P15-1107,0,0.0268866,"e times each. The max-pooling over time operation yields two feature lists f K2 and f K4 ∈ R3 . The final sentence embeddings have six dimensions. Document encoder Document Encoder The document encoder composes a sequence of sentences to obtain a document representation. We use a recurrent neural network with LSTM cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document D consisting of a sequence of sentences (s1 , s2 , . . . , sn ), we follow common practice and feed the sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015). Sentence Extractor Our sentence extractor sequentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues. It is implemented with another RNN with LSTM cells with an attention mechanism (Bahdanau et al., 2015) and a softmax layer. Our attention mechanism differs from the standard practice of attending intermediate states of the input (encoder). Instead, our extractor attends to a sequence of p pieces of external information E : (e1 , e2 , ...,"
P18-1188,N03-1020,0,0.198902,"e table present different variants of XN ET. We experimented with three types of external information: title (TITLE), image captions (CAPTION) and the first sentence (FS) of the document. The bottom block of the table presents models with more than one type of external information. The best performing model (highlighted in boldface) is used on the test set. ata (2016) report only on the DailyMail dataset. We used their code (https://github.com/ cheng6076/NeuralSum) to produce results on the CNN dataset.5 Automatic Evaluation To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003), a recall-oriented metric, to compare our model-generated summaries to manually-written highlights.6 Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency. In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to assess informativeness and fluency simultaneously. teresting direction of research but we do not pursue it here. It requires decoding with multiple types of attentions and this is not the focus of this paper. 5 We are unable to compare our results to the"
P18-1188,D15-1106,0,0.0230143,"es in problems such as language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehensio"
P18-1188,D16-1147,0,0.349286,"posed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the do"
P18-1188,N18-1158,1,0.835886,"on (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their constituent words. Our novel sentence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2015) over the external information to label sentences from the input document. Our model explicitly biases the extractor with external cues and 2020 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2020–2030 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics implicitly bias"
P18-1188,D16-1264,0,0.264849,"each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other. 2 Document Modeling For Sentence Extraction Given a document D consisting of a sequence of n sentences (s1 , s2 , ..., sn ) , we aim at labeling each sentence si in D with a label yi ∈ {0, 1} where yi = 1 indicates that si is extraction-worthy and 0 otherwise. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 20"
P18-1188,D15-1044,0,0.105793,"Missing"
P18-1188,P17-1099,0,0.479627,"et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the ar"
P18-1188,P17-1108,0,0.515521,"t al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed"
P18-1188,Q16-1019,0,0.271774,"do not use this information. We also conduct a human evaluation to judge which type of summary participants prefer. Our results overwhelmingly show that human subjects find our summaries more informative and complete. Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior"
P18-1188,N16-1090,0,0.0300827,"e second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide docum"
P18-1188,P16-1125,0,0.0303368,"long-term dependencies in problems such as language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine r"
P18-1188,P17-1018,0,0.0276416,"well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their cons"
P18-1188,C16-1127,0,0.0908422,"t a human evaluation to judge which type of summary participants prefer. Our results overwhelmingly show that human subjects find our summaries more informative and complete. Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where ea"
P18-1188,K17-1028,0,0.0234859,"RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representat"
P18-1188,D15-1237,0,0.0852545,"Missing"
P18-1188,N16-1174,0,0.0542042,"language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 20"
P19-1238,P07-1032,0,0.03944,"ermine the subcategorization frame of the words they label. For example, the category for a transitive verb is (SNP)/NP, which says that this word must combine with an (object) NP on its right (indicated by the forward slash), which will yield a category which must combine with a second (subject) NP on its left (indicated by the backward slash). In place of movement, CCG uses type raising and function composition rules to capture unbounded long distance dependencies. CCG already has a very well-established research tradition in wide-coverage parsing (see, e.g., Hockenmaier and Steedman 2002; Clark and Curran 2007b; Lewis and Steedman 2014; Xu 2016; Lewis et al. 2016; Wu et al. 2017). A key advancement in CCG parsing that enabled it to become efficient enough to support large-scale NLP tasks was the introduction of Markovian supertagging techniques in Clark and Curran (2007b) that were borrowed from Lexicalised Tree Adjoining Grammar (LTAG; Bangalore and Joshi 1999). Supertagging is essentially just part-of-speech tagging for strongly lexicalised formalisms, which have much larger tagsets than the 50 or so tags used in the PTB. Because the supertags predetermine much of the combinatorics, this is somet"
P19-1238,J07-4004,0,0.0785911,"ermine the subcategorization frame of the words they label. For example, the category for a transitive verb is (SNP)/NP, which says that this word must combine with an (object) NP on its right (indicated by the forward slash), which will yield a category which must combine with a second (subject) NP on its left (indicated by the backward slash). In place of movement, CCG uses type raising and function composition rules to capture unbounded long distance dependencies. CCG already has a very well-established research tradition in wide-coverage parsing (see, e.g., Hockenmaier and Steedman 2002; Clark and Curran 2007b; Lewis and Steedman 2014; Xu 2016; Lewis et al. 2016; Wu et al. 2017). A key advancement in CCG parsing that enabled it to become efficient enough to support large-scale NLP tasks was the introduction of Markovian supertagging techniques in Clark and Curran (2007b) that were borrowed from Lexicalised Tree Adjoining Grammar (LTAG; Bangalore and Joshi 1999). Supertagging is essentially just part-of-speech tagging for strongly lexicalised formalisms, which have much larger tagsets than the 50 or so tags used in the PTB. Because the supertags predetermine much of the combinatorics, this is somet"
P19-1238,P02-1042,1,0.591538,"abelled bi-lexical dependencies for each binary non-terminal in the Xbar phrase structure trees transduced from the MG derivation trees.9 To make up for the short8 This number of tags is closer to the 4727 elementary trees of the TAG treebank of Chen (2001) than to CCGbank’s (Hockenmaier and Steedman, 2007) 1286 lexical categories. 9 As in Collins (1999), the labels are triples of the parent, non-head child and head child categories. The dependencies include both local dependencies and those created by movement, hence this evaluation is more akin to the deep dependency evaluation discussed in Clark et al. (2002) for CCG than to the more standard practice of evaluating parsers in terms of just local dependencies (e.g. Collins 1999). The semantic head of the clause is taken to be the main verb, while its syntactic head, if present, is the overt complemenULAB LAB ULAB LAB Model description syntax 5.1 Experiments semantics 5 model Abstract Reified Abstract Reified Abstract Reified Abstract Reified F1 79.33 80.10 84.57 85.19 74.90 75.47 83.69 84.11 P 81.87 83.43 87.15 88.63 77.17 78.53 86.16 87.47 R 76.94 77.02 82.14 82.02 72.75 72.64 81.36 81.01 E 21.01 21.61 29.59 30.49 20.96 21.56 33.30 34.50 Table 1:"
P19-1238,W04-3215,1,0.788792,"ence lengths and the average curve. The average curve is less informative in very long sentences due to the smaller number of parses, but in regions where there are more data points a clear pattern can be observed: a cubic polynomial curve approximates average time taken to parse sentences extremely well, which means that the expected time complexity of MG 2492 primary motivation for using linguistically expressive parsers in NLP. Wh-object questions themselves are extremely rare in the PTB, but object relative clauses, which also involve unbounded movement, are relatively frequent. Following Clark et al. (2004), we manually evaluated our parser on the free and non-free object (and embedded subject) relative clauses in section 00 of the PTB, as well as on the two examples of so-called tough movement. The MGbank analyses of these constructions are discussed in Appendix B. 10 average 0.00012 n3 minutes 8 6 4 2 0 0 10 20 words 30 40 Figure 2: Parsing speed for Abstract model on test set. parsing with our grammar and statistical model is O(n3 ). This is much better than the worst case analysis, although the variance is high, with some sentences still requiring a very long time to parse. Recently, Stanoje"
P19-1238,P02-1043,1,0.613775,". The functional categories determine the subcategorization frame of the words they label. For example, the category for a transitive verb is (SNP)/NP, which says that this word must combine with an (object) NP on its right (indicated by the forward slash), which will yield a category which must combine with a second (subject) NP on its left (indicated by the backward slash). In place of movement, CCG uses type raising and function composition rules to capture unbounded long distance dependencies. CCG already has a very well-established research tradition in wide-coverage parsing (see, e.g., Hockenmaier and Steedman 2002; Clark and Curran 2007b; Lewis and Steedman 2014; Xu 2016; Lewis et al. 2016; Wu et al. 2017). A key advancement in CCG parsing that enabled it to become efficient enough to support large-scale NLP tasks was the introduction of Markovian supertagging techniques in Clark and Curran (2007b) that were borrowed from Lexicalised Tree Adjoining Grammar (LTAG; Bangalore and Joshi 1999). Supertagging is essentially just part-of-speech tagging for strongly lexicalised formalisms, which have much larger tagsets than the 50 or so tags used in the PTB. Because the supertags predetermine much of the combi"
P19-1238,J07-3004,1,0.651329,"on of the supertags for each word. The parameters are trained using an Adam optimizer with a learning rate of 0.0002. 5.2 Recovering MGBank dependencies We first tested the parser on its ability to recover global syntactic and semantic (local and non-local) dependencies extracted from MGbank. We extracted labelled and unlabelled bi-lexical dependencies for each binary non-terminal in the Xbar phrase structure trees transduced from the MG derivation trees.9 To make up for the short8 This number of tags is closer to the 4727 elementary trees of the TAG treebank of Chen (2001) than to CCGbank’s (Hockenmaier and Steedman, 2007) 1286 lexical categories. 9 As in Collins (1999), the labels are triples of the parent, non-head child and head child categories. The dependencies include both local dependencies and those created by movement, hence this evaluation is more akin to the deep dependency evaluation discussed in Clark et al. (2002) for CCG than to the more standard practice of evaluating parsers in terms of just local dependencies (e.g. Collins 1999). The semantic head of the clause is taken to be the main verb, while its syntactic head, if present, is the overt complemenULAB LAB ULAB LAB Model description syntax 5"
P19-1238,W13-3001,0,0.0507713,"hese parsers unsuitable for implementing MP analyses involving remnant movement (see Stabler 1999). 2.1 MG parsers A number of parsers have been developed for Stablerian MGs, which do allow for actual movement, including remnant movement. What all working MG parsers (Harkema, 2001; Hale, 2003; Stabler, 2013; Stanojevi´c and Stabler, 2018) have until now shared in common is that they are smallscale theoretical implementations equipped only with toy lexicons/grammars. There has been a limited amount of research into probabilistic MGs, notably in generative locally normalised models (Hale, 2003; Hunter and Dyer, 2013). However, these works remain so far untested owing to the unavailability, until very recently, of any MG treebank for training and evaluating models. 2.2 MGbank MGbank (Torr, 2017, 2018) is a treebank of MG derivation trees constructed in part manually by hand-annotating a subset of PTB sentences and in part automatically using a parser equipped with the manually constructed grammar and guided by the corresponding PTB and CCGbank (Hockenmaier and Steedman, 2007) structures. The corpus was continuously machine tested for over- and undergeneration throughout its development. It currently covers"
P19-1238,P02-1018,0,0.102944,"ays the kicker, “both these candidates are named Rudolph Giuliani.”), the licensing of polarity items such as anything, anymore and much by interrogative and negation heads (you have *(not) eaten anything), and the distributional dependency between expletive there and its obligatorily indefinite DP associate (there seem to be some/several/*the/*those problems). All of these long distance dependencies, along with those involved in control, raising, topicalization and wh movement, are integrated into the grammar itself, obviating the need for separate post-processing techniques to recover them (Johnson, 2002; Cahill et al., 2004). The MG lexical categories have also been annotated with over 100 fine-grained selectional and agreement restriction features (e.g. +3SG, -NOM, +INF, MASC, +INDEF, +FOR, MNR, +LOC, etc) to avoid many instances of unwanted overgeneration. Movement is clearly a very powerful operation. However, it is constrained here using many of the locality constraints proposed in the TG literature. These include not only Stabler’s (1997) strict version of the Shortest Move Constraint, but also a partially derelativized version (DSMC) inspired by Rizzi (1990), along with versions of the"
P19-1238,N03-1016,0,0.0916756,"dvancement in CCG parsing that enabled it to become efficient enough to support large-scale NLP tasks was the introduction of Markovian supertagging techniques in Clark and Curran (2007b) that were borrowed from Lexicalised Tree Adjoining Grammar (LTAG; Bangalore and Joshi 1999). Supertagging is essentially just part-of-speech tagging for strongly lexicalised formalisms, which have much larger tagsets than the 50 or so tags used in the PTB. Because the supertags predetermine much of the combinatorics, this is sometimes referred to as ‘almost parsing’. Inspired by the A* algorithm for PCFGs of Klein and Manning (2003), L&S present a simple yet highly effective CCG parsing model which is factored over the probabilities assigned by the lexical supertagger alone, with no explicit model of the derivation at all. This approach is highly efficient and avoids the need for aggressively pruning the search space, which degraded the performance of earlier CKY CCG parsers. Instead, the parser considers the complete distribution of the 425 most commonly occurring CCG lexical categories for each word. The supertagger was originally a unigram log-linear classifier, but Lewis et al. (2016) greatly enhanced its accuracy by"
P19-1238,W08-2315,0,0.210434,"in Figure 1. Therefore, we must redefine ↵ in Equations 2 and 3 to be the set of word indices covered by all the spans contained within an MG expression. The second issue is that, following T&S, the MGbank grammar allows for so-called Acrossthe-Board (ATB) head and phrasal movements in order to capture adjunct control, parasitic gaps, and certain coordination structures. ATB phrasal movement is illustrated in 2 below. (2) Whoi did Jack say Mary likes ti and Pete hates ti ? In 2, who has moved from two separate base generated object positions in across-the-board fashion. T&S (adapting ideas in Kobele 2008) propose to account for this by initially generating 5 See Berwick and Epstein (1995) on the convergence of Minimalist syntax and Categorial Grammar. two instances of who in the two object positions and then later unifying them into a single item when the second conjunct is merged into the main structure. For A*, when two expressions containing unifiable movers are merged together, only one of those movers must contribute to the cost of the resulting expression in order to avoid excessive penalisation for what is now just a single instance of the moving item. We can achieve this for both ATB h"
P19-1238,C90-3084,0,0.700341,"terpretation of mainstream MP that is weakly equivalent to Multiple Context-Free Grammars (MCFG; Seki et al. 1991). The parser itself is an adaptation of a highly efficient A* CCG parsing algorithm (Lewis and Steedman, 2014) with a bi-LSTM model trained on MGbank, an MG version of the English Penn Treebank (PTB; Marcus et al. 1993) currently under development. 2 Background Beginning in the 1960s, a number of parsers were developed which implemented aspects of the various iterations of Chomskyan syntactic theory (e.g. Petrick 1965; Zwicky et al. 1965; Woods 1970, 1973; Plath 1973; Marcus 1980; Kuhns 1990; Fong 1991; Stabler 1992; Fong and Ginsburg 2012), but most of these systems operated over relatively closed domains and were never evaluated against wide-coverage treebank test data. Principar (Lin, 1993), and its descendant Minipar (Lin, 1998, 2001), are the only truly widecoverage parsers in the Chomskyan tradition of which we are aware. Minipar incorporates MP’s bare phrase structure and some of its economy principles. It is also statistical, having been selftrained on a 1GB corpus. However, while these parsers model the phrase structure and locality constraints of TG, they are not transf"
P19-1238,N16-1026,0,0.116046,"s the semantic AGENT of eat; in 1b, meanwhile, it moves from the deep object position and so is interpreted instead as the semantic PATIENT of eat. Minimalist Grammars (Stabler, 1997) are a computationally oriented, and rigorous formalisation of many aspects of Chomsky’s (1995) Minimalist Program. This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing. The parser uses a linguistically expressive yet highly constrained grammar, together with an adaptation of the A* search algorithm currently used in CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016), with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank, a corpus of MG derivation trees. We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is O(n3 ). The parser is publicly available.1 1 (1) Introduction a. Whati do you think ti eats mice? b. Whati do you think mice eat ti ? Parsers bas"
P19-1238,D14-1107,1,0.908946,"therefore be interpreted as the semantic AGENT of eat; in 1b, meanwhile, it moves from the deep object position and so is interpreted instead as the semantic PATIENT of eat. Minimalist Grammars (Stabler, 1997) are a computationally oriented, and rigorous formalisation of many aspects of Chomsky’s (1995) Minimalist Program. This paper presents the first ever application of this formalism to the task of realistic wide-coverage parsing. The parser uses a linguistically expressive yet highly constrained grammar, together with an adaptation of the A* search algorithm currently used in CCG parsing (Lewis and Steedman, 2014; Lewis et al., 2016), with supertag probabilities provided by a bi-LSTM neural network supertagger trained on MGbank, a corpus of MG derivation trees. We report on some promising initial experimental results for overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is O(n3 ). The parser is publicly available.1 1 (1) Introduction a. Whati do you think ti eats mice? b. Whati do you think mice"
P19-1238,P93-1016,0,0.584734,"nd Steedman, 2014) with a bi-LSTM model trained on MGbank, an MG version of the English Penn Treebank (PTB; Marcus et al. 1993) currently under development. 2 Background Beginning in the 1960s, a number of parsers were developed which implemented aspects of the various iterations of Chomskyan syntactic theory (e.g. Petrick 1965; Zwicky et al. 1965; Woods 1970, 1973; Plath 1973; Marcus 1980; Kuhns 1990; Fong 1991; Stabler 1992; Fong and Ginsburg 2012), but most of these systems operated over relatively closed domains and were never evaluated against wide-coverage treebank test data. Principar (Lin, 1993), and its descendant Minipar (Lin, 1998, 2001), are the only truly widecoverage parsers in the Chomskyan tradition of which we are aware. Minipar incorporates MP’s bare phrase structure and some of its economy principles. It is also statistical, having been selftrained on a 1GB corpus. However, while these parsers model the phrase structure and locality constraints of TG, they are not transformational: movement is merely ‘simulat[ed]’ (Lin, 1993, page 116) by passing features up a precompiled network of nodes representing a tree, from the site of the trace to the site of the antecedent, with t"
P19-1238,H01-1046,0,0.0216294,"Missing"
P19-1238,W12-4615,0,0.0668432,"Missing"
P19-1238,W09-0104,0,0.0314966,"obele et al., 2013; Stabler, 2013; Graf and Marcinek, 2014; Graf et al., 2015; Gerth, 2015; Stanojevi´c and Stabler, 2018). On the other hand, TG has enjoyed far less popularity within computational linguistics more generally,2 which is unfortunate given that it is arguably the most extensively developed syntactic theory across the greatest number of languages, many of which are otherwise under-resourced. Conversely, the process of constructing large grammar fragments and 1 https://github.com/mgparsing/astar_ mg_parser 2 For an anti-Chomskyan perspective on why this disconnect came about, see Pullum (2009). 2486 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2486–2505 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics subjecting these to computational testing can have a salutary impact on the syntactic theory itself, forcing choices between competing analyses of the same construction, and exposing incompatibilities between analyses of different constructions, along with areas of over/undergeneration which may otherwise go unnoticed (Bierwisch 1963; Abney 1996; both cited in M¨uller 2016). The received wisdo"
P19-1238,1985.tmi-1.17,0,0.326254,". Whati do you think mice eat ti ? Parsers based on linguistically expressive formalisms, such as Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag 1994) and Combinatory Categorial Grammar (CCG; Steedman 1996), were shown in Rimell et al. (2009) and Nivre et al. (2010) to be more effective at recovering certain unbounded long-distance dependencies than those merely approximating human grammar with finite state or context-free covers. Such dependencies can be vital for tasks like open domain question answering, for example. Furthermore, as proven independently by Huybregts (1984) and Shieber (1985), some languages exhibit constructions which put them beyond even MP continues to dominate much of theoretical syntax, and Stabler’s (1997) rigorous formalisation of this framework has proven a popular choice for investigations into human sentence processing (Hale, 2003; Kobele et al., 2013; Stabler, 2013; Graf and Marcinek, 2014; Graf et al., 2015; Gerth, 2015; Stanojevi´c and Stabler, 2018). On the other hand, TG has enjoyed far less popularity within computational linguistics more generally,2 which is unfortunate given that it is arguably the most extensively developed syntactic theory acro"
P19-1238,D09-1085,1,0.746176,"or overall dependency recovery as well as on the recovery of certain unbounded long distance dependencies. Finally, although like other MG parsers, ours has a high order polynomial worst case time complexity, we show that in practice its expected time complexity is O(n3 ). The parser is publicly available.1 1 (1) Introduction a. Whati do you think ti eats mice? b. Whati do you think mice eat ti ? Parsers based on linguistically expressive formalisms, such as Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag 1994) and Combinatory Categorial Grammar (CCG; Steedman 1996), were shown in Rimell et al. (2009) and Nivre et al. (2010) to be more effective at recovering certain unbounded long-distance dependencies than those merely approximating human grammar with finite state or context-free covers. Such dependencies can be vital for tasks like open domain question answering, for example. Furthermore, as proven independently by Huybregts (1984) and Shieber (1985), some languages exhibit constructions which put them beyond even MP continues to dominate much of theoretical syntax, and Stabler’s (1997) rigorous formalisation of this framework has proven a popular choice for investigations into human se"
P19-1238,E17-3021,1,0.887977,"Missing"
P19-1468,J15-2003,0,0.306359,"taset. For the link prediction task, we compare the ConvE model with our proposed link prediction score. We test how MC and Aug MC entailment scores can improve the link prediction scores in both local and global settings. 5 Results and Discussion We first compare our proposed entailment score with the previous state-of-the-art results (§5.1) and then show that we can use entailment decisions to improve the link prediction task (§5.2). 9 Higher values of K was not feasible on our machines. We performed our experiments on a 32-core 2.3 GHz machine with 256GB of RAM. 10 The entailment graphs of Berant et al. (2015) yield similar results. 5.1 Entailment Scores based on Link Prediction In this section, we compare the variants of our method to the previous state-of-the-art results on the Levy/Holt’s dataset. We compute similarity scores and report precision-recall curve by changing the threshold for entailment between 0 and 1. In order to have a fair comparison with Berant’s ILP method, we first test a set of rule-based constraints proposed by them (Berant et al., 2011). We also apply the lemma baseline heuristic process of Levy and Dagan (2016) before testing the methods. Figure 3 shows the precision-reca"
P19-1468,P12-1013,0,0.0227334,"Missing"
P19-1468,P11-1062,0,0.162317,"how improvements over the raw link prediction scores. ele cte dp Abstract (B) run for presidency of be elected president of be nominated for presidency of Introduction Figure 1: A link prediction knowledge graph (A) and an Link prediction and entailment graph induction are often treated as different problems. The former (Figure 1A) is used to infer missing relations between entities in existing knowledge graphs (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013). The latter (Figure 1B) constructs entailment graphs with relations as nodes and entailment rules as edges between them (Berant et al., 2011, 2015; Hosseini et al., 2018) for the task of answering questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributional Inclusion Hypothesis which states that a word (relation) r entails another word (relation) q if and only if in any context that r can be"
P19-1468,D16-1146,0,0.0607028,"Missing"
P19-1468,P18-1011,0,0.112941,"ity scores for grounded logical rules as well as triples and learning entity and relation embeddings that score positive examples higher than negative ones. Guo et al. (2018) take an iterative approach where in each iteration a set of unseen triples are scored according to the current link prediction model and a small set of precomputed logical rules. The new triples and their scores are then used to update the current link prediction model. The above models need grounding of logical rules. A few recent works do not need grounding and are more space and time efficient (Demeester et al., 2016; Ding et al., 2018). They incorporate logical rules into distributed representations of relations. These models constrain entity or entity-pair vector representations to be nonnegative. They encourage partial ordering over relation embeddings based on implication rules; however, their methods can be only applied to (multi-)linear link prediction models such as ComplEx (Trouillon et al., 2016). In contrast, our method can be applied to any type of link prediction model. All these methods require entailment rules as their input. In most cases (Wang et al., 2015; Demeester et al., 2016; Guo et al., 2016), the entai"
P19-1468,P05-1014,0,0.431468,"g questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributional Inclusion Hypothesis which states that a word (relation) r entails another word (relation) q if and only if in any context that r can be used, q can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Kartsaklis and Sadrzadeh, 2016). They entailment graph (B) for entities of types politician,country. The solid lines are discovered correctly, but the dashed ones are missing. However, evidence from the link prediction model can be used to add the missing entailment rule in the entailment graph (B). Similarly, the entailment graph can be used to add the missing link in the knowledge graph (A). use types such as person, location and time, to disambiguate polysemous relations (e.g., person born in location and person born in time). Entailment graphs are then formed by imposing global constrain"
P19-1468,D16-1019,0,0.142714,"same patterns of entailments. Our method, in contrast, learns a new entailment score to improve local decisions, which in turn improves the entailment graphs. Entailment Rule Injection for link prediction. There are some attempts in recent years to improve link prediction by injecting entailment rules. Wang et al. (2015) incorporate various set of heuristic rules, including entailment rules, into embedding models for knowledge base completion. They formulate inference as an ILP problem, with the objective function generated from embeddings models and the constraints translated from the rules. Guo et al. (2016) extend the TransE model by defining plausibility scores for grounded logical rules as well as triples and learning entity and relation embeddings that score positive examples higher than negative ones. Guo et al. (2018) take an iterative approach where in each iteration a set of unseen triples are scored according to the current link prediction model and a small set of precomputed logical rules. The new triples and their scores are then used to update the current link prediction model. The above models need grounding of logical rules. A few recent works do not need grounding and are more spac"
P19-1468,C16-1268,0,0.26025,"Missing"
P19-1468,P16-2041,0,0.210562,"2.3 GHz machine with 256GB of RAM. 10 The entailment graphs of Berant et al. (2015) yield similar results. 5.1 Entailment Scores based on Link Prediction In this section, we compare the variants of our method to the previous state-of-the-art results on the Levy/Holt’s dataset. We compute similarity scores and report precision-recall curve by changing the threshold for entailment between 0 and 1. In order to have a fair comparison with Berant’s ILP method, we first test a set of rule-based constraints proposed by them (Berant et al., 2011). We also apply the lemma baseline heuristic process of Levy and Dagan (2016) before testing the methods. Figure 3 shows the precision-recall curve of all the methods in both local (A) and global (B) settings. From the SBOW methods, we only show the BInc score in the graphs as it got the best results on the development set. For Berant’s ILP method, we only have one point of precision and recall, as we had access to their entailment graphs for only one sparsity level. In both settings, Aug MC works better than all the other methods. This confirms that the link prediction method is indeed useful for finding entailment relations. Aug MC consistently outperforms MC suggest"
P19-1468,D14-1107,1,0.76789,"y of the triple being correct. We de2 note by S ∈ [0, 1]|R|×|E |the matrix containing triple probability scores. We define S(t1 , t2 ) ∈ 2 [0, 1]|R(t1 ,t2 )|×|E (t1 ,t2 ) |the submatrix of S with R(t1 , t2 ) as rows and E 2 (t1 , t2 ) as columns. We apply a link prediction model to a knowledge graph of predicate-argument structures extracted from text (§4.2). 2.2 Entailment Prediction The goal is to find entailment scores between all relations with the same types, where the 4737 3 For example by applying the Sigmoid function. entities can be in the same or opposite order (Berant et al., 2011; Lewis and Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2"
P19-1468,W14-2406,1,0.906373,"y of the triple being correct. We de2 note by S ∈ [0, 1]|R|×|E |the matrix containing triple probability scores. We define S(t1 , t2 ) ∈ 2 [0, 1]|R(t1 ,t2 )|×|E (t1 ,t2 ) |the submatrix of S with R(t1 , t2 ) as rows and E 2 (t1 , t2 ) as columns. We apply a link prediction model to a knowledge graph of predicate-argument structures extracted from text (§4.2). 2.2 Entailment Prediction The goal is to find entailment scores between all relations with the same types, where the 4737 3 For example by applying the Sigmoid function. entities can be in the same or opposite order (Berant et al., 2011; Lewis and Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2"
P19-1468,P98-2127,0,0.0642417,"et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can predict the probabi"
P19-1468,N19-1226,0,0.0219182,"28.43 Table 2: Link prediction results on the test set of NewsSpike for all entities (top) and infrequent entities (below). We test the effect of refining ConvE scores with entailment relations. scores (>0.95). 6 Related Work Link Prediction. In recent years, many link prediction models have been proposed that learn vector or matrix representations for relations and entities (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Wang et al., 2014; Lin et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018; Schlichtkrull et al., 2018; Nguyen et al., 2019). These models are trained by assigning higher plausibility scores to correct facts than incorrect ones. For example, the well-known TransE model (Bordes et al., 2013) captures relational similarity between entity pairs by considering a translation vector for the relations connecting them. In particular, it learns embeddings for entities and relations such that e~2 − e~1 ≈ ~r for any correct triple (r, e1 , e2 ). In our experiments we have used ConvE (Dettmers et al., 2018), however, our proposed score can be computed based on any link prediction model and the discovered entailment relations m"
P19-1468,N16-1054,1,0.846058,"9.26 46.10 1303.56 28.25 19.30 46.36 1154.06 28.33 19.29 46.60 1154.28 28.41 19.28 46.66 1118.09 28.43 Table 2: Link prediction results on the test set of NewsSpike for all entities (top) and infrequent entities (below). We test the effect of refining ConvE scores with entailment relations. scores (>0.95). 6 Related Work Link Prediction. In recent years, many link prediction models have been proposed that learn vector or matrix representations for relations and entities (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Wang et al., 2014; Lin et al., 2015; Toutanova et al., 2016; Nguyen et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018; Schlichtkrull et al., 2018; Nguyen et al., 2019). These models are trained by assigning higher plausibility scores to correct facts than incorrect ones. For example, the well-known TransE model (Bordes et al., 2013) captures relational similarity between entity pairs by considering a translation vector for the relations connecting them. In particular, it learns embeddings for entities and relations such that e~2 − e~1 ≈ ~r for any correct triple (r, e1 , e2 ). In our experiments we have used ConvE (Dettmers et al., 2018), however, our proposed s"
P19-1468,Q14-1030,1,0.839153,"nek et al., 2007); however, we chose to experiment on assertions extracted from raw text. This is because we can then evaluate the predicted entailments on existing entailment datasets with examples stated in natural language (§4.3). We use the multiple-source NewsSpike corpus of Zhang and Weld (2013). The NewsSpike corpus includes 550K news articles and is well-suited for finding entailment and paraphrasing relations as it includes different articles from different sources describing identical news stories. We use the triples released by Hosseini et al. (2018)6 who run the semantic parser of Reddy et al. (2014), GraphParser, to extract binary relations between a predicate and its arguments. GraphParser uses Combinatorial Categorial Grammer (CCG) syntactic derivations by running EasyCCG (Lewis and Steedman, 2014a). The parser converts sentences to neo-Davisonian semantics, a first order logic that uses event identifiers and extracts one binary relation for each event and pair of arguments (Parsons, 1990). The entities are typed by first linking to Freebase (Bollacker et al., 2008) and then selecting the most notable type of the entity from Freebase and mapping it to FIGER types (Ling 4739 6 Accessed"
P19-1468,N13-1008,0,0.475509,"her. The new entailment score outperforms prior state-of-the-art results on a standard entialment dataset and the new link prediction scores show improvements over the raw link prediction scores. ele cte dp Abstract (B) run for presidency of be elected president of be nominated for presidency of Introduction Figure 1: A link prediction knowledge graph (A) and an Link prediction and entailment graph induction are often treated as different problems. The former (Figure 1A) is used to infer missing relations between entities in existing knowledge graphs (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013). The latter (Figure 1B) constructs entailment graphs with relations as nodes and entailment rules as edges between them (Berant et al., 2011, 2015; Hosseini et al., 2018) for the task of answering questions from text. In this paper, we show that these two problems are complementary by demonstrating how link prediction can help identify entailments and how discovered entailments can help predict missing links. Methods to learn entailment graphs (Berant et al., 2011, 2015; Hosseini et al., 2018) process large text corpora to find local entailment scores between relations based on the Distributi"
P19-1468,C08-1107,0,0.904373,"∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can predict the probability of any triple being in the knowledge graph. Using pr"
P19-1468,P16-1136,0,0.379252,"elations that entail each other in both directions are regarded as paraphrases. 4736 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4736–4746 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics near can be used to answer such questions. 2 On the other hand, link prediction (or knowledge base completion) models are based on distributional methods and directly predict the source data. These models have received much attention in the recent years (Socher et al., 2013; Bordes et al., 2013; Riedel et al., 2013; Toutanova et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018). The current methods learn embeddings for all entities and relations and a function to score any potential relation between the entities. One of the main capabilities of these models is that they implicitly exploit entailment relations such as person born in country entails person be from country (Riedel et al., 2013). However, entailment relations are not learned explicitly. For example, we cannot simply compute the cosine similarity of the vector representations of the two relations to detect the entailment between them, because cosine similar"
P19-1468,W03-1011,0,0.830383,"nd Steedman, 2014b; Hosseini et al., 2018). We denote by W (t1 , t2 ) ∈ [0, 1]|R(t1 ,t2 )|×|R(t1 ,t2 ) |the (sparse) matrix containing all similarity scores Wr,q between relations r, q ∈ R(t1 , t2 ). We define W the (block diagonal) matrix consisting of all the similarity matrices W (t1 , t2 ). For a δ > 0, we define typed entailment  graphs as Gδ (t1 , t2 ) = R(t1 , t2 ), Eδ (t1 , t2 ) , where R(t1 , t2 ) are the nodes and E(t1 , t2 ) = {(r, q)|r, q ∈ R(t1 , t2 ), Wr,q ≥ δ} are the edges of the entailment graphs. Existing entailment similarity measures for relation entailment such as Weeds (Weeds and Weir, 2003), Lin (Lin, 1998), and Balanced Inclusion (BInc; Szpektor and Dagan, 2008) are typically defined on feature vectors consisting of entitypairs (e.g., Obama-Hawaii), where the values are frequencies or pointwise mutual information (PMI) between the relations and the features (Berant et al., 2011, 2012, 2015). While these methods currently hold state-of-the-art results on relation entailment datasets (Hosseini et al., 2018), they suffer from low recall because the feature vectors are usually sparse and do not have high overlap with each other. The link prediction models, on the other hand, can pr"
P19-1468,D13-1183,0,0.342652,"We then describe the details of the link prediction model (§4.2), the datasets used to test the models (§4.3) and the baseline systems (§4.4). 4.1 Text Corpus Link prediction models are often applied to existing knowledge graphs such as Freebase (Bollacker et al., 2008), DBPedia (Lehmann et al., 2015) and Yago (Suchanek et al., 2007); however, we chose to experiment on assertions extracted from raw text. This is because we can then evaluate the predicted entailments on existing entailment datasets with examples stated in natural language (§4.3). We use the multiple-source NewsSpike corpus of Zhang and Weld (2013). The NewsSpike corpus includes 550K news articles and is well-suited for finding entailment and paraphrasing relations as it includes different articles from different sources describing identical news stories. We use the triples released by Hosseini et al. (2018)6 who run the semantic parser of Reddy et al. (2014), GraphParser, to extract binary relations between a predicate and its arguments. GraphParser uses Combinatorial Categorial Grammer (CCG) syntactic derivations by running EasyCCG (Lewis and Steedman, 2014a). The parser converts sentences to neo-Davisonian semantics, a first order lo"
P19-1629,W13-2322,0,0.192662,"ient(e2 , x2 ) male(x1 ) e2 ≤ e1 because(k1 , k2 ) b. SDRS k1 k2 DRS max(x1) fall(e1) Agent(e1,x1) now(t1) temp_before(e1, t1) because(k1, k2) DRS POS DRS Max fell . john(x2) push(e2) Patient(e2, x1) male(x1) John might push him . temp_before(e2, e1) Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez"
P19-1629,N18-1118,0,0.0307579,"er than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as information retrieval (Zou et al., 2014), summarization (Goyal and Eisenstein, 2016), conversational agents (Vinyals and Le, 2015), machine translation (Sim Smith, 2017; Bawden et al., 2018), and question anwsering (Rajpurkar et al., 2018). Our contributions in this work can be summarized as follows: 1) We formally define Discourse Representation Tree structures for sentences and documents; 2) We present a general framework for parsing discourse structures of arbitrary length and granularity; our framework is based on a neural model which decomposes the generation of meaning representations into three stages following a coarse-to-fine approach (Liu et al., 2018; Dong and Lapata, 2018); 3) We further demonstrate that three modeling innovations are key to tree structure prediction:"
P19-1629,P17-1112,0,0.0190714,"hao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017). In this work we focus on parsing formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). Figure 1: Meaning representation for the discourse “Max fell. John might push him.” in box-like format (top) and as a tree (bottom). Red lines indicate terminals corresponding to words and green lines indicate non-terminals corresponding to sentences.  and POS are modality operators for possibility. DRT is a popular theory of meaning representation (Kamp, 1981; Kamp and Reyle, 1993; Asher, 1993; Asher and Lascarides, 2003) designed to account for a vari"
P19-1629,P17-1005,1,0.927333,"lemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017). In this work we focus on parsing formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). Figure 1: Meaning representation for the discourse “Max fell. John might push him.” in box-like format (top) and as a tree (bottom). Red lines indicate terminals corresponding to words and green lines indicate non-terminals corresponding to sentences.  and POS are modality operators for possibility. DRT is a popular theory of meaning representation"
P19-1629,H94-1010,0,0.0896294,"a. DRT parsing resembles the task of mapping sentences to Abstract Meaning Representations (AMRs; Banarescu et al. 2013) in that logical forms are broad-coverage, they represent compositional utterances with varied vocabu6248 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lary and syntax and are ungrounded, i.e., they are not tied to a specific database from which answers to queries might be retrieved (Zelle and Mooney, 1996; Cheng et al., 2017; Dahl et al., 1994). Our work departs from previous generalpurpose semantic parsers (Flanigan et al., 2016; Foland and Martin, 2017; Lyu and Titov, 2018; Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis o"
P19-1629,E17-1051,1,0.867659,"copying (Gu et al., 2016) and attention (Mikolov 6255 et al., 2013). Our copying mechanism is more specialized and linguistically-motivated: it considers the semantics of the input text for deciding which tokens to copy. While our multi-attention mechanism is fairly general, it extracts features from different encoder representations (word- or sentencelevel) and flexibly integrates supervised and unsupervised attention in a unified framework. A few recent approaches focus on the alignment between semantic representations and input text, either as a preprocessing step (Foland and Martin, 2017; Damonte et al., 2017) or as a latent variable (Lyu and Titov, 2018). Instead, our parser implicitly models word-level alignments with multi-attention and explicitly obtains sentence-level alignments with supervised attention, aiming to jointly train a semantic parser. 8 Conclusions In this work we proposed a novel semantic parsing task to obtain Discourse Representation Tree Structures and introduced a general framework for parsing texts of arbitrary length and granularity. Experimental results on two benchmarks show that our parser is able to obtain reasonably accurate sentence- and document-level discourse repre"
P19-1629,P16-1004,1,0.923501,"age to machine interpretable meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017). In this work we focus on parsing formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). Figure 1: Meaning representation for the discourse “Max fell. John might push him.” in box-like for"
P19-1629,P18-1068,1,0.847888,"nstein, 2016), conversational agents (Vinyals and Le, 2015), machine translation (Sim Smith, 2017; Bawden et al., 2018), and question anwsering (Rajpurkar et al., 2018). Our contributions in this work can be summarized as follows: 1) We formally define Discourse Representation Tree structures for sentences and documents; 2) We present a general framework for parsing discourse structures of arbitrary length and granularity; our framework is based on a neural model which decomposes the generation of meaning representations into three stages following a coarse-to-fine approach (Liu et al., 2018; Dong and Lapata, 2018); 3) We further demonstrate that three modeling innovations are key to tree structure prediction: a supervised hierarchical attention mechanism, a linguistically-motivated copy strategy, and constraint-based inference to ensure wellformed DRTS output; 4) Experimental results on sentence- and document-level benchmarks show that our model outperforms competitive baselines by a wide margin. We release our code and DRTS benchmarks in the hope of driving research in semantic parsing further.2 1 The shared task on Discourse Representation Structure parsing in IWCS 2019. https://sites.google.com/ vie"
P19-1629,S16-1186,0,0.0307466,"ations (AMRs; Banarescu et al. 2013) in that logical forms are broad-coverage, they represent compositional utterances with varied vocabu6248 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lary and syntax and are ungrounded, i.e., they are not tied to a specific database from which answers to queries might be retrieved (Zelle and Mooney, 1996; Cheng et al., 2017; Dahl et al., 1994). Our work departs from previous generalpurpose semantic parsers (Flanigan et al., 2016; Foland and Martin, 2017; Lyu and Titov, 2018; Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as informati"
P19-1629,P17-1043,0,0.158148,"et al. 2013) in that logical forms are broad-coverage, they represent compositional utterances with varied vocabu6248 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lary and syntax and are ungrounded, i.e., they are not tied to a specific database from which answers to queries might be retrieved (Zelle and Mooney, 1996; Cheng et al., 2017; Dahl et al., 1994). Our work departs from previous generalpurpose semantic parsers (Flanigan et al., 2016; Foland and Martin, 2017; Lyu and Titov, 2018; Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as information retrieval (Zou et al.,"
P19-1629,W16-5903,0,0.0196135,"Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as information retrieval (Zou et al., 2014), summarization (Goyal and Eisenstein, 2016), conversational agents (Vinyals and Le, 2015), machine translation (Sim Smith, 2017; Bawden et al., 2018), and question anwsering (Rajpurkar et al., 2018). Our contributions in this work can be summarized as follows: 1) We formally define Discourse Representation Tree structures for sentences and documents; 2) We present a general framework for parsing discourse structures of arbitrary length and granularity; our framework is based on a neural model which decomposes the generation of meaning representations into three stages following a coarse-to-fine approach (Liu et al., 2018; Dong and Lapa"
P19-1629,P16-1154,0,0.0429728,"Recently, Liu et al. (2018) conceptualized DRT parsing as a tree structure prediction problem which they modeled with a series of encoder-decoder architectures. van Noord et al. (2018b) adapt models from neural machine translation (Klein et al., 2017) to DRT parsing, also following a graph-based representation. Previous work has focused exclusively on sentences, whereas we design a general framework for parsing sentences and documents and provide a model which can be used interchangeably for both. Various mechanisms have been proposed to improve sequence-to-sequence models including copying (Gu et al., 2016) and attention (Mikolov 6255 et al., 2013). Our copying mechanism is more specialized and linguistically-motivated: it considers the semantics of the input text for deciding which tokens to copy. While our multi-attention mechanism is fairly general, it extracts features from different encoder representations (word- or sentencelevel) and flexibly integrates supervised and unsupervised attention in a unified framework. A few recent approaches focus on the alignment between semantic representations and input text, either as a preprocessing step (Foland and Martin, 2017; Damonte et al., 2017) or"
P19-1629,P82-1020,0,0.817306,"Missing"
P19-1629,P16-1002,0,0.0242781,"table meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017). In this work we focus on parsing formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). Figure 1: Meaning representation for the discourse “Max fell. John might push him.” in box-like format (top) and as a tr"
P19-1629,D14-1181,0,0.00702113,"embeddings ewij , pre-trained word embeddings e¯wij , and lemma embeddings e`ij (where f (·) is a non-linear function). Embeddings ewij and e`ij are randomly initialized and tuned during training, while e¯wij are fixed. The encoder represents words and sentences in a unified framework compatible with sentenceand document-level DRTS parsing. Our experiments employed recurrent neural networks with long-short term memory units (LSTMs; Hochreiter and Schmidhuber 1997), however, there is nothing inherent in our framework that is LSTM specific. For instance, representations based on convolutional (Kim, 2014) or recursive neural networks (Socher et al., 2012) are also possible. Word Representation We encode the input text with a bidirectional LSTM (biLSTM): ←→ ←−→ [hx00 : hxmn ] = biLSTM(x00 : xmn ), ←→ where hxij denotes the hidden representation of the encoder for xij , which denotes the input representation of token j in sentence i. 6 The left boundary of sentence i is the right boundary of sentence i − 1, the left boundary of the first sentence is hdi, and the right boundary of the last sentence is h/di. 6250 where g v (·) is a linear function with the name v.7 Given encoder representations Hx"
P19-1629,P17-4012,0,0.041168,"the parser cannot learn reliable representations for them. Moreover, as the size of documents increases, ambiguity for the resolution of coreferring expressions increases, suggesting that explicit modeling of anaphoric links might be necessary. 7 Related Work Le and Zuidema (2012) were the first to train a data-driven DRT parser using a graph-based representation. Recently, Liu et al. (2018) conceptualized DRT parsing as a tree structure prediction problem which they modeled with a series of encoder-decoder architectures. van Noord et al. (2018b) adapt models from neural machine translation (Klein et al., 2017) to DRT parsing, also following a graph-based representation. Previous work has focused exclusively on sentences, whereas we design a general framework for parsing sentences and documents and provide a model which can be used interchangeably for both. Various mechanisms have been proposed to improve sequence-to-sequence models including copying (Gu et al., 2016) and attention (Mikolov 6255 et al., 2013). Our copying mechanism is more specialized and linguistically-motivated: it considers the semantics of the input text for deciding which tokens to copy. While our multi-attention mechanism is f"
P19-1629,D16-1116,0,0.0420033,"Missing"
P19-1629,D11-1140,0,0.0342763,"2 ) b. SDRS k1 k2 DRS max(x1) fall(e1) Agent(e1,x1) now(t1) temp_before(e1, t1) because(k1, k2) DRS POS DRS Max fell . john(x2) push(e2) Patient(e2, x1) male(x1) John might push him . temp_before(e2, e1) Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 201"
P19-1629,C12-1094,0,0.397289,"es and beyond. In general, DeepCopy has an advantage over comparison systems due to the more sophisticated alignment information and the fact that it aims to generate global document-level structures. Our results also indicate that modeling longer documents which are relatively few in the training set is challenging mainly because the parser cannot learn reliable representations for them. Moreover, as the size of documents increases, ambiguity for the resolution of coreferring expressions increases, suggesting that explicit modeling of anaphoric links might be necessary. 7 Related Work Le and Zuidema (2012) were the first to train a data-driven DRT parser using a graph-based representation. Recently, Liu et al. (2018) conceptualized DRT parsing as a tree structure prediction problem which they modeled with a series of encoder-decoder architectures. van Noord et al. (2018b) adapt models from neural machine translation (Klein et al., 2017) to DRT parsing, also following a graph-based representation. Previous work has focused exclusively on sentences, whereas we design a general framework for parsing sentences and documents and provide a model which can be used interchangeably for both. Various mec"
P19-1629,P11-1060,0,0.0256204,") now(t1 ) e 1 ≤ t1 k2 : : john(x2 ) push(e2 ) Patient(e2 , x2 ) male(x1 ) e2 ≤ e1 because(k1 , k2 ) b. SDRS k1 k2 DRS max(x1) fall(e1) Agent(e1,x1) now(t1) temp_before(e1, t1) because(k1, k2) DRS POS DRS Max fell . john(x2) push(e2) Patient(e2, x1) male(x1) John might push him . temp_before(e2, e1) Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata,"
P19-1629,P18-1040,1,0.760495,"coverage, they represent compositional utterances with varied vocabu6248 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lary and syntax and are ungrounded, i.e., they are not tied to a specific database from which answers to queries might be retrieved (Zelle and Mooney, 1996; Cheng et al., 2017; Dahl et al., 1994). Our work departs from previous generalpurpose semantic parsers (Flanigan et al., 2016; Foland and Martin, 2017; Lyu and Titov, 2018; Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as information retrieval (Zou et al., 2014), summarization (Goyal and Eisens"
Q14-1036,P12-2017,0,0.0252599,"Missing"
Q14-1036,N10-1081,1,0.0610669,"parametric counterparts (Yao et al., 2009). A common approach to address this computational bottleneck is through variational inference (Wainwright and Jordan, 2008). One of the advantages of variational inference is that it can be easily parallelized (Nallapati et al., 2007) or transformed into an online algorithm (Hoffman et al., 2010), which often converges in fewer iterations than batch variational inference. Past variational inference techniques for adaptor grammars assume a preprocessing step that looks at all available data to establish the support of these nonparametric distributions (Cohen et al., 2010). Thus, these past approaches are not directly amenable to online inference. Markov chain Monte Carlo (MCMC) inference, an alternative to variational inference, does not have this disadvantage. MCMC is easier to implement, and it discovers the support of nonparametric models during inference rather than assuming it a priori. We apply stochastic hybrid inference (Mimno et al., 2012) to adaptor grammars to get the best of both worlds. We interleave MCMC inference inside variational inference. This preserves the scalability of variational inference while adding the sparse statistics and improved"
Q14-1036,I05-3017,0,0.0375005,"unigram and collocation grammars introduced in Johnson and Goldwater (2009) as listed in Table 1. Figure 2 illustrates the word segmentation accuracy in terms of word token F1 -scores on brent against the number of inside-outside function calls for all three approaches using unigram and collocation grammars. In both cases, our ONLINE approach converges faster than MCMC and VARIATIONAL approaches, yet yields comparable or better performance when seeing more data. In addition to the brent corpus, we also evaluate three approaches on three other Chinese datasets compiled by Xue et al. (2005) and Emerson (2005):8 • Chinese Treebank 7.0 (ctb7): 162k sentences, 57k distinct words, 4.5k distinct characters; our model under κ = {0.7, 0.9} and τ = {64, 256}. 8 We use all punctuation as natural delimiters (i.e., words cannot cross punctuation). 9 Their results are not directly comparable: they use different subsets and assume different preprocessing. 10 Note that this is only an approximation to the true held-out likelihood, since it is impossible to enumerate all the possible parse trees and hence compute the likelihood for a given sentence under the model. 11 We train all models with 5 topics with setti"
Q14-1036,D10-1028,1,0.82192,"former German chancellor) first appeared in minibatch 300, was successfully picked up by our model, and became one of the top ranked words in the topic. 6 Conclusion Probabilistic modeling is a useful tool in understanding unstructured data or data where the structure is latent, like language. However, developing these models is often a difficult process, requiring significant machine learning expertise. Adaptor grammars offer a flexible and quick way to prototype and test new models. Despite expensive inference, they have been used for topic modeling (Johnson, 2010), discovering perspective (Hardisty et al., 2010), segmentation (Johnson and Goldwater, 2009), and grammar induction (Cohen et al., 2010). We have presented a new online, hybrid inference scheme for adaptor grammars. Unlike previous approaches, it does not require extensive preprocessing. It is also able to faster discover useful structure in text; with further development, these algorithms could further speed the development and application of new nonparametric models to large datasets. Acknowledgments We would like to thank the anonymous reviewers, Kristina Toutanova, Mark Johnson, and Ke Wu for insightful discussions. This work was suppor"
Q14-1036,N09-1036,0,0.37781,"s (Mochihashi et al., 2009). We impose a reward term for longer phrases in addition to f˜ and sort all adapted productions in TNGa using the ranking score Λ(a ⇒ za,i ) = f˜(l) (a ⇒ za,i ) · log( · |s |+ 1), where |s |is the number of yields in production a ⇒ za,i . Because  decreases each minibatch, the reward for long phrases diminishes. This is similar to an annealed version of Cohen et al. (2010)—where the reward for long phrases is fixed, see also Mochihashi et al. (2009). After sorting, we remove all but the top Ka adapted productions. Rederiving Adapted Productions For MCMC inference, Johnson and Goldwater (2009) observe that atoms already associated with a yield may have trees that do not explain their yield well. They propose table label resampling to rederive yields. In our approach this is equivalent to “mutating” some derivations in a TNG . After pruning rules every u minibatches, we perform table label resampling for adapted nonterminals from general to specific (i.e., a topological sort). This provides better expected counts n(r, •) for rules used in phrasestructure subtrees. Empirically, we find table label resampling only marginally improves the wordsegmentation result. Initialization Our inf"
Q14-1036,N07-1018,0,0.0145467,"iational distribution over trees φ using the empirical distribution σ d , i.e., φd,i ∝ I[σd,j = td,i , ∀σd,j ∈ σ d ]. (5) This leads to a sparse approximation of variational distribution φ.3 Previous inference strategies (Johnson et al., 2006; B¨orschinger and Johnson, 2012) for adaptor grammars have used sampling. The adaptor grammar inference methods use an approximate PCFG to emulate the marginalized Pitman-Yor distributions 3 In our experiments, we use ten samples. 468 at each nonterminal. Given this approximate PCFG, we can then sample a derivation z for string x from the possible trees (Johnson et al., 2007). Sampling requires a derived PCFG G 0 that approximates the distribution over tree derivations conditioned on a yield. It includes the original PCFG rules R = {c → β} that define the base distribution and the new adapted productions R0 = {c ⇒ z, z ∈ TNG c }. Under G 0 , the probability θ 0 of adapted production c ⇒ z is   Eq [log πc,i ], if TNGc (i) = z 0 log θc⇒z = Eq [log πc,Kc ] + Eq [log θc⇒z ], (6)   otherwise where Kc is the truncation level of TNGc and πc,Kc represents the left-over stick weights in the stickbreaking process for adaptor c ∈ M . θc⇒z represents the probability of g"
Q14-1036,P10-1117,0,0.388833,"ximation to the true held-out likelihood, since it is impossible to enumerate all the possible parse trees and hence compute the likelihood for a given sentence under the model. 11 We train all models with 5 topics with settings: TNG refinement interval u = 100, truncation size KTopic = 3k, and the mini-batch size B = 50. We observe a similar behavior under κ ∈ {0.7, 0.9} and τ ∈ {64, 256}. 473 ⌧ : 512 512 128 128  pmi coherence pmi 1 1 1 10 0 10 101 0 10 0000 00 1 1 10 10 10 10 0 10 100 0000 1 1 10 10 10 1 10 1000 0000 0 1 Topic models often can be replicated using a carefully crafted PCFG (Johnson, 2010). These powerful extensions can capture topical collocations and sticky topics; these embelishments could further improve NLP applications of simple unigram topic models such as word sense disambiguation (Boyd-Graber and Blei, 2007), part of speech ⌧: 3232 1 5.2 Infinite Vocabulary Topic Modeling ⌧: 0.6 0.8 We compare our inference method against other approaches on F1 score. While other unsupervised word segmentation systems are available (Mochihashi et al. (2009), inter alia),9 our focus is on a direct comparison of inference techniques for adaptor grammar, which achieve competitive (if not"
Q14-1036,P09-1012,0,0.0240337,"ucts and expands all TNG s on the fly. To prevent the TNG from growing unwieldy, we prune TNG after every u minibatches. As a result, we need to impose an ordering over all the parse trees in the TNG . The underlying PYGEM distribution implicitly places an ranking over all the atoms according to their corresponding sufficient statistics (Kurihara et al., 2007), as shown in Equation 9. It measures the “usefulness” of every adapted production throughout inference process. In addition to accumulated sufficient statistics, Cohen et al. (2010) add a secondary term to discourage short constituents (Mochihashi et al., 2009). We impose a reward term for longer phrases in addition to f˜ and sort all adapted productions in TNGa using the ranking score Λ(a ⇒ za,i ) = f˜(l) (a ⇒ za,i ) · log( · |s |+ 1), where |s |is the number of yields in production a ⇒ za,i . Because  decreases each minibatch, the reward for long phrases diminishes. This is similar to an annealed version of Cohen et al. (2010)—where the reward for long phrases is fixed, see also Mochihashi et al. (2009). After sorting, we remove all but the top Ka adapted productions. Rederiving Adapted Productions For MCMC inference, Johnson and Goldwater (2009"
Q14-1036,P12-1046,0,0.0132947,"unsupervised models of grammar productions. This flexibility comes at the cost of expensive inference. We address the difficulty of inference through an online algorithm which uses a hybrid of Markov chain Monte Carlo and variational inference. We show that this inference strategy improves scalability without sacrificing performance on unsupervised word segmentation and topic modeling tasks. 1 Introduction Nonparametric Bayesian models are effective tools to discover latent structure in data (M¨uller and Quintana, 2004). These models have had great success in text analysis, especially syntax (Shindo et al., 2012). Nonparametric distributions provide support over a countably infinite long-tailed distributions common in natural language (Goldwater et al., 2011). We focus on adaptor grammars (Johnson et al., 2006), syntactic nonparametric models based on probabilistic context-free grammars. Adaptor grammars weaken the strong statistical independence assumptions PCFGs make (Section 2). The weaker statistical independence assumptions that adaptor grammars make come at the cost of expensive inference. Adaptor grammars are not alone in this trade-off. For example, nonparametric extensions of topic models (Te"
Q14-1036,P14-1004,1,\N,Missing
Q16-1030,N09-1003,0,0.20501,"Missing"
Q16-1030,P98-1013,0,0.651476,"; Mikolov et al., 2013c) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2015). While these general purpose word embeddings have achieved significant improvement in various tasks in NLP, it has been discovered that further tuning of these continuous word representations for specific tasks improves their performance by a larger margin. For example, in dependency parsing, word embeddings could be tailored to capture similarity in terms of context within syntactic parses (Bansal et al., 2014) or they could be refined using semantic lexicons such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) to improve various similarity tasks (Yu and Dredze, 2014; Faruqui et al., 2015; Rothe and Sch¨utze, 2015). This paper proposes a method to encode prior semantic knowledge in spectral word embeddings (Dhillon et al., 2015). Spectral learning algorithms are of great interest for their speed, scalability, theoretical guarantees and performance in various NLP applications. These algorithms are no strangers to word embeddings either. In latent semantic analysis (LSA, (Deerwester et al., 1990; Landauer et al., 1998)), word embeddings are learn"
Q16-1030,D14-1034,0,0.0417964,"ors of words which are adjacent in the prior knowledge graph to be close to each other in the new 424 5.3 Evaluation Benchmarks We evaluated the quality of our eigenword embeddings on three different tasks: word similarity, geographic analogies and NP bracketing. 7 https://github.com/mfaruqui/ retrofitting. sampled from words that occur at least 700 times in a large web corpus. The datasets, MTurk-287 (Radinsky et al., 2011) and MTurk-771 (Halawi et al., 2012), were scored by Amazon Mechanical Turk workers for relatedness of English word pairs. The YP-130 (Yang and Powers, 2005) and Verb-143 (Baker et al., 2014) datasets were developed for verb similarity predictions. The last two datasets, MC-30 (Miller and Charles, 1991) and RG-65 (Rubenstein and Goodenough, 1965) consist of 30 and 65 noun pairs respectively. For each dataset, we calculate the cosine similarity between the vectors of word pairs and measure Spearman’s rank correlation coefficient between the scores produced by the embeddings and human ratings. We report the average of the correlations on all 11 datasets. Each word similarity task in the above list represents a different aspect of word similarity, and as such, averaging the results p"
Q16-1030,P14-2131,0,0.0661786,"word embeddings and evaluating them on a myriad of datasets. 1 Introduction In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings. Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space. Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependency parsing (Bansal et al., 2014), unsupervised learning (Parikh et al., 2014) and others. Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets. These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks (Bengio et al., 2003; Mikolov et al., 2013b; Mikolov et al., 2013c) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2015). While these general p"
Q16-1030,D13-1167,0,0.0146295,"atrix can be constructed to encode prior knowledge directly to improve the quality of word vectors. Wang et al. (2015) investigate the notion of relatedness in embedding models by incorporating syntactic and lexicographic knowledge. In spectral learning, Yih et al. (2012) augment the word co-occurrence matrix on which LSA operates with relational information such that synonyms will tend to have positive cosine similarity, and antonyms will tend to have negative similarities. Their vector space representation successfully projects synonyms and antonyms on opposite sides in the projected space. Chang et al. (2013) further generalize this approach to encode multiple relations (and not just opposing relations, such as synonyms and antonyms) using multi-relational LSA. In spectral learning, most of the studies on incorporating prior knowledge in word vectors focus on LSA based word embeddings (Yih et al., 2012; Chang et al., 2013; Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). From the technical perspective, our work is also related to that of Jagarlamudi et al. (2011), who showed how to generalize CCA so that it uses locality preserving projections (He and Niyogi, 2004). They also assu"
Q16-1030,W15-1523,0,0.0458809,"Missing"
Q16-1030,E14-1049,0,0.0513205,"4 81.0 81.3 81.8 81.4 80.3 81.2 81.3 80.7 81.4 81.7 81.7 81.1 81.9 81.4 81.0 80.8 81.0 80.7 C F I FN 78.7 80.5 80.2 82.0 80.7 81.0 80.7 80.9 80.8 81.0 81.2 80.6 80.0 80.4 80.4 Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks (A–C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer (2014) and Dhillon et al. (2015) respectively. The second middle blocks (D–F) show the results of our eigenword embeddings encoded with prior knowledge using our method. Each row in the block corresponds to a specific use of an α value (smoothing factor), as described in Figure 3. In the lower blocks (G–I) we take the word embeddings from the second block, and retrofit them using the method of Faruqui et al. (2015). Best results in each block are in bold. art word embeddings, such as Glove (Pennington et al., 2014), Skip-Gram (Mikolov et al., 2013b), Global Context (Huang et al., 2012) and Multiling"
Q16-1030,P15-2076,0,0.0259764,"s to . This dataset consists of 506 word pairs. For given word pairs, a:b c:d where d is unknown, we use the vector offset method (Mikolov et al., 2013b), i.e., we compute a vector v = vb − va + vc where va , vb and vc are vector representations of the words a, b and c respectively; we then return the word d with the greatest cosine similarity to v. NP Bracketing Here the goal is to identify the correct bracketing of a three-word noun (Lazaridou et al., 2013). For example, the bracketing of annual (price growth) is “right,” while the bracketing of (entry level) machine is “left.” Similarly to Faruqui and Dyer (2015), we concatenate the word vectors of the three words, and use this vector for binary classification into left or right. Since most of the datasets that we evaluate on in this paper are not standardly separated into development and test sets, we report all results we calculated (with respect to hyperparameter differences) and do 425 not select just a subset of the results. 5.4 Evaluation Preliminary Experiments In our first set of experiments, we vary the dimension of the word embedding vectors. We try m ∈ {50, 100, 200, 300}. Our experiments showed that the results consistently improve when th"
Q16-1030,N15-1184,0,0.120857,"ngs have achieved significant improvement in various tasks in NLP, it has been discovered that further tuning of these continuous word representations for specific tasks improves their performance by a larger margin. For example, in dependency parsing, word embeddings could be tailored to capture similarity in terms of context within syntactic parses (Bansal et al., 2014) or they could be refined using semantic lexicons such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) to improve various similarity tasks (Yu and Dredze, 2014; Faruqui et al., 2015; Rothe and Sch¨utze, 2015). This paper proposes a method to encode prior semantic knowledge in spectral word embeddings (Dhillon et al., 2015). Spectral learning algorithms are of great interest for their speed, scalability, theoretical guarantees and performance in various NLP applications. These algorithms are no strangers to word embeddings either. In latent semantic analysis (LSA, (Deerwester et al., 1990; Landauer et al., 1998)), word embeddings are learned by performing SVD on the word by document matrix. Recently, Dhillon et al. (2015) have proposed to use canonical correlation analysi"
Q16-1030,N13-1092,0,0.254469,"Missing"
Q16-1030,J15-4004,0,0.139688,"introduced in the off-theshelf embeddings as a post-processing step (Faruqui et al., 2015; Rothe and Sch¨utze, 2015). In this paper, we focus on the retrofitting approach of Faruqui et al. (2015). Word Similarity For the word similarity task we experimented with 11 different widely used benchmarks. The WS-353-ALL dataset (Finkelstein et al., 2002) consists of 353 pairs of English words with their human similarity ratings. Later, Agirre et al. (2009) re-annotated WS-353-ALL for similarity (WS-353-SIM) and relatedness (WS-353-REL) with specific distinctions between them. The SimLex999 dataset (Hill et al., 2015) was built to measure how well models capture similarity, rather than relatedness or association. The MEN-TR-3000 dataset (Bruni et al., 2014) consists of 3000 word pairs Retrofitting works by optimizing an objective function which has two terms: one that tries to keep the distance between the word vectors close to the original distances, and the other which enforces the vectors of words which are adjacent in the prior knowledge graph to be close to each other in the new 424 5.3 Evaluation Benchmarks We evaluated the quality of our eigenword embeddings on three different tasks: word similarity"
Q16-1030,P12-1092,0,0.0566106,".7 81.7 81.2 81.0 82.4 81.0 81.3 81.8 81.4 80.3 81.2 81.3 80.7 81.4 81.7 81.7 81.1 81.9 81.4 81.0 80.8 81.0 80.7 C F I FN 78.7 80.5 80.2 82.0 80.7 81.0 80.7 80.9 80.8 81.0 81.2 80.6 80.0 80.4 80.4 Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks (A–C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer (2014) and Dhillon et al. (2015) respectively. The second middle blocks (D–F) show the results of our eigenword embeddings encoded with prior knowledge using our method. Each row in the block corresponds to a specific use of an α value (smoothing factor), as described in Figure 3. In the lower blocks (G–I) we take the word embeddings from the second block, and retrofit them using the method of Faruqui et al. (2015). Best results in each block are in bold. art word embeddings, such as Glove (Pennington et al., 2014), Skip-Gram (Mikolov et al., 2013b), Global Context (Huang et"
Q16-1030,D12-1002,0,0.067271,"Missing"
Q16-1030,D13-1196,0,0.0198438,"o ” where d is unknown. We report results on a subset of this dataset which focuses on finding capitals of common countries, e.g., Greece is to Athens as Iraq is to . This dataset consists of 506 word pairs. For given word pairs, a:b c:d where d is unknown, we use the vector offset method (Mikolov et al., 2013b), i.e., we compute a vector v = vb − va + vc where va , vb and vc are vector representations of the words a, b and c respectively; we then return the word d with the greatest cosine similarity to v. NP Bracketing Here the goal is to identify the correct bracketing of a three-word noun (Lazaridou et al., 2013). For example, the bracketing of annual (price growth) is “right,” while the bracketing of (entry level) machine is “left.” Similarly to Faruqui and Dyer (2015), we concatenate the word vectors of the three words, and use this vector for binary classification into left or right. Since most of the datasets that we evaluate on in this paper are not standardly separated into development and test sets, we report all results we calculated (with respect to hyperparameter differences) and do 425 not select just a subset of the results. 5.4 Evaluation Preliminary Experiments In our first set of experi"
Q16-1030,N13-1090,0,0.38127,"ustification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets. 1 Introduction In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings. Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space. Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependency parsing (Bansal et al., 2014), unsupervised learning (Parikh et al., 2014) and others. Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets. These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks (Bengio et al., 2003; Mikolov et al., 2013b; Mikolov et al., 2013c) or spectral methods (Deerwester et al., 1990;"
Q16-1030,P16-1146,1,0.776071,"h et al., 2012; Chang et al., 2013; Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). From the technical perspective, our work is also related to that of Jagarlamudi et al. (2011), who showed how to generalize CCA so that it uses locality preserving projections (He and Niyogi, 2004). They also assume the existence of a weight matrix in a multi-view setting that describes the distances between pairs of points in the two views. More generally, CCA is an important component for spectral learning algorithms in the unsupervised setting and with latent variables (Cohen et al., 2014; Narayan and Cohen, 2016; Stratos et al., 2016). Our method for incorporating prior knowledge into CCA could potentially be transferred to these algorithms. 7 Conclusion We described a method for incorporating prior knowledge into CCA. Our method requires a relatively simple change to the original canonical correlation analysis, where extra counts are added to the matrix on which singular value decomposition is performed. We used our method to derive word embeddings in the style of eigenwords, and tested them on a set of datasets. Our results demonstrate several advantages of encoding prior knowledge into eigenword e"
Q16-1030,P14-1100,1,0.803585,"iad of datasets. 1 Introduction In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings. Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space. Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependency parsing (Bansal et al., 2014), unsupervised learning (Parikh et al., 2014) and others. Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets. These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks (Bengio et al., 2003; Mikolov et al., 2013b; Mikolov et al., 2013c) or spectral methods (Deerwester et al., 1990; Dhillon et al., 2015). While these general purpose word embeddings have achieved signific"
Q16-1030,D14-1162,0,0.0836635,"keting WN PD 79.5 79.4 80.4 81.5 79.1 80.5 81.8 82.7 81.7 81.2 81.0 82.4 81.0 81.3 81.8 81.4 80.3 81.2 81.3 80.7 81.4 81.7 81.7 81.1 81.9 81.4 81.0 80.8 81.0 80.7 C F I FN 78.7 80.5 80.2 82.0 80.7 81.0 80.7 80.9 80.8 81.0 81.2 80.6 80.0 80.4 80.4 Table 1: Results for the word similarity datasets, geographic analogies and NP bracketing. The first upper blocks (A–C) present the results with retrofitting. NPK stands for no prior knowledge (no retrofitting is used), WN for WordNet, PD for PPDB and FN for FrameNet. Glove, Skip-Gram, Global Context, Multilingual and Eigen are the word embeddings of Pennington et al. (2014), Mikolov et al. (2013b), Huang et al. (2012), Faruqui and Dyer (2014) and Dhillon et al. (2015) respectively. The second middle blocks (D–F) show the results of our eigenword embeddings encoded with prior knowledge using our method. Each row in the block corresponds to a specific use of an α value (smoothing factor), as described in Figure 3. In the lower blocks (G–I) we take the word embeddings from the second block, and retrofit them using the method of Faruqui et al. (2015). Best results in each block are in bold. art word embeddings, such as Glove (Pennington et al., 2014), Skip-Gram (Mik"
Q16-1030,P15-1173,0,0.124156,"Missing"
Q16-1030,P13-1056,0,0.0282838,"r other problems. It follows a similar idea to the one proposed by Koren and Carmel (2003) for improving the visualization of principal vectors with principal component analysis (PCA). Our derivation represents the solution to CCA as that of an optimization problem which maximizes the distance between the two view projections of training examples, while weighting these distances using the external source of prior knowledge. As such, our approach applies to other uses of CCA in the NLP literature, such as the one of Jagarlamudi and Daum´e (2012), who used CCA for transliteration, or the one of Silberer et al. (2013), who used CCA for semantically representing visual attributes. 2 Background and Notation For an integer n, we denote by [n] the set of integers {1, . . . , n}. We assume the existence of a vocabulary of words, usually taken from a corpus. This set of words is denoted by H = {h1 , . . . , h|H |}. For a square matrix A, we denote by diag(A) a diagonal matrix B which has the same dimensions as A such that Bii = Aii for all i. For vector vq∈ Rd , we dePd 2 note its `2 norm by ||v||, i.e. ||v ||= i=1 vi . We also denote by vj or [v]j the jth coordinate of v. For a pair of vectors u and v, we denot"
Q16-1030,P13-1045,0,0.019109,"te prior knowledge into CCA, give a theoretical justification for it, and test it by deriving word embeddings and evaluating them on a myriad of datasets. 1 Introduction In recent years there has been an immense interest in representing words as low-dimensional continuous real-vectors, namely word embeddings. Word embeddings aim to capture lexico-semantic information such that regularities in the vocabulary are topologically represented in a Euclidean space. Such word embeddings have achieved state-of-theart performance on many natural language processing (NLP) tasks, e.g., syntactic parsing (Socher et al., 2013), word or phrase similarity (Mikolov et al., 2013b), dependency parsing (Bansal et al., 2014), unsupervised learning (Parikh et al., 2014) and others. Since the discovery that word embeddings are useful as features for various NLP tasks, research on word embeddings has taken on a life of its own, with a vibrant community searching for better word representations in a variety of problems and datasets. These word embeddings are often induced from large raw text capturing distributional co-occurrence information via neural networks (Bengio et al., 2003; Mikolov et al., 2013b; Mikolov et al., 2013"
Q16-1030,P15-1124,0,0.0251709,"our word embeddings with existing state-of-the2 We downloaded the data from https://dumps. wikimedia.org/, and preprocessed it using the tool available at http://mattmahoney.net/dc/textdata. html. 3 We use the XL subset of the PPDB. 4 https://github.com/paramveerdhillon/ swell. 5 Our implementation and the word embeddings that we calculated are available at http://cohort.inf.ed.ac. uk/cohort/eigen/. 6 We also use the square-root transformation as mentioned in Dhillon et al. (2015) which controls the variance in the counts accumulated from the corpus. See a justification for this transform in Stratos et al. (2015). Retrofitting CCAPrior CCAPrior+RF Glove Skip-Gram Global Context Multilingual Eigen (CCA) α = 0.1 α = 0.2 α = 0.5 α = 0.7 α = 0.9 α = 0.1 α = 0.2 α = 0.5 α = 0.7 α = 0.9 Word similarity average NPK WN PD FN 59.7 63.1 64.6 57.5 64.1 65.5 68.6 62.3 44.4 50.0 50.4 47.3 62.3 66.9 68.2 62.8 59.5 62.2 63.6 61.4 59.1 59.6 59.5 59.9 60.6 60.0 59.9 59.7 59.6 60.7 59.3 59.5 60.6 59.6 58.9 61.9 63.6 61.5 62.6 64.9 61.6 62.7 63.7 61.4 63.3 63.0 61.0 62.0 63.3 60.4 A D G Geographic analogies NPK WN PD FN 94.8 75.3 80.4 94.8 87.3 72.3 70.5 87.7 7.3 4.5 18.2 7.3 70.7 46.2 53.7 72.7 89.9 79.2 73.5 89.9 88.9"
Q16-1030,Q16-1018,0,0.0277759,"al., 2013; Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). From the technical perspective, our work is also related to that of Jagarlamudi et al. (2011), who showed how to generalize CCA so that it uses locality preserving projections (He and Niyogi, 2004). They also assume the existence of a weight matrix in a multi-view setting that describes the distances between pairs of points in the two views. More generally, CCA is an important component for spectral learning algorithms in the unsupervised setting and with latent variables (Cohen et al., 2014; Narayan and Cohen, 2016; Stratos et al., 2016). Our method for incorporating prior knowledge into CCA could potentially be transferred to these algorithms. 7 Conclusion We described a method for incorporating prior knowledge into CCA. Our method requires a relatively simple change to the original canonical correlation analysis, where extra counts are added to the matrix on which singular value decomposition is performed. We used our method to derive word embeddings in the style of eigenwords, and tested them on a set of datasets. Our results demonstrate several advantages of encoding prior knowledge into eigenword embeddings. m X d X i=1"
Q16-1030,J06-3003,0,0.03946,"that synonyms will tend to have positive cosine similarity, and antonyms will tend to have negative similarities. Their vector space representation successfully projects synonyms and antonyms on opposite sides in the projected space. Chang et al. (2013) further generalize this approach to encode multiple relations (and not just opposing relations, such as synonyms and antonyms) using multi-relational LSA. In spectral learning, most of the studies on incorporating prior knowledge in word vectors focus on LSA based word embeddings (Yih et al., 2012; Chang et al., 2013; Turney and Littman, 2005; Turney, 2006; Turney and Pantel, 2010). From the technical perspective, our work is also related to that of Jagarlamudi et al. (2011), who showed how to generalize CCA so that it uses locality preserving projections (He and Niyogi, 2004). They also assume the existence of a weight matrix in a multi-view setting that describes the distances between pairs of points in the two views. More generally, CCA is an important component for spectral learning algorithms in the unsupervised setting and with latent variables (Cohen et al., 2014; Narayan and Cohen, 2016; Stratos et al., 2016). Our method for incorporati"
Q16-1030,D14-1167,0,0.0298382,"tion. When using the retrofitting method by Faruqui et al. on top of these word embeddings, the results barely improved. 6 Related Work Our ideas in this paper for encoding prior knowledge in eigenword embeddings relate to three main threads in existing literature. One of the threads focuses on modifying the objective of word vector training algorithms. Yu and Dredze (2014), Xu et al. (2014), Fried and Duh (2015) and Bian et al. (2014) augment the training objective in neural language models of Mikolov et al. (2013a) to encourage semantically related word vectors to come closer to each other. Wang et al. (2014) propose a method for jointly embedding entities (from FreeBase, a large community-curated knowledge base) and words (from Wikipedia) into the same continuous vector space. Chen and de Melo (2015) propose a similar joint model to improve the word embeddings, but rather than using structured knowledge sources their model focuses on discovering stronger semantic connections in specific contexts in a text corpus. Another research thread relies on post-processing steps to encode prior knowledge from semantic lexicons in off-the-shelf word embeddings. The main intuition behind this trend is to upda"
Q16-1030,P15-2075,0,0.0117246,", SkipGram, Global Context, and Multilingual, on various word similarity tasks. Rothe and Sch¨utze (2015) also describe how standard word vectors can be extended to various data types in semantic lexicons, e.g., synsets and lexemes in WordNet. Most of the standard word vector training algorithms use co-occurrence within window-based contexts to measure relatedness among words. Several studies question the limitations of defining relatedness in this way and investigate if the word co-occurrence matrix can be constructed to encode prior knowledge directly to improve the quality of word vectors. Wang et al. (2015) investigate the notion of relatedness in embedding models by incorporating syntactic and lexicographic knowledge. In spectral learning, Yih et al. (2012) augment the word co-occurrence matrix on which LSA operates with relational information such that synonyms will tend to have positive cosine similarity, and antonyms will tend to have negative similarities. Their vector space representation successfully projects synonyms and antonyms on opposite sides in the projected space. Chang et al. (2013) further generalize this approach to encode multiple relations (and not just opposing relations, su"
Q16-1030,P95-1026,0,0.0695115,"rence statistics (Deerwester et al., 1990; Dhillon et al., 2015) to neural networks jointly learning a language model (Bengio et al., 2003; Mikolov et al., 2013a) or models for other NLP tasks (Collobert and Weston, 2008). 3 Canonical Correlation Analysis for Deriving Word Embeddings One recent approach to derive word embeddings, developed by Dhillon et al. (2015), is through the use of canonical correlation analysis, resulting in socalled “eigenwords.” CCA is a technique for multiview dimensionality reduction. It assumes the existence of two views for a set of data, similarly to co-training (Yarowsky, 1995; Blum and Mitchell, 1998), and then projects the data in the two views in a way that maximizes the correlation between the projected views. Dhillon et al. (2015) used CCA to derive word embeddings through the following procedure. They first break each document in a corpus of documents into n sequences of words of a fixed length 2k + 1, where k is a window size. For example, if k = 2, the short document “Harry Potter has been a bestseller” would be broken into “Harry Potter has been a” and “Potter has been a best-seller.” In each such sequence, the middle word is identified as a pivot. This le"
Q16-1030,D12-1111,0,0.0611095,"tended to various data types in semantic lexicons, e.g., synsets and lexemes in WordNet. Most of the standard word vector training algorithms use co-occurrence within window-based contexts to measure relatedness among words. Several studies question the limitations of defining relatedness in this way and investigate if the word co-occurrence matrix can be constructed to encode prior knowledge directly to improve the quality of word vectors. Wang et al. (2015) investigate the notion of relatedness in embedding models by incorporating syntactic and lexicographic knowledge. In spectral learning, Yih et al. (2012) augment the word co-occurrence matrix on which LSA operates with relational information such that synonyms will tend to have positive cosine similarity, and antonyms will tend to have negative similarities. Their vector space representation successfully projects synonyms and antonyms on opposite sides in the projected space. Chang et al. (2013) further generalize this approach to encode multiple relations (and not just opposing relations, such as synonyms and antonyms) using multi-relational LSA. In spectral learning, most of the studies on incorporating prior knowledge in word vectors focus"
Q16-1030,P14-2089,0,0.307726,"purpose word embeddings have achieved significant improvement in various tasks in NLP, it has been discovered that further tuning of these continuous word representations for specific tasks improves their performance by a larger margin. For example, in dependency parsing, word embeddings could be tailored to capture similarity in terms of context within syntactic parses (Bansal et al., 2014) or they could be refined using semantic lexicons such as WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013) to improve various similarity tasks (Yu and Dredze, 2014; Faruqui et al., 2015; Rothe and Sch¨utze, 2015). This paper proposes a method to encode prior semantic knowledge in spectral word embeddings (Dhillon et al., 2015). Spectral learning algorithms are of great interest for their speed, scalability, theoretical guarantees and performance in various NLP applications. These algorithms are no strangers to word embeddings either. In latent semantic analysis (LSA, (Deerwester et al., 1990; Landauer et al., 1998)), word embeddings are learned by performing SVD on the word by document matrix. Recently, Dhillon et al. (2015) have proposed to use canonic"
Q16-1030,C98-1013,0,\N,Missing
Q18-1001,D15-1075,0,0.0381959,"inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input. 1 Introduction The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rockt¨aschel et al., 2016), and notably question answering based on text (Hermann et al., 1 Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding. For example, it is assumed that understanding is an offline process, models are expected to digest large amounts of data before being able to answer a question, or make inferences. They"
Q18-1001,D13-1128,0,0.0286237,"the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model is, however, not static, it learns representations which evolve over time. Video Understanding Work on video understanding has assumed several guises such as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009). Movies have also been aligned to scre"
Q18-1001,N15-1113,1,0.773651,"aphide and Huang, 2001). A few datasets have been released recently which include movies and textual data. MovieQA (Tapaswi et al., 2016) is a large-scale dataset which contains 408 movies and 14,944 questions, each accompanied with five candidate answers, one of which is correct. For some movies, the dataset also contains subtitles, video clips, scripts, plots, and text from the Described Video Service (DVS), a narration service for the visually impaired. MovieDescription (Rohrbach et al., 2017) is a related dataset which contains sentences aligned to video clips from 200 movies. Scriptbase (Gorinski and Lapata, 2015) is another movie database which consists of movie screenplays (without video) and has been used to generate script summaries. In contrast to the story comprehension tasks envisaged in MovieQA and MovieDescription, we focus on a single cinematic genre (i.e., crime series), and have access to entire episodes (and their corresponding screenplays) as opposed to video-clips or DVSs for some of the data. Rather than answering multiple factoid questions, we aim to solve a single problem, albeit one that is inherently challenging to both humans and machines. Question Answering A variety of question a"
Q18-1001,D14-1005,0,0.0209416,"deling problem (Section 4). We describe our experiments in Section 5. 2 Related Work Our research has connections to several lines of work in natural language processing, computer vision, and more generally multi-modal learning. We review related literature in these areas below. Language Grounding Recent years have seen increased interest in the problem of grounding language in the physical world. Various semantic space models have been proposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model"
Q18-1001,N15-1016,0,0.0247117,"3) and formalize the modeling problem (Section 4). We describe our experiments in Section 5. 2 Related Work Our research has connections to several lines of work in natural language processing, computer vision, and more generally multi-modal learning. We review related literature in these areas below. Language Grounding Recent years have seen increased interest in the problem of grounding language in the physical world. Various semantic space models have been proposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in addition"
Q18-1001,N15-1174,1,0.859125,"roposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model is, however, not static, it learns representations which evolve over time. Video Understanding Work on video understanding has assumed several guises such as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009). Movies have"
Q18-1001,D14-1162,0,0.0818224,"Missing"
Q18-1001,D16-1264,0,0.275576,"odal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input. 1 Introduction The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rockt¨aschel et al., 2016), and notably question answering based on text (Hermann et al., 1 Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding. For example, it is assumed that understanding is an offline process, models are expected to digest large amounts of data before being"
Q18-1001,D13-1020,0,0.183112,"which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input. 1 Introduction The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rockt¨aschel et al., 2016), and notably question answering based on text (Hermann et al., 1 Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding. For example, it is assumed that understanding is an offline process, models are expected to digest large amo"
Q18-1001,N15-1173,0,0.028616,"uage query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model is, however, not static, it learns representations which evolve over time. Video Understanding Work on video understanding has assumed several guises such as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009). Movies have also been aligned to screenplays (Cour et al., 2008), plot synopses (Tapaswi et al., 2015), and books (Zhu et al., 2015) with the aim of improving scene prediction and semantic browsing. Other work uses low-level features (e.g., based on face detection) to establish social networks of main characters in order to summarize movies or perform genre Peter Berglund: Grissom doesn't look"
Q18-1001,D15-1237,0,0.0941421,"Missing"
Q18-1048,P11-1062,0,0.135964,"t and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity constraints to be effective in learning entailment graphs, the Integer Linear Programming (ILP) solution of Berant et al. is not scalable beyond a few hundred nodes. In fact, the problem of finding a maximally weighted transitive subgraph of a graph with arbitrary edge weights is NP-hard (Berant et al., 2011). This paper instead proposes a scalable solution that does not rely on transitivity closure, but 703 Transactions of the Association fo"
Q18-1048,D15-1075,0,0.125403,"Missing"
Q18-1048,D17-1070,0,0.0422865,"Missing"
Q18-1048,P14-1061,1,0.833691,"better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions fro"
Q18-1048,P15-1034,0,0.0372871,"n C1 =3 unique predicates; (2) remove any predicate that is observed with fewer than C2 =3 unique argument-pairs. This leaves us with |P |=101K unique predicates in 346 entailment graphs. The maximum graph size is 53K nodes,8 and the total number of non-zero local scores in all graphs is 66M. In the future, we plan to test our method on an even larger corpus, but preliminary experiments suggest that data sparsity will persist regardless of the corpus size, because of the power law distribution of the terms. We compared our extractions qualitatively with Stanford Open IE (Etzioni et al., 2011; Angeli et al., 2015). Our CCG-based extraction generated noticeably 7 In our experiments, the total number of edges is ≈ .01|V |2 and most of predicate pairs are seen in less than 20 subgraphs, rather than |T |2 . 8 There are 4 graphs with more than 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of B"
Q18-1048,J15-2003,0,0.30255,"ing the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a combinatory categorial gramma"
Q18-1048,D17-1091,1,0.841356,"only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) con"
Q18-1048,P12-1013,0,0.0150022,"ith PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within-graph (light blue) connections. on assumptions concerning the graph structure. Berant et al. (2012, 2015) propose Tree-Node-Fix (TNF), an approximation method that scales better by additionally assuming the entailment graphs are “forest reducible,” where a predicate cannot entail two (or more) predicates j and k such that neither j→k nor k→j (FRG assumption). However, the FRG assumption is not correct for many real-world domains. For example, a person visiting a place entails both arriving at that place and leaving that place, although the latter do not necessarily entail each other. Our work injects two other types of prior knowledge about the structure of the graph that are less expensiv"
Q18-1048,P16-2041,0,0.530878,"han 20K nodes, 3 graphs with 10K to 20K nodes, and 16 graphs with 1K to 10K nodes. 708 better relations for longer sentences with longrange dependencies such as those involving coordination. 5.2 ment graphs based on the predicates in their corpus. The data set contains 3,427 edges (positive), and 35,585 non-edges (negative). We evaluate our method on all the examples of Berant’s entailment data set. The types of this data set do not match with FIGER types, but we perform a simple handmapping between their types and FIGER types.10 Evaluation Entailment Data Sets Levy/Holt’s Entailment Data Set Levy and Dagan (2016) proposed a new annotation method (and a new data set) for collecting relational inference data in context. Their method removes a major bias in other inference data sets such as Zeichner’s (Zeichner et al., 2012), where candidate entailments were selected using a directional similarity measure. Levy and Dagan form questions of the type which city (qtype ), is located near (qrel ), mountains (qarg )? and provide possible answers of the form Kyoto (aanswer ), is surrounded by (arel ), mountains (aarg ). Annotators are shown a question with multiple possible answers, where aanswer is masked by q"
Q18-1048,S17-1026,0,0.22754,"Missing"
Q18-1048,N13-1092,0,0.0938356,"Missing"
Q18-1048,D13-1064,1,0.942149,"third argument by concatenating the relation name with the object. For example, for the sentence China has a border with India, we extract a relation have border1,with between China and India. We perform a similar process for prepositional phrases attached to verb phrases. Most of the light verbs and multiword predicates will be extracted by the above post-processing (e.g., take care1,of ), which will recover many salient ternary relations. Although entailments and paraphrasing can benefit from n-ary relations—for example, person visits a location in a time—we currently follow previous work (Lewis and Steedman, 2013a); (Berant et al., 2015) in confining our attention to binary relations, leaving the construction of n-ary graphs to future work. wisdom is that entailment relations are a byproduct of these methods (Riedel et al., 2013). However, this assumption has not usually been explicitly evaluated. Explicit entailment rules provide explainable resources that can be used in downstream tasks. Our experiments show that our method significantly outperforms a state-of-the-art link prediction method. 3 Computing Local Similarity Scores We first extract binary relations as predicateargument pairs using a comb"
Q18-1048,W14-2406,1,0.894326,"Missing"
Q18-1048,P05-1014,0,0.315327,"o arguments, where the type of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions signif"
Q18-1048,P98-2127,0,0.422662,"es as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies"
Q18-1048,P13-2078,0,0.0219818,"ype of each predicate is determined by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imp"
Q18-1048,C16-1268,0,0.106009,"ed by the types of its arguments. We construct typed entailment graphs, with typed predicates as nodes and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on"
Q18-1048,P18-1188,1,0.78834,"Missing"
Q18-1048,W04-3250,0,0.0561007,"Missing"
Q18-1048,W17-2623,0,0.121698,"ign workers . . . . . . Barnes & Noble CEO William Lynch said as he unveiled his company’s Nook Tablet on Monday. The report said opium has accounted for more than half of Afghanistan’s gross domestic product in 2007. Who praised Mitt Romney’s credentials? Which gene did the ALS association discover ? How many Americans suffer from food allergies? What law might the deal break? Who launched the Nook Tablet? What makes up half of Afghanistans GDP ? Table 3: Examples where explicit entailment relations improve the rankings. The related words are boldfaced. contains questions about CNN articles (Trischler et al., 2017). Machine reading comprehension is usually evaluated by posing questions about a text passage and then assessing the answers of a system (Trischler et al., 2017). The data sets that are used for this task are often in the form of (document,question,answer) triples, where answer is a short span of the document. Answer selection is an important task, where the goal is to select the sentence(s) that contain the answer. We show improvements by adding knowledge from our learned entailments without changing the graphs or tuning them to this task in any way. Inverse sentence frequency (ISF) is a stro"
Q18-1048,P15-2070,0,0.0778647,"Missing"
Q18-1048,D14-1162,0,0.0911904,"Missing"
Q18-1048,P15-1129,0,0.0166683,"arallelizable and takes only a few hours to apply to more than 100K predicates.1,2 Our experiments (§6) show that the global scores improve significantly over local scores and outperform state-of-the-art entailment graphs on two standard entailment rule data sets (Berant et al., 2011; Holt, 2018). We ultimately intend the typed entailment graphs to provide a resource for entailment and paraphrase rules for use in semantic parsing and open domain question answering, as has been done for similar resources such as the Paraphrase Database (PPDB; Ganitkevitch et al., 2013; Pavlick et al., 2015) in Wang et al. (2015) and Dong et al. (2017).3 With that end in view, we have included a comparison with PPDB in our evaluation on the entailment data sets. We also show that the learned entailment rules improve performance on a question-answering task (§7) with no tuning or prior knowledge of the task. 2 Figure 2: Learning entailments that are consistent (A) across different but related typed entailment graphs and (B) within each graph. 0 ≤ β ≤ 1 determines how much different graphs are related. The dotted edges are missing, but will be recovered by considering relationships shown by across-graph (red) and within"
Q18-1048,Q14-1030,1,0.832844,"(if any). We thus type all entities that can be grounded in Wikipedia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relati"
Q18-1048,W03-1011,0,0.834164,"and entailment rules as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although"
Q18-1048,N13-1008,0,0.245356,"dge about the structure of the graph that are less expensive to incorporate and yield better results on entailment rule data sets. Abend et al. (2014) learn entailment relations over multi-word predicates with different levels of compositionality. Pavlick et al. (2015) add variety of relations, including entailment, to phrase pairs in PPDB. This includes a broader range of entailment relations such as lexical entailment. In contrast to our method, these works rely on supervised data and take a local learning approach. Another related strand of research is link prediction (Bordes et al., 2013; Riedel et al., 2013; Socher et al., 2013; Yang et al., 2015; Trouillon et al., 2016; Dettmers et al., 2018), where the source data are extractions from text, facts in knowledge bases, or both. Unlike our work, which directly learns entailment relations between predicates, these methods aim at predicting the source data—that is, whether two entities have a particular relationship. The common Related Work Our work is closely related to Berant et al. (2011), where entailment graphs are learned by imposing transitivity constraints on the entailment relations. However, the exact solution to the problem is not scalabl"
Q18-1048,D10-1106,0,0.090144,"Missing"
Q18-1048,P12-2031,0,0.170474,"Missing"
Q18-1048,D13-1183,0,0.540702,"edia. We first map the Wikipedia URL of the entities to Freebase (Bollacker et al., 2008). We select the most notable type of the entity from Freebase and map it to FIGER types (Ling and Weld, 2012) such as building, disease, person, and location, using only the first level of the FIGER type hierarchy.4 For example, instead of event/sports_event, we use event as type. If an entity cannot be grounded in Wikipedia or its Freebase type does not have a mapping to FIGER, we assign the default type thing to it. The semantic parser of Reddy et al. (2014), GraphParser, is run on the NewsSpike corpus (Zhang and Weld, 2013) to extract binary relations between a predicate and its arguments from sentences. GraphParser uses CCG syntactic derivations and λ-calculus to convert sentences to neo-Davisonian semantics, a first-order logic that uses event identifiers (Parsons, 1990). For example, for the sentence, Obama visited Hawaii in 2012, GraphParser produces the logical form ∃e.visit1 (e, Obama) ∧ visit2 (e, Hawaii)∧ visitin (e, 2012), where e denotes an event. We will consider a relation for each pair of arguments, hence, there will be three relations for the given sentence: visit1,2 with arguments (Obama, Hawaii),"
Q18-1048,C08-1107,0,0.828734,"as edges. Figure 1 shows simple examples of such graphs with arguments of types company,company and person,location. Entailment relations are detected computing a similarity score between the typed predicates based on the distributional inclusion hypothesis, which states that a word (predicate) u entails another word (predicate) v if in any context that u can be used, v can be used in its place (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013; Kartsaklis and Sadrzadeh, 2016). Most previous work has taken a “local learning” approach (Lin, 1998; Weeds and Weir, 2003; Szpektor and Dagan, 2008; Schoenmackers et al., 2010), namely, learning entailment rules independently from each other. One problem facing local learning approaches is that many correct edges are not identified because of data sparsity and many wrong edges are spuriously identified as valid entailments. A “global learning” approach, where dependencies between entailment rules are taken into account, can improve the local decisions significantly. Berant et al. (2011) imposed transitivity constraints on the entailments, such that the inclusion of rules i→j and j→k implies that of i→k. Although they showed transitivity"
Q19-1005,E17-1000,0,0.209719,"Missing"
Q19-1005,W04-3224,0,0.088901,"of the art on English and German discontinuous constituency treebanks. We further provide a per-phenomenon analysis of its errors on discontinuous constituents. 1 VP[saw] −→ VP[saw] PP[telescope]. The probability of such a rule models the likelihood that telescope is a suitable modifier for saw. In contrast, unlexicalized parsing models renounce modeling bilexical statistics, based on the assumptions that they are too sparse to be estimated reliably. Indeed, Gildea (2001) observed that removing bilexical statistics from Collins’ (1997) model lead to at most a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Alth"
Q19-1005,E17-2053,1,0.903976,"Missing"
Q19-1005,W13-4916,0,0.0443769,"Missing"
Q19-1005,D17-1172,0,0.395174,"Missing"
Q19-1005,N16-1024,0,0.328577,"P that gives access to older elements in the stack and can be performed several times before a reduction. In practice, they made the following modifications over a standard shift-reduce system: Figure 1: Tree from the Discontinuous Penn Treebank (Evang and Kallmeyer, 2011). actions (Zhu et al., 2013; Zhang and Clark, 2011, 2009; Crabb´e, 2014; Wang et al., 2015, among others), including proposals for discontinuous constituency parsing (Versley, 2014a; Maier, 2015; Coavoux and Crabb´e, 2017a). A few recent proposals use an unlexicalized model (Watanabe and Sumita, 2015; Cross and Huang, 2016b; Dyer et al., 2016). Interestingly, these latter models all use recurrent neural networks (RNN) to compute constituent representations. Our contributions are the following. We introduce an unlexicalized discontinuous parsing model, as well as its lexicalized counterpart. We evaluate them in identical experimental conditions. Our main finding is that, in our experiments, unlexicalized models consistently outperform lexicalized models. We assess the robustness of this result by performing the comparison of unlexicalized and lexicalized models with a second pair of transition systems. We further analyze the empiric"
Q19-1005,C14-1052,1,0.935422,"Missing"
Q19-1005,W11-2913,0,0.53595,"Missing"
Q19-1005,W13-5701,0,0.49505,"Missing"
Q19-1005,P15-1147,0,0.689312,"model and identify which types of discontinuous constituents are hard to predict. 2 1. The stack, that stores subtrees being constructed, is split into two parts S and D; 2. reductions are applied to the respective tops of S and D; 3. the GAP action pops an element from S and adds it to D, making the next element of S available for a reduction. Related Work Several approaches to discontinuous constituency parsing have been proposed. Hall and Nivre (2008) reduces the problem to non-projective dependency parsing, via a reversible transformation, a strategy developed by Fern´andez-Gonz´alez and Martins (2015) and Corro et al. (2017). Chart parsers are based on probabilistic Linear Context-Free Rewriting Systems (LCFRS) (Evang and Kallmeyer, 2011; Kallmeyer and Maier, 2010), the DataOriented Parsing (DOP) framework (van Cranenburgh and Bod, 2013; van Cranenburgh et al., 2016), or pseudo-projective parsing (Versley, 2016). 74 Their parser outperforms swap-based systems. However, they only experiment with a linear classifier, and assume access to gold part-of-speech (POS) tags for most of their experiments. All these proposals use a lexicalized model, as defined in the introduction: they assign heads"
Q19-1005,P16-2006,0,0.0895632,"7) model lead to at most a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed with REDUCERIGHT and REDUCE-LEFT actions. Most proposals in transition-based constituency parsing since Sagae and Lavie (2005) have used a lexicalized transition system, and features involving heads to score Introduction This"
Q19-1005,D16-1001,0,0.169647,"7) model lead to at most a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed with REDUCERIGHT and REDUCE-LEFT actions. Most proposals in transition-based constituency parsing since Sagae and Lavie (2005) have used a lexicalized transition system, and features involving heads to score Introduction This"
Q19-1005,N18-1091,0,0.0272766,"2017) 4,700 80 260 ≈7.3 Table 5: Running times on development sets of the Tiger and the DPTB, reported in tokens per second (tok/s) and sentences per second (sent/s). Runtimes are only indicative; they are not comparable with those reported by other authors, since they use different hardware. 5.4.1 Effect of Lexicalization parsing results. A possible interpretation is that the bi-LSTM transducer may implicitly learn latent lexicalization, as suggested by Kuncoro et al. (2017), which is consistent with recent analyses of other types of syntactic information captured by LSTMs in parsing models (Gaddy et al., 2018) or language models (Linzen et al., 2016). Lexicalized vs. Unlexicalized Models We first compare the unlexicalized ML-GAP system with the ML-GAP-LEX system (Table 4). The former consistently obtains higher results. The F-score difference is small on English (0.1 to 0.3) but substantial on the German treebanks (more than 1.0 absolute point) and in general on discontinuous constituents (Disc. F). In order to assess the robustness of the advantage of unlexicalized models, we also compare our implementation of SR-GAP (Coavoux and Crabb´e, 2017a)6 with an unlexicalized variant (SR-GAP-UNLEX) that u"
Q19-1005,P03-1013,0,0.67802,"t al., 1993). For the Tiger corpus, we use the Statistical Parsing of Morphologically Rich Languages (SPMRL) split (Seddah et al., 2013). We obtained the dependency labels and the morphological information for each token from the dependency treebank versions of the SPMRL release. We converted the Negra corpus to labeled dependency trees with the DEPSY tool3 in order to annotate each token with a dependency label. We do not predict morphological attributes for the Negra corpus (only POS tags) since only a small section is annotated with a full morphological analysis. We use the standard split (Dubey and Keller, 2003) for this corpus, and no limit on sentence length. For the Penn Treebank, we use the standard split (sections 2-21 for training, 22 for development and 23 for test). We retrieved the dependency labels from the dependency version of the Penn Treebank (PTB), obtained by the Stanford Parser (de Marneffe et al., 2006). We used the relevant module of discodop4 (van Cranenburgh et al., 2016) for evaluation. It provides an F1 measure on labeled constituents, as well as an F1 score computed only on discontinuous constituents (Disc. F1). Following standard practice, we used the evaluation parameters in"
Q19-1005,C18-1258,0,0.366439,"s on the development sets are the ML-GAP (DPTB, Tiger) and the SR-GAP-UNLEX (Negra) models with BASE features. We report their results on the test sets in Table 7. They are compared with other published results: transition-based parsers using a SWAP action (Maier, 2015; Stanojevi´ c and Garrido Alhama, 2017) or a GAP action (Coavoux and Crabb´e, 2017a), the pseudo-projective parser of Versley (2016), parsers based on non-projective dependency parsing (Fern´andez-Gonz´alez and Martins, 2015; Corro et al., 2017), and finally chart parsers based on probabilistic LCFRS (Evang and Kallmeyer, 2011; Gebhardt, 2018) or dataoriented parsing (van Cranenburgh et al., 2016). Note that some of these publications report results in a gold POS-tag scenario, a much easier experimental setup that is not comparable to ours (bottom part of the table). In Table 7, we also indicate models that use a neural scoring system with a ‘∗ ’. Our models obtain state-of-the-art results and outperform every other system, including the LSTM-based parser of Stanojevi´c and Garrido Alhama (2017) that uses a SWAP action to predict discontinuities. This observation confirms in another setting the results of Coavoux and Crabb´e (2017a"
Q19-1005,W01-0521,0,0.0985809,"empirical evidence that lexicalization is not necessary to achieve strong parsing results. Our best unlexicalized model sets a new state of the art on English and German discontinuous constituency treebanks. We further provide a per-phenomenon analysis of its errors on discontinuous constituents. 1 VP[saw] −→ VP[saw] PP[telescope]. The probability of such a rule models the likelihood that telescope is a suitable modifier for saw. In contrast, unlexicalized parsing models renounce modeling bilexical statistics, based on the assumptions that they are too sparse to be estimated reliably. Indeed, Gildea (2001) observed that removing bilexical statistics from Collins’ (1997) model lead to at most a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundarie"
Q19-1005,E17-1117,0,0.0900177,"put to a softmax classifier to output a probability distribution over part-of-speech (POS) tags for each token: P (ti = ·|xn1 ; θ t ) = Softmax(W(t) · h(1,i) + b(t) ), where W(t) , b(t) ∈ θ t are parameters. In addition to predicting POS tags, we also predict other morphosyntactic attributes when they are available (i.e., for the Tiger corpus) such as the case, tense, mood, person, and gender, since the POS tagset does not necessarily contain this information. Finally, we predict the syntactic 2 A more involved strategy would be to rely on Recurrent Neural Network Grammars (Dyer et al., 2016; Kuncoro et al., 2017). However, the adaptation of this model to discontinuous parsing is not straightforward and we leave it to future work. 77 functions of tokens, since this auxiliary task has been shown to be beneficial for constituency parsing (Coavoux and Crabb´e, 2017b). For each type of label l, we use a separate softmax classifier, with its own parameters W(l) and b(l) : Configuration: hS |(Is1 , hs1 )|(Is0 , hs0 ), D|(Id1 , hd1 )|(Id0 , hd0 ), i, C i P (li = ·|xn1 ; θ t ) = Softmax(W(l) · h(1,i) + b(l) ). We decompose the probability of a sequence of n actions am 1 = (a1 , a2 , . . . , am ) for a sentence"
Q19-1005,P14-1022,0,0.0354012,"ving bilexical statistics from Collins’ (1997) model lead to at most a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed with REDUCERIGHT and REDUCE-LEFT actions. Most proposals in transition-based constituency parsing since Sagae and Lavie (2005) have used a lexicalized transition system, and feat"
Q19-1005,Q16-1037,0,0.0323314,"times on development sets of the Tiger and the DPTB, reported in tokens per second (tok/s) and sentences per second (sent/s). Runtimes are only indicative; they are not comparable with those reported by other authors, since they use different hardware. 5.4.1 Effect of Lexicalization parsing results. A possible interpretation is that the bi-LSTM transducer may implicitly learn latent lexicalization, as suggested by Kuncoro et al. (2017), which is consistent with recent analyses of other types of syntactic information captured by LSTMs in parsing models (Gaddy et al., 2018) or language models (Linzen et al., 2016). Lexicalized vs. Unlexicalized Models We first compare the unlexicalized ML-GAP system with the ML-GAP-LEX system (Table 4). The former consistently obtains higher results. The F-score difference is small on English (0.1 to 0.3) but substantial on the German treebanks (more than 1.0 absolute point) and in general on discontinuous constituents (Disc. F). In order to assess the robustness of the advantage of unlexicalized models, we also compare our implementation of SR-GAP (Coavoux and Crabb´e, 2017a)6 with an unlexicalized variant (SR-GAP-UNLEX) that uses a single type of reduction (REDUCE) i"
Q19-1005,C10-1061,0,0.452485,"Missing"
Q19-1005,Q17-1029,0,0.0160162,"ework (van Cranenburgh and Bod, 2013; van Cranenburgh et al., 2016), or pseudo-projective parsing (Versley, 2016). 74 Their parser outperforms swap-based systems. However, they only experiment with a linear classifier, and assume access to gold part-of-speech (POS) tags for most of their experiments. All these proposals use a lexicalized model, as defined in the introduction: they assign heads to new constituents and use them as features to inform parsing decisions. Previous work on unlexicalized transition-based parsing models only focused on projective constituency trees (Dyer et al., 2016; Liu and Zhang, 2017). In particular, Cross and Huang (2016b) introduced a system that does not require explicit binarization. Their system decouples the construction of a tree and the labeling of its nodes by assigning types (structure or label) to each action, and alternating between a structural action for even steps and Structural actions SHIFT MERGE GAP Input hS, D, i, C i hS |Is0 , D|Id0 , i, C i hS |Is0 , D, i, C i Labeling actions LABEL-X NO-LABEL Output ⇒ hS |D, {i + 1}, i + 1, C i ⇒ hS |D, Is0 ∪ Id0 , i, C i ⇒ hS, Is0 |D, i, C i Input Output hS, Id0 , i, C i hS, Id0 , i, C i ⇒ hS, Id0 , i, C ∪ {(X, Id0 )"
Q19-1005,P15-1116,0,0.715632,", 2019. Action Editor: Stephen Clark. Submission batch: 9/2018; Revision batch: 11/2018; Published 4/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. S ┌─────────┴────┬───────┐ VP │ │ ┌───────────┴───────────── │ ──┐ │ NP NP │ │ ┌──────┼───────────┬─────────┐ │ │ │ DT JJ JJ NN PRP VBZ . │ │ │ │ │ │ │ An excellent environmental actor he is . Some transition-based discontinuous constituency parsers use the swap action, adapted from dependency parsing (Nivre, 2009) either with an easy-first strategy (Versley, 2014a,b) or with a shift-reduce strategy (Maier, 2015; Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017). Nevertheless, the swap strategy tends to produce long derivations (in number of actions) to construct discontinuous constituents; as a result, the choice of an oracle that minimizes the number of swap actions has a substantial positive effect in accuracy (Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017). In contrast, Coavoux and Crabb´e (2017a) extended a shift-reduce transition system to handle discontinuous constituents. Their system allows binary reductions to apply to the top element in the stack, and any other e"
Q19-1005,Q16-1023,0,0.226967,"P-LEX with ML-GAP is that there are two MERGE actions, MERGE-LEFT and MERGE-RIGHT, and that each of them assigns the head of the new set of indexes (and implicitly creates a new directed dependency arc): Eager Oracle For the ML-GAP system, we use an oracle that builds every n-ary constituent in a leftto-right fashion, as illustrated by the derivation in Table 2.1 This implicitly corresponds to a left-branching binarization. 4 The statistical model we used is based on a Long Short-Term Memory network (bi-LSTM) transducer that builds context-aware representations for each token in the sentence (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016a). The token representations are then fed as input to (i) a tagging component for assigning POS tags and (ii) a parsing component for scoring parsing hS |(Is0 , hs0 ), D|(Id0 , hd0 ), i, C i • MERGE-LEFT: ⇒ hS |D, (Is0 ∪ Id0 , hs0 ), i, C i; • 3.3 MERGE-RIGHT: Neural Architecture hS |(Is0 , hs0 ), D|(Id0 , hd0 ), i, C i ⇒ hS |D, (Is0 ∪ Id0 , hd0 ), i, C }i. Oracles In this work, we use deterministic static oracles. We briefly describe an oracle that builds constituents from their head outwards (head-driven 1 The systems exhibit spurious ambiguity for constructing n-ary"
Q19-1005,P18-1249,0,0.38624,"re. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed with REDUCERIGHT and REDUCE-LEFT actions. Most proposals in transition-based constituency parsing since Sagae and Lavie (2005) have used a lexicalized transition system, and features involving heads to score Introduction This paper introduces an unlexicalized parsing mode"
Q19-1005,W16-0906,0,0.45093,"n Editor: Stephen Clark. Submission batch: 9/2018; Revision batch: 11/2018; Published 4/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. S ┌─────────┴────┬───────┐ VP │ │ ┌───────────┴───────────── │ ──┐ │ NP NP │ │ ┌──────┼───────────┬─────────┐ │ │ │ DT JJ JJ NN PRP VBZ . │ │ │ │ │ │ │ An excellent environmental actor he is . Some transition-based discontinuous constituency parsers use the swap action, adapted from dependency parsing (Nivre, 2009) either with an easy-first strategy (Versley, 2014a,b) or with a shift-reduce strategy (Maier, 2015; Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017). Nevertheless, the swap strategy tends to produce long derivations (in number of actions) to construct discontinuous constituents; as a result, the choice of an oracle that minimizes the number of swap actions has a substantial positive effect in accuracy (Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017). In contrast, Coavoux and Crabb´e (2017a) extended a shift-reduce transition system to handle discontinuous constituents. Their system allows binary reductions to apply to the top element in the stack, and any other element in the stack (ins"
Q19-1005,P03-1054,0,0.0781324,"lexicalized parsing models renounce modeling bilexical statistics, based on the assumptions that they are too sparse to be estimated reliably. Indeed, Gildea (2001) observed that removing bilexical statistics from Collins’ (1997) model lead to at most a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed w"
Q19-1005,J93-2004,0,0.0672087,"Section 5.1) and the optimization protocol (Section 5.2). Then, we discuss empirical runtime efficiency (Section 5.3), before presenting the results of our experiments (Section 5.4). Feature Templates The two feature template sets we used are presented in Table 3. The BASE templates form a minimal set that extracts 78 5.1 • The sentence bi-LSTM has 2 layers, with states of size 128 (in each direction); Data To evaluate our models, we used the Negra corpus (Skut et al., 1997), the Tiger corpus (Brants et al., 2002), and the discontinuous version of the Penn Treebank (Evang and Kallmeyer, 2011; Marcus et al., 1993). For the Tiger corpus, we use the Statistical Parsing of Morphologically Rich Languages (SPMRL) split (Seddah et al., 2013). We obtained the dependency labels and the morphological information for each token from the dependency treebank versions of the SPMRL release. We converted the Negra corpus to labeled dependency trees with the DEPSY tool3 in order to annotate each token with a dependency label. We do not predict morphological attributes for the Negra corpus (only POS tags) since only a small section is annotated with a full morphological analysis. We use the standard split (Dubey and Ke"
Q19-1005,P05-1010,0,0.0773827,"s renounce modeling bilexical statistics, based on the assumptions that they are too sparse to be estimated reliably. Indeed, Gildea (2001) observed that removing bilexical statistics from Collins’ (1997) model lead to at most a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed with REDUCERIGHT and REDU"
Q19-1005,W05-1513,0,0.0660551,"ral information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed with REDUCERIGHT and REDUCE-LEFT actions. Most proposals in transition-based constituency parsing since Sagae and Lavie (2005) have used a lexicalized transition system, and features involving heads to score Introduction This paper introduces an unlexicalized parsing model and addresses the question of lexicalization, as a parser design choice, in the context of transition-based discontinuous constituency parsing. Discontinuous constituency trees are constituency trees where crossing arcs are allowed in order to represent long-distance dependencies, and in general phenomena related to word order variations (e.g., the left dislocation in Figure 1). Lexicalized parsing models (Collins, 1997; Charniak, 1997) are based o"
Q19-1005,D13-1032,0,0.0771532,"Missing"
Q19-1005,P16-1146,1,0.903564,"Missing"
Q19-1005,W04-0308,0,0.0309462,"ality than those of ML-GAP-LEX (Section 6.1) and are more economical in terms of number of GAP actions needed to derive discontinuous trees (Section 6.2). Corpus Average length of stack (S+D) ML-GAP-LEX English (DPTB) German (Negra) German (Tiger) ML-GAP 5.62 3.69 3.56 4.86 2.88 2.98 Table 8: Incrementality measured by the average size of the stack during derivations. The average is calculated across all configurations (not across all sentences). the constituent. For example, to construct the following head-final NP, NP[actor] 6.1 Incrementality An We adopt the definition of incrementality of Nivre (2004): an incremental algorithm minimizes the number of connected components in the stack during parsing. An unlexicalized system can construct a new constituent by incorporating each new component immediately, whereas a lexicalized system waits until it has shifted the head of a constituent before starting to build excellent environmental actor a lexicalized system must shift every token before starting reductions in order to be able to predict the dependency arcs between the head actor and its three dependents.7 In contrast, an unlexicalized 7 SH(IFT), SH, SH, SH, SH, M(ERGE)-R(IGHT), M-R, M-R, M"
Q19-1005,P09-1040,0,0.107316,"e Paris Diderot. 73 Transactions of the Association for Computational Linguistics, vol. 7, pp. 73–89, 2019. Action Editor: Stephen Clark. Submission batch: 9/2018; Revision batch: 11/2018; Published 4/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. S ┌─────────┴────┬───────┐ VP │ │ ┌───────────┴───────────── │ ──┐ │ NP NP │ │ ┌──────┼───────────┬─────────┐ │ │ │ DT JJ JJ NN PRP VBZ . │ │ │ │ │ │ │ An excellent environmental actor he is . Some transition-based discontinuous constituency parsers use the swap action, adapted from dependency parsing (Nivre, 2009) either with an easy-first strategy (Versley, 2014a,b) or with a shift-reduce strategy (Maier, 2015; Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017). Nevertheless, the swap strategy tends to produce long derivations (in number of actions) to construct discontinuous constituents; as a result, the choice of an oracle that minimizes the number of swap actions has a substantial positive effect in accuracy (Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017). In contrast, Coavoux and Crabb´e (2017a) extended a shift-reduce transition system to handle discontinuous constituen"
Q19-1005,A97-1014,0,0.605939,"s The experiments we performed aim at assessing the role of lexicalization in transition-based constituency parsing. We describe the data (Section 5.1) and the optimization protocol (Section 5.2). Then, we discuss empirical runtime efficiency (Section 5.3), before presenting the results of our experiments (Section 5.4). Feature Templates The two feature template sets we used are presented in Table 3. The BASE templates form a minimal set that extracts 78 5.1 • The sentence bi-LSTM has 2 layers, with states of size 128 (in each direction); Data To evaluate our models, we used the Negra corpus (Skut et al., 1997), the Tiger corpus (Brants et al., 2002), and the discontinuous version of the Penn Treebank (Evang and Kallmeyer, 2011; Marcus et al., 1993). For the Tiger corpus, we use the Statistical Parsing of Morphologically Rich Languages (SPMRL) split (Seddah et al., 2013). We obtained the dependency labels and the morphological information for each token from the dependency treebank versions of the SPMRL release. We converted the Negra corpus to labeled dependency trees with the DEPSY tool3 in order to annotate each token with a dependency label. We do not predict morphological attributes for the Neg"
Q19-1005,P06-1055,0,0.293192,"xical statistics, based on the assumptions that they are too sparse to be estimated reliably. Indeed, Gildea (2001) observed that removing bilexical statistics from Collins’ (1997) model lead to at most a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed with REDUCERIGHT and REDUCE-LEFT actions. Most"
Q19-1005,P16-2038,0,0.0124849,"ure is trained end-toend. We illustrate the bi-LSTM and the tagging components in Figure 3. In the following paragraphs, we describe the architecture that builds shared representations (Section 4.1), the tagging component (Section 4.2), the parsing component (Section 4.3), and the objective function (Section 4.4). 4.1 (hx1 , hx2 , . . . , hxn ), to obtain vector representations that depend on the whole sentence: (h(1) , . . . , h(n) ) = bi-LSTM(hx1 , hx2 , . . . , hxn ). In practice, we use a two-layer bi-LSTM in order to supervise parsing and tagging at different layers, following results by Søgaard and Goldberg (2016). In what follows, we denote the ith state of the j th layer with h(j,i) . Building Context-aware Token Representations 4.2 We use a hierarchical bi-LSTM (Plank et al., 2016) to construct context-aware vector representations for each token. A lexical entry x is represented by the concatenation hx = [wx ; cx ], where wx is a standard word embedding and cx = bi-LSTM(x) is the output of a character bi-LSTM encoder, i.e., the concatenation of its last forward and backward states. We run a sentence-level bi-LSTM transducer over the sequence of local embeddings Tagger We use the context-aware repres"
Q19-1005,P81-1022,0,0.643966,"Missing"
Q19-1005,D17-1174,0,0.319409,"Missing"
Q19-1005,P17-1076,0,0.0458544,"a 0.5 drop in F-score. Furthermore, Bikel (2004) showed that bilexical statistics were in fact rarely used during decoding, and that when used, they were close to that of backoff distributions used for unknown word pairs. Instead, unlexicalized models may rely on grammar rule refinements to alleviate the strong independence assumptions of PCFGs (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006; Narayan and Cohen, 2016). They sometimes rely on structural information, such as the boundaries of constituents (Hall et al., 2014; Durrett and Klein, 2015; Cross and Huang, 2016b; Stern et al., 2017; Kitaev and Klein, 2018). Although initially coined for chart parsers, the notion of lexicalization naturally transfers to transition-based parsers. We take lexicalized to denote a model that (i) assigns a lexical head to each constituent and (ii) uses heads of constituents as features to score parsing actions. Head assignment is typically performed with REDUCERIGHT and REDUCE-LEFT actions. Most proposals in transition-based constituency parsing since Sagae and Lavie (2005) have used a lexicalized transition system, and features involving heads to score Introduction This paper introduces an u"
Q19-1005,P15-1113,0,0.039029,"Missing"
Q19-1005,W14-6104,0,0.708559,"on for Computational Linguistics, vol. 7, pp. 73–89, 2019. Action Editor: Stephen Clark. Submission batch: 9/2018; Revision batch: 11/2018; Published 4/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. S ┌─────────┴────┬───────┐ VP │ │ ┌───────────┴───────────── │ ──┐ │ NP NP │ │ ┌──────┼───────────┬─────────┐ │ │ │ DT JJ JJ NN PRP VBZ . │ │ │ │ │ │ │ An excellent environmental actor he is . Some transition-based discontinuous constituency parsers use the swap action, adapted from dependency parsing (Nivre, 2009) either with an easy-first strategy (Versley, 2014a,b) or with a shift-reduce strategy (Maier, 2015; Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017). Nevertheless, the swap strategy tends to produce long derivations (in number of actions) to construct discontinuous constituents; as a result, the choice of an oracle that minimizes the number of swap actions has a substantial positive effect in accuracy (Maier and Lichte, 2016; Stanojevi´c and Garrido Alhama, 2017). In contrast, Coavoux and Crabb´e (2017a) extended a shift-reduce transition system to handle discontinuous constituents. Their system allows binary reductions to apply"
Q19-1005,W09-3825,0,0.19973,"Missing"
Q19-1005,J11-1005,0,0.0143519,"discontinuous constituents. Their system allows binary reductions to apply to the top element in the stack, and any other element in the stack (instead of the two top elements in standard shift-reduce parsing). The second constituent for a reduction is chosen dynamically, with an action called GAP that gives access to older elements in the stack and can be performed several times before a reduction. In practice, they made the following modifications over a standard shift-reduce system: Figure 1: Tree from the Discontinuous Penn Treebank (Evang and Kallmeyer, 2011). actions (Zhu et al., 2013; Zhang and Clark, 2011, 2009; Crabb´e, 2014; Wang et al., 2015, among others), including proposals for discontinuous constituency parsing (Versley, 2014a; Maier, 2015; Coavoux and Crabb´e, 2017a). A few recent proposals use an unlexicalized model (Watanabe and Sumita, 2015; Cross and Huang, 2016b; Dyer et al., 2016). Interestingly, these latter models all use recurrent neural networks (RNN) to compute constituent representations. Our contributions are the following. We introduce an unlexicalized discontinuous parsing model, as well as its lexicalized counterpart. We evaluate them in identical experimental condition"
Q19-1005,W16-0907,0,0.438152,"f S available for a reduction. Related Work Several approaches to discontinuous constituency parsing have been proposed. Hall and Nivre (2008) reduces the problem to non-projective dependency parsing, via a reversible transformation, a strategy developed by Fern´andez-Gonz´alez and Martins (2015) and Corro et al. (2017). Chart parsers are based on probabilistic Linear Context-Free Rewriting Systems (LCFRS) (Evang and Kallmeyer, 2011; Kallmeyer and Maier, 2010), the DataOriented Parsing (DOP) framework (van Cranenburgh and Bod, 2013; van Cranenburgh et al., 2016), or pseudo-projective parsing (Versley, 2016). 74 Their parser outperforms swap-based systems. However, they only experiment with a linear classifier, and assume access to gold part-of-speech (POS) tags for most of their experiments. All these proposals use a lexicalized model, as defined in the introduction: they assign heads to new constituents and use them as features to inform parsing decisions. Previous work on unlexicalized transition-based parsing models only focused on projective constituency trees (Dyer et al., 2016; Liu and Zhang, 2017). In particular, Cross and Huang (2016b) introduced a system that does not require explicit b"
Q19-1005,P13-1043,0,0.0839876,"n system to handle discontinuous constituents. Their system allows binary reductions to apply to the top element in the stack, and any other element in the stack (instead of the two top elements in standard shift-reduce parsing). The second constituent for a reduction is chosen dynamically, with an action called GAP that gives access to older elements in the stack and can be performed several times before a reduction. In practice, they made the following modifications over a standard shift-reduce system: Figure 1: Tree from the Discontinuous Penn Treebank (Evang and Kallmeyer, 2011). actions (Zhu et al., 2013; Zhang and Clark, 2011, 2009; Crabb´e, 2014; Wang et al., 2015, among others), including proposals for discontinuous constituency parsing (Versley, 2014a; Maier, 2015; Coavoux and Crabb´e, 2017a). A few recent proposals use an unlexicalized model (Watanabe and Sumita, 2015; Cross and Huang, 2016b; Dyer et al., 2016). Interestingly, these latter models all use recurrent neural networks (RNN) to compute constituent representations. Our contributions are the following. We introduce an unlexicalized discontinuous parsing model, as well as its lexicalized counterpart. We evaluate them in identical"
Q19-1005,P97-1003,0,\N,Missing
Q19-1005,D16-1257,0,\N,Missing
W11-2208,N10-1083,0,0.327563,"vere problem of local maxima, and suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature engineering to incorporate knowledge and improve perfo"
W11-2208,D11-1005,1,0.858908,"sented in WFSTs. Das and Petrov (2011) also consider the problem of unsupervised bilingual POS induction. They make use of independent conventional HMM monolingual tagging models that are parameterized with feature-rich log-linear models (Berg-Kirkpatrick et al., 2010). However, training is constrained with tag dictionaries inferred using bilingual contexts derived from aligned parallel data. In this way, the complex inference and modeling challenges associated with a bilingual tagging model are avoided. Finally, multilingual POS induction has also been considered without using parallel data. Cohen et al. (2011) present a multilingual estimation technique for part-of-speech tagging (and grammar induction), where the lack of parallel data is compensated by the use of labeled data for some languages and unlabeled data for other languages. 3 Model Our model is a Markov random field whose random variables correspond to words in two parallel sentences and POS tags for those words. Let s = hs1 , . . . , sNs i and t = ht1 , . . . , tNt i denote the two word sequences; these correspond to Ns + Nt observed random variables.1 Let x and y denote the sequences of POS tags for s and t, respectively. These are the"
W11-2208,P11-1061,0,0.230006,"nd suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature engineering to incorporate knowledge and improve performance. We conjecture t"
W11-2208,erjavec-2004-multext,0,0.0739108,"Missing"
W11-2208,P07-1094,0,0.0711216,"task of unsupervised bilingual POS induction was originally suggested and explored by Snyder et al. (2008). Their work proposes a joint model over pairs of tag sequences and words that can be understood as a pair of hidden Markov models (HMMs) Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 64–71, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics in which aligned words share states (a fixed and observable word alignment is assumed). Figure 1 gives an example for a French-English sentence pair. Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. The hyperparameters of the prior distributions are inferred from data in an empirical Bayesian fashion. Why repeat x1/y1 X2/y2 that catastrophe ? x4/y5 x5/y6 catastrophe ? x3 Pourquoi répéter y3 y4 la même Figure 1: Bilingual Directed POS induction model When word alignments are monotonic (i.e., there are no crossing links in the alignment graph), the model of Snyder et al. is straightforward to construct. However, crossing alignment links pose a"
W11-2208,N06-1041,0,0.692952,"model. Their solution is to eliminate such alignment pairs (their algorithm for doing so is discussed below). Unfortunately, this is a potentially a serious loss of information. Crossing alignments often correspond to systematic word order differences between languages (e.g., SVO vs. SOV languages). As such, leaving them out prevents useful information about entire subsets of POS types from exploiting of bilingual context. In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. An example of such a monolingual model is shown in Figure 2. Both papers developed different approximations of the computationally expensive partition function. Haghighi and Klein (2006) approximated by ignoring all sentences of length greater than some maximum, and the “contrastive estimation” of Smith and Eisner (2005) approximates the partition function with a set 65 A N V V Economic discrepancies are growing Figure 2: Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples w"
W11-2208,2005.mtsummit-papers.11,0,0.106634,"Missing"
W11-2208,J93-2004,0,0.0361661,"Missing"
W11-2208,J03-1002,0,0.00473676,"Missing"
W11-2208,P05-1044,1,0.952248,"However, crossing alignment links pose a problem: they induce cycles in the tag sequence graph, which corresponds to an ill-defined probability model. Their solution is to eliminate such alignment pairs (their algorithm for doing so is discussed below). Unfortunately, this is a potentially a serious loss of information. Crossing alignments often correspond to systematic word order differences between languages (e.g., SVO vs. SOV languages). As such, leaving them out prevents useful information about entire subsets of POS types from exploiting of bilingual context. In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. An example of such a monolingual model is shown in Figure 2. Both papers developed different approximations of the computationally expensive partition function. Haghighi and Klein (2006) approximated by ignoring all sentences of length greater than some maximum, and the “contrastive estimation” of Smith and Eisner (2005) approximates the partition function with a set 65 A N V"
W11-2208,D08-1109,0,0.502551,"likelihood with joint MRFs suffers from a severe problem of local maxima, and suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature"
W11-2208,N03-1033,0,\N,Missing
W11-2208,W06-2920,0,\N,Missing
W11-2208,N09-1009,1,\N,Missing
W11-2208,H94-1020,0,\N,Missing
W13-3507,P12-1024,1,0.909167,"g statistically consistent parameter estimates: even with very large amounts of data, EM is not guaranteed to estimate parameters which are close to the “correct” model parameters. In this paper, we derive a spectral algorithm for learning the parameters of R-HMMs. Unlike EM, this technique is guaranteed to find the true parameters of the underlying model under mild conditions on the singular values of the model. The algorithm we derive is simple and efficient, relying on singular value decomposition followed by standard matrix operations. We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na¨ıve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs. We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical Introduction Consider the task of supervised sequence labeling. We are given a training set where the j’th train"
W13-3507,N13-1015,1,0.903398,"hidden state as well as the label. Unfortunately, estimating the parameters of an R-HMM is complicated by the unobserved hidden variables. A standard approach is to use the expectation-maximization (EM) algorithm which 56 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56–64, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics 3.1 success for the R-HMM spectral algorithm. The spectral algorithm performs competitively with EM on a phoneme recognition task, and is more stable with respect to the number of hidden states. Cohen et al. (2013) present experiments with a parsing algorithm and also demonstrate it is competitive with EM. Our set of experiments comes as an additional piece of evidence that spectral algorithms can function as a viable, efficient and more principled alternative to the EM algorithm. 2 We distinguish row vectors from column vectors when such distinction is necessary. We use a superscript > to denote the transpose operation. We write [n] to denote the set {1, 2, . . . , n} for any integer n ≥ 1. For any vector v ∈ Rm , diag(v) ∈ Rm×m is a diagonal matrix with entries v1 . . . vm . For any statement S, we us"
W13-3507,E12-1042,0,0.0933967,"le and efficient, relying on singular value decomposition followed by standard matrix operations. We also describe the connection of R-HMMs to L-PCFGs. Cohen et al. (2012) present a spectral algorithm for L-PCFG estimation, but the na¨ıve transformation of the L-PCFG model and its spectral algorithm to R-HMMs is awkward and opaque. We therefore work through the non-trivial derivation the spectral algorithm for R-HMMs. We note that much of the prior work on spectral algorithms for discrete structures in NLP has shown limited experimental success for this family of algorithms (see, for example, Luque et al., 2012). Our experiments demonstrate empirical Introduction Consider the task of supervised sequence labeling. We are given a training set where the j’th training example consists of a sequence of ob(j) (j) servations x1 ...xN paired with a sequence of (j) (j) labels a1 ...aN and asked to predict the correct labels on a test set of observations. A common approach is to learn a joint distribution over sequences p(a1 . . . aN , x1 . . . xN ) as a hidden Markov model (HMM). The downside of HMMs is that they assume each label ai is independent of labels before the previous label ai−1 . This independence"
W13-3507,P05-1010,0,0.0362596,"or performance. Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a,"
W13-3507,P92-1017,0,0.411969,"ning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a, h) is the probability of genera"
W13-3507,P06-1055,0,0.0566726,"his type are crucial for performance. Spectral learning has been applied to related models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] a"
W13-3507,D07-1094,0,0.0373788,"ated models beyond HMMs including: head automata for dependency parsing (Luque et al., 2012), tree-structured directed Bayes nets (Parikh et al., 2011), finite-state transducers (Balle et al., 2011), and mixture models (Anandkumar et al., 2012a; Anandkumar et al., 2012b). Of special interest is Cohen et al. (2012), who describe a derivation for a spectral algorithm for L-PCFGs. This derivation is the main driving force behind the derivation of our R-HMM spectral algorithm. For work on L-PCFGs estimated with EM, see Petrov et al. (2006), Matsuzaki et al. (2005), and Pereira and Schabes (1992). Petrov et al. (2007) proposes a split-merge EM procedure for phoneme recognition analogous to that used in latent-variable parsing. 3 Notation 3.2 Definition of an R-HMM An R-HMM is a 7-tuple hl, m, n, π, o, t, f i for integers l, m, n ≥ 1 and functions π, o, t, f where • [l] is a set of labels. • [m] is a set of hidden states. • [n] is a set of observations. • π(a, h) is the probability of generating a ∈ [l] and h ∈ [m] in the first position in the labeled sequence. • o(x|a, h) is the probability of generating x ∈ [n], given a ∈ [l] and h ∈ [m]. • t(b, h0 |a, h) is the probability of generating b ∈ [l] and h0 ∈"
W16-6625,N03-1003,0,0.171655,"l corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for t"
W16-6625,P14-1133,0,0.181583,"robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approach"
W16-6625,Q15-1039,0,0.0619899,"Missing"
W16-6625,D13-1160,0,0.0680962,"k dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-ba"
W16-6625,C04-1180,0,0.0518088,"st find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct answer—derivable from the question’s ungrounded graphs. These oracle graphs are then used to train a structured perceptron model. These steps are discussed in detail below. 157 3.1 Ungrounded Graphs from Paraphrases We use G RAPH PARSER (Reddy et al., 2014) to convert paraphrases to ungrounded graphs. This conversion involves three steps: 1) parsing the paraphrase using a CCG parser to extract syntactic derivations (Lewis and Steedman, 2014), 2) extracting logical forms from the CCG derivations (Bos et al., 2004), and 3) converting the logical forms to an ungrounded graph.8 The ungrounded graph for the example question and its paraphrases are shown in Figure 3(a), Figure 3(b) and Figure 3(c), respectively. 3.2 Grounded Graphs from Ungrounded Graphs The ungrounded graphs are grounded to Freebase subgraphs by mapping entity nodes, entity-entity edges and entity type nodes in the ungrounded graph to Freebase entities, relations and types, respectively. For example, the graph in Figure 3(b) can be converted to a Freebase graph in Figure 3(d) by replacing the entity node Czech Republic with the Freebase en"
W16-6625,P05-1022,0,0.0827711,"araphrases of the input question q. These paraphrases are further filtered with a classifier to improve the precision of the generated paraphrases. L-PCFG Estimation We train the L-PCFG Gsyn on the Paralex corpus (Fader et al., 2013). Paralex is a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what ex"
W16-6625,N13-1015,1,0.837008,"ase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent states. Then it foll"
W16-6625,W02-1001,0,0.0392051,"gˆ) under the model θ ∈ Rn : (ˆ p, u ˆ, gˆ) = arg max θ · Φ(p, u, g, q, K) , (p,u,g) where Φ(p, u, g, q, K) ∈ Rn denotes the features for the tuple of paraphrase, ungrounded and grounded graphs. The feature function has access to the paraphrase, ungrounded and grounded graphs, the original question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See §4.3 for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002). Given a training questionanswer pair (q, A), the update is: θt+1 ← θt +Φ(p+ , u+ , g + , q, K)−Φ(ˆ p, u ˆ, gˆ, q, K) , where (p+ , u+ , g + ) denotes the tuple of gold paraphrase, gold ungrounded and grounded graphs for 158 q. Since we do not have direct access to the gold paraphrase and graphs, we instead rely on the set of oracle tuples, OK,A (q), as a proxy: (p+ , u+ , g + ) = arg max (p,u,g)∈OK,A (q) θ · Φ(p, u, g, q, K) , where OK,A (q) is defined as the set of tuples (p, u, g) derivable from the question q, whose denotation |g|K has minimal F1 -loss against the gold answer A. We find t"
W16-6625,N06-2009,0,0.358515,"ebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 15"
W16-6625,P13-1158,0,0.615591,"ney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases."
W16-6625,N13-1092,0,0.134832,"Missing"
W16-6625,P96-1027,0,0.368333,"First, the sampling from an L-PCFG grammar lessens the lexical ambiguity problem evident in lexicalized grammars such as tree adjoining grammars (Narayan and Gardent, 2012) and combinatory categorial grammars (White, 2004). Our grammar is not lexicalized, only unary context-free rules are lexicalized. Second, the top-down sampling restricts the combinatorics inherent to bottom-up search (Shieber et al., 1990). Third, we do not restrict the generation by the order information in the input. The lack of order information in the input often raises the high combinatorics in lexicalist approaches (Kay, 1996). In our case, however, we use sampling to reduce this problem, and it allows us to produce syntactically diverse questions. And fourth, we impose no constraints on the grammar thereby making it easier to maintain bi-directional (recursive) grammars that can be used both for parsing and for generation (Shieber, 1988). 2.2 Bi-Layered L-PCFGs As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here. In their traditional use, the latent states in LPCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states:"
W16-6625,P07-2045,0,0.00417424,"Missing"
W16-6625,P02-1003,0,0.0603275,"e and outside trees as they use, capturing contextual syntactic information about nonterminals. We refer the reader to Narayan and Cohen (2015) for more detailed description of these features. In our experiments, we set the number of latent states to 24. Once we estimate Gsyn from the Paralex corpus, we restrict it for each question to a grammar G0syn by keeping only the rules that could lead to a derivation over the lattice. This step is similar to lexical pruning in standard grammar-based generation process to avoid an intermediate derivation which can never lead to a successful derivation (Koller and Striegnitz, 2002; Narayan and Gardent, 2012). Paraphrase Sampling Sampling a question from the grammar G0syn is done by recursively sampling nodes in the derivation tree, together with their latent states, in a top-down breadth-first fashion. Sampling from the pruned grammar G0syn raises an issue of oversampling words that are more frequent in the training data. To lessen this problem, we follow a controlled sampling approach where sampling is guided by the word lattice Wq . Once a word w from a path e in Wq is sampled, all other parallel or conflicting paths to e are removed from Wq . For example, generating"
W16-6625,D12-1069,0,0.0277247,"y idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasi"
W16-6625,D13-1161,0,0.0181592,"parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mai"
W16-6625,P98-1116,0,0.119099,"a large monolingual parallel corpus, containing 18 million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An ex"
W16-6625,D14-1107,0,0.0240241,"ly on manually assembled questionanswer pairs. For each training question, we first find the set of oracle grounded graphs—Freebase subgraphs which when executed yield the correct answer—derivable from the question’s ungrounded graphs. These oracle graphs are then used to train a structured perceptron model. These steps are discussed in detail below. 157 3.1 Ungrounded Graphs from Paraphrases We use G RAPH PARSER (Reddy et al., 2014) to convert paraphrases to ungrounded graphs. This conversion involves three steps: 1) parsing the paraphrase using a CCG parser to extract syntactic derivations (Lewis and Steedman, 2014), 2) extracting logical forms from the CCG derivations (Bos et al., 2004), and 3) converting the logical forms to an ungrounded graph.8 The ungrounded graph for the example question and its paraphrases are shown in Figure 3(a), Figure 3(b) and Figure 3(c), respectively. 3.2 Grounded Graphs from Ungrounded Graphs The ungrounded graphs are grounded to Freebase subgraphs by mapping entity nodes, entity-entity edges and entity type nodes in the ungrounded graph to Freebase entities, relations and types, respectively. For example, the graph in Figure 3(b) can be converted to a Freebase graph in Fig"
W16-6625,N12-1019,0,0.0463653,"ar is not fine-grained enough and often merges different paraphrase information into the same latent state. This problem is often severe for nonterminals at the top level of the bilayered tree. Hence, we rely only on unary lexical rules (the rules that produce terminal nodes) to extract paraphrase patterns in our experiments. 6 We have 154 positive and 846 negative paraphrase pairs. 7 We do not use the paraphrase pairs from the Paralex corpus to train our classifier, as they do not represent the distribution of our sampled paraphrases and the classifier trained on them performs poorly. follow Madnani et al. (2012), who used MT metrics for paraphrase identification, and experiment with 8 MT metrics as features for our binary classifier. In addition, we experiment with a binary feature which checks if the sampled paraphrase preserves named entities from the input sentence. We use WEKA (Hall et al., 2009) to replicate the classifier of Madnani et al. (2012) with our new feature. We tune the feature set for our classifier on the development data. 3 Semantic Parsing using Paraphrasing In this section we describe how the paraphrase algorithm is used for converting natural language to Freebase queries. Follow"
W16-6625,P14-5010,0,0.00325599,"r paraphrase generation (Quirk et al., 2004; Wubben et al., 2010). In particular, we use Moses (Koehn et al., 2007) to train a monolingual phrase-based MT system on the Paralex corpus. Finally, we use Moses decoder to generate 10-best distinct paraphrases for the test questions. MT 4.3 Implementation Details Entity Resolution For WebQuestions, we use 8 handcrafted part-of-speech patterns (e.g., the pattern (DT)?(JJ.?|NN.?){0,2}NN.? matches the noun phrase the big lebowski) to identify candidate named entity mention spans. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.10 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. We take the top 10 paths through the lattice as possible entity disambiguations. For each possibility, we generate n-best paraphrases that contains the entity mention spans. In the end, this process creates a total of 10n paraphrases. We generate ungrounded graphs for thes"
W16-6625,P05-1010,0,0.02558,"rove semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clust"
W16-6625,P15-1126,0,0.0215356,"latent states for each nonterminals ensures that we retrieve the correct syntactic representation of the sentence. Here, however, we are more interested in the second latent states assigned to each nonterminals which capture the paraphrase information of the sentence at various levels. For example, we have a unary lexical rule (NN-*-142 day) indicating that we observe day with NN of the paraphrase type 142. We could use this information to extract unary rules of the form (NN-*-142 w) in the treebank that will generate 4 For other cases of separating syntax from semantics in a similar way, see Mitchell and Steedman (2015). 156 words w which are paraphrases to day. Similarly, any node WHNP-*-291 in the treebank will generate paraphrases for what day, SBARQ-*-403, for what day is nochebuena. This way we will be able to generate paraphrases when is nochebuena and when is nochebuena celebrated as they both have SBARQ-*-403 as their roots.5 To generate a word lattice Wq for a given question q, we parse q with the bi-layered grammar Glayered . For each rule of the form X-m1 -m2 → w in the bilayered tree with X ∈ P, m1 ∈ {1, . . . , 24}, m2 ∈ {1, . . . , 1000} and w a word in q, we extract rules of the form X-∗-m2 →"
W16-6625,D15-1214,1,0.808172,". Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 153 Proceedings of The 9th International Natural Language Generation conference, pages 153–162, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics The main contributions of this paper are two fold. First, we present an algorithm (§2) to generate paraphrases using latent-variable PCFGs. We use the spectral method of Narayan and Cohen (2015) to estimate L-PCFGs on a large scale question treebank. Our grammar model leads to a robust and an efficient system for paraphrase generation in opendomain question answering. While CFGs have been explored for paraphrasing using bilingual parallel corpus (Ganitkevitch et al., 2013), ours is the first implementation of CFG that uses only monolingual data. Second, we show that generated paraphrases can be used to improve semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline e"
W16-6625,P16-1146,1,0.817238,"baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent states. Then it follows with a maximum likelihood estimation step, that"
W16-6625,C12-1124,1,0.916187,"se, capturing contextual syntactic information about nonterminals. We refer the reader to Narayan and Cohen (2015) for more detailed description of these features. In our experiments, we set the number of latent states to 24. Once we estimate Gsyn from the Paralex corpus, we restrict it for each question to a grammar G0syn by keeping only the rules that could lead to a derivation over the lattice. This step is similar to lexical pruning in standard grammar-based generation process to avoid an intermediate derivation which can never lead to a successful derivation (Koller and Striegnitz, 2002; Narayan and Gardent, 2012). Paraphrase Sampling Sampling a question from the grammar G0syn is done by recursively sampling nodes in the derivation tree, together with their latent states, in a top-down breadth-first fashion. Sampling from the pruned grammar G0syn raises an issue of oversampling words that are more frequent in the training data. To lessen this problem, we follow a controlled sampling approach where sampling is guided by the word lattice Wq . Once a word w from a path e in Wq is sampled, all other parallel or conflicting paths to e are removed from Wq . For example, generating for the word lattice in Fig"
W16-6625,N03-1024,0,0.0794827,"million pairs of question paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for the question What la"
W16-6625,P06-1055,0,0.0144064,"questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outside” trees, and then clusters them into latent"
W16-6625,W05-1512,0,0.00962666,"n be used to improve semantic parsing of questions into Freebase logical forms (§3). We build on a strong baseline of Reddy et al. (2014) and show that our grammar model competes with MT baseline even without using any parallel paraphrase resources. 2 Paraphrase Generation Using Grammars Our paraphrase generation algorithm is based on a model in the form of an L-PCFG. L-PCFGs are PCFGs where the nonterminals are refined with latent states that provide some contextual information about each node in a given derivation. L-PCFGs have been used in various ways, most commonly for syntactic parsing (Prescher, 2005; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen et al., 2013; Narayan and Cohen, 2015; Narayan and Cohen, 2016). In our estimation of L-PCFGs, we use the spectral method of Narayan and Cohen (2015), instead of using EM, as has been used in the past by Matsuzaki et al. (2005) and Petrov et al. (2006). The spectral method we use enables the choice of a set of feature functions that indicate the latent states, which proves to be useful in our case. It also leads to sparse grammar estimates and compact models. The spectral method works by identifying feature functions for “inside” and “outsid"
W16-6625,W04-3219,0,0.273262,"estion paraphrases with 2.4M distinct questions in the corpus. It is suitable for our task of generating paraphrases since its large scale makes our model robust for opendomain questions. We construct a treebank by parsing 2.4M distinct questions from Paralex using the BLLIP parser (Charniak and Johnson, 2005).3 Given the treebank, we use the spectral algorithm of Narayan and Cohen (2015) to learn an L-PCFG 1 Word lattices, formally weighted finite state automata, have been used in previous works for paraphrase generation (Langkilde and Knight, 1998; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004). We use an unweighted variant of word lattices in our algorithm. 2 For our experiments, we extract rules from the PPDBSmall to maintain the high precision (Ganitkevitch et al., 2013). 3 We ignore the Paralex alignments for training Gsyn . people speak what kind members of the public what exactly what what sort talking about Czech Czech Republic just what language linguistic do human beings people 's in Republic Czech the Czech Republic Cze express itself talk about ? to talk the population is speaking the citizens Figure 1: An example word lattice for the question What language do people in C"
W16-6625,Q14-1030,1,0.410455,"ng baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical p"
W16-6625,P07-1059,0,0.0439532,"al of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does not require aligned parallel paraphrase corpora. In addition we do not require hand annotated grammars for paraphrase generation but instead learn the grammar directly from a large scale question corpus. 153 Proceedings of The 9t"
W16-6625,C88-2128,0,0.670481,"e top-down sampling restricts the combinatorics inherent to bottom-up search (Shieber et al., 1990). Third, we do not restrict the generation by the order information in the input. The lack of order information in the input often raises the high combinatorics in lexicalist approaches (Kay, 1996). In our case, however, we use sampling to reduce this problem, and it allows us to produce syntactically diverse questions. And fourth, we impose no constraints on the grammar thereby making it easier to maintain bi-directional (recursive) grammars that can be used both for parsing and for generation (Shieber, 1988). 2.2 Bi-Layered L-PCFGs As mentioned earlier, one of our lattice types is based on bi-layered PCFGs introduced here. In their traditional use, the latent states in LPCFGs aim to capture syntactic information. We introduce here the use of an L-PCFG with two layers of latent states: one layer is intended to capture the usual syntactic information, and the other aims to capture semantic and topical information by using a SBARQ-33-403 SBARQ-30-403 SQ-8-925 WHNP-7-291 WP-7-254 NN-45-142 AUX-22-300 NN-41-854 what day is nochebuena SBARQ-24-403 WRB-42-707 SQ-8-709 WRB-42-707 when AUX-12-300 NN-41-85"
W16-6625,P15-1129,0,0.0272823,", 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et al., 2013; Berant and Liang, 2014; Wang et al., 2015). We propose a novel framework for paraphrasing using latent-variable PCFGs (L-PCFGs). Earlier approaches to paraphrasing used phrase-based machine translation for textbased QA (Duboue and Chu-Carroll, 2006; Riezler et al., 2007), or hand annotated grammars for KB-based QA (Berant and Liang, 2014). We find that phrase-based statistical machine translation (MT) approaches mainly produce lexical paraphrases without much syntactic diversity, whereas our grammar-based approach is capable of producing both lexically and syntactically diverse paraphrases. Unlike MT based approaches, our system does"
W16-6625,N06-1056,0,0.0309204,"phrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines. 1 Introduction Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). In this paper we consider the problem of semantically parsing questions into Freebase logical forms for the goal of question answering. Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), strongly-typed CCG grammars (Kwiatkowski et al., 2013; Reddy et al., 2014), We address the above problems by using paraphrases of the original question. Paraphrasing has shown to be promising for semantic parsing (Fader et a"
W16-6625,W10-4223,0,0.165589,"Missing"
W16-6625,P16-1220,1,0.837668,"lts on the test data. We get similar results on the test data as we reported on the development data. Again, the PPDB model performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et al. ’16 Yih et al. ’1"
W16-6625,P15-1049,0,0.0148143,"odel performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et al. ’16 Yih et al. ’15 52.8 Xu et al. ’16 53.1 This paper ORIGINAL 53.2 48.0 MT NAIVE 48.1 PPDB 48.4 BILAYERED 47.0 avg R. 46.6 60.4 55.7 61.1 60"
W16-6625,P15-1128,0,0.0431321,"2 shows our final results on the test data. We get similar results on the test data as we reported on the development data. Again, the PPDB model performs best with an F1 score of 47.7. The baselines, ORIGINAL and MT, lag with scores of 45.0 and 47.1, respectively. We also present the results of existing literature on this dataset. Among these, Berant and Liang (2014) also uses paraphrasing but unlike ours it is based on a template grammar (containing 8 grammar rules) and requires logical forms beforehand to generate paraphrases. Our PPDB outperforms Berant and Liang’s model by 7.8 F1 points. Yih et al. (2015) and Xu et al. (2016) use neural network models for semantic parsing, in addition to using sophisticated entity resolution (Yang and Chang, 2015) and a very large unsupervised corpus as additional training data. Note that we use G RAPH PARSER as our semantic parsing 160 avg oracle F1 # oracle graphs avg F1 65.1 71.5 71.2 71.8 71.6 11.0 77.2 53.6 59.8 55.0 44.7 47.0 47.5 47.9 47.1 ORIGINAL MT NAIVE PPDB BILAYERED Table 1: Oracle statistics and results on the WebQuestions development set. Method avg P. Berant and Liang ’14 40.5 Bast and Haussmann ’15 49.8 Berant and Liang ’15 50.4 49.0 Reddy et"
W16-6625,C98-1112,0,\N,Missing
W16-6625,P14-1091,0,\N,Missing
W16-6625,N15-3014,0,\N,Missing
W16-6625,P15-1026,0,\N,Missing
W17-3405,W14-6110,0,0.0264844,"Missing"
W17-3405,H91-1060,0,0.0341759,"ds to select only the largest singular values. The smaller ones can be removed from the SVD approximation. It is important to note that this issue with the EM algorithm and L-PCFGs is more of a theoretical concern than a practical concern. In practice, if the EM algorithm is initialized in the way specified by Matsuzaki et al. (2005), it converges to a local maximum that provides a relatively high parsing accuracy for syntactic parsing of English and other languages. In addition, the log-likelihood is not fully correlated with parsing performance (such as that measured by the PARSEVAL metric; Black et al., 1991) and therefore identifying the global maximum of the loglikelihood function does not guarantee optimal parsing performance. Coarse-to-Fine Techniques Building on the expectation-maximization algorithm, Petrov et al. (2006) introduced a coarse-to-fine technique (Charniak and Johnson, 2005) for estimating LPCFGs. This technique uses the EM algorithm as a subroutine. It first starts by running the EM algorithm with a small number of latent states for each nonterminal. It then works by successively “splitting” and “merging” nonterminals. In a split step, more latent states are added to the nonterm"
W17-3405,C10-2013,0,0.0292087,"esults were not state of the art, but close to it. In subsequent work, Petrov et al. (2006) used coarseto-fine techniques to further improve EM estimation of L-PCFGs. This led to the development of the Berkeley parser, which has given state-of-theart results for English and other languages. Spectral algorithms also yield results which are close to state of the art in a multilingual setting. Since their inception, L-PCFGs have been used for syntactic parsing in multiple studies for a variety of languages such as English, French, German, Chinese, Arabic and other morphologically rich languages (Candito et al., 2010; Attia et al., 2010; Green and Manning, 2010; Tounsi and Van Genabith, 2010; Goldberg and Elhadad, 2011; Dehdari et al., 2011; Bj¨orkelund et al., 2014; Zeng et al., 2014; Sun et al., 2014; Huang et al., 2014; Narayan and Cohen, 2016) 5.2 0 1 2 3 4 5 6 0 1 2 3 4 5 6 7 8 0 1 0 1 2 3 4 5 0 1 2 3 4 5 Interpretation for Latent States One of the advantages of using latent variable models is that often the latent variables can be assigned an interpretation once they are inferred. This post-hoc interpretation can be revealing about linguistic patterns that are present in the data and are learned aut"
W17-3405,P05-1022,0,0.0469272,"alized in the way specified by Matsuzaki et al. (2005), it converges to a local maximum that provides a relatively high parsing accuracy for syntactic parsing of English and other languages. In addition, the log-likelihood is not fully correlated with parsing performance (such as that measured by the PARSEVAL metric; Black et al., 1991) and therefore identifying the global maximum of the loglikelihood function does not guarantee optimal parsing performance. Coarse-to-Fine Techniques Building on the expectation-maximization algorithm, Petrov et al. (2006) introduced a coarse-to-fine technique (Charniak and Johnson, 2005) for estimating LPCFGs. This technique uses the EM algorithm as a subroutine. It first starts by running the EM algorithm with a small number of latent states for each nonterminal. It then works by successively “splitting” and “merging” nonterminals. In a split step, more latent states are added to the nonterminals (usually by multiplying the number of latent states associated with them by two). To avoid overfitting the model to the training data, this is accompanied by a merge step, in which latent states are merged together to decrease the number of latent states associated with each nonterm"
W17-3405,N13-1015,1,0.808265,"rface that includes several maxima, some better than others. Finding the global one (i.e. the local maximum that gives the highest value to the log-likelihood) is a computationally difficult problem for L-PCFGs, and there are no known solutions that guarantee such identification in an efficient manner. As a result, much of the theory of maximum likelihood estimation that is mentioned above does not apply to the EM algorithm with LPCFGs. Spectral Learning At their core, spectral algorithms exploit the conditional independence that L-PCFGs makes to extract the parameters with the latent states (Cohen et al., 2013, 2014). More specifically, L-PCFGs assume that an “inside” tree and an “outside” tree, shown in Figure 2 are conditionally independent of each other given the nonterminal and latent state that attaches them to each other. As such, the correlation between patterns in the inside tree and outside tree distributions dictate the identity of the latent states and their distribution. To identify such a correlation, one can extract the latent state parameters by building a co-occurrence matrix (or a cross-covariance matrix) of inside and outside trees (in skeletal form; these are represented by featu"
W17-3405,J03-4003,0,0.152899,"such grammars and to parse text with them. We also give an overview of what the latent states represent for English Penn treebank parsing, and provide an overview of extensions and related models to these grammars. 1 Introduction Probabilistic grammars have been one of the most important modeling tools available in the natural language processing toolkit. They are often humanly interpretable because of their symbolic backbone, while their probabilistic component helps with reasoning under uncertainty. Probabilistic grammars have mostly been used for syntactic analysis in NLP (Charniak, 1997; Collins, 2003; Hockenmaier and Steedman, 2002), but they are also useful for other problems both in and outside of NLP (Sakakibara et al., 1994; Guerra and Aloimonos, 2005; Lin et al., 2009). Latent-variable models, on the other hand, are also a modeling tool of great importance in natural language processing. They have been used for many applications, including machine translation, natural language generation, question answering and semantics. Latent-variable models are centered around learning from incomplete data. This means that the underlying statistical model is defined over latent random variables t"
W17-3405,C10-1061,0,0.0200711,"vparent , vlc , vrc ∈ Rm denoting vectors associated with a parent node, its left child and its right child, and F : Rm × Rm → Rm takes as input two children node vectors and returns the output node vector. In the case of L-PCFGs, F is a function that is associated with a rule a → b c, T a→b c , where b is the nonterminal for the left child, c is the nonterminal for the right child and a is the parent nonterminal. T a→b c is then defined as: Extensions of L-PCFGs and Related Models Extensions Natural generalizations of PCFGs, such as probabilistic linear-context free rewriting systems (LCFRS; Kallmeyer and Maier, 2010) and synchronous grammars can also be turned into probabilistic grammars with latent states. As long as the backbone structure of the skeletal grammar is of the form a → α where α includes nonterminals in one form or the other, the nonterminals can [T a→b c (vlc , vrc )]h1 X = p(a(h1 ) → b(h2 ) c(h3 ) |a(h1 ))[vlc ]h2 [vrc ]h3 . h2 ,h3 54 0 1 2 3 4 5 6 7 8 ”Aug. 30 , 1988”, ”Aug. 31 , 1987”, ”Dec. 31 , 1988”, ”Oct. 16 , 1996”, ”Oct. 1 , 1999”, ”Oct. 1 , 2019”, ”Nov. 8 , 1996”, ”Oct. 15 , 1999”, ”April 30 , 1988”, ”Nov. 8 , 1994” ”12,000 miles”, ”About 20,000 years”, “this year”, “A year”, “a y"
W17-3405,W11-3802,0,0.0275823,"Missing"
W17-3405,P03-1054,0,0.0243038,"terminals are decorated with a head word propagated from the bottom of the tree. Most often, the head word is propagated using head rules, which decide which child of a given node is the head node based on linguistically motivated rules. The context-free grammar formalism that corresponds to head lexicalization is bilexical grammar, which was introduced by Eisner and Satta (1999). Head lexicalization was used by Charniak (1997) and Collins (2003) to achieve state-of-theart parsing results for English. Head lexicalization of grammars served as the basis for much of the subsequent parsing work. Klein and Manning (2003) further built on the idea of tree transformations, and created linguistically motivated nonterminal refinements to parse the English treebank. Their work avoided the use of head lexicalization, but still produced a relatively high level of accuracy (though not state of the art) for parsing the Penn treebank. Some of the refinements they proposed generalize parent annotation (to higher order “vertical” Markovization) in a rather generic manner, but other refinements rely heavily on linguistic knowledge of English, and as such they do not generalize to treebanks in other languages. With all of"
W17-3405,D12-1019,0,0.0168653,"ntity of the latent states and their distribution. To identify such a correlation, one can extract the latent state parameters by building a co-occurrence matrix (or a cross-covariance matrix) of inside and outside trees (in skeletal form; these are represented by feature vectors over such trees; see below), and then apply singular value decomposition (SVD; Strang et al., 1993) on this matrix. This approach was originally introduced for hidden Markov models (Hsu et al., 2012) and has been used for other types of grammars and parsing formalisms as well (Bailly et al., 2010; Luque et al., 2012; Dhillon et al., 2012). As mentioned above, the inside and outside trees are represented by feature vectors in the cooccurrence matrix. This means that the inside and outside trees are mapped to real vectors. This is a common way to reduce a structured object into a manageable mathematical object that can be statistically processed. In the case of spectral algorithms for parsing, the feature functions indicate local neighborhood surrounding the top node (for inside trees) or footer node (for outside trees). As such, these methods distill information that was previously used by approaches such as parent annotation,"
W17-3405,D14-1081,0,0.0303104,"Missing"
W17-3405,W06-1638,0,0.0976615,"ertical” Markovization) in a rather generic manner, but other refinements rely heavily on linguistic knowledge of English, and as such they do not generalize to treebanks in other languages. With all of this previous work, nonterminal refinement is central to the underlying parsing formalism. However, these decorations are extracted from the treebank by means of transformations on trees. It was not until the work by Matsuzaki et al. (2005) and Prescher (2005) that the decoration became a “latent annotation.” At that point, LPCFGs were performing close to state of the art in syntactic parsing. Dreyer and Eisner (2006) suggested a more complex training algorithm for LPCFGs to improve their accuracy. Then, Petrov et al. (2006) further improved the parsing results of L-PCFGs to match state of the art and also suggested a coarse-to-fine approach that made parsing much more efficient (the asymptotic computational complexity of parsing with L-PCFGs, in their vanilla form, grows cubically with the number of latent states). It was at this time that many other researchers started to make use of L-PCFGs for a variety of syntax parsers in different languages, some of which are described in the rest of the paper. 4 1"
W17-3405,P99-1059,0,0.0249255,"ted from an L-PCFG model. The indices next to each nonterminal in the tree denote the latent states associated with that node in the derivation. (Punctuation omitted.) This idea is also strongly related to lexicalized grammars,1 in which nonterminals are decorated with a head word propagated from the bottom of the tree. Most often, the head word is propagated using head rules, which decide which child of a given node is the head node based on linguistically motivated rules. The context-free grammar formalism that corresponds to head lexicalization is bilexical grammar, which was introduced by Eisner and Satta (1999). Head lexicalization was used by Charniak (1997) and Collins (2003) to achieve state-of-theart parsing results for English. Head lexicalization of grammars served as the basis for much of the subsequent parsing work. Klein and Manning (2003) further built on the idea of tree transformations, and created linguistically motivated nonterminal refinements to parse the English treebank. Their work avoided the use of head lexicalization, but still produced a relatively high level of accuracy (though not state of the art) for parsing the Penn treebank. Some of the refinements they proposed generaliz"
W17-3405,D15-1178,1,0.790061,"h and Basque. • Lexicalization of closed-word tags: For the closed-word part-of-speech (POS) tags, both the EM algorithm and the spectral algorithm asso53 be decorated with additional latent state information. Work about using other grammar formalisms with latent states includes the work of Fowler and Penn (2010) who introduced latent states into a combinatory categorial grammar (CCG) for syntactic parsing, the work of Saluja et al. (2014), who generalized L-PCFGs to synchronous LPCFGs and proposed to estimate them using both a spectral algorithm and EM for machine translation and the work of Louis and Cohen (2015) who modeled online forum topic structure by using LCFRS with latent states (the latent states corresponded to topics that need to be inferred from data). Models similar to L-PCFGs have been used for parsing with discontinuous elements (Nederhof and Yli-Jyr¨a, 2017). They have also been used to describe transition-based systems for dependency formalisms (Nederhof, 2016). ciate a latent state with mostly a single word. For example, for determiners, there would be a latent state for “the” and for “a.” Often words with different casing or contracted form joined in the same cluster. Table 1, for e"
W17-3405,P10-1035,0,0.0185475,"POS tag and latent state in the Penn treebank). There are a few observations that can be concluded when inspecting these results: 2 Available in http://cohort.inf.ed.ac.uk/ lpcfgviewer. The online tool includes analysis of other languages, including French, German, Hebrew, Hungarian, Korean, Polish, Swedish and Basque. • Lexicalization of closed-word tags: For the closed-word part-of-speech (POS) tags, both the EM algorithm and the spectral algorithm asso53 be decorated with additional latent state information. Work about using other grammar formalisms with latent states includes the work of Fowler and Penn (2010) who introduced latent states into a combinatory categorial grammar (CCG) for syntactic parsing, the work of Saluja et al. (2014), who generalized L-PCFGs to synchronous LPCFGs and proposed to estimate them using both a spectral algorithm and EM for machine translation and the work of Louis and Cohen (2015) who modeled online forum topic structure by using LCFRS with latent states (the latent states corresponded to topics that need to be inferred from data). Models similar to L-PCFGs have been used for parsing with discontinuous elements (Nederhof and Yli-Jyr¨a, 2017). They have also been used"
W17-3405,E12-1042,0,0.014369,"ions dictate the identity of the latent states and their distribution. To identify such a correlation, one can extract the latent state parameters by building a co-occurrence matrix (or a cross-covariance matrix) of inside and outside trees (in skeletal form; these are represented by feature vectors over such trees; see below), and then apply singular value decomposition (SVD; Strang et al., 1993) on this matrix. This approach was originally introduced for hidden Markov models (Hsu et al., 2012) and has been used for other types of grammars and parsing formalisms as well (Bailly et al., 2010; Luque et al., 2012; Dhillon et al., 2012). As mentioned above, the inside and outside trees are represented by feature vectors in the cooccurrence matrix. This means that the inside and outside trees are mapped to real vectors. This is a common way to reduce a structured object into a manageable mathematical object that can be statistically processed. In the case of spectral algorithms for parsing, the feature functions indicate local neighborhood surrounding the top node (for inside trees) or footer node (for outside trees). As such, these methods distill information that was previously used by approaches such"
W17-3405,P11-2124,0,0.0138067,"oarseto-fine techniques to further improve EM estimation of L-PCFGs. This led to the development of the Berkeley parser, which has given state-of-theart results for English and other languages. Spectral algorithms also yield results which are close to state of the art in a multilingual setting. Since their inception, L-PCFGs have been used for syntactic parsing in multiple studies for a variety of languages such as English, French, German, Chinese, Arabic and other morphologically rich languages (Candito et al., 2010; Attia et al., 2010; Green and Manning, 2010; Tounsi and Van Genabith, 2010; Goldberg and Elhadad, 2011; Dehdari et al., 2011; Bj¨orkelund et al., 2014; Zeng et al., 2014; Sun et al., 2014; Huang et al., 2014; Narayan and Cohen, 2016) 5.2 0 1 2 3 4 5 6 0 1 2 3 4 5 6 7 8 0 1 0 1 2 3 4 5 0 1 2 3 4 5 Interpretation for Latent States One of the advantages of using latent variable models is that often the latent variables can be assigned an interpretation once they are inferred. This post-hoc interpretation can be revealing about linguistic patterns that are present in the data and are learned automatically. To do this kind of analysis, we computed the marginals for each node in each tree in the Pen"
W17-3405,J93-2004,0,0.0591989,"Missing"
W17-3405,P96-1024,0,0.101748,"iments were done later with spectral algorithms, and showed that essentially using multiple grammars has the effect of regularization. Petrov and Klein (2008) extended L-PCFGs to log-linear latent grammars – this means that the τ ∗ = arg max τ X h p(τ (h) |w). The maximization of a sum of products in this form (the product originates in the probability p(τ (h) |w), which is proportional to a product of rule probabilities) is computationally intractable. As such, other approaches to parsing with L-PCFGs have been developed. The most common one used is based on minimum Bayes risk decoding (MBR; Goodman, 1996). With MBR, the maximization problem turns into maximizing the sum of the marginal probabilities of each node that appear in the tree. It is motivated by maximizing the recall of correct constituents in the predicted tree. 52 State MBR can be quite expensive to run with a large number of latent states, as the dynamic programming algorithm that performs the parsing scales cubically as a function of the number of latent states. This is where coarse-to-fine techniques have an advantage – one can parse incrementally with coarser models until reaching the most refined model, at each point pruning t"
W17-3405,C10-1045,0,0.012481,"e to it. In subsequent work, Petrov et al. (2006) used coarseto-fine techniques to further improve EM estimation of L-PCFGs. This led to the development of the Berkeley parser, which has given state-of-theart results for English and other languages. Spectral algorithms also yield results which are close to state of the art in a multilingual setting. Since their inception, L-PCFGs have been used for syntactic parsing in multiple studies for a variety of languages such as English, French, German, Chinese, Arabic and other morphologically rich languages (Candito et al., 2010; Attia et al., 2010; Green and Manning, 2010; Tounsi and Van Genabith, 2010; Goldberg and Elhadad, 2011; Dehdari et al., 2011; Bj¨orkelund et al., 2014; Zeng et al., 2014; Sun et al., 2014; Huang et al., 2014; Narayan and Cohen, 2016) 5.2 0 1 2 3 4 5 6 0 1 2 3 4 5 6 7 8 0 1 0 1 2 3 4 5 0 1 2 3 4 5 Interpretation for Latent States One of the advantages of using latent variable models is that often the latent variables can be assigned an interpretation once they are inferred. This post-hoc interpretation can be revealing about linguistic patterns that are present in the data and are learned automatically. To do this kind of analysis, we c"
W17-3405,P05-1010,0,0.36159,"elatively high level of accuracy (though not state of the art) for parsing the Penn treebank. Some of the refinements they proposed generalize parent annotation (to higher order “vertical” Markovization) in a rather generic manner, but other refinements rely heavily on linguistic knowledge of English, and as such they do not generalize to treebanks in other languages. With all of this previous work, nonterminal refinement is central to the underlying parsing formalism. However, these decorations are extracted from the treebank by means of transformations on trees. It was not until the work by Matsuzaki et al. (2005) and Prescher (2005) that the decoration became a “latent annotation.” At that point, LPCFGs were performing close to state of the art in syntactic parsing. Dreyer and Eisner (2006) suggested a more complex training algorithm for LPCFGs to improve their accuracy. Then, Petrov et al. (2006) further improved the parsing results of L-PCFGs to match state of the art and also suggested a coarse-to-fine approach that made parsing much more efficient (the asymptotic computational complexity of parsing with L-PCFGs, in their vanilla form, grows cubically with the number of latent states). It was at th"
W17-3405,N06-1020,0,0.010784,"f L-PCFGs for syntactic parsing. 5.1 Other Learning Algorithms Parsing with L-PCFGs Parsing with L-PCFGs usually entails finding a skeletal tree for a given string. While the latent variables assist in the modeling by adding contextual information to the derivation, they are not necessarily a target for prediction, and therefore we are interested in marginalizing them out during parsing. Given a string w, this means that we are interested in finding the following tree: Other scenarios and algorithms for learning LPCFGs have been proposed. One such example is that of the work on self-training (McClosky et al., 2006) of L-PCFGs by Huang et al. (2010) and Huang and Harper (2009). With self-training, a parser is trained from seed annotated data (such as the Penn treebank), and then the parser learned is used to parse a large amount of unlabeled data. After that step, a new parser is learned from both the annotated data and the parsed data, as if the latter are gold-standard data. Petrov (2010) exploited the fact that the EM algorithm is sensitive to its initialization point (and returns a different model in each execution) and estimated an L-PCFG multiple times from annotated data using a coarse-to-fine EM"
W17-3405,P02-1043,0,0.0615429,"and to parse text with them. We also give an overview of what the latent states represent for English Penn treebank parsing, and provide an overview of extensions and related models to these grammars. 1 Introduction Probabilistic grammars have been one of the most important modeling tools available in the natural language processing toolkit. They are often humanly interpretable because of their symbolic backbone, while their probabilistic component helps with reasoning under uncertainty. Probabilistic grammars have mostly been used for syntactic analysis in NLP (Charniak, 1997; Collins, 2003; Hockenmaier and Steedman, 2002), but they are also useful for other problems both in and outside of NLP (Sakakibara et al., 1994; Guerra and Aloimonos, 2005; Lin et al., 2009). Latent-variable models, on the other hand, are also a modeling tool of great importance in natural language processing. They have been used for many applications, including machine translation, natural language generation, question answering and semantics. Latent-variable models are centered around learning from incomplete data. This means that the underlying statistical model is defined over latent random variables that are not observed in the data"
W17-3405,D15-1214,1,0.88109,"14; Sun et al., 2014; Huang et al., 2014; Narayan and Cohen, 2016) 5.2 0 1 2 3 4 5 6 0 1 2 3 4 5 6 7 8 0 1 0 1 2 3 4 5 0 1 2 3 4 5 Interpretation for Latent States One of the advantages of using latent variable models is that often the latent variables can be assigned an interpretation once they are inferred. This post-hoc interpretation can be revealing about linguistic patterns that are present in the data and are learned automatically. To do this kind of analysis, we computed the marginals for each node in each tree in the Penn treebank after training a model with the spectral algorithm of Narayan and Cohen (2015). The results are available in the LPCFGV IEWER tool.2 Frequent words IN (preposition) of ×323 about ×248 than ×661, as ×648, because ×209 from ×313, at ×324 into ×178 over ×122 Under ×127 DT (determiners) These ×105 Some ×204 that ×190 both ×102 any ×613 the ×574 those ×247, all ×242 all ×105 another ×276, no ×211 CD (numbers) 8 ×132 million ×451, billion ×248 RB (adverb) up ×175 as ×271 not ×490, n’t ×2695 not ×236 only ×159 well ×129 CC (conjunction) But ×255 and ×101 and ×218 But ×196 or ×162 and ×478 Table 1: A list of part-of-speech tags and most frequent words associated with latent sta"
W17-3405,P16-1146,1,0.828742,"the marginal log-likelihood as EM does, they find a local maximum for the conditional marginal log-likelihood. One of the earlier training algorithms of LPCFGs selectively refines nonterminals by using EM with annealing (Dreyer and Eisner, 2006). The authors slightly modify the L-PCFG model to pass features between nodes in the parse tree, motivated by prior linguistic work on feature passing. Finally, Stanojevi´c and Sima’an (2015) used a refinement model for the induction of a Reordering Grammar for machine translation. They used the EM algorithm for estimating their model. That being said, Narayan and Cohen (2016) showed that the number of latent states can be further optimized with spectral algorithms by using coarse-to-fine techniques such as Petrov et al. (2006) used. This means that while the criterion above with top singular values is natural and easy to implement, further refinements can be made to improve it. It is important to note that unlike spectral algorithms, the EM algorithm has an interpretation that is valid even when the data it is applied on is not generated from an L-PCFG in the family we are estimating from. It can be viewed as minimizing Kullback-Leibler (KL) divergence, a measure"
W17-3405,W16-2403,0,0.012392,"ar (CCG) for syntactic parsing, the work of Saluja et al. (2014), who generalized L-PCFGs to synchronous LPCFGs and proposed to estimate them using both a spectral algorithm and EM for machine translation and the work of Louis and Cohen (2015) who modeled online forum topic structure by using LCFRS with latent states (the latent states corresponded to topics that need to be inferred from data). Models similar to L-PCFGs have been used for parsing with discontinuous elements (Nederhof and Yli-Jyr¨a, 2017). They have also been used to describe transition-based systems for dependency formalisms (Nederhof, 2016). ciate a latent state with mostly a single word. For example, for determiners, there would be a latent state for “the” and for “a.” Often words with different casing or contracted form joined in the same cluster. Table 1, for example, shows that latent state 2 for RB (adverbs) is associated with “not” and “n’t.” • Semantic clustering of phrases: Consider the noun phrases in Table 2, which are the most frequently ones associated with each latent state in the learned L-PCFG model. We see that there is in certain cases semantic clustering of such noun phrases, in cases where such semantic cluste"
W17-3405,D09-1087,0,0.0156576,"s Parsing with L-PCFGs Parsing with L-PCFGs usually entails finding a skeletal tree for a given string. While the latent variables assist in the modeling by adding contextual information to the derivation, they are not necessarily a target for prediction, and therefore we are interested in marginalizing them out during parsing. Given a string w, this means that we are interested in finding the following tree: Other scenarios and algorithms for learning LPCFGs have been proposed. One such example is that of the work on self-training (McClosky et al., 2006) of L-PCFGs by Huang et al. (2010) and Huang and Harper (2009). With self-training, a parser is trained from seed annotated data (such as the Penn treebank), and then the parser learned is used to parse a large amount of unlabeled data. After that step, a new parser is learned from both the annotated data and the parsed data, as if the latter are gold-standard data. Petrov (2010) exploited the fact that the EM algorithm is sensitive to its initialization point (and returns a different model in each execution) and estimated an L-PCFG multiple times from annotated data using a coarse-to-fine EM technique. Once all grammars were learned, they were combined"
W17-3405,D10-1002,0,0.0154416,"Other Learning Algorithms Parsing with L-PCFGs Parsing with L-PCFGs usually entails finding a skeletal tree for a given string. While the latent variables assist in the modeling by adding contextual information to the derivation, they are not necessarily a target for prediction, and therefore we are interested in marginalizing them out during parsing. Given a string w, this means that we are interested in finding the following tree: Other scenarios and algorithms for learning LPCFGs have been proposed. One such example is that of the work on self-training (McClosky et al., 2006) of L-PCFGs by Huang et al. (2010) and Huang and Harper (2009). With self-training, a parser is trained from seed annotated data (such as the Penn treebank), and then the parser learned is used to parse a large amount of unlabeled data. After that step, a new parser is learned from both the annotated data and the parsed data, as if the latter are gold-standard data. Petrov (2010) exploited the fact that the EM algorithm is sensitive to its initialization point (and returns a different model in each execution) and estimated an L-PCFG multiple times from annotated data using a coarse-to-fine EM technique. Once all grammars were"
W17-3405,N10-1003,0,0.128181,"Given a string w, this means that we are interested in finding the following tree: Other scenarios and algorithms for learning LPCFGs have been proposed. One such example is that of the work on self-training (McClosky et al., 2006) of L-PCFGs by Huang et al. (2010) and Huang and Harper (2009). With self-training, a parser is trained from seed annotated data (such as the Penn treebank), and then the parser learned is used to parse a large amount of unlabeled data. After that step, a new parser is learned from both the annotated data and the parsed data, as if the latter are gold-standard data. Petrov (2010) exploited the fact that the EM algorithm is sensitive to its initialization point (and returns a different model in each execution) and estimated an L-PCFG multiple times from annotated data using a coarse-to-fine EM technique. Once all grammars were learned, they were combined in a product-of-experts style, and then they were used to parse unseen data. Similar experiments were done later with spectral algorithms, and showed that essentially using multiple grammars has the effect of regularization. Petrov and Klein (2008) extended L-PCFGs to log-linear latent grammars – this means that the τ"
W17-3405,P06-1055,0,0.2259,"English, and as such they do not generalize to treebanks in other languages. With all of this previous work, nonterminal refinement is central to the underlying parsing formalism. However, these decorations are extracted from the treebank by means of transformations on trees. It was not until the work by Matsuzaki et al. (2005) and Prescher (2005) that the decoration became a “latent annotation.” At that point, LPCFGs were performing close to state of the art in syntactic parsing. Dreyer and Eisner (2006) suggested a more complex training algorithm for LPCFGs to improve their accuracy. Then, Petrov et al. (2006) further improved the parsing results of L-PCFGs to match state of the art and also suggested a coarse-to-fine approach that made parsing much more efficient (the asymptotic computational complexity of parsing with L-PCFGs, in their vanilla form, grows cubically with the number of latent states). It was at this time that many other researchers started to make use of L-PCFGs for a variety of syntax parsers in different languages, some of which are described in the rest of the paper. 4 1 This is different than the lexicalization of grammars where all derivation rules are put into the lexicon, su"
W17-3405,J98-4004,0,0.548762,"t latent state 2 for example for an NP has no relationship to latent state 2 for VP. The generative story that such an L-PCFG model induces is similar to one of PCFGs. We begin with the top node of the derivation tree with its latent state by drawing a nonterminal and a latent state from p(a(h)), and then recursively draw rules in the form of a(h) → x or a(h1 ) → 3 Evolution of Latent-Variable PCFGs The idea of decorating nonterminals with additional information, and breaking the statistical independence assumptions that PCFGs typically make, has a long history in natural language processing. Johnson (1998) introduced a variety of tree transformations on the Penn treebank, with an aim to improve the parsing accuracy of a PCFG extracted from that treebank. One of the transformations introduced was that of “parent annotation” where each nonterminal is annotated with its parent symbol. 48 S(2) S(1) S(1) IN(2) so NP(4) DT(1) The NP(50 VP(6) NN(5) DT(1) NN(5) governor the lieutenant MD(1) RB(1) could n’t VP(6) VB(2) NP(3) make PRP(4) VP(7) VBD(2) welcomed NP(3) DT(1) NNS(2) the guests it Figure 1: An example of a phrase-structure tree in English inspired by the Penn treebank (Marcus et al., 1993), po"
W17-3405,D14-1210,1,0.824892,"2 Available in http://cohort.inf.ed.ac.uk/ lpcfgviewer. The online tool includes analysis of other languages, including French, German, Hebrew, Hungarian, Korean, Polish, Swedish and Basque. • Lexicalization of closed-word tags: For the closed-word part-of-speech (POS) tags, both the EM algorithm and the spectral algorithm asso53 be decorated with additional latent state information. Work about using other grammar formalisms with latent states includes the work of Fowler and Penn (2010) who introduced latent states into a combinatory categorial grammar (CCG) for syntactic parsing, the work of Saluja et al. (2014), who generalized L-PCFGs to synchronous LPCFGs and proposed to estimate them using both a spectral algorithm and EM for machine translation and the work of Louis and Cohen (2015) who modeled online forum topic structure by using LCFRS with latent states (the latent states corresponded to topics that need to be inferred from data). Models similar to L-PCFGs have been used for parsing with discontinuous elements (Nederhof and Yli-Jyr¨a, 2017). They have also been used to describe transition-based systems for dependency formalisms (Nederhof, 2016). ciate a latent state with mostly a single word."
W17-3405,P13-1045,0,0.0189189,"in statistical latent-variable parsing models for Arabic, English and French. In Proceedings of the NAACL-HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages. F (vlc , vrc ) = tanh(W [vlc , vrc ] + b), where W ∈ Rm×2m and b ∈ Rm are weight matrices and biases that are learned when training the neural network, [u1 , u2 ] denotes the concatenation of two vectors u1 and u2 , and tanh : Rm → Rm is a function that applies the hyperbolic tangent function coordinate-wise. Even more closely related to L-PCFGs is the further refinement of these recursive neural networks by Socher et al. (2013) where the weights in the neural network are parametrized by labels in the tree, corresponding to syntactic categories. Similarly to a formulation of the inside tree as a vector propagation procedure (in Eq. 2), there is also a formulation for the outside algorithm (Cohen et al., 2014). Le and Zuidema (2014) also extended the recursive neural networks mentioned above to make use of the outside tree information. Finally, it is also important to note that LPCFGs are related to probabilistic regular tree grammars (PRTGs; Knight and Graehl, 2005) where the righthand side trees of the PRTG rules ar"
W17-3405,D15-1005,0,0.0287647,"Missing"
W17-3405,1993.eamt-1.1,0,0.652268,"Missing"
W17-3405,D14-1035,0,0.0165754,"t of the Berkeley parser, which has given state-of-theart results for English and other languages. Spectral algorithms also yield results which are close to state of the art in a multilingual setting. Since their inception, L-PCFGs have been used for syntactic parsing in multiple studies for a variety of languages such as English, French, German, Chinese, Arabic and other morphologically rich languages (Candito et al., 2010; Attia et al., 2010; Green and Manning, 2010; Tounsi and Van Genabith, 2010; Goldberg and Elhadad, 2011; Dehdari et al., 2011; Bj¨orkelund et al., 2014; Zeng et al., 2014; Sun et al., 2014; Huang et al., 2014; Narayan and Cohen, 2016) 5.2 0 1 2 3 4 5 6 0 1 2 3 4 5 6 7 8 0 1 0 1 2 3 4 5 0 1 2 3 4 5 Interpretation for Latent States One of the advantages of using latent variable models is that often the latent variables can be assigned an interpretation once they are inferred. This post-hoc interpretation can be revealing about linguistic patterns that are present in the data and are learned automatically. To do this kind of analysis, we computed the marginals for each node in each tree in the Penn treebank after training a model with the spectral algorithm of Narayan and Cohen (2"
W17-3405,tounsi-van-genabith-2010-arabic,0,0.0201413,"Missing"
W17-3405,P10-1040,0,0.052678,"Examples of most likely phrases for the noun phrase category (NP) for a latent-variable PCFG extracted using the algorithm of Narayan and Cohen (2015) from the Penn treebank. Numbers next to the phrases indicate that the phrase appeared multiple times in the list. 55 Acknowledgments This general formulation as in Eq. 2 gives rise to generalizations of other formulations of latent representations that are propagated in a (parse) tree structure. Perhaps the most related one to LPCFGs is the recursive neural network of Socher et al. (2010). This recursive neural network propagates word vectors (Turian et al., 2010) from the bottom of an unlabeled tree all the way to its top using the function: I would like to thank Shashi Narayan, Nikos Papasarantopoulos and Giorgio Satta for useful feedback and comments. This work was supported by an EU H2020 grant (688139/H2020-ICT-2015; SUMMA) and a grant from Bloomberg. References M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and J. Van Genabith. 2010. Handling unknown words in statistical latent-variable parsing models for Arabic, English and French. In Proceedings of the NAACL-HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages"
W17-3405,W10-1408,0,\N,Missing
W19-1203,E17-2039,0,0.16373,"Missing"
W19-1203,W19-1201,0,0.137366,"Missing"
W19-1203,P17-4012,0,0.0503272,"sformer transformer True dot 5 Table 1: Choice of hyperparameters for our neural network models. remain them case-sensitive. Following previous work (van Noord et al., 2018), the indices of variables in clauses are relative, as shown in Figure 3(b), which is the same to the character-level preprocessing. 2.2 Neural Models We adopt Recurrent Neural Networks (RNNs) equipped with Long Shot-Term Memory (LSTM; Hochreiter and Schmidhuber 1997) units and the Transformer model (Vaswani et al., 2017) as our neural models. For the model implementation, we use the one provided by the OpenNMT-py toolkit (Klein et al., 2017). The hyperparameters we used are shown in Table 1 which are institutionally set without optimization. Fine-tuning We propose a fine-tuning approach to enable the system to effectively use more training data in various quality, i.e. bronze and silver data. The fine-tuning approach allows the system train to convergence on one dataset (e.g. silver and gold data) and then continues to train to convergence on another dataset (e.g. gold data), where the optimizers are reset. LSTM sg-data sg-data + g-data Transformer sg-data sg-data + g-data P 73.91 86.05 P 69.11 82.32 character R F1 75.00 74.45 84"
W19-1203,P18-1040,1,0.439748,"al. 2017) provides a large collection of English texts annotated with Discourse Representation Structures (DRS), while the Parallel Meaning Bank (PMB; Abzianidze et al. 2017) provides DRSs in English, German, Italian and Dutch. Furthermore, the PMB introduces clause representation, as shown on the top of Figure 1. With the recent introduction of neural network learning to the Natural Language Processing community, several neural DRS parsers have been developed for the problem of DRS parsing, i.e. the problem of taking a document or a sentence as input, and outputting their corresponding DRS. Liu et al. (2018) convert box-style DRSs to tree-style DRSs and propose the three-step tree DRS parser on the GMB, while van Noord et al. (2018) adopt a neural machine translation approach to parse sentences to their clause-style DRSs on PMB. Due to the different standard of annotations between GMB and PMB, and that the IWCS-2019 Shared Task of DRS Parsing mainly focuses on averagely short sentences in PMB annotations, our systems take sentences as input and output a clause-style DRS of PMB represented as a sequence for the IWCS-2018 Shared Task of DRS parsing (Abzianidze et al., 2019). 2 The Parsing System Fi"
W19-1203,D14-1162,0,0.089445,"(2018) to transform back the output of our models to the clause format, and then use COUNTER (van Noord et al., 2018) as our evaluation metric. 3 Experiments In this section, we introduce the training data that we used and the results on the PMB benchmarks. 3.1 Data The training data consists of all of the bronze data (bronze), all of the silver data (silver), and the training section of the gold data (gold). All data is preprocessed. We mix bronze, silver and gold as bsg-data, and mix silver and gold as sg-data, and name the training section of gold data as g-data. Meanwhile, we adopt GloVe (Pennington et al., 2014) pre-trained word embeddings5 to initialize the representation of input tokens. 3.2 Results Table 2 shows the results on test data, where sg-data means that the models are only trained on sg-data, and + g-data means that the models are continually fine-tuned on g-data. With LSTM, the character model performs marginally better than the word model. However, with Transformer, the word model performs significantly better than the character model. With both LSTM and Transformer, fine-tuning on g-data significantly improves the performance. Although the character LSTM is marginally better than the w"
W19-1203,L18-1267,0,0.236563,"Missing"
W19-1203,Q18-1043,0,0.245075,"Missing"
W19-3104,P17-1104,0,0.0182951,"roach to the syntax-semantics interface exploits multi-component synchronous tree-adjoining grammars; see Nesson and Shieber (2006) and references therein. However, these formal models yield tree-like semantic representations, as opposed to general graphs. A common approach in semantic parsing is to extend existing syntactic dependency parsers to produce graphs, realizing translation models from strings to graphs, as opposed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement grammars, who provide unsupe"
W19-3104,D17-1130,0,0.0193403,"formalism and ours is made in Remark 1. An alternative approach to the syntax-semantics interface exploits multi-component synchronous tree-adjoining grammars; see Nesson and Shieber (2006) and references therein. However, these formal models yield tree-like semantic representations, as opposed to general graphs. A common approach in semantic parsing is to extend existing syntactic dependency parsers to produce graphs, realizing translation models from strings to graphs, as opposed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous"
W19-3104,C12-1083,0,0.0596709,"Missing"
W19-3104,W13-2322,0,0.0236221,"ence resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based systems such as the one by Wang et al. (2015b) and, more in general, for work on the syntax-semantics interface. Bottom-up tree transducers (Thatcher, 1973) have gained significant attention in the field of machine translation, where they are used to map syntactic phrase structure trees from source to target 7 Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing, pages 7–17 c Dresden, Germany, Septembe"
W19-3104,Q15-1040,0,0.0192351,"from strings to graphs, as opposed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement grammars, who provide unsupervised learning algorithms for grammar extraction. Finally, Groschwitz et al. (2018) use a neural supertag parser to map a string into a dependency-style tree representation of the com2 Preliminaries In this section we introduce the notation and terminology that is used throughout this paper. General Notation. The set of natural numbers (including zero) is denoted by N, and N+ = N  {0}. For n"
W19-3104,J16-4009,0,0.0181745,"a natural language sentence and has to output a directed graph representing an associated, mostlikely semantic analysis. Semantic parsing integrates tasks that have usually been addressed separately in statistical natural language processing, such as named entity recognition, word sense disambiguation, semantic role labeling, and coreference resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based systems such as the one by Wang et al. (2015b) and, more in general, for work on the syntax-semantics interface. Bottom"
W19-3104,E17-1051,1,0.839032,"k 1. An alternative approach to the syntax-semantics interface exploits multi-component synchronous tree-adjoining grammars; see Nesson and Shieber (2006) and references therein. However, these formal models yield tree-like semantic representations, as opposed to general graphs. A common approach in semantic parsing is to extend existing syntactic dependency parsers to produce graphs, realizing translation models from strings to graphs, as opposed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement g"
W19-3104,S15-2153,0,0.0903727,"Missing"
W19-3104,S14-2008,0,0.0288759,"r. 1 Frank Drewes Ume˚a University Ume˚a, Sweden Introduction In dependency semantic parsing, one is given a natural language sentence and has to output a directed graph representing an associated, mostlikely semantic analysis. Semantic parsing integrates tasks that have usually been addressed separately in statistical natural language processing, such as named entity recognition, word sense disambiguation, semantic role labeling, and coreference resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based system"
W19-3104,P18-1170,0,0.0364482,"Missing"
W19-3104,K15-1004,0,0.0190018,"by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement grammars, who provide unsupervised learning algorithms for grammar extraction. Finally, Groschwitz et al. (2018) use a neural supertag parser to map a string into a dependency-style tree representation of the com2 Preliminaries In this section we introduce the notation and terminology that is used throughout this paper. General Notation. The set of natural numbers (including zero) is denoted by N, and N+ = N  {0}. For n ∈ N the set {1, . . . , n} is abbreviated to [n]. In particular, [0] = ∅. The set of all finite sequences of elements of a set S is wr"
W19-3104,S15-1031,0,0.0242301,"ed to the treeto-graph model investigated here. On this line, transition-based, greedy parsers have been adapted by Ballesteros and Al-Onaizan (2017), Damonte et al. (2017), Hershcovich et al. (2017), Peng et al. (2018) and Vilares and G´omez-Rodr´ıguez (2018). Despite the fact that the input is a bare string, these systems exploit features obtained from a precomputed run of a dependency parser, thus committing to some best parse tree, similarly to the pipeline model of Wang et al. (2015b). Dynamic programming parsers have also been adapted to produce graphs by Kuhlmann and Jonsson (2015) and Schluter (2015). Semantic translation from strings to graphs is further investigated by Jones et al. (2012) and Peng et al. (2015) using synchronous hyperedge replacement grammars, who provide unsupervised learning algorithms for grammar extraction. Finally, Groschwitz et al. (2018) use a neural supertag parser to map a string into a dependency-style tree representation of the com2 Preliminaries In this section we introduce the notation and terminology that is used throughout this paper. General Notation. The set of natural numbers (including zero) is denoted by N, and N+ = N  {0}. For n ∈ N the set {1, . ."
W19-3104,N18-2023,0,0.0336067,"Missing"
W19-3104,P15-2141,0,0.291807,"ociated, mostlikely semantic analysis. Semantic parsing integrates tasks that have usually been addressed separately in statistical natural language processing, such as named entity recognition, word sense disambiguation, semantic role labeling, and coreference resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based systems such as the one by Wang et al. (2015b) and, more in general, for work on the syntax-semantics interface. Bottom-up tree transducers (Thatcher, 1973) have gained significant attention in"
W19-3104,N15-1040,0,0.40211,"ociated, mostlikely semantic analysis. Semantic parsing integrates tasks that have usually been addressed separately in statistical natural language processing, such as named entity recognition, word sense disambiguation, semantic role labeling, and coreference resolution. Semantic parsing is currently receiving considerable attention, as attested by the number of approaches being proposed for its solution (Oepen et al., 2014, 2015) and by the variety of existing semantic representations and available datasets (Kuhlmann and Oepen, 2016). A successful approach to dependency semantic parsing by Wang et al. (2015b,a) first parses the input sentence into a dependency tree t, and then applies a transition-based algorithm that translates t into a dependency graph in Abstract Meaning Representation (AMR), a popular semantic representation developed by Banarescu et al. (2013). In this work, we present a finite-state transducer for tree-to-graph translation that can serve as a mathematical model for transition-based systems such as the one by Wang et al. (2015b) and, more in general, for work on the syntax-semantics interface. Bottom-up tree transducers (Thatcher, 1973) have gained significant attention in"
