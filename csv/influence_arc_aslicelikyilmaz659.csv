2020.acl-main.440,E09-1003,0,0.0173744,"et al., 2019) consists of 11,827 videos of 180 tasks in 12 daily life domains. YouMakeup (Wang et al., 2019) consists of 2,800 YouTube videos, annotated with natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. Leveraging Document Level Alignments Our work relies on the assumption that text recipes and instructional cooking videos of the same dish are comparable. This idea has been used to extract parallel sentences from comparable corpora to increase the number of training examples for machine translation (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010; Gr´egoire and Langlais, 2018). Likewise, TalkSumm (Lev et al., 2019) use the transcripts of scientific conference talks to automatically extract summaries. Zhu et al. (2015) use books and movie adaptations of the books to extract descriptive explanations of movie scenes. Related Tasks A related task is localizing and classifying steps in instructional videos (Alayrac et al., 2016; Zhukov et al., 2019) where they detect when an action is performed in the video whereas we focus on describing actions. Dense event captioning of instructional videos (Zhou et al., 2018b; Li et"
2020.acl-main.440,J93-2003,0,0.169746,"Missing"
2020.acl-main.440,2021.ccl-1.108,0,0.0414258,"Missing"
2020.acl-main.440,D15-1166,0,0.0881191,"Missing"
2020.acl-main.440,N15-1015,0,0.0466122,"tions (or video transcript sentences) in the target recipe. We make two modifications to the alignment algorithm described above: First, our recipe pairs, unlike the wet lab protocol data, does not follow the same temporal sequence. The alignment algorithm must thus learn to jump within a longer range. We set the window of jump probabilities at [−2, 2].9 Second, we use transcriptions to learn alignments rather than the objects detected in videos. We hypothesize that the richness of language used in instructional videos may facilitate better alignment with transcripts (as others have observed (Malmaud et al., 2015; Sener et al., 2015)). We use all words (except stop words) in video transcript sentences and all words in text instructions while learning the IBM1 word level probabilities. An instruction in one recipe can be aligned to multiple instructions in the other recipe. 3.2 Joint Alignment among Multiple Recipes We use the pairwise alignments to derive a joint alignment at the dish level between multiple text and video recipes. For each dish, we construct a graph where each node represents an instruction from a text recipe or a transcript sentence from a video recipe. We use the pairwise alignments"
2020.acl-main.440,J05-4003,0,0.106277,"e knowledge, we describe our approach for constructing the M ICROSOFT R ESEARCH M ULTI MODAL A LIGNED R ECIPE C ORPUS . We first extract a large number of text and video recipes from the web. Our goal is to find joint alignments between multiple text recipes and multiple video recipes for the same dish (see Figure 2). The task is challenging, as different recipes vary in their order of instructions and use of ingredients. Moreover, video instructions can be noisy, and text and video instructions include different levels of specificity in their descriptions. Most previous alignment approaches (Munteanu and Marcu, 2005) deal with pairwise alignments. Since our goal is to align multiple instruction sets, we introduce a novel twostage unsupervised algorithm. In the first stage, we learn pairwise alignments between two text recipes, two video recipes, and between a text and a video recipe using an unsupervised alignment algorithm (§ 3.1). In the second stage, we use the pairwise alignments between all recipes within a dish to construct a graph for each dish and find a maximum spanning tree of this graph to derive joint alignments across multiple recipes (§3.2). We train our unsupervised algorithm on 4,262 dishe"
2020.acl-main.440,D14-1162,0,0.0873914,"on, we compute its similarity score with every target instruction and align it to the target instruction with the highest score. a. Exact word match: Given two instructions, we define exact word match as the ratio of the number of common words between the two divided by the number of words in the longer of the two. This gives us a measure of word match that is comparable across instructions of different lengths. Methods Random Uniform alignment BM25 retrieval Textual Similarity Exact word match TF-IDF GloVe BERT RoBERTa HMM+IBM1 Nouns Nouns+Verbs All words c. GloVe: We train GloVe embeddings (Pennington et al., 2014) on an in-domain corpus of 3 million words put together by combining text recipes and video transcriptions. Given an instruction, we average the GloVe embeddings (Pennington Recall 14.00 31.85 55.27 F1 12.69 33.22 49.30 53.90 52.78 56.04 50.72 52.49 48.39 46.82 51.89 55.07 55.86 46.98 45.12 50.30 49.10 50.44 62.11 64.72 66.21 48.99 50.76 52.42 50.73 52.97 54.55 Table 3: Results for text-text recipe alignment on Common Crawl dataset. et al., 2014) of nouns and verbs12 to obtain its embedding vector. Given two instructions, we define their embedding similarity as the cosine similarity of their e"
2020.acl-main.440,D19-1410,0,0.0133423,"nstruction, we average the GloVe embeddings (Pennington Recall 14.00 31.85 55.27 F1 12.69 33.22 49.30 53.90 52.78 56.04 50.72 52.49 48.39 46.82 51.89 55.07 55.86 46.98 45.12 50.30 49.10 50.44 62.11 64.72 66.21 48.99 50.76 52.42 50.73 52.97 54.55 Table 3: Results for text-text recipe alignment on Common Crawl dataset. et al., 2014) of nouns and verbs12 to obtain its embedding vector. Given two instructions, we define their embedding similarity as the cosine similarity of their embedding vectors. d. BERT: Given an instruction, we compute its embedding vector using BERT-based sentence embedding (Reimers and Gurevych, 2019). We experiment with different variants and find that the BERT-base model trained on AllNLI, then on STS benchmark training set13 performed the best for us. Given two instructions, we define their BERT similarity as the cosine similarity between their sentence embedding vectors. e. RoBERTa: We also experiment with a variant of the above baseline where we use RoBERTa (Liu et al., 2019) instead of BERT to compute the sentence embeddings. We use RoBERTa-large trained on AllNLI, then on STS benchmark training set. 4.3 b. TF-IDF: We use all the recipes in our training set to create a term frequency"
2020.acl-main.440,D19-1517,0,0.0287429,"ch larger scale dataset. Multi-modal Instructional Datasets Marin et al. (2019) introduce a corpus of 1 million cooking recipes paired with 13 million food images for the task of retrieving a recipe given an image. YouCook2 dataset (Zhou et al., 2018a) consists of 2,000 recipe videos with human written descriptions for each video segment. The How2 dataset (Sanabria et al., 2018) consists of 79,114 instructional videos with English subtitles and crowdsourced Portuguese translations. The COIN dataset (Tang et al., 2019) consists of 11,827 videos of 180 tasks in 12 daily life domains. YouMakeup (Wang et al., 2019) consists of 2,800 YouTube videos, annotated with natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. Leveraging Document Level Alignments Our work relies on the assumption that text recipes and instructional cooking videos of the same dish are comparable. This idea has been used to extract parallel sentences from comparable corpora to increase the number of training examples for machine translation (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010; Gr´egoire and Langlais, 2018). Likewise, TalkSumm (Lev et"
2020.acl-main.440,N10-1063,0,0.0144346,"27 videos of 180 tasks in 12 daily life domains. YouMakeup (Wang et al., 2019) consists of 2,800 YouTube videos, annotated with natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. Leveraging Document Level Alignments Our work relies on the assumption that text recipes and instructional cooking videos of the same dish are comparable. This idea has been used to extract parallel sentences from comparable corpora to increase the number of training examples for machine translation (Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010; Gr´egoire and Langlais, 2018). Likewise, TalkSumm (Lev et al., 2019) use the transcripts of scientific conference talks to automatically extract summaries. Zhu et al. (2015) use books and movie adaptations of the books to extract descriptive explanations of movie scenes. Related Tasks A related task is localizing and classifying steps in instructional videos (Alayrac et al., 2016; Zhukov et al., 2019) where they detect when an action is performed in the video whereas we focus on describing actions. Dense event captioning of instructional videos (Zhou et al., 2018b; Li et al., 2018; Hessel et"
2020.acl-main.440,C96-2141,0,0.21939,"-fry for another minute. Add onion, garlic, peas, and carrots. You can use peas or whatever else you want to add in there. Figure 4: A maximum span tree for fried rice dish with text instructions and transcript segments as nodes, alignments as edges, and alignment probabilities as edge weights. Nodes representing text instructions are labeled “T”. Nodes representing transcript segments are labeled “V”. Each color indicates a different recipe. The bounding box shows a magnified section of the tree with edge weights and the instruction/transcript associated with each node. (HMM) (Rabiner, 1989; Vogel et al., 1996) to generate each video segment f (m) from one of the text sentences e(n) . They then use IBM1 model (Brown et al., 1993) emission probabilities to gen(m) (m) erate the blobs {f1 , ..., fJ } in f (m) from the (n) (n) nouns {e1 , ..., eI } in e(n) as follows: P (f (m) |e (n) J J  Y X (m) (n) p(fj |ei ) )= (I)J (1) j=1 i=1 The hidden state in the HMM model corresponds to the alignment between video segment and text sentence, and the state transition probabilities correspond to the jump between adjacent alignments. For computational tractability, a video segment can be aligned to only one senten"
2020.emnlp-main.349,K19-1079,0,0.115309,"Missing"
2020.emnlp-main.349,P19-1200,1,0.889615,"GE scores were obtained by producing text that is more repetitive and generic.10 In contrast, P LOT M ACHINES generally achieves good performance on both ROUGE and diversity scores, with self-BLEU scores that are lower than most other models. Notably, they generally have more similar self-BLEU scores to the actual gold stories, indicating that the language diversity is more similar to what humans write. Automatic Metrics In this section, we evaluate performance using different automatic metrics. We compute ROUGE scores (Lin, 2004) and self-BLEU (Zhu et al., 2018) following from previous work (Shen et al., 2019; Zhu et al., 2018) showing that a large ROUGE score together with a low self-BLEU score can demonstrate a model’s ability to generate realisticlooking as well as diverse generations. Coverage We compute ROUGE scores (Lin, 2004) with respect to the gold stories (Table 2). Results show that the full P LOT M ACHINES achieves comparable or higher ROUGE on all three datasets. Both P LOT M ACHINES variants (using GPT or GPT-2 as a base) achieve improvements over G ROVER, even though G ROVER includes significantly more parameters than the model using GPT. Ablations In the bottom block of Table 2, we"
2020.emnlp-main.349,D16-1032,1,\N,Missing
2020.emnlp-main.349,P18-1082,0,\N,Missing
2020.emnlp-main.349,N18-1156,0,\N,Missing
2020.emnlp-main.349,D18-1462,0,\N,Missing
2020.emnlp-main.349,W13-4069,0,\N,Missing
2020.emnlp-main.349,W18-1505,0,\N,Missing
2020.emnlp-main.526,N18-1016,1,0.818692,"pt, with outputs for a single step from other models for comparison. Our model replaces the violating ingredient (in red) with a substitution (in green), as well as modifying or adding new ingredients and techniques in every step (underlined). Recipe generation: Recipe generation has been a research focus for decades, using methods ranging from rule-based planning systems (Hammond, 1986) to more recent neural network models that use targeted information such as entity types (Parvez et al., 2018), cooking actions (Bosselut et al., 2017), ingredients (Kiddon et al., 2016), or order information (Bosselut et al., 2018) to guide the generations. Building on the insight that knowledge about ingredients improves recipe generation, our work uses ingredient prompts to guide the generation of each recipe step. While there has been extensive work on recipe generation, few studies focus on controlled recipe generation. Majumder et al. (2019) recently introduced the task of personalized recipe generation, producing customized recipes based on user preferences. To our knowledge, our work is the first to generate recipes that conform to a given dietary constraint. 7 Conclusion We introduce the novel task of document-l"
2020.emnlp-main.526,D19-1588,0,0.0178528,"nce we do not restrict the words that the model can alter. Document-level controlled generation: The difficulty of text attribute transfer is amplified when the task expands to the document level. While controlled generation models such as Grover (Zellers et al., 2019), PPLM (Dathathri et al., 2020), and CTRL (Keskar et al., 2019) have been successful at the document level, they do not make use of context beyond an initial prompt. For many NLP tasks, contextual information from surrounding sentences can improve the quality of a generated sentence. We have seen this for coreference resolution (Joshi et al., 2019), relation extraction (Tang et al., 2020), and machine translation (Werlen et al., 2018; Mac´e and Servan, 2019). In this work, we show the effectiveness of including document-level context when rewriting recipes to fit a dietary constraint. 21 Appendix shows a breakdown of each model’s accuracy by dietary restriction. 6492 Source recipe Egg Muffins Contextual Rewriter + Rule Prompt Vegetarian Egg Muffins Crack egg into a bowl and break it up with a fork or small whisk. Separate 2 of the eggs. Use a fork to crack the two eggs in. Spray ramekin or muffin cup with oil, coating the cup well. Grea"
2020.emnlp-main.526,D16-1032,0,0.0304182,"itten by the Contextual Rewriter + Rule Prompt, with outputs for a single step from other models for comparison. Our model replaces the violating ingredient (in red) with a substitution (in green), as well as modifying or adding new ingredients and techniques in every step (underlined). Recipe generation: Recipe generation has been a research focus for decades, using methods ranging from rule-based planning systems (Hammond, 1986) to more recent neural network models that use targeted information such as entity types (Parvez et al., 2018), cooking actions (Bosselut et al., 2017), ingredients (Kiddon et al., 2016), or order information (Bosselut et al., 2018) to guide the generations. Building on the insight that knowledge about ingredients improves recipe generation, our work uses ingredient prompts to guide the generation of each recipe step. While there has been extensive work on recipe generation, few studies focus on controlled recipe generation. Majumder et al. (2019) recently introduced the task of personalized recipe generation, producing customized recipes based on user preferences. To our knowledge, our work is the first to generate recipes that conform to a given dietary constraint. 7 Conclu"
2020.emnlp-main.526,N18-1169,0,0.155701,"ed content transfer, defined as rewriting a document to obey a user-provided constraint resulting in some systematic alteration of the document’s content. Success at this task involves both transfer and controlled generation at the document level. Prior work on controlled generation guides the output of a model using attribute classifiers (Dathathri et al., 2020) or control codes (Keskar et al., 2019), but we find that these models do not perform well on our transfer task (§4.1.2). In contrast, models built for the transfer task are generally trained at the sentence level (Hu et al., 2017b,a; Li et al., 2018; Rao and Tetreault, 2018; Syed et al., 2019). Document-level transfer has typically found success by rewriting each sentence independently (Maruf et al., 2019). However, many real-world rewriting scenarios require interdependent changes across multiple sentences. A clear example is cooking, where rewriting a hot cocoa recipe to make it vegan requires more than just substituting “coconut milk” for “milk” in a single step—it may also require changing the cooking times and techniques, adjusting ingredient amounts, or replacing other ingredients like toppings or spices (Figure 1). Such 6485 Proce"
2020.emnlp-tutorials.7,K16-1002,0,0.255067,"ext generation: language modeling and the encoder-decoder frameworks. We will also discuss the limitations of the simple encoder-decoder frameworks and motivate the rest of the tutorial. 2. Building blocks of Neural Network Models for Language Generation (60 minutes long): This section will comprise three closely related topics corresponding to three fundamental aspects of building a neural language generation system: (1) selecting the architecture of the model among a variety of choices such as pre-trained language models (Devlin et al., 2018; Radford et al., 2019), variational autoencoders (Bowman et al., 2016; Hu et al., 2017), generative adversarial networks (Fedus et al., 2018; Subramanian et al., 2018), or neural template based methods (Wiseman et al., 2018; Xu et al., 2018); (2) training the model using techniques which can range from simple maximum likelihood estimate up to more advanced training techniques like scheduled 5. Benchmarks and Evaluation (30 minutes long): Given the diversity of text generation tasks and domains, it can be challenging to design reliable benchmarks and evaluation metrics (Lowe et al., 2017; Reiter, 2018; Clark et al., 2019; See et al., 2019). In this section, we w"
2020.emnlp-tutorials.7,P19-1264,1,0.829141,"et al., 2019), variational autoencoders (Bowman et al., 2016; Hu et al., 2017), generative adversarial networks (Fedus et al., 2018; Subramanian et al., 2018), or neural template based methods (Wiseman et al., 2018; Xu et al., 2018); (2) training the model using techniques which can range from simple maximum likelihood estimate up to more advanced training techniques like scheduled 5. Benchmarks and Evaluation (30 minutes long): Given the diversity of text generation tasks and domains, it can be challenging to design reliable benchmarks and evaluation metrics (Lowe et al., 2017; Reiter, 2018; Clark et al., 2019; See et al., 2019). In this section, we will summarize the current status on these topics. 6. Building Neural Models for Generation (20 minutes long): This section will provide hand-on exercise, using existing deep learning packages, to build a neural language generation model. This section will also demonstrates how different learning/decoding strategies can have a strong impact on the quality of generated texts. 39 7. Open problems and directions (10 minutes long): In this final section, we will summarize the topics covered in the tutorial and point to a selection of open problems and futur"
2020.emnlp-tutorials.7,N18-1204,1,0.790428,"Missing"
2020.emnlp-tutorials.7,P18-1165,0,0.0386284,"Missing"
2020.emnlp-tutorials.7,N19-1015,0,0.0190964,"re also from different countries and continents (the Netherlands and USA). 4 Outline 4.1 Schedule 3. Break (20 minutes) The tutorial will be 3 hours long. 4. Generation with Rich Context (25 minutes long): This section will discuss recent works on incorporating various types of context information in neural language generation. Going beyond simple context information provided by single sentence contexts, we will overview the growing body of work exploring various strategies to incorporate different types of context information either textual, e.g., syntactic, topic, and discourse information (Wang et al., 2019; Shen et al., 2019; Clark et al., 2018; Bosselut et al., 2018), or beyond text, including knowledge graph, database and images (Parthasarathi and Pineau, 2018; Dinan et al., 2018). 1. Introduction of Natural Language Generation (15 minutes long): This section will introduce the tutorial by presenting the recent impact of neural network modeling approaches on the field. We will briefly overview the classical text generation pipeline, and introduce basic building blocks of neural text generation: language modeling and the encoder-decoder frameworks. We will also discuss the limitations of the s"
2020.emnlp-tutorials.7,D17-1230,0,0.0292861,"n generated outputs, an uncertain reliance on provided factual information, and more general open questions on architecture design and optimization settings. In this tutorial, we will start with an introduction to neural language generation, presenting neural language models and encoder-decoder models. We will then discuss the capabilities and limitations of recent text generation models, the suitable architectures for text generation in various specific applications, and then provide insights into why and how these generation models can be adapted for a particular task (Wiseman et al., 2017; Li et al., 2017; See et al., 2017; Xie, 2017). The discussion on evaluation metrics will start from ngram matching up to the recent progress on text generation evaluation metrics. In the end, this tutorial will be concluded by presenting and discussing major current research directions in the field of neural language generation. All materials (including slides, code, and demos) will be publicly available online on the day of the tutorial. We do not assume any particular prior knowlNeural Language Generation (NLG) – using neural network models to generate coherent text – is among the most promising methods fo"
2020.emnlp-tutorials.7,P17-1103,0,0.0116951,"ls (Devlin et al., 2018; Radford et al., 2019), variational autoencoders (Bowman et al., 2016; Hu et al., 2017), generative adversarial networks (Fedus et al., 2018; Subramanian et al., 2018), or neural template based methods (Wiseman et al., 2018; Xu et al., 2018); (2) training the model using techniques which can range from simple maximum likelihood estimate up to more advanced training techniques like scheduled 5. Benchmarks and Evaluation (30 minutes long): Given the diversity of text generation tasks and domains, it can be challenging to design reliable benchmarks and evaluation metrics (Lowe et al., 2017; Reiter, 2018; Clark et al., 2019; See et al., 2019). In this section, we will summarize the current status on these topics. 6. Building Neural Models for Generation (20 minutes long): This section will provide hand-on exercise, using existing deep learning packages, to build a neural language generation model. This section will also demonstrates how different learning/decoding strategies can have a strong impact on the quality of generated texts. 39 7. Open problems and directions (10 minutes long): In this final section, we will summarize the topics covered in the tutorial and point to a se"
2020.emnlp-tutorials.7,D18-1073,0,0.0269025,"rs long. 4. Generation with Rich Context (25 minutes long): This section will discuss recent works on incorporating various types of context information in neural language generation. Going beyond simple context information provided by single sentence contexts, we will overview the growing body of work exploring various strategies to incorporate different types of context information either textual, e.g., syntactic, topic, and discourse information (Wang et al., 2019; Shen et al., 2019; Clark et al., 2018; Bosselut et al., 2018), or beyond text, including knowledge graph, database and images (Parthasarathi and Pineau, 2018; Dinan et al., 2018). 1. Introduction of Natural Language Generation (15 minutes long): This section will introduce the tutorial by presenting the recent impact of neural network modeling approaches on the field. We will briefly overview the classical text generation pipeline, and introduce basic building blocks of neural text generation: language modeling and the encoder-decoder frameworks. We will also discuss the limitations of the simple encoder-decoder frameworks and motivate the rest of the tutorial. 2. Building blocks of Neural Network Models for Language Generation (60 minutes long):"
2020.emnlp-tutorials.7,D17-1239,0,0.0408475,"Missing"
2020.emnlp-tutorials.7,D18-1356,0,0.0154992,"otivate the rest of the tutorial. 2. Building blocks of Neural Network Models for Language Generation (60 minutes long): This section will comprise three closely related topics corresponding to three fundamental aspects of building a neural language generation system: (1) selecting the architecture of the model among a variety of choices such as pre-trained language models (Devlin et al., 2018; Radford et al., 2019), variational autoencoders (Bowman et al., 2016; Hu et al., 2017), generative adversarial networks (Fedus et al., 2018; Subramanian et al., 2018), or neural template based methods (Wiseman et al., 2018; Xu et al., 2018); (2) training the model using techniques which can range from simple maximum likelihood estimate up to more advanced training techniques like scheduled 5. Benchmarks and Evaluation (30 minutes long): Given the diversity of text generation tasks and domains, it can be challenging to design reliable benchmarks and evaluation metrics (Lowe et al., 2017; Reiter, 2018; Clark et al., 2019; See et al., 2019). In this section, we will summarize the current status on these topics. 6. Building Neural Models for Generation (20 minutes long): This section will provide hand-on exercise,"
2020.emnlp-tutorials.7,D18-1462,0,0.0216314,"e tutorial. 2. Building blocks of Neural Network Models for Language Generation (60 minutes long): This section will comprise three closely related topics corresponding to three fundamental aspects of building a neural language generation system: (1) selecting the architecture of the model among a variety of choices such as pre-trained language models (Devlin et al., 2018; Radford et al., 2019), variational autoencoders (Bowman et al., 2016; Hu et al., 2017), generative adversarial networks (Fedus et al., 2018; Subramanian et al., 2018), or neural template based methods (Wiseman et al., 2018; Xu et al., 2018); (2) training the model using techniques which can range from simple maximum likelihood estimate up to more advanced training techniques like scheduled 5. Benchmarks and Evaluation (30 minutes long): Given the diversity of text generation tasks and domains, it can be challenging to design reliable benchmarks and evaluation metrics (Lowe et al., 2017; Reiter, 2018; Clark et al., 2019; See et al., 2019). In this section, we will summarize the current status on these topics. 6. Building Neural Models for Generation (20 minutes long): This section will provide hand-on exercise, using existing dee"
2020.emnlp-tutorials.7,J18-3002,0,0.0137733,"2018; Radford et al., 2019), variational autoencoders (Bowman et al., 2016; Hu et al., 2017), generative adversarial networks (Fedus et al., 2018; Subramanian et al., 2018), or neural template based methods (Wiseman et al., 2018; Xu et al., 2018); (2) training the model using techniques which can range from simple maximum likelihood estimate up to more advanced training techniques like scheduled 5. Benchmarks and Evaluation (30 minutes long): Given the diversity of text generation tasks and domains, it can be challenging to design reliable benchmarks and evaluation metrics (Lowe et al., 2017; Reiter, 2018; Clark et al., 2019; See et al., 2019). In this section, we will summarize the current status on these topics. 6. Building Neural Models for Generation (20 minutes long): This section will provide hand-on exercise, using existing deep learning packages, to build a neural language generation model. This section will also demonstrates how different learning/decoding strategies can have a strong impact on the quality of generated texts. 39 7. Open problems and directions (10 minutes long): In this final section, we will summarize the topics covered in the tutorial and point to a selection of ope"
2020.emnlp-tutorials.7,P17-1099,0,0.0502144,"ts, an uncertain reliance on provided factual information, and more general open questions on architecture design and optimization settings. In this tutorial, we will start with an introduction to neural language generation, presenting neural language models and encoder-decoder models. We will then discuss the capabilities and limitations of recent text generation models, the suitable architectures for text generation in various specific applications, and then provide insights into why and how these generation models can be adapted for a particular task (Wiseman et al., 2017; Li et al., 2017; See et al., 2017; Xie, 2017). The discussion on evaluation metrics will start from ngram matching up to the recent progress on text generation evaluation metrics. In the end, this tutorial will be concluded by presenting and discussing major current research directions in the field of neural language generation. All materials (including slides, code, and demos) will be publicly available online on the day of the tutorial. We do not assume any particular prior knowlNeural Language Generation (NLG) – using neural network models to generate coherent text – is among the most promising methods for automated text c"
2020.emnlp-tutorials.7,N19-1170,0,0.0265524,"tional autoencoders (Bowman et al., 2016; Hu et al., 2017), generative adversarial networks (Fedus et al., 2018; Subramanian et al., 2018), or neural template based methods (Wiseman et al., 2018; Xu et al., 2018); (2) training the model using techniques which can range from simple maximum likelihood estimate up to more advanced training techniques like scheduled 5. Benchmarks and Evaluation (30 minutes long): Given the diversity of text generation tasks and domains, it can be challenging to design reliable benchmarks and evaluation metrics (Lowe et al., 2017; Reiter, 2018; Clark et al., 2019; See et al., 2019). In this section, we will summarize the current status on these topics. 6. Building Neural Models for Generation (20 minutes long): This section will provide hand-on exercise, using existing deep learning packages, to build a neural language generation model. This section will also demonstrates how different learning/decoding strategies can have a strong impact on the quality of generated texts. 39 7. Open problems and directions (10 minutes long): In this final section, we will summarize the topics covered in the tutorial and point to a selection of open problems and future research directio"
2020.emnlp-tutorials.7,P19-1200,1,0.828036,"ent countries and continents (the Netherlands and USA). 4 Outline 4.1 Schedule 3. Break (20 minutes) The tutorial will be 3 hours long. 4. Generation with Rich Context (25 minutes long): This section will discuss recent works on incorporating various types of context information in neural language generation. Going beyond simple context information provided by single sentence contexts, we will overview the growing body of work exploring various strategies to incorporate different types of context information either textual, e.g., syntactic, topic, and discourse information (Wang et al., 2019; Shen et al., 2019; Clark et al., 2018; Bosselut et al., 2018), or beyond text, including knowledge graph, database and images (Parthasarathi and Pineau, 2018; Dinan et al., 2018). 1. Introduction of Natural Language Generation (15 minutes long): This section will introduce the tutorial by presenting the recent impact of neural network modeling approaches on the field. We will briefly overview the classical text generation pipeline, and introduce basic building blocks of neural text generation: language modeling and the encoder-decoder frameworks. We will also discuss the limitations of the simple encoder-decod"
2020.eval4nlp-1.8,W01-0813,0,0.267375,"Missing"
2020.eval4nlp-1.8,P16-1046,0,0.0312008,"ed in detail in their paper. For additional agreement evaluation, we had 10 documents evaluated by two sets of judges. The first set of judges was comprised of 4 developers involved in the design of A RTEMIS and its guide• Lead-3 baseline selects first three sentences of a document as the summary. • Oracle scores are obtained using a jackknifed procedure. Reference summary from each judge is considered a predicted summary and evaluated against all the other reference summaries for the document. The Oracle ROUGE score is computed by averaging the scores for all judge summaries. • Cheng&Lapata (Cheng and Lapata, 2016) is an encoder-decoder summarization model where each sentence is first encoded using a CNN (Convolutional Neural Network). These sentence level encodings are then passed 75 Method Lead-3 Cheng & Lapata SummaRunner Seq2SeqRNN Oracle Rouge-1 44.94 60.21 63.56 63.89 73.28 Rouge-2 34.37 49.81 53.57 54.22 66.60 Rouge-L 43.39 58.62 61.89 62.36 72.20 is consistent with the results reported in Kedzie et al. (2018). There is still a gap between these systems and the Oracle method, which achieves a ROUGE-1 score of 73.28. 6 Table 6: Results for different baselines on the test data. Concluding Remarks I"
2020.eval4nlp-1.8,N18-2097,0,0.0190764,"sentences at different summary levels could be used to train a pair-wise sentence ranking system such as LambdaMart (Burges, 2010) or come up with more refined evaluation metrics. 3.3 3.4 Recent Trends in Summarization Evaluation Recent summarization evaluations are done using large scale datasets collected automatically from the web. Most of these datasets are from the news domain, including CNN/DailyMail (Nallapati et al., 2016), N EWSROOM (Grusky et al., 2018), New York Times (Sandhaus, 2008) and Gigaword (Rush et al., 2015). Some of the other domains investigated are scientific articles (Cohan et al., 2018), patents (Sharma et al., 2019), and Reddit stories (Kim et al., 2019). Datasets built from naturally occurring summaries found online tend to focus on domains for which manually written summaries are easily available such as news and scientific articles. These datasets are not sufficient for building a multidomain document summarization application. Additionally, given the nature of data collection, often only a single summary is available for each document. This makes error analysis of individual examples difficult because different judges might deem different information as summary-worthy ("
2020.eval4nlp-1.8,D18-1208,0,0.0551745,"Missing"
2020.eval4nlp-1.8,N19-1260,0,0.0142398,"e sentence ranking system such as LambdaMart (Burges, 2010) or come up with more refined evaluation metrics. 3.3 3.4 Recent Trends in Summarization Evaluation Recent summarization evaluations are done using large scale datasets collected automatically from the web. Most of these datasets are from the news domain, including CNN/DailyMail (Nallapati et al., 2016), N EWSROOM (Grusky et al., 2018), New York Times (Sandhaus, 2008) and Gigaword (Rush et al., 2015). Some of the other domains investigated are scientific articles (Cohan et al., 2018), patents (Sharma et al., 2019), and Reddit stories (Kim et al., 2019). Datasets built from naturally occurring summaries found online tend to focus on domains for which manually written summaries are easily available such as news and scientific articles. These datasets are not sufficient for building a multidomain document summarization application. Additionally, given the nature of data collection, often only a single summary is available for each document. This makes error analysis of individual examples difficult because different judges might deem different information as summary-worthy (Louis and Nenkova, 2013) as discussed in Section 3.3. A RTEMIS provide"
2020.eval4nlp-1.8,D19-1051,0,0.030897,"Missing"
2020.eval4nlp-1.8,N18-1065,0,0.0439423,"Missing"
2020.eval4nlp-1.8,W04-1013,0,0.0825943,"summarization. DUC 2001-2004 focused on single and multi-document summarization (Dang, 2005). In DUC evaluation for summary content, first a single human judge creates a model summary for each document. The model summary is split automatically into content units. For evaluating a system generated summary, a human judge compares the sentences in the system summary with model content units and estimates the fact overlap. The use of a single model summary in DUC evaluations raised concerns in the research community and led to the proposal of Pyramid evaluation, which we describe in Section 3.3. Lin (2004a) concluded that given enough samples, the use of single model summaries was valid, but using multiple model summaries increased correlation with human judgments. In later years, DUC experimented with ROUGE (Lin, 2004b), an automatic metric for summary evaluation that uses n-gram co-occurrence statistics for scoring system generated summaries against the model summaries. ROUGE is the standard automatic evaluation method used in recent summarization evaluations, which we describe in Section 3.4. In A RTEMIS, the sentences selected by judges for document or short-level summary can be used as mo"
2020.eval4nlp-1.8,P19-1330,0,0.0371171,"Missing"
2020.eval4nlp-1.8,J13-2002,0,0.0271004,", patents (Sharma et al., 2019), and Reddit stories (Kim et al., 2019). Datasets built from naturally occurring summaries found online tend to focus on domains for which manually written summaries are easily available such as news and scientific articles. These datasets are not sufficient for building a multidomain document summarization application. Additionally, given the nature of data collection, often only a single summary is available for each document. This makes error analysis of individual examples difficult because different judges might deem different information as summary-worthy (Louis and Nenkova, 2013) as discussed in Section 3.3. A RTEMIS provides a methodology for obtaining Pyramid evaluation Nenkova and Passonneau (2004) introduced Pyramid method as a more reliable method for summary evaluation by incorporating the idea that no single best model summary exists. Given a set of humangenerated model summaries for a document, the Pyramid method starts by manually identifying Summary Content Units (SCUs) in the model summaries. A SCU represents a single unit of information (e.g. “Two men were indicted”) which can have different surface realizations in different summaries (e.g. “Court indicted"
2020.eval4nlp-1.8,W97-0704,0,0.196992,"Missing"
2020.eval4nlp-1.8,kan-etal-2002-using,0,0.205744,"ges don’t need to look at all the sentences in a document when making an importance judgment for one of the sentences, while providing similarly rich sentence importance annotations. We describe the annotation process in detail and compare it with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents. 1 Introduction Given an input source document, summarization systems produce a condensed summary which can be either informative or indicative. Informative summaries try to convey all the important points of the document (Kan et al., 2002, 2001b), while indicative summaries hint at the topics of the document, pointing to information alerting the reader about the document content (Saggion and Lapalme, 2002). An informative summary aims to replace the source document, so that the user does not need to read the full document (Edmundson, 1969). An indicative summary, on the other hand, aims to † ‡ Figure 1: One of the documents from our web-crawled sample annotated dataset along with indicative summaries annotated by three different judges. The sentence numbers in round brackets are not in the original document but are added here"
2020.eval4nlp-1.8,K16-1028,0,0.0612257,"Missing"
2020.eval4nlp-1.8,P11-5003,0,0.107739,"Missing"
2020.eval4nlp-1.8,N04-1019,0,0.609359,"MIS obtains similar judgments, but with a light-weight process where judges don’t need to look at the entire input document when annotating a sentence. Following this, we discuss DUC evaluations, ROUGE and the Pyramid method. Finally, we discuss some of the recent trends in summarization evaluation. 72 3.2 DUC evaluations and ROUGE ing in five model summaries has a higher weight than an SCU appearing in three model summaries. Given the SCU inventory over all model summaries, the Pyramid score of a system generated summary is obtained based on the number and weights of the SCUs in the summary. Nenkova and Passonneau (2004) observe that the number of SCUs grows as the number of model summaries increases, confirming a similar observation by van Halteren and Teufel (2003), supporting the claim that different judges deem different facts as important. Finding SCUs in model summaries and then matching them to system summaries is an expensive semantic judgment task. Once created, the SCU inventory can be used to assign an importance weight to any sentence in a system generated extractive summary based on the weights of SCUs in it. Our methodology provides a cheaper method for assigning importance weight for each sente"
2020.eval4nlp-1.8,D15-1044,0,0.0357927,"es for ROUGE evaluation, as we demonstrate in Section 5. In addition, the labels for sentences at different summary levels could be used to train a pair-wise sentence ranking system such as LambdaMart (Burges, 2010) or come up with more refined evaluation metrics. 3.3 3.4 Recent Trends in Summarization Evaluation Recent summarization evaluations are done using large scale datasets collected automatically from the web. Most of these datasets are from the news domain, including CNN/DailyMail (Nallapati et al., 2016), N EWSROOM (Grusky et al., 2018), New York Times (Sandhaus, 2008) and Gigaword (Rush et al., 2015). Some of the other domains investigated are scientific articles (Cohan et al., 2018), patents (Sharma et al., 2019), and Reddit stories (Kim et al., 2019). Datasets built from naturally occurring summaries found online tend to focus on domains for which manually written summaries are easily available such as news and scientific articles. These datasets are not sufficient for building a multidomain document summarization application. Additionally, given the nature of data collection, often only a single summary is available for each document. This makes error analysis of individual examples di"
2020.eval4nlp-1.8,J02-4005,0,0.385889,"portance annotations. We describe the annotation process in detail and compare it with other similar evaluation systems. We also present analysis and experimental results over a sample set of 532 annotated documents. 1 Introduction Given an input source document, summarization systems produce a condensed summary which can be either informative or indicative. Informative summaries try to convey all the important points of the document (Kan et al., 2002, 2001b), while indicative summaries hint at the topics of the document, pointing to information alerting the reader about the document content (Saggion and Lapalme, 2002). An informative summary aims to replace the source document, so that the user does not need to read the full document (Edmundson, 1969). An indicative summary, on the other hand, aims to † ‡ Figure 1: One of the documents from our web-crawled sample annotated dataset along with indicative summaries annotated by three different judges. The sentence numbers in round brackets are not in the original document but are added here for readability. Summary sentences are truncated for readability as well. Work done while an intern at Microsoft. Work done while an employee of Microsoft. 69 Proceedings"
2020.eval4nlp-1.8,P19-1212,0,0.0255934,"y levels could be used to train a pair-wise sentence ranking system such as LambdaMart (Burges, 2010) or come up with more refined evaluation metrics. 3.3 3.4 Recent Trends in Summarization Evaluation Recent summarization evaluations are done using large scale datasets collected automatically from the web. Most of these datasets are from the news domain, including CNN/DailyMail (Nallapati et al., 2016), N EWSROOM (Grusky et al., 2018), New York Times (Sandhaus, 2008) and Gigaword (Rush et al., 2015). Some of the other domains investigated are scientific articles (Cohan et al., 2018), patents (Sharma et al., 2019), and Reddit stories (Kim et al., 2019). Datasets built from naturally occurring summaries found online tend to focus on domains for which manually written summaries are easily available such as news and scientific articles. These datasets are not sufficient for building a multidomain document summarization application. Additionally, given the nature of data collection, often only a single summary is available for each document. This makes error analysis of individual examples difficult because different judges might deem different information as summary-worthy (Louis and Nenkova, 2013) as dis"
2020.eval4nlp-1.8,W97-0703,0,\N,Missing
2020.eval4nlp-1.8,W03-0508,0,\N,Missing
2020.eval4nlp-1.8,W01-0100,0,\N,Missing
2020.findings-emnlp.157,2020.emnlp-main.703,1,0.827082,"MM agent recursively models conversations with instances of itself to choose the right questions to ask (and answers to give) to reach the goal. Introduction A key challenge for embodied language is moving beyond instruction following to instruction generation, which can require understanding the listener. The turn-based dialogue paradigm raises a myriad of new research questions, from grounded versions of traditional problems like co-reference resolution (Das et al., 2017a) to explicitly modeling theory of mind in order to consider the listener’s ability to understand generated instructions (Bisk et al., 2020). In this paper, we develop end-to-end dialogue agents to navigate photorealistic, indoor scenes to reach goal rooms. We train agents using the human-human Collaborative Vision-andDialogue Navigation (CVDN) (Thomason et al., 2019) dataset. CVDN dialogues are turn-based, with a navigator following guide instructions and asking questions when needed. Modeling turn-based dialogues involves four core challenges: C1 A navigator deciding when to ask a question. C2 Generating navigator questions. C3 Generating guide question answers. C4 Generating navigator actions. Prior work has addressed individua"
2020.findings-emnlp.157,P82-1020,0,0.80111,"Missing"
2020.findings-emnlp.157,2020.acl-main.232,0,0.312223,"or generating an answer to estimate their effects on navigation. Viewed as a single system, the agents cooperatively search through the space of dialogues to efficiently perform embodied navigation. 2 Related Work and Background We build on research in multimodal navigation and the wider literature involving goal oriented dialogue. Table 1 summarizes how our work differs from existing work in vision-and-language navigation and task-oriented dialogue modelling. Instruction Following tasks an embodied agent with interpreting natural language instructions and visual observations to reach a goal (Jayannavar et al., 2020; Wang et al., 2019; Ma et al., 2019; Anderson et al., 2018; Chen and Mooney, 2011). These instructions describe step-by-step actions the agent needs to take, and can involve the creation of speaker models for data augmentation that provide additional instructions (Fried et al., 2018). This paradigm has been extended to longer trajectories and outdoor environments (Chen et al., 2019), as well as to agents in the real world (Chai et al., 2018; Tellex et al., 2014). In this work, we focus on the the simulated, photorealistic indoor environments of the MatterPort dataset (Chang et al., 2017), and"
2020.findings-emnlp.157,P17-1163,0,0.0602341,"Missing"
2020.findings-emnlp.157,P19-1537,0,0.0224005,"d et al., 2018). This paradigm has been extended to longer trajectories and outdoor environments (Chen et al., 2019), as well as to agents in the real world (Chai et al., 2018; Tellex et al., 2014). In this work, we focus on the the simulated, photorealistic indoor environments of the MatterPort dataset (Chang et al., 2017), and go beyond instruction following alone to a twoagent dialogue setting. Navigation Dialogues task a navigator and a guide to cooperate to find a destination. Previous work includes substantial information asymmetry between the navigator and guide (de Vries et al., 2018; Narayan-Chen et al., 2019). Information asymmetry can take the form of the navigator seeing a bird’s eye, abstract semantic map while the guide sees egocentric simulation frames (de Vries et al., 2018), affecting the kind of dialog possible when low-level visual cues cannot be grounded by the navigator. Other work only investigates the navigation portion of the dialogue without considering text question generation and answering (Thomason et al., 2019). Going beyond models that perform navigation from dialogue history alone (Wang et al., 2020; Zhu et al., 2020; Hao et al., 2020), or decide when to ask navigator question"
2020.findings-emnlp.157,D19-1063,0,0.0523249,"Missing"
2020.findings-emnlp.157,P02-1040,0,0.107036,"tion with respect to the goal location. Dialogue navigation proceeds by iterating through the three roles until either the N avigator loc = p0 ; hist = t0 ; ~a ∼ N (hist); loc, hist = update(~a, loc, hist); while ~a 6= STOP and len(hist) &lt; 20 do q ∼ Q(hist, loc) ; // Question ~s = path(loc, goal, horizon = 5) ; o ∼ O(hist, loc, q, ~s) ; // Answer hist ← hist + (q, o); for a ∈ N (hist) do loc ← loc + a ; // Move hist ← hist + a; end end return (goal − t0 ) − (loc − t0 ) chooses to stop or a maximum number of turns are played (Algorithm 1). In addition to “Goal Progress”, we report BLEU scores (Papineni et al., 2002) for evaluating the generation of questions and answers by comparing against human questions and answers. Note, in our dialogue setting, Goal Progress also implicitly measures the utility of generated language and is therefore complementary to BLEU when evaluating utility versus fluency. 4 Models We introduce the Recursive Mental Model (RMM) as an initial approach to our new full dialogue CVDN task formulation. Key to this approach is allowing component models (N avigator, Questioner, and Guide) to learn from each other and roll out possible dialogues and trajectories. We compare our model to"
2020.findings-emnlp.157,D17-1237,1,0.854087,", 2017; Chai et al., 2018). Modeling goal-oriented dialogue requires skills that go beyond language modeling, such as asking questions to clearly define a user request, querying knowledge bases, and interpreting results from queries as options to complete a transaction. Many recent task oriented systems are data-driven and trained end-to-end using semisupervised or transfer learning methods (Ham et al., 2020; Mrksic et al., 2017). However, these datadriven approaches may lack grounding between the text and the environment state. Reinforcement learning-based dialogue modeling (Su et al., 2016; Peng et al., 2017; Liu et al., 2017) can improve completion rate and user experience by helping ground conversational data to environments. 1733 3 Task and Data Algorithm 1: Dialogue Navigation Our work creates a two-agent dialogue task, building on the CVDN dataset (Thomason et al., 2019) of human-human dialogues. In that dataset, a human N avigator and Guide collaborate to find a goal room containing a target object. The N avigator moves through the environment, and the Guide views this navigation until the N avigator asks a question in natural language (C1, C2). Then, the Guide can see the next few steps a"
2020.findings-emnlp.157,P16-1230,0,0.0323414,"Bordes and Weston, 2017; Chai et al., 2018). Modeling goal-oriented dialogue requires skills that go beyond language modeling, such as asking questions to clearly define a user request, querying knowledge bases, and interpreting results from queries as options to complete a transaction. Many recent task oriented systems are data-driven and trained end-to-end using semisupervised or transfer learning methods (Ham et al., 2020; Mrksic et al., 2017). However, these datadriven approaches may lack grounding between the text and the environment state. Reinforcement learning-based dialogue modeling (Su et al., 2016; Peng et al., 2017; Liu et al., 2017) can improve completion rate and user experience by helping ground conversational data to environments. 1733 3 Task and Data Algorithm 1: Dialogue Navigation Our work creates a two-agent dialogue task, building on the CVDN dataset (Thomason et al., 2019) of human-human dialogues. In that dataset, a human N avigator and Guide collaborate to find a goal room containing a target object. The N avigator moves through the environment, and the Guide views this navigation until the N avigator asks a question in natural language (C1, C2). Then, the Guide can see th"
2021.acl-long.537,P18-1063,1,0.834288,"; then the PageRank algorithm is applied to obtain the rank 5 We also tested some other heuristics: e.g., the first sentence of the last email, the last 3-5 sentences of the email thread, etc. However, none of them perform better than Lead1-Email. scores for each sentence, and top-rank sentences are selected as the summary. BertSumExt. Liu and Lapata (2019b) propose to build a sentence extractor upon BERT (Devlin et al., 2019) to perform extractive summarization, which achieves a good performance on CNN/DM. 3.2 Abstractive Fast Abs RL. As the simple non-pretrained abstractive baseline, we use Chen and Bansal (2018), which is a hybrid model that first extracts sentences from the source document, then rewrites the extracted sentences by an abstractive rewriter. They pair summary sentences with the extracted sentences to train the abstractive rewriter. Adapting their model to our email thread summarization task, we make two adjustments: (1) We extract emails instead of sentences, which is a natural unit for email thread; (2) Since summary sentences usually follow the temporal order of the emails, we enhance this pairing procedure by using the Neeleman-Wunsch algorithm (Needleman and Wunsch, 1970; Rameshkum"
2021.acl-long.537,2021.ccl-1.108,0,0.0342117,"Missing"
2021.acl-long.537,2020.acl-main.173,0,0.0179186,"ely. for the model is to identify the roles of different speakers and their relations, i.e., who does what to whom. As shown in the second example of Table 6, the model wrongly takes “2 fixes in 382 are in the patch installer” as information provided by Nilesh, whereas it is supposed to be by Diana. The same issue can also be observed in the first example: Om is just summarizing what Nihar said instead of telling Nihar. This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020). 6902 5.3 Correlation with Human Judgement ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al., 2019) is essentially based on “soft” uni-gram matching. However, according to our analysis presented above, the email thread summarization models mainly fail to be abstractive, salient, and faithful, which are hard to be evaluated by n-gram overlap. Furthermore, as pointed out by Bhandari et al. (2020), different datasets usually require different evaluation metrics. Therefore, here, we study the correlation between automatic metrics and human judgments. Specifically, we evaluate"
2021.acl-long.537,2020.acl-main.450,0,0.0235029,"ort and E MAIL S UMlong tasks, respectively. for the model is to identify the roles of different speakers and their relations, i.e., who does what to whom. As shown in the second example of Table 6, the model wrongly takes “2 fixes in 382 are in the patch installer” as information provided by Nilesh, whereas it is supposed to be by Diana. The same issue can also be observed in the first example: Om is just summarizing what Nihar said instead of telling Nihar. This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020). 6902 5.3 Correlation with Human Judgement ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al., 2019) is essentially based on “soft” uni-gram matching. However, according to our analysis presented above, the email thread summarization models mainly fail to be abstractive, salient, and faithful, which are hard to be evaluated by n-gram overlap. Furthermore, as pointed out by Bhandari et al. (2020), different datasets usually require different evaluation metrics. Therefore, here, we study the correlation between automatic metrics and"
2021.acl-long.537,W04-3252,0,0.0233489,"old summary. “Ext-Oracle-R1” in Table 2 is computed from an oracle summary that maximizes ROUGE-1 (Lin, 2004). Lead. This model simply picks the first sentence from the source document as the summary, which has surprisingly good performance on CNN/DM dataset (Narayan et al., 2018). We test two variants by selecting: (1) the first sentence of the email thread, which is usually the subject (see the example in Table 1), referred as Lead-1; (2) the first sentence of the email thread (the subject) plus the first sentences of every email, named Lead-1-Email.5 TextRank. This is a graph-based method (Mihalcea and Tarau, 2004). It first builds a graph between sentences by their embedding similarities; then the PageRank algorithm is applied to obtain the rank 5 We also tested some other heuristics: e.g., the first sentence of the last email, the last 3-5 sentences of the email thread, etc. However, none of them perform better than Lead1-Email. scores for each sentence, and top-rank sentences are selected as the summary. BertSumExt. Liu and Lapata (2019b) propose to build a sentence extractor upon BERT (Devlin et al., 2019) to perform extractive summarization, which achieves a good performance on CNN/DM. 3.2 Abstract"
2021.acl-long.537,2020.acl-main.459,0,0.294098,"rayan et al., 2018). However, living in an information era, we are facing with diverse content 1 Our code and summary data have been made available at: https://github.com/ZhangShiyue/EmailSum in different structures. The summarization need is varied along with different application scenarios. Recently, there is an increasing research interest in diverse summarization tasks (Gao et al., 2020), e.g., timeline (Allan et al., 2001), query-based (Li and Li, 2014), multi-modal (Zhu et al., 2018), meeting (Carletta et al., 2006), dialogue or discussion thread (Misra et al., 2015; Gliwa et al., 2019; Rameshkumar and Bailey, 2020), etc. Following the branch of dialogue or thread summarization, we introduce a new abstractive Email Thread Summarization (E MAIL S UM) dataset. Email threads are widely used at work. An email thread is a special type of dialogue that usually has a specific structure (sender, receiver, greeting line, main body, and the signature), contains technical information, and involves multiple speakers. Unlike a conversational dialog turn, an email in a 6895 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan"
2021.acl-long.537,2020.findings-emnlp.19,0,0.0342911,"T5) to generate summaries for unlabelled threads, then mix the model-labeled and human-labeled data to finetune T5 again, referred as SemiSupx (x stands for the unlabelled data source we use, i.e., W3C, Avocado, or together). Hierarchical T5. Hierarchical summarization models have been shown to improve the performance of multi-document summarization task (Liu and Lapata, 2019a). Although an email thread can be treated as a single document due to the temporal dependency between consecutive emails, it also has a clear turn structure that encourages using of the hierarchical encoders. Recently, Zhu et al. (2020) proposed a hierarchical model (HMNet) for meeting summarization. Inspired by their work, we propose a hierarchical model that is similar to HMNet in structure but uses T5 as the backbone, therefore, it can take advantage of both the hierarchical structure and the pre-trained knowledge. As shown in Figure 1, this model contains two encoders: the token-level encodes the whole email Experiments Evaluation Metrics ROUGE (Lin, 2004) is a commonly used automatic metric for summarization tasks. It has several variants: (1) ROUGE-1 (R1) measures the unigram overlap between the generated and reference"
2021.eacl-main.2,N18-1150,1,0.915286,"Missing"
2021.eacl-main.2,P17-1123,0,0.0469643,"Missing"
2021.eacl-main.2,W09-0611,0,0.0245224,"Missing"
2021.eacl-main.2,D17-1090,0,0.0624025,"Missing"
2021.eacl-main.2,C18-1150,0,0.0181187,"learning sentence representations (Clark et al., 2020). To our best knowledge, we are the first to leverage contrastive learning and establish set-induced penalization in the context of question generation. Question Generation: Most prior work on question generation has been on single document i.e. given a document and an answer phrase in the document, generate a question that is answered by the answer phrase (Heilman, 2011; Rus et al., 2010). For a survey, see Pan et al. (2019). However, in our work, we aim to generate a multi-document question that is answerable by multiple input documents. Fan et al. (2018) propose a visual question generation model to generate natural questions about images using reinforcement learning where they use naturalness and human-like as reward signals. In our work, we use retrieval statistics, similar to Nogueira and Cho (2017), derived from a document-question ranker as the reward for training our coordinator model in isolation, rather than the entire generating pipeline. 5 Conclusion We proposed a novel coordinator model that can generate questions that are more grounded on documents of interest. This coordinator model consists of transformer blocks, and is trained"
2021.eacl-main.2,D16-1026,0,0.0531351,"Missing"
2021.eacl-main.2,D13-1176,0,0.090163,"Missing"
2021.eacl-main.2,W18-6326,0,0.055818,"Missing"
2021.eacl-main.2,D15-1166,0,0.0545656,"Missing"
2021.eacl-main.2,D17-1259,0,0.023444,"p. This task is particularly challenging because i) there does not exist direct supervised ground-truth multi-document question given positive and negative sets of documents. ii) The whole procedure involves multiple aspects including language understanding, inter-document information aggregation, coordinative planning and language generation. In theory, the generator can be trained to maximize the chance that the generated question specifically retrieves the given document cluster, using RL. However, the space of possible sequence is prohibitively large which results in large variance in RL (Lewis et al., 2017). To effectively reduce the search space of RL, we employ a hybrid supervised learning (SL) and RL strategy. We also propose a novel reward-shaping auxiliary objecDocument-specific Generator: At the first pre-training stage, we load the publicly available GPT-2 (Radford et al., 2019) model as our underlying document-specific generator. The GPT-2 model leverages massive out-domain data and serves as a good initialization to generate grammatical and 13 Training an Inter-generator Coordinator via RL: Next, we train a coordinator system using policy gradient to optimize the reward described above."
2021.eacl-main.2,D18-1478,0,0.042988,"Missing"
2021.eacl-main.2,I17-1047,1,0.884813,"Missing"
2021.eacl-main.2,P17-2031,0,0.0694702,"Missing"
2021.eacl-main.2,D14-1162,0,0.0839586,"Missing"
2021.eacl-main.2,N18-1202,0,0.0455274,"Missing"
2021.eacl-main.2,2020.findings-emnlp.3,0,0.0200936,", model-generated questions from this method are rather generic and not specific to the input documents. Right: contrastive modeling, which considers both positive and negative document sets, and learns to generate questions that are more grounded on the positive document set. guity by suggesting clarification options back to the user in the form of questions (Braslavski et al., 2017; Aliannejadi et al., 2019; Zamani et al., 2020). However, asking the right clarification questions is a challenging information-seeking task, given a plethora of possible questions (Rao and Daumé III, 2018, 2019; Qi et al., 2020). One workaround is to take informational cues from the search engine results given the initial query. The clarification options are then generated from non-ranked and non-overlapping thematic partitions of the search engine results. The whole pipeline is akin to the pseudo-relevance feedback (Rocchio, 1971; Cao et al., 2008). This can significantly reduce the search space, and has the potential to generate correct clarification questions within the context (Cho Introduction User queries on web search engines can sometimes be vague. Search engines may resolve this ambi† Work done when the auth"
2021.eacl-main.2,W00-1009,0,0.444303,"Missing"
2021.eacl-main.2,P19-1220,0,0.0396745,"Missing"
2021.eacl-main.2,radev-etal-2002-evaluating,0,0.178572,"larity; VE for Vector Extrema similarity; and GM for Greedy Matching. consider the top-10 documents retrieved for q 0 as our negative set D− . In total, we gather 100K train/10K dev/10K eval data points. Details of the pre-processing, usage of additional annotations from the secondary dataset, and experimental configuration are in Appendix. negative set representational information. T 1 Xh X t t LH (θ) = wi,θ log wi,θ T t=1 i∈D+ i X t t log vi,θ + vi,θ (15) i∈D− Automatic evaluation: The generated questions are evaluated through standard retrieval-based metrics: MRR and MRR10 (Voorhees, 1999; Radev et al., 2002), nDCG (Järvelin and Kekäläinen, 2002), precision, mAP. These metrics are computed from the 10 positive and 10 negative document sets (=: Out-Sample IR). In addition, as a standardized evaluation routine in the MS-MARCO Retrieval task, for each generated question, we use Lucene‡ to retrieve the most relevant 100 MARCO documents via BM25 (Robertson and Zaragoza, 2009), and use the retrieved document set and a trained model to rank (document, generated question) pairs, thus compute the retrieval statistics (=: Search-Engine Augmented IR). The generated questions are also evaluated in We finally"
2021.eacl-main.2,W18-2711,0,0.41782,"Missing"
2021.eacl-main.2,D17-1061,0,0.0265605,"question generation has been on single document i.e. given a document and an answer phrase in the document, generate a question that is answered by the answer phrase (Heilman, 2011; Rus et al., 2010). For a survey, see Pan et al. (2019). However, in our work, we aim to generate a multi-document question that is answerable by multiple input documents. Fan et al. (2018) propose a visual question generation model to generate natural questions about images using reinforcement learning where they use naturalness and human-like as reward signals. In our work, we use retrieval statistics, similar to Nogueira and Cho (2017), derived from a document-question ranker as the reward for training our coordinator model in isolation, rather than the entire generating pipeline. 5 Conclusion We proposed a novel coordinator model that can generate questions that are more grounded on documents of interest. This coordinator model consists of transformer blocks, and is trained through reinforcement learning and an effective auxiliary: Set-induced Contrastive Regularization. The rewards are derived from a publicly available state-of-the-art pre-trained ranker (Section 2) to compute retrieval statistics among D+ and D− . Our no"
2021.eacl-main.2,2001.mtsummit-papers.46,0,0.618375,"Missing"
2021.eacl-main.2,W18-6545,0,0.0389249,"Missing"
2021.eacl-main.2,2020.acl-demos.30,1,0.722314,"ion generation: generate a clarification question by finding an “overlap” among documents in each cluster. In principle, the clarification questions should be specific to each cluster rather than generic and bland, otherwise it is counter to the objective of clarification (Radlinski and Craswell, 2017). In this work, we focus on developing a multi-document question generator to generate cluster-specific questions in the iii) step. Nevertheless, we believe our approach can be readily applied to multi-document text generation such as summarization (Liu and Lapata, 2019) and response generation (Zhang et al., 2020). We address this challenge by leveraging contrastive learning. Given a set of positive documents D+ and a set of negative documents D− (where D− is yet semantically close to D+ ), we propose a new strategy to generate a question that is semantically relevant to D+ and far away from D− . Ideally, the model would use both D+ and D− to identify distinguishing features between the two sets and constrain the generation to be specific to D+ . The similarity between the D+ and D− makes the generation more challenging and forces the model to be as specific as possible in order to distinguish between"
2021.eacl-main.2,D08-1079,0,0.132409,"Missing"
2021.eacl-main.2,P18-1178,0,0.0433462,"Missing"
2021.eacl-main.22,P15-2136,0,0.0226159,"layers. We further conduct document-level encoding on the sentence-level representations from the [CLS] tokens, denoted as Esi , as well as their positional embeddings, Ei0 , with another stack of transformer layers. We add a document embedding ED before the sequence of sentence embeddings to represent the whole document. The final representation of D and each sentence si can be obtained from the output of the multiple transformer layers, denoted as hsi and hD . 4.2 Most recent redundancy-aware extractive summarization systems use heuristics to select diverse sentences after salience scoring (Cao et al., 2015; Ren 1 In § 4 and experiments, we use ROUGE to define M (·) 283 Document Encoder AR ED S UM -S EQ: Sequence Generation Our first model, AR ED S UM -S EQ, strictly considers the order of the target selected sentences while jointly modeling the redundancy and salience of the next sentence. It uses a transformer decoder Figure 1: Overview of the proposed models AR ED S UM -S EQ and AR ED S UM -C TX sharing the same B ERT-based encoder from B ERT S UM E XT. module (Vaswani et al., 2017) to learn to select and order a sequence of sentences from the document as a summary. Our model is different fro"
2021.eacl-main.22,P16-1046,0,0.460416,"(Liu and Lapata, 2019; Zhang et al., 2019; Zhou et al., 2018). Given a partial summary, the decision to include another sentence in the summary depends on two aspects: salience, which represents how much information the sentence carries; and redundancy, which represents how much information in the sentence is already included in the previously selected sentences. Although there have been a few studies on redundancy a long time ago, most recent research on extractive summarization focuses on salience alone. They usually model sentence salience as a sequence labeling task (Kedzie et al., 2018; Cheng and Lapata, 2016) or classification task (Zhang et al., 2019) and do not conduct redundancy removal. Previous methods that consider redundancy usually use a separate step after salience scoring to handle redundancy, denoted as sentence selection (Carbonell and Goldstein, 1998; McDonald, 2007; Lin and Bilmes, 2011). Sentence selection often follows a greedy iterative ranking process that outputs one sentence at a time by taking into account the redundancy of candidate sentences with previously selected sentences. Several approaches for modeling redundancy in sentence selection have been explored: heuristicsbase"
2021.eacl-main.22,D18-1409,0,0.0347385,"Missing"
2021.eacl-main.22,P16-1188,0,0.0267625,"Lapata (2019). Entities are not anonymized in our experiments as in Zhou et al. (2018); See et al. (2017); Zhang et al. (2019); Liu and Lapata (2019). We truncate articles up to 512 tokens. To collect sentence labels for extractive summarization, we use a greedy strategy similar to (Nallapati et al., 2017; Zhang et al., 2019). We label the subset of sentences that can maximize ROUGE scores against the human-generated summary as 1 (sentence to be included in the summary). The remaining ones are labeled as 0. NYT50 is an annotated corpus of the New York Times. Following Paulus et al. (2017) and Durrett et al. (2016), we discard marks and words such as “(s)” and “photo” at the end of the abstract and filter out the articles with summaries shorter than 50. We sort the articles chronologically and split the data into training/validation/test sets according to the ratio of 0.8/0.1/0.1, yielding 133,602/16,700/16,700 documents, respectively. We following the same remaining steps for preprocessing and extractive label collection as the CNN/DailyMail. 5.2 Implementation Details Our implementation 3 is based on PyTorch and B ERT S UM(Liu and Lapata, 2019) 4 . We use “bertbase-uncased” version of B ERT5 to do sen"
2021.eacl-main.22,D18-1208,0,0.288014,"ntences in a document (Liu and Lapata, 2019; Zhang et al., 2019; Zhou et al., 2018). Given a partial summary, the decision to include another sentence in the summary depends on two aspects: salience, which represents how much information the sentence carries; and redundancy, which represents how much information in the sentence is already included in the previously selected sentences. Although there have been a few studies on redundancy a long time ago, most recent research on extractive summarization focuses on salience alone. They usually model sentence salience as a sequence labeling task (Kedzie et al., 2018; Cheng and Lapata, 2016) or classification task (Zhang et al., 2019) and do not conduct redundancy removal. Previous methods that consider redundancy usually use a separate step after salience scoring to handle redundancy, denoted as sentence selection (Carbonell and Goldstein, 1998; McDonald, 2007; Lin and Bilmes, 2011). Sentence selection often follows a greedy iterative ranking process that outputs one sentence at a time by taking into account the redundancy of candidate sentences with previously selected sentences. Several approaches for modeling redundancy in sentence selection have been"
2021.eacl-main.22,P11-1052,0,0.084923,"sentence is already included in the previously selected sentences. Although there have been a few studies on redundancy a long time ago, most recent research on extractive summarization focuses on salience alone. They usually model sentence salience as a sequence labeling task (Kedzie et al., 2018; Cheng and Lapata, 2016) or classification task (Zhang et al., 2019) and do not conduct redundancy removal. Previous methods that consider redundancy usually use a separate step after salience scoring to handle redundancy, denoted as sentence selection (Carbonell and Goldstein, 1998; McDonald, 2007; Lin and Bilmes, 2011). Sentence selection often follows a greedy iterative ranking process that outputs one sentence at a time by taking into account the redundancy of candidate sentences with previously selected sentences. Several approaches for modeling redundancy in sentence selection have been explored: heuristicsbased methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998), Trigram Blocking (T RI B LK) (Liu and Lapata, 2019), or model based approaches (Ren et al., 2016), etc. Heuristic-based methods are not adaptive since they usually apply the same rule to all the documents. Model-ba"
2021.eacl-main.22,P09-5005,0,0.0832065,"Missing"
2021.eacl-main.22,D19-1387,0,0.0822417,"ing sentence selection; and a two-step AR ED S UM -C TX that scores salience first, then learns to balance salience and redundancy, enabling the measurement of the impact of each aspect. Empirical results on CNN/DailyMail and NYT50 datasets show that by modeling diversity explicitly in a separate step, AR ED S UM -C TX achieves significantly better performance than AR ED S UM -S EQ as well as state-of-the-art extractive summarization baselines. 1 Introduction Extractive summarization is the task of creating a summary by identifying and concatenating the most important sentences in a document (Liu and Lapata, 2019; Zhang et al., 2019; Zhou et al., 2018). Given a partial summary, the decision to include another sentence in the summary depends on two aspects: salience, which represents how much information the sentence carries; and redundancy, which represents how much information in the sentence is already included in the previously selected sentences. Although there have been a few studies on redundancy a long time ago, most recent research on extractive summarization focuses on salience alone. They usually model sentence salience as a sequence labeling task (Kedzie et al., 2018; Cheng and Lapata, 2016"
2021.eacl-main.22,W04-3252,0,0.0443113,"2018; Dong et al., 2018). More recently, summarization methods based on B ERT (Devlin et al., 2018) have been shown to achieve state-of-the-art performance (Liu and Lapata, 2019; Zhang et al., 2019; Zhong et al., 2019; Zhou et al., 2020) on salience for extractive summarization. Related Work Extractive summarization methods are usually decomposed into two subtasks, i.e., sentence scoring and sentence selection, which deal with salience and redundancy, respectively. Salience Scoring. Graph-based models are widely used methods to score sentence salience in summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan and Yang, 2006). There are also extensions to such methods, e.g., with clus282 Sentence Selection. There are relatively fewer methods that study sentence selection to avoid redundancy. Integer Linear Programming based methods (McDonald, 2007) formulate sentence selection as an optimizing problem under the summary length constraint. Lin and Bilmes (2011) propose to find the optimal subset of sentences with submodular functions. Greedy strategies such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) select the sentence that has maximal salience score and is minimally red"
2021.eacl-main.22,N18-1158,0,0.0206078,"sborne, 2002), conditional random fields (Galley, 2006) or hidden markov model (Conroy et al., 2004). Human engineered features are heavily used in these methods such as word frequency and sentence length (Nenkova et al., 2006). In recent years, neural models have replaced older models to score the salience of sentences. Hierarchical LSTMs and CNNs have replaced manually engineered features. LSTM decoders are employed to do sequence labeling (Cheng and Lapata, 2016; Nallapati et al., 2017; Kedzie et al., 2018). These architectures are widely used and also extended with reinforcement learning (Narayan et al., 2018; Dong et al., 2018). More recently, summarization methods based on B ERT (Devlin et al., 2018) have been shown to achieve state-of-the-art performance (Liu and Lapata, 2019; Zhang et al., 2019; Zhong et al., 2019; Zhou et al., 2020) on salience for extractive summarization. Related Work Extractive summarization methods are usually decomposed into two subtasks, i.e., sentence scoring and sentence selection, which deal with salience and redundancy, respectively. Salience Scoring. Graph-based models are widely used methods to score sentence salience in summarization (Erkan and Radev, 2004; Mihal"
2021.eacl-main.22,W06-1643,0,0.0880424,"tive summarization extending B ERTS UM E XT; 2) we conduct comparative studies between our redundancy-aware models as well as the heuristic-based method that B ERT S UM E XT uses; 3) our proposed AR ED S UM -C TX significantly outperforms B ERT S UM E XT and other competitive baselines on CNN/DailyMail and NYT50. 2 tering (Wan and Yang, 2008) or leveraging graph neural networks (Wang et al., 2020). Classical supervised extractive summarization uses classification or sequence labeling methods such as Naive Bayes (Kupiec et al., 1999), maximum entropy (Osborne, 2002), conditional random fields (Galley, 2006) or hidden markov model (Conroy et al., 2004). Human engineered features are heavily used in these methods such as word frequency and sentence length (Nenkova et al., 2006). In recent years, neural models have replaced older models to score the salience of sentences. Hierarchical LSTMs and CNNs have replaced manually engineered features. LSTM decoders are employed to do sequence labeling (Cheng and Lapata, 2016; Nallapati et al., 2017; Kedzie et al., 2018). These architectures are widely used and also extended with reinforcement learning (Narayan et al., 2018; Dong et al., 2018). More recently"
2021.eacl-main.22,W02-0401,0,0.299281,"-aware iterative ranking methods for extractive summarization extending B ERTS UM E XT; 2) we conduct comparative studies between our redundancy-aware models as well as the heuristic-based method that B ERT S UM E XT uses; 3) our proposed AR ED S UM -C TX significantly outperforms B ERT S UM E XT and other competitive baselines on CNN/DailyMail and NYT50. 2 tering (Wan and Yang, 2008) or leveraging graph neural networks (Wang et al., 2020). Classical supervised extractive summarization uses classification or sequence labeling methods such as Naive Bayes (Kupiec et al., 1999), maximum entropy (Osborne, 2002), conditional random fields (Galley, 2006) or hidden markov model (Conroy et al., 2004). Human engineered features are heavily used in these methods such as word frequency and sentence length (Nenkova et al., 2006). In recent years, neural models have replaced older models to score the salience of sentences. Hierarchical LSTMs and CNNs have replaced manually engineered features. LSTM decoders are employed to do sequence labeling (Cheng and Lapata, 2016; Nallapati et al., 2017; Kedzie et al., 2018). These architectures are widely used and also extended with reinforcement learning (Narayan et al"
2021.eacl-main.22,C16-1004,0,0.0685668,"ence scoring to handle redundancy, denoted as sentence selection (Carbonell and Goldstein, 1998; McDonald, 2007; Lin and Bilmes, 2011). Sentence selection often follows a greedy iterative ranking process that outputs one sentence at a time by taking into account the redundancy of candidate sentences with previously selected sentences. Several approaches for modeling redundancy in sentence selection have been explored: heuristicsbased methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998), Trigram Blocking (T RI B LK) (Liu and Lapata, 2019), or model based approaches (Ren et al., 2016), etc. Heuristic-based methods are not adaptive since they usually apply the same rule to all the documents. Model-based approaches depend heavily on feature engineering and learn to score sentences via regression with point-wise loss, which has been shown to be inferior to pairwise loss or list-wise loss in ranking problems (Liu et al., 2009). Redundancy has also been handled jointly with salience during the scoring process using neural sequence models (Zhou et al., 2018). N EU S UM (Zhou et al., 2018) scores sentences considering their salience as well as previous sentences in the output seq"
2021.eacl-main.22,P17-1099,0,0.039639,"es as context. 5 Experimental Setup 5.1 Datasets We evaluate our model on two standard extractive summarization datasets, namely CNN/DailyMail (Hermann et al., 2015) and NewYork Times (NYT) (Sandhaus, 2008). CNN/DailyMail contains news articles associated with a few bullet points as the article’s highlight. We use the standard splits of Hermann et al. (2015) which has 287,226 documents for training, 13,368 for validation, and 11,490 for testing. We conduct preprocessing following the same method in Liu and Lapata (2019). Entities are not anonymized in our experiments as in Zhou et al. (2018); See et al. (2017); Zhang et al. (2019); Liu and Lapata (2019). We truncate articles up to 512 tokens. To collect sentence labels for extractive summarization, we use a greedy strategy similar to (Nallapati et al., 2017; Zhang et al., 2019). We label the subset of sentences that can maximize ROUGE scores against the human-generated summary as 1 (sentence to be included in the summary). The remaining ones are labeled as 0. NYT50 is an annotated corpus of the New York Times. Following Paulus et al. (2017) and Durrett et al. (2016), we discard marks and words such as “(s)” and “photo” at the end of the abstract an"
2021.eacl-main.22,N06-2046,0,0.110066,"More recently, summarization methods based on B ERT (Devlin et al., 2018) have been shown to achieve state-of-the-art performance (Liu and Lapata, 2019; Zhang et al., 2019; Zhong et al., 2019; Zhou et al., 2020) on salience for extractive summarization. Related Work Extractive summarization methods are usually decomposed into two subtasks, i.e., sentence scoring and sentence selection, which deal with salience and redundancy, respectively. Salience Scoring. Graph-based models are widely used methods to score sentence salience in summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan and Yang, 2006). There are also extensions to such methods, e.g., with clus282 Sentence Selection. There are relatively fewer methods that study sentence selection to avoid redundancy. Integer Linear Programming based methods (McDonald, 2007) formulate sentence selection as an optimizing problem under the summary length constraint. Lin and Bilmes (2011) propose to find the optimal subset of sentences with submodular functions. Greedy strategies such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) select the sentence that has maximal salience score and is minimally redundant iteratively. T"
2021.eacl-main.22,P19-1499,0,0.304378,"; and a two-step AR ED S UM -C TX that scores salience first, then learns to balance salience and redundancy, enabling the measurement of the impact of each aspect. Empirical results on CNN/DailyMail and NYT50 datasets show that by modeling diversity explicitly in a separate step, AR ED S UM -C TX achieves significantly better performance than AR ED S UM -S EQ as well as state-of-the-art extractive summarization baselines. 1 Introduction Extractive summarization is the task of creating a summary by identifying and concatenating the most important sentences in a document (Liu and Lapata, 2019; Zhang et al., 2019; Zhou et al., 2018). Given a partial summary, the decision to include another sentence in the summary depends on two aspects: salience, which represents how much information the sentence carries; and redundancy, which represents how much information in the sentence is already included in the previously selected sentences. Although there have been a few studies on redundancy a long time ago, most recent research on extractive summarization focuses on salience alone. They usually model sentence salience as a sequence labeling task (Kedzie et al., 2018; Cheng and Lapata, 2016) or classification"
2021.eacl-main.22,P19-1100,0,0.0117149,"t al., 2006). In recent years, neural models have replaced older models to score the salience of sentences. Hierarchical LSTMs and CNNs have replaced manually engineered features. LSTM decoders are employed to do sequence labeling (Cheng and Lapata, 2016; Nallapati et al., 2017; Kedzie et al., 2018). These architectures are widely used and also extended with reinforcement learning (Narayan et al., 2018; Dong et al., 2018). More recently, summarization methods based on B ERT (Devlin et al., 2018) have been shown to achieve state-of-the-art performance (Liu and Lapata, 2019; Zhang et al., 2019; Zhong et al., 2019; Zhou et al., 2020) on salience for extractive summarization. Related Work Extractive summarization methods are usually decomposed into two subtasks, i.e., sentence scoring and sentence selection, which deal with salience and redundancy, respectively. Salience Scoring. Graph-based models are widely used methods to score sentence salience in summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan and Yang, 2006). There are also extensions to such methods, e.g., with clus282 Sentence Selection. There are relatively fewer methods that study sentence selection to avoid redundancy. Int"
2021.eacl-main.22,2020.coling-main.492,0,0.013237,"ent years, neural models have replaced older models to score the salience of sentences. Hierarchical LSTMs and CNNs have replaced manually engineered features. LSTM decoders are employed to do sequence labeling (Cheng and Lapata, 2016; Nallapati et al., 2017; Kedzie et al., 2018). These architectures are widely used and also extended with reinforcement learning (Narayan et al., 2018; Dong et al., 2018). More recently, summarization methods based on B ERT (Devlin et al., 2018) have been shown to achieve state-of-the-art performance (Liu and Lapata, 2019; Zhang et al., 2019; Zhong et al., 2019; Zhou et al., 2020) on salience for extractive summarization. Related Work Extractive summarization methods are usually decomposed into two subtasks, i.e., sentence scoring and sentence selection, which deal with salience and redundancy, respectively. Salience Scoring. Graph-based models are widely used methods to score sentence salience in summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan and Yang, 2006). There are also extensions to such methods, e.g., with clus282 Sentence Selection. There are relatively fewer methods that study sentence selection to avoid redundancy. Integer Linear Programm"
2021.eacl-main.22,P18-1061,0,0.181805,"ED S UM -C TX that scores salience first, then learns to balance salience and redundancy, enabling the measurement of the impact of each aspect. Empirical results on CNN/DailyMail and NYT50 datasets show that by modeling diversity explicitly in a separate step, AR ED S UM -C TX achieves significantly better performance than AR ED S UM -S EQ as well as state-of-the-art extractive summarization baselines. 1 Introduction Extractive summarization is the task of creating a summary by identifying and concatenating the most important sentences in a document (Liu and Lapata, 2019; Zhang et al., 2019; Zhou et al., 2018). Given a partial summary, the decision to include another sentence in the summary depends on two aspects: salience, which represents how much information the sentence carries; and redundancy, which represents how much information in the sentence is already included in the previously selected sentences. Although there have been a few studies on redundancy a long time ago, most recent research on extractive summarization focuses on salience alone. They usually model sentence salience as a sequence labeling task (Kedzie et al., 2018; Cheng and Lapata, 2016) or classification task (Zhang et al.,"
2021.eacl-main.34,N18-1150,1,0.836267,"oposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelwal et al., 2019; Zhang et al., 2019). While our model builds upon this work, it is, to our knowledge, the first transformer summarization framework to explicitly model narrative flow and scientific fact-checking across domains. Related Work 9 Narrative Flow and Factuality Modeling coherent narrative flow remains a major challenge in the field of text generation, due to the need for accurate understanding of narrative structure (ChrisConclusion In th"
2021.eacl-main.34,N13-1136,0,0.0815339,"Missing"
2021.eacl-main.34,P19-1264,1,0.898612,"Missing"
2021.eacl-main.34,D19-1383,0,0.153017,"Embed ... ... S1 S2 S3 . . . Si S1 Summarization Model Probabilities p1, p2, ... , pn Source Document Figure 2: Model architecture for adjacency reranking variation of Co-opNet discriminator models that encourage different communicative norms associated with high-quality language generation. 3.1 Discourse We explore different discriminator architectures as additional discourse scoring functions during the generator’s decoding process. For these discriminators, we generally score discourse in two ways. First, we use inferred sentence-level scien1 tific abstract discourse role labels defined by Cohan et al. (2019) and predict them using a sequence 2 classifier based on SciBERT (Beltagy et al., 2019). Using these predictions, we score the discourse properties of the abstract relative to their coverage (§3.1.1) or ordering (§3.1.2). Second, we learn a function that can score the likelihood that sentences within generated abstracts should be adjacent to one another (§3.1.3). 3.1.1 Coverage We measure the completeness of the narrative structure within a scientific abstract by defining the following coverage score: Lcov = log(Dabs /Dall ), Si−1 Si BACKGROUND BACKGROUND ∨ METHOD ∨ OBJECTIVE BACKGROUND ∨ OBJE"
2021.eacl-main.34,N18-2097,0,0.0183801,"ies are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelw"
2021.eacl-main.34,N19-1423,0,0.0225493,"hlights from the context? ArXiv We crawled over 700K samples (472K abstracts) from scientific articles on arxiv.org. 5 In our experiments we primarily focus on the CS 6 and Bio domain subsets. The task we define is to generate an abstract given a introduction, which presents a challenge to existing summarization models. This task also requires models to learn relevant domain knowledge for the scientific domain of interest and recognize common discourse structure for papers written in that domain. 6 Experimental Setup Our implementation is based on the Huggingface 8 implementation of the BERT (Devlin et al., 2019) and GPT-2 language models (Radford et al., 2019). Generator We perform WordPiece tokenization for the input context and output summaries. Because of the fixed input size of the transformer language model, the input context is truncated to a maximum of 800 tokens, and summaries are truncated to a maximum of 200 tokens. We use a learning rate of 2e-5 and a batch size of 16 to finetune the generator. We train the base summarization transformer model for 12 epochs. All experiments are run on either a Titan-X or Quadro RTX 8000 GPU. Training time for the AAN and ArXiv Bio datasets is about 30 minu"
2021.eacl-main.34,P18-1082,0,0.022587,"ummaries. Because of the fixed input size of the transformer language model, the input context is truncated to a maximum of 800 tokens, and summaries are truncated to a maximum of 200 tokens. We use a learning rate of 2e-5 and a batch size of 16 to finetune the generator. We train the base summarization transformer model for 12 epochs. All experiments are run on either a Titan-X or Quadro RTX 8000 GPU. Training time for the AAN and ArXiv Bio datasets is about 30 minutes per epoch. Training time for the ArXiv CS dataset is 2.5 hours per epoch. In our experiments we use top-k sampling with k=4 (Fan et al., 2018) to generate candidate summaries for each model. AAN Additionally, we include an existing dataset of scientific articles that focuses on papers in the NLP computer science domain. This dataset consists of a 12k paper subset from the ACL Anthology Network (AAN; Radev et al., 2009) with extracted introduction and abstract pairs. Discriminator At training time we use a maximum sentence length of 200 tokens to accommodate the fixed input size of BERT (512 tokens), reduce inference time, and discourage the model from generating abnormally long run-on sentences 9 that indicate the presence of cohere"
2021.eacl-main.34,D19-1388,0,0.0114349,"the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelwal et al., 2019; Zhang et al., 2019). While our model builds upon this work, it is, to our k"
2021.eacl-main.34,D18-1443,0,0.0149025,"integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelwal et al., 2019; Zhang et al., 2019). While our model builds upon this work, it is, to our knowledge, the first transformer summarization framework to explicitly model narrative flow and scientific fact-checking across domains. Related Work 9 Narrative Flow and Factuality Modeling coherent narrative flow remains a major cha"
2021.eacl-main.34,P18-1152,1,0.710836,"s copying from the introduction at the loss of narrative structure. For example, the generator will select a summary that opens with “we present a method for jointly solving penn treebank style empty category (e.g. figure 1)..."", while the adjacency discriminator selects a summary that opens with “we present a method to jointly solve the problem of empty categories..."" and does not refer to a particular figure. Both summaries are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scie"
2021.eacl-main.34,D16-1032,1,0.823864,"tly solve the problem of empty categories..."" and does not refer to a particular figure. Both summaries are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promisin"
2021.eacl-main.34,P17-1099,0,0.0464192,"d on AAN for decoding both ArXiv CS and AAN, while the other discriminator is fine-tuned on ArXiv Bio and used exclusively for decoding that subset. We weigh the generation and discriminator models equally when decoding by setting λgen =λdisc =.5. Additional implementation details are provided in Appendices 10 A.3 and A.4. 7 Experiments We compare against extractive approaches using the Lede-3 and LexRank (Erkan and Radev, 2004) baselines. We also compare against two abstractive approaches: a 2-layer bi-LSTM sequenceto-sequence model with attention (LSTM), and a pointer-generator model (PGen; See et al., 2017). Training details of the supervised baselines can be found in the Appendix A.2. In addition, we compare to a subset of our approach that only uses the generator to produce summaries, rather than the full framework. Automatic Evaluation Following previous work on summarization, we use the ROUGE metric (Lin, 2004) for automatic evaluation of generative models and Co-opNet. Specifically, we report ROUGE-1, ROUGE-2 and ROUGE-L F1 scores. To capture similarity in contextual meaning, we look at BERTScore F1 (Zhang et al., 2020a), which has been shown to more closely correlate with human judgements"
2021.eacl-main.34,N19-1238,0,0.0189078,"ss of narrative structure. For example, the generator will select a summary that opens with “we present a method for jointly solving penn treebank style empty category (e.g. figure 1)..."", while the adjacency discriminator selects a summary that opens with “we present a method to jointly solve the problem of empty categories..."" and does not refer to a particular figure. Both summaries are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Sum"
2021.eacl-main.34,P19-1282,0,0.0195439,"adjacency discriminator to minimize the negative log likelihood of predicting whether two sentences are adjacent or not: Factuality and Faithfulness To measure factuality of generated summaries, we predict which tokens in the summary are likely to belong to a fact-checking evidence span (i.e., a span of the text used to prove a scientific claim us4 ing a finetuned BERT token classification model. Recent work has shown that inspecting attention weights alone is not necessarily a reliable metric for determining saliency of particular aspects in the input context to the output of neural models (Serrano and Smith, 2019). The saliency weights representing the likelihood of tokens belonging to evidence spans provides us with a more explicit representation of factual importance. We obtain proxy saliency labels for the importance of a particular token t appearing in an abstract using a BERT model trained on evidence NLP existing semantic schema, annotation effort, music knowledge representation, siri assistant BIO biological system, ptotic, cybernetics entropy , shannon established fundamental limits spans annotated for scientific fact-checking (Wadden et al., 2020). Specifically, if t is not a stopword and t ∈"
2021.eacl-main.34,2020.emnlp-main.750,0,0.071057,"Missing"
2021.eacl-main.34,W04-1013,0,0.144419,"Missing"
2021.eacl-main.34,2020.acl-main.173,0,0.038143,"Missing"
2021.eacl-main.34,D18-1206,0,0.253826,"of the time when generating summaries of news articles (Maynez et al., 2020). To address these issues, we focus our study on generating abstractive summaries with factuality and narrative flow. Given an input document, the goal is to generate a paragraph-length abstractive summary with proper discourse structure that contains factually correct claims. Our study builds on and extends previous work that focuses on either extractive document-level summarization (Nenkova and McKeown, 2012; Allahyari et al., 2017) or abstractive sentence-level summarization (Rush et al., 2015; Grusky et al., 2019; Narayan et al., 2018). In pursuit of this goal, we introduce Cooperative Generator-Discriminator Networks (Co-opNet), a framework for abstractive summarization that considers subtle aspects of fact-checking and discourse necessary for coherent text generation. In this framework, the generator, a transformer language model fine-tuned for abstractive summarization, proposes a pool of candidate summaries (§2). The 435 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 435–447 April 19 - 23, 2021. ©2021 Association for Computational Linguistics discrimina"
2021.eacl-main.34,D19-1509,1,0.846882,"oduction at the loss of narrative structure. For example, the generator will select a summary that opens with “we present a method for jointly solving penn treebank style empty category (e.g. figure 1)..."", while the adjacency discriminator selects a summary that opens with “we present a method to jointly solve the problem of empty categories..."" and does not refer to a particular figure. Both summaries are faithful to the introduction, but the discriminatorselected summary makes more sense in the context of a paper abstract. 8 tensen et al., 2013; Nikolov et al., 2018; Holtzman et al., 2018; Qin et al., 2019; Koncel-Kedziorski et al., 2019; Gabriel et al., 2021). Early approaches to incorporating structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summa"
2021.eacl-main.34,W09-3607,0,0.0542746,"ain the base summarization transformer model for 12 epochs. All experiments are run on either a Titan-X or Quadro RTX 8000 GPU. Training time for the AAN and ArXiv Bio datasets is about 30 minutes per epoch. Training time for the ArXiv CS dataset is 2.5 hours per epoch. In our experiments we use top-k sampling with k=4 (Fan et al., 2018) to generate candidate summaries for each model. AAN Additionally, we include an existing dataset of scientific articles that focuses on papers in the NLP computer science domain. This dataset consists of a 12k paper subset from the ACL Anthology Network (AAN; Radev et al., 2009) with extracted introduction and abstract pairs. Discriminator At training time we use a maximum sentence length of 200 tokens to accommodate the fixed input size of BERT (512 tokens), reduce inference time, and discourage the model from generating abnormally long run-on sentences 9 that indicate the presence of coherence issues. For the adjacency discourse models, we fine-tune the discriminator using a learning rate of 2e-5, a linear warmup learning rate schedule, and a batch size 7 See Appendix A.6 for comparison of datasets. https://github.com/huggingface/ transformers 9 See the original pa"
2021.eacl-main.34,D15-1044,0,0.0430751,"structure include integration of explicit discourse markers into automatic summarization (Alonso i Alemany and Fuentes Fort, 2003). Recently proposed solutions include globaltracking of entities (Kiddon et al., 2016; Bosselut et al., 2018; Mei et al., 2016), as well as discourseaware attention (Cohan et al., 2018). While there has been prior work on factual consistency (Cao et al., 2018; Gao et al., 2019; Kry´sci´nski et al., 2020; Zhang et al., 2020b), these works did not focus on scientific paper summarization. Neural Abstractive Summarization In the past, abstractive summarization models (Rush et al., 2015; Gehrmann et al., 2018) have relied upon seq2seq encoder-decoder architectures (Sutskever et al., 2014; Narayan et al., 2018; Celikyilmaz et al., 2018). Transformer models have emerged as a promising architecture for text generation and summarization (Liu et al., 2018; Hoang et al., 2019; Khandelwal et al., 2019; Zhang et al., 2019). While our model builds upon this work, it is, to our knowledge, the first transformer summarization framework to explicitly model narrative flow and scientific fact-checking across domains. Related Work 9 Narrative Flow and Factuality Modeling coherent narrative"
2021.eacl-main.34,W19-2303,0,0.0303943,"Missing"
2021.eacl-main.34,2020.emnlp-main.609,1,0.850033,"Missing"
2021.eacl-main.34,2020.acl-main.451,0,0.0419566,"distribution over the output vocabulary as follows: L P (wi ∣w0 , ...wi−1 ) = softmax(hi−1 We ) (3) where We is the same embedding matrix as in EquaL tion 1 and hi−1 is the final layer transformer block output. Generator Networks We use the transformer architecture of Radford et al. (2019) as our generator’s architecture. Following the work of Liu et al. (2018), we adapt a language model to the task of abstractive summarization by concatenating the ar436 3 Discriminator Networks Because summarization models are prone to narrative flow and factual consistency issues (Kry´sci´nski et al., 2020; Xu et al., 2020), we use a discriminator to score generated summaries for discourse and factuality properties. Due to the challenge of explicitly defining discourse and factuality properties as scores, these properties are approximated using parameterized scoring functions. These scoring functions determine if generated text demonstrates discourse and factuality properties in three ways: (1) predicting the discourse role of sentences within a full summary, (2) predicting the likelihood of adjacency given a sentence pair, and (3) measuring the presence of salient facts in the generated summary from the origina"
2021.findings-acl.42,W15-4918,0,0.0545411,"Missing"
2021.findings-acl.42,W07-0718,0,0.0285569,".04 0.00 -0.11 -0.12 0.04 0.07 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0.82 0.59 0.18 0.83 0.64 Table 5: Correlation (Corr) for 250 annotated XSUM and 250 SAMSUM generated summaries with fine-grained labeling. The arrow next to “Corr” indicates the direction of a correct correlation. 6 Related Work Prior work concerning evaluation of automatic metrics and human evaluation for NLG systems has mainly focused on general analysis of output quality or coherence and fluency (Callison-Burch et al., 2007; Graham, 2015; Fabbri et al., 2021), rather than factuality. Recent efforts by NLP researchers have drawn attention to the issue of factual errors 481 6.1 Discussion of Meta Evaluation and Conclusion Our analyses show that in contrast to prior work on factual consistency that mostly concentrated on one specific domain and dataset, our GO FIGURE framework is effective at evaluating sensitivity and validity of factual consistency metrics with only reference summaries, rather than requiring computationally intensive testing across summarization model variants to identify metric strengths and sho"
2021.findings-acl.42,P18-1060,0,0.0284132,"factuality level of Si may be unclear. Metric bounds provide points of comparison. A bounded but insensitive factuality metric may assign higher values to mostly nonfactual or unrelated summaries over summaries that are close to the reference. A metric that is sensitive only to a subset of errors might ignore a significant number of modelgenerated errors (Figure 1). Prior work such as Reiter and Belz (2009) highlight the risk of claiming validity without testing generality. The scoring function H(D, Si ) represented by human evaluation is a gold standard for assessment of generation quality (Chaganty et al., 2018), so M (D, Si ) should be an approximation. Table 1: Details of factuality metric conditions. Here M is a metric scoring function, D is a source document and Si is a summary. M (D, Sr ) where D is the source document and Sr is a randomly sampled summary from the corpus.2 We define the Upper Bound for the metric as M (D, Sf ), where Sf is the reference groundtruth summary. Since our controlled experiments use transformed versions of the reference summary with injected errors, the original reference is guaranteed to be at least as factually consistent as a transformed summary. To test sensitivit"
2021.findings-acl.42,P19-1264,1,0.837289,"Missing"
2021.findings-acl.42,2020.acl-main.454,0,0.0371766,"factual consistency and standard generation metrics, including QA metrics. It also reveals that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated. 1 2 Introduction The goal of text generation systems is to produce text that is fluent, coherent, relevant, as well as factually correct. Recent progress in neural approaches to building semantically constrained text generation systems has shown tremendous improvements in this direction (Liu and Lapata, 2019; Guo et al., 2018; Durmus et al., 2020; Wang et al., 2020). However, an important issue in text generation systems is that they can yield factually inconsistent text, caused by somewhat distorted or fabricated facts about the source text. Especially in document summarization tasks, models that abstract away salient aspects, have been shown to generate text ∗ Work done while first author was interning at MSR. Factuality Metric Meta Evaluation Since reference summaries may be an incomplete representation of the salient facts in a source document or unavailable, we consider factuality in terms of how well candidate summaries are fact"
2021.findings-acl.42,N19-1395,0,0.0436521,"Missing"
2021.findings-acl.42,P19-1213,0,0.0411927,"Missing"
2021.findings-acl.42,D19-5409,0,0.0132681,"o arise in realistic sum3 Evaluation Datasets 2 While this may not be the strictest lower bound in theoretical terms, we consider it appropriate as an empirical lower bound since the content is irrelevant to the document. A single random summary is used. 3 For our experiments, we inject up to a maximum of x errors with x ∈ {1, 2, 3}. We evaluate metrics on three datasets: 1-sentence BBC news summaries from the XSUM extreme summarization dataset (Narayan et al., 2018), multi-sentence summaries from the 479 CNN/DailyMail dataset (Nallapati et al., 2016), and the recently released SAMSUM corpus (Gliwa et al., 2019) consisting of English language conversations written by linguists and aligned multisentence summaries. 3.1 Diagnostic Datasets To test the ability of proposed metrics to fulfill our predefined conditions, we set up two diagnostic datasets consisting of (i) transformed reference summaries with simulated factuality errors that allow us to induce and measure factuality levels in a controlled setting and (ii) summaries generated by state-of-the-art transformer summarization models that allows us to measure the effectiveness of metrics in a real data setting. We sample 500 source / summary pairs f"
2021.findings-acl.42,D15-1013,0,0.0302286,"7 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0.82 0.59 0.18 0.83 0.64 Table 5: Correlation (Corr) for 250 annotated XSUM and 250 SAMSUM generated summaries with fine-grained labeling. The arrow next to “Corr” indicates the direction of a correct correlation. 6 Related Work Prior work concerning evaluation of automatic metrics and human evaluation for NLG systems has mainly focused on general analysis of output quality or coherence and fluency (Callison-Burch et al., 2007; Graham, 2015; Fabbri et al., 2021), rather than factuality. Recent efforts by NLP researchers have drawn attention to the issue of factual errors 481 6.1 Discussion of Meta Evaluation and Conclusion Our analyses show that in contrast to prior work on factual consistency that mostly concentrated on one specific domain and dataset, our GO FIGURE framework is effective at evaluating sensitivity and validity of factual consistency metrics with only reference summaries, rather than requiring computationally intensive testing across summarization model variants to identify metric strengths and shortcomings. We"
2021.findings-acl.42,D19-1320,0,0.0123154,"h or samplebased decoding strategies. We then annotate the generated summaries for fine-grained factual errors using the types in Figure 1 to create a hand-curated factual consistency diagnostic dataset. 4 Factuality Metrics for Evaluation We mainly focus on meta-evaluating most recently proposed factual consistency metrics which use two types of proxy natural language understanding (NLU) objectives aimed at implicitly capturing factuality in generated text: question-answering (QA) and a masked token prediction cloze task. For QA we evaluate using SummaQA (which uses QA pairs from the source, Scialom et al., 2019) and FEQA (which uses QA pairs from the summary, Durmus et al., 2020), while for the cloze task setting we use BLANC-Help and BLANC-Tune (Vasilyev et al., 2020, see the appendix for details of metrics). We also measure the factual-awareness of BERTScore (Zhang et al., 2020), a summarization metric that is aimed primarily at improving coherency rather than factual consistency, and standard summarization evaluation metrics (e.g. ROUGE (Lin, 2004)). 4 See the Appendix for details of linguistic feature extraction for injecting errors. 5 5.1 Meta-Analysis of Factuality Metrics Controlled Data Exper"
2021.findings-acl.42,2020.acl-main.704,0,0.0307123,"Missing"
2021.findings-acl.42,2020.emnlp-main.373,1,0.796905,"Missing"
2021.findings-acl.42,2020.eval4nlp-1.2,0,0.0168669,"ated factual consistency diagnostic dataset. 4 Factuality Metrics for Evaluation We mainly focus on meta-evaluating most recently proposed factual consistency metrics which use two types of proxy natural language understanding (NLU) objectives aimed at implicitly capturing factuality in generated text: question-answering (QA) and a masked token prediction cloze task. For QA we evaluate using SummaQA (which uses QA pairs from the source, Scialom et al., 2019) and FEQA (which uses QA pairs from the summary, Durmus et al., 2020), while for the cloze task setting we use BLANC-Help and BLANC-Tune (Vasilyev et al., 2020, see the appendix for details of metrics). We also measure the factual-awareness of BERTScore (Zhang et al., 2020), a summarization metric that is aimed primarily at improving coherency rather than factual consistency, and standard summarization evaluation metrics (e.g. ROUGE (Lin, 2004)). 4 See the Appendix for details of linguistic feature extraction for injecting errors. 5 5.1 Meta-Analysis of Factuality Metrics Controlled Data Experiments We provide the results of the sensitivity analysis over our controlled data on the XSUM domain in Table 2, on CNNDM in Table 3 and on SAMSUM in Table 4."
2021.findings-acl.42,2020.acl-main.450,0,0.0139054,"and standard generation metrics, including QA metrics. It also reveals that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated. 1 2 Introduction The goal of text generation systems is to produce text that is fluent, coherent, relevant, as well as factually correct. Recent progress in neural approaches to building semantically constrained text generation systems has shown tremendous improvements in this direction (Liu and Lapata, 2019; Guo et al., 2018; Durmus et al., 2020; Wang et al., 2020). However, an important issue in text generation systems is that they can yield factually inconsistent text, caused by somewhat distorted or fabricated facts about the source text. Especially in document summarization tasks, models that abstract away salient aspects, have been shown to generate text ∗ Work done while first author was interning at MSR. Factuality Metric Meta Evaluation Since reference summaries may be an incomplete representation of the salient facts in a source document or unavailable, we consider factuality in terms of how well candidate summaries are factually grounded with"
2021.findings-acl.42,2020.findings-emnlp.203,0,0.0170721,".28 / 84.19 84.13 / 84.07 80.79 Correlation p-value -1.00 / -0.96 0.05* / 0.18 -1.00 / -0.91 0.01** / 0.28 -0.99 / -0.94 0.11 / 0.23 -1.00 0.05* -1.00 / -0.99 0.02* / 0.07 -1.00 &lt;0.01** / 0.03* -1.00 0.03* -1.00 / -0.99 0.05* / 0.08 -1.00 0.01** / 0.04* -1.00 /-0.99 0.01** / 0.07 Table 4: Results of simulated factual error data experiments (SAMSUM, average of 5 runs). (See Table 2 caption for details.) Metric BLANC-Help BLANC-Tune SummaQA-C SummaQA-F1 FEQA R-1 R-2 R-3 R-L BERTScore XSUM and hallucinations in the output of neural summarization models (Cao et al., 2018; Massarelli et al., 2019; Zhao et al., 2020; Falke et al., 2019b; Goodrich et al., 2019; Celikyilmaz et al., 2020). A number of works have highlighted the effectiveness of QA and cloze task objectives for evaluating or improving factuality on specific domains (Eyal et al., 2019; Huang et al., 2020). We aim to evaluate these metrics more broadly, and consider a wider range of domains (notably dialogue). SAMSUM Corr (- ←) p-value Corr (- ←) 0.04 0.00 -0.11 -0.12 0.04 0.07 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0."
2021.findings-emnlp.377,2021.naacl-main.109,0,0.0578886,"Missing"
2021.findings-emnlp.377,N18-2097,0,0.0178644,"equence Summarization Recent summarization models are based on Transformer (Vaswani et al., 2017) that has a quadratic time and memory complexity with respect to the input length, preventing it from being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong e"
2021.findings-emnlp.377,N19-1423,0,0.0170129,"to sliding window attention + global attention, which is more memory efficient. Longformer can accept up to 16K tokens and has shown improvement over long document summarization using its long-encoder-decoder (LED) variant. We allow the maximum input of 4,096 tokens for Longformer and cutoff the rest of the input, as we found further increasing such limit yields no improvements. To incorporate queries in QMSum for these endto-end models, we simply append the queries to the front of the meeting transcripts, as it is a standard practice for query-based summarization and also question answering (Devlin et al., 2019). 3.3 Experiment Setup For a fair comparison between all models, we fit all of the models into the same RTX 8000 GPU with 48 GiB of GPU memory. We adopt the fairseq3 implementation for BART, and the original code base for both Longformer4 and HMNet5 . We inherit the hyperparameters for all those models for fine-tuning in our experiments.6 Our most expensive experiments are fine-tuning for HMNet and Longformer, which take around 8 hours, while the runtime for BART model is less than one hour. We use ROUGE (Lin, 2004) as our main evaluation metric and pyrouge library7 as the ROUGE implementation"
2021.findings-emnlp.377,P19-1102,1,0.852432,"eries at the beginning of the input. For the input to the two models, we use the gold relevant text spans given a query in QMSum to avoid the influences of retrieval models. The results show that encoding queries has a large impact on both types of models, especially for BART, even if the gold utterances are given. 4.4 Transfer Ability between Different Tasks Pretraining has been shown effective for document summarization by introducing external knowledge As we discussed, some dialogues (e.g., QMSum) from other similar tasks (Hermann et al., 2015; contain more than 20k tokens. They exceed the Fabbri et al., 2019). We hypothesize that it is input limitation of most existing summarization especially important for dialogue summarization models. In this section, we further analyze the per- because the dataset size is usually small. Thereformance of summarization models as the input fore, we study the transfer learning between diflength changes. To compare the robustness be- ferent dialogue summarization tasks via pretraintween two types of models (mainly BART and HM- ing. Tab. 3 shows the performance of BART-large Net), we divide the test dialogues by the number of models that are pretrained using differe"
2021.findings-emnlp.377,D19-5409,0,0.102612,"ong et al., 2021; Zhu et al., 2021). Dia- els such as Longformer (Beltagy et al., 2020), and logue summarization aims to generate a short sum- several dialogue utterance retrieval methods for a mary for long dialogues to help the readers capture retrieve-then-summarize pipeline model, as well important information more efficiently. as hierarchical dialogue encoding models. For the A number of existing works on dialogue sum- specific challenges in dialogues, we explore difmarization focus on extracting the main events of ferent datasets for pretraining to test the transfera short conversation (Gliwa et al., 2019; Rohde ability between similar summarization tasks. We et al., 2021). However, unlike the short dialogues evaluate these models on three recent long dia∗ logue summarization datasets: QMSum for meetEqual Contribution. ‡ The work was done when Asli was at MSR. ings (Zhong et al., 2021), MediaSum for inter4426 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4426–4433 November 7–11, 2021. ©2021 Association for Computational Linguistics views (Zhu et al., 2021), SummScreen for TV series transcripts (Chen et al., 2021). In our experiments, we find that the pipeline met"
2021.findings-emnlp.377,D19-5406,0,0.0131131,"017) that has a quadratic time and memory complexity with respect to the input length, preventing it from being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong et al., 2021) is a query-based multi-domain meeting summarization dataset annotated by humans. It contai"
2021.findings-emnlp.377,2020.acl-main.703,0,0.0305473,"contains less than 20 utterances, some tasks Abstract for summarizing much longer dialogues have been proposed recently (Chen et al., 2021; Zhong et al., Dialogue summarization helps readers capture 2021). These datasets are usually derived from salient information from long conversations in meetings and interviews, with hundreds of turns meetings, interviews, and TV series. However, in each dialogue. The length of such dialogues real-world dialogues pose a great challenge typically exceeds the input limits imposed by reto current summarization models, as the diacent transformer-based models (Lewis et al., 2020), logue length typically exceeds the input limmaking it difficult to train an end-to-end summaits imposed by recent transformer-based pretrained models, and the interactive nature of rization model for such tasks. This poses the chaldialogues makes relevant information more lenge: How can we effectively use the current neucontext-dependent and sparsely distributed ral summarization models on dialogues that greatly than news articles. In this work, we perexceed their length limits? form a comprehensive study on long dialogue Additionally, compared with document summasummarization by investigati"
2021.findings-emnlp.377,W04-1013,0,0.0552362,"practice for query-based summarization and also question answering (Devlin et al., 2019). 3.3 Experiment Setup For a fair comparison between all models, we fit all of the models into the same RTX 8000 GPU with 48 GiB of GPU memory. We adopt the fairseq3 implementation for BART, and the original code base for both Longformer4 and HMNet5 . We inherit the hyperparameters for all those models for fine-tuning in our experiments.6 Our most expensive experiments are fine-tuning for HMNet and Longformer, which take around 8 hours, while the runtime for BART model is less than one hour. We use ROUGE (Lin, 2004) as our main evaluation metric and pyrouge library7 as the ROUGE implementation throughout all experiments. 4 Result and Analysis Here we demonstrate our findings in four corresponding subsections. We also show some concrete examples and perform qualitative analysis in § 4.5 4.1 Dealing with Long Dialogues We compare several methods for addressing the long input issue for dialogue summarization, including different utterance retrieval methods describe in § 3.2.1 for a retrieve-then-summarize framework, heuristics for shortening the dialogue 3 https://github.com/pytorch/fairseq https://github.c"
2021.findings-emnlp.377,D18-1206,0,0.0283295,"his work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu et al. Tab. 1 shows the statistics for these three (2020b) described a hierarchical model for both long dialogue datasets. Additionally, we also inner- and cross-utterance attention, while Chen consider CNN/Dailymail (Hermann et al., 2015) and Yang (2020) proposed a multi-view decoder (CNN/DM), XSum (Narayan et al., 2018), and to leverage different extracted views of dialogues, SAMSum (Gliwa et al., 2019) as datasets for presuch as topic view and stage view. training in our experiments. 4427 3.2 Models 3.2.1 Retrieve-then-summarize Pipeline Dialogues tend to be relatively long, and most existing summarization models cannot process such long inputs. The two-stage retrieve-thensummarize pipeline first retrieves the most relevant subtext in the dialogue and then feeds to a summarizer. We experiment with the following retrievers: • TF-IDF (Jones, 1972) Based on bag-of-words representation, TF-IDF measuers term fre"
2021.findings-emnlp.377,P17-1098,0,0.0268736,"., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu et al. Tab. 1 shows the statistics for these three (2020b) described a hierarchical model for both long dialogue datasets. Additionally, we also inner- and cross-utterance attention, while Chen consider CNN/Dailymail (Hermann et al., 2015) and Yang (2020) proposed a multi-view decoder (CNN/DM), XSum (Narayan et al., 2018), and to leverage different extracted views of dialogues,"
2021.findings-emnlp.377,P18-1062,0,0.0550762,"Missing"
2021.findings-emnlp.377,P19-1212,0,0.0228442,"m being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong et al., 2021) is a query-based multi-domain meeting summarization dataset annotated by humans. It contains 1,808 queries together with 232 long meeting transcripts, with topics as software product, a"
2021.findings-emnlp.377,2020.findings-emnlp.19,1,0.927658,"ontains annotated gold spans which could be used as the gold labels for training the retrievers; MediaSum (Zhu et al., 2021) is a large-scale media interview dataset consisting of 463.6K transcripts Dialogue Summarization Dialogue summariza- collected from NPR and CNN. Because MediaSum tion aims to generate concise summaries for dia- contains short summaries, i.e. only a short sentence logues, such as meetings (McCowan et al., 2005; representing the topic, we only use this dataset for Janin et al., 2003; Zhong et al., 2021; Shang et al., pretraining and analysis. Due to the huge size of 2018; Zhu et al., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et"
2021.findings-emnlp.377,2020.coling-main.39,0,0.0374974,"e summariza- collected from NPR and CNN. Because MediaSum tion aims to generate concise summaries for dia- contains short summaries, i.e. only a short sentence logues, such as meetings (McCowan et al., 2005; representing the topic, we only use this dataset for Janin et al., 2003; Zhong et al., 2021; Shang et al., pretraining and analysis. Due to the huge size of 2018; Zhu et al., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu"
2021.findings-emnlp.377,2021.naacl-main.474,1,0.874688,"ty can be further improved with In this paper, we systematically investigate these a stronger retrieval model and pretraining on issues on dialog summarization: we first explore proper external summarization datasets. the various solutions to the lengthy input problem. Then, we analyze and compare the methods to im1 Introduction prove generic summarization models on challengLarge amount of dialogue data have been produced ing dialogue datasets. To address the long input in meetings, TV series, and interviews (Chen et al., issue, we investigate extended transformer mod2021; Zhong et al., 2021; Zhu et al., 2021). Dia- els such as Longformer (Beltagy et al., 2020), and logue summarization aims to generate a short sum- several dialogue utterance retrieval methods for a mary for long dialogues to help the readers capture retrieve-then-summarize pipeline model, as well important information more efficiently. as hierarchical dialogue encoding models. For the A number of existing works on dialogue sum- specific challenges in dialogues, we explore difmarization focus on extracting the main events of ferent datasets for pretraining to test the transfera short conversation (Gliwa et al., 2019; Rohde ability b"
2021.mtsummit-loresmt.6,W16-4117,0,0.0549587,"Missing"
2021.mtsummit-loresmt.6,2020.acl-main.147,0,0.0289353,"in Inuktitut in addition to data augmentation and pretraining. Using Transformer-based models for translation In recent times, there have been several work that use variations of Transformer (Vaswani et al., 2017) model for the task of machine translation. Chen et al. (2018) combine the power of recurrent neural network and transformer. Dehghani et al. (2019) introduce universal transformers as a generalization of transformers whereas Deng et al. (2018) combine transformer architecture with several other techniques such as BPE, back translation, data selection, model ensembling and reranking. Bugliarello and Okazaki (2020) incorporate syntactic knowledge into transformer model to show improvements on English to German, Turkish and Japanese translation tasks. Currey and Heafield (2019) introduce two methods to incorporate English syntax when translating from English into other Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 59 languages with Transformers. Liu et al. (2020b) introduce mBART, an auto-encoder pretrained on large-scale monolingual corpora and show gains on several languages. Using TPRs T"
2021.mtsummit-loresmt.6,P18-1008,0,0.0190961,"Inuktitut and also explored using morphological segmentation for alignment as well as neural and statistical machine translation. This work was followed up by Knowles et al. (2020) who introduce additional methods techniques on the Inuktitut dataset. Roest et al. (2020) and Scherrer et al. (2020) also investigated morphological segmentation in Inuktitut in addition to data augmentation and pretraining. Using Transformer-based models for translation In recent times, there have been several work that use variations of Transformer (Vaswani et al., 2017) model for the task of machine translation. Chen et al. (2018) combine the power of recurrent neural network and transformer. Dehghani et al. (2019) introduce universal transformers as a generalization of transformers whereas Deng et al. (2018) combine transformer architecture with several other techniques such as BPE, back translation, data selection, model ensembling and reranking. Bugliarello and Okazaki (2020) incorporate syntactic knowledge into transformer model to show improvements on English to German, Turkish and Japanese translation tasks. Currey and Heafield (2019) introduce two methods to incorporate English syntax when translating from Engli"
2021.mtsummit-loresmt.6,W19-5203,0,0.022571,"ations of Transformer (Vaswani et al., 2017) model for the task of machine translation. Chen et al. (2018) combine the power of recurrent neural network and transformer. Dehghani et al. (2019) introduce universal transformers as a generalization of transformers whereas Deng et al. (2018) combine transformer architecture with several other techniques such as BPE, back translation, data selection, model ensembling and reranking. Bugliarello and Okazaki (2020) incorporate syntactic knowledge into transformer model to show improvements on English to German, Turkish and Japanese translation tasks. Currey and Heafield (2019) introduce two methods to incorporate English syntax when translating from English into other Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 59 languages with Transformers. Liu et al. (2020b) introduce mBART, an auto-encoder pretrained on large-scale monolingual corpora and show gains on several languages. Using TPRs TPRs have gained traction recently with the interest in neurosymbolic computation to achieve out-of-domain generalization. They have been used in a variety of domains"
2021.mtsummit-loresmt.6,I17-1015,0,0.0236615,"e Transformers more sample-efficient, enabling them to perform better from smaller amounts of data. 1 Introduction The task of machine translation has seen major progress in recent times with the advent of large-scale Transformer-based models (e.g., Vaswani et al., 2017; Dehghani et al., 2019; Liu et al., 2020a). However, there has been less progress on language pairs that specifically involve morphologically rich languages. Moreover, although there has been previous work that builds linguistic structure into translation models to deal with morphological complexity (Sennrich and Haddow, 2016; Dalvi et al., 2017; Matthews et al., 2018), to the best to our knowledge there has not been work that applies such strategies to large-scale Transformer-based models. We hypothesize that providing Transformers access to structured linguistic representations can significantly boost their performance on translation into languages with complex morphology that encodes linguistic structure. In this work, we investigate two methods for introducing such structural bias into Transformer-based models. In the first method, we use the TP-Transformer (TPT) (Schlag et al., 2019), in which a traditional Transformer is augmen"
2021.mtsummit-loresmt.6,P16-1162,0,0.0728165,"Missing"
2021.mtsummit-loresmt.6,N18-2074,0,0.0303896,"nsard 5 Training 28,694,211 4,000,000 1,300,000 207,678 1,312,791 Validation 3,586,776 500,000 65,000 3,007 5,494 Test 3,586,777 500,000 65,000 3,000 6,181 Experimental Results We aim to answer the following research questions (RQ) through our experimentation: 1. Do either or both of our structural methods improve translation? 2. If so, how does that advantage interact with: (a) Training data quantity? (b) Transfer learning? (c) Morphological richness of language? As a baseline, we trained the standard Transformer model (Vaswani et al., 2017) with the addition of relative position embeddings (Shaw et al., 2018). Model training details and computing resources can be found in Section 1 and 2 of the supplementary materials. For each model, we used either byte pair encoding (BPE) (Sennrich et al., 2016) or morphological tokenization as described in §3. In order to see how our changes relate to sample efficiency, we Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 55 vary the size of the subset of the Open Subtitles dataset used for training. We used the SETimes dataset to finetune these model"
2021.mtsummit-loresmt.6,2020.blackboxnlp-1.23,1,0.736321,"and show gains on several languages. Using TPRs TPRs have gained traction recently with the interest in neurosymbolic computation to achieve out-of-domain generalization. They have been used in a variety of domains, including mathematical problem solving (Schlag et al., 2019), reasoning (Schlag and Schmidhuber, 2018), image captioning (Huang et al., 2018), question-answering (Palangi et al., 2018), and program synthesis (Chen et al., 2020). A separate line of work uses TPRs as an interpretation tool to understand representations in networks that do not explicitly use TPRs (McCoy et al., 2019; Soulos et al., 2020). 8 Conclusion We investigated two methods for improving translation into morphologically rich languages with Transformers. The TP-Transformer adds an additional component to Transformer attention to represent relational structure. This model had the largest improvement on smaller datasets and modest improvement on larger datasets. We also investigated morphological tokenization which had substantial improvements on small datasets and transfer learning. When used together, our methods improve on the state of the art for translation from English into Inuktitut by 8 BLEU. The models were analyze"
2021.mtsummit-loresmt.6,A97-1047,0,0.11972,"pora (Supplementary materials section 4). Higher values of this measure correspond to more regular structure/information in words, and thus, greater morphological complexity. We computed the measure over the first 100,000 characters of the test set of each dataset. We computed CD as 1.89 for the Hansard dataset, while the Turkish datasets ranged from CD 1.45-1.49. This corresponds to the relatively large increase in BLEU seen for Inuktitut. 7 Related Work Translating into Morphologically-rich languages Previous work has leveraged morphology for translating into morphologically-rich languages. Turhan (1997) uses a recursive symbolic system to translate from English into Turkish including a morphological generator. Ataman et al. (2020) use hierarchical latent variable models to model both character and morpheme level statistics for translating into morphologically rich languages (Arabic, Czech, Turkish) with GRUs. Passban et al. (2018a) introduce a character-level neural machine translation model for translating into morphologically rich languages which incorporates a morphology lookup table into the decoder whereas Passban et al. (2018b) propose a subword-level model that uses separate embedding"
2021.naacl-main.381,W18-2501,0,0.0190822,"Missing"
2021.naacl-main.381,N18-1065,0,0.0237488,"Missing"
2021.naacl-main.381,N19-1419,0,0.0638963,"Missing"
2021.naacl-main.381,N18-1114,1,0.83673,"We also believe there is a connection between the above two effects, as the structural, syntactic information favors a lower-dimensional or even discrete space while the distributed, semantic information favors a higher-dimensional space. 5 Related Work Explicit TPR Structures in Neural Networks While earlier TPR work based on (Smolensky, 1990) focused on computability rather than learnability questions, recently TPRs have been incorporated into several recurrent deep learning models in order to solve various NLP tasks including Part-of-Speech tagging, constituency parsing, image captioning (Huang et al., 2018, 2019), question answering (Palangi et al., 2018; Schlag and Schmidhuber, 2018), and natural-to-formal language generation (program synthesis) (Chen et al., 2020). Most recently, TPRs have been introduced into Transformer architectures, starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside i"
2021.naacl-main.381,D18-1206,0,0.316024,"and (3) demonstrate the emergent structures in representations by revealing the disentangled syntactic and semantic information encoded in the role and filler spaces. To test the ability of our TP-T RANSFORMER with discrete roles against the standard Transformer and the TP-T RANSFORMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequenc"
2021.naacl-main.381,P19-1209,0,0.036398,"Missing"
2021.naacl-main.381,C18-1101,0,0.0419582,"Missing"
2021.naacl-main.381,W04-1013,0,0.0317552,"RMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequence of tokens i = 1, ..., I can be seen as T RANSFORMER are indeed better than the Trans- a 2-dimensional lattice of cells (i, l) where i is the 4781 2.2 Linear TPR Concat Concat Multi-Head Attention Fillers (F) Roles (R) Scaled Dot-Product Attention Role-Attention Linear L"
2021.naacl-main.381,W19-4825,0,0.0481743,"Missing"
2021.naacl-main.381,Q18-1005,0,0.021196,"lag and Schmidhuber, 2018), and natural-to-formal language generation (program synthesis) (Chen et al., 2020). Most recently, TPRs have been introduced into Transformer architectures, starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside in a highly discrete space. only the semantic content (Liu and Lapata, 2018, 2019). To preserve salient source relations and generate abstractive summaries of the source document, previous work infused models with semantic parsers: while Song et al. (2018) introduces a new structure-infused copy mechanism that combines the source syntactic structure with the copy mechanism, Liao et al. (2018) uses abstract meaning representations (AMR). While these approaches require that the document sentence semantic parsers are provided beforehand, our models can implicitly learn to approximate the syntactic structure and semantic content in their representations. 6 Conclusion In"
2021.naacl-main.381,P19-1500,0,0.033989,"Missing"
2021.naacl-main.381,P14-5010,0,0.00451574,"Missing"
2021.naacl-main.381,2020.acl-main.173,0,0.0184431,"erated by the TP-T RANSFORMER are significantly better in grammar. This corroborates our claim that having the TPR can improve the model’s ability to follow the correct syntax in composing the summary. On the Wikihow dataset, the Transformer receives more votes in regarding the saliency. However, our TP-T RANSFORMER maintains an advantage in grammar and achieves significantly better overall preferences. Unfaithful XSum Examples It is well-known that the XSum dataset contains a portion of unfaithful reference summaries that mention facts not included in the source article (Durmus et al., 2020; Maynez et al., 2020). Therefore, we are interested to find out whether our TP-T RANSFORMER is better than the baseline only at expressing the faithful content or it can also generate some external, “unfaithful"" facts that the baseline can’t cover. To answer this question, we randomly sample 100 examples from the XSum dev set and manually examine the source document, reference summary, and the two generated summaries. Among these 100 examples, we identify 71 examples whose reference summary includes “unfaithful"" facts that are not mentioned in the source. In 21 out of 71 examples, the Transformer baseline manages"
2021.naacl-main.381,K16-1028,0,0.203201,"ability of our TP-T RANSFORMER with discrete roles against the standard Transformer and the TP-T RANSFORMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequence of tokens i = 1, ..., I can be seen as T RANSFORMER are indeed better than the Trans- a 2-dimensional lattice of cells (i, l) where i is the 4781 2.2 Linear TPR Concat Concat Mult"
2021.naacl-main.381,N18-2102,1,0.854901,"r summarizing any type of company internal datasets (e.g., internal documents, reports, meetings, legal forms, etc.) to further improve the productivity and efficiency of the users in their daily activities without needing to read long documents. Structured Representations for Abstractive Summarization Compared to the extractive methods, abstractive summarization models usually fail to show extractive properties, and have ten- Failure mode. Even though our models yield facdency to copy text from the source (See et al., 2017; tually consistent summaries, as judged by human Paulus et al., 2018; Pasunuru and Bansal, 2018; Ce- evaluation, they can still generate factually inconlikyilmaz et al., 2018). More recent approaches sistent summaries or sometimes hallucinate inforthat use standard transformers deal with this issue mation that the source document does not include. by introducing hierarchical structures to encode lo- This might be due to the bias or noise in the traincal and global information separately focusing on ing data. Model builders wanting to use our archi4788 tecture to build models on their company internal datasets should build models with consideration of intellectual properties and privacy"
2021.naacl-main.381,P17-1099,0,0.305661,"at least 0.3 advantage are bolded. Arxiv (Cohan et al., 2018) is a long document summarization dataset of scientific publications from arXiv.org (113k). The task is to generate the abstract from the paper body. (2019), while a sentence in an XSum or Wikihow summary usually aggregates information from multiple source sentences. 3.2 CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset contains 93k articles from CNN and 220k articles from the Daily Mail. Every article is accompanied by a few human-written bullet points about its content. We use the nonanonymized version used in See et al. (2017). Experimental Setup The Transformer and the two TP-T RANSFORMERS all have 6 layers, 8 heads per layer, dimension per head dk =64, model dimension dm =512, and feedforward dimension df =2048 for the encoder and decoder. Our TP-T RANSFORMER with discrete roles has Nr =50 role embeddings of dimension dr =64 at every layer. For each dataset above, we train the all three models from scratch using an Adafactor Optimizer (Shazeer and Stern, 2018) with square root learning rate decay and dropout rate of 0.1. We evaluate the models using automatic metrics including ROUGE F1 score and METEOR. Dataset A"
2021.naacl-main.381,C18-1146,0,0.0207274,"starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside in a highly discrete space. only the semantic content (Liu and Lapata, 2018, 2019). To preserve salient source relations and generate abstractive summaries of the source document, previous work infused models with semantic parsers: while Song et al. (2018) introduces a new structure-infused copy mechanism that combines the source syntactic structure with the copy mechanism, Liao et al. (2018) uses abstract meaning representations (AMR). While these approaches require that the document sentence semantic parsers are provided beforehand, our models can implicitly learn to approximate the syntactic structure and semantic content in their representations. 6 Conclusion In this work, we enrich the Transformer model with the structured Tensor Product Representation for abstractive summarization tasks. We represent every token as a pair of role and fill"
2021.naacl-main.381,P19-1452,0,0.137421,"ely one-hot, thus restricting the role vectors to a highly discrete space. This structural inductive bias encourages the TP-T RANSFORMER to encode the syntactic information in the discrete roles while isolating the semantics in the continuous fillers. former’s generations. Furthermore, to investigate the structural representation that naturally emerges during training and the advantage of having compositional TPR hidden states, we design a suite of decoder probing tasks to explore the information encoded in the role, filler, and TPR space. We adopt the encoder probing task design presented in Tenney et al. (2019b) and create four decoder probing tasks: Part-of-speech tagging (POS), Dependency Labeling (DEP), Semantic Role Labeling (SRL), and Named Entity Labeling (NEL). Our findings collectively show that the decoder’s role vectors encode a wealth of syntactic structures, aiding the decoder in deducing the syntactic features (e.g., being a proper noun, being the object of the root predicate) of the next token to be generated. The decoder’s filler vectors on the other hand encode more semantic information (e.g., being a person’s name). Furthermore, we observe that having the compositional TPR results"
2021.naacl-main.472,N18-1150,1,0.850619,"ting models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of"
2021.naacl-main.472,P18-1063,0,0.0367563,"lving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting sum"
2021.naacl-main.472,2020.findings-emnlp.329,1,0.691909,"nsive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the meeting transcripts to extract templates and generate summaries with the guidance of the templates. Shang et al. (2018) utilize multi-sentence compression techniques to generate summaries under an unsupervised setting. Li et al. (2019) attempt to incorporate multi-modal information to facilitate the meeting summarization. Zhu et al. (2020) propose a model which builds a hierarchical structure on word-level and turn-level information and uses news summary data to alleviate the inadequacy of meeting data. Unlike previous works, instead of merely generating summaries for the complete meeting, we propose a novel task where we focus on summarizing multi-granularity contents which cater to different people’s need for the entire meetings, and help people comprehensively understand meetings. 3 Data Construction In this section, we show how we collected meeting data from three different domains: academic meetings, product meetings, and"
2021.naacl-main.472,N12-1041,0,0.66189,"2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meet"
2021.naacl-main.472,P19-1098,0,0.0151867,"are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2"
2021.naacl-main.472,N16-1012,0,0.0336034,"s and analysis from different perspectives reveal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a mor"
2021.naacl-main.472,N18-2097,0,0.0198273,". 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006;"
2021.naacl-main.472,W06-0707,0,0.0593589,"Missing"
2021.naacl-main.472,P06-1039,0,0.208583,"Missing"
2021.naacl-main.472,N18-1065,0,0.039943,"Missing"
2021.naacl-main.472,2020.emnlp-main.295,0,0.0149003,"queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch o"
2021.naacl-main.472,W17-1004,0,0.108697,"of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921 June 6–11, 2021. ©2021 Association for Computational Linguistics standing out, while others may be more interested in what other attendees thought about different elements of the design. It is challenging to compress or compose a short summary that contains all the salient information. Alternatively, summarization systems should adopt a more flexible and interactive approach that allows people to express their interests and caters to their diverse intents when generating summaries (Dang, 2005, 2006; Litvak and Vanetik, 2017; Baumel et al., 2018). With comprehensive consideration of the multigranularity meeting contents, we propose a new task, query-based meeting summarization. To enable research in this area, we also create a highquality multi-domain summarization dataset. In this task, as shown in Figure 1, given a query and a meeting transcript, a model is required to generate the corresponding summary. The query-based approach is a flexible setup that enables the system to satisfy different intents and different levels of granularity. Besides the annotated queries and corresponding gold summaries at different"
2021.naacl-main.472,D18-1208,0,0.0649839,"Missing"
2021.naacl-main.472,D19-1387,1,0.849152,"generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from acade"
2021.naacl-main.472,D14-1181,0,0.00351051,", Pointer Network will point to the start turn and the end turn for each query. It is worth noting that one query can correspond to multiple spans in our dataset, so we always extract three spans as the corresponding text for each query when we use Pointer Network as Locator in the experiments. In addition, we design a hierarchical rankingbased model structure as the Locator. As shown in Figure 3, we first input the tokens in each turn to a feature-based BERT to obtain the word embedding, where feature-based means we fix the parameters of BERT, so it is actually an embedding layer. Next, CNN (Kim, 2014) is applied as a turn-level encoder to capture the local features such as bigram, 5 Experiments trigram and so on in each turn. Here we do not use Transformer because previous work (Kedzie et al., In this section, we introduce the implementation details, effectiveness of Locator, experimental results 2018) shows that this component does not matter and multi-domain experiments on QMSum. too much for the final performance. We combine different features to represent the utterance ui in 5.1 Implementation Details each turn, and concatenate the speaker embedding si as the turn-level representation:"
2021.naacl-main.472,2020.coling-main.499,0,0.0176046,"dia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the meeting transcripts to extract templates and generate summaries with the guidance of the templates. Shang et al. (2018) utilize multi-sentence compression techniques to generate summaries u"
2021.naacl-main.472,P19-1209,0,0.0161457,"meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2"
2021.naacl-main.472,2020.acl-main.703,0,0.0293968,"our goal in the second stage is to summarize the selected text spans based on the query. We instantiate our Summarizer with the current powerful abstractive models to explore whether the query-based meeting summarization task on our dataset is challenging. To be more specific, we choose the following three models: Pointer-Generator Network (See et al., 2017) is a popular sequence-to-sequence model with copy mechanism and coverage loss, and it acts as a baseline system in many generation tasks. The input to Pointer-Generator Network (PGNet) is: “&lt;s&gt; Query &lt;/s&gt; Relevant Text Spans &lt;/s&gt;”. BART (Lewis et al., 2020) is a denoising pretrained model for language generation, translation and comprehension. It has achieved new state-ofthe-art results on many generation tasks, including summarization and abstractive question answering. The input to BART is the same as PGNet. HMNet (Zhu et al., 2020) is the state-of-the-art meeting summarization model. It contains a hierarchical structure to process long meeting transcripts and a role vector to depict the difference among speakers. Besides, a cross-domain pretraining process is also included in this strong model. We add a turn representing the query at the begi"
2021.naacl-main.472,P19-1210,0,0.211278,"cussing user interface? User Interface Designer said the remote should perform standard features right out-of-the-box ...... Turn 121: User Interface Designer: The idea of having a remote is you have different keys and different structures. ...... Turn 162: Project Manager: Sure. Let&apos;s push forward the interface design. ...... Turn 316: Project Manager: Thanks. Have a nice day! Figure 1: Examples of query-based meeting summarization task. Users are interested in different facets of the meeting. In this task, a model is required to summarize the contents that users are interested in and query. Li et al., 2019; Zhu et al., 2020) is a task where summarization models are leveraged to generate summaries of entire meetings based on meeting transcripts. The resulting summaries distill the core contents of a meeting that helps people efficiently catch up to meetings. Most existing work and datasets on meeting summarization (Janin et al., 2003; Carletta et al., 2005) 1 Introduction pose the problem as a single document summarizaMeetings remain the go-to tool for collaboration, tion task where a single summary is generated for the whole meeting. Unlike news articles where with 11 million meetings taking pl"
2021.naacl-main.472,W13-2117,0,0.0344771,"2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the mee"
2021.naacl-main.472,P17-1098,0,0.0183837,"meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehd"
2021.naacl-main.472,W14-4407,0,0.133484,"et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meet"
2021.naacl-main.472,D15-1044,0,0.0238088,"n QMSum. Our results and analysis from different perspectives reveal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting"
2021.naacl-main.472,P17-1099,0,0.400999,"veal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summar"
2021.naacl-main.472,D19-1324,0,0.0115444,"the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and mor"
2021.naacl-main.472,P18-1062,0,0.35554,"ing a toll on our productivity and well- decisions (Wang and Cardie, 2013). This poses the being (Spataro, 2020). The proliferation of meet- question of whether a single paragraph is enough to summarize the content of an entire meeting? ings makes it hard to stay on top of this sheer Figure 1 shows an example of a meeting about volume of information and increases the need for automated methods for accessing key informa- “remote control design”. The discussions in the tion exchanged during them. Meeting summariza- meeting are multi-faceted and hence different users tion (Wang and Cardie, 2013; Shang et al., 2018; might be interested in different facets. For exam∗ ple, someone may be interested in learning about These two authors contributed equally. The order of authorship decided by the flip of a coin. the new trends that may lead to the new product 5905 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921 June 6–11, 2021. ©2021 Association for Computational Linguistics standing out, while others may be more interested in what other attendees thought about different elements of the design. It i"
2021.naacl-main.472,2020.acl-main.553,1,0.826971,"s to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 20"
2021.naacl-main.472,P13-1137,0,0.0731651,"Missing"
2021.naacl-main.472,2020.acl-main.552,1,0.832276,"dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2"
2021.naacl-main.472,P19-1100,1,0.888302,"ghting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this tas"
2021.naacl-main.472,D19-5410,1,0.872427,"Missing"
2021.naacl-main.472,2020.findings-emnlp.19,0,0.764092,"rface? User Interface Designer said the remote should perform standard features right out-of-the-box ...... Turn 121: User Interface Designer: The idea of having a remote is you have different keys and different structures. ...... Turn 162: Project Manager: Sure. Let&apos;s push forward the interface design. ...... Turn 316: Project Manager: Thanks. Have a nice day! Figure 1: Examples of query-based meeting summarization task. Users are interested in different facets of the meeting. In this task, a model is required to summarize the contents that users are interested in and query. Li et al., 2019; Zhu et al., 2020) is a task where summarization models are leveraged to generate summaries of entire meetings based on meeting transcripts. The resulting summaries distill the core contents of a meeting that helps people efficiently catch up to meetings. Most existing work and datasets on meeting summarization (Janin et al., 2003; Carletta et al., 2005) 1 Introduction pose the problem as a single document summarizaMeetings remain the go-to tool for collaboration, tion task where a single summary is generated for the whole meeting. Unlike news articles where with 11 million meetings taking place each day people"
C18-3006,P17-1045,1,0.833186,"al., 2016) demonstrated that using neural dialogue models can overcome current obstacles of deploying dialogue systems in larger dialogue domains. Rastogi et al. (2017) also proposed a multi-domain dialogue state tracker to achieve effective and efficient domain adaptation. Dialogue Management – Dialogue Policy Optimization The dialogue policy can be learned in either a supervised or a reinforcement learning manner (Su et al., 2016). The reinforcement learning based dialogue agent has been recently developed in different tasks and shown applicable for interactive scenarios (Li et al., 2017b; Dhingra et al., 2017; Shah et al., 2016). In order to enable reinforcement learning, a simulated environment is required. Several approaches are proposed for building user simulators as the interactive environment (Li et al., 2016; El Asri et al., 2016; Crook and Marin, 2017), so that the dialogue policy can be trained in a reinforcement framework. Natural Language Generation The RNN-based models have been applied to language generation for both chit-chat and task-orientated dialogue systems (Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence p"
C18-3006,W16-3622,0,0.0392894,"Missing"
C18-3006,W13-4073,0,0.0289462,"a good mechanism for integrating longer term knowledge context and shorter term dialogue context into these models (Chen et al., 2016b; Chen et al., 2016c). In addition, the importance of the LU module is investigated in Li et al. (2017a), where different types of errors from LU may degrade the whole system performance in an reinforcement learning setting. Dialogue Management – Dialogue State Tracking The state-of-the-art dialogue managers focus on monitoring the dialogue progress by neural dialogue state trackers. Among the initial models are the RNN based dialogue state tracking approaches (Henderson et al., 2013) that has shown to outperform Bayesian networks (Thomson and Young, 2010). More recent work that provided conjoint representations between the utterances, slot-value pairs as well as knowledge graph representations (Wen et al., 2016; Mrkˇsi´c et al., 2016) demonstrated that using neural dialogue models can overcome current obstacles of deploying dialogue systems in larger dialogue domains. Rastogi et al. (2017) also proposed a multi-domain dialogue state tracker to achieve effective and efficient domain adaptation. Dialogue Management – Dialogue Policy Optimization The dialogue policy can be l"
C18-3006,C16-1038,0,0.0214715,"asks, which supported flexible question types, allowed user-initiated requests during conversation, and finally achieved better robustness. Human feedback is also effectively leveraged into the learning framework for on-line training in an end-to-end manner (Liu et al., 2018). Dialogue Breadth In order to extend the coverage of the systems, transfer learning has been applied to different extended systems in order to proceed to a multi-domain scenario. Chen et al. (2016a) transfered the dialogue acts across different domains so that the performance of the newly-developed domain can be boosted. Kim et al. (2016) proposed to learn a domain-specific and domain-independent information in order to transfer the shared knowledge more efficiently and effectively. In addition, Gaˇsi´c et al. (2015) presented the policy committee in order to boost the performance for policy training in a new domain. All above work extended the dialogue coverage using different directions. Dialogue Depth Most current systems focus on knowledge-based understanding, but there are hierarchical understanding according to the dialogue complexity. For example, an intent about party scheduling may include restaurant reserving and inv"
C18-3006,I17-1074,1,0.889711,"Missing"
C18-3006,N18-1187,1,0.878801,"Missing"
C18-3006,P16-1230,0,0.0236728,", 2010). More recent work that provided conjoint representations between the utterances, slot-value pairs as well as knowledge graph representations (Wen et al., 2016; Mrkˇsi´c et al., 2016) demonstrated that using neural dialogue models can overcome current obstacles of deploying dialogue systems in larger dialogue domains. Rastogi et al. (2017) also proposed a multi-domain dialogue state tracker to achieve effective and efficient domain adaptation. Dialogue Management – Dialogue Policy Optimization The dialogue policy can be learned in either a supervised or a reinforcement learning manner (Su et al., 2016). The reinforcement learning based dialogue agent has been recently developed in different tasks and shown applicable for interactive scenarios (Li et al., 2017b; Dhingra et al., 2017; Shah et al., 2016). In order to enable reinforcement learning, a simulated environment is required. Several approaches are proposed for building user simulators as the interactive environment (Li et al., 2016; El Asri et al., 2016; Crook and Marin, 2017), so that the dialogue policy can be trained in a reinforcement framework. Natural Language Generation The RNN-based models have been applied to language generat"
C18-3006,N18-2010,1,0.822287,"(Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence planning and surface realization, and language variation can be easily achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism for controlling the dialogue act during generation in order to avoid semantics repetition, showing promising results. Several aspects of improvement have been achieved using contextual and structured information (Duˇsek and Jurcicek, 2016; Nayak et al., 2017; Su et al., 2018) 4 Recent Trends and Challenges on Learning Dialogues This part will focus on discussing the recent trends and current challenges on dialogue system technology. End-to-End Learning for Dialogue Systems With the power of neural networks, there are more and more attempts for learning dialogue systems in an end-to-end fashion. Different learning frameworks are applied, including supervised learning and reinforcement learning. This part will discuss the work about end-to-end learning for dialogues (Dhingra et al., 2016; Wen et al., 2016; Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Li et al."
C18-3006,W15-4639,0,0.0722328,"Missing"
C18-3006,D15-1199,0,0.0234091,"fferent tasks and shown applicable for interactive scenarios (Li et al., 2017b; Dhingra et al., 2017; Shah et al., 2016). In order to enable reinforcement learning, a simulated environment is required. Several approaches are proposed for building user simulators as the interactive environment (Li et al., 2016; El Asri et al., 2016; Crook and Marin, 2017), so that the dialogue policy can be trained in a reinforcement framework. Natural Language Generation The RNN-based models have been applied to language generation for both chit-chat and task-orientated dialogue systems (Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence planning and surface realization, and language variation can be easily achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism for controlling the dialogue act during generation in order to avoid semantics repetition, showing promising results. Several aspects of improvement have been achieved using contextual and structured information (Duˇsek and Jurcicek, 2016; Nayak et al., 2017; Su et al., 2018) 4 Recent Trends and Ch"
C18-3006,W16-3601,0,0.02409,"yak et al., 2017; Su et al., 2018) 4 Recent Trends and Challenges on Learning Dialogues This part will focus on discussing the recent trends and current challenges on dialogue system technology. End-to-End Learning for Dialogue Systems With the power of neural networks, there are more and more attempts for learning dialogue systems in an end-to-end fashion. Different learning frameworks are applied, including supervised learning and reinforcement learning. This part will discuss the work about end-to-end learning for dialogues (Dhingra et al., 2016; Wen et al., 2016; Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Li et al., 2017b). Recent advance of deep learning has inspired many applications of neural models to dialogue systems. Wen et al. (2016) and Bordes and Weston (2016) introduced a network-based end-to-end trainable taskoriented dialogue system, which treated dialogue system learning as the problem of learning a mapping from dialogue histories to system responses, and applied an encoder-decoder model to train the whole system. However, the system is trained in a supervised fashion, thus requires a lot of training data, and may not be able to explore the unknown space that does not exist in th"
D09-1057,C02-1150,0,0.030369,"of dictionary entities (as presented in Section 5.2) associated with a particular class type (either true or false, indicating a sentence can or cannot answer the question). λi are the parameters need to be estimated which reflects the importance of fi (c, d) in prediction. 3 letter other plant product religion sport substance symbol technique term vehicle word DESC definition Head Word head word is defined as one single word specifying the object that the question seeks. For example the head word of What is a group of turkeys called, is turkeys. This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example). The single word definition effectively avoids the noisy information brought by non-head word of the span (group in this case). A syntactic parser (Petrov and Klein, 2007) and the Collins rules (Collins, 1999) are modified to extract such head words. Question Classification Features Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. They have compiled the UIUC question classification dataset1 which consists of 5500 training and 500 t"
D09-1057,N03-5008,0,0.0114599,"es 543 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 543–550, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP accuracy over UIUC question dataset. Section 5 presents the question answer features. Section 6 illustrates the results based on TREC question answer dataset. And Section 7 draws the conclusion. 2 Table 1: 6 coarse and 50 fine Question types defined in UIUC question dataset. ABBR abb exp ENTITY animal body color creative currency dis.med. event food instrument lang Maximum Entropy Models Maximum entropy (ME) models (Berger et al., 1996; Manning and Klein, 2003), also known as log-linear and exponential learning models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging, named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. Given a training set of (C, D), where C is a set of class labels and D is a set of feature represented data points, the maximal entropy model attempts to max"
D09-1057,de-marneffe-etal-2006-generating,0,0.0281398,"mber number). 5.5 Dependency validity features when born feature 2 The question begins with when is/was and follows by a person name and then follows by key word born; The candidate sentence contains such person name, a NUM:date entity, and a key word born. Like (Cui et al., 2004), we extract the dependency path from the question word to the common word (existing in both question and sentence), and the path from candidate answer (such as CoNLL NE and numerical entity) to the common word for each pair of question and candidate sentence using Stanford dependency parser (Klein and Manning, 2003; Marneffe et al., 2006). For example, for question When did James Dean die and candidate sentence In 1955, actor James Dean was killed in a two-car collision near Cholame, Calif., we extract the pathes of When:advmod:nsubj:Dean and 1955:prep-in:nsubjpass:Dean for question and sentence respectively, where advmod and nsubj etc. are grammatical relations. We propose the dependency validity feature (DEP) as following. For all paired paths between a question and a candidate sentence, if at least one pair of path in which all pairs of grammatical relations have been seen in the training, then the DEP feature is set to be"
D09-1057,P07-1098,0,0.0540645,"ntity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. Given a training set of (C, D), where C is a set of class labels and D is a set of feature represented data points, the maximal entropy model attempts to maximize the log likelihood log P (C|D, λ) = X (c,d)∈(C,D) desc manner reason HUMAN group individual title desc LOC city country mountain other state NUM code count date distance money order other period percent speed temp size weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. The features are briefly described as following. More detailed information can be found at (Huang et al., 2008). P exp i λi fi (c, d) P log P , exp j λj fi (c, d) c′ (1) Question wh-word The wh-word feature is the question wh-word in given questions. For example, the wh-word of question What is the population of China is what. where fi (c, d) are feature indicator functions. We use ME models for both question classification and question answer rankin"
D09-1057,N07-1051,0,0.0118975,". 3 letter other plant product religion sport substance symbol technique term vehicle word DESC definition Head Word head word is defined as one single word specifying the object that the question seeks. For example the head word of What is a group of turkeys called, is turkeys. This is different to previous work including (Li and Roth, 2002; Krishnan et al., 2005) which has suggested a contiguous span of words (a group of turkeys in this example). The single word definition effectively avoids the noisy information brought by non-head word of the span (group in this case). A syntactic parser (Petrov and Klein, 2007) and the Collins rules (Collins, 1999) are modified to extract such head words. Question Classification Features Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. They have compiled the UIUC question classification dataset1 which consists of 5500 training and 500 test questions.2 All questions in the dataset have been manually labeled according to the coarse and fine grained categories as shown in Table 1, with coarse classes (in bold) followed by their fine classes. The UIUC dataset has laid a platform for the follow-up research including"
D09-1057,J96-1002,0,0.0130975,"Missing"
D09-1057,P06-1112,0,0.0294887,"e both established, which makes such feature activated for question 6 Question Answer Experiments Recall that most of the question answer features depend on the question classifier. For instance, the NE feature checks the presence or absence of CoNLL style named entities subject to the classified question type. In this section, we evaluate how the quality of question classifiers affects the question answering performance. 6.1 Experiment setup We use TREC99-03 factoid questions for training and TREC04 factoid questions for testing. To facilitate the comparison to others work (Cui et al., 2004; Shen and Klakow, 2006), we first retrieve all relevant documents which are compiled by Ken Litkowski8 to create training and test datasets. We 8 547 Available at http://trec.nist.gov/data/qa.html. then apply key word search for each question and retrieve the top 20 relevant sentences. We create a feature represented data point using each pair of question and candidate sentence and label it either true or false depending on whether the sentence can answer the given question or not. The labeling is conducted by matching the gold factoid answer pattern against the candidate sentence. There are two extra steps performe"
D09-1057,P05-1045,0,0.0485165,"Missing"
D09-1057,N03-2010,0,0.0219607,"Missing"
D09-1057,D08-1097,1,0.410506,"a set of feature represented data points, the maximal entropy model attempts to maximize the log likelihood log P (C|D, λ) = X (c,d)∈(C,D) desc manner reason HUMAN group individual title desc LOC city country mountain other state NUM code count date distance money order other period percent speed temp size weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. The features are briefly described as following. More detailed information can be found at (Huang et al., 2008). P exp i λi fi (c, d) P log P , exp j λj fi (c, d) c′ (1) Question wh-word The wh-word feature is the question wh-word in given questions. For example, the wh-word of question What is the population of China is what. where fi (c, d) are feature indicator functions. We use ME models for both question classification and question answer ranking. In question answer context, such function, for instance, could be the presence or absence of dictionary entities (as presented in Section 5.2) associated with a particular class type (either true or false, indicating a sentence can or cannot answer the q"
D09-1057,P03-1054,0,0.0137218,"ows by the pattern of (number number). 5.5 Dependency validity features when born feature 2 The question begins with when is/was and follows by a person name and then follows by key word born; The candidate sentence contains such person name, a NUM:date entity, and a key word born. Like (Cui et al., 2004), we extract the dependency path from the question word to the common word (existing in both question and sentence), and the path from candidate answer (such as CoNLL NE and numerical entity) to the common word for each pair of question and candidate sentence using Stanford dependency parser (Klein and Manning, 2003; Marneffe et al., 2006). For example, for question When did James Dean die and candidate sentence In 1955, actor James Dean was killed in a two-car collision near Cholame, Calif., we extract the pathes of When:advmod:nsubj:Dean and 1955:prep-in:nsubjpass:Dean for question and sentence respectively, where advmod and nsubj etc. are grammatical relations. We propose the dependency validity feature (DEP) as following. For all paired paths between a question and a candidate sentence, if at least one pair of path in which all pairs of grammatical relations have been seen in the training, then the D"
D09-1057,H05-1040,0,0.175567,"speech tagging, named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. Given a training set of (C, D), where C is a set of class labels and D is a set of feature represented data points, the maximal entropy model attempts to maximize the log likelihood log P (C|D, λ) = X (c,d)∈(C,D) desc manner reason HUMAN group individual title desc LOC city country mountain other state NUM code count date distance money order other period percent speed temp size weight Krishnan et al., 2005; Moschitti et al., 2007). In contrast to Li and Roth (2006)’s approach which makes use of a very rich feature set, we propose to use a compact yet effective feature set. The features are briefly described as following. More detailed information can be found at (Huang et al., 2008). P exp i λi fi (c, d) P log P , exp j λj fi (c, d) c′ (1) Question wh-word The wh-word feature is the question wh-word in given questions. For example, the wh-word of question What is the population of China is what. where fi (c, d) are feature indicator functions. We use ME models for both question classification a"
D09-1057,J03-4003,0,\N,Missing
D09-1128,N07-1030,0,0.0763136,"Missing"
D09-1128,P05-1045,0,0.0092986,"the first word, for example, a or the, but also models the head word, since the head word usually is the last word in the NP. Head word: We use Collins style rules (Collins, 1999) to extract the head words for given NPs. These features should be most informative if the training corpus is large enough.2 For example, the head word company of the NP the company immediately determines its SC being organization. However, due to the sparseness of training data, its potential importance is adversely affected. 3.3 Semantic features NE feature is extracted from Stanford named entity recognizer (NER) (Finkel et al., 2005). Three types of named entities: person, location and organization can be recognized for a given NP. This feature is primarily useful for SC classification of proper nouns. WordNet is a large English lexicon in which semantically related words are connected via cognitive synonyms (synsets). The WordNet is a useful tool for word semantics analysis and has been widely used in natural language processing applications. In WordNet, synsets are organized into hierarchies with hypernym/hyponym relationships: Y is a hypernym of X if every X is a (kind of) Y (X is called a hyponym of Y in this case). T"
D09-1128,P06-1060,0,0.0913263,"es. WordNet is just one of the several knowledge sources which have been utilized. However, the WordNet based features is not informative compared to other features such as the semantic neighbor feature. Similarly, Ponzetto and Strube (2006) have discovered that the WordNet feature is no more informative than the community-generated Wikipedia feature. In this paper, we focus on the investigation of various usages of WordNet for the SC classification task. The work which is directly comparable to ours would be (Ng, 2007a; Ng, 2007b). Other similar work includes the mention detection (MD) task (Florian et al., 2006) and joint probabilistic model of coreference (Daum´e III and Marcu, 2005). The MD task identifies the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, organization). Unlike them, we do not perform the boundary detection, as we make use of the noun phrases directly from the noun phrase chunker and NE recognizer. The joint probabilistic model models the MD and coreference simultaneously, while our work focuses on them separately. 3 Semantic Class Classification In this section, we describe how we compile the training corpus and extract features"
D09-1128,N01-1008,0,0.527712,"Missing"
D09-1128,N04-1037,0,0.189195,"Missing"
D09-1128,P98-2127,0,0.0460765,"Missing"
D09-1128,W06-1633,0,0.600888,"Missing"
D09-1128,P04-1019,0,0.0519931,"Missing"
D09-1128,N06-1025,0,0.755903,"sion tree classifier to label pairs of mentions as coreferent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs. However, the difficulty is to accurately compute the semantic class features. In this paper, we show that the WordNet may no"
D09-1128,P08-4003,0,0.0324742,"entions (in contrast to the gold mentions) are used. To facilitate the comparison with previous work, we report performance using two different scoring metrics: the commonlyused MUC scorer (Vilain et al., 1995) and the accuracy of the anaphoric references (Ponzetto and Strube, 2006). An anaphoric reference is correctly resolved if it and its closest antecedent are in the same coreference chain in the resulting partition. 4.2 Baseline features We briefly review the baseline features used in this paper as follows. More detailed information and implementations can be found at (Soon et al., 2001; Versley et al., 2008). For example, the ALIAS feature takes values of true or false. The value of true means that the antecedent and the anaphor refer to the same entity (date, person, organization or location). The ALIAS feature detection works differently depending on the named entity type. For date, the day, month, and year values are extracted and compared. For person, the last words of the noun phrases are compared. For organization names, the alias detection checks for acronym match such as IBM and International Business Machines Corp. Lexical features STRING MATCH: true if N Pi and N Pj have the same spelli"
D09-1128,M98-1006,0,0.0915819,"Missing"
D09-1128,J00-4003,0,0.019709,"Missing"
D09-1128,P04-1018,0,0.125709,"Missing"
D09-1128,M95-1005,0,0.0209211,"the closest preceding NP that is classified as coreferent with N Pj as the antecedent of N Pj . If no such NP exists, no antecedent is selected for N Pj . Unlike other natural language processing tasks such as information extraction which have de facto evaluation metrics, it is an open question which evaluation is the most suitable one. The evaluation becomes more complicated when automatically extracted mentions (in contrast to the gold mentions) are used. To facilitate the comparison with previous work, we report performance using two different scoring metrics: the commonlyused MUC scorer (Vilain et al., 1995) and the accuracy of the anaphoric references (Ponzetto and Strube, 2006). An anaphoric reference is correctly resolved if it and its closest antecedent are in the same coreference chain in the resulting partition. 4.2 Baseline features We briefly review the baseline features used in this paper as follows. More detailed information and implementations can be found at (Soon et al., 2001; Versley et al., 2008). For example, the ALIAS feature takes values of true or false. The value of true means that the antecedent and the anaphor refer to the same entity (date, person, organization or location)"
D09-1128,J05-3004,0,0.119803,"Missing"
D09-1128,P07-1067,0,0.522373,"Missing"
D09-1128,P07-1068,0,0.42411,"referent or not. Recent work aims to improve the performance from two aspects: new models and new features. The former cast the pair wise mention classifications into various forms such as the best path in a Bell tree (Luo et al., 2004), the best graph cut (Nicolae and Nicolae, 2006), integer linear programming (Denis and Baldridge, 2007) and graph partition based conditional model (McCallum and Wellner, 2004). The latter develop and investigate new linguistic features for the problem. For instance, WordNet (Poesio et al., 2004), Wikipedia (Ponzetto and Strube, 2006), semantic neighbor words (Ng, 2007a), and pattern based features (Yang and Su, 2007) have been extensively studied. Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance (Kehler et al., 2004). An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs. However, the difficulty is to accurately compute the semantic class features. In this paper, we show that the WordNet may not be efficiently employed in the tr"
D09-1128,N03-5008,0,\N,Missing
D09-1128,H05-1013,0,\N,Missing
D09-1128,J03-4003,0,\N,Missing
D09-1128,J96-1002,0,\N,Missing
D09-1128,J01-4004,0,\N,Missing
D09-1128,C98-2122,0,\N,Missing
D14-1223,W12-3504,0,0.0231025,"e.g. the last book)1 . To achieve a natural and accurate human to machine conversation, it is crucial to accurately identify and resolve referring expressions in utterances. As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs. Those that do focus on the impact of the input from multimodal interfaces such as gesture for understanding (Bolt, 1980; Heck et al., 2013; Johnston et al., 2002), touch for ASR error correction (Huggins-Daines and Rudnicky, 2008), or cues from the screen (Balchandran et al., 2008; Anastasiou et al., 2012). Most of these systems are engineered for a specific 1 An item could be anything from a list, e.g. restaurants, games, contact list, organized in different lay-outs on the screen. 2094 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094–2104, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics task, making it harder to generalize for different domains or SDSs. In this paper, we investigate a rather generic contextual model for resolving natural language REs for on-screen item selection to improve conversatio"
D14-1223,W10-0701,0,0.0298155,"Missing"
D14-1223,W09-0609,0,0.0201843,"mber of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has several features to parse the posed question and keep history of the dialog to resolve co-reference issues. Their questionanswering model uses gesture as features to resolve queries such as “what’s the name of that [pointin"
D14-1223,W12-1633,0,0.0149236,"resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has several features to parse the posed question and keep history of the dialog to resolve co-reference issues. Their questionanswering model uses gesture as features to resolve queries such as “what’s the name of that [pointing gesture] player?”, but they do not resolve locational referrals such as “the middle one” or “the second harry potter movie”. Others such as (Funakoshi et al., 2012) resolve anaphoric (“it”) or exophoric (“this one”) types of expressions in user utterances to identify geometric objects. In this paper, we study several types of REs to build a natural and flexible interaction for the user. (Heck et al., 2013) present an intent prediction model enriched with gesture detector to help disambiguate between different user intents related to the interface. In (Misu et al., 2014) a situated incar dialog model is presented to answer drivers’ spoken queries about their surroundings (no display screen). They integrate multi-modal inputs of 2095 speech, geo-location a"
D14-1223,D10-1040,0,0.123759,"Missing"
D14-1223,P08-4005,0,0.0285568,"s shown in Table 1. Note that, there are multiple ways of referring to the same item, (e.g. the last book)1 . To achieve a natural and accurate human to machine conversation, it is crucial to accurately identify and resolve referring expressions in utterances. As important as interpreting referring expressions (REs) is for modern NUI designs, relatively few studies have investigated withing the SDSs. Those that do focus on the impact of the input from multimodal interfaces such as gesture for understanding (Bolt, 1980; Heck et al., 2013; Johnston et al., 2002), touch for ASR error correction (Huggins-Daines and Rudnicky, 2008), or cues from the screen (Balchandran et al., 2008; Anastasiou et al., 2012). Most of these systems are engineered for a specific 1 An item could be anything from a list, e.g. restaurants, games, contact list, organized in different lay-outs on the screen. 2094 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094–2104, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics task, making it harder to generalize for different domains or SDSs. In this paper, we investigate a rather generic contextual model for resol"
D14-1223,W10-4324,0,0.0183448,", among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has several features to parse the posed question and keep history of the dialog to resolve co-reference issues. Their questionanswering model uses gesture as features to resolve queries such as “what’s the name of that [pointing gesture] player?”, but they do not resolve lo"
D14-1223,P03-1054,0,0.00549152,"id display. Also in “third from the ::: last”, the “third” :::: is the column-position, and the “last” is the column-pivot, the pivotal reference of the column in a multi-column grid display. The fourth tag, row-position, is used when the specific row is explicitly referred, such as in “the Harry Potter movie in the ::: first row”. To train our CRF-based SLL model we use three types of features: the current word, window words e.g., previous-word, next-word, etc., using five-window around the current word, and syntactic features from the part-of-speech (POS) tagger using the Stanford’s parser (Klein and Manning, 2003). Row Indicator Feature: This feature sets the relationship between the n-gram in an utterance indicated by the row-position or row-pivot tag and the item’s row number on the screen. For instance, given SSL output row-pivot(’top’) and item’s location row=1, the value of the feature is set to ’1’. If no row tag is found by SLL, this feature is set to ’0’. We use regular expressions to parse the numerical indicators, e.g., ’top’=’1’. Column Indicator Feature: Similarly, this feature indicates if a phrase in utterance indicated by the column-position or column-pivot tag matches the item’s column"
D14-1223,W14-4304,0,0.0303549,"uch as “what’s the name of that [pointing gesture] player?”, but they do not resolve locational referrals such as “the middle one” or “the second harry potter movie”. Others such as (Funakoshi et al., 2012) resolve anaphoric (“it”) or exophoric (“this one”) types of expressions in user utterances to identify geometric objects. In this paper, we study several types of REs to build a natural and flexible interaction for the user. (Heck et al., 2013) present an intent prediction model enriched with gesture detector to help disambiguate between different user intents related to the interface. In (Misu et al., 2014) a situated incar dialog model is presented to answer drivers’ spoken queries about their surroundings (no display screen). They integrate multi-modal inputs of 2095 speech, geo-location and gaze. We investigate a variety of REs for visual interfaces, and analyze automatic resolution in a classification task introducing a wide range of syntactic, semantic and contextual features. We look at how REs change with screen layout comparing different devices. To the best of our knowledge, our work is first to analyze REs from these aspects. 3 Data Crowdsourcing services, such as Amazon Mechanical Tur"
D14-1223,W97-1401,0,0.0320853,"dal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze. Since the first appearance of the Put-That-There system (Bolt, 1980), a number of multimodal systems have been built, among which there are systems that combine speech, pointing (Neal, 1991), and gaze (Koons et al., 1993), systems that engage users in an intelligent conversation (Gustafson et al., 2000). Earlier studies have shown that multimodal interfaces enable users to interact with computers naturally and effectively (Schober and Clark, 1989; Oviatt et al., 1997). Considered as part of the situated interactive frameworks, many work focus on the problem of predicting how the user has resolved REs that is generated by the system, e.g., (Clark and Wilkes-Gibbs, ; Dale and Viethen, 2009; Gieselmann, 2004; Janarthanam and Lemon, 2010; Golland et al., 2010). In this work, focusing on smart devices, we investigate how the system resolves the REs in user utterances to take the next correct action. In (Pfleger and J.Alexandersson, 2006) a reference resolution model is presented for a questionanswering system on a mobile, multi-modal interface. Their system has"
D14-1223,P02-1048,0,\N,Missing
D17-1237,P17-1045,1,0.83128,"we need to book air tickets, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a conversation) are delaye"
D17-1237,W17-5526,0,0.00786429,"Missing"
D17-1237,I17-1074,1,0.409205,"ts, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a conversation) are delayed and sparse. As we"
D17-1237,W11-2033,0,0.125959,"ition (Dietterich, 2000). In this paper, we choose the options framework for its conceptual simplicity and generality (Sutton et al., 1998); more details are found in the next section. Our work is also motivated by hierarchicalDQN (Kulkarni et al., 2016) which integrates hierarchical value functions to operate at different temporal scales. The model achieved superior performance on a complicated ATARI game “Montezuma’s Revenge” with a hierarchical structure. A related but different extension to singledomain dialogues is multi-domain dialogues, where each domain is handled by a separate agent (Lison, 2011; Gasic et al., 2015a,b; Cuay´ahuitl et al., 2016). In contrast to compositedomain dialogues studied in this paper, a conversation in a multi-domain dialogue normally involves one domain, so completion of a task does not require solving sub-tasks in different domains. Consequently, work on multi-domain dialogues focuses on different technical challenges such as transfer learning across different domains (Gasic 2232 et al., 2015a) and domain selection (Cuay´ahuitl et al., 2016). 3 Dialogue Policy Learning Our composite task-completion dialogue agent consists of four components: (1) an LSTMbased"
D17-1237,D15-1199,0,0.0314686,"Missing"
D17-1237,N07-2038,0,0.83697,"A Rule+ Agent requests and informs all the slots in a pre-defined order exhaustedly, and then confirms with the user about the reserved tickets. The average turn of this agent is longer than that of the Rule agent. • A flat RL Agent is trained with a standard flat deep reinforcement learning method (DQN) which learns a flat dialogue policy using extrinsic rewards only. 4.3 User Simulator Training reinforcement learners is challenging because they need an environment to interact with. In the dialogue research community, it is common to use simulated users as shown in Figure 3 for this purpose (Schatzmann et al., 2007; Asri et al., 2016). In this work, we adapted the publiclyavailable user simulator, developed by Li et al. (2016), to the composite task-completion dialogue setting using the human-human conversation data described in Section 4.1.2 During training, the simulator provides the agent with an (extrinsic) reward signal at the end of the dialogue. A dialogue is considered to be successful only when a travel plan is made successfully, and the information provided by the agent satisfies user’s constraints. At the end of each dialogue, the agent receives a positive reward of 2⇤max turn (max turn = 60"
D17-1237,P17-1062,0,0.331325,"r to make a travel plan, we need to book air tickets, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a c"
D17-1237,E06-1032,0,\N,Missing
D17-1237,J09-4008,0,\N,Missing
D17-1237,P02-1040,0,\N,Missing
D17-1237,P09-2025,0,\N,Missing
D17-1237,P14-2074,0,\N,Missing
D17-1237,P15-1044,0,\N,Missing
D17-1237,D16-1230,0,\N,Missing
D17-1237,P16-2043,0,\N,Missing
D17-1237,P16-2008,0,\N,Missing
D17-1237,W16-3622,0,\N,Missing
D17-1237,C16-1105,0,\N,Missing
D17-1237,W17-5525,0,\N,Missing
D17-1237,W16-6644,0,\N,Missing
D17-1237,N16-1086,0,\N,Missing
D19-1159,D14-1162,0,0.0821077,"puting top-k rollouts. 3 P RETRAINED LM S AND S TOCHASTIC S AMPLING 1495 • fθx→e : x → e, where x = [x1 , · · · , xL ] is represented as its (contextualized) word embedding form e = [e1 , · · · , eL ], with ei as the representation for word xi ; • fθe→z : e → z t : For each embedded instruction e, we ground its representations as ci,t for state st via neural attention. To handle language variability, one may aggregate features of multiple instructions Ct = {ci,t }M i=1 1 PM into a single joint feature z t = M c .4 i,t i=1 Previous methods in VLN learn e either from pretrained word embeddings (Pennington et al., 2014) which do not take into account word context, or from scratch. As a result, their representations do not capture contextual information within each instruction. More importantly, they tend to overfit the training instructions associated with seen environments, limiting their utility in unseen environments. To remedy these issues, we propose to represent e with contextualized word embeddings produced using large-scale pretrained language models, such as BERT and GPT. Instruction Encoder. The agent’s memory vector ht−1 captures the perception and action history and is used to attend to the instr"
D19-1159,N19-1268,0,0.10248,". gio et al., 2015). Two widely used training strategies are student-forcing and teacher-forcing (described in detail in Section 2.2). It is well-known that the sequence length determines which training strategy is more effective. In the VLN literature, student-forcing has been widely used, as early work (Anderson et al., 2018) used long trajectories (up to 20 steps) with a simple discrete action space. Most recent work, however, has relied on a panoramic action space (Fried et al., 2018) in which most trajectories are only up to seven steps long. In such cases, teacher-forcing is preferable (Tan et al., 2019). Neither strategy is perfect: teacher-forcing has exposure bias, while studentforcing’s random actions can cause an agent to deviate far from the correct path, rendering the original instruction invalid.2 To tackle these challenges, we have developed two techniques to enable the agent to navigate more efficiently. For the first challenge, we leverage the recent large-scale pretrained language models, BERT (Devlin et al., 2019) and GPT (Radford et al., 2018), to improve the agent’s robustness in unseen environments. We show that large-scale language-only pretraining improves generalization in"
D19-1159,N19-1423,0,\N,Missing
D19-1159,N19-1197,1,\N,Missing
N13-1043,Q13-1005,1,0.800807,"clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012)"
N13-1043,P09-1010,1,0.794158,"(2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand situated non-visual language use. For example, there has been work on learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and un"
N13-1043,P11-1028,0,0.0172845,"learning to execute instructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-gene"
N13-1043,P11-1020,0,0.0176367,"ptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has also received significant attention, where the goal is to learn to understand"
N13-1043,W02-1001,0,0.012853,") These features will allow the model to capture correlations between our feature norms which provide descriptions of visual attributes, like black, and sentimental words, like gothic. Word happi student friend music confid sport casual youth waitress smart fashion monei cool relax game musician parti content friendli smooth S-Independent is used for both word prediction and ranking. For prediction, we train a linear model using averaged binary perceptron. For ranking, we try to rank all positive instances above negative instances. We use an averaged structured perceptron to train the ranker (Collins, 2002). To rank with respect to an entire query q~i , we sum the scores of each word q ∈ q~i . 6.2 Joint Sentimental Model The second approach (S-Joint) jointly models the query words to learn the relationships between literal and sentimental words with score s: s(~a|~q, D) = |~a ||~ q| X X θT f (~ ai , q~j , d~ai ) i=1 j=1 Where every word in the query has a separate factor and every position is treated independently subject to the constraint that ~a is valid. The feature function f uses the same features as the word independent model above. This model is used for ranking and generation. For rankin"
N13-1043,D09-1100,0,0.0174581,"tructions (Branavan et al., 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013), provide sports commentary (Chen et al., 2010), understand high level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal De"
N13-1043,N10-1125,0,0.0254996,"queries are abstract (for example, images about freedom). Finally, in descriptions of people undergoing emotional distress, Fussell and Moss (1998) show that literal descriptions co-occur frequently with sentimental ones. There has been significant work on more literal aspects of grounded language understanding, both visual and non-visual. The WordsEye project (Coyne and Sproat, 2001) generates 3D scenes from literal paragraph-length descriptions. Generating literal textual descriptions of visual scenes has also been studied, including both captions (Kulkarni et al., 2011; Yang et al., 2011; Feng and Lapata, 2010) and descriptions (Farhadi et al., 2010). Furthermore, Chen and Dolan (2011) collected literal descriptions of videos with the goal of learning paraphrases while Zitnick and Parikh (2013) describe a corpus of descriptions for clip art that supports the discovery of semantic elements of visual scenes. There has also been significant recent work on automatically recovering visual attributes, both absolute (Farhadi et al., 2009) and relative (Kovashka et al., 2012), a challenge that we avoid having to solve with our use of feature norms (Mcrae et al., 2005). Grounded language understanding has al"
N13-1043,W10-4210,0,0.0712232,"Missing"
N13-1043,W02-1011,0,0.0137282,"level strategy guides to improve game L ITERAL D ESCRIPTIONS full-sleeved executive blue shirt blue , long-sleeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a facial feature or an accessory and asked them to produce short descriptions. Figu"
N13-1043,P10-1071,0,0.0175316,"eeved button-up shirt mens blue button dress shirt with dark blue stripes multi-blue striped long-sleeve button-up dress shirt with cuffs and breast pocket Table 1: Literal descriptions of shirt in Figure 2. Figure 2: The number of assets per category and example images from the hair, shirt and hat categories. play (Branavan et al., 2011; Eisenstein et al., 2009), and understand referring expression (Matuszek et al., 2012). Finally, our work is similar in spirit to sentiment analysis (Pang et al., 2002), emotion detection from images and speech (Zeng et al., 2009), and metaphor understanding (Shutova, 2010a; Shutova, 2010b). However, we focus on more general visual context. 3 Data Collection We gathered a large number of natural language descriptions from Mechanical Turk (MTurk). They include: (1) literal descriptions of specific facial features, clothing or accessories and (2) high level subjective descriptions of human-generated avatars.3 Literal Descriptions We showed annotators a single image of clothing, a facial feature or an accessory and asked them to produce short descriptions. Figure 2 shows the distribution over object types. We restricted descriptions to be between 3 and 15 words. I"
N13-1043,N10-1147,0,\N,Missing
N13-1043,D11-1041,0,\N,Missing
N16-3010,D14-1223,1,0.78207,"Missing"
N16-3010,W15-4654,0,0.0361476,"Missing"
N18-1016,P05-1018,0,0.054586,"ds to fine-tune neural generation models using automatic measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a wo"
N18-1016,D16-1127,1,0.931436,"gure 1: The generator is rewarded for imitating the discourse structure of the gold sequence. Importantly, most automatic measures are based on local n-gram patterns, providing only a limited and myopic perspective of overall text quality. As a result, while models trained to directly optimize these measures can yield improvements on the same measures, they may not lead to better quality in terms of overall coherence or discourse structure. Indeed, recent studies have reported cases where commonly used measures do not align well with desired aspects of generation quality (Rennie et al., 2017; Li et al., 2016). The challenge, however, is to define a global score that can measure the complex aspects of text quality beyond local n-gram patterns. In this paper, we investigate learning neural rewards and their use in a reinforcement learning regime with a specific focus on learning more discourse-aware and coherent text generation. Our approach shares the spirit of the work of Lowe et al. (2017), where neural scores were learned to approximate human judgments of dialogue quality. The key difference is that our rewards can be fully automatically constructed without requiring human judgments and can be t"
N18-1016,J08-1001,0,0.328444,"Missing"
N18-1016,N04-1015,0,0.102263,"models using automatic measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a word embedding and sj is a sente"
N18-1016,W04-1013,0,0.0121079,"al loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Proceedings of NAACL-HLT 2018, pages 173–184 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics underlying text generator (see"
N18-1016,P09-1068,0,0.0299016,"ilar to our work is work on using neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical resu"
N18-1016,P17-1103,0,0.0265649,"ity in terms of overall coherence or discourse structure. Indeed, recent studies have reported cases where commonly used measures do not align well with desired aspects of generation quality (Rennie et al., 2017; Li et al., 2016). The challenge, however, is to define a global score that can measure the complex aspects of text quality beyond local n-gram patterns. In this paper, we investigate learning neural rewards and their use in a reinforcement learning regime with a specific focus on learning more discourse-aware and coherent text generation. Our approach shares the spirit of the work of Lowe et al. (2017), where neural scores were learned to approximate human judgments of dialogue quality. The key difference is that our rewards can be fully automatically constructed without requiring human judgments and can be trained in an unsupervised manner. More specifically, we propose a neural reward learning scheme that is trained to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many exis"
N18-1016,mori-etal-2014-flow,0,0.0430861,"n et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our teacher-trained generator better models the latent event sequences of cooking recipes,"
N18-1016,D14-1179,0,0.018117,"Missing"
N18-1016,P02-1040,0,0.101245,"ng text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Proceedings of NAACL-HLT 2018, pages 173–184 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics underlying text generator (see Figure 1), which is trained u"
N18-1016,D17-1103,0,0.0163234,"del for the mixed reward was chosen as the one that achieved the highest average geometric mean of BLEU-4 reward and average relative ordering reward for each generated sequence y in the development set: Mixed Training As the model learns parameters to optimize the amount of reward it receives from the teacher, it is not explicity encouraged to produce fluent generations. The model quickly learns to generate simple sequences that exploit the teacher for high rewards despite being incoherent recipes (e.g., Figure 4). Consequently, it is possible that generated sequences are no longer readable (Pasunuru and Bansal, 2017; Paulus et al., 2018). Title: Chili Grits Ingredients: boiling water, butter, shredded cheddar cheese, jalapenos, eggs, chicken cream of soup, salt Generated Recipe: Here . T rb4 (y) X r¯ = rRO (yt ) T (17) t=1 Figure 4: Recipe generated from a self-critical model with no mixed training where rb4 is the BLEU-4 score of the whole generated sequence, and rRO is computed using Equa177 Model Cross-entropy (MLE) BLEU-4 (Rennie et al., 2017) CIDEr (Rennie et al., 2017) ROUGE-L (Paulus et al., 2018) BLEU-1 (γ = 0.97) BLEU-4 (γ = 0.99) CIDEr (γ = 0.97) ROUGE-L (γ = 0.97) Absolute Ordering (AO) Relati"
N18-1016,D15-1114,1,0.861518,"image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our teacher-trained generator better models the latent event sequences o"
N18-1016,D16-1032,1,0.893981,"to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft"
N18-1016,P16-1027,0,0.0227005,"sing neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our tea"
N18-1016,D14-1218,0,0.0229299,"measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a word embedding and sj is a sentence embedding. Each"
N18-1016,D17-1197,0,0.0277131,"nce ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Procee"
N18-1016,D17-1062,0,0.0267216,"0.056 0.054 0.052 0.050 0.048 State Change BLEU-4 0.95 0.97 0.98 0.300 0.295 0.290 0.285 0.280 0.275 Figure 5: Action and State Change BLEU Metrics for different initializations of `max and γ guage model to deteriorate. Interestingly, a higher `max leads to better performance on global coherence scores, implying that relative order rewards conditioned on more sentences allow the model to learn longer-range context co-occurrences. 7 Most similar to our work is work on using neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn"
N18-1016,D15-1044,0,0.164506,"d in an unsupervised manner. More specifically, we propose a neural reward learning scheme that is trained to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning met"
N18-1016,D17-1239,0,0.0387937,"Missing"
N18-1016,1983.tc-1.13,0,0.67195,"Missing"
N18-1150,N16-1012,0,0.139479,"e capable of generating fluent language, variants of encoder-decoder RNNs (Sutskever et al., 2014; Bahdanau et al., 2015) have shown promising results on the abstractive summarization task (Rush et al., 2015; Nallapati et al., 2017). The fundamental challenge, however, is that the strong performance of neural models at encoding short text does not generalize well to long text. The motivation behind our approach is to be able to dynamically attend to different parts of the input to capture salient facts. While recent work in summarization addresses these issues using improved attention models (Chopra et al., 2016), pointer networks with coverage mechanisms (See et al., 2017), and coherence-focused training objectives (Paulus et al., 2018; Jaques et al., 2017), an effective mechanism for representing a long document remains a challenge. Simultaneous work has investigated the use of deep communicating agents (Sukhbaatar et al., 2016) for collaborative tasks such as logic puzzles (Foerster et al., 2016), visual dialog (Das et al., 2017), and reference games (Lazaridou et al., 2016). Our work builds on these approaches to propose the first study on using communicating agents to encode long text for summari"
N18-1150,P16-1188,0,0.222848,"Missing"
N18-1150,W16-3617,0,0.0592334,"Missing"
N18-1150,D15-1166,0,0.153582,"Missing"
N18-1150,P14-5010,0,0.00914565,"Missing"
N18-1150,1983.tc-1.13,0,0.463783,"Missing"
N18-1150,N16-1174,1,0.51722,"capture all the facts in the human summary, while (m7) is able to include all the facts with few extra details, generating more relevant and diverse summaries. 6 Related Work Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the 1669 context that the decoder should focus on within the input encoding (Luong et al., 2015; Vinyals et al., 2015b; Bahdanau et al., 2015). For example, Luong et al. (2015) proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document classification (Yang et al., 2016), sentiment classification (Chen et al., 2016), and dialog response selection (Zhou et al., 2016). Attention mechanisms have shown to be crucial for summarization as well (Rush et al., 2015; Zeng et al., 2016; Nallapati et al., 2017), and pointer networks (Vinyals et al., 2015a), in particular, help address redundancy and saliency in generated summaries (Cheng and Lapata, 2016; See et al., 2017; Paulus et al., 2018; Fan et al., 2017). While we share the same motivation as these works, our work uniquely presents an approach based on CommNet, the deep communicating agent framework (Sukhbaatar et"
N18-1150,D17-1103,0,0.0367234,"h sentence s0q , q=1. . . Q, where s0q ∈{st :yt =‘·’, 1≤t≤T }, are used to compute the cosine similarity between two consecutively generated sentences. To minimize the similarity between end-of-sentence hidden states we define a semantic cohesion loss: 1665 LSEM = PQ 0 0 q=2 cos(sq , sq−1 ) (18) The final training objective is then: LMLE-SEM = LMLE + λLSEM (19) where λ is a tunable hyperparameter. Reinforcement Learning (RL) Loss Policy gradient methods can directly optimize discrete target evaluation metrics such as ROUGE that are non-differentiable (Paulus et al., 2018; Jaques et al., 2017; Pasunuru and Bansal, 2017; Wu et al., 2016). At each time step, the word generated by the model can be viewed as an action taken by an RL agent. Once the full sequence yˆ is generated, it is compared against the ground truth sequence y ∗ to compute the reward r(ˆ y ). Our model learns using a self-critical training approach (Rennie et al., 2016), which learns by exploring new sequences and comparing them to the best greedily decoded sequence. For each training example d, two output sequences are generated: yˆ, which is sampled from the probability distribution at each time step, p(ˆ yt |ˆ y1 . . . yˆt−1 , d), and y˜,"
N18-1150,D14-1162,0,0.0795994,"Missing"
N18-1150,D16-1036,0,0.0187609,"extra details, generating more relevant and diverse summaries. 6 Related Work Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the 1669 context that the decoder should focus on within the input encoding (Luong et al., 2015; Vinyals et al., 2015b; Bahdanau et al., 2015). For example, Luong et al. (2015) proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for document classification (Yang et al., 2016), sentiment classification (Chen et al., 2016), and dialog response selection (Zhou et al., 2016). Attention mechanisms have shown to be crucial for summarization as well (Rush et al., 2015; Zeng et al., 2016; Nallapati et al., 2017), and pointer networks (Vinyals et al., 2015a), in particular, help address redundancy and saliency in generated summaries (Cheng and Lapata, 2016; See et al., 2017; Paulus et al., 2018; Fan et al., 2017). While we share the same motivation as these works, our work uniquely presents an approach based on CommNet, the deep communicating agent framework (Sukhbaatar et al., 2016). Compared to prior multi-agent works on logic puzzles (Foerster et al., 2017), langua"
N18-1150,D15-1044,0,0.839936,"input text. Introduction We focus on the task of abstractive summarization of a long document. In contrast to extractive summarization, where a summary is composed of a subset of sentences or words lifted from the input text as is, abstractive summarization requires the generative ability to rephrase and restructure sentences to compose a coherent and concise summary. As recurrent neural networks (RNNs) are capable of generating fluent language, variants of encoder-decoder RNNs (Sutskever et al., 2014; Bahdanau et al., 2015) have shown promising results on the abstractive summarization task (Rush et al., 2015; Nallapati et al., 2017). The fundamental challenge, however, is that the strong performance of neural models at encoding short text does not generalize well to long text. The motivation behind our approach is to be able to dynamically attend to different parts of the input to capture salient facts. While recent work in summarization addresses these issues using improved attention models (Chopra et al., 2016), pointer networks with coverage mechanisms (See et al., 2017), and coherence-focused training objectives (Paulus et al., 2018; Jaques et al., 2017), an effective mechanism for representi"
N18-1150,P17-1108,0,0.594329,"formance gains. We fix γ = 0.97 for the RL term in Equation (21) and λ = 0.1 for the SEM term in MLE and MIXED training. Additional details are provided in Appendix A.2. Evaluation We evaluate our system using ROUGE-1 (unigram recall), ROUGE-2 (bigram recall) and ROUGE-L (longest common sequence).1 We select the MLE models with the lowest negative log-likelihood and the MLE+RL models with the highest ROUGE-L scores on a sample of validation data to evaluate on the test 1666 1 We use pyrouge (pypi.python.org/pypi/pyrouge/0.1.3). Model SummaRuNNer (Nallapati et al., 2017) graph-based attention (Tan et al., 2017) pointer generator (See et al., 2017) pointer generator + coverage (See et al., 2017) controlled summarization with fixed values (Fan et al., 2017) RL, with intra-attention (Paulus et al., 2018) ML+RL, with intra-attention(Paulus et al., 2018) (m1) MLE, pgen, no-comm (1-agent) (our baseline-1) (m2) MLE+SEM, pgen, no-comm (1-agent) (our baseline-2) (m3) MLE+RL, pgen, no-comm (1-agent) (our baseline-3) (m4) DCA MLE+SEM, pgen, no-comm (3-agents) (m5) DCA MLE+SEM, mpgen, with-comm (3-agents) (m6) DCA MLE+SEM, mpgen, with-comm, with caa (3-agents) (m7) DCA MLE+SEM+RL, mpgen, with-comm, with caa (3-"
N18-1150,D15-1011,0,\N,Missing
N18-1150,P16-1046,0,\N,Missing
N19-1021,W14-3346,0,0.0226809,"choosing a college, and the model learns to generate responses from Caller Bob. The cyclical schedule generated highly diverse answers that cover multiConditional VAE for Dialog We use a cyclical schedule to improve the latent codes in (Zhao et al., 2017), which are key to diverse dialog-response generation. Follow247 ple plausible dialog acts. On the contrary, the responses from the monotonic schedule are limited to repeat plain responses, i.e., “i’m not sure”. Quantitative results are shown in Table 3, using the evaluation metrics from (Zhao et al., 2017). (i) Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified n-gram precision with a length penalty. We use BLEU-1 to 4 as our lexical similarity metric and normalize the score to 0 to 1 scale. (ii) Cosine Distance of Bag-of-word Embedding (Liu et al., 2016): a simple method to obtain sentence embeddings is to take the average or extreme of all the word embeddings in the sentences. We used Glove embedding and denote the average method as A bow and extreme method as E bow. The score is normalized to [0, 1]. Higher values indicate more plausible responses. The BoW indeed reduces the K"
N19-1021,D16-1230,0,0.0326472,"diverse dialog-response generation. Follow247 ple plausible dialog acts. On the contrary, the responses from the monotonic schedule are limited to repeat plain responses, i.e., “i’m not sure”. Quantitative results are shown in Table 3, using the evaluation metrics from (Zhao et al., 2017). (i) Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified n-gram precision with a length penalty. We use BLEU-1 to 4 as our lexical similarity metric and normalize the score to 0 to 1 scale. (ii) Cosine Distance of Bag-of-word Embedding (Liu et al., 2016): a simple method to obtain sentence embeddings is to take the average or extreme of all the word embeddings in the sentences. We used Glove embedding and denote the average method as A bow and extreme method as E bow. The score is normalized to [0, 1]. Higher values indicate more plausible responses. The BoW indeed reduces the KL vanishing issue, as indicated by the increased KL and decreased reconstruction perplexity. When applying the proposed cyclical schedule to CVAE, we also see a reduced KL vanishing issue. Interestingly, it also yields the highest BLEU scores. This suggests that the cy"
N19-1021,J93-2004,0,0.0651737,"rted. Since SA-VAE tends to overfit, we report its best results in row M⇤ . cal schedule (while keeping all other settings the same). The default hyper-parameters of the cyclical schedule are used in all cases unless stated otherwise. We study the impact of hyper-parameters in the SM, and show that larger M can provide higher performance for various R. We show the major results in this section, and put more details in the SM. The monotonic and cyclical schedules are denoted as M and C, respectively. 6.1 Language Modeling We first consider language modeling on the Penn Tree Bank (PTB) dataset (Marcus et al., 1993). Language modeling with VAEs has been a challenging problem, and few approaches have been shown to produce rich generative models that do not collapse to standard language models. Ideally a deep generative model trained with variational inference would pursue higher ELBO, making use of the latent space (i.e., maintain a nonzero KL term) while accurately modeling the underlying distribution (i.e., lower reconstruction errors). We implemented different schedules based on the code4 published by Kim et al. (2018). The latent variable is 32-dimensional, and 40 epochs are used. We compare the propo"
N19-1021,D16-1031,0,0.04067,"cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training. 1 Introduction Variational autoencoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) have been applied in many NLP tasks, including language modeling (Bowman et al., 2015; Miao et al., 2016), dialog response generation (Zhao et al., 2017; Wen et al., 2017), semi-supervised text classification (Xu et al., 2017), controllable text generation (Hu et al., 2017), and text compression (Miao and Blunsom, 2016). A prominent component of a VAE is the distribution-based latent representation for text sequence observations. This flexible representation allows the VAE to explicitly model holistic properties of sentences, such as style, topic, and high-level linguistic and semantic features. Samples from the prior latent distribution can produce ⇤ Corresponding author † Equal Contribution 240 Proceedings of NAACL-HLT 2019, pages 240–250 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics vanishing issue, and develop an understanding of the strengths and weaknes"
N19-1021,P17-1061,0,0.139526,"med sentences through simple deterministic decoding (Bowman et al., 2015). Due to the sequential nature of text, an autoregressive decoder is typically employed in the VAE. This is often implemented with a recurrent neural network (RNN); the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) RNN is used widely. This introduces one notorious issue when a VAE is trained using traditional methods: the decoder ignores the latent variable, yielding what is termed the KL vanishing problem. Several attempts have been made to ameliorate this issue (Yang et al., 2017; Dieng et al., 2018; Zhao et al., 2017; Kim et al., 2018). Among them, perhaps the simplest solution is monotonic KL annealing, where the weight of the KL penalty term is scheduled to gradually increase during training (Bowman et al., 2015). While these techniques can effectively alleviate the KL-vanishing issue, a proper unified theoretical interpretation is still lacking, even for the simple annealing scheme. In this paper, we analyze the variable dependency in a VAE, and point out that the autoregressive decoder has two paths (formally defined in Section 3.1) that work together to generate text sequences. One path is conditione"
P09-1081,P06-1017,0,0.0203008,"d cast the inference recognition as classification problem to determine if a question text follows candidate text. One of the challenges we face with is that we have very limited amount of labeled data, i.e., correctly labeled (true/false entailment) sentences. Recent research indicates that using labeled and unlabeled data in semi-supervised learning (SSL) environment, with an emphasis on graph-based methods, can improve the performance of information extraction from data for tasks such as question classification (Tri et al., 2006), web classification (Liu et al., 2006), relation extraction (Chen et al., 2006), passage-retrieval (Otterbacher et al., 2009), various natural language processing tasks such as partof-speech tagging, and named-entity recognition (Suzuki and Isozaki, 2008), word-sense disamWe present a graph-based semi-supervised learning for the question-answering (QA) task for ranking candidate sentences. Using textual entailment analysis, we obtain entailment scores between a natural language question posed by the user and the candidate sentences returned from search engine. The textual entailment between two sentences is assessed via features representing high-level attributes of the"
P09-1081,P06-1112,0,0.0938065,"Missing"
P09-1081,P08-1076,0,0.0275144,"y limited amount of labeled data, i.e., correctly labeled (true/false entailment) sentences. Recent research indicates that using labeled and unlabeled data in semi-supervised learning (SSL) environment, with an emphasis on graph-based methods, can improve the performance of information extraction from data for tasks such as question classification (Tri et al., 2006), web classification (Liu et al., 2006), relation extraction (Chen et al., 2006), passage-retrieval (Otterbacher et al., 2009), various natural language processing tasks such as partof-speech tagging, and named-entity recognition (Suzuki and Isozaki, 2008), word-sense disamWe present a graph-based semi-supervised learning for the question-answering (QA) task for ranking candidate sentences. Using textual entailment analysis, we obtain entailment scores between a natural language question posed by the user and the candidate sentences returned from search engine. The textual entailment between two sentences is assessed via features representing high-level attributes of the entailment problem such as sentence structure matching, question-type named-entity matching based on a question-classifier, etc. We implement a semi-supervised learning (SSL) a"
P09-1081,P06-1114,0,0.641982,"at Berkeley Berkeley, CA, 94720 marcus.2.thint@bt.com Berkeley, CA, 94720 asli@berkeley.edu zhiheng@eecs.berkeley.edu Abstract glish, whereby the answer is a short string that indicates a fact, usually a named entity. A typical QA system has a pipeline structure starting from extraction of candidate sentences to ranking true answers. In order to improve QA systems’ performance many research focus on different structures such as question processing (Huang et al., 2008), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006), textual entailment (TE) (Harabagiu and Hickl, 2006) for ranking, answer extraction, etc. Our QA system has a similar pipeline structure and implements a new TE module for information extraction phase of the QA task. TE is a task of determining if the truth of a text entails the truth of another text (hypothesis). Harabagui and Hickl (2006) has shown that using TE for filtering or ranking answers can enhance the accuracy of current QA systems, where the answer of a question must be entailed by the text that supports the correctness of this answer. We derive information from pair of texts, i.e., question as hypothesis and candidate sentence as t"
P09-1081,D08-1097,1,0.867605,"rch Centre EECS Department University of California British Telecom (BT Americas) University of California at Berkeley Jacksonville, FL 32256, USA at Berkeley Berkeley, CA, 94720 marcus.2.thint@bt.com Berkeley, CA, 94720 asli@berkeley.edu zhiheng@eecs.berkeley.edu Abstract glish, whereby the answer is a short string that indicates a fact, usually a named entity. A typical QA system has a pipeline structure starting from extraction of candidate sentences to ranking true answers. In order to improve QA systems’ performance many research focus on different structures such as question processing (Huang et al., 2008), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006), textual entailment (TE) (Harabagiu and Hickl, 2006) for ranking, answer extraction, etc. Our QA system has a similar pipeline structure and implements a new TE module for information extraction phase of the QA task. TE is a task of determining if the truth of a text entails the truth of another text (hypothesis). Harabagui and Hickl (2006) has shown that using TE for filtering or ranking answers can enhance the accuracy of current QA systems, where the answer of a question must be entailed by"
P09-1081,P03-1054,0,0.00377695,"ay, month; various measurements such as weight, money, percentage; contact information like address, web-page, phone-number, etc. This is one of the fundamental layers of information extraction of our QA system. The NER module is based on a combination of user defined rules based on Lesk word disambiguation (Lesk, 1988), WordNet (Miller, 1995) lookups, and many userdefined dictionary lookups, e.g. renown places, people, job types, organization names, etc. During the NER extraction, we also employ phrase analysis based on our phrase utility extraction method using Standford dependency parser ((Klein and Manning, 2003)). We can categorize entities up to 6 coarse and 50 fine categories to match them with the NER types from QC module. Feature Extraction for Entailment Implementation of different TE models has previously shown to improve the QA task using supervised learning methods (Harabagiu and Hickl, 2006). We present our recent work on the task of QA, wherein systems aim at determining if a text returned by a search engine contains the correct answer to the question posed by the user. The major categories of information extraction produced by our QA system characterizes features for our TE model based on"
P09-1081,P05-1049,0,0.0737878,"Missing"
P10-1084,P06-1039,0,0.411341,"rid model can produce coherent and non-redundant summaries. 2 Background and Motivation There are many studies on the principles governing multi-document summarization to produce coherent and semantically relevant summaries. Previous work (Nenkova and Vanderwende, 2005; Conroy et al., 2006), focused on the fact that frequency of words plays an important factor. While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; Daum´ eIII and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable improvements. Our work builds on both methods by constructing a hybrid approach to summarization. Our objective is to discover from document clusters, the latent topics that are organized into hierarchies following (Haghighi and Vanderwende, 2009). A hierarchical model is particularly appealing to summarization than a ”flat” model, e.g. LDA (Blei et al., 2003b), in that one can discover ”abstract” and ”specific” topics. For instance, discovering that ”baseball” and ”football” are both contained in an abstract class ”sp"
P10-1084,N04-1015,0,0.0190875,"aluations confirm that our hybrid model can produce coherent and non-redundant summaries. 2 Background and Motivation There are many studies on the principles governing multi-document summarization to produce coherent and semantically relevant summaries. Previous work (Nenkova and Vanderwende, 2005; Conroy et al., 2006), focused on the fact that frequency of words plays an important factor. While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; Daum´ eIII and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable improvements. Our work builds on both methods by constructing a hybrid approach to summarization. Our objective is to discover from document clusters, the latent topics that are organized into hierarchies following (Haghighi and Vanderwende, 2009). A hierarchical model is particularly appealing to summarization than a ”flat” model, e.g. LDA (Blei et al., 2003b), in that one can discover ”abstract” and ”specific” topics. For instance, discovering that ”baseball” and ”football” are both contai"
P10-1084,W04-1013,0,0.0674545,"d summary, we incrementally add onto the summary the highest ranked sentence om and check if om significantly repeats the information already included in the summary until the algorithm reaches word count limit. We use a word overlap measure between sentences normalized to sentence length. A om is discarded if its similarity to any of the previously selected sentences is greater than a threshold identified by a greedy search on the training dataset. 6 We applied feature extraction of § 5.1 to compile the training and testing datasets. ROUGE is used for performance measure (Lin and Hovy, 2003; Lin, 2004), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries. We use R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams). Experiment 1: sumHLDA Parameter Analysis: In sumHLDA we introduce a prior different than the standard nested CRP (nCRP). Here, we illustrate that this prior is practical in learning hierarchical topics for summarization task. We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in document clust"
P10-1084,N09-1041,0,\N,Missing
P10-1084,P06-2020,0,\N,Missing
P10-1084,N03-1020,0,\N,Missing
P10-1084,P08-1036,0,\N,Missing
P11-1050,N04-1015,0,0.0207722,"ch has demonstrated the usefulness of sentence extraction for summarization based on lexical, semantic, and discourse constraints. Such models often rely on different approaches including: identifying important keywords (Nenkova et al., 2006); topic signatures based on user queries (Lin and Hovy, 2002; Conroy et al., 2006; Harabagiu et al., 2007); high frequency content word feature based learning (Nenkova and Vanderwende, 2005a; Nenkova and Vanderwende, 2005b), to name a few. Recent research focusing on the extraction of latent concepts from document clusters are close in spirit to our work (Barzilay and Lee, 2004; Daum´ eIII and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Wang et al., 2009). Some of these work (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010) focus on the discovery of hierarchical concepts from documents (from abstract to specific) using extensions of hierarchal topic models (Blei et al., 2004) and reflect this hierarchy on the sentences. Hierarchical concept learning models help to discover, for instance, that ”baseball” and ”football” are both contained in a general class ”sports”, so that the summaries reference terms related to more abstract co"
P11-1050,P99-1071,0,0.0931687,"eir correlations to extract topically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text 491 taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daum´ e-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topi"
P11-1050,P10-1084,1,0.861572,"hes including: identifying important keywords (Nenkova et al., 2006); topic signatures based on user queries (Lin and Hovy, 2002; Conroy et al., 2006; Harabagiu et al., 2007); high frequency content word feature based learning (Nenkova and Vanderwende, 2005a; Nenkova and Vanderwende, 2005b), to name a few. Recent research focusing on the extraction of latent concepts from document clusters are close in spirit to our work (Barzilay and Lee, 2004; Daum´ eIII and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Wang et al., 2009). Some of these work (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010) focus on the discovery of hierarchical concepts from documents (from abstract to specific) using extensions of hierarchal topic models (Blei et al., 2004) and reflect this hierarchy on the sentences. Hierarchical concept learning models help to discover, for instance, that ”baseball” and ”football” are both contained in a general class ”sports”, so that the summaries reference terms related to more abstract concepts like ”sports”. Although successful, the issue with concept learning methods for summarization is that the extracted sentences usually contain correlated concepts. We need a model"
P11-1050,P06-2020,0,0.053385,"ries discussed in §6. Our models achieve comparable qualitative results on summarization of multiple newswire documents. Human evaluations of generated summaries confirm that our model can generate non-redundant and topically coherent summaries. 2 Multi-Document Summarization Models Prior research has demonstrated the usefulness of sentence extraction for summarization based on lexical, semantic, and discourse constraints. Such models often rely on different approaches including: identifying important keywords (Nenkova et al., 2006); topic signatures based on user queries (Lin and Hovy, 2002; Conroy et al., 2006; Harabagiu et al., 2007); high frequency content word feature based learning (Nenkova and Vanderwende, 2005a; Nenkova and Vanderwende, 2005b), to name a few. Recent research focusing on the extraction of latent concepts from document clusters are close in spirit to our work (Barzilay and Lee, 2004; Daum´ eIII and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Wang et al., 2009). Some of these work (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010) focus on the discovery of hierarchical concepts from documents (from abstract to specific) using extensions of hie"
P11-1050,P06-1039,0,0.256303,"opically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text 491 taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daum´ e-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involve"
P11-1050,D08-1035,0,0.0784198,"nces. Prior research has demonstrated the usefulness of sentence extraction for generating summary text 491 taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daum´ e-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involves identifying key concepts, the"
P11-1050,N09-1041,0,0.767879,"491 taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daum´ e-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involves identifying key concepts, the relationships between these concepts, and linking these relationships into a hierarchy. In this paper, w"
P11-1050,N06-2046,0,0.0200565,"r’s query addressing different levels of detail. Recent approaches to the summarization task has somewhat focused on the redundancy and coherence issues. In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences. Prior research has demonstrated the usefulness of sentence extraction for generating summary text 491 taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daum´ e-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than s"
P11-1050,P09-2075,0,0.255258,"action for generating summary text 491 taking advantage of surface level features such as word repetition, position in text, cue phrases, etc, (Radev, 2004; Nenkova and Vanderwende, 2005a; Wan and Yang, 2006; Nenkova et al., 2006). Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daum´ e-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. Nonetheless, all these systems crucially rely on extracting various levels of generality from documents, focusing little on redundancy and coherence issues in model building. A model than can focus on both issues is deemed to be more beneficial for a summarization task. Topical coherence in text involves identifying key concepts, the relationships between these concepts, and linking these r"
P11-1050,C00-1072,0,\N,Missing
P12-1035,W04-3003,0,0.0551261,"nent is a challenging task not only because there are no a priori constraints on what a user might say, but also systems must generalize from a tractably small amount of labeled training data. In this paper, we argue that each of these components are interdependent and should be modeled simultaneously. We build a joint understanding framework and introduce a multi-layer context model for semantic representation of utterances of multiple domains. Although different strategies can be applied, typically a cascaded approach is used where each semantic component is modeled separately/sequentially (Begeja et al., 2004), focusing less on interrelated aspects, i.e., dialog’s domain, user’s intentions, and semantic tags that can be shared across domains. Recent work on SLU (Jeong and Lee, 2008; Wang, 2010) presents joint modeling of two components, i.e., the domain and slot or dialog act and slot components together. Furthermore, most of these systems rely on labeled training utterances, focusing little on issues such as information sharing between the discourse and word level components across different domains, or variations in use of language. To deal with de330 Proceedings of the 50th Annual Meeting of the"
P12-1035,P06-1039,0,0.0575331,"tion of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 & §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daum´ e-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework 331 is to model its semantic compon"
P12-1035,E09-1024,0,0.0199052,"rior to design a novel utterance understanding model in §3 & §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daum´ e-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework 331 is to model its semantic components separately, assuming that the context (domain) is given a priori. Earli"
P12-1035,P06-1114,0,0.0219595,"Bayesian framework for semantic parsing of natural language (NL) utterances in a unifying framework in §4, (ii) representation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 & §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daum´ e-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, et"
P12-1035,P10-1136,0,0.176359,"i) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daum´ e-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework 331 is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a classifi"
P12-1035,P11-1060,0,0.0471928,"antic parsing of natural language (NL) utterances in a unifying framework in §4, (ii) representation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 & §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daum´ e-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the corre"
P12-1035,W10-2607,0,0.0288136,"a sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework 331 is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a classification task to capture the user’s intentions (Margolis et al., 2010) and slot filling as a sequence learning task specific to a given domain class (Wang et al., 2009; Li, 2010). Since these tasks are considered as a pipeline, the errors of each component are transfered to the next, causing robustness issues. Ideally, these components should be modeled simultaneously considering the dependencies between them. For example, in a local domain application, users may require information about a sub-domain (movies, hotels, etc.), and for each sub-domain, they may want to take different actions (find a movie, call a restaurant or book a hotel) using domain specific at"
P12-1035,D09-1026,0,0.01027,"owledge. ‡ Here HMM assumption over utterance words is used. In hierarchical topic models (Blei et al., 2003; Mimno et al., 2007), etc., topics are represented as distributions over words, and each document expresses an admixture of these topics, both of which have symmetric Dirichlet (Dir) prior distributions. Symmetric Dirichlet distributions are often used, since there is typically no prior knowledge favoring one component over another. In the topic model literature, such constraints are sometimes used to deterministically allocate topic assignments to known labels (Labeled Topic Modeling (Ramage et al., 2009)) or in terms of pre-learnt topics encoded as prior knowledge on topic distributions in documents (Reisinger and Pas¸ca, 2009). Similar to previous work, we define a latent topic per each known semantic component label, e.g., five domain topics for five defined domains. Different from earlier work though, we also inject knowledge that we extract from several resources including entity lists from web search query click logs as well as seed labeled training utterances as prior information. We constrain the generation of the semantic components of our model by encoding prior knowledge in terms of"
P12-1035,P09-1070,0,0.0485326,"Missing"
P12-1035,P11-1120,0,0.0198551,"son of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daum´ e-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework 331 is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a classification task to capture the u"
P12-1035,P10-1122,0,0.0125332,"(NL) utterances in a unifying framework in §4, (ii) representation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 & §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daum´ e-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approa"
P12-1035,P11-1036,0,0.0317206,"associated with d = 1..KD multinod . Each domain d, mial domain-topic distributions θD is represented as a distribution over a = 1, .., KA da (θ d → θ da ). In our MCM model, we dialog acts θA D A assume that each utterance is represented as a hidden Markov model with KS slot states. Each state generates n-grams according to a multinomial n-gram distribution. Once domain Du and act Aud topics are sampled for u, a slot state topic Sujd is drawn to generate each segment wuj of u by considering the word-tag sequence frequencies based on a simple HMM assumption, similar to the content models of (Sauper et al., 2011). Initial and transition probability distributions over the HMM states are sampled from Dirichlet distribution over slots θSds . Each slot state s generates words according to multinomial word distribution φsS . We also keep track of the frequency of vocabulary terms wj ’s in a V ×KD matrix MD . Every time a wj is sampled for a domain d, we increment its count, a degree of domain bearing 333 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: for each domain d ← 1, ..., KD d ? † draw domain dist. θD ∼ Dir(αD ) , for each dialog-act a ← 1, ..., KA da ? draw dialog act dist. θA ∼ Dir(αA ), fo"
P13-1090,D07-1109,0,0.0384019,"A common property of several context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”wha"
P13-1090,J92-4003,0,0.687657,"es of sampling words from the correct semantic tag. MTR constrains the generation of a tag si given the previous tag si−1 and the current wi based on cj,i by using a vocabulary specific Beta prior, ψv ∼Beta(ηv ) 1 , on each word in vocabulary wv=1,..V . We inject the prior information on semantic tags to define values of the base measure ηv using external knowledge from two sources: (a) Entity Priors (ηS ): Prior probability on named-entities and descriptive tags denoted as (II) Semantic Clustering. A common property of several context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010"
P13-1090,W10-2608,0,0.175697,"ally, a semantic tagging model require large amount of domain specific data to achieve good 914 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 914–923, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cally labeled target data, it re-trains a new CRFmodel. Although our iterative SSL learning model can deal with the training and test data mismatch, it neglects the performance effects caused by adapting the source domain to the target domain. In fact, most SSL methods used for adaptation, e.g., (Zhu, 2005), (Daum´ e-III, 2010), (Subramanya et al., 2010), etc., do not emphasize this issue. With this in mind, we introduce a new iterative training algorithm, Retrospective Learning, as our second contribution. While retrospective learning iteratively trains CRF models with the automatically annotated target data (explained above), it keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. In short, through a series of experiments we show how MTR clustering provides additional information to SSL on the target domain utterances, and greatly impacts semanti"
P13-1090,N09-1032,0,0.0240209,"ive tags denoted as (II) Semantic Clustering. A common property of several context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to s"
P13-1090,P06-1063,0,0.0201079,"Missing"
P13-1090,P10-1116,0,0.0240466,"al context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary]genre"
P13-1090,P10-1136,0,0.0288695,"[Named Entities] director: James Cameron, Woody Allen,... actor: Ryan Gosling, Woody Allen,... title: Manhattan, Midnight in Paris,... [Descriptive Tags] restriction: similar, suitable, free,rate,... description: oscar winning, new release, gardening,... genre: spooky, comedies, feel good, romance,... Table 1: Samples of semantically tagged utterances from movie domain, named-entities and descriptive tags. ing the need for significant manual labor (Tur and DeMori, 2011). Recent work on similar tasks overcome these challenges using SSL methods as follows: • (Wang et al., 2009; Li et al., 2009; Li, 2010; Liu et al., 2011) investigate web query tagging using semi-supervised sequence models. They extract semantic lexicons from unlabeled web queries, to use as features. Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to achieving go"
P13-1090,J93-2004,0,0.0445574,"function to have a dependency on the prior model predictions. Thus, R-SSL encodes the history of the prior pre919 and contains a test-set vocabulary that is twice as large as the one in the development set. As for unlabeled data we crawled the web and collected around 100,000 questions that are similar in style and length to the ones in QuestionBank, e.g. ”wh” questions. There are 36 different tag sets in the Penn dataset which includes tag labels for verbs, nouns, adjectives, adverbs, modal, determiners, prepositions, etc. More information about the Penn Tree-bank tag set can be found here (Marcus et al., 1993). interact with the media system as if they were talking to a person. Our data from target domain is internally collected from real-use scenarios of our spoken dialog system. The transcribed text forms of these utterances are obtained from speech recognition engine. Although the crowd-sourced data is similar to target domain, in terms of pre-defined user intentions, the target domain contains more descriptive vocabulary, which is almost twice as large as the source domain. This causes data-mismatch issues and hence provides a perfect test-bed for a domain adaptation task. In total, our corpus"
P13-1090,D10-1020,0,0.0257542,"h as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary]genre movies by [Hitchcock]director ?”. In LDA, common words tend to dominate all topics causing related words to end up in different topics. In (Petterson et al., 2010), the vectorbased features of words are used as prior information in LDA"
P13-1090,P12-1009,0,0.0263526,"Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary]genre movies by [Hitchcock]director ?”. In LDA, common words tend to dominate all topics causing related words to end up in"
P13-1090,D11-1130,0,0.0149871,"rent distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to achieving good performance across domains. Recent adaptation methods for SSL use: expectation minimization (Daum´ e-III, 2010) graph-based learning (Chapelle et al., 2006; Zhu, 2005), etc. In (Subramanya et al., 2010) an efficient iterative SSL method is described for syntactic tagging, using graph-based learning to smooth POS tag posteriors. However, (Reisinger and Mooney, 2011) argues that vector space models, such as graph-learning, may fail to capture the richness of word meaning, as similarity is not a globally consistent metric. Rather than graph-learning, we present a new SSL using a probabilistic model, MTR, to cluster words based on co-occurrence statistics. • Most iterative SSL methods, do not keep track of the errors made, nor consider the divergence from the original model. (Lavoie et al., 2011) argues that iterative learning models should mitigate new errors made by the model at each iteration by Related Work and Motivation (I) Semi-Supervised Tagging. Su"
P13-1090,N10-1009,0,0.0130897,"rown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary]genre movies by [Hitchcock]director ?”. In LDA, common words tend to dominate"
P13-1090,D10-1017,0,0.216536,"c tagging model require large amount of domain specific data to achieve good 914 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 914–923, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cally labeled target data, it re-trains a new CRFmodel. Although our iterative SSL learning model can deal with the training and test data mismatch, it neglects the performance effects caused by adapting the source domain to the target domain. In fact, most SSL methods used for adaptation, e.g., (Zhu, 2005), (Daum´ e-III, 2010), (Subramanya et al., 2010), etc., do not emphasize this issue. With this in mind, we introduce a new iterative training algorithm, Retrospective Learning, as our second contribution. While retrospective learning iteratively trains CRF models with the automatically annotated target data (explained above), it keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. In short, through a series of experiments we show how MTR clustering provides additional information to SSL on the target domain utterances, and greatly impacts semantic tagging performance. Spec"
P17-5004,W13-4073,0,0.0263856,"ponse Natural Language Generation (NLG) Where are you located? Dialogue Management (DM) • Dialogue State Tracking (DST) • Dialogue Policy Optimization System Action / Policy request_location Backend Knowledge Providers Figure 1: Pipeline framework of spoken dialog system. W S I find action ↓ ↓ O B-genre find movie movies ↓ O this ↓ B-date weekend ↓ I-date forcement learning setting. Dialogue Management The state-of-the-art dialog managers focus on monitoring the dialog progress by neural dialog state tracking models. Among the initial models are the RNN based dialog state tracking approaches (Henderson et al., 2013) that has shown to outperform Bayesian networks (Thomson and Young, 2010). More recent work on Neural Dialog Managers that provide conjoint representations between the utterances, slot-value pairs as well as knowledge graph representations (Wen et al., 2016; Mrkˇsi´c et al., 2016) demonstrate that using neural dialog models can overcome current obstacles of deploying dialogue systems in larger dialog domains. Figure 2: An example utterance with annotations of semantic slots in IOB format (S) and intent (I), B-date and I-date denote the date slot. 4 Deep Learning Based Dialogue System With the"
P17-5004,I17-1074,1,0.888554,"chitecture can be merged with CRFs (Xu and Sarikaya, 2013). Yao et al. (2013) and Mesnil et al. (2015) later employed RNNs for sequence labeling in order to perform slot filling. Such architectures have later been extended to jointly model intent detection and slot filling in multiple domains (Hakkani-T¨ur et al., 2016; Jaech et al., 2016). End-to-end memory networks have been shown to provide a good mechanism for integrating longer term knowledge context and shorter term dialogue context into these models (Chen et al., 2016b,c). In addition, the importance of the LU module is investigated in Li et al. (2017a), where different types of errors from LU may degrade the whole system performance in an reinNatural Language Generation The RNNbased models have been applied to language generation for both chit-chat and task-orientated dialogue systems (Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence planning and surface realization, and language variation can be easily achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism for controlling the"
P17-5004,P17-1163,0,0.078703,"Missing"
P17-5004,W15-4639,0,0.0489578,"Missing"
P17-5004,J80-3005,0,0.746031,"Missing"
P17-5004,D15-1199,0,0.0125654,"ultiple domains (Hakkani-T¨ur et al., 2016; Jaech et al., 2016). End-to-end memory networks have been shown to provide a good mechanism for integrating longer term knowledge context and shorter term dialogue context into these models (Chen et al., 2016b,c). In addition, the importance of the LU module is investigated in Li et al. (2017a), where different types of errors from LU may degrade the whole system performance in an reinNatural Language Generation The RNNbased models have been applied to language generation for both chit-chat and task-orientated dialogue systems (Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned data by jointly optimizing sentence planning and surface realization, and language variation can be easily achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism for controlling the dialogue act during generation in order to avoid semantics repetition, showing promising results. 5 Recent Trends and Challenges on Learning Dialogues This part will focus on discussing the recent trends and current challenges on dialogue system technology. 10 End-to-End Learnin"
P17-5004,W16-3601,0,0.0243641,"tion, showing promising results. 5 Recent Trends and Challenges on Learning Dialogues This part will focus on discussing the recent trends and current challenges on dialogue system technology. 10 End-to-End Learning for Dialogue System With the power of neural networks, there are more and more attempts for learning dialogue systems in an end-to-end fashion. Different learning frameworks are applied, including supervised learning and reinforcement learning. This part will discuss the work about end-to-end learning for dialogues (Dhingra et al., 2016; Wen et al., 2016; Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Li et al., 2017b). Recent advance of deep learning has inspired many applications of neural models to dialogue systems. Wen et al. (2016) and Bordes and Weston (2016) introduced a network-based end-to-end trainable task-oriented dialogue system, which treated dialogue system learning as the problem of learning a mapping from dialogue histories to system responses, and applied an encoder-decoder model to train the whole system. However, the system is trained in a supervised fashion, thus requires a lot of training data, and may not be able to explore the unknown space that does not exist in t"
P19-1011,D17-1254,1,0.828278,"ained from a large amount of data have been shown to be effective when transferred to a wide range of downstream tasks. Prior work along this line can be roughly divided into two categories: i) pre-trained models that require fine-tuning on the specific transferring task (Dai and Le, 2015; Ruder and Howard, 2018; Radford et al., 2018; Devlin et al., 2018; Cer et al., 2018); ii) methods that extract general-purpose sentence embeddings, which can be effectively applied to downstream NLP tasks without finetuning the encoder parameters (Kiros et al., 2015; Hill et al., 2016; Jernite et al., 2017; Gan et al., 2017; Adi et al., 2017; Logeswaran and Lee, 2018; Pagliardini et al., 2018; Tang and de Sa, 2018). Our proposed methods belong to the second category and provide a generic and easy-to-use encoder to extract highly informative sentence representations. However, our work is unique since the embeddings inferred from our models are binarized and compact, and thus possess the advantages of small memory footprint and much faster sentence retrieval. Learning memory-efficient embeddings with deep neural networks has attracted substantial attention recently. One general strategy towards this goal is to ext"
P19-1011,N16-1162,0,0.162075,"h the inner product operation between continuous embeddings. Detailed analysis and case study further validate the effectiveness of proposed methods. 1 Introduction Learning general-purpose sentence representations from large training corpora has received widespread attention in recent years. The learned sentence embeddings can encapsulate rich prior knowledge of natural language, which has been demonstrated to facilitate a variety of downstream tasks (without fine-tuning the encoder weights). The generic sentence embeddings can be trained either in an unsupervised manner (Kiros et al., 2015; Hill et al., 2016; Jernite et al., 2017; Gan ∗ Equal contribution. 107 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 107–116 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics sentations performs on par or even better than calculating the cosine similarity between their continuous counterparts (which is less computationallyefficient). pre-trained generic continuous embeddings to their binary forms. Our exploration spans from simple operations, such as a random projection, to deep neural network models, such as a regulariz"
P19-1011,D15-1075,0,0.0345958,"on semantic hashing, which aims to learn binary Moreover, on several sentence matching benchmarks, we demonstrate that the relatedness between a sentence pair can be evaluated by simply calculating the Hamming distance between their binary codes, which perform on par with or even superior than measuring the cosine similarity between continuous embeddings (see Table 1). Note that computing the Hamming distance is much more computationally efficient than the inner product operation in a continuous space. We further perform a K-nearest neighbor sentence retrieval experiment on the SNLI dataset (Bowman et al., 2015), and show that those semanticallysimilar sentences can indeed be efficiently retrieved with off-the-shelf binary sentence representations. Summarizing, our contributions in this paper are as follows: i) to the best of our knowledge, we conduct the first systematic exploration on learning general-purpose binarized (memory-efficient) sentence representations, and four different strategies are proposed; ii) an autoencoder architecture with a carefullydesigned semantic-preserving loss exhibits strong empirical results on a set of downstream NLP tasks; iii) more importantly, we demonstrate, on sev"
P19-1011,D18-1524,0,0.154582,"in the database needs to be compared, and the inner product operation is computationally involved. These two disadvantages hinder the applicability of generic sentence representations to mobile devices, where a relatively tiny memory footprint and low computational capacity are typically available (Ravi and Kozareva, 2018). In this paper, we aim to mitigate the above issues by binarizing the continuous sentence embeddings. Consequently, the embeddings require much smaller footprint, and similar sentences can be obtained by simply selecting those with closest binary codes in the Hamming space (Kiros and Chan, 2018). One simple idea is to naively binarize the continuous vectors by setting a hard threshold. However, we find that this strategy leads to significant performance drop in the empirical results. Besides, the dimension of the binary sentence embeddings cannot be flexibly chosen with this strategy, further limiting the practice use of the direct binarization method. In this regard, we propose three alternative strategies to parametrize the transformation from Vector representations of sentences, trained on massive text corpora, are widely used as generic sentence embeddings across a variety of NLP"
P19-1011,L18-1269,0,0.0342114,"FF employs FastText (F) embeddings with Fixed (F) padding. The empirical results of InferLite with different lengths of binary embeddings, i.e., 256, 1024 and 4096, are considered. For the sentence matching benchmarks, to allow fair comparison with the continuous embeddings, we do not use the same classifier architecture in SentEval. Instead, we obtain the predicted relatedness by directly computing the cosine similarity between the continuous embeddings. Consequently, there are no classifier parameters for both the binary and continuous representations. The same valuation metrics in SentEval(Conneau and Kiela, 2018) are utilized for all the tasks. For MRPC, the predictions are made by simply judging whether a sentence pair’s score is larger or smaller than the averaged Hamming distance (or cosine similarity). The training with the autoencoder setup takes only about 1 hour to converge, and thus can be readily applicable to even larger datasets. 4.3 Evaluation To facilitate comparisons with other baseline methods, we use SentEval toolkit1 (Conneau and Kiela, 2018) to evaluate the learned binary (compact) sentence embeddings. Concretely, the learned representations are tested on a series of downstream tasks"
P19-1011,D17-1070,0,0.415508,"eraged as the transformation to convert the pre-trained continuous embeddings into the binary form. 3.4.1 Discussion Another possible strategy is to directly train the general-purpose binary embeddings from scratch, i.e., jointly optimizing the continuous embeddings training objective and continuous-to-binary parameterization. However, our initial attempts demonstrate that this strategy leads to inferior empirical results. This observation is consistent with the results reported in (Kiros and Chan, 2018). Specifically, a binarization layer is directly appended over the InferSent architecture (Conneau et al., 2017) during training, which gives rise to much larger drop in terms of the embeddings’ quality (we have conducted empirical comparisons with (Kiros and Chan, 2018) in Table 1). Therefore, here we focus on learning universal binary embeddings based on pretained continuous sentence representations. Semantic-preserving Regularizer Although the reconstruction objective can help the binary variable to endow with richer semantics, there is no loss that explicitly encourages the binary vectors to preserve the similarity information contained in the original continuous embeddings. Consequently, the model"
P19-1011,P19-1442,0,0.0557413,"Missing"
P19-1011,N18-1049,0,0.0367237,"ive when transferred to a wide range of downstream tasks. Prior work along this line can be roughly divided into two categories: i) pre-trained models that require fine-tuning on the specific transferring task (Dai and Le, 2015; Ruder and Howard, 2018; Radford et al., 2018; Devlin et al., 2018; Cer et al., 2018); ii) methods that extract general-purpose sentence embeddings, which can be effectively applied to downstream NLP tasks without finetuning the encoder parameters (Kiros et al., 2015; Hill et al., 2016; Jernite et al., 2017; Gan et al., 2017; Adi et al., 2017; Logeswaran and Lee, 2018; Pagliardini et al., 2018; Tang and de Sa, 2018). Our proposed methods belong to the second category and provide a generic and easy-to-use encoder to extract highly informative sentence representations. However, our work is unique since the embeddings inferred from our models are binarized and compact, and thus possess the advantages of small memory footprint and much faster sentence retrieval. Learning memory-efficient embeddings with deep neural networks has attracted substantial attention recently. One general strategy towards this goal is to extract discrete or binary data representations (Jang et al., 2016; Shu a"
P19-1011,D18-1092,0,0.0617906,"Missing"
P19-1011,P18-1031,0,0.0429109,"Missing"
P19-1011,P18-1190,1,0.783214,"the second category and provide a generic and easy-to-use encoder to extract highly informative sentence representations. However, our work is unique since the embeddings inferred from our models are binarized and compact, and thus possess the advantages of small memory footprint and much faster sentence retrieval. Learning memory-efficient embeddings with deep neural networks has attracted substantial attention recently. One general strategy towards this goal is to extract discrete or binary data representations (Jang et al., 2016; Shu and Nakayama, 2017; Dai et al., 2017; Chen et al., 2018; Shen et al., 2018; Tissier et al., 2019). Binarized embeddings are especially attractive because they are more memory-efficient (relative to discrete embeddings), and they also enjoy the advantages of fast retrieval based upon a Hamming distance calculation. Previous work along this line in NLP has mainly focused on learning compact representations at the word-level (Shu and Nakayama, 2017; Chen et al., 2018; Tissier et al., 2019), while much less effort has been devoted to extracting binarized embeddings at the sentence-level. Our work aims to bridge this gap, and serves as an initial attempt to facilitate th"
P19-1011,P10-2043,0,0.0848625,"Missing"
P19-1011,P18-1042,0,0.0439939,"Missing"
P19-1200,C18-1142,0,0.0208925,"the latent variable z. In the double-variable model ml-VAE-D, the feature vector is transformed with two MLP layers, and then is used to compute the mean and variance of the top-level latent variable. 4 Related Work VAE for text generation. VAEs trained under the neural variational inference (NVI) framework, has been widely used for generating text sequences: (Bowman et al., 2016; Yang et al., 2017; Semeniuta et al., 2017; Miao et al., 2016; Serban et al., 2017; Miao et al., 2017; Zhao et al., 2017; Shen et al., 2017; Guu et al., 2018; Kim et al., 2018; Yin et al., 2018; Kaiser et al., 2018; Bahuleyan et al., 2018; Chen et al., 2018b; Deng et al., 2018; Shah and Barber, 2018). By encouraging the latent feature space to match a prior distribution within an encoderdecoder architecture, the learned latent variable could potentially encode high-level semantic features and serve as a global representation during the decoding process (Bowman et al., 2016). The generated results are also endowed with better diversity due to the sampling procedure of the latent codes (Zhao et al., 2017). Generative Adversarial Networks (GANs) (Yu et al., 2017; Hu et al., 2017; Zhang et al., 2017; Fedus et al., 2018; Chen et al"
P19-1200,K16-1002,0,0.308159,"Missing"
P19-1200,J82-2003,0,0.737043,"Missing"
P19-1200,D18-1020,0,0.0459969,"In the double-variable model ml-VAE-D, the feature vector is transformed with two MLP layers, and then is used to compute the mean and variance of the top-level latent variable. 4 Related Work VAE for text generation. VAEs trained under the neural variational inference (NVI) framework, has been widely used for generating text sequences: (Bowman et al., 2016; Yang et al., 2017; Semeniuta et al., 2017; Miao et al., 2016; Serban et al., 2017; Miao et al., 2017; Zhao et al., 2017; Shen et al., 2017; Guu et al., 2018; Kim et al., 2018; Yin et al., 2018; Kaiser et al., 2018; Bahuleyan et al., 2018; Chen et al., 2018b; Deng et al., 2018; Shah and Barber, 2018). By encouraging the latent feature space to match a prior distribution within an encoderdecoder architecture, the learned latent variable could potentially encode high-level semantic features and serve as a global representation during the decoding process (Bowman et al., 2016). The generated results are also endowed with better diversity due to the sampling procedure of the latent codes (Zhao et al., 2017). Generative Adversarial Networks (GANs) (Yu et al., 2017; Hu et al., 2017; Zhang et al., 2017; Fedus et al., 2018; Chen et al., 2018a), is anoth"
P19-1200,N16-1012,0,0.0303612,"rious text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it may be difficult to scale RNNs for long-form text generation, as they tend to generate text that is repetitive, ungrammatical, selfcontradictory, overly generic and often lacking coherent long-term structure (Holtzman et al., 2018). Two samples of text generated using standard VAE with an RNN decoder is shown in Table 1. In this work, we propose various multi-level network structures for the VAE model (ml-VAE), to address coherency and repetitiveness challenges associated with long-form text generation. To gener"
P19-1200,D18-1354,0,0.0382184,"set. The standard model struggles with repetitions of the same context or words (in blue), yielding non-coherent text. A hierarhical decoder with multi-layered latent variables eliminates redundancy and yields more coherent text planned around focused concepts. Introduction The variational autoencoder (VAE) for text (Bowman et al., 2016) is a generative model in which a stochastic latent variable provides additional information to modulate the sequential text-generation process. VAEs have been used for various text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it ma"
P19-1200,N19-1021,1,0.837599,"ergence annealing and word dropout, however, none of them help to improve the perplexity compared to a plain neural language model. (Yang et al., 2017) argue that the small KL term relates to the strong autoregressive nature of an LSTM generative network, and they proposed to utilize a dilated CNN as a decoder to improve the informativeness of the latent variable. (Zhao et al., 2018b) proposed to augment the VAE training objective with an additional mutual information term. This yields an intractable integral in the case where the latent variables are continuous. Recent work (He et al., 2019; Fu et al., 2019) has shown that advanced scheduling can mitigate the posterior collapse issue. We instead introduce more flexible priors and hierarchical encoder and decoder structures to deal with posterior collapse. Hierarchical Structures. Natural language is inherently hierarchical (characters form a word, words form a sentence, sentences form a paragraph, paragraphs from a document, etc.). Previous work used multi-level LSTM encoders (Yang et al., 2016) or hierarchical autoencoders (Li et al., 2015a) to learn hierarchical representations for long text or defined a stochastic latent variable for each sent"
P19-1200,Q18-1031,0,0.032465,"aph feature vector into the linear layers to infer the mean and variance of the latent variable z. In the double-variable model ml-VAE-D, the feature vector is transformed with two MLP layers, and then is used to compute the mean and variance of the top-level latent variable. 4 Related Work VAE for text generation. VAEs trained under the neural variational inference (NVI) framework, has been widely used for generating text sequences: (Bowman et al., 2016; Yang et al., 2017; Semeniuta et al., 2017; Miao et al., 2016; Serban et al., 2017; Miao et al., 2017; Zhao et al., 2017; Shen et al., 2017; Guu et al., 2018; Kim et al., 2018; Yin et al., 2018; Kaiser et al., 2018; Bahuleyan et al., 2018; Chen et al., 2018b; Deng et al., 2018; Shah and Barber, 2018). By encouraging the latent feature space to match a prior distribution within an encoderdecoder architecture, the learned latent variable could potentially encode high-level semantic features and serve as a global representation during the decoding process (Bowman et al., 2016). The generated results are also endowed with better diversity due to the sampling procedure of the latent codes (Zhao et al., 2017). Generative Adversarial Networks (GANs) (Yu"
P19-1200,P15-1107,0,0.57083,"s demonstrated in (Bowman et al., 2016) that due to the autoregressive nature of the RNN, the decoder tends to ignore the information from z entirely, resulting in an extremely small KL term (see Section 4). 3 3.1 Multi-Level Generative Networks Single Latent Variable (ml-VAE-S:) Our first multi-level model improves upon standard VAE models by introducing a plan-ahead ability to sequence generation. Instead of directly making word-level predictions only conditioned on the semantic information from z, a series of plan vectors are first generated based upon z with a sentence-level LSTM decoder (Li et al., 2015b). Our hypothesis is that an explicit design of (inherently hierarchical) paragraph structure can capture sentence-level coherence and potentially mitigate repetitiveness. Intuitively, when predicting each token, the decoder can use information from 2080 Inference (Encoder) Network Higher Level CNN I love this place. ? ?? Word-Level LSTM Decoder ?? ? ? Lower Level CNN ? I love this place. KL Losses Sentence Level LSTM Decoder ?? ?? Lots of veggie options. Try veggie quesadilla. … … ? ? ?? … ?? ?? Generative (Decoder) Network Lots of veggie options. Try veggie quesadilla. Figure 1: The propose"
P19-1200,P02-1040,0,0.104322,"bution. As a result, the model is endowed with more flexibility to encode informative semantic features in the latent variables, yet matching their posterior distributions to the corresponding priors. ml-VAE-D achieves the best PPL results on both datasets (on the arXiv dataset, our hierarchical decoder outperforms the ml-LM by reducing the PPL from 58.1 down to 54.3). 5.3 Unconditional Text Generation We evaluate the quality of generated paragraphs as follows. We randomly sample 1000 latent codes and send them to all trained generative models to generate text. We use corpus-level BLEU score (Papineni et al., 2002) to quantitatively evaluate the generated paragraphs. Following strategy in (Yu et al., 2017; Zhang et al., 2017) we use the entire test set as the reference for each generated Figure 2: t-SNE visualization of the learned latent codes. text, and get the average BLEU scores6 over 1000 generated sentences for each model. The results are in Table 3. VAE tends to be a stronger baseline for paragraph generation, exhibiting higher corpus-level BLEU scores than both AAE and ARAE. This observation is consistent with the results in (C´ıfka et al., 2018) in Table 3. The VAE with multi-level decoder demo"
P19-1200,N18-1162,0,0.147556,"the corresponding plan vector via an MLP layer. V represents the weight matrix for computing distribution over words, and We are word embeddings to be learned. For each sentence, once the special END token is generated, the word-level 1 We use teacher-forcing during training and greedy decoding at test time. LSTM stops decoding 2 . LSTMword decoder parameters are shared for each generated sentence. 3.2 Double Latent Variables (ml-VAE-D): Similar architectures of our single latent variable ml-VAE-S model have been applied recently for multi-turn dialog response generation (Serban et al., 2017; Park et al., 2018), mainly focusing on short (one-sentence) response generation. Different from these works, our goal is to generate long text which introduces additional challenges to the hierarchical generative network. We hypothesize that with the two-level LSTM decoder embedded into the VAE framework, the load of capturing global and local semantics are handled differently than the flat-VAEs (Chen et al., 2016). While the multi-level LSTM decoder can capture relatively detailed information (e.g., word-level (local) coherence) via the word- and sentence-level LSTM networks, the latent codes of the VAE are en"
P19-1200,D17-1066,0,0.400135,"generated from two generative models on the Yelp reviews dataset. The standard model struggles with repetitions of the same context or words (in blue), yielding non-coherent text. A hierarhical decoder with multi-layered latent variables eliminates redundancy and yields more coherent text planned around focused concepts. Introduction The variational autoencoder (VAE) for text (Bowman et al., 2016) is a generative model in which a stochastic latent variable provides additional information to modulate the sequential text-generation process. VAEs have been used for various text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most"
P19-1200,P18-1152,0,0.0229683,"ces up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it may be difficult to scale RNNs for long-form text generation, as they tend to generate text that is repetitive, ungrammatical, selfcontradictory, overly generic and often lacking coherent long-term structure (Holtzman et al., 2018). Two samples of text generated using standard VAE with an RNN decoder is shown in Table 1. In this work, we propose various multi-level network structures for the VAE model (ml-VAE), to address coherency and repetitiveness challenges associated with long-form text generation. To generate globally-coherent long text sequences, it is desirable that both the higher-level abstract features (e.g., topic, sentiment, etc.) and lowerlevel fine-granularity details (e.g., specific word choices) of long text can be leveraged by the generative network. It’s difficult for a standard 2079 Proceedings of th"
P19-1200,H93-1035,0,0.58467,"Missing"
P19-1200,P18-1190,1,0.925113,"petitions of the same context or words (in blue), yielding non-coherent text. A hierarhical decoder with multi-layered latent variables eliminates redundancy and yields more coherent text planned around focused concepts. Introduction The variational autoencoder (VAE) for text (Bowman et al., 2016) is a generative model in which a stochastic latent variable provides additional information to modulate the sequential text-generation process. VAEs have been used for various text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it may be difficult to scale RNNs for long-form"
P19-1200,D18-1480,0,0.0721031,"e context or words (in blue), yielding non-coherent text. A hierarhical decoder with multi-layered latent variables eliminates redundancy and yields more coherent text planned around focused concepts. Introduction The variational autoencoder (VAE) for text (Bowman et al., 2016) is a generative model in which a stochastic latent variable provides additional information to modulate the sequential text-generation process. VAEs have been used for various text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it may be difficult to scale RNNs for long-form text generation, as the"
P19-1200,N16-1174,0,0.0554587,"dditional mutual information term. This yields an intractable integral in the case where the latent variables are continuous. Recent work (He et al., 2019; Fu et al., 2019) has shown that advanced scheduling can mitigate the posterior collapse issue. We instead introduce more flexible priors and hierarchical encoder and decoder structures to deal with posterior collapse. Hierarchical Structures. Natural language is inherently hierarchical (characters form a word, words form a sentence, sentences form a paragraph, paragraphs from a document, etc.). Previous work used multi-level LSTM encoders (Yang et al., 2016) or hierarchical autoencoders (Li et al., 2015a) to learn hierarchical representations for long text or defined a stochastic latent variable for each sentence at decoding time (Serban et al., 2017). In contrast, our model encodes the entire paragraph into one single latent variable. The latent variable learned in our model relates more to the global semantic information of a paragraph, whereas those in (Serban et al., 2017) mainly contain the local information of a specific sentence. Park et al.(Park et al., 2018) introduced a variational hierarchical conversational model (VHCR) with global an"
P19-1200,P18-1070,0,0.0239578,"ayers to infer the mean and variance of the latent variable z. In the double-variable model ml-VAE-D, the feature vector is transformed with two MLP layers, and then is used to compute the mean and variance of the top-level latent variable. 4 Related Work VAE for text generation. VAEs trained under the neural variational inference (NVI) framework, has been widely used for generating text sequences: (Bowman et al., 2016; Yang et al., 2017; Semeniuta et al., 2017; Miao et al., 2016; Serban et al., 2017; Miao et al., 2017; Zhao et al., 2017; Shen et al., 2017; Guu et al., 2018; Kim et al., 2018; Yin et al., 2018; Kaiser et al., 2018; Bahuleyan et al., 2018; Chen et al., 2018b; Deng et al., 2018; Shah and Barber, 2018). By encouraging the latent feature space to match a prior distribution within an encoderdecoder architecture, the learned latent variable could potentially encode high-level semantic features and serve as a global representation during the decoding process (Bowman et al., 2016). The generated results are also endowed with better diversity due to the sampling procedure of the latent codes (Zhao et al., 2017). Generative Adversarial Networks (GANs) (Yu et al., 2017; Hu et al., 2017; Zhang"
P19-1200,P18-1101,0,0.117326,"ng relatively longer units of text has been less explored. Optimization Challenges. The “posterior collapse” issue associated with training text-VAEs was first outlined by (Bowman et al., 2016). They 2082 used two strategies, KL divergence annealing and word dropout, however, none of them help to improve the perplexity compared to a plain neural language model. (Yang et al., 2017) argue that the small KL term relates to the strong autoregressive nature of an LSTM generative network, and they proposed to utilize a dilated CNN as a decoder to improve the informativeness of the latent variable. (Zhao et al., 2018b) proposed to augment the VAE training objective with an additional mutual information term. This yields an intractable integral in the case where the latent variables are continuous. Recent work (He et al., 2019; Fu et al., 2019) has shown that advanced scheduling can mitigate the posterior collapse issue. We instead introduce more flexible priors and hierarchical encoder and decoder structures to deal with posterior collapse. Hierarchical Structures. Natural language is inherently hierarchical (characters form a word, words form a sentence, sentences form a paragraph, paragraphs from a docu"
P19-1200,P17-1061,0,0.115931,"Missing"
P19-1264,W05-0909,0,0.157461,"OUGE. 1 Introduction Automatic text evaluation reduces the need for human evaluations, which can be expensive and time-consuming to collect, particularly when evaluating long, multi-sentence texts. Automatic metrics allow faster measures of progress when training and testing models and easier development of text generation systems. However, existing automatic metrics for evaluating text are problematic. Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE (Lin, 2004) for summarization, BLEU (Papineni et al., 2002) for machine translation, and METEOR (Banerjee and Lavie, 2005) or CIDER (Vedantam et al., 2015) for image captioning. Nevertheless, these metrics of∗ Work done while author was at Microsoft Research. A: The family is on a picnic. They have fun. S+WMS: 5.13 6.3 3.7 6.2 7.6 5.5 5.1 6.1 5.1 B: The children eat lunch and play in the park. Figure 1: An illustration of S + WMS (a sentence mover similarity metric that uses both word and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings (orange) and the sentence embeddings (blue) in Document A to those in Document B. An arrow’s width is the propor"
P19-1264,J08-1001,0,0.149367,"Missing"
P19-1264,E06-1040,0,0.186976,"Missing"
P19-1264,N18-1033,0,0.0539092,"Missing"
P19-1264,P14-2074,0,0.0626316,"Missing"
P19-1264,W18-2501,0,0.022427,"Missing"
P19-1264,J18-1008,0,0.0174029,"of embedding. Finally, we show in §5 that sentence mover’s similarity metrics can also be used when learning to generate text. Generating summaries using reinforcement learning with sentence mover’s similarity as the reward results in higher quality summaries than those generated using a ROUGE - L or WMD reward, according to both automatic metrics and human evaluations. 2 that evaluates the distance between two sequences (e.g., sentences, paragraphs, etc.), each represented with relative word frequencies. It combines (1) item similarity2 on bag-of-word (BOW) histogram representations of text (Goldberg et al., 2018) with (2) word embedding similarity. For any two documents A and B, WMD is defined as the minimum cost of transforming one document into the other. Each document is represented by the relative frequencies of words it contains, i.e., for the ith word type, dA,i = count(i)/|A| where |A |is the total word count of document A, and dB,i is defined similarly. Now let the ith word be represented by vi ∈ Rm , i.e., an m-length embedding,3 allowing us to define distances between the ith and jth words, denoted ∆(i, j). V is the vocabulary size. We follow Kusner et al. (2015) and use the Euclidean distan"
P19-1264,N19-1169,0,0.0352922,"Missing"
P19-1264,N18-1150,1,0.88414,"Missing"
P19-1264,P18-1060,0,0.284561,"Missing"
P19-1264,P16-1046,0,0.0606545,"Missing"
P19-1264,L18-1269,0,0.0619859,"Missing"
P19-1264,W07-0734,0,0.106654,"Missing"
P19-1264,W04-1013,0,0.156526,"and human evaluations of summaries learned in this way, finding that our approach outperforms ROUGE. 1 Introduction Automatic text evaluation reduces the need for human evaluations, which can be expensive and time-consuming to collect, particularly when evaluating long, multi-sentence texts. Automatic metrics allow faster measures of progress when training and testing models and easier development of text generation systems. However, existing automatic metrics for evaluating text are problematic. Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE (Lin, 2004) for summarization, BLEU (Papineni et al., 2002) for machine translation, and METEOR (Banerjee and Lavie, 2005) or CIDER (Vedantam et al., 2015) for image captioning. Nevertheless, these metrics of∗ Work done while author was at Microsoft Research. A: The family is on a picnic. They have fun. S+WMS: 5.13 6.3 3.7 6.2 7.6 5.5 5.1 6.1 5.1 B: The children eat lunch and play in the park. Figure 1: An illustration of S + WMS (a sentence mover similarity metric that uses both word and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings ("
P19-1264,D16-1230,0,0.0313795,"that uses both word and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings (orange) and the sentence embeddings (blue) in Document A to those in Document B. An arrow’s width is the proportion of the embedding’s weight being moved, and its label is the Euclidean distance. Here we show only the highest weighted connections. ten fail to capture information that has been reworded or reordered from the reference text, as shown in Kilickaya et al. (2017) and Table 1.1 They have also been found to correlate weakly with human judgments (Liu et al., 2016; Novikova et al., 2017). To avoid these shortcomings, word mover’s distance (WMD; Kusner et al., 2015) can be used to evaluate text in a continuous space using pretrained word embeddings instead of relying on exact word matching. WMD has been used successfully for tasks including image caption evaluation (Kilickaya et al., 2017), automatic essay evaluation (Tashu and Horv´ath, 2018), and affect detection (Alshahrani et al., 2017). This bag-ofembeddings approach is flexible but fails to reflect the grouping of words and ideas, a shortcoming that becomes more problematic as the length of the do"
P19-1264,L18-1008,0,0.0299579,"Missing"
P19-1264,D17-1238,0,0.0626451,"d and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings (orange) and the sentence embeddings (blue) in Document A to those in Document B. An arrow’s width is the proportion of the embedding’s weight being moved, and its label is the Euclidean distance. Here we show only the highest weighted connections. ten fail to capture information that has been reworded or reordered from the reference text, as shown in Kilickaya et al. (2017) and Table 1.1 They have also been found to correlate weakly with human judgments (Liu et al., 2016; Novikova et al., 2017). To avoid these shortcomings, word mover’s distance (WMD; Kusner et al., 2015) can be used to evaluate text in a continuous space using pretrained word embeddings instead of relying on exact word matching. WMD has been used successfully for tasks including image caption evaluation (Kilickaya et al., 2017), automatic essay evaluation (Tashu and Horv´ath, 2018), and affect detection (Alshahrani et al., 2017). This bag-ofembeddings approach is flexible but fails to reflect the grouping of words and ideas, a shortcoming that becomes more problematic as the length of the document grows. We modify"
P19-1264,P02-1040,0,0.110817,"learned in this way, finding that our approach outperforms ROUGE. 1 Introduction Automatic text evaluation reduces the need for human evaluations, which can be expensive and time-consuming to collect, particularly when evaluating long, multi-sentence texts. Automatic metrics allow faster measures of progress when training and testing models and easier development of text generation systems. However, existing automatic metrics for evaluating text are problematic. Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE (Lin, 2004) for summarization, BLEU (Papineni et al., 2002) for machine translation, and METEOR (Banerjee and Lavie, 2005) or CIDER (Vedantam et al., 2015) for image captioning. Nevertheless, these metrics of∗ Work done while author was at Microsoft Research. A: The family is on a picnic. They have fun. S+WMS: 5.13 6.3 3.7 6.2 7.6 5.5 5.1 6.1 5.1 B: The children eat lunch and play in the park. Figure 1: An illustration of S + WMS (a sentence mover similarity metric that uses both word and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings (orange) and the sentence embeddings (blue) in Do"
P19-1264,D17-1103,0,0.0322114,"Missing"
P19-1264,D14-1162,0,0.0809008,"Missing"
P19-1264,N18-1202,0,0.111763,"Missing"
P19-1264,W11-2704,0,0.0771538,"Missing"
P19-1264,W06-1422,0,0.0705409,"Missing"
P19-1264,J09-4008,0,0.0417624,"Missing"
P19-1264,P17-1099,0,0.161561,"Missing"
P19-1264,W17-5306,0,0.0409033,"Missing"
P19-1264,D18-1482,0,0.074386,"Missing"
P19-1264,D17-1207,0,0.0507763,"Missing"
P19-1470,Q17-1010,0,0.0364731,"Missing"
P19-1470,K17-1034,0,0.0462817,"Missing"
P19-1470,P18-2065,0,0.0144448,"traction (Suchanek et al., 2007; Hoffart et al., 2013; Auer et al., 2007; Bollacker et al., 2008) and unstructured text extraction (Dong et al., 2014; Carlson et al., 2010; Nakashole et al., 2011, 2012; Niu, 2012). In our work, we focus on construction of commonsense knowledge bases which require the use of open-text events rather than a well-defined relational schema structure. Other work in information extraction can also be applied to knowledge base construction with open-text entities (Soderland et al., 2010; Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012; Fan et al., 2010; Cui et al., 2018), but these methods typically extract explicitly stated text relations. Conversely, our approach generates new knowledge that is often unstated in text, as commonsense information typically is (Gordon and Van Durme, 2013). Commonsense knowledge base completion Existing work on generation of novel commonsense knowledge has also used ConceptNet and ATOMIC as underlying KBs. Specifically, Li et al. (2016) proposed a set of neural network models for scoring tuples in ConceptNet. Our work differs from this approach as their models evaluate full tuples rather than learning to generate the phrases to"
P19-1470,D11-1142,0,0.366896,"the previous layer blocks from earlier time steps are input to the multi-headed attention with the preceding block for the current time step as the query. (c) Each token is an input to a first-layer block along with all preceding tokens. Dotted lines indicate outputs to all future blocks in the next layer and inputs from all preceding blocks in the previous layer. to model “entities&quot; as natural language phrases and relations as any concept that can link them (Li et al., 2016; Sap et al., 2019). OpenIE approaches display this property of open text entities and relations (Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012), but being extractive, they only capture knowledge that is explicitly mentioned in text, limiting their applicability for capturing commonsense knowledge, which is often implicit (Gordon and Van Durme, 2013). Meanwhile, recent progress in training deep contextualized language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB construction. These large-scale language models display impressive performance when their underlying representations are tuned to solve"
P19-1470,W10-0915,0,0.0821692,"Missing"
P19-1470,P16-1137,0,0.140334,"n extraction can also be applied to knowledge base construction with open-text entities (Soderland et al., 2010; Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012; Fan et al., 2010; Cui et al., 2018), but these methods typically extract explicitly stated text relations. Conversely, our approach generates new knowledge that is often unstated in text, as commonsense information typically is (Gordon and Van Durme, 2013). Commonsense knowledge base completion Existing work on generation of novel commonsense knowledge has also used ConceptNet and ATOMIC as underlying KBs. Specifically, Li et al. (2016) proposed a set of neural network models for scoring tuples in ConceptNet. Our work differs from this approach as their models evaluate full tuples rather than learning to generate the phrases to make new nodes in the knowledge graph. Saito et al. (2018) builds upon this work by proposing a joint model for completion and generation of commonsense tuples. Their work, however, focuses on using tuple generation to augment their KB completion model, rather than to increase coverage in commonsense KB construction. Finally, Sap et al. (2019) use LSTM encoder-decoder models to generate commonsense kn"
P19-1470,D12-1048,0,0.143926,"blocks from earlier time steps are input to the multi-headed attention with the preceding block for the current time step as the query. (c) Each token is an input to a first-layer block along with all preceding tokens. Dotted lines indicate outputs to all future blocks in the next layer and inputs from all preceding blocks in the previous layer. to model “entities&quot; as natural language phrases and relations as any concept that can link them (Li et al., 2016; Sap et al., 2019). OpenIE approaches display this property of open text entities and relations (Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012), but being extractive, they only capture knowledge that is explicitly mentioned in text, limiting their applicability for capturing commonsense knowledge, which is often implicit (Gordon and Van Durme, 2013). Meanwhile, recent progress in training deep contextualized language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB construction. These large-scale language models display impressive performance when their underlying representations are tuned to solve end tasks, achieving"
P19-1470,D12-1104,0,0.0851585,"Missing"
P19-1470,D14-1162,0,0.0851866,"Missing"
P19-1470,N18-1202,0,0.0992616,"l preceding blocks in the previous layer. to model “entities&quot; as natural language phrases and relations as any concept that can link them (Li et al., 2016; Sap et al., 2019). OpenIE approaches display this property of open text entities and relations (Etzioni et al., 2011; Fader et al., 2011; Mausam et al., 2012), but being extractive, they only capture knowledge that is explicitly mentioned in text, limiting their applicability for capturing commonsense knowledge, which is often implicit (Gordon and Van Durme, 2013). Meanwhile, recent progress in training deep contextualized language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB construction. These large-scale language models display impressive performance when their underlying representations are tuned to solve end tasks, achieving state-of-the-art results on a variety of complex problems. In this work, we define the COMmonsEnse Transformer (COMET ), which constructs commonsense KBs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a pre-trained language model learns to adapt its learned re"
P19-1470,K18-1014,0,0.0460182,"plicitly stated text relations. Conversely, our approach generates new knowledge that is often unstated in text, as commonsense information typically is (Gordon and Van Durme, 2013). Commonsense knowledge base completion Existing work on generation of novel commonsense knowledge has also used ConceptNet and ATOMIC as underlying KBs. Specifically, Li et al. (2016) proposed a set of neural network models for scoring tuples in ConceptNet. Our work differs from this approach as their models evaluate full tuples rather than learning to generate the phrases to make new nodes in the knowledge graph. Saito et al. (2018) builds upon this work by proposing a joint model for completion and generation of commonsense tuples. Their work, however, focuses on using tuple generation to augment their KB completion model, rather than to increase coverage in commonsense KB construction. Finally, Sap et al. (2019) use LSTM encoder-decoder models to generate commonsense knowledge about social situations. We use transformers and investigate the effect of using pre-trained language representations (Radford et al., 2018) to initialize them. Transformers and pre-training Finally, our work builds on previous work on adapting p"
W10-1201,U06-1009,0,0.0111971,"A typical QA system has a pipeline structure starting from extraction of candidate sentences to ranking true answers. Some approaches to QA use keyword-based techniques to locate candidate passages/sentences in the retrieved documents and then filter based on the presence of the desired answer type in candidate text. Ranking is then done using syntactic features to characterize similarity to query. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003;"
W10-1201,W01-0509,0,0.0103904,"med-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cascade features learnt separately from other modules in a QA pipeline, thus propagating errors. The structures to be learned can become more complex than the amount of training data, e.g., alignment, entailment, translation, etc. In such cases, other source of information, e.g., unlabeled examples, or human prior knowledge, should be used to improve perfo"
W10-1201,P03-1003,0,0.0134613,"ition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cascade features learnt separately from other modules in a QA pipeline, thus propagating errors. The structures to be learned can become more complex than the amount of training data, e.g., alignment, entailment, translation, etc. In such cases, other source of information, e.g., unlabeled examples, or human prior knowledge, should be used to improve performance. Generative modelin"
W10-1201,P06-1114,0,0.0452493,"approaches to QA use keyword-based techniques to locate candidate passages/sentences in the retrieved documents and then filter based on the presence of the desired answer type in candidate text. Ranking is then done using syntactic features to characterize similarity to query. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for impro"
W10-1201,P09-2084,0,0.0123974,"niques to locate candidate passages/sentences in the retrieved documents and then filter based on the presence of the desired answer type in candidate text. Ranking is then done using syntactic features to characterize similarity to query. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al., 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised,"
W10-1201,P06-1112,0,0.00878227,"icedo and Ferrandez, 2000), logical inferences (abduction 1 or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages. Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cascade features learnt separately from other modules in a QA pipeline, thus propagating errors. The structures to be learned can become more complex than the amount of training data, e.g., alignment, entailment, translation, etc. In such cases, other source of information, e.g., unlabeled examples, or human prior knowledge, should be used to improve performance. Generative modeling is a way of encoding this additional information"
W10-1201,D09-1057,1,\N,Missing
W10-1201,P09-1081,1,\N,Missing
W10-1204,N07-1026,0,0.0262417,"oduction One of the important steps in Question Answering (QA) is question understanding to identify semantic components of questions. In this paper, we investigate question understanding based on a machine learning approach to discover semantic components (Table 1). An important issue in information extraction from text is that one often deals with insufficient labeled data and large number of unlabeled data, which have led to improvements in semi-supervised learning (SSL) methods, e.g., (Belkin and Niyogi., 2002b), (Zhou et al., 2004). Recently, graph based SSL methods have gained interest (Alexandrescu and Kirchhoff, 2007), (Goldberg and Zhu, 2009). These methods create graphs whose vertices correspond to labeled and unlabeled data, while the edge weights encode the similarity between each pair of data points. Classification is performed using these graphs by scoring unlabeled points in such a way 27 Dilek Hakkani-Tur International Computer Science Institute Berkeley, CA dilek@icsi.berkeley.edu W hat} f ilm introduced Jar Jar |{z {z Binks}? |{z } |{z } | other f ocus event topic Semantic Components & Named-Entitiy Types topic: ’Jar’ (Begin-Topic); ’Jar’ (In-Topic) ; ’Binks’ (In-Topic)(HUMAN:Individual) focus: ’"
W10-1204,W04-2504,0,0.0092967,"ike answer type, focus, event, etc. The ’answer-type’ is a quantity that a question is seeking. A question ’topic’ usually represents major context/constraint of a question (”Jar Jar Binks” in Table 1). A question ’focus’ (e.g., film) denotes a certain aspect (or descriptive feature) of a question ’topic’. To extract topic-focus from questions, (Hajicova et al., 1993) used rule-based approaches via dependency parser structures. (Burger, 2006) implemented parsers and a mixture of rule-based and learning methods to extract different salient features such as question type, event, entities, etc. (Chai and Jin, 2004) explored semantic units based on their discourse relations via rule-based systems. In (Duan et al., 2008) a language model is presented to extract semantic components from questions. Similarly, (Fan et al., 2008)’s semantic chunk annotation uses conditional random fields (CRF) (Lafferty et al., 2001) to annotate semantic chunks of questions in Chinese. Our work aparts from these studies in that we use a graph-based SSL method to extract semantic components from unla28 beled questions. Graph-based methods are suitable for labeling tasks because when two lexical units in different questions are"
W10-1204,P08-1019,0,0.0339081,"’topic’ usually represents major context/constraint of a question (”Jar Jar Binks” in Table 1). A question ’focus’ (e.g., film) denotes a certain aspect (or descriptive feature) of a question ’topic’. To extract topic-focus from questions, (Hajicova et al., 1993) used rule-based approaches via dependency parser structures. (Burger, 2006) implemented parsers and a mixture of rule-based and learning methods to extract different salient features such as question type, event, entities, etc. (Chai and Jin, 2004) explored semantic units based on their discourse relations via rule-based systems. In (Duan et al., 2008) a language model is presented to extract semantic components from questions. Similarly, (Fan et al., 2008)’s semantic chunk annotation uses conditional random fields (CRF) (Lafferty et al., 2001) to annotate semantic chunks of questions in Chinese. Our work aparts from these studies in that we use a graph-based SSL method to extract semantic components from unla28 beled questions. Graph-based methods are suitable for labeling tasks because when two lexical units in different questions are close in the intrinsic geometry of question forms, their semantic components (labels) will be similar to"
W10-1204,W08-1601,0,0.0247264,"focus’ (e.g., film) denotes a certain aspect (or descriptive feature) of a question ’topic’. To extract topic-focus from questions, (Hajicova et al., 1993) used rule-based approaches via dependency parser structures. (Burger, 2006) implemented parsers and a mixture of rule-based and learning methods to extract different salient features such as question type, event, entities, etc. (Chai and Jin, 2004) explored semantic units based on their discourse relations via rule-based systems. In (Duan et al., 2008) a language model is presented to extract semantic components from questions. Similarly, (Fan et al., 2008)’s semantic chunk annotation uses conditional random fields (CRF) (Lafferty et al., 2001) to annotate semantic chunks of questions in Chinese. Our work aparts from these studies in that we use a graph-based SSL method to extract semantic components from unla28 beled questions. Graph-based methods are suitable for labeling tasks because when two lexical units in different questions are close in the intrinsic geometry of question forms, their semantic components (labels) will be similar to each other. Labels vary smoothly along the geodesics, i.e., manifold assumption, which plays an essential r"
W10-1204,W09-2203,0,0.0217431,"in Question Answering (QA) is question understanding to identify semantic components of questions. In this paper, we investigate question understanding based on a machine learning approach to discover semantic components (Table 1). An important issue in information extraction from text is that one often deals with insufficient labeled data and large number of unlabeled data, which have led to improvements in semi-supervised learning (SSL) methods, e.g., (Belkin and Niyogi., 2002b), (Zhou et al., 2004). Recently, graph based SSL methods have gained interest (Alexandrescu and Kirchhoff, 2007), (Goldberg and Zhu, 2009). These methods create graphs whose vertices correspond to labeled and unlabeled data, while the edge weights encode the similarity between each pair of data points. Classification is performed using these graphs by scoring unlabeled points in such a way 27 Dilek Hakkani-Tur International Computer Science Institute Berkeley, CA dilek@icsi.berkeley.edu W hat} f ilm introduced Jar Jar |{z {z Binks}? |{z } |{z } | other f ocus event topic Semantic Components & Named-Entitiy Types topic: ’Jar’ (Begin-Topic); ’Jar’ (In-Topic) ; ’Binks’ (In-Topic)(HUMAN:Individual) focus: ’film’ (Begin-Focus) (DESCR"
W10-1204,E93-1022,0,0.0600814,"iments in section 7 yield performance improvement in comparison to other labeling methods on different datasets. Finally we draw conclusions. 2 Related Work on Question Analysis An important step in question analysis is extracting semantic components like answer type, focus, event, etc. The ’answer-type’ is a quantity that a question is seeking. A question ’topic’ usually represents major context/constraint of a question (”Jar Jar Binks” in Table 1). A question ’focus’ (e.g., film) denotes a certain aspect (or descriptive feature) of a question ’topic’. To extract topic-focus from questions, (Hajicova et al., 1993) used rule-based approaches via dependency parser structures. (Burger, 2006) implemented parsers and a mixture of rule-based and learning methods to extract different salient features such as question type, event, entities, etc. (Chai and Jin, 2004) explored semantic units based on their discourse relations via rule-based systems. In (Duan et al., 2008) a language model is presented to extract semantic components from questions. Similarly, (Fan et al., 2008)’s semantic chunk annotation uses conditional random fields (CRF) (Lafferty et al., 2001) to annotate semantic chunks of questions in Chin"
W10-1204,P03-1054,0,0.00285714,"y, only one node per token is introduced to the graph for known(true) token/label relations. We find the best question label sequence via Viterbi algorithm (Forney, 1973). 3.1 Feature Extraction For Labeling Task The following pre-processing modules are built for feature extraction prior to graph construction. 3.1.1 Pre-Processing For Feature Extraction Phrase Analysis(PA): Using basic syntactic analysis (shallow parsing), the PA module re-builds phrases from linguistic structures such as nounphrases (NN), basic prepositional phrases (PP) or verb groups (VG). Using Stanford dependency parser (Klein and Manning, 2003), (Marneffe et al., 2006), which produces 48 different grammatical relations, PA module re-constructs the phrases. For example for the question in Table 1, dependency parser generates two relations: − nn(Binks-3, Jar-1) and nn(Binks-3, Jar-2), PA reveals ”Jar Jar Binks” as a noun phrase reconstructing the nn:noun compound modifier. We also extract part of speech tags of questions via dependency parser to be used for feature extraction. Question Dependency Relations (QDR): Using shallow semantics, we decode underlying Stanford dependency trees (Marneffe et al., 2006) that embody linguistic rela"
W10-1204,de-marneffe-etal-2006-generating,0,\N,Missing
W12-1816,P11-1009,0,0.127302,"to be sent to dialog manager for taking the appropriate system action. Three key tasks of an SLU system are domain classification, intent determination and slot filling (Tur and Mori, 2011). While the state-of-the-art SLU systems rely on data-driven methods, collecting and annotating naturally spoken utterances to train the required statistical models is often costly The use of click information obtained through massive search query click logs has been the focus of previous research. Specifically, query logs have been used for building more robust web search and better information retrieval (Pantel and Fuxman, 2011; Li et al., 2008), improve personalization experience and understand social networking behaviors (Wang et al., 2011), etc. The use of query logs in spoken dialog research is fairly new. In this paper, we will survey the recent research on utilizing the search query logs to obtain more accurate and robust spoken dialog systems, focusing on the SLU. Later in the discussion section, we will discuss the implimications on the dialog models. The paper is organized as follows: In § 2, we briefly describe query click logs. We then summarize recent research papers to give a snapshot of how user search"
