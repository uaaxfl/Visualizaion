1992.tmi-1.9,P91-1034,0,0.344464,"Missing"
1992.tmi-1.9,P91-1022,0,0.282503,"Missing"
1992.tmi-1.9,P91-1017,0,0.117581,"Missing"
1992.tmi-1.9,H90-1070,0,0.0202559,"Missing"
1992.tmi-1.9,C92-2070,1,0.770211,"Missing"
2003.mtsummit-semit.11,J00-1004,0,0.011985,"ial motion that is observed, in terms of adjective-noun swapping, is well modeled by the relative-positionbased distortion models of the classic IBM approach. Unfortunately, these distortion models are less effective for languages such as Japanese or Arabic, which have substantially different top-level sentential word orders from English. Wu (1997) and Jones and Havrilla (1998) have sought to more closely tie the allowed motion of constituents between languages to those syntactic transductions supported by the independent rotation of parse tree constituents. Yamada and Knight (2000, 2001) and Alshawi et al. (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a limitation shared with probabilistic context free grammars that the preferred ordering of subtrees is insufficiently constrained by their embedding context, which is especially problematic for very deep syntactic parses. In contrast, Och et al. (1999) have avoided the constraints of"
2003.mtsummit-semit.11,W00-0508,0,0.0128298,"flat embedded noun-phrases in a flat sentential constituent-based chunk sequence that can be driven by syntactic bracketers and POS tag models rather than a full parser, facilitating its transfer to lower density languages. The flatter 2-level structures also better support transductions conditioned to full sentential context than do deeply embedded tree models, while retaining the empirically observed advantages of translation ordering independence of nounphrases. Another improvement over Och et al. and Yamada and Knight is the use of the finite state machine (FSM) modelling framework (e.g. Bangalore and Riccardi, 2000), which offers the considerable advantage of a flexible framework for decoding, as well as a representation which is suitable for the fixed two-level phrasal modelling employed here. Finally, the original cross-part-of-speech lexical coercion models presented in Section 4.3.3 have related work in the primarily-syntactic coercion models utilized by Dorr and Habash (2002) and Habash and Dorr (2003), although their induction and modelling are quite different from the approach here. 3 Resources As in other SMT approaches, the primary training resource is a sentence-aligned parallel bilingual corpu"
2003.mtsummit-semit.11,J93-2003,0,0.00453659,"nce-level order of VerbSubject-Object, which means that translation into English (with a standard ordering of Subject-VerbObject) commonly requires motion of entire phrasal constituents, which is not true of French-to-English translation, to cite one language pair whose characteristics have wielded great influence in the history of work on statistical machine translation. A key motivation for and objective of this work was to build a translation model and feature space to effectively handle the above-described phenomenon. 2 Prior Work Statistical machine translation, as pioneered by IBM (e.g. Brown et al., 1993), is grounded in the noisy channel model. And similar to the related channel problems of speech and handwriting recognition, the original SMT language pair French-English exhibits a relatively close linear correlation in source and target sequence. Most common non-sequential motion that is observed, in terms of adjective-noun swapping, is well modeled by the relative-positionbased distortion models of the classic IBM approach. Unfortunately, these distortion models are less effective for languages such as Japanese or Arabic, which have substantially different top-level sentential word orders f"
2003.mtsummit-semit.11,P91-1023,0,0.0190875,"Missing"
2003.mtsummit-semit.11,N03-1013,0,0.0205268,"advantages of translation ordering independence of nounphrases. Another improvement over Och et al. and Yamada and Knight is the use of the finite state machine (FSM) modelling framework (e.g. Bangalore and Riccardi, 2000), which offers the considerable advantage of a flexible framework for decoding, as well as a representation which is suitable for the fixed two-level phrasal modelling employed here. Finally, the original cross-part-of-speech lexical coercion models presented in Section 4.3.3 have related work in the primarily-syntactic coercion models utilized by Dorr and Habash (2002) and Habash and Dorr (2003), although their induction and modelling are quite different from the approach here. 3 Resources As in other SMT approaches, the primary training resource is a sentence-aligned parallel bilingual corpus. We further require that each side of the corpus be part-of-speech (POS) tagged and phrase chunked. Our translation experiments were carried out using the United Nations Arabic-English parallel corpus made available (with sentence alignments) by the Linguistic Data Consortium. POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit (Ngai an"
2003.mtsummit-semit.11,J93-2004,0,0.0323043,"h here. 3 Resources As in other SMT approaches, the primary training resource is a sentence-aligned parallel bilingual corpus. We further require that each side of the corpus be part-of-speech (POS) tagged and phrase chunked. Our translation experiments were carried out using the United Nations Arabic-English parallel corpus made available (with sentence alignments) by the Linguistic Data Consortium. POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit (Ngai and Florian, 2001); both were trained from the annotated Penn Treebank corpus (Marcus et al., 1993). For Arabic, we used a colleague’s POS tagger and tokenizer (clitic separation was also performed prior to POS tagging), which was rapidly developed in our laboratory. Phrase segmentation was achieved via a simple decision list of chunk join/split decisions, based on variable-length right and left context patterns, as illustrated in Table 1. The highest-ranked matching pattern was used at each decision point (between any two contiguous words we consider there to be a decision point, at which a binary decision must be made between split and join). Each such segmentation decision was made in is"
2003.mtsummit-semit.11,N01-1006,0,0.0143376,"(2003), although their induction and modelling are quite different from the approach here. 3 Resources As in other SMT approaches, the primary training resource is a sentence-aligned parallel bilingual corpus. We further require that each side of the corpus be part-of-speech (POS) tagged and phrase chunked. Our translation experiments were carried out using the United Nations Arabic-English parallel corpus made available (with sentence alignments) by the Linguistic Data Consortium. POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit (Ngai and Florian, 2001); both were trained from the annotated Penn Treebank corpus (Marcus et al., 1993). For Arabic, we used a colleague’s POS tagger and tokenizer (clitic separation was also performed prior to POS tagging), which was rapidly developed in our laboratory. Phrase segmentation was achieved via a simple decision list of chunk join/split decisions, based on variable-length right and left context patterns, as illustrated in Table 1. The highest-ranked matching pattern was used at each decision point (between any two contiguous words we consider there to be a decision point, at which a binary decision mus"
2003.mtsummit-semit.11,P00-1056,0,0.247419,"N+NNA N+NN VN+A Table 1: A small sample of the phrase segmentation patterns used. |means insert a phrase chunk boundary at this point. + mean join left and right context into a single phrase at this point. N , D , A , V etc. refer to Arabic core parts of speech. A further input to our system is a set of word alignment links on the parallel corpus. These are used to compute word translation probabilities and phrasal alignments. The word alignments can in principle come from any source: a dictionary, a specialized alignment program, or another SMT system. We used alignments generated by Giza++ (Och and Ney, 2000) by running it in both directions on our parallel corpus. The union of these bidirectional alignments was used to compute crosslanguage phrase correspondences (alignments) by simple plurality voting. Specifically, each Arabic phrase in the training corpus was allowed to vote on the single English phrase to which it was most strongly aligned. Each word alignment link between a token in the Arabic phrase and a token in an English phrase was counted as a unit vote. Ties among English phrases with equal scores were broken by taking the leftmost such English phrase. The resulting phrasal alignments"
2003.mtsummit-semit.11,W99-0604,0,0.0356857,"and Knight (2000, 2001) and Alshawi et al. (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a limitation shared with probabilistic context free grammars that the preferred ordering of subtrees is insufficiently constrained by their embedding context, which is especially problematic for very deep syntactic parses. In contrast, Och et al. (1999) have avoided the constraints of tree-based syntactic models and allow the relatively flat motion of empirically derived phrasal chunks, which need not adhere to traditional constituent boundaries. Our current paper takes a middle path, by grounding motion in syntactic transduction, but in a much flatter 2-level model of syntactic analysis, based on flat embedded noun-phrases in a flat sentential constituent-based chunk sequence that can be driven by syntactic bracketers and POS tag models rather than a full parser, facilitating its transfer to lower density languages. The flatter 2-level stru"
2003.mtsummit-semit.11,2001.mtsummit-papers.68,0,0.0125935,"purposes only: search for decoding was performed on unpruned FSMs). This is presented to further illustrate the structure of the FSMs used for decoding. son system – the Giza++ IBM Model 4 implementation (Och and Ney, 2000) with the ReWrite decoder (Marcu and Germann, 2002) – are included as a baseline. For the Arabic UN corpus, we trained our system on a large subset of the UN corpus and evaluated on a 200-sentence held-out set. For this 150K sentence Arabic training set, Giza++ and the shallow syntax model achieved very similar performance. Results are scored via the Bleu metric proposed by Papineni et al. (2001). Bleu Score System 150K Trn. Sent. Giza++/ReWrite Decoder 0.17 2-level Syntax Model 0.17 Table 10: Results comparison for Arabic-English translation on UN corpus. (200-sentence evaluation set) 7 Conclusions This paper has presented an original model for statistical machine translation inspired by and tailored to the syntactic divergences and other characteristics of Arabic-English statistical machine translation. The two-level syntactic transduction model supports both sentence-level and intra-phrase structural reordering, as well as a word translation component which benefits from empiricall"
2003.mtsummit-semit.11,J97-3002,0,0.061801,"in the noisy channel model. And similar to the related channel problems of speech and handwriting recognition, the original SMT language pair French-English exhibits a relatively close linear correlation in source and target sequence. Most common non-sequential motion that is observed, in terms of adjective-noun swapping, is well modeled by the relative-positionbased distortion models of the classic IBM approach. Unfortunately, these distortion models are less effective for languages such as Japanese or Arabic, which have substantially different top-level sentential word orders from English. Wu (1997) and Jones and Havrilla (1998) have sought to more closely tie the allowed motion of constituents between languages to those syntactic transductions supported by the independent rotation of parse tree constituents. Yamada and Knight (2000, 2001) and Alshawi et al. (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a limitation s"
2003.mtsummit-semit.11,P01-1067,0,0.0607545,"Missing"
2003.mtsummit-semit.11,P02-1039,0,0.0243647,"Missing"
2006.amta-papers.18,2005.mtsummit-ebmt.1,0,0.0461117,"tes where glossers are located in our system. Glossers take a text in one os s Tr ain ing Glossed Source Bixtexts Gloss -&gt; English Translation System Decoding Test Input Glossed Test Text Gl s Dirix et al. (2005) propose a system for machine translation without bitext. The system, Metis-II, assumes a great deal more resources than our system, including a chunker or parser and a part-of-speech tagger in the source language. Like our system, they assume the existence of a bilingual dictionary. They also use context, although with more intensive linguistic preprocessing, to choose a translation. Badia et al. (2005) also propose a system for machine translation without bitext. They also require a part-of-speech tagger and lemmatizer like Dirix et al. (2005). They use an n−gram language model to choose between various options but also rely on the POS tagger to guide their choice. Lavie et al. (2003) take a different approach for low resource languages. They use small amounts of word-aligned data and elicit information from a native speaker. By learning the structure of one language, they are able to transfer knowledge to a resource poor language, which provides a better translation. Itialian os Related Wo"
2006.amta-papers.18,N03-1017,0,0.00319474,"great deal more resources than our system, including a chunker or parser and a part-of-speech tagger in the source language. Like our system, they assume the existence of a bilingual dictionary. They also use context, although with more intensive linguistic preprocessing, to choose a translation. Badia et al. (2005) also propose a system for machine translation without bitext. They also require a part-of-speech tagger and lemmatizer like Dirix et al. (2005). They use an n−gram language model to choose between various options but also rely on the POS tagger to guide their choice. Lavie et al. (2003) take a different approach for low resource languages. They use small amounts of word-aligned data and elicit information from a native speaker. By learning the structure of one language, they are able to transfer knowledge to a resource poor language, which provides a better translation. Itialian os Related Work French Gl 2 Spanish Gloss the glossed bitexts, there is a good chance the machine translation system has learned that the house white should be translated as the white house. If the test language fragment is glossed as the house white, then the system will work. If instead the test la"
2006.amta-papers.18,koen-2004-pharaoh,0,0.016115,"nces) per test set. Table 5 lists the languages used in the experiments and the abbreviations used for the languages. 4.2 Results 4.2.1 Baselines To establish a baseline, we first treated the unaltered source language side of the test set as if it were the output of a glosser and evaluated it. This yields a BLEU score higher than 0 because of borrowed words and names. The top of Table 6 shows the results for three languages. We established another baseline by using the bilingual dictionary as an (impoverished) bitext. A Giza++ (Och and Ney, 2003) system was trained on this bitext and Pharaoh (Koehn, 2004) was used to decode the test set1 . Many languages have translations of the Bible available even when no other 1 Section 6 gives more details on our machine translation system. 160 Target alternative stage its representative the following document the commission of the european communities common organisation of the market annex to that regulation Source fase alternativa su representante el documento siguiente la comisin de las comunidades europeas organizacin comn de el mercado anexo de dicho reglamento Glossed alternative stage its representative the following document you commission of them"
2006.amta-papers.18,J03-1002,0,0.0102467,"ple, instead of glossing abarcaba and comunidad separately as WWsd does, they would be jointly glossed as included community. gPWsd (w, wn , L) = arg max φT (g, gn ) (5) (g∈γ(w),gn ∈γ(wn ))∪(g∈γ(wn ),gn ∈γ(w)) Depending on how the function words which are skipped when choosing wn are handled, it is possible for PWsd to delete function words during glossing. The consequences of this are addressed in Section 6.3.3. 3.7 Bitext Based Glossing (Bible) We created one glosser which made use of a small amount of bitext. For several languages, we had a bitext of the Bible available. We trained Giza++ (Och and Ney, 2003) on each of these bitexts. Combining the bidrectional alignments using the grow-diagfinal algorithm (Koehn et al., 2003), we created oneto-one mappings using the mostly likely target language word given the source language word. These mappings were treated as a dictionary and used to gloss text. Language German Spanish French Dutch Portuguese Swedish Table 5: Abbreviations Used for Languages 4 Glossing Experiments 4.1 Data Our experiments are performed on the Europarl-04 data. All data were tokenized using a tokenizer based on the one distributed with Egypt (Al-Onaizan et al., 1999) but extend"
2006.amta-papers.21,W99-0612,1,0.861237,"Missing"
2006.amta-papers.21,N04-4038,0,0.0839038,"Missing"
2006.amta-papers.21,W05-0703,0,0.0509388,"deletion of the Arabic article [Al] in some instances, his algorithm yields improved translation accuracy over a baseline phrase-based SMT system. Buckwalter (2004) presents a lexicon-based morphological analyzer for MSA. A publicly available tool by Diab (2004) trained on a large amount MSA newswire using a Support Vector Machine (SVM) based approach, provides part-of-speech tagging and morpheme segmentation as an intermediate step. Habash and Rambow (2005) present an SVM-based classifier approach to morphological analysis for MSA trained on the Penn Arabic Treebank (Maamouri et al., 2004). Habash et al. (2005) describe a work in progress implementing a morphological analysis tool involving the modeling of Arabic morphological and phonemic phenomena to provide morphological analysis taking into account the nonstandard orthography found in transcribed data for many spoken dialects of Arabic. 3 Dialectal Speech Corpora Levantine Arabic. The Iraqi Arabic corpus is from the Defense Advanced Research Projects Agency (DARPA) Transtac program, derived from 40 hours of recorded and transcribed audio. The Levantine Arabic corpus is derived from the Levantine Arabic Conversational Telephone Speech Collection,"
2006.amta-papers.21,P05-1071,0,0.127848,"resent an algorithm for morpheme segmentation seeded by a 110,000-word manually segmented corpus. Subsequently, Lee (2004) shows that, in conjunction with manual deletion of the Arabic article [Al] in some instances, his algorithm yields improved translation accuracy over a baseline phrase-based SMT system. Buckwalter (2004) presents a lexicon-based morphological analyzer for MSA. A publicly available tool by Diab (2004) trained on a large amount MSA newswire using a Support Vector Machine (SVM) based approach, provides part-of-speech tagging and morpheme segmentation as an intermediate step. Habash and Rambow (2005) present an SVM-based classifier approach to morphological analysis for MSA trained on the Penn Arabic Treebank (Maamouri et al., 2004). Habash et al. (2005) describe a work in progress implementing a morphological analysis tool involving the modeling of Arabic morphological and phonemic phenomena to provide morphological analysis taking into account the nonstandard orthography found in transcribed data for many spoken dialects of Arabic. 3 Dialectal Speech Corpora Levantine Arabic. The Iraqi Arabic corpus is from the Defense Advanced Research Projects Agency (DARPA) Transtac program, derived"
2006.amta-papers.21,P03-1051,0,0.0327481,"43, 471 247, 741 5.7 26, 147 Table 2. Summary statistics for the Levantine Arabic and the English/Iraqi Arabic speech corpora. show that with little modification for each dialect, and minimal data for training, our model yields improved segmentation accuracy over a standard rulebased lexicon approach described in (Riesa et al., 2006). In addition, the trained model is applied to an English-Iraqi Arabic parallel corpus of 36,895 utterances and yields a significant improvement in BLEU score (Papineni et al., 2002) over a baseline system without use of morphological segmentation. 2 Related Work Lee et al. (2003) present an algorithm for morpheme segmentation seeded by a 110,000-word manually segmented corpus. Subsequently, Lee (2004) shows that, in conjunction with manual deletion of the Arabic article [Al] in some instances, his algorithm yields improved translation accuracy over a baseline phrase-based SMT system. Buckwalter (2004) presents a lexicon-based morphological analyzer for MSA. A publicly available tool by Diab (2004) trained on a large amount MSA newswire using a Support Vector Machine (SVM) based approach, provides part-of-speech tagging and morpheme segmentation as an intermediate step"
2006.amta-papers.21,N04-4015,0,0.0526433,"w that with little modification for each dialect, and minimal data for training, our model yields improved segmentation accuracy over a standard rulebased lexicon approach described in (Riesa et al., 2006). In addition, the trained model is applied to an English-Iraqi Arabic parallel corpus of 36,895 utterances and yields a significant improvement in BLEU score (Papineni et al., 2002) over a baseline system without use of morphological segmentation. 2 Related Work Lee et al. (2003) present an algorithm for morpheme segmentation seeded by a 110,000-word manually segmented corpus. Subsequently, Lee (2004) shows that, in conjunction with manual deletion of the Arabic article [Al] in some instances, his algorithm yields improved translation accuracy over a baseline phrase-based SMT system. Buckwalter (2004) presents a lexicon-based morphological analyzer for MSA. A publicly available tool by Diab (2004) trained on a large amount MSA newswire using a Support Vector Machine (SVM) based approach, provides part-of-speech tagging and morpheme segmentation as an intermediate step. Habash and Rambow (2005) present an SVM-based classifier approach to morphological analysis for MSA trained on the Penn Ar"
2006.amta-papers.21,P00-1056,0,0.0659718,"Missing"
2006.amta-papers.21,J04-4002,0,0.0498945,"Missing"
2006.amta-papers.21,P02-1040,0,0.0749494,"Running words Words per utterance Unique words 36, 895 438, 911 11.9 8, 776 36, 895 305, 889 8.3 29, 238 43, 471 247, 741 5.7 26, 147 Table 2. Summary statistics for the Levantine Arabic and the English/Iraqi Arabic speech corpora. show that with little modification for each dialect, and minimal data for training, our model yields improved segmentation accuracy over a standard rulebased lexicon approach described in (Riesa et al., 2006). In addition, the trained model is applied to an English-Iraqi Arabic parallel corpus of 36,895 utterances and yields a significant improvement in BLEU score (Papineni et al., 2002) over a baseline system without use of morphological segmentation. 2 Related Work Lee et al. (2003) present an algorithm for morpheme segmentation seeded by a 110,000-word manually segmented corpus. Subsequently, Lee (2004) shows that, in conjunction with manual deletion of the Arabic article [Al] in some instances, his algorithm yields improved translation accuracy over a baseline phrase-based SMT system. Buckwalter (2004) presents a lexicon-based morphological analyzer for MSA. A publicly available tool by Diab (2004) trained on a large amount MSA newswire using a Support Vector Machine (SVM"
2016.amta-researchers.14,J07-2003,0,0.235974,"Missing"
2016.amta-researchers.14,P08-2015,0,0.0273342,"ically (§4). 3. We then produce phrasal translation pairs by extracting English phrases from these sentences and pairing them with the foreign language through the UniMorph tag (§5). We investigate different methods for extracting and scoring phrase pairs. 4. Finally, we evaluate the utility of these phrase pairs to improve machine translation in simulated low-resource settings (§6). A depiction of the full pipeline is in Figure 1. 2 Prior Work Maximizing the utility of a baseline phrase table has been the focus of a large body of prior work in translating from morphologically rich languages. Habash (2008) work on the OOV problem in Arabic, mapping OOV types to in-vocabulary (INV) types by orthographic and morphological smoothing methods. Mirkin et al. (2009) take inspiration from the Textual Entailment (TE) problem, using WordNet to determine a set of entailed alternatives for English OOV tokens. However, since this OOV-resolution scheme is dependent on the existence of a semantic resource like WordNet in the source language, it is unsuitable in general low-resource settings. Yang and Kirchhoff (2006) implement a backoff model for Finnish and German, stemming and splitting OOV tokens at test t"
2016.amta-researchers.14,hajic-etal-2012-announcing,0,0.027935,"Missing"
2016.amta-researchers.14,P13-2121,0,0.0606978,"Missing"
2016.amta-researchers.14,W04-3250,0,0.304107,"Missing"
2016.amta-researchers.14,2005.mtsummit-papers.11,0,0.0678909,"lation systems are typically trained on large bilingual parallel corpora (bitext). Low-resource machine translation focuses on translation of languages for which there exists little bitext, and where translation quality is subsequently often poor. Highly inﬂected languages—those that exhibit large inﬂectional paradigms of words with a common dictionary entry—excacerbate the problems of a low-resource setting. Many inﬂections of words in an inﬂectional paradigm are complex and rare, and their translations are unlikely to be wellestimated even in a moderately large parallel corpus. For example, Koehn (2005) point to the highly inﬂected nature of Finnish as a reason for poor translation performance into English even in high-resource settings. However, even where bitext may be lacking or scarce, there are often many other resources available. One source of rich morphological information is Wiktionary.1 This paper describes a method for using resources extracted from Wiktionary to automatically map inﬂections in paradigms of morphologically rich languages to ranked sets of English phrasal translations. This is done by the following procedure: 1 https://www.wiktionary.org/ 3URFHHGLQJVRI$07$"
2016.amta-researchers.14,2012.amta-papers.9,0,0.0347861,"et to determine a set of entailed alternatives for English OOV tokens. However, since this OOV-resolution scheme is dependent on the existence of a semantic resource like WordNet in the source language, it is unsuitable in general low-resource settings. Yang and Kirchhoff (2006) implement a backoff model for Finnish and German, stemming and splitting OOV tokens at test time and searching a baseline phrase table for the resulting simpliﬁed forms. Many systems attempt to address the incorrect independence assumptions traditional phrase-based MT systems impose on inﬂections in the same paradigm. Koehn and Haddow (2012) train a baseline phrase-based translation model, and back off to a factored model that 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Inﬂection bude absolvovat absolvuj absolvujete absolvoval jste Lemma absolvovat absolvovat absolvovat absolvovat Mood IND IMP IND IND POS VB VB VB VB Tense FUT Gender PST PST MASC MASC Number 2 2 2 2 Animacy ANIM INAN Person SG SG SG SG Table 1: Czech verb inﬂections and partial annotations from Wiktionary. Empty cells indicate that the inﬂection is not marked in that dimension. decomposes OOV tokens into lemma+"
2016.amta-researchers.14,P09-1089,0,0.0231176,"ge through the UniMorph tag (§5). We investigate different methods for extracting and scoring phrase pairs. 4. Finally, we evaluate the utility of these phrase pairs to improve machine translation in simulated low-resource settings (§6). A depiction of the full pipeline is in Figure 1. 2 Prior Work Maximizing the utility of a baseline phrase table has been the focus of a large body of prior work in translating from morphologically rich languages. Habash (2008) work on the OOV problem in Arabic, mapping OOV types to in-vocabulary (INV) types by orthographic and morphological smoothing methods. Mirkin et al. (2009) take inspiration from the Textual Entailment (TE) problem, using WordNet to determine a set of entailed alternatives for English OOV tokens. However, since this OOV-resolution scheme is dependent on the existence of a semantic resource like WordNet in the source language, it is unsuitable in general low-resource settings. Yang and Kirchhoff (2006) implement a backoff model for Finnish and German, stemming and splitting OOV tokens at test time and searching a baseline phrase table for the resulting simpliﬁed forms. Many systems attempt to address the incorrect independence assumptions traditio"
2016.amta-researchers.14,P15-2111,1,0.803087,"ibes a method for using resources extracted from Wiktionary to automatically map inﬂections in paradigms of morphologically rich languages to ranked sets of English phrasal translations. This is done by the following procedure: 1 https://www.wiktionary.org/ 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Figure 1: The translation model (TM) construction pipeline. This depicts the process by which we map each morphologically annotated inﬂection to a ranked set of English phrasal translations. 1. We begin with resources from the UniMorph project (Sylak-Glassman et al., 2015), which produced millions of tuples pairing inﬂected words forms with their lemmas and a rich morphological tag (which we refer to as a UniMorph tag or vector) that was designed to be a universal representation of morphological features (§3). 2. We next take a small set of pairs of UniMorph vectors and short English sentences that were produced in an Elicitation Corpus, designed to collect inﬂections that in English are expressed phrasally instead of morphologically (§4). 3. We then produce phrasal translation pairs by extracting English phrases from these sentences and pairing them with the f"
2016.amta-researchers.14,E06-1006,0,0.0198916,"able has been the focus of a large body of prior work in translating from morphologically rich languages. Habash (2008) work on the OOV problem in Arabic, mapping OOV types to in-vocabulary (INV) types by orthographic and morphological smoothing methods. Mirkin et al. (2009) take inspiration from the Textual Entailment (TE) problem, using WordNet to determine a set of entailed alternatives for English OOV tokens. However, since this OOV-resolution scheme is dependent on the existence of a semantic resource like WordNet in the source language, it is unsuitable in general low-resource settings. Yang and Kirchhoff (2006) implement a backoff model for Finnish and German, stemming and splitting OOV tokens at test time and searching a baseline phrase table for the resulting simpliﬁed forms. Many systems attempt to address the incorrect independence assumptions traditional phrase-based MT systems impose on inﬂections in the same paradigm. Koehn and Haddow (2012) train a baseline phrase-based translation model, and back off to a factored model that 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Inﬂection bude absolvovat absolvuj absolvujete absolvoval jste Lemma ab"
2020.coling-main.387,I13-1112,0,0.674454,"cross alignments (Yarowsky and Ngai, 2001), and interlinear glossing. Further, fuller word lists enable neogrammarians to better explore phylogeny and phonology across languages (Hewson, 1973; Lowe and Mazaudon, 1994). This work is motivated by the tremendous capacity for humans to generalize during translation, producing forms for words that have not been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from neighboring or other influential languages, and even esoteric features like temporal similarity (Schafer and Yarowsky, 2002; Wijaya et al., 2017) or image similarity (Bergsma and Van Durme, 2011). In this work, we focus on these cognate relationships, because cognates form a large amount of both core vocabulary (Wu et al., 2020) and technical language (Mulloni, 2007).1 Unlike conventional bilingual lexicon induction (Rapp, 1995), we do not wish to limit the predictions to words that have been previously seen in a corpus. Automated methods to induce plausible translations for l"
2020.coling-main.387,W09-0106,0,0.00987249,"p h o n e t ´ı s t o p h o n e´ t i s t a phonetist phonetists p h o n e´ t i s t phonetisto p h o n e t ´ı s t p h a˜ o n e t i s t phonetista p h o n e t ´ı s t o p h o n e t i s¸ t phonetiste fonetista fonetist f o n e´ t i s t a f o n e t ´ı s t a fonetiste f o n eˆ t i s t a fonetistm f o n e t ´ı s t fonetists f o n e t i s t a´ Table 10: Portuguese example of how MMI reorders k-best lists to improve accuracy. 7 Future Work The work we presented here has particular applications to low-resource languages. As it is misguided to claim that our system is language-agnostic without verifying (Bender, 2009), we plan to expand this work to other language families, such as the Austronesian phonological cognate dataset of Bouchard-Cˆot´e et al. (2013). Another direction involves experimenting with non-uniform mixing weights that can adaptively give preference to certain languages, as in Wu and Yarowsky (2018). We would also like to extend this work to generate cognates of inflected forms, rather than lemmas and without explicit lemmatization and inflection subcomponents. Unlike existing cross-lingual morphological inflection tasks (McCarthy et al., 2019; Vylomova et al., 2020) the source and target"
2020.coling-main.387,P15-2071,0,0.0132898,"e the human effort needed for both elicitation (Chelliah, 2001) and building machine translation systems for less heavily supported languages. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 We use a pragmatic definition of cognacy based on orthographic or phonetic similarity across languages (Kondrak, 2001), which is adopted by other computational cognate research (Inkpen et al., 2005; Dinu and Ciobanu, 2014; Wu and Yarowsky, 2018). A stricter linguistic definition demands shared etymology. Ciobanu and Dinu (2015) distinguish between these computationally. 4373 Proceedings of the 28th International Conference on Computational Linguistics, pages 4373–4384 Barcelona, Spain (Online), December 8-13, 2020 Our approach to the problem of translation matrix completion is a neural model (parameterized as a character-level sequence-to-sequence network) that handles multiple language pairs, along with an objective function that maximizes both forward and backward probabilities. By leveraging both probabilities, we try to maximize the flow of information between both source and target languages, leading to more ac"
2020.coling-main.387,dinu-ciobanu-2014-building,0,0.585468,"orpus. Automated methods to induce plausible translations for lexical translation would significantly reduce the human effort needed for both elicitation (Chelliah, 2001) and building machine translation systems for less heavily supported languages. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 We use a pragmatic definition of cognacy based on orthographic or phonetic similarity across languages (Kondrak, 2001), which is adopted by other computational cognate research (Inkpen et al., 2005; Dinu and Ciobanu, 2014; Wu and Yarowsky, 2018). A stricter linguistic definition demands shared etymology. Ciobanu and Dinu (2015) distinguish between these computationally. 4373 Proceedings of the 28th International Conference on Computational Linguistics, pages 4373–4384 Barcelona, Spain (Online), December 8-13, 2020 Our approach to the problem of translation matrix completion is a neural model (parameterized as a character-level sequence-to-sequence network) that handles multiple language pairs, along with an objective function that maximizes both forward and backward probabilities. By leveraging both probabilit"
2020.coling-main.387,D19-1453,0,0.0228754,"ive improvement in whole-word accuracy of predictions over a single-source baseline. To seed the completion when multilingual data is unavailable, it is better to decode with an MMI objective. 1 Introduction Translation matrices, i.e. concept-aligned word lists across the world’s languages (Buck, 1949; Swadesh, 1950; Swadesh, 1952; Swadesh, 1955; Swadesh, 1971; Nastase and Strube, 2013; Wu et al., 2018), enable several avenues of exploration in computational linguistics and human language technologies. They strengthen word alignment models, which can in turn be useful for machine translation (Garg et al., 2019), robust projection of morphosyntactic information across alignments (Yarowsky and Ngai, 2001), and interlinear glossing. Further, fuller word lists enable neogrammarians to better explore phylogeny and phonology across languages (Hewson, 1973; Lowe and Mazaudon, 1994). This work is motivated by the tremendous capacity for humans to generalize during translation, producing forms for words that have not been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate r"
2020.coling-main.387,2020.sigmorphon-1.2,1,0.781013,"the model using a hidden size of 1024 in both the encoder and decoder, and embedding size of 512 with the NAG optimizer (Botev et al., 2017). In addition, we use a dropout of 0.25, clip gradients to 0.1, and use early stopping with a validation set after 5 epochs of no improvement. All MMI tradeoff λ values are 0.5 unless otherwise specified. We decode using a beam size of 10 and create k-best lists of length k=100. 3 We focus on LSTM models as opposed to Transformer architectures due to their superior performance on grapheme-tophoneme conversion, another monotonic sequence transduction task (Gorman et al., 2020). 4376 ro es fr it po tr ro es fr it po tr ro 0 46 62 42 45 39 ro 0 60 62 56 53 48 es 55 0 50 61 60 44 es 60 0 50 65 69 53 fr 58 56 0 53 43 44 fr 83 62 0 67 48 56 it 54 50 67 0 50 30 it 56 62 67 0 57 37 po 58 66 65 61 0 38 po 62 73 65 65 0 49 tr 49 34 78 33 53 0 tr 65 52 78 45 58 0 (a) NOVEL (b) SINGLE Table 2: NOVEL (Left), SINGLE (Right), exact-match accuracy percentage without reranking. Source language on Y-axis. Target language on X-axis. ro es fr it po tr avg NOVEL 55 50 64 50 50 39 51 SINGLE 65 62 64 60 57 49 59 NOVEL -MMI 60 53 63 51 53 47 55 SINGLE -MMI 66 62 63 58 59 63 60 M ULTI 65"
2020.coling-main.387,C73-1021,0,0.202086,"d word lists across the world’s languages (Buck, 1949; Swadesh, 1950; Swadesh, 1952; Swadesh, 1955; Swadesh, 1971; Nastase and Strube, 2013; Wu et al., 2018), enable several avenues of exploration in computational linguistics and human language technologies. They strengthen word alignment models, which can in turn be useful for machine translation (Garg et al., 2019), robust projection of morphosyntactic information across alignments (Yarowsky and Ngai, 2001), and interlinear glossing. Further, fuller word lists enable neogrammarians to better explore phylogeny and phonology across languages (Hewson, 1973; Lowe and Mazaudon, 1994). This work is motivated by the tremendous capacity for humans to generalize during translation, producing forms for words that have not been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from neighboring or other influential languages, and even esoteric features like temporal similarity (Schafer and Yarowsky, 2002; Wijaya et al., 2017) or ima"
2020.coling-main.387,E17-1113,0,0.0645607,"Missing"
2020.coling-main.387,W02-0902,0,0.0942059,"KNEE Table 1: Completion of cognate clusters. Given partially observed cognate clusters, the task is to infer missing values. “???” denotes words that are held out for testing purposes. 2 Related Work The task of translation matrix completion, the filling-out of a universal conceptual inventory, has been approached by three broad classes of methods. The first is to manually construct concept inventories, as in Swadesh (1950) and followup work. The next is to automatically identify cognate relationships, e.g. in word lists (Kondrak, 2001; Wijaya et al., 2017; J¨ager et al., 2017) or raw text (Koehn and Knight, 2002). The third, which is our focus, is to generate putative cognates by performing transduction in the form of sound or orthographic shifts. In this vein, Mann and Yarowsky (2001) generate cognates by a pipeline of dictionary lookup and probabilistic orthographic shifts. Mulloni (2007) uses an SVM to perform cognate generation. Ciobanu (2016) uses a CRF with reranking to the same end. Beinborn et al. (2013) and Wu and Yarowsky (2018) perform translation matrix completion with extracted cognate lists in 6 and 60 language families respectfully, using character-level statistical machine translation"
2020.coling-main.387,N01-1014,0,0.726601,", 1995), we do not wish to limit the predictions to words that have been previously seen in a corpus. Automated methods to induce plausible translations for lexical translation would significantly reduce the human effort needed for both elicitation (Chelliah, 2001) and building machine translation systems for less heavily supported languages. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 We use a pragmatic definition of cognacy based on orthographic or phonetic similarity across languages (Kondrak, 2001), which is adopted by other computational cognate research (Inkpen et al., 2005; Dinu and Ciobanu, 2014; Wu and Yarowsky, 2018). A stricter linguistic definition demands shared etymology. Ciobanu and Dinu (2015) distinguish between these computationally. 4373 Proceedings of the 28th International Conference on Computational Linguistics, pages 4373–4384 Barcelona, Spain (Online), December 8-13, 2020 Our approach to the problem of translation matrix completion is a neural model (parameterized as a character-level sequence-to-sequence network) that handles multiple language pairs, along with an o"
2020.coling-main.387,N16-1014,0,0.239647,"rmation across language pairs. We also take inspiration from recent successes in other generation tasks. Nishimura et al. (2018)’s multi-source missing data problem used multiple encoders and a single decoder to leverage multiple source language inputs, which we build on to employ the multiple sources simultaneously during inference. Further, we introduce introduce a maximum mutual information (MMI) objective to the problem, motivated by the translational equivalence of cognates (Hauer and Kondrak, 2020). MMI has been explored in speech recognition (Bahl et al., 1986; Brown, 1987) and dialog (Li et al., 2016). Besides MMI, there are a few existing methods for incorporating backward probabilities into the task of translation. Yee et al. (2019) and Ng et al. (2019) follow a noisy channel approach, using Bayes’ rule to integrate forward, backward and target language model probabilities. We follow Yee et al. (2019)’s approach and implement both the MMI objective and an ensemble MMI objective. 3 Translation Matrix Completion The core of this work is the translation matrix T (partially realized in Table 1), whose m rows are indexed by concepts c and whose n columns are indexed by languages `. The entrie"
2020.coling-main.387,J94-3004,0,0.0608374,"cross the world’s languages (Buck, 1949; Swadesh, 1950; Swadesh, 1952; Swadesh, 1955; Swadesh, 1971; Nastase and Strube, 2013; Wu et al., 2018), enable several avenues of exploration in computational linguistics and human language technologies. They strengthen word alignment models, which can in turn be useful for machine translation (Garg et al., 2019), robust projection of morphosyntactic information across alignments (Yarowsky and Ngai, 2001), and interlinear glossing. Further, fuller word lists enable neogrammarians to better explore phylogeny and phonology across languages (Hewson, 1973; Lowe and Mazaudon, 1994). This work is motivated by the tremendous capacity for humans to generalize during translation, producing forms for words that have not been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from neighboring or other influential languages, and even esoteric features like temporal similarity (Schafer and Yarowsky, 2002; Wijaya et al., 2017) or image similarity (Bergsma and"
2020.coling-main.387,N01-1020,1,0.683584,"g purposes. 2 Related Work The task of translation matrix completion, the filling-out of a universal conceptual inventory, has been approached by three broad classes of methods. The first is to manually construct concept inventories, as in Swadesh (1950) and followup work. The next is to automatically identify cognate relationships, e.g. in word lists (Kondrak, 2001; Wijaya et al., 2017; J¨ager et al., 2017) or raw text (Koehn and Knight, 2002). The third, which is our focus, is to generate putative cognates by performing transduction in the form of sound or orthographic shifts. In this vein, Mann and Yarowsky (2001) generate cognates by a pipeline of dictionary lookup and probabilistic orthographic shifts. Mulloni (2007) uses an SVM to perform cognate generation. Ciobanu (2016) uses a CRF with reranking to the same end. Beinborn et al. (2013) and Wu and Yarowsky (2018) perform translation matrix completion with extracted cognate lists in 6 and 60 language families respectfully, using character-level statistical machine translation systems trained on separate source-target language pairs. Wu et al. (2020) performed the same cognate transliteration task with a multi-source multi-target character-level vari"
2020.coling-main.387,W19-4226,1,0.808793,"at our system is language-agnostic without verifying (Bender, 2009), we plan to expand this work to other language families, such as the Austronesian phonological cognate dataset of Bouchard-Cˆot´e et al. (2013). Another direction involves experimenting with non-uniform mixing weights that can adaptively give preference to certain languages, as in Wu and Yarowsky (2018). We would also like to extend this work to generate cognates of inflected forms, rather than lemmas and without explicit lemmatization and inflection subcomponents. Unlike existing cross-lingual morphological inflection tasks (McCarthy et al., 2019; Vylomova et al., 2020) the source and target are in different languages, rather than relying on transfer. Finally, to assess the downstream value of this linguistic tool, future work could populate a statistical translation model’s phrase table with predictions from the model. 8 Conclusion We present a single neural model to handle multilingual many-to-many translation of single words. In addition, by indirectly leveraging multi-lingual information in sequence-to-sequence models, we can improve accuracy in the matrix completion task (NOVEL vs SINGLE). By allowing knowledge of concepts that w"
2020.coling-main.387,P07-3005,0,0.500975,"c information across alignments (Yarowsky and Ngai, 2001), and interlinear glossing. Further, fuller word lists enable neogrammarians to better explore phylogeny and phonology across languages (Hewson, 1973; Lowe and Mazaudon, 1994). This work is motivated by the tremendous capacity for humans to generalize during translation, producing forms for words that have not been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from neighboring or other influential languages, and even esoteric features like temporal similarity (Schafer and Yarowsky, 2002; Wijaya et al., 2017) or image similarity (Bergsma and Van Durme, 2011). In this work, we focus on these cognate relationships, because cognates form a large amount of both core vocabulary (Wu et al., 2020) and technical language (Mulloni, 2007).1 Unlike conventional bilingual lexicon induction (Rapp, 1995), we do not wish to limit the predictions to words that have been previously seen in a corpus. Automated methods to induce plau"
2020.coling-main.387,W19-5333,0,0.020574,"problem used multiple encoders and a single decoder to leverage multiple source language inputs, which we build on to employ the multiple sources simultaneously during inference. Further, we introduce introduce a maximum mutual information (MMI) objective to the problem, motivated by the translational equivalence of cognates (Hauer and Kondrak, 2020). MMI has been explored in speech recognition (Bahl et al., 1986; Brown, 1987) and dialog (Li et al., 2016). Besides MMI, there are a few existing methods for incorporating backward probabilities into the task of translation. Yee et al. (2019) and Ng et al. (2019) follow a noisy channel approach, using Bayes’ rule to integrate forward, backward and target language model probabilities. We follow Yee et al. (2019)’s approach and implement both the MMI objective and an ensemble MMI objective. 3 Translation Matrix Completion The core of this work is the translation matrix T (partially realized in Table 1), whose m rows are indexed by concepts c and whose n columns are indexed by languages `. The entries Tc,` in the matrix T are orthographic sequences drawn from each language’s alphabet Σ` that each form a word or multi-word expression. These entries may be"
2020.coling-main.387,W18-2711,0,0.0159578,"013) and Wu and Yarowsky (2018) perform translation matrix completion with extracted cognate lists in 6 and 60 language families respectfully, using character-level statistical machine translation systems trained on separate source-target language pairs. Wu et al. (2020) performed the same cognate transliteration task with a multi-source multi-target character-level variant of Johnson et al. (2017). We adopt the single system multilingual setup of Wu et al. (2020), which allows sharing information across language pairs. We also take inspiration from recent successes in other generation tasks. Nishimura et al. (2018)’s multi-source missing data problem used multiple encoders and a single decoder to leverage multiple source language inputs, which we build on to employ the multiple sources simultaneously during inference. Further, we introduce introduce a maximum mutual information (MMI) objective to the problem, motivated by the translational equivalence of cognates (Hauer and Kondrak, 2020). MMI has been explored in speech recognition (Bahl et al., 1986; Brown, 1987) and dialog (Li et al., 2016). Besides MMI, there are a few existing methods for incorporating backward probabilities into the task of transl"
2020.coling-main.387,N19-4009,0,0.0184518,"te generation quality. For all, higher is better. The first is exact string match accuracy, following Wu and Yarowsky (2018): does the model’s 1-best prediction correctly predict the unknown word? The others refine the notion of “inaccurate.” Character-level BLEU using SacreBLEU (Papineni et al., 2002; Post, 2018), awards partial credit for inexact matches. Mean reciprocal rank (MRR), following Ciobanu (2016), answers: how far down the k-best list is the correct form? Experimental details We use a sequence-to-sequence LSTM model with attention (Bahdanau et al., 2015) from the FAIRSEQ toolkit (Ott et al., 2019).3 We train the model using a hidden size of 1024 in both the encoder and decoder, and embedding size of 512 with the NAG optimizer (Botev et al., 2017). In addition, we use a dropout of 0.25, clip gradients to 0.1, and use early stopping with a validation set after 5 epochs of no improvement. All MMI tradeoff λ values are 0.5 unless otherwise specified. We decode using a beam size of 10 and create k-best lists of length k=100. 3 We focus on LSTM models as opposed to Transformer architectures due to their superior performance on grapheme-tophoneme conversion, another monotonic sequence transdu"
2020.coling-main.387,P02-1040,0,0.106774,"st set. While not all slots in the Cartesian product of data scenarios (NOVEL, SINGLE, MULTI), decoding objectives (conditional log-likelihood or MMI), and ensembling methods (mixture or product of experts) are plausible, this product subsumes the experiments we run. Evaluation methods We report three metrics of cognate generation quality. For all, higher is better. The first is exact string match accuracy, following Wu and Yarowsky (2018): does the model’s 1-best prediction correctly predict the unknown word? The others refine the notion of “inaccurate.” Character-level BLEU using SacreBLEU (Papineni et al., 2002; Post, 2018), awards partial credit for inexact matches. Mean reciprocal rank (MRR), following Ciobanu (2016), answers: how far down the k-best list is the correct form? Experimental details We use a sequence-to-sequence LSTM model with attention (Bahdanau et al., 2015) from the FAIRSEQ toolkit (Ott et al., 2019).3 We train the model using a hidden size of 1024 in both the encoder and decoder, and embedding size of 512 with the NAG optimizer (Botev et al., 2017). In addition, we use a dropout of 0.25, clip gradients to 0.1, and use early stopping with a validation set after 5 epochs of no imp"
2020.coling-main.387,W18-6319,0,0.0271702,"lots in the Cartesian product of data scenarios (NOVEL, SINGLE, MULTI), decoding objectives (conditional log-likelihood or MMI), and ensembling methods (mixture or product of experts) are plausible, this product subsumes the experiments we run. Evaluation methods We report three metrics of cognate generation quality. For all, higher is better. The first is exact string match accuracy, following Wu and Yarowsky (2018): does the model’s 1-best prediction correctly predict the unknown word? The others refine the notion of “inaccurate.” Character-level BLEU using SacreBLEU (Papineni et al., 2002; Post, 2018), awards partial credit for inexact matches. Mean reciprocal rank (MRR), following Ciobanu (2016), answers: how far down the k-best list is the correct form? Experimental details We use a sequence-to-sequence LSTM model with attention (Bahdanau et al., 2015) from the FAIRSEQ toolkit (Ott et al., 2019).3 We train the model using a hidden size of 1024 in both the encoder and decoder, and embedding size of 512 with the NAG optimizer (Botev et al., 2017). In addition, we use a dropout of 0.25, clip gradients to 0.1, and use early stopping with a validation set after 5 epochs of no improvement. All"
2020.coling-main.387,P95-1050,0,0.304693,"ved in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from neighboring or other influential languages, and even esoteric features like temporal similarity (Schafer and Yarowsky, 2002; Wijaya et al., 2017) or image similarity (Bergsma and Van Durme, 2011). In this work, we focus on these cognate relationships, because cognates form a large amount of both core vocabulary (Wu et al., 2020) and technical language (Mulloni, 2007).1 Unlike conventional bilingual lexicon induction (Rapp, 1995), we do not wish to limit the predictions to words that have been previously seen in a corpus. Automated methods to induce plausible translations for lexical translation would significantly reduce the human effort needed for both elicitation (Chelliah, 2001) and building machine translation systems for less heavily supported languages. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 We use a pragmatic definition of cognacy based on orthographic or phonetic similarity across languages (Kondrak"
2020.coling-main.387,W02-2026,1,0.560596,"phylogeny and phonology across languages (Hewson, 1973; Lowe and Mazaudon, 1994). This work is motivated by the tremendous capacity for humans to generalize during translation, producing forms for words that have not been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from neighboring or other influential languages, and even esoteric features like temporal similarity (Schafer and Yarowsky, 2002; Wijaya et al., 2017) or image similarity (Bergsma and Van Durme, 2011). In this work, we focus on these cognate relationships, because cognates form a large amount of both core vocabulary (Wu et al., 2020) and technical language (Mulloni, 2007).1 Unlike conventional bilingual lexicon induction (Rapp, 1995), we do not wish to limit the predictions to words that have been previously seen in a corpus. Automated methods to induce plausible translations for lexical translation would significantly reduce the human effort needed for both elicitation (Chelliah, 2001) and building machine translation"
2020.coling-main.387,D17-1152,0,0.115028,"oss languages (Hewson, 1973; Lowe and Mazaudon, 1994). This work is motivated by the tremendous capacity for humans to generalize during translation, producing forms for words that have not been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from neighboring or other influential languages, and even esoteric features like temporal similarity (Schafer and Yarowsky, 2002; Wijaya et al., 2017) or image similarity (Bergsma and Van Durme, 2011). In this work, we focus on these cognate relationships, because cognates form a large amount of both core vocabulary (Wu et al., 2020) and technical language (Mulloni, 2007).1 Unlike conventional bilingual lexicon induction (Rapp, 1995), we do not wish to limit the predictions to words that have been previously seen in a corpus. Automated methods to induce plausible translations for lexical translation would significantly reduce the human effort needed for both elicitation (Chelliah, 2001) and building machine translation systems for less heav"
2020.coling-main.387,L18-1538,1,0.419971,"to induce plausible translations for lexical translation would significantly reduce the human effort needed for both elicitation (Chelliah, 2001) and building machine translation systems for less heavily supported languages. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 We use a pragmatic definition of cognacy based on orthographic or phonetic similarity across languages (Kondrak, 2001), which is adopted by other computational cognate research (Inkpen et al., 2005; Dinu and Ciobanu, 2014; Wu and Yarowsky, 2018). A stricter linguistic definition demands shared etymology. Ciobanu and Dinu (2015) distinguish between these computationally. 4373 Proceedings of the 28th International Conference on Computational Linguistics, pages 4373–4384 Barcelona, Spain (Online), December 8-13, 2020 Our approach to the problem of translation matrix completion is a neural model (parameterized as a character-level sequence-to-sequence network) that handles multiple language pairs, along with an objective function that maximizes both forward and backward probabilities. By leveraging both probabilities, we try to maximize"
2020.coling-main.387,L18-1263,1,0.524193,"Missing"
2020.coling-main.387,2020.lrec-1.519,1,0.65023,"been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from neighboring or other influential languages, and even esoteric features like temporal similarity (Schafer and Yarowsky, 2002; Wijaya et al., 2017) or image similarity (Bergsma and Van Durme, 2011). In this work, we focus on these cognate relationships, because cognates form a large amount of both core vocabulary (Wu et al., 2020) and technical language (Mulloni, 2007).1 Unlike conventional bilingual lexicon induction (Rapp, 1995), we do not wish to limit the predictions to words that have been previously seen in a corpus. Automated methods to induce plausible translations for lexical translation would significantly reduce the human effort needed for both elicitation (Chelliah, 2001) and building machine translation systems for less heavily supported languages. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 We use a"
2020.coling-main.387,N01-1026,1,0.629695,"seed the completion when multilingual data is unavailable, it is better to decode with an MMI objective. 1 Introduction Translation matrices, i.e. concept-aligned word lists across the world’s languages (Buck, 1949; Swadesh, 1950; Swadesh, 1952; Swadesh, 1955; Swadesh, 1971; Nastase and Strube, 2013; Wu et al., 2018), enable several avenues of exploration in computational linguistics and human language technologies. They strengthen word alignment models, which can in turn be useful for machine translation (Garg et al., 2019), robust projection of morphosyntactic information across alignments (Yarowsky and Ngai, 2001), and interlinear glossing. Further, fuller word lists enable neogrammarians to better explore phylogeny and phonology across languages (Hewson, 1973; Lowe and Mazaudon, 1994). This work is motivated by the tremendous capacity for humans to generalize during translation, producing forms for words that have not been seen before. This becomes valuable especially for lowerfrequency words, which may not have been observed in training data but could be inferrable through regular processes such as cognate relationships with related languages (Mulloni, 2007; Beinborn et al., 2013), borrowing from nei"
2020.coling-main.387,D19-1571,0,0.0190754,"i-source missing data problem used multiple encoders and a single decoder to leverage multiple source language inputs, which we build on to employ the multiple sources simultaneously during inference. Further, we introduce introduce a maximum mutual information (MMI) objective to the problem, motivated by the translational equivalence of cognates (Hauer and Kondrak, 2020). MMI has been explored in speech recognition (Bahl et al., 1986; Brown, 1987) and dialog (Li et al., 2016). Besides MMI, there are a few existing methods for incorporating backward probabilities into the task of translation. Yee et al. (2019) and Ng et al. (2019) follow a noisy channel approach, using Bayes’ rule to integrate forward, backward and target language model probabilities. We follow Yee et al. (2019)’s approach and implement both the MMI objective and an ensemble MMI objective. 3 Translation Matrix Completion The core of this work is the translation matrix T (partially realized in Table 1), whose m rows are indexed by concepts c and whose n columns are indexed by languages `. The entries Tc,` in the matrix T are orthographic sequences drawn from each language’s alphabet Σ` that each form a word or multi-word expression."
2020.coling-main.413,acs-2014-pivot,0,0.0720173,"Missing"
2020.coling-main.413,I11-1160,0,0.00993727,"ort is UniMorph (Kirov et al., 2016; Kirov et al., 2018; McCarthy et al., 2020), a large broad-coverage resource comprising morphological paradigms of nouns, adjectives, and verbs in 118 languages extracted from Wiktionary. Other large-scale parsing efforts for targeted tasks include NULEX (McFate and Forbus, 2011) for parsing, IWNLP (Liebeck and Conrad, 2015) for lemmatization, and WikiPron (Lee et al., 2020) for pronunciations. Related to the word formation mechanisms we examine, Kulkarni and Wang (2018) examine word formation in slang, specifically blends, clippings, and reduplication, and Brooke et al. (2011) predict clipping using a LSA-based approach. Contractions are not typically studied in a predictive context; Volk and Sennrich (2011) disambiguates contractions as a preprocessing step in machine translation. Researchers have recently examined eye dialect in the context of spelling correction (Eryani et al., 2020; Himoro and Pareja-Lora, 2020), but to our knowledge, this paper is the first study on eye dialect generation. 3 Extracting Translations from Etymology Glosses Wiktionary contains translations in a specialized Translation section. W&Y extract these translations, as well as “translati"
2020.coling-main.413,2020.lrec-1.508,0,0.014909,"for parsing, IWNLP (Liebeck and Conrad, 2015) for lemmatization, and WikiPron (Lee et al., 2020) for pronunciations. Related to the word formation mechanisms we examine, Kulkarni and Wang (2018) examine word formation in slang, specifically blends, clippings, and reduplication, and Brooke et al. (2011) predict clipping using a LSA-based approach. Contractions are not typically studied in a predictive context; Volk and Sennrich (2011) disambiguates contractions as a preprocessing step in machine translation. Researchers have recently examined eye dialect in the context of spelling correction (Eryani et al., 2020; Himoro and Pareja-Lora, 2020), but to our knowledge, this paper is the first study on eye dialect generation. 3 Extracting Translations from Etymology Glosses Wiktionary contains translations in a specialized Translation section. W&Y extract these translations, as well as “translations” from the definition section of non-English word entries. Since non-English words have English definitions (in the English Wiktionary), short definitions can be regarded as viable translations. One unusual but particularly fruitful source of translations that has not been previously considered is glosses in th"
2020.coling-main.413,2020.sigmorphon-1.2,0,0.278797,"g. Though 1-best and 5-best accuracies across all three tasks seem low, actually on average the results are only 1–2 characters off from the gold; we see the model consistently making plausible predictions with similar sounds. In addition, the models with copy attention consistently outperform the models with a standard Luong attention. Due to space constraints, sample predictions are presented in Appendix B, and improvements of the copy attention model over the Luong attention model are in Appendix C. 3 For monotonic sequence-to-sequence tasks, LSTMs tend to perform better than Transformers (Gorman et al., 2020). 4686 Analysis Clippings tend to keep the beginning part of the word (speculation → spec), which the model learned (Spotlight → Spot), albeit sometimes incorrectly (Alfredino → Alfe, gold is Dino). A large percentage of clippings are in Japanese; if the input is written in katakana, the model can sometimes make a correct prediction, but if written in kanji, the model gets it completely wrong, due to the rarity of the characters. These errors are corrected by the copy attention model, which learns to copy over characters that would otherwise be unlikely to be generated. Contraction is perhaps"
2020.coling-main.413,P16-1154,0,0.0118991,"at generating clippings, contradictions, and eye dialectical variations. For example, changing the language style of chatbots has been shown to increase user satisfaction (Elsholz et al., 2019). Models We use a character neural machine translation setup. Using OpenNMT-py (Klein et al., 2017), we employ a 2-layer LSTM encoder-decoder3 with 256-dimension hidden and embedding size, batch size 64, Adam optimizer with learning rate 0.001, and patience of 5. We train two model variants, a baseline with Luong attention (Luong et al., 2015) (the default in OpenNMT), and a second with copy attention (Gu et al., 2016). For eye dialect, we only use English data, as the overwhelming majority of annotations are English. For clipping and contraction, we employ the entire range of languages annotated, thus making our models multi-source, multi-target systems. We use a randomly shuffled 80-10-10 train-dev-test split. The input and output format of each experiment, as well as results are presented in Table 5. Task Top 5 languages (count) Total Languages Clipping Contraction Eye Dialect en (575), ja (246), pt (118), de (67), fr (56) en (414), pt (96), de (79), dum (63), ga (50) en (1646), pt (149), vi (89), da (35"
2020.coling-main.413,2020.lrec-1.327,0,0.0147859,"Liebeck and Conrad, 2015) for lemmatization, and WikiPron (Lee et al., 2020) for pronunciations. Related to the word formation mechanisms we examine, Kulkarni and Wang (2018) examine word formation in slang, specifically blends, clippings, and reduplication, and Brooke et al. (2011) predict clipping using a LSA-based approach. Contractions are not typically studied in a predictive context; Volk and Sennrich (2011) disambiguates contractions as a preprocessing step in machine translation. Researchers have recently examined eye dialect in the context of spelling correction (Eryani et al., 2020; Himoro and Pareja-Lora, 2020), but to our knowledge, this paper is the first study on eye dialect generation. 3 Extracting Translations from Etymology Glosses Wiktionary contains translations in a specialized Translation section. W&Y extract these translations, as well as “translations” from the definition section of non-English word entries. Since non-English words have English definitions (in the English Wiktionary), short definitions can be regarded as viable translations. One unusual but particularly fruitful source of translations that has not been previously considered is glosses in the Etymology section of an entry"
2020.coling-main.413,2020.sigmorphon-1.3,0,0.0140221,"full histogram of which is in Appendix A. While different relations have different requirements as to where they can appear in an entry (e.g. some relations can only appear in the etymology section), form-of relations are relatively straightforward to extract and normalize due to the consistency of their templates. Many inflectional relations for both nouns and verbs, including relations such as inflection-of, genetivesingular-of, or past-participle-of, are already packaged in UniMorph and have been used in tasks such as morphological inflection analysis and prediction (McCarthy et al., 2019; Kann et al., 2020). Other relations, such as plural-of and feminine-form-of can augment training data for morphological analysis systems such as that of Nicolai and Yarowsky (2019). However, much of the rest of this form-of data has not been thoroughly explored. Below, we present preliminary experiments on clipping, contraction, and 1 2 Rendered in HTML as: from Latin ab (“from, away from”) A comprehensive list is at https://en.wiktionary.org/wiki/Category:Form-of_templates 4685 eye dialect, three understudied types of data whose further research is enabled through our extraction and normalization. 4.1 Experime"
2020.coling-main.413,L16-1498,1,0.816665,"y parsing efforts (e.g. knoWitiary (Nastase and Strapparava, 2015), DBnary (S´erasset, 2015), and ENGLAWI (Sajous et al., 2020)), but with different goals and coverage. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 4683 Proceedings of the 28th International Conference on Computational Linguistics, pages 4683–4692 Barcelona, Spain (Online), December 8-13, 2020 ´ (2014) Most studies on translation extraction have utilized the translation section of an entry: Acs using a triangulation approach, Kirov et al. (2016) for morphophological analysis, and Wu and Yarowsky (2020) as part of a comprehensive Wiktionary parsing effort. DBnary (S´erasset, 2015) is a similar effort at parsing certain lexical data, including translations, from Wiktionary into a structured format. Regarding extracting morphological relations between words, the foremost effort is UniMorph (Kirov et al., 2016; Kirov et al., 2018; McCarthy et al., 2020), a large broad-coverage resource comprising morphological paradigms of nouns, adjectives, and verbs in 118 languages extracted from Wiktionary. Other large-scale parsing efforts for targe"
2020.coling-main.413,L18-1293,1,0.896051,"Missing"
2020.coling-main.413,P17-4012,0,0.0161998,"otations across a wide range of languages. The amount of annotations is also quite small: the total amount of data is only around 1-2K instances per task (Table 4). While there has not been much published computational literature on these tasks, we envision interesting potential downstream applications for systems successful at generating clippings, contradictions, and eye dialectical variations. For example, changing the language style of chatbots has been shown to increase user satisfaction (Elsholz et al., 2019). Models We use a character neural machine translation setup. Using OpenNMT-py (Klein et al., 2017), we employ a 2-layer LSTM encoder-decoder3 with 256-dimension hidden and embedding size, batch size 64, Adam optimizer with learning rate 0.001, and patience of 5. We train two model variants, a baseline with Luong attention (Luong et al., 2015) (the default in OpenNMT), and a second with copy attention (Gu et al., 2016). For eye dialect, we only use English data, as the overwhelming majority of annotations are English. For clipping and contraction, we employ the entire range of languages annotated, thus making our models multi-source, multi-target systems. We use a randomly shuffled 80-10-10"
2020.coling-main.413,N18-1129,0,0.0182075,"Wiktionary into a structured format. Regarding extracting morphological relations between words, the foremost effort is UniMorph (Kirov et al., 2016; Kirov et al., 2018; McCarthy et al., 2020), a large broad-coverage resource comprising morphological paradigms of nouns, adjectives, and verbs in 118 languages extracted from Wiktionary. Other large-scale parsing efforts for targeted tasks include NULEX (McFate and Forbus, 2011) for parsing, IWNLP (Liebeck and Conrad, 2015) for lemmatization, and WikiPron (Lee et al., 2020) for pronunciations. Related to the word formation mechanisms we examine, Kulkarni and Wang (2018) examine word formation in slang, specifically blends, clippings, and reduplication, and Brooke et al. (2011) predict clipping using a LSA-based approach. Contractions are not typically studied in a predictive context; Volk and Sennrich (2011) disambiguates contractions as a preprocessing step in machine translation. Researchers have recently examined eye dialect in the context of spelling correction (Eryani et al., 2020; Himoro and Pareja-Lora, 2020), but to our knowledge, this paper is the first study on eye dialect generation. 3 Extracting Translations from Etymology Glosses Wiktionary cont"
2020.coling-main.413,2020.lrec-1.521,0,0.0130616,"t, 2015) is a similar effort at parsing certain lexical data, including translations, from Wiktionary into a structured format. Regarding extracting morphological relations between words, the foremost effort is UniMorph (Kirov et al., 2016; Kirov et al., 2018; McCarthy et al., 2020), a large broad-coverage resource comprising morphological paradigms of nouns, adjectives, and verbs in 118 languages extracted from Wiktionary. Other large-scale parsing efforts for targeted tasks include NULEX (McFate and Forbus, 2011) for parsing, IWNLP (Liebeck and Conrad, 2015) for lemmatization, and WikiPron (Lee et al., 2020) for pronunciations. Related to the word formation mechanisms we examine, Kulkarni and Wang (2018) examine word formation in slang, specifically blends, clippings, and reduplication, and Brooke et al. (2011) predict clipping using a LSA-based approach. Contractions are not typically studied in a predictive context; Volk and Sennrich (2011) disambiguates contractions as a preprocessing step in machine translation. Researchers have recently examined eye dialect in the context of spelling correction (Eryani et al., 2020; Himoro and Pareja-Lora, 2020), but to our knowledge, this paper is the first"
2020.coling-main.413,P15-2068,0,0.0289703,"a comprehensive Wiktionary parsing effort. DBnary (S´erasset, 2015) is a similar effort at parsing certain lexical data, including translations, from Wiktionary into a structured format. Regarding extracting morphological relations between words, the foremost effort is UniMorph (Kirov et al., 2016; Kirov et al., 2018; McCarthy et al., 2020), a large broad-coverage resource comprising morphological paradigms of nouns, adjectives, and verbs in 118 languages extracted from Wiktionary. Other large-scale parsing efforts for targeted tasks include NULEX (McFate and Forbus, 2011) for parsing, IWNLP (Liebeck and Conrad, 2015) for lemmatization, and WikiPron (Lee et al., 2020) for pronunciations. Related to the word formation mechanisms we examine, Kulkarni and Wang (2018) examine word formation in slang, specifically blends, clippings, and reduplication, and Brooke et al. (2011) predict clipping using a LSA-based approach. Contractions are not typically studied in a predictive context; Volk and Sennrich (2011) disambiguates contractions as a preprocessing step in machine translation. Researchers have recently examined eye dialect in the context of spelling correction (Eryani et al., 2020; Himoro and Pareja-Lora, 2"
2020.coling-main.413,D15-1166,0,0.0489076,"we envision interesting potential downstream applications for systems successful at generating clippings, contradictions, and eye dialectical variations. For example, changing the language style of chatbots has been shown to increase user satisfaction (Elsholz et al., 2019). Models We use a character neural machine translation setup. Using OpenNMT-py (Klein et al., 2017), we employ a 2-layer LSTM encoder-decoder3 with 256-dimension hidden and embedding size, batch size 64, Adam optimizer with learning rate 0.001, and patience of 5. We train two model variants, a baseline with Luong attention (Luong et al., 2015) (the default in OpenNMT), and a second with copy attention (Gu et al., 2016). For eye dialect, we only use English data, as the overwhelming majority of annotations are English. For clipping and contraction, we employ the entire range of languages annotated, thus making our models multi-source, multi-target systems. We use a randomly shuffled 80-10-10 train-dev-test split. The input and output format of each experiment, as well as results are presented in Table 5. Task Top 5 languages (count) Total Languages Clipping Contraction Eye Dialect en (575), ja (246), pt (118), de (67), fr (56) en (4"
2020.coling-main.413,W19-4226,0,0.0187257,"168 relation types, a full histogram of which is in Appendix A. While different relations have different requirements as to where they can appear in an entry (e.g. some relations can only appear in the etymology section), form-of relations are relatively straightforward to extract and normalize due to the consistency of their templates. Many inflectional relations for both nouns and verbs, including relations such as inflection-of, genetivesingular-of, or past-participle-of, are already packaged in UniMorph and have been used in tasks such as morphological inflection analysis and prediction (McCarthy et al., 2019; Kann et al., 2020). Other relations, such as plural-of and feminine-form-of can augment training data for morphological analysis systems such as that of Nicolai and Yarowsky (2019). However, much of the rest of this form-of data has not been thoroughly explored. Below, we present preliminary experiments on clipping, contraction, and 1 2 Rendered in HTML as: from Latin ab (“from, away from”) A comprehensive list is at https://en.wiktionary.org/wiki/Category:Form-of_templates 4685 eye dialect, three understudied types of data whose further research is enabled through our extraction and normali"
2020.coling-main.413,P11-2063,0,0.0181406,"lysis, and Wu and Yarowsky (2020) as part of a comprehensive Wiktionary parsing effort. DBnary (S´erasset, 2015) is a similar effort at parsing certain lexical data, including translations, from Wiktionary into a structured format. Regarding extracting morphological relations between words, the foremost effort is UniMorph (Kirov et al., 2016; Kirov et al., 2018; McCarthy et al., 2020), a large broad-coverage resource comprising morphological paradigms of nouns, adjectives, and verbs in 118 languages extracted from Wiktionary. Other large-scale parsing efforts for targeted tasks include NULEX (McFate and Forbus, 2011) for parsing, IWNLP (Liebeck and Conrad, 2015) for lemmatization, and WikiPron (Lee et al., 2020) for pronunciations. Related to the word formation mechanisms we examine, Kulkarni and Wang (2018) examine word formation in slang, specifically blends, clippings, and reduplication, and Brooke et al. (2011) predict clipping using a LSA-based approach. Contractions are not typically studied in a predictive context; Volk and Sennrich (2011) disambiguates contractions as a preprocessing step in machine translation. Researchers have recently examined eye dialect in the context of spelling correction ("
2020.coling-main.413,W09-3303,0,0.0368755,"Missing"
2020.coling-main.413,P19-1172,1,0.825824,"ons can only appear in the etymology section), form-of relations are relatively straightforward to extract and normalize due to the consistency of their templates. Many inflectional relations for both nouns and verbs, including relations such as inflection-of, genetivesingular-of, or past-participle-of, are already packaged in UniMorph and have been used in tasks such as morphological inflection analysis and prediction (McCarthy et al., 2019; Kann et al., 2020). Other relations, such as plural-of and feminine-form-of can augment training data for morphological analysis systems such as that of Nicolai and Yarowsky (2019). However, much of the rest of this form-of data has not been thoroughly explored. Below, we present preliminary experiments on clipping, contraction, and 1 2 Rendered in HTML as: from Latin ab (“from, away from”) A comprehensive list is at https://en.wiktionary.org/wiki/Category:Form-of_templates 4685 eye dialect, three understudied types of data whose further research is enabled through our extraction and normalization. 4.1 Experiments We experiment with predicting three form-of relations. Clipping is a process of word formation in which a part of the word gets “clipped” or truncated to form"
2020.coling-main.413,2020.lrec-1.369,0,0.0330297,"y of data from multiple language editions of Wiktionary into a structured machine-readable format. Yawipa’s goal is to be comprehensive and extensible. To that end, Yawipa goes beyond existing parsers in extracting and normalizing information, such as etymology and translations, that exist outside of structured Wiktionary markup (we further this goal in this paper), and it facilitates the creation of new parsers for other Wiktionary editions. In the literature, there are similar Wiktionary parsing efforts (e.g. knoWitiary (Nastase and Strapparava, 2015), DBnary (S´erasset, 2015), and ENGLAWI (Sajous et al., 2020)), but with different goals and coverage. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 4683 Proceedings of the 28th International Conference on Computational Linguistics, pages 4683–4692 Barcelona, Spain (Online), December 8-13, 2020 ´ (2014) Most studies on translation extraction have utilized the translation section of an entry: Acs using a triangulation approach, Kirov et al. (2016) for morphophological analysis, and Wu and Yarowsky (2020) as part of a comprehensive Wiktionary parsing eff"
2020.coling-main.413,W11-4633,0,0.0357404,"logical paradigms of nouns, adjectives, and verbs in 118 languages extracted from Wiktionary. Other large-scale parsing efforts for targeted tasks include NULEX (McFate and Forbus, 2011) for parsing, IWNLP (Liebeck and Conrad, 2015) for lemmatization, and WikiPron (Lee et al., 2020) for pronunciations. Related to the word formation mechanisms we examine, Kulkarni and Wang (2018) examine word formation in slang, specifically blends, clippings, and reduplication, and Brooke et al. (2011) predict clipping using a LSA-based approach. Contractions are not typically studied in a predictive context; Volk and Sennrich (2011) disambiguates contractions as a preprocessing step in machine translation. Researchers have recently examined eye dialect in the context of spelling correction (Eryani et al., 2020; Himoro and Pareja-Lora, 2020), but to our knowledge, this paper is the first study on eye dialect generation. 3 Extracting Translations from Etymology Glosses Wiktionary contains translations in a specialized Translation section. W&Y extract these translations, as well as “translations” from the definition section of non-English word entries. Since non-English words have English definitions (in the English Wiktion"
2020.coling-main.413,2020.lrec-1.397,1,0.309569,"normalize translations from etymology glosses, and morphological form-of relations, resulting in 300K unique translations and over 4 million instances of 168 annotated morphological relations. We propose a method to identify typos in translation annotations. Using the extracted morphological data, we develop multilingual neural models for predicting three types of word formation— clipping, contraction, and eye dialect—and improve upon a standard attention baseline by using copy attention. 1 Introduction Wiktionary is a large, free multilingual dictionary with a wealth of information. Yawipa (Wu and Yarowsky, 2020), henceforth W&Y, is a recent Wiktionary parser billed as “comprehensive and extensible.” It has the ability to extract numerous types information from Wiktionary, including pronunciations, part of speech, translations, etymology, and a wide range of word relations, and normalize it into an easy to process tabular format. In particular, one of Yawipa’s innovations over existing parsers was extracting translations from the definition section of a dictionary definition. Confirming its easy extensibility and improving upon its comprehensiveness, we extend Yawipa’s extraction and normalization of"
2020.emnlp-main.456,S07-1012,0,0.0655164,"of nouns clustered by their gender, with the same nouns clustered by the adjectives that modify them or the verbs that take them as arguments. Although we adopt information theoretic measures, here there are two other major classes of cluster evaluation measures: set-matching measures, and pair-counting measures, which tally which pairs of items are in the same or different communities. One popular set-matching measure in information retrieval, purity (Manning et al., 2008), is asymmetric and biased by the size and number of communities (Danon et al., 2005). Its symmetric form, the F-measure (Artiles et al., 2007), has clear bounds but gives no indication of average-case performance. The adjusted Rand index (ARI; Hubert and Arabie, 1985) is the preeminent pair-counting measure. It is related to AMI, adjusting the Rand index in the same way that AMI adjusts MI. ARI also computes an expectation, which can be computed over the proper distribution (Gates and Ahn, 2017), but it Dataset Measure Score St. Dev. Swadesh MI VI AMI Random MI VI AMI Random 344 312 344 1184 1231 1164 1548 2531 133.4 209.6 NorthEuraLex Table 1: Distances of generated trees from gold tree. is empirically better suited to large, balan"
2020.emnlp-main.456,N19-1415,1,0.845587,"in our languages, with measurable success. Separate Indo-European branches are no more similar than chance. We emphasize that our methods are not specifically tailored to gender systems. One could apply them more broadly other aspects of the lexicon, e.g. to Indo-European verb classes, Bantu noun classes, or diachronic time slices of a single language’s gender system, data permitting. A related challenge is East and Southeast Asian numeral classifier systems, which associate nouns with classifiers based largely on the semantic properties of the nouns (Kuo and Sera, 2009; Zhan and Levy, 2018; Liu et al., 2019). They display more idiolectal variation, and often more than one classifier can accompany a given noun (Hu, 1993), unlike for gender (where this is rare). We note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work. 5671 Acknowledgments We thank Tongfei Chen for comments on the Slavic languages, Jean-Gabriel Young for suggesting that we consider Variation of Information, and Johannes Bjerva for providing us with code to compute the tree distance. We also thank Tiago Pimentel for his help w"
2020.emnlp-main.456,D13-1032,0,0.0377147,"Missing"
2020.emnlp-main.456,D09-1142,0,0.0348282,"r Grammatical gender is a highly fixed classification system for nouns. Native speakers rarely make errors in gender recall, which might tentatively argue against tremendous arbitrary variation (Corbett, 1991). Some regularity can surely be found in the associations between gender and various features of the noun, such as orthographic or phonological form, or semantics. With respect to form-based regularities, Cucerzan and Yarowsky (2003a) devise a system for inferring noun gender (masculine or feminine) from contextual clues and character representations, even in inflected forms of the noun. Nastase and Popescu (2009) also find that phonological form can lead to predictability of gender in two three-gender systems. With respect to word semantics, (Williams et al., 2019) quantify the relationship between the gender on inanimate nouns and their distributional word vectors. We can’t rely on form. Using phonological or orthographic form to derive gender is fraught with complications: particular to our study, epicene nouns (i.e., words that can appear in multiple genders) can pose issues. In German, only gender concord on the definite article and adjectives can disambiguate the gender of some nouns; the same wo"
2020.emnlp-main.456,P17-1049,0,0.0515973,"ections surface between Romanian and both Slovene and Ukrainian, but the majority of the Balto-Slavic languages are quite distant from it. 6.2 Phylogeny Inspired by the findings in the previous section (especially the high similarity among Romance languages), we further validate our measure, asking whether the resulting similarities reflect known phylogenetic ground truth—namely, the developmental history of Indo-European languages. Obviously, there are many more facets to languages’ relatedness than their gender systems, so it is interesting to find signal this strong from a single category. Rabinovich et al. (2017) cluster languages based on simple features of their translations into a common 4 This claim can be debated (Bateman and Polinsky, 2010): The neuter gender manifests as masculine when singular and feminine when plural (Corbett, 1991). target language to craft phylogenetic trees. We take a similar approach, asking whether the pairwise similarities of gender systems are enough to reveal phylogenetic truth or some other relationship. We create phylogenetic trees through agglomerative hierarchical clustering, using both VI and one minus the AMI as distance measures. We use the weighted pair group"
2020.emnlp-main.456,N18-1181,0,0.0126966,"logenic relationships in our languages, with measurable success. Separate Indo-European branches are no more similar than chance. We emphasize that our methods are not specifically tailored to gender systems. One could apply them more broadly other aspects of the lexicon, e.g. to Indo-European verb classes, Bantu noun classes, or diachronic time slices of a single language’s gender system, data permitting. A related challenge is East and Southeast Asian numeral classifier systems, which associate nouns with classifiers based largely on the semantic properties of the nouns (Kuo and Sera, 2009; Zhan and Levy, 2018; Liu et al., 2019). They display more idiolectal variation, and often more than one classifier can accompany a given noun (Hu, 1993), unlike for gender (where this is rare). We note that we could further extend our measures to fuzzy partitions, which remain less explored in community detection, but are a promising avenue for future work. 5671 Acknowledgments We thank Tongfei Chen for comments on the Slavic languages, Jean-Gabriel Young for suggesting that we consider Variation of Information, and Johannes Bjerva for providing us with code to compute the tree distance. We also thank Tiago Pime"
2020.emnlp-main.456,D19-1577,1,0.741496,"Missing"
2020.emnlp-main.456,2020.acl-main.597,1,0.832162,"i.a.) and on phonology (Bidot, 1925; Tucker et al., 1977; Newman, 1979; Hayward and Corbett, 1988; Marchese, 1988). Intensional approaches, particularly those with typological leanings, contribute very fine grained research on particular pairwise similarities for particular languages and dialects. Although we cannot survey these in detail here, we would love for our measures to contribute findings that can complement these approaches. Relatedly, other recent works have investigated grammatical gender and other types of noun classification systems with information theoretic tools. For example, Williams et al. 2020b uses mutual information to quantify the strength of the relationships between declension class, grammatical gender, distributional semantics, and orthographic form respectively in several languages. Williams et al. 2020a, which is arguably closest to this work, measures the strength of semantic relationships between inanimate nouns and verbs or adjectives that takes those nouns as arguments, and that work can be seen as comparing the similarity of nouns clustered by their gender, with the same nouns clustered by the adjectives that modify them or the verbs that take them as arguments. Althou"
2020.lrec-1.352,P19-1310,0,0.0676769,"Missing"
2020.lrec-1.352,P15-2044,0,0.0825655,"Missing"
2020.lrec-1.352,D17-1011,0,0.0843599,"Missing"
2020.lrec-1.352,W08-0336,0,0.0231984,"Missing"
2020.lrec-1.352,D19-1632,0,0.0692809,"Missing"
2020.lrec-1.352,L18-1293,1,0.886792,"Missing"
2020.lrec-1.352,2005.mtsummit-papers.11,0,0.16262,"Missing"
2020.lrec-1.352,E17-1072,0,0.0251337,"eat map of the 66 Bible books’ presence by language. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungus"
2020.lrec-1.352,E17-2002,0,0.02835,"not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language can be classified with a particular typological feature if the description contains one of a handful of phrases used on Ethnologue to describe that feature, but does not contain any of the handful of phrases used"
2020.lrec-1.352,mayer-cysouw-2014-creating,0,0.0764406,"treat the verse alignment problem within a chapter as an instance of the longest common subsequence problem. Here, in a chapter with a pre-known number of verses m, the sequence of verse numbers A = [1, 2, . . . , m] is matched to the sequence B of n numbers extracted from the chapter text. The longest common subsequence is going to give the best explanation of the numbers B seen in text, “explaining” them either as a verse ID or a number seen in text. It can be solved efficiently using dynamic programming in O(m × n) time (Wagner and Fischer, 1974). 3.2. Verse alignment Our re-release of the Mayer and Cysouw (2014) Bibles and the web-scraped Bibles (Asgari and Sch¨utze, 2017) is already verse-aligned. The CMU Wilderness Bibles (Black, 2019) are verse-aligned using the dynamic programming approach explained in §3.1. Normalization We normalize all characters to their canonical (Unicode NFKC) form.3 Cross-references, footnote markers, and explanatory parentheticals are stripped. We replace archaisms in the King James Version of the English Bible (‘thou’ forms; ‘-est’ and ‘-eth’ verb forms) with their modern equivalents. Tokenization We preserve the tokenization of the Mayer and Cysouw (2014)–derived Bibles"
2020.lrec-1.352,2020.lrec-1.483,1,0.701077,"individual Bibles in a language that is a member of each family. The percent denotes the percent of the JHUBC Bibles that come from that language family. and clusivity. We leverage the parallelism of the Bible to annotate pronouns in the English Bibles for these features which are otherwise unmarked in English. We first collect a pronoun list for a small set of languages that mark pronouns for a number of different phenomena such as number, gender, plurality, and clusivity, and annotated these lists with UniMorph-style inflectional information (Sylak-Glassman et al., 2015; Kirov et al., 2018; McCarthy et al., 2020). Next, we word-align the English Bibles with the Bibles in those languages. Projecting from the source onto English, we obtain source–English pronoun hypotheses for each English pronoun. For each feature, we vote among the languages to arrive at a final annotation for English. To mitigate the ubiquity of certain feature values over others (i.e., the nominative case is much more prevalent in our languages than the essive case), we normalize each feature by the number of languages in which it is present. We can then use these annotations to identify pronouns in other languages via alignment. Bi"
2020.lrec-1.352,2020.lrec-1.458,1,0.792992,"here are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language can be classified with a particular typological feature if the d"
2020.lrec-1.352,P19-1172,1,0.803869,"sed in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic Chibchan Chipaya-Uru Mapudungu Tacanan Mixe-Zoquean Uralic Constructed language Witotoan Muskogean Panoan Huavean North Bougainville Nakh-Daghestanian Jivar"
2020.lrec-1.352,2020.lrec-1.488,1,0.718874,"a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic Chibchan Chipaya-Uru Mapudungu Tacanan Mixe-Zoquean Uralic Constructed language Witotoan Muskogean Panoan Huavean North Bougainville Nakh-Daghestanian Jivaroan Indo-European Arauan Guaykuruan Austro-Asiatic Misumalpan Karaj´a Tarascan South Bougainville Matacoan TTR 0.621 0.590 0.535 0.439 0.427 0.426 0.370 0.35"
2020.lrec-1.352,N18-2084,0,0.0326875,"Missing"
2020.lrec-1.352,E17-1021,0,0.0207306,"r 1 John 2 John 3 John Jude Revelation 0.0 Figure 1: Heat map of the 66 Bible books’ presence by language. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan"
2020.lrec-1.352,P15-2111,1,0.772899,"contacting the authors. 9. Table 3: The count of individual Bibles in a language that is a member of each family. The percent denotes the percent of the JHUBC Bibles that come from that language family. and clusivity. We leverage the parallelism of the Bible to annotate pronouns in the English Bibles for these features which are otherwise unmarked in English. We first collect a pronoun list for a small set of languages that mark pronouns for a number of different phenomena such as number, gender, plurality, and clusivity, and annotated these lists with UniMorph-style inflectional information (Sylak-Glassman et al., 2015; Kirov et al., 2018; McCarthy et al., 2020). Next, we word-align the English Bibles with the Bibles in those languages. Projecting from the source onto English, we obtain source–English pronoun hypotheses for each English pronoun. For each feature, we vote among the languages to arrive at a final annotation for English. To mitigate the ubiquity of certain feature values over others (i.e., the nominative case is much more prevalent in our languages than the essive case), we normalize each feature by the number of languages in which it is present. We can then use these annotations to identify p"
2020.lrec-1.352,tiedemann-2012-parallel,0,0.219573,"Missing"
2020.lrec-1.352,L18-1150,1,0.836665,"averaged over each language family, sorted by this ratio. When there are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologue entries have similar phrasing or terminology, a language"
2020.lrec-1.352,L18-1263,1,0.837513,"morphological richness. We report type–token ratios, averaged over each language family, sorted by this ratio. When there are several translations within a single language, we average these first. (Tai-Kadai would be expected to have a low type–token ratio. It does not because we did not segment the text in languages that do not indicate word boundaries with spaces.)) take this a step further, releasing fine-grained morphosyntactic analysis and generation tools for more than 1,000 languages. Additional practical resources can be derived from the Bible: translation matrices of named entities (Wu et al., 2018), lowresource transliteration tools (Wu and Yarowsky, 2018), and bitext for multilingual translation (Mueller et al., 2020). Feature Compilation In order to compile a typological description of this corpus, we utilize the URIEL typological database (Littell et al., 2017) and supplement this with a surface-level parser of Ethnologue (Eberhard et al., 2019) typology descriptions in order to leverage the most recent entries. This parser functions similarly to the original parser described by Littell et al. (2017)—using simple Boolean logic on contents of Ethnologue descriptions. As most Ethnologu"
2020.lrec-1.352,2020.lrec-1.519,1,0.719738,"Missing"
2020.lrec-1.352,I17-2076,1,0.841713,"New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Piawi Iroquoian Har´akmbut Pauwasi Quechuan Unclassified Maipurean East Geelvink Bay Yuat Senagi Tequistlatecan Pidgin Turkic"
2020.lrec-1.352,H01-1035,1,0.451858,"ge. (Twenty non-canon books which appear in only a handful of languages are omitted.) Nearly all languages have a complete New Testament, and several also have a complete Old Testament. 5. Bibles as a Low-Resource Asset employ the fact that multiple interpretations can be used in tandem. Exploiting the Bible, Agi´c et al. (2015) learn POS taggers for 100 languages and evaluate on 25 languages with test sets. Parallel Bibles also aid a variety of cross-lingual tasks, e.g., dependency parsing (Schlichtkrull and Søgaard, 2017), sentence embedding (Levy et al., 2017), verbal morphology induction (Yarowsky et al., 2001), and multilingual optical character recognition (Kanungo et al., 2005). None of these By contrast, Xia and Yarowsky (2017) leverage 27 English translations of the Bible. They use alignment and consensus to transfer dependency parses across languages. Nicolai and Yarowsky (2019) build on this, using projection with the same 27 English translations to develop morphosyntactic analyzers for low-resource languages. Nicolai et al. (2020) 2886 Family Tai-Kadai Arai (Left May) Khoe-Kwadi South-Central Papuan Kiowa-Tanoan Eskimo-Aleut Aymaran Tungusic Dravidian Eyak-Athabaskan Algic Mixed language Pia"
2020.lrec-1.397,W13-2507,0,0.0123937,"uses the model. Many of the misclassifying “other” mistakes included words that were inherited from Old English, like font and cress. Similar analysis can be performed for other languages, and future work includes collapsing languages of a single line (like Old, Middle, and Modern English) into a single label. 4. Translations Wiktionary also contains translations. Wiktionary provides an API to access translations, but this is not convenient for bulk analysis. Within the scientific literature, there are a couple projects that have extracted data directly from the Wiktionary dumps: WIKT 2 DICT (Acs et al., 2013) extracts translations from the translation tables in the Wiktionary articles. This codebase supports triangulation between language to discover new translations. Kirov et al. (2016) (henceforth K IROV) also extracts translations from translation tables, in addition to morphological paradigms, which were the main focus of their work. Our Wiktionary parser extracts translations from translation tables as well as from definitions of the word. Definitions are a valuable source of translations, and we are not aware of existing work that extracts translations from definitions. Extracting translatio"
2020.lrec-1.397,de-melo-2014-etymological,0,0.049232,"Missing"
2020.lrec-1.397,Y09-2026,0,0.0121272,"heir words. However, until very recently, the discovery of these relationships has not been computational driven. In recent years, researchers have developed computational methods for determining relationships between languages (see Nichols and Warnow (2008) and Dunn (2015) for surveys of the field of linguistic phylogenetics), but there is little work on computationally learning the etymological relationships between individual words. Researchers have constructed a Proto-Indo European lexicon (Pyysalo, 2017), and showed that knowing a word’s etymology can help with text classification tasks (Fang et al., 2009; Nastase and Strapparava, 2013) and reconstructing language phylogenies (Nouri and Yangarber, 2016). In an era of abundant linguistic data, we seek to address the dearth of computational approaches to modeling etymology. To this end, we develop a parser that extracts etymology information and translations from Wiktionary, an open multilingual dictionary. Using this data, we present several approaches to model when (word emergence), from where, and how a word enters a language. We employ RNN-based models to accurately predict a word’s formation mechanism, parent language, and year of emergence"
2020.lrec-1.397,2020.lrec-1.392,0,0.0232339,"ch computer, which is derived from the Latin computo. The -er suffix is inherited from the Middle English -er, which is inherited from the Old English (Anglo-Saxon) -ere. There are a few existing efforts to parse etymological information from Wiktionary at different granularities to construct such graphs: Etymological WordNet (de Melo, 2014) contains coarse-grained relations between pairs of words. The relations include is-derived-from, has-derived-form, etymologically-related, etymological-origin-of, etymology, and variant:orthography. This data covers 2.8 million terms. EtymDB (Sagot, 2017; Fourrier and Sagot, 2020) 1 wiktionary.org Wiktionary contains separate entries for affixes like -er, so we call them “words” here. 2 3252 Label Count affix back-form blend borrowed calque clipping cognate compound confix 28366 24 144 104817 964 44 32095 42524 2185 Label derived inherited mention noncognate prefix semantic loan short for suffix Count 132404 159239 265220 188 18169 15 3 49505 Table 1: Etymological relationships we extracted from Wiktionary. Note that cognate and noncognate relationships are bidirectional relations, while the rest are unidirectional. extracted more fine-grained relations including borro"
2020.lrec-1.397,L16-1498,1,0.8438,"nguages, and future work includes collapsing languages of a single line (like Old, Middle, and Modern English) into a single label. 4. Translations Wiktionary also contains translations. Wiktionary provides an API to access translations, but this is not convenient for bulk analysis. Within the scientific literature, there are a couple projects that have extracted data directly from the Wiktionary dumps: WIKT 2 DICT (Acs et al., 2013) extracts translations from the translation tables in the Wiktionary articles. This codebase supports triangulation between language to discover new translations. Kirov et al. (2016) (henceforth K IROV) also extracts translations from translation tables, in addition to morphological paradigms, which were the main focus of their work. Our Wiktionary parser extracts translations from translation tables as well as from definitions of the word. Definitions are a valuable source of translations, and we are not aware of existing work that extracts translations from definitions. Extracting translations from definitions is a challenging task, since definitions are unstructured and generally freeform text, while translation tables are structured. We utilized a combination of strin"
2020.lrec-1.397,P13-1064,0,0.01608,", until very recently, the discovery of these relationships has not been computational driven. In recent years, researchers have developed computational methods for determining relationships between languages (see Nichols and Warnow (2008) and Dunn (2015) for surveys of the field of linguistic phylogenetics), but there is little work on computationally learning the etymological relationships between individual words. Researchers have constructed a Proto-Indo European lexicon (Pyysalo, 2017), and showed that knowing a word’s etymology can help with text classification tasks (Fang et al., 2009; Nastase and Strapparava, 2013) and reconstructing language phylogenies (Nouri and Yangarber, 2016). In an era of abundant linguistic data, we seek to address the dearth of computational approaches to modeling etymology. To this end, we develop a parser that extracts etymology information and translations from Wiktionary, an open multilingual dictionary. Using this data, we present several approaches to model when (word emergence), from where, and how a word enters a language. We employ RNN-based models to accurately predict a word’s formation mechanism, parent language, and year of emergence. For emergence, we also experim"
2020.lrec-1.397,W16-1905,0,0.0160455,"computational driven. In recent years, researchers have developed computational methods for determining relationships between languages (see Nichols and Warnow (2008) and Dunn (2015) for surveys of the field of linguistic phylogenetics), but there is little work on computationally learning the etymological relationships between individual words. Researchers have constructed a Proto-Indo European lexicon (Pyysalo, 2017), and showed that knowing a word’s etymology can help with text classification tasks (Fang et al., 2009; Nastase and Strapparava, 2013) and reconstructing language phylogenies (Nouri and Yangarber, 2016). In an era of abundant linguistic data, we seek to address the dearth of computational approaches to modeling etymology. To this end, we develop a parser that extracts etymology information and translations from Wiktionary, an open multilingual dictionary. Using this data, we present several approaches to model when (word emergence), from where, and how a word enters a language. We employ RNN-based models to accurately predict a word’s formation mechanism, parent language, and year of emergence. For emergence, we also experiment with various historical datadriven models. Our methods are langu"
2020.lrec-1.397,W17-0234,0,0.0442158,"existing dictionaries as new evidence comes to light about the relationships between languages and their words. However, until very recently, the discovery of these relationships has not been computational driven. In recent years, researchers have developed computational methods for determining relationships between languages (see Nichols and Warnow (2008) and Dunn (2015) for surveys of the field of linguistic phylogenetics), but there is little work on computationally learning the etymological relationships between individual words. Researchers have constructed a Proto-Indo European lexicon (Pyysalo, 2017), and showed that knowing a word’s etymology can help with text classification tasks (Fang et al., 2009; Nastase and Strapparava, 2013) and reconstructing language phylogenies (Nouri and Yangarber, 2016). In an era of abundant linguistic data, we seek to address the dearth of computational approaches to modeling etymology. To this end, we develop a parser that extracts etymology information and translations from Wiktionary, an open multilingual dictionary. Using this data, we present several approaches to model when (word emergence), from where, and how a word enters a language. We employ RNN-"
2020.lrec-1.397,2020.scil-1.43,0,0.0258704,"the modeling of this to future work). Dictionaries of neologisms (e.g. Algeo and Algeo (1993)) list years or even specific dates of the first use of a word. In recent years, people have started investigating neologisms computationally (e.g. Ahmad (2000; Kerremans et al. (2011)), and online dictionaries and datasets provide convenient electronic versions of a word’s year of first use. However, these resources vary in the amount of information they provide and are often limited to a handful of languages. Most similar to this work is Petersen et al. (2012), who analyze word birth and death, and Ryskina et al. (2020), who examine factors that affect the creation of neologisms through the lens of word embeddings. In the remainder of this paper, we present our work on modeling word emergence, an integral part of a word’s etymology. We distinguish between, word birth, the year a word was first recorded as being used, and word emergence, the year in which the word starts gaining popularity in usage, and we argue that the latter is more informative than the former. We examine two datasets of historical word usage, the Google N-Grams corpus (Michel et al., 2011) and Merriam-Webster’s Dictionary (MerriamWebster,"
2020.lrec-1.458,N19-1388,0,0.327459,"ecurrent sequence-to-sequence neural networks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) in combination with attention (Bahdanau et al., 2015; Luong et al., 2015); even better performance has been achieved using self-attention (Vaswani et al., 2017). The neural approach has been effective because of the high fluency and adequacy of its output, as well as its language-agnostic methods. Indeed, given enough data, one need not know any linguistic features of one’s source and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in better models for neural machine translation, regardless of the languages (or combinations thereof) used. These results, however, have primarily been found for languages possessing large amounts of aligned data, and few such languages exist among the world’s 7,000+ languages (Lewis et al., 2015). While high neural machine translation performance has been observed in some bilingual lowresource2 contexts (Sennrich and Zhang, 2019), we have access to a multi-way aligned corpus of Bibles that allow us to perform a more in-depth analysis of th"
2020.lrec-1.458,1983.tc-1.13,0,0.318512,"Missing"
2020.lrec-1.458,P18-1073,0,0.0150898,"ora (Koehn and Knowles, 2017; Lample et al., 2018), NMT has achieved performance exceeding phrase-based MT in such settings by exploiting the same neural architectures as high-resource systems— primarily LSTMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017)—and by performing fine-grained hyperparameter tuning (Sennrich and Zhang, 2019). Another proposed strategy in for low-resource NMT is learning a latent variable NMT model with variational inference (McCarthy et al., 2019), though this has not been tried in a low-resource setting similar to ours. Unsupervised NMT (Artetxe et al., 2018) was suggested as a solution to the scarcity of parallel data for many languages, though this was found to be generally ineffective for lowresource and morphologically rich languages (Guzmán et al., 2019). Unsupervised phrase-based and neural models have also been proposed (Lample et al., 2018) and are reasonably effective when parallel data does not exist, though this is perhaps unrealistic given that the Bible exists in over 1,000 languages. Consequently, we aim to translate in a more supervised fashion. One surprisingly effective approach in high-resource settings has been to train translat"
2020.lrec-1.458,D17-1011,0,0.25729,"ed languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate this effect for our particular context in a more largescale manner; we perform multilingual NMT by concatenating many low-resource corpora derived from the Bible, up to over 1,000 languages. 3. Data Preparation The monolingual Bibles used in this study are from the Bible corpus of McCarthy et al. (2020), which contains over 4,000 translations (of varying lengths) in over 1,000 languages. This corpus is an aggregation of prior Bible corpora (Mayer and Cysouw, 2014; Asgari and Schütze, 2017; Black, 2019) and web-scraped Bible data, all postprocessed to be in the same format. Namely, these monolingual corpora are all verse-aligned,3 normalized to Unicode NFKC, modified such that archaic English forms are replaced with their contemporary equivalents (e.g., “thou” is changed to “you”, and “-est” and “-eth” verb inflections are replaced with their modern “-es” or “-s” forms), tokenized,4 and deduplicated. The Old Testament (OT) contains approximately 31,000 verses, and the New Testament (NT) contains approximately 8,000 verses. Some translations have the entire OT and NT, while othe"
2020.lrec-1.458,W09-0106,0,0.0125839,"using either bilingual models, 5-language models, or 10-language models. Moreover, the relatedness of the multiple languages certainly has an effect on performance, though whether this effect is positive or negative is language-dependent. This implies that the idea of simply adding more data to a training corpus regardless of language will not always result in the best performance—though it sometimes does when done carefully. This highlights the need for multiple evaluation languages when working in the multilingual setting, for methods that work in one language may not generalize to others (Bender, 2009; Bender, 2011). If one is pursuing the best model for a specific low-resource language pair, it is imperative to try a variety of source language sets when training multilingually and to train a good bilingual baseline. This variable cross-lingual performance could be due to the bottleneck effect mentioned in §2 or the destructive interference described in §6. Future work could treat the manylanguage corpora as high-resource datasets and use more typical high-resource hyperparameters instead of the lowresource hyperparameters used here. One could also investigate a larger variety of source la"
2020.lrec-1.458,N18-1032,0,0.0183431,"lel corpus, up 103 languages. These approaches have used exclusively high-resource or a mix of high-resource and lowresource languages. By comparison, we do not include auxiliary data to perform a controlled study in a limited setting; we just use the Bible, which is the same size and domain across all of our experiments. Of note, Arivazhagan et al. (2019) point to the difficulty of balancing data, which is less of an issue in our multi-parallel low-resource setting. Recent studies have investigated transfer learning in the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages wi"
2020.lrec-1.458,D19-1632,0,0.0280165,"STMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017)—and by performing fine-grained hyperparameter tuning (Sennrich and Zhang, 2019). Another proposed strategy in for low-resource NMT is learning a latent variable NMT model with variational inference (McCarthy et al., 2019), though this has not been tried in a low-resource setting similar to ours. Unsupervised NMT (Artetxe et al., 2018) was suggested as a solution to the scarcity of parallel data for many languages, though this was found to be generally ineffective for lowresource and morphologically rich languages (Guzmán et al., 2019). Unsupervised phrase-based and neural models have also been proposed (Lample et al., 2018) and are reasonably effective when parallel data does not exist, though this is perhaps unrealistic given that the Bible exists in over 1,000 languages. Consequently, we aim to translate in a more supervised fashion. One surprisingly effective approach in high-resource settings has been to train translation models on multiple language pairs at once (Johnson et al., 2017). This is done by concatenating the bitexts for multiple language pairs together into one large parallel corpus. Such approaches have en"
2020.lrec-1.458,Q17-1024,0,0.320519,"e pairs by employing recurrent sequence-to-sequence neural networks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) in combination with attention (Bahdanau et al., 2015; Luong et al., 2015); even better performance has been achieved using self-attention (Vaswani et al., 2017). The neural approach has been effective because of the high fluency and adequacy of its output, as well as its language-agnostic methods. Indeed, given enough data, one need not know any linguistic features of one’s source and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in better models for neural machine translation, regardless of the languages (or combinations thereof) used. These results, however, have primarily been found for languages possessing large amounts of aligned data, and few such languages exist among the world’s 7,000+ languages (Lewis et al., 2015). While high neural machine translation performance has been observed in some bilingual lowresource2 contexts (Sennrich and Zhang, 2019), we have access to a multi-way aligned corpus of Bibles that allow us to perform a more"
2020.lrec-1.458,D13-1176,0,0.300674,"ges to a training set is often better, but too many harms performance—the best number depends on the source language. Furthermore, training on related languages can improve or degrade performance, depending on the language. As there is no one-size-fits-most answer, we find that it is critical to tailor one’s approach to the source language and its typology. Keywords: neural machine translation, low-resource, multilinguality, Bible 1. Introduction Recently, machine translation (MT) has made significant progress in many language pairs by employing recurrent sequence-to-sequence neural networks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) in combination with attention (Bahdanau et al., 2015; Luong et al., 2015); even better performance has been achieved using self-attention (Vaswani et al., 2017). The neural approach has been effective because of the high fluency and adequacy of its output, as well as its language-agnostic methods. Indeed, given enough data, one need not know any linguistic features of one’s source and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in"
2020.lrec-1.458,W17-3204,0,0.0254985,"of-the-art phrase-based statistical approach (Koehn et al., 2003) in Sutskever et al. (2014) and Bahdanau et al. (2015). Its success was due to the combination of the sequence-to-sequence neural network, the use of the LSTM (Hochreiter and Schmidhuber, 1997), and the introduction and refinement of the attention mechanism (Luong et al., 2015). Although the main focus of investigation and improvement in NMT has been high-resource settings with millions of sentences, NMT has made great strides in low-resource settings as well. Initially considered ineffective without very large parallel corpora (Koehn and Knowles, 2017; Lample et al., 2018), NMT has achieved performance exceeding phrase-based MT in such settings by exploiting the same neural architectures as high-resource systems— primarily LSTMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017)—and by performing fine-grained hyperparameter tuning (Sennrich and Zhang, 2019). Another proposed strategy in for low-resource NMT is learning a latent variable NMT model with variational inference (McCarthy et al., 2019), though this has not been tried in a low-resource setting similar to ours. Unsupervised NMT (Artetxe et al., 2018) was su"
2020.lrec-1.458,N03-1017,0,0.0560702,"nding that the best approach depends to a large extent on the language pair. We hope that these results will help elucidate best practices for leveraging massively multilingual low-resource parallel datasets in future MT work. 2. Related Work Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013) has become the state-of-the-art approach to MT in recent years, employing new innovations in machine learning to achieve high performance in many language pairs. This approach was first shown to be effective with respect to the previously state-of-the-art phrase-based statistical approach (Koehn et al., 2003) in Sutskever et al. (2014) and Bahdanau et al. (2015). Its success was due to the combination of the sequence-to-sequence neural network, the use of the LSTM (Hochreiter and Schmidhuber, 1997), and the introduction and refinement of the attention mechanism (Luong et al., 2015). Although the main focus of investigation and improvement in NMT has been high-resource settings with millions of sentences, NMT has made great strides in low-resource settings as well. Initially considered ineffective without very large parallel corpora (Koehn and Knowles, 2017; Lample et al., 2018), NMT has achieved p"
2020.lrec-1.458,D18-1549,0,0.0191999,"tatistical approach (Koehn et al., 2003) in Sutskever et al. (2014) and Bahdanau et al. (2015). Its success was due to the combination of the sequence-to-sequence neural network, the use of the LSTM (Hochreiter and Schmidhuber, 1997), and the introduction and refinement of the attention mechanism (Luong et al., 2015). Although the main focus of investigation and improvement in NMT has been high-resource settings with millions of sentences, NMT has made great strides in low-resource settings as well. Initially considered ineffective without very large parallel corpora (Koehn and Knowles, 2017; Lample et al., 2018), NMT has achieved performance exceeding phrase-based MT in such settings by exploiting the same neural architectures as high-resource systems— primarily LSTMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017)—and by performing fine-grained hyperparameter tuning (Sennrich and Zhang, 2019). Another proposed strategy in for low-resource NMT is learning a latent variable NMT model with variational inference (McCarthy et al., 2019), though this has not been tried in a low-resource setting similar to ours. Unsupervised NMT (Artetxe et al., 2018) was suggested as a solution"
2020.lrec-1.458,D15-1166,0,0.37457,"nguage. Furthermore, training on related languages can improve or degrade performance, depending on the language. As there is no one-size-fits-most answer, we find that it is critical to tailor one’s approach to the source language and its typology. Keywords: neural machine translation, low-resource, multilinguality, Bible 1. Introduction Recently, machine translation (MT) has made significant progress in many language pairs by employing recurrent sequence-to-sequence neural networks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) in combination with attention (Bahdanau et al., 2015; Luong et al., 2015); even better performance has been achieved using self-attention (Vaswani et al., 2017). The neural approach has been effective because of the high fluency and adequacy of its output, as well as its language-agnostic methods. Indeed, given enough data, one need not know any linguistic features of one’s source and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in better models for neural machine translation, regardless of the languages (or combinations thereof)"
2020.lrec-1.458,mayer-cysouw-2014-creating,0,0.653267,"using more closely related languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate this effect for our particular context in a more largescale manner; we perform multilingual NMT by concatenating many low-resource corpora derived from the Bible, up to over 1,000 languages. 3. Data Preparation The monolingual Bibles used in this study are from the Bible corpus of McCarthy et al. (2020), which contains over 4,000 translations (of varying lengths) in over 1,000 languages. This corpus is an aggregation of prior Bible corpora (Mayer and Cysouw, 2014; Asgari and Schütze, 2017; Black, 2019) and web-scraped Bible data, all postprocessed to be in the same format. Namely, these monolingual corpora are all verse-aligned,3 normalized to Unicode NFKC, modified such that archaic English forms are replaced with their contemporary equivalents (e.g., “thou” is changed to “you”, and “-est” and “-eth” verb inflections are replaced with their modern “-es” or “-s” forms), tokenized,4 and deduplicated. The Old Testament (OT) contains approximately 31,000 verses, and the New Testament (NT) contains approximately 8,000 verses. Some translations have the en"
2020.lrec-1.458,2020.lrec-1.352,1,0.91034,"ine translation, regardless of the languages (or combinations thereof) used. These results, however, have primarily been found for languages possessing large amounts of aligned data, and few such languages exist among the world’s 7,000+ languages (Lewis et al., 2015). While high neural machine translation performance has been observed in some bilingual lowresource2 contexts (Sennrich and Zhang, 2019), we have access to a multi-way aligned corpus of Bibles that allow us to perform a more in-depth analysis of the performance of multilingual low-resource NMT. This parallel corpus of Bible texts (McCarthy et al., 2020) contains a set of “monoApproximately multi-parallel corpus Similar or random subset of languages Align Target × Source language text NMT Model Figure 1: Our process for constructing training sets from a multi-parallel corpus. We select some n − 1 languages that share either phylogeny or script or randomly sample n − 1 languages to augment the source–target pair. Using a multi-parallel corpus controls for variations in data quality or domain; this lets us more cleanly assess our scientific questions, but it is not an engineering requirement for MT systems that use several helper languages. Mor"
2020.lrec-1.458,I17-2050,0,0.0186778,"e and lowresource languages. By comparison, we do not include auxiliary data to perform a controlled study in a limited setting; we just use the Bible, which is the same size and domain across all of our experiments. Of note, Arivazhagan et al. (2019) point to the difficulty of balancing data, which is less of an issue in our multi-parallel low-resource setting. Recent studies have investigated transfer learning in the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate"
2020.lrec-1.458,N19-4009,0,0.0159382,"o do so, we compare the performance of neural models trained on our various multilingual corpora when translating from our evaluation languages to English. This is essentially the same as the many-to-one approach of Johnson et al. (2017). All corpora have their tokens split into subwords using BPE (Sennrich et al., 2016). This is run jointly on all languages in the dataset, so each multilingual corpus will have different subword splits. We use 32,000 merge operations for all corpora despite their varying sizes; this results in more aggressive word splitting for larger corpora. We use fairseq (Ott et al., 2019) to run both training and inference. Separate Transformer-based models are trained for each of our multilingual corpora using the low-resource 5. Results Table 3 contains BLEU scores for the translation task for all models trained on varying numbers of source languages. All scores are for translations from the given evaluation language into English. We score on detokenized output translations using SacreBLEU (Post, 2018). We first note that the BLEU increase/decrease with respect to the number of training languages is not uniform across evaluation languages. Indeed, for the 5-language model, w"
2020.lrec-1.458,W18-6319,0,0.0125836,"rd splits. We use 32,000 merge operations for all corpora despite their varying sizes; this results in more aggressive word splitting for larger corpora. We use fairseq (Ott et al., 2019) to run both training and inference. Separate Transformer-based models are trained for each of our multilingual corpora using the low-resource 5. Results Table 3 contains BLEU scores for the translation task for all models trained on varying numbers of source languages. All scores are for translations from the given evaluation language into English. We score on detokenized output translations using SacreBLEU (Post, 2018). We first note that the BLEU increase/decrease with respect to the number of training languages is not uniform across evaluation languages. Indeed, for the 5-language model, we see notable gains over bilingual models for Arabic, Turkish, 5 1-layer encoder, 1-layer decoder, hidden size 1024, embedding size 512, tied decoder embeddings, hidden and embedding dropout 0.5, source and target word dropout 0.3, label smoothing 0.2, batch size 1000, initial learning rate 0.0005, Adam optimizer. 6 More data in general may help for a variety of reasons that are not well understood, especially in low-res"
2020.lrec-1.458,W18-6327,0,0.0162549,"gated transfer learning in the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate this effect for our particular context in a more largescale manner; we perform multilingual NMT by concatenating many low-resource corpora derived from the Bible, up to over 1,000 languages. 3. Data Preparation The monolingual Bibles used in this study are from the Bible corpus of McCarthy et al. (2020), which contains over 4,000 translations (of varying lengths) in over 1,000 languages."
2020.lrec-1.458,P19-1021,0,0.29208,"ce and target languages to effectively translate.1 With the success of multilingual training (Johnson et al., 2017; Aharoni et al., 2019), the trend seems clear: more data generally results in better models for neural machine translation, regardless of the languages (or combinations thereof) used. These results, however, have primarily been found for languages possessing large amounts of aligned data, and few such languages exist among the world’s 7,000+ languages (Lewis et al., 2015). While high neural machine translation performance has been observed in some bilingual lowresource2 contexts (Sennrich and Zhang, 2019), we have access to a multi-way aligned corpus of Bibles that allow us to perform a more in-depth analysis of the performance of multilingual low-resource NMT. This parallel corpus of Bible texts (McCarthy et al., 2020) contains a set of “monoApproximately multi-parallel corpus Similar or random subset of languages Align Target × Source language text NMT Model Figure 1: Our process for constructing training sets from a multi-parallel corpus. We select some n − 1 languages that share either phylogeny or script or randomly sample n − 1 languages to augment the source–target pair. Using a multi-p"
2020.lrec-1.458,P16-1162,0,0.0216115,"relationship between the number of languages in a multilingual dataset and the performance of NMT into English in a low-resource setting. We observe whether the performance increase is monotonic with respect to the number of languages, as well as at what point we begin to see diminishing returns. To do so, we compare the performance of neural models trained on our various multilingual corpora when translating from our evaluation languages to English. This is essentially the same as the many-to-one approach of Johnson et al. (2017). All corpora have their tokens split into subwords using BPE (Sennrich et al., 2016). This is run jointly on all languages in the dataset, so each multilingual corpus will have different subword splits. We use 32,000 merge operations for all corpora despite their varying sizes; this results in more aggressive word splitting for larger corpora. We use fairseq (Ott et al., 2019) to run both training and inference. Separate Transformer-based models are trained for each of our multilingual corpora using the low-resource 5. Results Table 3 contains BLEU scores for the translation task for all models trained on varying numbers of source languages. All scores are for translations fr"
2020.lrec-1.458,D18-1326,0,0.0184802,"n the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages will reduce the bottleneck effect and increase BLEU compared to using more unrelated languages. We aim to investigate this effect for our particular context in a more largescale manner; we perform multilingual NMT by concatenating many low-resource corpora derived from the Bible, up to over 1,000 languages. 3. Data Preparation The monolingual Bibles used in this study are from the Bible corpus of McCarthy et al. (2020), which contains over 4,000 translations (of varying lengths) in over 1,000 languages. This corpus is an ag"
2020.lrec-1.458,D19-3043,0,0.057393,"the 5-language models for German and Tagalog therein). The similar-language Xhosa model obtains more modest gains over the unrelated-language model, especially considering the extent of the BLEU gains on German and Tagalog. Once again, it seems that the effectiveness of any particular multilingual approach is highly language-dependent. 6. Qualitative Analysis Metrics do not fully encapsulate translation performance. They may not capture critical phenomena, and may not 7 We go into further detail on these possibilities and suggest improvements for future work in §7. align with human judgments (Wang et al., 2019). Further, they do not give a clear understanding of patterns of errors. Thus, this section focuses on the analysis of system outputs.8 We provide examples of translations from the multilingual experiments and the similar-language and similarscript experiments in Table 5. First, we note that fluency is strongly preferred over adequacy, especially for the models that do not degenerate. That is, taking the perspective of the decoder as a conditional language model, in some settings we have developed strong English language models which largely ignore their source text. This effect is clearer for"
2020.lrec-1.458,D16-1163,0,0.0183329,"3 languages. These approaches have used exclusively high-resource or a mix of high-resource and lowresource languages. By comparison, we do not include auxiliary data to perform a controlled study in a limited setting; we just use the Bible, which is the same size and domain across all of our experiments. Of note, Arivazhagan et al. (2019) point to the difficulty of balancing data, which is less of an issue in our multi-parallel low-resource setting. Recent studies have investigated transfer learning in the low-resource multilingual setting using multiple unrelated languages (Gu et al., 2018; Zoph et al., 2016), and some have done the similar work using multiple related languages (Nguyen and Chiang, 2017). These approaches employed a small number of helper languages during training, but no more than 5 to 10 at once. Prior work suggests that this could either result in transfer learning across languages, leading to increased performance (as in the abovecited works), or perhaps it could result in a bottleneck where there are too few parameters for too many languages (Sachan and Neubig, 2018; Wang et al., 2018). These latter works suggest that using more closely related languages will reduce the bottle"
2020.lrec-1.483,P19-1310,0,0.0958144,"Missing"
2020.lrec-1.483,C12-2009,1,0.843263,"Missing"
2020.lrec-1.483,P19-1156,0,0.050149,"Missing"
2020.lrec-1.483,P16-1156,1,0.881371,"Missing"
2020.lrec-1.483,K17-2001,1,0.904106,"Missing"
2020.lrec-1.483,N07-1048,0,0.0363457,"Missing"
2020.lrec-1.483,P08-1115,0,0.0323664,"Missing"
2020.lrec-1.483,K19-1014,1,0.901227,"Missing"
2020.lrec-1.483,N12-1032,0,0.0791982,"Missing"
2020.lrec-1.483,D19-1328,0,0.0253837,"Missing"
2020.lrec-1.483,S19-1026,1,0.819906,"Missing"
2020.lrec-1.483,L16-1498,1,0.928068,"Missing"
2020.lrec-1.483,L18-1293,1,0.890225,"Missing"
2020.lrec-1.483,W18-6011,1,0.883111,"Missing"
2020.lrec-1.483,W19-4226,1,0.885665,"Missing"
2020.lrec-1.483,D14-1095,0,0.0393603,"Missing"
2020.lrec-1.483,2020.lrec-1.488,1,0.822461,"Missing"
2020.lrec-1.483,L16-1262,0,0.126329,"Missing"
2020.lrec-1.483,Q15-1026,0,0.0701072,"Missing"
2020.lrec-1.483,W18-1813,1,0.887977,"Missing"
2020.lrec-1.483,P15-2111,1,0.855194,"Missing"
2020.lrec-1.483,A94-1008,0,0.317255,"Missing"
2020.lrec-1.483,N01-1026,1,0.569711,"Missing"
2020.lrec-1.488,P15-2044,0,0.154424,"Missing"
2020.lrec-1.488,P16-1184,0,0.0447663,"Missing"
2020.lrec-1.488,K17-2001,1,0.86632,"Missing"
2020.lrec-1.488,K18-3001,1,0.931236,"n this paper, we describe a resource intended to aid with the inflectional sparsity problem: a set of inflectional analyzers and generators for more than 1000 languages. Often, inflectional tools are created on an “as-needed” basis — a researcher will create and distribute a tool for a particular language of interest, or a small number of languages are incrementally added to an existing tool — the Porter stemmer (Porter, 1980), which evolved into the Snowball stemming suite3 is one such example. With the exception of the SIGMORPHON shared tasks (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018a; McCarthy et al., 2019), we are unaware of any efforts to produce a large number of inflectional tools across a large number of languages. Leveraging a large, multi-way parallel corpus of Bible texts (McCarthy et al., 2020), we exploit languages with highaccuracy annotation tools to hypothesize and project inflectional morphology across an induced alignment (Figure 1. We hope that providing these tools to the community will www.ethnologue.com internetworldstats.com/stats7.htm 3 3963 https://snowballstem.org/ encourage and facilitate research in a much wider range of the world’s languages. Th"
2020.lrec-1.488,I05-1075,0,0.0876687,"Missing"
2020.lrec-1.488,P17-1044,0,0.0182987,"rators and analyzers are trained on inflectional paradigms extracted from the Bible corpus of McCarthy et al. (under review), which contains 4032 parallel translations in 1108 languages. Among these are 27 English translations, whose archaic forms have been replaced with their modern equivalents (i.e., “believeth” is replaced with “believes”). The English and target Bibles have been aligned using the Berkeley aligner (Liang et al., 3966 2006), POS-tagged and syntactically-parsed using the Stanford NLP toolkit (Manning et al., 2014), and semanticallyparsed using the Deep Semantic Role Labeler (He et al., 2017). If a target Bible has multiple translations, they are all used to extract inflectional paradigms. The Bible corpus contains languages representing a wide spectrum of morphological phenomena, including languages that do not inflect at all. Many Polynesian languages, such as Indonesian, Tangoa, and Balinese, for example, have very little inflectional morphology on nouns, verbs, and adjectives. Likewise, the Sino-Tibetan language family, containing languages like Mandarin Chinese, observes very little inflectional morphology (cf. Tibetan). The URIEL typological database (Littel et al., 2016) ca"
2020.lrec-1.488,N10-1103,0,0.0610536,"Missing"
2020.lrec-1.488,E17-2018,0,0.0137588,"ual dictionary. Fossum and Abney (2005) and Agi´c et al. (2015) exploit the parallel nature of the Bible to project POS tags to lower-resources languages, using this projection to then train POS-taggers. Buys and Botha (2016) extend this tagging paradigm to morphological tagging, projecting morphological tags onto a low-resource language, and training a tagger on these morphologically-aware tags. In the op3964 Figure 3: Projecting induced morphological categories and lemmas across an alignment from English to Finnish. Red arrows indicate a right-to-left syntactic dependency. posite direction, Kirov et al. (2017) uses a morphologically rich language to train a morphologically-aware tagger in English. Instead of tags, Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner, recovering the lemma from inflected forms. We expand upon the work of Nicolai and Yarowsky (2019), who first richly annotate the English side of a bitext before projecting morphological information onto the lowresource text. The paucity of English morphological tagging is augmented through the heuristic interpretation of syntactic and semantic parses, as well as a reverse projection of a number of"
2020.lrec-1.488,L18-1293,1,0.899678,"Missing"
2020.lrec-1.488,D18-2012,0,0.0611284,"Missing"
2020.lrec-1.488,N06-1014,0,0.280767,"Missing"
2020.lrec-1.488,C18-1008,0,0.0196104,"7.1. before describing our novel type-to-token conversion, which allows an approximation of token-level accuracy when annotated corpora are unavailable. 7.1. Type Accuracy We first evaluate our systems on type accuracy: given a morphological dictionary, we report the percentage of instances that are correctly analyzed. We present the type accuracy for generation, lemmatization, and analysis in Table 5. We report the average over all 50 evaluation languages. We observe that ensembling and reranking is much more successful going from inflections to lemmas than the reverse. The neural system of Makarov and Clematide (2018) was specifically designed for inflection generation, and is very successful at producing inflected forms, even with noisy training data - the non-neural system has little to add in an ensemble. Likewise, the inflectional sparsity problem means that most inflectional forms will not be observed in the corpus used for reranking, and thus few forms are promoted. On the lemmatization side, the ensemble clearly improves over either individual system. Lemmatization is an exerSystem DTL@1 M&C@1 Ensemble@1 Ensemble+RR@1 DTL@5 M&C@5 Ensemble@5 Ensemble+RR@5 DTL@50 M&C@50 Ensemble@50 Ensemble+RR@50 Gene"
2020.lrec-1.488,P14-5010,0,0.00247993,"op of the list while preserving the order of the hypotheses. 5. Data All of our generators and analyzers are trained on inflectional paradigms extracted from the Bible corpus of McCarthy et al. (under review), which contains 4032 parallel translations in 1108 languages. Among these are 27 English translations, whose archaic forms have been replaced with their modern equivalents (i.e., “believeth” is replaced with “believes”). The English and target Bibles have been aligned using the Berkeley aligner (Liang et al., 3966 2006), POS-tagged and syntactically-parsed using the Stanford NLP toolkit (Manning et al., 2014), and semanticallyparsed using the Deep Semantic Role Labeler (He et al., 2017). If a target Bible has multiple translations, they are all used to extract inflectional paradigms. The Bible corpus contains languages representing a wide spectrum of morphological phenomena, including languages that do not inflect at all. Many Polynesian languages, such as Indonesian, Tangoa, and Balinese, for example, have very little inflectional morphology on nouns, verbs, and adjectives. Likewise, the Sino-Tibetan language family, containing languages like Mandarin Chinese, observes very little inflectional mo"
2020.lrec-1.488,W19-4226,1,0.863383,"a resource intended to aid with the inflectional sparsity problem: a set of inflectional analyzers and generators for more than 1000 languages. Often, inflectional tools are created on an “as-needed” basis — a researcher will create and distribute a tool for a particular language of interest, or a small number of languages are incrementally added to an existing tool — the Porter stemmer (Porter, 1980), which evolved into the Snowball stemming suite3 is one such example. With the exception of the SIGMORPHON shared tasks (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018a; McCarthy et al., 2019), we are unaware of any efforts to produce a large number of inflectional tools across a large number of languages. Leveraging a large, multi-way parallel corpus of Bible texts (McCarthy et al., 2020), we exploit languages with highaccuracy annotation tools to hypothesize and project inflectional morphology across an induced alignment (Figure 1. We hope that providing these tools to the community will www.ethnologue.com internetworldstats.com/stats7.htm 3 3963 https://snowballstem.org/ encourage and facilitate research in a much wider range of the world’s languages. This paper progresses as fo"
2020.lrec-1.488,2020.lrec-1.352,1,0.850705,"basis — a researcher will create and distribute a tool for a particular language of interest, or a small number of languages are incrementally added to an existing tool — the Porter stemmer (Porter, 1980), which evolved into the Snowball stemming suite3 is one such example. With the exception of the SIGMORPHON shared tasks (Cotterell et al., 2016; Cotterell et al., 2017; Cotterell et al., 2018a; McCarthy et al., 2019), we are unaware of any efforts to produce a large number of inflectional tools across a large number of languages. Leveraging a large, multi-way parallel corpus of Bible texts (McCarthy et al., 2020), we exploit languages with highaccuracy annotation tools to hypothesize and project inflectional morphology across an induced alignment (Figure 1. We hope that providing these tools to the community will www.ethnologue.com internetworldstats.com/stats7.htm 3 3963 https://snowballstem.org/ encourage and facilitate research in a much wider range of the world’s languages. This paper progresses as follows: Section 2. gives a brief overview of inflectional morphology, and describes the key operations covered by our inflectional tools. Section 3. establishes the current state of affairs in computat"
2020.lrec-1.488,P19-1172,1,0.689801,"ing morphological tags onto a low-resource language, and training a tagger on these morphologically-aware tags. In the op3964 Figure 3: Projecting induced morphological categories and lemmas across an alignment from English to Finnish. Red arrows indicate a right-to-left syntactic dependency. posite direction, Kirov et al. (2017) uses a morphologically rich language to train a morphologically-aware tagger in English. Instead of tags, Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner, recovering the lemma from inflected forms. We expand upon the work of Nicolai and Yarowsky (2019), who first richly annotate the English side of a bitext before projecting morphological information onto the lowresource text. The paucity of English morphological tagging is augmented through the heuristic interpretation of syntactic and semantic parses, as well as a reverse projection of a number of other high-resource languages. Our main contribution over their work is the expansion of the language set by a factor of 40 (from 26 languages to more than 1000). We also augment the set of inflectional features covered by their methods and incorporate frequency statistics into their learning mo"
2020.lrec-1.488,N15-1186,0,0.0223024,"o lower-resources languages, using this projection to then train POS-taggers. Buys and Botha (2016) extend this tagging paradigm to morphological tagging, projecting morphological tags onto a low-resource language, and training a tagger on these morphologically-aware tags. In the op3964 Figure 3: Projecting induced morphological categories and lemmas across an alignment from English to Finnish. Red arrows indicate a right-to-left syntactic dependency. posite direction, Kirov et al. (2017) uses a morphologically rich language to train a morphologically-aware tagger in English. Instead of tags, Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner, recovering the lemma from inflected forms. We expand upon the work of Nicolai and Yarowsky (2019), who first richly annotate the English side of a bitext before projecting morphological information onto the lowresource text. The paucity of English morphological tagging is augmented through the heuristic interpretation of syntactic and semantic parses, as well as a reverse projection of a number of other high-resource languages. Our main contribution over their work is the expansion of the language set by a factor of 40 (from"
2020.lrec-1.488,P15-2111,1,0.811235,"t of a single column of acceptable word forms. 7. Evaluation We construct both inflection generators and morphological analyzers for more than 1000 languages. In this section, we evaluate the quality of the tools that we present to the community. It is not possible to collect evaluation sets for each and every language represented in this dataset — the evaluation of morphological tools requires annotated inflectional dictionaries, which do not exist for a majority of languages. We instead evaluate on a subset of the languages for which we do have inflecional dictionaries. We turn to UniMorph (Sylak-Glassman et al., 2015; Kirov et al., 2018), a collection of morphological dictionaries that spans more than 100 languages. Of these languages, 50 overlap with our languages, and can be used as a test set. For each language, we extract a validation set of 500 randomly-sampled tuples of the form {LEMMA, INFLECTED, MORPH}, and a test set of 1000 instances. For example, an English instance of the word “played” would appear as {play, played, PST}. The validation set is used to tune hyper-parameters, and for early stopping of the neural models. The languages are listed in Table 4. From these tuples, we construct test se"
2020.lrec-1.488,H01-1035,1,0.579966,"Missing"
2020.lrec-1.519,C10-3010,0,0.0316292,"one 4. dog 7. eye 10. blood 13. bone 16. tooth 19. die 22. hear 25. mouth 28. eat 31. smoke 34. black 37. man 40. three 43. liver 46. hide 49. drink 52. good 55. fat 58. cloud 61. neck 64. cold 67. earth 70. go 73. that 76. mother 79. sit 82. five 85. what 88. root 91. grind 94. who 97. house 100. back 103. little 106. know 109. short 112. female 115. old 118. sky 121. ash 124. six 127. stick 130. dull 133. eight 136. he 139. the 142. near 145. this 148. where Construction For the construction of our core vocabulary, we utilize LanguageNet1 , a multilingual lexicon that is a subset of PanLex (Baldwin et al., 2010), a freely available multilingual dictionary. PanLex contains lexical translations across thousands of the world’s languages and has recently garnered interest in the multilingual research community. Its lexical translations are sourced from existing dictionaries and thesauri such as Wiktionary and WordNet. LanguageNet, as of September 2019, contains 1895 languages. We employ a simple procedure: using English as a pivot, we collect counts of how many languages have a translation for each English concept. (This dictionary pivoting strategy has previously been applied to model color terminology"
2020.lrec-1.519,L16-1379,0,0.0282135,"you moon Dictionaries are available for most of the world’s languages, but coverage can be sparse for those with fewer resources. In sparse dictionaries, many entries are core vocabulary words from lists such as the Swadesh list (Swadesh, 1952; Swadesh, 1955), probably the most wellknown formulation of a core vocabulary containing around 100–200 words, depending on the version. This list of basic words is used in historical comparative linguistics to determine the relationships between languages, and there have been many attempts to revise or expand these concept lists for this purpose. (See List et al. (2016) for a recent survey and compilation of such lists.) Morris Swadesh chose the words in the Swadesh lists based on certain criteria: the words should be culturally universal, stable over time (not likely to change meaning), and not likely to be borrowed. Swadesh lists now exist in over 1000 languages and can be used as a dictionary to perform lexical translations. However, in a low-resource setting, the ability to translate a mere 100 concepts is insufficient for understanding in a language. In addition, the Swadesh list, like many other lists, was manually created and revised through years of"
2020.lrec-1.519,macleod-etal-2000-american,0,0.0433208,"Missing"
2020.lrec-1.519,D19-1229,1,0.828749,"a freely available multilingual dictionary. PanLex contains lexical translations across thousands of the world’s languages and has recently garnered interest in the multilingual research community. Its lexical translations are sourced from existing dictionaries and thesauri such as Wiktionary and WordNet. LanguageNet, as of September 2019, contains 1895 languages. We employ a simple procedure: using English as a pivot, we collect counts of how many languages have a translation for each English concept. (This dictionary pivoting strategy has previously been applied to model color terminology (McCarthy et al., 2019).) The concepts are then sorted in decreasing order by this count, resulting in our core vocabulary list. Up until recently, such a computational procedure would have been impossible without the computing resources and datasets available today. Figure 1 shows the top 30 concepts along with the number of dictionaries that contain them.2 The fact that so many languages’ dictionaries contain these words is a strong indicator of the coreness of these words. This point is even more salient for dictionaries of low-resource languages: that so many lexicographers have included these words in their lan"
2020.lrec-1.519,2020.lrec-1.488,1,0.591366,"g being cognates in related languages and often not being compositional. In addition, the core words span multiple domains and cover high frequency concepts which ought to be translatable in any language. We employed a cognate prediction model to translate the core vocabulary words with promising results. Based on the consensus of thousands of lexicographers across the world’s languages, in constructing dictionaries for low-resource languages, translations of these core words can be elicited by field linguists or computationally via a cognate methods or inflectional generation methods such as Nicolai et al. (2020). Code and data used in this paper, including the full list of core vocabulary words, is available at https://github.com/wswu/corevoc. 8. pounding model of Wu and Yarowsky (2018c). They analyze words by splitting the word into two component parts. By accumulating counts of the these components across all languages, they derive “recipes” for a concept, e.g. the concept of hospital is often realized as a compound of sick and house in many languages, even those unrelated to each other. We use this compounding model to analyze translations of our core vocabulary across languages. We find 278 conce"
2020.lrec-1.519,L18-1150,1,0.881392,".17 0.19 0.57 0.51 0.73 0.45 0.61 0.23 0.67 0.51 0.27 0.28 0.59 0.63 0.94 0.56 0.75 Figure 4: Coverage of lists over various corpora. The number of types and tokens for each corpus is in Table 4. Comparisons are only valid between same size lists, i.e. between columns 1 and 2, 3 and 4, and 5 and 6. Corpus Types Tokens Bible UDHR BNC ANC GNG 8,674 197 5,464 10,000 10,000 790K 1,773 62M 20M 341B Table 4: Corpus sizes phylogenetic relationships between languages. Thus if two languages are related, their respective Swadesh words are likely to be cognates. In this section, we expand on the work of Wu and Yarowsky (2018b), who devised a cognate translation method for the bilingual lexicon induction task. They discovered cognates from a multilingual dictionary in an unsupervised manner by using English as a pivot and then clustered these translations into cognate groups based on edit distance. Taking the Cartesian product of words in each cluster as word pairs, they run an aligner to extract character insertion, deletion, and substitution probabilities to be used as costs in a weighted edit distance in a second clustering iteration. The results of the second clustering were used to train characterbased machin"
2020.lrec-1.519,L18-1538,1,0.77089,".17 0.19 0.57 0.51 0.73 0.45 0.61 0.23 0.67 0.51 0.27 0.28 0.59 0.63 0.94 0.56 0.75 Figure 4: Coverage of lists over various corpora. The number of types and tokens for each corpus is in Table 4. Comparisons are only valid between same size lists, i.e. between columns 1 and 2, 3 and 4, and 5 and 6. Corpus Types Tokens Bible UDHR BNC ANC GNG 8,674 197 5,464 10,000 10,000 790K 1,773 62M 20M 341B Table 4: Corpus sizes phylogenetic relationships between languages. Thus if two languages are related, their respective Swadesh words are likely to be cognates. In this section, we expand on the work of Wu and Yarowsky (2018b), who devised a cognate translation method for the bilingual lexicon induction task. They discovered cognates from a multilingual dictionary in an unsupervised manner by using English as a pivot and then clustered these translations into cognate groups based on edit distance. Taking the Cartesian product of words in each cluster as word pairs, they run an aligner to extract character insertion, deletion, and substitution probabilities to be used as costs in a weighted edit distance in a second clustering iteration. The results of the second clustering were used to train characterbased machin"
2020.lrec-1.519,L18-1612,1,0.925934,".17 0.19 0.57 0.51 0.73 0.45 0.61 0.23 0.67 0.51 0.27 0.28 0.59 0.63 0.94 0.56 0.75 Figure 4: Coverage of lists over various corpora. The number of types and tokens for each corpus is in Table 4. Comparisons are only valid between same size lists, i.e. between columns 1 and 2, 3 and 4, and 5 and 6. Corpus Types Tokens Bible UDHR BNC ANC GNG 8,674 197 5,464 10,000 10,000 790K 1,773 62M 20M 341B Table 4: Corpus sizes phylogenetic relationships between languages. Thus if two languages are related, their respective Swadesh words are likely to be cognates. In this section, we expand on the work of Wu and Yarowsky (2018b), who devised a cognate translation method for the bilingual lexicon induction task. They discovered cognates from a multilingual dictionary in an unsupervised manner by using English as a pivot and then clustered these translations into cognate groups based on edit distance. Taking the Cartesian product of words in each cluster as word pairs, they run an aligner to extract character insertion, deletion, and substitution probabilities to be used as costs in a weighted edit distance in a second clustering iteration. The results of the second clustering were used to train characterbased machin"
2020.sigmorphon-1.25,N10-1103,0,0.0281455,"l et al., 2016, 2017, 2018; McCarthy et al., 2019), and typically consists of a modified sequence-to-sequence model with attention (Makarov and Clematide, 2018). However, these systems are fully supervised, and hand-curated morphological dictionaries often do not exist. We instead turn to the methods of Nicolai and Yarowsky (2019), who use English annotation as distant supervision to induce target language morphology, using a widelytranslated, verse-parallel text: the Bible. Starting from the inflection pairs extracted by their method, we ensemble generators trained using an RNN and DirecTL+ (Jiampojamarn et al., 2010). For each lemma in the respective UniMorph, we generate hypotheses for each feature bundle, ensembling via a linear combination of confidence scores. This gives us a set of inflections for each of the lexemes in the evaluation set which can then be searched for in the speech. The lexeme-set KWS Pipeline The pipeline starts with a lexeme of interest from the evaluation set (§2.1). Inflections of the lexeme are generated using some generation tool or manual resource (§2.2). These inflections are then converted to a phonemic representation (§2.3) before being added to the lexicon used in speech"
2020.sigmorphon-1.25,P19-1172,1,0.838134,"eely available online.2 2 Inflection generation is the task of producing an inflection, given a lemma and a bundle of morphosyntactic features. For example, run + {P RES ;3;S G} 7→ “runs”. The state of the art in inflection generation has arisen from the CoNLL– SIGMORPHON Shared Tasks (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019), and typically consists of a modified sequence-to-sequence model with attention (Makarov and Clematide, 2018). However, these systems are fully supervised, and hand-curated morphological dictionaries often do not exist. We instead turn to the methods of Nicolai and Yarowsky (2019), who use English annotation as distant supervision to induce target language morphology, using a widelytranslated, verse-parallel text: the Bible. Starting from the inflection pairs extracted by their method, we ensemble generators trained using an RNN and DirecTL+ (Jiampojamarn et al., 2010). For each lemma in the respective UniMorph, we generate hypotheses for each feature bundle, ensembling via a linear combination of confidence scores. This gives us a set of inflections for each of the lexemes in the evaluation set which can then be searched for in the speech. The lexeme-set KWS Pipeline"
2020.sigmorphon-1.25,L18-1293,1,0.898687,"Missing"
2020.sigmorphon-1.25,N06-1030,0,0.0609829,"other methods. The result is an evaluation set tailored to morphologically salient word forms, with 1250 Turkish paradigms and 59 Bengali paradigms. The set of evaluation languages that can be extended to other languages in the Babel set for which we have ground truth paradigms. 2 Inflection Generation Grapheme-to-Phoneme Conversion To include hypothesized inflections in the KWS pipeline, orthographic forms of inflections must be mapped to a phonemic form consistent with the units used by the acoustic model (Maskey et al., 2004; Chen et al., 2016; Mortensen et al., 2018; Schultz et al., 2007; Kominek and Black, 2006; Deri and Knight, 2016; Trmal et al., 2017). We use a finite-state transducer model trained with Phonetisaurus3 on 5,000 word forms in the target language. 2.4 Keyword Search After generating inflections of lemmas in the evaluation set, these inflections are then included in the lexicon used in KWS. The KWS involves decoding the speech into lattices, and assessing lattice’s inclusion of the keyword of interest. Our pipeline builds on the Kaldi OpenKWS system (Trmal et al., 2017), which uses the standard lattice indexing approach of (Can and Saraclar, 2011). We 3 github.com/AdolfVonKleist/ Pho"
2020.sigmorphon-1.25,K18-3001,1,0.853286,"mponents serves as a novel downstream evaluation of inflection generation approaches, as well the other components in the pipeline. We make this recipe and evaluation set freely available online.2 2 Inflection generation is the task of producing an inflection, given a lemma and a bundle of morphosyntactic features. For example, run + {P RES ;3;S G} 7→ “runs”. The state of the art in inflection generation has arisen from the CoNLL– SIGMORPHON Shared Tasks (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019), and typically consists of a modified sequence-to-sequence model with attention (Makarov and Clematide, 2018). However, these systems are fully supervised, and hand-curated morphological dictionaries often do not exist. We instead turn to the methods of Nicolai and Yarowsky (2019), who use English annotation as distant supervision to induce target language morphology, using a widelytranslated, verse-parallel text: the Bible. Starting from the inflection pairs extracted by their method, we ensemble generators trained using an RNN and DirecTL+ (Jiampojamarn et al., 2010). For each lemma in the respective UniMorph, we generate hypotheses for each feature bundle, ensembling via a linear combination of co"
2020.sigmorphon-1.25,W19-4226,1,0.893738,"Missing"
2020.sigmorphon-1.25,L18-1429,0,0.0156979,"ructed evaluation sets to compare against our other methods. The result is an evaluation set tailored to morphologically salient word forms, with 1250 Turkish paradigms and 59 Bengali paradigms. The set of evaluation languages that can be extended to other languages in the Babel set for which we have ground truth paradigms. 2 Inflection Generation Grapheme-to-Phoneme Conversion To include hypothesized inflections in the KWS pipeline, orthographic forms of inflections must be mapped to a phonemic form consistent with the units used by the acoustic model (Maskey et al., 2004; Chen et al., 2016; Mortensen et al., 2018; Schultz et al., 2007; Kominek and Black, 2006; Deri and Knight, 2016; Trmal et al., 2017). We use a finite-state transducer model trained with Phonetisaurus3 on 5,000 word forms in the target language. 2.4 Keyword Search After generating inflections of lemmas in the evaluation set, these inflections are then included in the lexicon used in KWS. The KWS involves decoding the speech into lattices, and assessing lattice’s inclusion of the keyword of interest. Our pipeline builds on the Kaldi OpenKWS system (Trmal et al., 2017), which uses the standard lattice indexing approach of (Can and Sarac"
2020.sigmorphon-1.25,P15-2111,1,0.858481,"Missing"
2020.sigmorphon-1.25,D14-1095,0,0.0254514,"inclusion of the keyword of interest. Our pipeline builds on the Kaldi OpenKWS system (Trmal et al., 2017), which uses the standard lattice indexing approach of (Can and Saraclar, 2011). We 3 github.com/AdolfVonKleist/ Phonetisaurus https://github.com/oadams/inflection-kws 211 use augmented pronunciation lexicons for KWS, which has been shown to outperform proxy KWS, a popular alternative (Chen et al., 2013). The novel problem of lexeme-set KWS is related to work on out-of-vocabulary KWS, which has been approached by handling sub-word units such as syllables and morphemes (Trmal et al., 2014; Narasimhan et al., 2014; van Heerden et al., 2017; He et al., 2016). In contrast to KWS with sub-word granularity, our approach is to generate likely full-word inflections given a lemma. For language modeling, we used a 4-gram modified Kneser-Ney baseline (Kneser and Ney, 1995). We compare using as training data the indomain Babel text to the Bible, a resource available for many languages, and which was the resource used for cross-lingual distant supervision for inflection generation described in Section 2.2. Hypothesized inflections not seen in the training data receive some probability mass in language model smoot"
2021.findings-acl.353,I13-1112,0,0.103983,"or language preservation and revitalization, where models can help coin neologisms for modern terms. Owing to recent successes of machine translation models for similar tasks (Tsvetkov and Dyer, 2015; Gorman et al., 2020; Wu and Yarowsky, 2020a,b), this paper investigates the application of neural sequence-to-sequence models for the task of etymology prediction. Specifically, we focus on word borrowings, where a word enters a language via a non-related donor language.1 Whereas inherited words and cognates tend to follow regular sound shifts and can be modeled well with transliteration models (Beinborn et al., 2013; Wu and Yarowsky, 2018b), words borrowed from unrelated languages undergo various processes (Section 3) that may not preserve the structure or phonetics of the original word. We propose to model borrowings in two tasks (Figure 1), motivated in Section 4. In Task 1, given a donor word and etymological relation, can we predict the form of the incorporated word in the borrowing language? In the opposite direction, in 1 This is in contrast to other etymological relations, such as inheritance, where words enter through a related language, e.g. from Latin to French. 4032 Findings of the Association"
2021.findings-acl.353,P16-1038,0,0.0291965,"Similar approaches have also been applied to the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology information from the English edition of Wiktionary using Yawipa (Wu and Yarowsky, 2020a), a recent Wiktionary parser. We focus on six specific types of borrowings (whose Wiktionary label is in monospaced font below) across a spectrum of semantic and phonetic fidelity: • calque: Also called a loan translation. Components of the original word are literally translated into the target language, e.g. the English brainwash, from the Chinese 洗脑 xi ‘wash’ + nao ‘brain’. • partial calque: A calque where not every component is"
2021.findings-acl.353,2020.sigmorphon-1.2,0,0.0446945,"Missing"
2021.findings-acl.353,2020.amta-research.9,0,0.032981,"sed voting procedure that combines the output of the LSTM-sep, LSTM, and TF models. Each model gives 5 votes for their top prediction, 4 votes for their second place prediction, and so on (1 vote for fifth place). For each test instance, the votes are tallied up, and the prediction with the highest number of votes is the prediction of the ensemble. Ties are broken by picking the prediction with the highest model decoder score among all the models. For Task 2, we experiment with a baseline LSTM model and the same model with copy attention. All models were trained using the OpenNMTpy framework (Klein et al., 2020). The LSTM models are two-layer encoder-decoders with 500dimension hidden state, trained with the ADAM optimizer. The Transformer model has a 6-layer encoder and decoder with 8 heads, trained with ADAM with learning rate scheduling. For reproducibility, we provide the training scripts which include the full model details. Accounting for the extreme imbalance in our dataset, we performed a stratified split of the dataset into a 80-10-10 traindev-test split, where each split contains the same proportion of languages and borrowing relations. 6 To tackle these two tasks, we employ character neural"
2021.findings-acl.353,D18-2012,0,0.0238587,"sk is to reconstruct these missing edges. As Wiktionary is a humanannotated resource, there is much variance in the quality and completeness of annotations, and good performance on this task can help fill in etymology even in high-resource languages like English. 5 Experiments In: abe k a b i j Out: eng c a b b a g e bor For Task 1, we experiment with separate LSTM models trained for each borrowing relation (LSTMsep), a single multi-task LSTM model trained on the combined data (LSTM), the same model trained with both the source and target data preprocessed by the unigram SentencePiece method (Kudo and Richardson, 2018) with a vocabulary size of 4000 (LSTM-spm), the same model with copy attention (See et al., 2017) (LSTM-copy), a Transformer (Vaswani et al., 2017) model (TF), and an ensembling method (Ensemble). This method is a scorebased voting procedure that combines the output of the LSTM-sep, LSTM, and TF models. Each model gives 5 votes for their top prediction, 4 votes for their second place prediction, and so on (1 vote for fifth place). For each test instance, the votes are tallied up, and the prediction with the highest number of votes is the prediction of the ensemble. Ties are broken by picking t"
2021.findings-acl.353,2020.coling-main.387,1,0.754464,"the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology information from the English edition of Wiktionary using Yawipa (Wu and Yarowsky, 2020a), a recent Wiktionary parser. We focus on six specific types of borrowings (whose Wiktionary label is in monospaced font below) across a spectrum of semantic and phonetic fidelity: • calque: Also called a loan translation. Components of the original word are literally translated into the target language, e.g. the English brainwash, from the Chinese 洗脑 xi ‘wash’ + nao ‘brain’. • partial calque: A calque where not every component is translated, e.g. the English apple strudel"
2021.findings-acl.353,2021.naacl-main.353,0,0.0330816,"ata from Wiktionary indicate that modeling borrowings is a challenging task with much room for future research. 2 eng lat fra spa ara san grc deu rus ita Related Work Though the tasks defined in this paper are new, there are several related threads of work. In the task of cognate transliteration, a system is trained to generate cognates in a different language (Beinborn et al., 2013; Wu and Yarowsky, 2018b). This paper uses a multilingual cognate transliteration approach applied specifically to borrowings. Similar approaches have also been applied to the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology informatio"
2021.findings-acl.353,W18-6319,0,0.0126303,"dicting the incorporated word, the input is a sequence containing: the donor language, each character of the donor word, the etymological relation, and the target language. The output is the characters of the incorporated word. In: eng c a b b a g e bor abe Out: k a b i j For Task 2, the input is a sequence containing the word’s language and each character of the word, while the output is the donor language, donor word characters, and relation. 6.1 Results and Analysis Task 1 We evaluate each model on a held-out 15,288 example test set. Table 2 presents character BLEU (computed with SacreBLEU Post (2018)) as well as accuracy and character edit distance from the gold (CED). We also report 5-best results for accuracy (was the correct answer in the top 5 results?) and CED (within the top 5 results, what is the minimum edit distance to the correct answer?) At a cursory glance, the single models trained on all the data performs slightly better compared to the separate relation-specific models, following a 4034 Model BLEU Acc CED 5Acc 5CED LSTM-sep 53.77 20.00 2.42 33.51 1.82 LSTM LSTM-copy LSTM-spm Transformer 55.83 55.90 45.62 61.30 21.43 19.92 10.68 22.19 2.31 2.32 2.85 2.06 34.98 34.46 20.31 41"
2021.findings-acl.353,P17-1099,0,0.0310502,"ce in the quality and completeness of annotations, and good performance on this task can help fill in etymology even in high-resource languages like English. 5 Experiments In: abe k a b i j Out: eng c a b b a g e bor For Task 1, we experiment with separate LSTM models trained for each borrowing relation (LSTMsep), a single multi-task LSTM model trained on the combined data (LSTM), the same model trained with both the source and target data preprocessed by the unigram SentencePiece method (Kudo and Richardson, 2018) with a vocabulary size of 4000 (LSTM-spm), the same model with copy attention (See et al., 2017) (LSTM-copy), a Transformer (Vaswani et al., 2017) model (TF), and an ensembling method (Ensemble). This method is a scorebased voting procedure that combines the output of the LSTM-sep, LSTM, and TF models. Each model gives 5 votes for their top prediction, 4 votes for their second place prediction, and so on (1 vote for fifth place). For each test instance, the votes are tallied up, and the prediction with the highest number of votes is the prediction of the ensemble. Ties are broken by picking the prediction with the highest model decoder score among all the models. For Task 2, we experimen"
2021.findings-acl.353,P15-2021,0,0.0251483,"am applications. Perhaps the most salient is lexicon expansion: more comprehensive dictionaries will enable better communication between cultures as well as better training material for machine translation systems. Computational etymology is also important for historical linguistics, whose focus is on discovering the relationships between languages and their words. An accurate model of word borrowing can also be a boon for language preservation and revitalization, where models can help coin neologisms for modern terms. Owing to recent successes of machine translation models for similar tasks (Tsvetkov and Dyer, 2015; Gorman et al., 2020; Wu and Yarowsky, 2020a,b), this paper investigates the application of neural sequence-to-sequence models for the task of etymology prediction. Specifically, we focus on word borrowings, where a word enters a language via a non-related donor language.1 Whereas inherited words and cognates tend to follow regular sound shifts and can be modeled well with transliteration models (Beinborn et al., 2013; Wu and Yarowsky, 2018b), words borrowed from unrelated languages undergo various processes (Section 3) that may not preserve the structure or phonetics of the original word. We"
2021.findings-acl.353,N12-1039,0,0.0350715,"of cognate transliteration, a system is trained to generate cognates in a different language (Beinborn et al., 2013; Wu and Yarowsky, 2018b). This paper uses a multilingual cognate transliteration approach applied specifically to borrowings. Similar approaches have also been applied to the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology information from the English edition of Wiktionary using Yawipa (Wu and Yarowsky, 2020a), a recent Wiktionary parser. We focus on six specific types of borrowings (whose Wiktionary label is in monospaced font below) across a spectrum of semantic and phonetic fidelity: • calque: Also"
2021.findings-acl.353,L18-1263,1,0.84961,", a system is trained to generate cognates in a different language (Beinborn et al., 2013; Wu and Yarowsky, 2018b). This paper uses a multilingual cognate transliteration approach applied specifically to borrowings. Similar approaches have also been applied to the task of proto-language reconstruction (Meloni et al., 2021). Related to cognate transliteration is the task of grapheme-to-phoneme conversion, which has a long history of research. Cognate transliteration can be viewed as G2P across languages, where the words are cognates, for example in the case of names (Waxmonsky and Reddy, 2012; Wu et al., 2018; Wu and Yarowsky, 2018a). Recently, researchers have studied massively multilingual versions of these tasks, where single (neural) models are trained on the combination of hundreds of languages (e.g. Deri and Knight, 2016; Gorman et al., 2020; Lewis et al., 2020). 3 Lang Data We extract etymology information from the English edition of Wiktionary using Yawipa (Wu and Yarowsky, 2020a), a recent Wiktionary parser. We focus on six specific types of borrowings (whose Wiktionary label is in monospaced font below) across a spectrum of semantic and phonetic fidelity: • calque: Also called a loan tra"
2021.findings-acl.353,L18-1150,1,0.909102,"n and revitalization, where models can help coin neologisms for modern terms. Owing to recent successes of machine translation models for similar tasks (Tsvetkov and Dyer, 2015; Gorman et al., 2020; Wu and Yarowsky, 2020a,b), this paper investigates the application of neural sequence-to-sequence models for the task of etymology prediction. Specifically, we focus on word borrowings, where a word enters a language via a non-related donor language.1 Whereas inherited words and cognates tend to follow regular sound shifts and can be modeled well with transliteration models (Beinborn et al., 2013; Wu and Yarowsky, 2018b), words borrowed from unrelated languages undergo various processes (Section 3) that may not preserve the structure or phonetics of the original word. We propose to model borrowings in two tasks (Figure 1), motivated in Section 4. In Task 1, given a donor word and etymological relation, can we predict the form of the incorporated word in the borrowing language? In the opposite direction, in 1 This is in contrast to other etymological relations, such as inheritance, where words enter through a related language, e.g. from Latin to French. 4032 Findings of the Association for Computational Ling"
2021.findings-acl.353,L18-1538,1,0.92552,"n and revitalization, where models can help coin neologisms for modern terms. Owing to recent successes of machine translation models for similar tasks (Tsvetkov and Dyer, 2015; Gorman et al., 2020; Wu and Yarowsky, 2020a,b), this paper investigates the application of neural sequence-to-sequence models for the task of etymology prediction. Specifically, we focus on word borrowings, where a word enters a language via a non-related donor language.1 Whereas inherited words and cognates tend to follow regular sound shifts and can be modeled well with transliteration models (Beinborn et al., 2013; Wu and Yarowsky, 2018b), words borrowed from unrelated languages undergo various processes (Section 3) that may not preserve the structure or phonetics of the original word. We propose to model borrowings in two tasks (Figure 1), motivated in Section 4. In Task 1, given a donor word and etymological relation, can we predict the form of the incorporated word in the borrowing language? In the opposite direction, in 1 This is in contrast to other etymological relations, such as inheritance, where words enter through a related language, e.g. from Latin to French. 4032 Findings of the Association for Computational Ling"
2021.findings-acl.353,2020.lrec-1.397,1,0.871464,"task is in green and orange, respectively. Introduction Words are borrowed into a language through various processes. For example, the English internet was incorporated into Welsh as rhyngrwyd (rhyng‘between’ + rhwyd ‘net’) through a calqueing process where each component is translated literally. In contrast, the English chimpanzee became the Welsh tsimpansˆı through a process of sound correspondences. Borrowing is prevalent across the world’s languages, and modeling how and from where words enter a language are interesting but understudied tasks under the umbrella of computational etymology (Wu and Yarowsky, 2020a). This is a relatively new field with many downstream applications. Perhaps the most salient is lexicon expansion: more comprehensive dictionaries will enable better communication between cultures as well as better training material for machine translation systems. Computational etymology is also important for historical linguistics, whose focus is on discovering the relationships between languages and their words. An accurate model of word borrowing can also be a boon for language preservation and revitalization, where models can help coin neologisms for modern terms. Owing to recent succes"
2021.findings-acl.353,2020.coling-main.413,1,0.892907,"task is in green and orange, respectively. Introduction Words are borrowed into a language through various processes. For example, the English internet was incorporated into Welsh as rhyngrwyd (rhyng‘between’ + rhwyd ‘net’) through a calqueing process where each component is translated literally. In contrast, the English chimpanzee became the Welsh tsimpansˆı through a process of sound correspondences. Borrowing is prevalent across the world’s languages, and modeling how and from where words enter a language are interesting but understudied tasks under the umbrella of computational etymology (Wu and Yarowsky, 2020a). This is a relatively new field with many downstream applications. Perhaps the most salient is lexicon expansion: more comprehensive dictionaries will enable better communication between cultures as well as better training material for machine translation systems. Computational etymology is also important for historical linguistics, whose focus is on discovering the relationships between languages and their words. An accurate model of word borrowing can also be a boon for language preservation and revitalization, where models can help coin neologisms for modern terms. Owing to recent succes"
2021.sigmorphon-1.25,K18-3001,1,0.679933,"l marker used for expressing case, familiarity, plurality, and (sometimes) gender within animate nouns. Pronouns are marked for different cases and honorificity levels. These paradigms are generated on the basis of a manually annotated corpus of Magahi folktales. We used a raw dataset from the literary domain. First, we annotated the dataset with the Universal Dependency morphological feature tags at token level using the CoNLL-U editor (Heinecke, 2019). We then converted the annotated dataset into the UniMorph schema using the script available for converting UD data into the UniMorph tagset (McCarthy et al., 2018). To finalize the data, we manually validated the dataset against the UniMorph schema (Sylak-Glassman et al., 2015a). Brajbhasha, or Braj is one of the Indo-Aryan languages spoken in the Western Indian states of Uttar Pradesh, Madhya Pradesh, and Rajasthan. Grierson (1908) groups Brajbhasha under Western Hindi of the Central Group in the Indo-Aryan family, along with other languages like Hindustani, Bangaru, Kannauji, and Bundeli. Braj is not generally used in education or for any official purposes in any Braj spoken state, but it has a very rich literary tradition. Also in order to preserve,"
2021.sigmorphon-1.25,K17-2001,1,0.858355,"Missing"
2021.sigmorphon-1.25,U19-1001,1,0.893844,"nje-ng ‘1/3PL-again-wrong-BENmeat-cook-PP’ (“I cooked the wrong meat for them again”). As shown, the form has several prefixes and suffixes attached to the stem. As in other Australian languages, long vowels are typically represented by double characters, and trills with “rr”.3 According to Evans’ (2003) analysis, the verb template contains 12 affix slots which include two incorporated noun classes, and derivational affixes such as the benefactive and comitative. The data included in this set are verbs extracted from the Kunwinjku translation of the Bible using the morphological analyzer from Lane and Bird (2019) and manually verified by human annotators. 3.2 Afro-Asiatic The Afro-Asiatic language family is represented by the Semitic subgroup. 3.2.1 Semitic: Classical Syriac Classical Syriac is a dialect of the Aramaic language and is attested as early as the 1st century CE. As with most Semitic languages, it displays non-concatenative morphology involving primarily tri-consonantal roots. Syriac nouns and adjectives are conventionally classified into three ‘states’— Emphatic, Absolute, Construct—which loosely correlate with the syntactic features of definiteness, indeterminacy and the genitive. There"
2021.sigmorphon-1.25,U19-1005,1,0.782664,"respect to morphology and realized in the UniMorph schema (Sylak-Glassman et al., 2015b). Morphosyntactic features (such as “the dative case” or “the past tense”) in the UniMorph occupy an intermediate position between the descriptive categories and comparative concepts. The set of features was initially established on the basis of analysis of typological literature, and refined with the addition of new languages to the UniMorph database (Kirov et al., 2018; McCarthy et al., 2020). Since 2016, SIGMORPHON organized shared tasks on morphological reinflection (Cotterell et al., 2016, 2017, 2018; McCarthy et al., 2019; Vylomova et al., 2020) that aimed at evaluating contemporary systems. Parallel to that, they also served as a platform for enriching the UniMorph database with new languages. For instance, the 2020 shared task (Vylomova et al., 2020) featured 90 typologically diverse languages derived from various linguistic resources. This year, we are bringing many under-resourced languages (languages of Peru, Russia, India, Australia, Papua New Guinea) and dialects (e.g., for Arabic and Kurdish). The sample is highly diverse: it contains languages with templatic, concatenative (fusional and agglutinative)"
2021.sigmorphon-1.25,W02-0604,0,0.0878472,"Missing"
2021.sigmorphon-1.25,U08-1018,0,0.0643919,"through affixation, compounding, or reduplication. The four types of Indonesian affixes are prefixes, suffixes, circumfixes (combination of prefixes and suffixes), and infixes (inside the base form). Indonesian uses both full and partial reduplication processes to form words. Full reduplication is often used to express the plural forms of nouns, while partial reduplication is typically used to derive forms that might have a different category than their base forms. Unlike English, the distinction between inflectional and derivational morphological processes in Indonesian is not always clear (Pisceldo et al., 2008). In this shared task, the Indonesian data is created by bootstrapping the data from an Indonesian Wikipedia dump. Using a list of possible Indonesian affixes, we collect unique word forms from Wikipedia and analyze them using MorphInd (Larasati et al., 2011), a morphological analyzer tool for Indonesian based on an FST. We manually create a mapping between the MorphInd tagset and the UniMorph schema. We then use this mapping and apply some additional rule-based formulas created by Indonesian linguists to build the final dataset (Table 9). 3.9.2 Malayo-Polynesian: Kodi/Kodhi Kodi or Kodhi [koâ"
2021.sigmorphon-1.25,N19-1119,0,0.0213266,"ugmentation technique presented by Anastasopoulos and Neubig (2019). More specifically, the team implemented an encoder–decoder model with an attention mechanism. The encoder processes a character sequence using an LSTM-based RNN with attention. Tags are encoded with a selfattention (Vaswani et al., 2017) position-invariant module. The decoder is an LSTM with separate attention mechanisms for the lemma and the tags. GUClasp focus their efforts on exploring strategies for training a multilingual model, in particular, they implement the following strategies: curriculum learning with competence (Platanios et al., 2019) based on character frequency and L BME GUClasp afb amh ara arz heb syc ame cni ind kod aym ckt itl gup bra bul ces ckb deu kmr mag nld pol por rus spa see ail evn sah tyv krl lud olo vep 92.39 98.16 99.76 95.27 97.46 21.71 82.46 99.5 81.31 94.62 99.98 44.74 32.4 14.75 58.52 98.9 98.03 99.46 97.98 98.21 70.2 98.28 99.54 99.85 98.07 99.82 78.28 6.84 51.9 99.95 99.97 99.88 59.46 99.72 99.72 81.71 93.81 94.86 87.12 89.93 10.57 55.94 93.36 55.68 87.1 99.97 52.63 31.28 21.31 56.91 96.46 94.00 96.60 91.94 98.09 72.24 94.91 98.52 99.11 94.32 97.65 40.97 6.46 51.5 99.69 99.78 98.50 59.46 98.2 97.05 sj"
2021.sigmorphon-1.25,2020.acl-main.597,1,0.915066,"arget inflected form—and removed all forms other than verbs, nouns, or adjectives. We then capped the dataset sizes to a maximum of 100,000 instances per language, subsampling when necessary. Finally, we create a 70–10–20 train–dev–test split per language, splitting the data across these sets at the instance level (as opposed to, e.g., the lemma one). As such, the information about a lemma’s declension or inflection class is spread out across these train, dev and test sets, making this task much simpler than if one had to predict the entire class from the lemma’s form alone, as done by, e.g., Williams et al. (2020) and Liu and Hulden (2021). 5 Baseline Systems The organizers provide four neural systems as baselines, a product of two models and optional data augmentation. The first model is a transformer (Vaswani et al., 2017, TRM), and the second model is an adaption of the transformer to character-level transduction tasks (Wu et al., 2021, CHR-TRM), which holds the state-of-the-art on the 2017 SIGMORPHON shared task data. Both models follow the hyperparameters of Wu et al. (2021). The optional data augmentation follows the technique proposed by Anastasopoulos and Neubig (2019). Rely14 The new languages"
C02-1070,J95-4004,0,0.010277,"reated, we apply an English IE system to the English texts and transfer the IE annotations to the target language as follows: 1. Sentence align the parallel corpus.1 2. Word-align the parallel corpus using the Giza++ system (Och and Ney, 2000). 3. Transfer English IE annotations and nounphrase boundaries to French via the mechanism described in Yarowsky et al. (2001), yielding annotated sentence pairs as illustrated in Figure 1. 4. Train a stand-alone IE tagger on these projected annotations (described in Section 4). 4 Transformation-Based Learning We used transformation-based learning (TBL) (Brill, 1995) to learn information extraction rules for French. TBL is well-suited for this task because it uses rule templates as the basis for learning, which can be easily modeled after English extraction patterns. However, information extraction systems typically rely on a shallow parser to identify syntactic elements (e.g., subjects and direct objects) and verb 1 This is trivial because each sentence has a numbered anchor preserved by the MT system. Rule Condition Rule Effect 1. w1 =crashed w2 =in w3 is LOC . 2. w1 =wreckage w2 =of w3 is VEH . 3. w1 =injuring w2 is VIC . 4. w1 =NOUN w2 =crashed w1 is"
C02-1070,P00-1035,1,0.88979,"Missing"
C02-1070,P98-1067,0,0.0265457,"or a specific domain, and the types of facts to be extracted are defined in advance. In this paper, we will focus on the domain of plane crashes and will try to extract descriptions of the vehicle involved in the crash, victims of the crash, and the location of the crash. Most IE systems use some form of extraction patterns to recognize and extract relevant information. Many techniques have been developed to generate extraction patterns for a new domain automatically, including PALKA (Kim & Moldovan, 1993), AutoSlog (Riloff, 1993), CRYSTAL (Soderland et al., 1995), RAPIER (Califf, 1998), SRV (Freitag, 1998), meta-bootstrapping (Riloff & Jones, 1999), and ExDisco (Yangarber et al., 2000). For this work, we will use AutoSlog-TS (Riloff, 1996b) to generate IE patterns for the plane crash domain. AutoSlog-TS is a derivative of AutoSlog that automatically generates extraction patterns by gathering statistics from a corpus of relevant texts (within the domain) and irrelevant texts (outside the domain). Each extraction pattern represents a linguistic expression that can extract noun phrases from one of three syntactic positions: subject, direct object, or object of a prepositional phrase. For example,"
C02-1070,N01-1006,0,0.0351997,"Missing"
C02-1070,P00-1056,0,0.0248368,"ause it frees cross-language projection research from the relatively few large existing bilingual corpora (such as the Canadian Hansards). MT allows projection to be performed on any corpus, such as the domain-specific planecrash news stories employed here. Section 5 gives the details of the MT system and corpora that we used. Once the artificial parallel corpus has been created, we apply an English IE system to the English texts and transfer the IE annotations to the target language as follows: 1. Sentence align the parallel corpus.1 2. Word-align the parallel corpus using the Giza++ system (Och and Ney, 2000). 3. Transfer English IE annotations and nounphrase boundaries to French via the mechanism described in Yarowsky et al. (2001), yielding annotated sentence pairs as illustrated in Figure 1. 4. Train a stand-alone IE tagger on these projected annotations (described in Section 4). 4 Transformation-Based Learning We used transformation-based learning (TBL) (Brill, 1995) to learn information extraction rules for French. TBL is well-suited for this task because it uses rule templates as the basis for learning, which can be easily modeled after English extraction patterns. However, information extra"
C02-1070,C00-2136,0,0.0314821,"in advance. In this paper, we will focus on the domain of plane crashes and will try to extract descriptions of the vehicle involved in the crash, victims of the crash, and the location of the crash. Most IE systems use some form of extraction patterns to recognize and extract relevant information. Many techniques have been developed to generate extraction patterns for a new domain automatically, including PALKA (Kim & Moldovan, 1993), AutoSlog (Riloff, 1993), CRYSTAL (Soderland et al., 1995), RAPIER (Califf, 1998), SRV (Freitag, 1998), meta-bootstrapping (Riloff & Jones, 1999), and ExDisco (Yangarber et al., 2000). For this work, we will use AutoSlog-TS (Riloff, 1996b) to generate IE patterns for the plane crash domain. AutoSlog-TS is a derivative of AutoSlog that automatically generates extraction patterns by gathering statistics from a corpus of relevant texts (within the domain) and irrelevant texts (outside the domain). Each extraction pattern represents a linguistic expression that can extract noun phrases from one of three syntactic positions: subject, direct object, or object of a prepositional phrase. For example, the following patterns could extract vehicles involved in a plane crash: “<subjec"
C02-1070,H01-1035,1,0.939542,"ish, annotated corpora and text analysis tools are readily available. However, for the large majority of the world’s languages, resources such as treebanks, part-of-speech taggers, and parsers do not exist. And even for many of the better-supported languages, cutting edge analysis tools in areas such as information extraction are not readily available. One solution to this NLP-resource disparity is to transfer linguistic resources, tools, and domain knowledge from resource-rich languages to resource-impoverished ones. In recent years, there has been a burst of projects based on this paradigm. Yarowsky et al. (2001) developed cross-language projection models for part-of-speech tags, base noun phrases, named-entity tags, and morphological analysis (lemmatization) for four languages. Resnik et al. (2001) developed related models for projecting dependency parsers from English to Chinese. There has also been extensive work on the cross-language transfer and development of ontologies and WordNets (e.g., (Atserias et al., 1997)). 3.2 Mechanics of Projection The cross-language projection methodology employed in this paper is based on Yarowsky et al. (2001), with one important exception. Given the absence of ava"
C02-1070,C98-1064,0,\N,Missing
C92-2070,P91-1034,0,0.72369,"enjoyed some success handcoding detailed ""word experts"" (Small and Rieger, 1982; HirsL 1987), but this labor intensive process has severely limited coverage beyond small vocabularies. Others such as Lesk (1986), Walker (1987), Veronis and Ide (1990), and Guthrie et al. (1991) have turned to machine readable dictionaries (MRD's) in an effort to achieve broad vocabulary coverage. MRD's have the useful property that some indicative words for each sense are directly available in numbered definitions and examples. However, definitions arc often too short to AClXSDECOLING-92, NANqVS,23-28 Aovr 1992 Brown et al. (1991), Dagan (1991), and Gale ct at. (1992) have looked to parallel bilingual corpora to further automate training set acquisition. By identifying word correspondences in a bilingual text such as the Canadian Parliamentary Proceedings (Hansards), the translations found fur each English word may serve as sense tags. For example, the senses of sentence may be identified through their correspondence in the French to phrase (grammatical sentence) or peine (legal sentence). While this method has been used successfully on a portion of the vocabulary, its coverage is also limited. Currently available bili"
C92-2070,P91-1019,0,0.0935263,"Missing"
C92-2070,P90-1032,0,0.00647558,"Missing"
C92-2070,C90-2067,0,\N,Missing
C92-2070,J92-4003,0,\N,Missing
C92-2070,P91-1017,0,\N,Missing
D08-1108,W04-1102,0,0.21796,"Missing"
D08-1108,W06-0103,0,0.0404607,"Missing"
D08-1108,P07-2045,0,0.00683529,"Missing"
D08-1108,A97-1010,0,0.0431181,"g and Lai, 2004; Chang and Teng, 2006) is supervised because it requires the full-abbreviation relations as training data. Li and Yarowsky (2008) propose an unsupervised method to extract the relations between full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; 1039 Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the featu"
D08-1108,P04-1021,0,0.0329899,"r abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; 1039 Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the hypotheses are unsupervised by exploiting the data co-occurrence phenomena. 7 Conclusions In this"
D08-1108,P08-1049,1,0.775865,"thers 2.3 44.8 Table 2: Chinese Formal-informal Relations: Categories and Examples. Literal glosses in brackets. For illustrative purposes, we can present the transformation path showing how the informal phrase is obtained from the formal phrase. In particular, the transformation path for this category is “Formal → PinYin → Informal (similar or same PinYin as the formal phrase)”. 2.2 Abbreviation and Acronym A Chinese abbreviation of a formal phrase is obtained by selecting one or more characters from this formal phrase, and the selected characters can be at any position in the formal phrase (Li and Yarowsky, 2008; Lee, 2005; Yin, 1999). In comparison, an acronym is a special form of abbreviation, where only the first character of each word in the formal phrase is selected to form the informal phrase. Table 2 presents three examples belonging to this category. While the first example is an abbreviation, and the other two examples are acronyms. The transformation path for the second example is “Formal → PinYin → Acronym”, and the transformation path for the third example is “Formal → English → Acronym”. Clearly, they differ in whether PinYin or English is used as a bridge. 2.3 Transliteration A translit"
D08-1108,W02-2018,0,0.0282166,"as follows, LLR (~ α) = N X log Pα~ (yj |xj ) − j=1 ||~ α||2 2σ 2 (3) where N is the number of training examples, and the α||2 regularization term ||~ is a Gaussian prior with a 2σ 2 2 variance σ (Roark et al., 2007). The optimal weight vector α ~ ∗ is obtained by maximizing the regularized log-likelihood (LLR ), that is, α ~ ∗ = arg max LLR (~ α) α ~ (4) To maximize the above function, we use a limitedmemory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). 1036 During test time, the following decision rule is normally used to predict the optimal formal phrase y ∗ for a given informal phrase x, y ∗ = arg max s(x, y). y (5) 4.4.2 Feature Functions As mentioned before, we incorporate both the rule- and data-driven intuitions as feature functions in the log-linear model. Rule-driven feature functions: Clearly, if a pair (x, y) matches the rule patterns described in Table 2, the pair has a high possibility to be a true formalinformal relation. To reflect this intuition, we develop several feature functions as follows. • LD-PinYin(x, y): the Levensh"
D08-1108,P02-1021,0,0.0701142,"(2008) propose an unsupervised method to extract the relations between full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; 1039 Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the"
D08-1108,W01-0516,0,0.0721854,"quires the full-abbreviation relations as training data. Li and Yarowsky (2008) propose an unsupervised method to extract the relations between full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; 1039 Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a sma"
D08-1108,W03-1508,0,0.0476899,"full-form phrases and their abbreviations. They exploit the data co-occurrence phenomena in the newswire text, as we have done in this paper. Moreover, they augment and improve a statistical machine translation by incorporating the extracted relations into the baseline translation system. Other interesting work that addresses a similar task as ours includes the work on homophones (e.g., Lee and Chen (1997)), abbreviations with their definitions (e.g., Park and Byrd (2001)), abbreviations and acronyms in the medical domain (Pakhomov, 2002), and transliteration (e.g., (Knight and Graehl, 1998; Virga and Khudanpur, 2003; Li et al., 2004; 1039 Wu and Chang, 2007)). While all the above work deals with the relations occurring within the formal text, we consider the formal-informal relations that occur across both formal and informal text, and we extract the relations from the web corpora, instead from just formal text. Moreover, our method is semi-supervised in the sense that the weights of the feature functions are tuned in a supervised log-linear model using a small number of seed relations while the generation and ranking of the hypotheses are unsupervised by exploiting the data co-occurrence phenomena. 7 Co"
D08-1108,P95-1026,1,0.0864619,"as new relations between informal and formal phrases emerge every day on the Internet. Alternatively, one can employ a large amount of formal text (e.g., newswire) and informal text (e.g., Internet blogs) to derive such a list as follows. Specifically, from the informal corpus we can extract those phrases whose frequency in the informal corpus is significantly different from that in the formal corpus. However, such a list may be quite noisy, i.e., many of them are not informal phrases at all. An alternative approach to extracting the informal phrases is to use a bootstrapping algorithm (e.g., Yarowsky (1995)). Specifically, we first manually collect a small set of example relations. Then, using these relations as a seed set, we extract the text patterns (e.g., the definition pattern showing how the informal and formal phrases co-occur in the data as discussed in Section 3.1). With these patterns, we identify many more new relations from the data and augment them into the seed set. The procedure iterates. Using such an approach, we should be able to extract a large list of formal-informal relations. Clearly, the list extracted in this way may be quite noisy, and thus it is important to exploit bot"
D08-1108,J98-4003,0,\N,Missing
D13-1187,W11-0705,0,0.0170317,"hone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2 Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ 1816 us"
D13-1187,baccianella-etal-2010-sentiwordnet,0,0.102318,"using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for E NGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct H INDI and F RENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a S PANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate G ERMAN, RUSSIAN and A RABIC subjective lexicons. 3 www.cs.pitt.edu/mpqa/opinionfinder We use the corpus-based, language-independent approach proposed by Volkova et al. (2013) to bootstrap Twitter-specific subjectivity lexicons. To start, the new lexicon is seeded with terms from the initial lexicon LI . On each iteration, tweets in the unlabeled data are labeled using the current lexicon. If a tweet contains one or more terms from the lexicon it is mar"
D13-1187,banea-etal-2008-bootstrapping,0,0.0148633,"Missing"
D13-1187,C10-2005,0,0.0164597,"ender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g.,"
D13-1187,W12-2108,1,0.855686,"Missing"
D13-1187,D11-1120,0,0.256898,"., female 2 Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ 1816 users express positive emoticons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the use of subjective lan"
D13-1187,C12-1037,0,0.0121949,"LE LE I B 2.3 16.8 2.8 4.7 5.1 21.5 Spanish LSI LSB 2.9 7.7 5.2 14.6 8.1 22.3 Russian LR LR I B 1.4 5.3 2.3 5.5 3.7 10.8 Table 2: The initial LI and the bootstrapped LB (highlighted) lexicon term count (LI ⊂ LB ) with polarity across languages (thousands). 1818 to the corresponding initial lexicons LI and the existing state-of-the-art subjective lexicons including: • 8K strongly subjective English terms from SentiWordNet χE (Baccianella et al., 2010); • 1.5K full strength terms from the Spanish sentiment lexicon χS (Perez-Rosas et al., 2012); • 5K terms from the Russian sentiment lexicon χR (Chetviorkin and Loukachevitch, 2012). For that we apply rule-based subjectivity classification on the test data.4 This subjectivity classifier predicts that a tweet is subjective if it contains at least one, or at least two subjective terms from the lexicon. To make a fair comparison, we automatically expand χE with plurals and inflectional forms, χS with the inflectional forms for verbs, and χR with the inflectional forms for adverbs, adjectives and verbs. We report precision, recall and Fmeasure results in Table 3 and show that our bootstrapped lexicons outperform the corresponding initial lexicons and the external resources."
D13-1187,cieri-etal-2004-fisher,0,0.00819554,"derdependent sentiment terms that is needed for statistically significant improvements. To the best of our knowledge, this work is the first to show that incorporating gender leads to significant improvements for sentiment analysis, particularly subjectivity and polarity classification, for multiple languages in social media. 2 Related Work Numerous studies since the early 1970’s have investigated gender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et a"
D13-1187,C10-2028,0,0.0280183,"nderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2 Gender-dependent and independent lexical resources of subjective terms in Twitter for Russ"
D13-1187,D10-1124,0,0.273384,"Missing"
D13-1187,P09-1080,1,0.435912,"ing degrees on MySpace, e.g., female 2 Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ 1816 users express positive emoticons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the"
D13-1187,P11-2008,0,0.0388984,"Missing"
D13-1187,P11-1016,0,0.0261437,"differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2 Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu."
D13-1187,D07-1115,0,0.00771613,"sis in Twitter (Pak and Paroubek, 2010; Kouloumpis et al., 2011). 4.1 Bootstrapping Subjectivity Lexicons Recent work by Banea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for E NGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct H INDI and F RENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a S PANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate G ERMAN, RUSSIAN and A RABIC subjective lexicons. 3 www.cs.pitt.edu/mpqa/opinionfinder We use the corpus-based, language-independent approac"
D13-1187,Y12-1013,0,0.0109199,"analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2 Gender-dependent and independent lexical resources of subjective terms in Twitter for Russian, Spanish and English can be found here: http://www.cs.jhu.edu/~svitlana/ 1816 users express positive emoticons more than male users. Other researchers included subjective patterns as"
D13-1187,P12-4004,0,0.0456002,"Missing"
D13-1187,W11-1709,0,0.0219586,"cant improvements. To the best of our knowledge, this work is the first to show that incorporating gender leads to significant improvements for sentiment analysis, particularly subjectivity and polarity classification, for multiple languages in social media. 2 Related Work Numerous studies since the early 1970’s have investigated gender-language differences in interaction, theme, and grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012;"
D13-1187,D10-1021,0,0.0270991,"ons more than male users. Other researchers included subjective patterns as features for gender classification of Twitter users (Rao et al., 2010). They found that the majority of emotion-bearing features, e.g., emoticons, repeated letters, exasperation, are used more by female than male users, which is consistent with results reported in other recent work (Garera and Yarowsky, 2009; Burger et al., 2011; Goswami et al., 2009; Argamon et al., 2007). Other related work is that of Otterbacher (2010), who studied stylistic differences between male and female reviewers writing product reviews, and Mukherjee and Liu (2010), who applied positive, negative and emotional connotation features for gender classification in microblogs. Although previous work has investigated gender differences in the use of subjective language, and features of sentiment have been used in gender identification, to the best of our knowledge no one has yet investigated whether gender differences in the use of subjective language can be exploited to improve sentiment classification in English or any other language. In this paper we seek to answer this question for the domain of social media. 3 Data For the experiments in this paper, we us"
D13-1187,P11-1032,0,0.0178586,"01; Mohammad and Yang, 2011; Eisenstein et al., 2010; O’Connor et al., 2010; Bamman et al., 2012). However, one area that remains largely unexplored is the effect of demographic language variation on subjective language use, and whether these David Yarowsky Center for Language and Speech Processing Johns Hopkins University Baltimore, MD yarowsky@cs.jhu.edu differences may be exploited for automatic sentiment analysis. With the growing commercial importance of applications such as personalized recommender systems and targeted advertising (Fan and Chang, 2009), detecting helpful product review (Ott et al., 2011), tracking sentiment in real time (Resnik, 2013), and large-scale, low-cost, passive polling (O’Connor et al., 2010), we believe that sentiment analysis guided by user demographics is a very important direction for research. In this paper, we focus on gender demographics and language in social media to investigate differences in the language used to express opinions in Twitter for three languages: English, Spanish, and Russian. We focus on Twitter data because of its volume, dynamic nature, and diverse population worldwide.1 We find that some words are more or less likely to be positive or neg"
D13-1187,pak-paroubek-2010-twitter,0,0.0684419,"grammar among other topics (Schiffman, 2002; Sunderland et al., 2002). More recent research has studied gender differences in telephone speech (Cieri et al., 2004; Godfrey et al., 1992) and emails (Styler, 2011). Mohammad and Yang (2011) analyzed gender differences in the expression of sentiment in love letters, hate mail, and suicide notes, and emotional word usage across genders in email. There has also been a considerable amount of work in subjectivity and sentiment analysis over the past decade, including, more recently, in microblogs (Barbosa and Feng, 2010; Bermingham and Smeaton, 2010; Pak and Paroubek, 2010; Bifet and Frank, 2010; Davidov et al., 2010; Li et al., 2010; Kouloumpis et al., 2011; Jiang et al., 2011; Agarwal et al., 2011; Wang et al., 2011; Calais Guerra et al., 2011; Tan et al., 2011; Chen et al., 2012; Li et al., 2012). In spite of the surge of research in both sentiment and social media, only a limited amount of work focusing on gender identification has looked at differences in subjective language across genders within social media. Thelwall (2010) found that men and women use emoticons to differing degrees on MySpace, e.g., female 2 Gender-dependent and independent lexical reso"
D13-1187,perez-rosas-etal-2012-learning,0,0.0134663,"d lexicons, we compare our Twitter-specific lexicons LB Pos Neg Total English LE LE I B 2.3 16.8 2.8 4.7 5.1 21.5 Spanish LSI LSB 2.9 7.7 5.2 14.6 8.1 22.3 Russian LR LR I B 1.4 5.3 2.3 5.5 3.7 10.8 Table 2: The initial LI and the bootstrapped LB (highlighted) lexicon term count (LI ⊂ LB ) with polarity across languages (thousands). 1818 to the corresponding initial lexicons LI and the existing state-of-the-art subjective lexicons including: • 8K strongly subjective English terms from SentiWordNet χE (Baccianella et al., 2010); • 1.5K full strength terms from the Spanish sentiment lexicon χS (Perez-Rosas et al., 2012); • 5K terms from the Russian sentiment lexicon χR (Chetviorkin and Loukachevitch, 2012). For that we apply rule-based subjectivity classification on the test data.4 This subjectivity classifier predicts that a tweet is subjective if it contains at least one, or at least two subjective terms from the lexicon. To make a fair comparison, we automatically expand χE with plurals and inflectional forms, χS with the inflectional forms for verbs, and χR with the inflectional forms for adverbs, adjectives and verbs. We report precision, recall and Fmeasure results in Table 3 and show that our bootstra"
D13-1187,E09-1077,0,0.0159432,"al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for E NGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct H INDI and F RENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a S PANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate G ERMAN, RUSSIAN and A RABIC subjective lexicons. 3 www.cs.pitt.edu/mpqa/opinionfinder We use the corpus-based, language-independent approach proposed by Volkova et al. (2013) to bootstrap Twitter-specific subjectivity lexicons. To start, the new lexicon is seeded with"
D13-1187,W03-1014,0,0.297833,"the criteria to add to the lexicon. The parameters are optimized using a grid search on the development data using F-measure for subjectivity classification. In Table 2 we report size and term polarity from the initial LI and the bootstrapped LB lexicons. Although more sophisticated bootstrapping methods exist, this approach has been shown to be effective for atomically learning subjectivity lexicons in multiple languages on a large scale without any external, rich, lexical resources, e.g., WordNet, or advanced NLP tools, e.g., syntactic parsers (Wiebe, 2000) or information extraction tools (Riloff and Wiebe, 2003). For English, seed terms for bootstrapping are the strongly subjective terms in the MPQA lexicon (Wilson et al., 2005). For Spanish and Russian, the seed terms are obtained by translating the English seed terms using a bi-lingual dictionary, collecting subjectivity judgments from MTurk on the translations, filtering out translations that are not strongly subjective, and expanding the resulting word lists with plurals and inflectional forms. To verify that bootstrapping does provide a better resource than existing dictionary-expanded lexicons, we compare our Twitter-specific lexicons LB Pos Ne"
D13-1187,D08-1027,0,0.108331,"Missing"
D13-1187,J11-2001,0,0.00360266,"ion approaches: (I) rule-based classifier which uses only subjective terms from the lexicons designed to verify if the gender differences in subjective language create enough of a signal to influence sentiment classification; (II) state-of-the-art supervised models which rely on lexical features as well as lexicon set-count features.10,11 Moreover, to show that the gender9 For polarity classification we distinguish between positive and negative instances, which is the approach typically reported in the literature for recognizing polarity (Velikovich et al., 2010; Yessenalina and Cardie, 2011; Taboada et al., 2011) 10 A set-count feature is a count of the number of instances from a set of terms that appears in a tweet. 11 We also experimented with repeated punctuation (!!, ??) and letters (nooo, reealy), which are often used in sentiment classification in social media. However, we found these features 1821 sentiment signal can be learned by more than one classifier we apply a variety of classifiers implemented in Weka (Hall et al., 2009). For that we do 10-fold cross validation over English, Spanish, and Russian test data (ET EST, ST EST and RT EST) labeled with subjectivity (pos, neg, both vs. neut) an"
D13-1187,P02-1053,0,0.00371276,"l media, and in fact are sometimes used to create noisy training data for sentiment analysis in Twitter (Pak and Paroubek, 2010; Kouloumpis et al., 2011). 4.1 Bootstrapping Subjectivity Lexicons Recent work by Banea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Corpusbased methods extract subjectivity lexicons from unlabeled data using different similarity metrics to measure the relatedness between words, e.g., Pointwise Mutual Information (PMI). Corpus-based methods have been used to bootstrap lexicons for E NGLISH (Turney, 2002) and other languages, including ROMANIAN (Banea et al., 2008) and JAPANESE (Kaji and Kitsuregawa, 2007). Dictionary-based methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct H INDI and F RENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a S PANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate G ERMAN, RUSSIAN and A RABIC subjecti"
D13-1187,N10-1119,0,0.00923507,"ment classification. We experiment with two classification approaches: (I) rule-based classifier which uses only subjective terms from the lexicons designed to verify if the gender differences in subjective language create enough of a signal to influence sentiment classification; (II) state-of-the-art supervised models which rely on lexical features as well as lexicon set-count features.10,11 Moreover, to show that the gender9 For polarity classification we distinguish between positive and negative instances, which is the approach typically reported in the literature for recognizing polarity (Velikovich et al., 2010; Yessenalina and Cardie, 2011; Taboada et al., 2011) 10 A set-count feature is a count of the number of instances from a set of terms that appears in a tweet. 11 We also experimented with repeated punctuation (!!, ??) and letters (nooo, reealy), which are often used in sentiment classification in social media. However, we found these features 1821 sentiment signal can be learned by more than one classifier we apply a variety of classifiers implemented in Weka (Hall et al., 2009). For that we do 10-fold cross validation over English, Spanish, and Russian test data (ET EST, ST EST and RT EST) l"
D13-1187,P13-2090,1,0.198352,"ased methods rely on relations between words in existing lexical resources. For example, Rao and Ravichandran (2009) construct H INDI and F RENCH sentiment lexicons using relations in WordNet (Miller, 1995), Rosas et. al. (2012) bootstrap a S PANISH lexicon using SentiWordNet (Baccianella et al., 2010) and OpinionFinder,3 Clematide and Klenner (2010), Chetviorkin et al. (2012) and Abdul-Mageed et. al. (2011) automatically expand and evaluate G ERMAN, RUSSIAN and A RABIC subjective lexicons. 3 www.cs.pitt.edu/mpqa/opinionfinder We use the corpus-based, language-independent approach proposed by Volkova et al. (2013) to bootstrap Twitter-specific subjectivity lexicons. To start, the new lexicon is seeded with terms from the initial lexicon LI . On each iteration, tweets in the unlabeled data are labeled using the current lexicon. If a tweet contains one or more terms from the lexicon it is marked subjective, otherwise neutral. Tweet polarity is determined in a similar way, but takes into account negation. For every term not in the lexicon with a frequency threshold, the probability of that word appearing in a subjective sentence is calculated. The top k terms with a subjective probability are then added t"
D13-1187,H05-1044,1,0.0380328,"ure for subjectivity classification. In Table 2 we report size and term polarity from the initial LI and the bootstrapped LB lexicons. Although more sophisticated bootstrapping methods exist, this approach has been shown to be effective for atomically learning subjectivity lexicons in multiple languages on a large scale without any external, rich, lexical resources, e.g., WordNet, or advanced NLP tools, e.g., syntactic parsers (Wiebe, 2000) or information extraction tools (Riloff and Wiebe, 2003). For English, seed terms for bootstrapping are the strongly subjective terms in the MPQA lexicon (Wilson et al., 2005). For Spanish and Russian, the seed terms are obtained by translating the English seed terms using a bi-lingual dictionary, collecting subjectivity judgments from MTurk on the translations, filtering out translations that are not strongly subjective, and expanding the resulting word lists with plurals and inflectional forms. To verify that bootstrapping does provide a better resource than existing dictionary-expanded lexicons, we compare our Twitter-specific lexicons LB Pos Neg Total English LE LE I B 2.3 16.8 2.8 4.7 5.1 21.5 Spanish LSI LSB 2.9 7.7 5.2 14.6 8.1 22.3 Russian LR LR I B 1.4 5.3"
D13-1187,D11-1016,0,0.00668118,"xperiment with two classification approaches: (I) rule-based classifier which uses only subjective terms from the lexicons designed to verify if the gender differences in subjective language create enough of a signal to influence sentiment classification; (II) state-of-the-art supervised models which rely on lexical features as well as lexicon set-count features.10,11 Moreover, to show that the gender9 For polarity classification we distinguish between positive and negative instances, which is the approach typically reported in the literature for recognizing polarity (Velikovich et al., 2010; Yessenalina and Cardie, 2011; Taboada et al., 2011) 10 A set-count feature is a count of the number of instances from a set of terms that appears in a tweet. 11 We also experimented with repeated punctuation (!!, ??) and letters (nooo, reealy), which are often used in sentiment classification in social media. However, we found these features 1821 sentiment signal can be learned by more than one classifier we apply a variety of classifiers implemented in Weka (Hall et al., 2009). For that we do 10-fold cross validation over English, Spanish, and Russian test data (ET EST, ST EST and RT EST) labeled with subjectivity (pos,"
D13-1187,P11-2103,0,\N,Missing
D17-1074,N15-1107,0,0.0573578,"literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational m"
D17-1074,W14-4012,0,0.107262,"Missing"
D17-1074,K17-2001,1,0.914766,"t -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigmatic, where slots refer to semantic categories, e.g., corrode+RESULT7→corrosion, we draw upon recent advances in the generation of infl"
D17-1074,K15-1017,1,0.908059,"Missing"
D17-1074,N13-1138,0,0.0477791,"ervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplor"
D17-1074,N16-1077,0,0.0470827,"y complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigmatic, where slots refer"
D17-1074,P16-2090,0,0.122677,"Missing"
D17-1074,P96-1004,0,0.25946,"ional morphology. For example, English is labeled as morphologically impoverished, whereas German and Russian are considered morphologically rich, e.g., see the introduction of Tsarfaty et al. (2010). As regards derivation, English is quite complex and even similar in richness to German or Russian as it contains productive formations from two substrata: Germanic and Latinate. From this perspective, English is very much a morphologically rich language. Indeed, a corpus study on the Brown Corpus showed that the majority of English words are morphologically complex when derivation is considered (Light, 1996). Note that there many languages that have exhibit neither rich inflection, nor rich derivational morphology, e.g., Chinese, which most commonly employs compounding for word word formation (Chung et al., 2014). 3 Task and Models We discuss our two systems for derivational paradigm completion and the results they achieve. 3.1 Data We experiment on English derivational triples extracted from NomBank (Meyers et al., 2004).4 Each triple consists of a base form, the semantics of the derivation and a corresponding derived form e.g., hameliorate, RESULT, ameliorationi. Note that in this task we do no"
D17-1074,W04-2705,0,0.179042,"Missing"
D17-1074,Q15-1012,0,0.0186166,"advancely from advance, rather than inadvance, as well as in PATIENT nominalizations, e.g., the model produces containee in place of content—this last distinction is unpredictable. 5 Related Work Previous work in unsupervised morphological segmentation and has implicitly incorporated derivational morphology. Such systems attempt to segment words into all constituent morphs, treating inflectional and derivational affixes as equivalent. The popular Morfessor tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al. (2015) inter alia. Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectio"
D17-1074,N15-1093,0,0.0263326,"nalysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature."
D17-1074,N09-1024,0,0.0345515,", e.g., we generate advancely from advance, rather than inadvance, as well as in PATIENT nominalizations, e.g., the model produces containee in place of content—this last distinction is unpredictable. 5 Related Work Previous work in unsupervised morphological segmentation and has implicitly incorporated derivational morphology. Such systems attempt to segment words into all constituent morphs, treating inflectional and derivational affixes as equivalent. The popular Morfessor tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al. (2015) inter alia. Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in t"
D17-1074,N16-1076,1,0.853692,"eak down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigma"
D17-1074,W13-3504,0,0.024175,"or tool (Creutz and Lagus, 2007) is one example of such an unsupervised segmentation system, but many others exist, e.g., Poon et al. (2009), Narasimhan et al. (2015) inter alia. Supervised segmentation and analysis models in the literature can also break down derivationally complex forms into their morphs, provided preaffix F1 affix F1 affix F1 -ly -er -ation -or 1.0 0.86 0.78 0.59 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-se"
D17-1074,E17-3017,0,0.0739484,"Missing"
D17-1074,W10-1401,0,0.027253,"e less restricted. A parsimonious model of derivational morphology would describe forms using productive rules when possible, but may store forms with highly restricted patterns directly as full lexical items. A Note On Terminology. We would like to make a subtle, but important point regarding terminology: the phrase morphologically rich in the NLP community almost exclusively refers to inflectional, rather than derivational morphology. For example, English is labeled as morphologically impoverished, whereas German and Russian are considered morphologically rich, e.g., see the introduction of Tsarfaty et al. (2010). As regards derivation, English is quite complex and even similar in richness to German or Russian as it contains productive formations from two substrata: Germanic and Latinate. From this perspective, English is very much a morphologically rich language. Indeed, a corpus study on the Brown Corpus showed that the majority of English words are morphologically complex when derivation is considered (Light, 1996). Note that there many languages that have exhibit neither rich inflection, nor rich derivational morphology, e.g., Chinese, which most commonly employs compounding for word word formatio"
D17-1074,E17-2019,1,0.788873,"9 -ity -ment -ist -ness 0.54 0.45 0.43 0.40 -ence -ure -ee -age 0.32 0.22 0.20 0.20 Table 4: F1 for various suffix attachments with the sequenceto-sequence model segmented and labeled data is available for training (Ruokolainen et al., 2013; Cotterell et al., 2015; Cotterell and Sch¨utze, 2017). Our work, however, builds directly upon recent efforts in the generation of inflectional morphology (Durrett and DeNero, 2013; Nicolai et al., 2015; Ahlberg et al., 2015; Rastogi et al., 2016; Faruqui et al., 2016). We differ in that we focus on derivational morphology. In another recent line of work, Vylomova et al. (2017) predict derivationally complex forms using sentential context. Our work differs from their approach in that we attempt to generate derivational forms divorced from the context, but the underlying neural sequence-to-sequence architecture is quite similar. 6 Conclusion We have presented a statistical model for the generation of derivationally complex forms, a task that has gone essentially unexplored in the literature. Viewing derivational morphology as paradigmatic, where slots refer to semantic categories, e.g., corrode+RESULT7→corrosion, we draw upon recent advances in the generation of infl"
D19-1229,C10-3010,0,0.0789159,"elicited from 30 speakers over several days to ensure salience, then filtered by a dictionary to keep only conventional (rather than novel) color descriptors. We omit 12 of these which do not appear in the datasets we employ. Translation may act as a useful resource for disambiguating word senses (Diab and Resnik, 2002). By translating English BCTs into other languages, we can find their basic color terms. Then, backtranslating to English, we obtain a list of the potential senses of a given color term. We draw translations of color terms from two large type-level dictionary resources, PanLex (Baldwin et al., 2010; Kamholz et al., 2014) and Wiktionary, which together provide color word translations for 2491 languages or dialects. For each of the 11 basic color concepts and 80 secondary terms e in English, we translate it into every available foreign language ` by dictionary lookup to get a set of non-English color words F` . We then back-translate each term into English (again by lookup) to get a S (e) set E` = f 2F` {e0 |trans`!En ( f ) = e0 } of possible round-trip translations through the language `. The final dataset contains tuples of the form (English color word, foreign language, foreign word, E"
D19-1229,P02-1033,0,0.0487358,"c color terms are red, orange, yellow, green, blue, purple, brown, pink, black, white, and grey. These align to the eleven basic color categories identified by Berlin and Kay (1969). In addition to these eleven, we consider a list of 92 second-tier color terms identified by Casson (1994). These were elicited from 30 speakers over several days to ensure salience, then filtered by a dictionary to keep only conventional (rather than novel) color descriptors. We omit 12 of these which do not appear in the datasets we employ. Translation may act as a useful resource for disambiguating word senses (Diab and Resnik, 2002). By translating English BCTs into other languages, we can find their basic color terms. Then, backtranslating to English, we obtain a list of the potential senses of a given color term. We draw translations of color terms from two large type-level dictionary resources, PanLex (Baldwin et al., 2010; Kamholz et al., 2014) and Wiktionary, which together provide color word translations for 2491 languages or dialects. For each of the 11 basic color concepts and 80 secondary terms e in English, we translate it into every available foreign language ` by dictionary lookup to get a set of non-English"
D19-1229,kamholz-etal-2014-panlex,0,0.0181453,"ers over several days to ensure salience, then filtered by a dictionary to keep only conventional (rather than novel) color descriptors. We omit 12 of these which do not appear in the datasets we employ. Translation may act as a useful resource for disambiguating word senses (Diab and Resnik, 2002). By translating English BCTs into other languages, we can find their basic color terms. Then, backtranslating to English, we obtain a list of the potential senses of a given color term. We draw translations of color terms from two large type-level dictionary resources, PanLex (Baldwin et al., 2010; Kamholz et al., 2014) and Wiktionary, which together provide color word translations for 2491 languages or dialects. For each of the 11 basic color concepts and 80 secondary terms e in English, we translate it into every available foreign language ` by dictionary lookup to get a set of non-English color words F` . We then back-translate each term into English (again by lookup) to get a S (e) set E` = f 2F` {e0 |trans`!En ( f ) = e0 } of possible round-trip translations through the language `. The final dataset contains tuples of the form (English color word, foreign language, foreign word, English back-translation"
D19-1229,P18-1007,0,0.025461,"nvert them to another class—e.g., “тут” in Archi (aqc) in Table 3, which is a fused morpheme denoting adjectivalization and marking Archi’s fourth gender. This morpheme appears in the Archi’s terms for black and white. As with Nahuatl, this implies that Archi lacks basic color terms— according to a strict interpretation of the criteria. 6.2 using the Viterbi algorithm, where the individual segment probabilities are maximum a posteriori estimates under a Dirichlet prior (a = 0.01). The model, inspired by Ge et al. (1999), is similar to other unigram segmentation models (Creutz and Lagus, 2005; Kudo, 2018). We then search for these affixes across the terms recorded in that language, to determine whether the affix is broadly derivational or specific to color terms. Select results are given in Table 3. Consider the -tic suffix in Nahuatl (nci). This morpheme semantically denotes “color”; one combines real-world referents with this suffix to obtain colors, e.g., chichiltic (chili-color). This appears even with black and white—the first basic color Compound detection To particularly identify color names which are formed by compounding of words, we extend a model for compound discovery to identify c"
D19-1229,L18-1612,1,0.313878,"“color”; one combines real-world referents with this suffix to obtain colors, e.g., chichiltic (chili-color). This appears even with black and white—the first basic color Compound detection To particularly identify color names which are formed by compounding of words, we extend a model for compound discovery to identify color terms which were produced compositionally. This lets us ask two questions about the BCT definition: (1) Across languages, are there “basic” color terms that are not monolexemic, and (2) Are “basic” terms less likely to be compounds? The answer to both, we find, is “yes”. Wu and Yarowsky (2018) propose a multilingual compound analysis and generation method that only requires a readily available multilingual dictionary. They first extract potential compounds by splitting any word into three substrings corresponding to a left component, glue, and right com2245 Process Basic Secondary Inheritance Derivation Cognate Borrowing None of these 1161 82 303 18 42566 2356 183 483 84 65969 compounding than secondary colors; nevertheless, this should be taken with a grain of salt: The annotations for secondary colors are less complete, so while we use these scores, we do not take the criterion o"
D19-1229,J93-2004,0,0.0650183,"Missing"
E09-1035,N03-2024,0,0.0193094,"Missing"
E09-1035,P06-2002,0,0.0531409,"o this 1 E.g.: http://www.nndb.com, http://www.biography.com, Infoboxes in Wikipedia Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300–308, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 300 seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as “Birthdate”, “Deathdate"
E09-1035,P02-1006,0,0.407649,"raphies. In particular, we have proposed and evaluated the following 6 distinct original approaches to this 1 E.g.: http://www.nndb.com, http://www.biography.com, Infoboxes in Wikipedia Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300–308, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 300 seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextua"
E09-1035,S07-1012,0,0.024789,"Missing"
E09-1035,W99-0202,0,0.0459531,"Missing"
E09-1035,P98-1012,0,0.0388521,"Missing"
E09-1035,E06-1002,0,0.0453434,"Missing"
E09-1035,2000.bcs-1.23,0,0.0625346,"e attributes of co-occurring entities, broad-context topical profiles, inter-attribute correlations and likely human age distributions. For illustrative purposes, we motivate each technique using one or two attributes but in practice they can be applied to a wide range of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes. Related Work The literature for biography extraction falls into two major classes. The first one deals with identifying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few 301 4 P c(A1 xA2 yA3 ) P (r(p, q)|A1 pA2 qA3 ) = Px,y∈rc(A xA zA ) 1 2 3 Contextual Pattern-Based Model x,z A standard model for extracting biographic facts is to learn templatic contextual patterns such as &lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such templatic patterns can be le"
E09-1035,D07-1074,0,0.0345038,"Missing"
E09-1035,P01-1059,0,0.409575,"ccurring entities, broad-context topical profiles, inter-attribute correlations and likely human age distributions. For illustrative purposes, we motivate each technique using one or two attributes but in practice they can be applied to a wide range of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes. Related Work The literature for biography extraction falls into two major classes. The first one deals with identifying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few 301 4 P c(A1 xA2 yA3 ) P (r(p, q)|A1 pA2 qA3 ) = Px,y∈rc(A xA zA ) 1 2 3 Contextual Pattern-Based Model x,z A standard model for extracting biographic facts is to learn templatic contextual patterns such as &lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such templatic patterns can be learned using seed example"
E09-1035,N06-1038,0,0.0217841,"hapter of the ACL, pages 300–308, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 300 seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as “Birthdate”, “Deathdate”, “Birthplace” and “Occupation” have been shown to work well in the pattern-learning framework (Mann and Yarowsky, 2005; Alfonesca, 200"
E09-1035,W02-1028,0,0.286689,"use of diverse sources of information present in biographies. In particular, we have proposed and evaluated the following 6 distinct original approaches to this 1 E.g.: http://www.nndb.com, http://www.biography.com, Infoboxes in Wikipedia Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300–308, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 300 seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts lik"
E09-1035,H05-1015,0,0.0587006,"Missing"
E09-1035,A97-1030,0,0.0515047,"Missing"
E09-1035,C92-2082,0,0.0715588,"ity”, and “Religion”, making use of diverse sources of information present in biographies. In particular, we have proposed and evaluated the following 6 distinct original approaches to this 1 E.g.: http://www.nndb.com, http://www.biography.com, Infoboxes in Wikipedia Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300–308, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 300 seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography cl"
E09-1035,C04-1188,0,0.0512316,"Missing"
E09-1035,W06-1671,0,0.0474465,"Missing"
E09-1035,W03-0405,1,0.748948,"ve proposed and evaluated the following 6 distinct original approaches to this 1 E.g.: http://www.nndb.com, http://www.biography.com, Infoboxes in Wikipedia Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300–308, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 300 seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a fe"
E09-1035,W04-3256,0,0.0214788,"-context topical profiles, inter-attribute correlations and likely human age distributions. For illustrative purposes, we motivate each technique using one or two attributes but in practice they can be applied to a wide range of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes. Related Work The literature for biography extraction falls into two major classes. The first one deals with identifying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004). The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc. For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few 301 4 P c(A1 xA2 yA3 ) P (r(p, q)|A1 pA2 qA3 ) = Px,y∈rc(A xA zA ) 1 2 3 Contextual Pattern-Based Model x,z A standard model for extracting biographic facts is to learn templatic contextual patterns such as &lt;NAME&gt; “was born in” &lt;Birthplace&gt;. Such templatic patterns can be learned using seed examples of the attribute i"
E09-1035,P05-1060,1,0.90394,"nct original approaches to this 1 E.g.: http://www.nndb.com, http://www.biography.com, Infoboxes in Wikipedia Proceedings of the 12th Conference of the European Chapter of the ACL, pages 300–308, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 300 seeds of the semantic relationship of interest and learns contextual patterns such as “&lt;NAME&gt; was born in &lt;Birthplace&gt;” or “&lt;NAME&gt; (born &lt;Birthdate&gt;)” (Hearst, 1992; Riloff, 1996; Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006). There has also been some work on extracting biographic facts directly from Wikipedia pages. Culotta et al. (2006) deal with learning contextual patterns for extracting family relationships from Wikipedia. Ruiz-Casado et al. (2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages. While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as"
E09-1035,C98-1012,0,\N,Missing
E12-1014,D09-1109,0,0.0491141,"(log(n/nk ) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Alfonseca et al., 2009). The intuition is that news stories in different languages will tend to discuss the same world events on the same day. The frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates. For instance, if the phrase asian tsunami is used frequently during a particular time span, the Spanish translation maremoto asi´atico is likely to also be used frequently during that time. Figure 4 illustrates how the temporal distribution of terrorist is more similar to Spanish terrorista than to other Spanish phrases. We calculate the temporal similari"
E12-1014,D11-1029,0,0.0205152,"gnments across the two phrases. Because individual words are more frequent than multiword phrases, the accuracy of clex , tlex , and wlex tends to be higher than their phrasal equivalents (this is similar to the effect observed in Figure 2). Orthographic / phonetic similarity. The final lexical similarity feature that we incorporate is o(f, e), which measures the orthographic similarity between words in a phrase pair. Etymologically related words often retain similar spelling across languages with the same writing system, and low string edit distance sometimes signals translation equivalency. Berg-Kirkpatrick and Klein (2011) present methods for learning correspondences between the alphabets of two languages. We can also extend this idea to language pairs not sharing the same writing system since many cognates, borrowed words, and names remain phonetically similar. Transliterations can be generated for tokens in a source phrase (Knight and Graehl, 1997), with o(f, e) calculating phonetic similarity rather than orthographic. The three phrasal and four lexical similarity scores are incorporated into the log linear translation model as feature functions, replacing the bilingually estimated phrase translation probabil"
E12-1014,C88-1016,0,0.0456416,"Missing"
E12-1014,J93-2003,0,0.0204204,"mate the monolingual parameters in Section 3. This approach allows us to replace expensive/rare bilingual parallel training data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. sollte Wieviel 2 Background How much should you charge m m d for your d m Facebook d profile s Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: Figure 1: The reorde"
E12-1014,W10-1703,1,0.657033,"ver the full Europarl v5 parallel corpus (Koehn, 2005). With the exception of maximum phrase length (set to 3 in our experiments), we used default values for all of the parameters. All experiments use a trigram language model trained on the English side of the Europarl corpus using SRILM with Kneser-Ney smoothing. To tune feature weights in minimum error rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles. These development and test datasets were distributed in the WMT shared task (Callison-Burch et al., 2010).4 MERT 4 Specifcially, news-test2008 plus news-syscomb2009 for dev and newstest2009 for test. • Next, we estimate the features from truly monolingual corpora. To estimate the contextual and temporal similarity features, we use the Spanish and English Gigaword corpora.5 These corpora are substantially larger than the Europarl corpora, providing 27x as much Spanish and 67x as much English for contextual similarity, and 6x as many paired dates for temporal similarity. Topical similarity is estimated using Spanish and English Wikipedia articles that are paired with interlanguage links. To project"
E12-1014,2006.amta-papers.3,0,0.0362064,"Missing"
E12-1014,P08-2040,0,0.0237341,"been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-e"
E12-1014,P05-1033,0,0.0992244,"ining data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. sollte Wieviel 2 Background How much should you charge m m d for your d m Facebook d profile s Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: Figure 1: The reordering probabilities from the phrasebased models are estimated from bilingual data by calculating how often in the"
E12-1014,P11-2071,0,0.397556,"Missing"
E12-1014,P98-1069,0,0.436554,"e can re-use them directly without modification. The features are combined in a log linear model, and their weights are set through minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning trans"
E12-1014,W09-1117,1,0.878209,"res a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical machine translation. Given the obvious benefits of not having to rely on scarce bilingual parallel training data, it is surprising that bilingual lexicon induction has not been used for SMT before now. There are several open questions"
E12-1014,W01-1409,0,0.0135462,"Missing"
E12-1014,P08-1088,0,0.735932,"ual corpus, and (4) outputting the English words with the highest similarity as the most likely translations. 40 2.2 Bilingual lexicon induction for SMT 0 100 200 300 400 500 Top 1 Top 10 600 Corpus Frequency Figure 2: Accuracy of single-word translations induced using contextual similarity as a function of the source word corpus frequency. Accuracy is the proportion of the source words with at least one correct (bilingual dictionary) translation in the top 1 and top 10 candidate lists. nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al. (2008)). Although previous work reported high translation accuracy, it may be misleading to extrapolate the results to SMT, where it is necessary to translate a much larger set of words and phrases, including many low frequency items. In a preliminary study, we plotted the accuracy of translations against the frequency of the source words in the monolingual corpus. Figure 2 shows the result for translations induced using contextual similarity (defined in Section 3.1). Unsurprisingly, frequent terms have a substantially better chance of being paired with a correct translation, with words that only oc"
E12-1014,H05-1061,0,0.0096068,"ena et al. (2011) supplement an SMT phrase table with translation pairs extracted from a bilingual dictionary and give each a frequency of one for computing translation scores. Ravi and Knight (2011) treat MT without parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Concl"
E12-1014,P06-1103,1,0.859199,"tor as follows: wk = nf,k × (log(n/nk ) + 1) where nf,k and nk are the number of times sk appears in the context of f and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with f and the less common it is in the corpus in general, the higher its component value. Similarity between two vectors is measured as the cosine of the angle between them. Temporal similarity. In addition to contextual similarity, phrases in two languages may 133 be scored in terms of their temporal similarity (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006; Alfonseca et al., 2009). The intuition is that news stories in different languages will tend to discuss the same world events on the same day. The frequencies of translated phrases over time give them particular signatures that will tend to spike on the same dates. For instance, if the phrase asian tsunami is used frequently during a particular time span, the Spanish translation maremoto asi´atico is likely to also be used frequently during that time. Figure 4 illustrates how the temporal distribution of terrorist is more similar to Spanish terrorista than to other Spanish phrases. We calcul"
E12-1014,P97-1017,0,0.0974624,"e), which measures the orthographic similarity between words in a phrase pair. Etymologically related words often retain similar spelling across languages with the same writing system, and low string edit distance sometimes signals translation equivalency. Berg-Kirkpatrick and Klein (2011) present methods for learning correspondences between the alphabets of two languages. We can also extend this idea to language pairs not sharing the same writing system since many cognates, borrowed words, and names remain phonetically similar. Transliterations can be generated for tokens in a source phrase (Knight and Graehl, 1997), with o(f, e) calculating phonetic similarity rather than orthographic. The three phrasal and four lexical similarity scores are incorporated into the log linear translation model as feature functions, replacing the bilingually estimated phrase translation probabilities φ and lexical weighting probabilities w. Our seven similarity scores are not the only ones that could be incorporated into the translation model. Various other similarity scores can be computed depending on the available monolingual data and its associated metadata (see, e.g. Schafer and Yarowsky (2002)). 3.3 Reordering The re"
E12-1014,W02-0902,0,0.891337,"ernatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical machine translation. Given the obvious benefits of not having to rely on scarce bilingual parallel traini"
E12-1014,N03-1017,0,0.0174139,"replace expensive/rare bilingual parallel training data with two large monolingual corpora, a small bilingual dictionary, and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. sollte Wieviel 2 Background How much should you charge m m d for your d m Facebook d profile s Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: Figure 1: The reordering probabilities from the phrasebased models are estimated from bilingu"
E12-1014,J82-2005,0,0.784423,"Missing"
E12-1014,2005.mtsummit-papers.11,0,0.0139923,"re our method against the normal bilingual training procedure. We expect bilingual training to result in higher translation quality because it is a more direct method for learning translation probabilities. We systematically remove different parameters from the standard phrase-based model, and then replace them with our monolingual equivalents. Our goal is to recover as much of the loss as possible for each of the deleted bilingual components. The standard phrase-based model that we use as our top-line is the Moses system (Koehn et al., 2007) trained over the full Europarl v5 parallel corpus (Koehn, 2005). With the exception of maximum phrase length (set to 3 in our experiments), we used default values for all of the parameters. All experiments use a trigram language model trained on the English side of the Europarl corpus using SRILM with Kneser-Ney smoothing. To tune feature weights in minimum error rate training, we use a development bitext of 2,553 sentence pairs, and we evaluate performance on a test set of 2,525 single-reference translated newswire articles. These development and test datasets were distributed in the WMT shared task (Callison-Burch et al., 2010).4 MERT 4 Specifcially, ne"
E12-1014,W11-2132,0,0.0284881,"Missing"
E12-1014,D09-1092,0,0.202334,"normalized topic signatures. In our experiments, we use interlingual links between Wikipedia articles to estimate topic similarity. We treat each linked article pair as a topic and collect counts for each phrase across all articles in its corresponding language. Thus, the size of a phrase topic signature is the number of article pairs with interlingual links in Wikipedia, and each component contains the number of times the phrase appears in (the appropriate side of) the corresponding pair. Our Wikipedia-based topic similarity feature, w(f, e), is similar in spirit to polylingual topic models (Mimno et al., 2009), but it is scalable to full bilingual lexicon induction. 3.2 Lexical similarity features In addition to the three phrase similarity features used in our model – c(f, e), t(f, e) and w(f, e) – we include four additional lexical similarity features for each of phrase pair. The first three lexical features clex (f, e), tlex (f, e) and wlex (f, e) are the lexical equivalents of the phrase-level contextual, temporal and wikipedia topic similarity scores. They score the similarity of individual words within the phrases. To compute these lexical similarity features, we average similarity scores over"
E12-1014,P06-1011,0,0.119033,"parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obt"
E12-1014,J03-1002,0,0.00369842,"and ≈2,000 sentence bilingual development set, which are comparatively plentiful/inexpensive. sollte Wieviel 2 Background How much should you charge m m d for your d m Facebook d profile s Parameters of phrase-based SMT Statistical machine translation (SMT) was first formulated as a series of probabilistic models that learn word-to-word correspondences from sentence-aligned bilingual parallel corpora (Brown et al., 1993). Current methods, including phrase-based (Och, 2002; Koehn et al., 2003) and hierarchical models (Chiang, 2005), typically start by word-aligning a bilingual parallel corpus (Och and Ney, 2003). They extract multiword phrases that are consistent with the Viterbi word alignments and use these phrases to build new translations. A variety of parameters are estimated using the bitexts. Here we review the parameters of the standard phrase-based translation model (Koehn et al., 2007). Later we will show how to estimate them using monolingual texts instead. These parameters are: Figure 1: The reordering probabilities from the phrasebased models are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f, e) is orientated with the preceding phrase pair"
E12-1014,J04-4002,0,0.0468358,"he preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ei |fj ), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f ) also has associated reordering parameters, po (orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f ) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, φ(e|f ) and φ(f |e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates φ for phrase pairs with sparse counts, lexical weighting FFs are used"
E12-1014,P03-1021,0,0.00834943,"bilities, φ(e|f ) and φ(f |e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates φ for phrase pairs with sparse counts, lexical weighting FFs are used to smooth. Aver• Other features. Other typical features are n-gram language model scores and a phrase penalty, which governs whether to use fewer longer phrases or more shorter phrases. These are not bilingually estimated, so we can re-use them directly without modification. The features are combined in a log linear model, and their weights are set through minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated"
E12-1014,P95-1050,0,0.908394,"able features. 1 Introduction The parameters of statistical models of translation are typically estimated from large bilingual parallel corpora (Brown et al., 1993). However, these resources are not available for most language pairs, and they are expensive to produce in quantities sufficient for building a good translation system (Germann, 2001). We attempt an entirely different approach; we use cheap and plentiful monolingual resources to induce an end-toend statistical machine translation system. In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables. We then introduce a • In Section 2.2 we analyze the challenges of using bilingual lexicon induction for statistical MT (performance on low frequency items, and moving from words to phrases). • In Sections 3.1 and 3.2 we use multiple cues present in monolingual data to estimate lexical and phrasal translation scores. • In Section 3.3 we propose a novel algorithm for estimating phrase reordering features from monolingual texts. •"
E12-1014,P99-1067,0,0.629355,"be used to replace bilingual estimation of phrase- and lexical-translation probabilities, making a significant step towards SMT without parallel corpora. 3 Monolingual Parameter Estimation We use bilingual lexicon induction methods to estimate the parameters of a phrase-based translation model from monolingual data. Instead of scores estimated from bilingual parallel data, we make use of cues present in monolingual data to provide multiple orthogonal estimates of similarity between a pair of phrases. 3.1 Phrasal similarity features Contextual similarity. We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. More formally, assume that (s1 , s2 , . . . sN ) and (t1 , t2 , . . . tM ) are (arbitrarily indexed) source and target vocabularies, respectively. A source phrase f is represented with an Figure 4: Temporal histograms of the English phrase terrorist, its Spanish translation terrorista, and riqueza (wealth) collected from monolingual texts spanning a 13 year period. While the correct translation has a good temporal match, the non-translation riqueza has a distinctly different signature. N - and target phrase e with an M"
E12-1014,E09-1003,0,0.0187282,"Missing"
E12-1014,P11-1002,0,0.295311,"mated scores capture some novel information not contained in the standard feature set. 6 Additional Related Work Carbonell et al. (2006) described a data-driven MT system that used no parallel text. It produced translation lattices using a bilingual dictionary and scored them using an n-gram language model. Their method has no notion of translation similarity aside from a bilingual dictionary. Similarly, S´anchez-Cartagena et al. (2011) supplement an SMT phrase table with translation pairs extracted from a bilingual dictionary and give each a frequency of one for computing translation scores. Ravi and Knight (2011) treat MT without parallel training data as a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corp"
E12-1014,2011.mtsummit-papers.64,0,0.0247298,"Missing"
E12-1014,W02-2026,1,0.822051,"h minimum error rate training (Och, 2003). We use the same log linear formulation and MERT but propose alternatives derived directly from monolingual data for all parameters except for the phrase pairs themselves. Our pipeline still requires a small bitext of approximately 2,000 sentences to use as a development set for MERT parameter tuning. 131 A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window. For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to cooccur in time across languages. Koehn and Knight (2002) used similarity in spelling as another kind of cue that a pair of words may be translations of one another. Garera et al. (2009) defined context vectors using dependency relations rather than adjacent words. Bergsma and Van Durme (2011) used the visual similarity of labeled web images to learn translations of nouns. Additional related work on learning translations from monolingual corpora is discussed in Section 6. In this paper, we apply bilingual lexicon induction methods to statistical mac"
E12-1014,2009.mtsummit-posters.17,0,0.0111132,"ader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize our monolingually-estimated features to recover"
E12-1014,2008.iwslt-papers.6,0,0.00839041,"method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluated the performance of our algorithms in a full end-to-end translation system. Assuming that a bilingual-corpus-derived phrase table is available, we were able utilize"
E12-1014,N10-1063,0,0.0235881,"a decipherment task and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new language"
E12-1014,N04-4026,0,0.0201637,"asebased models are estimated from bilingual data by calculating how often in the parallel corpus a phrase pair (f, e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ei |fj ), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f ) also has associated reordering parameters, po (orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f ) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, φ(e|f ) and φ(f |e), which are typically calculated via m"
E12-1014,W03-1001,0,0.0241578,"rientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ei |fj ), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f ) also has associated reordering parameters, po (orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f ) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, φ(e|f ) and φ(f |e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates φ for phrase pairs with sparse counts, lexical we"
E12-1014,C10-1124,0,0.0123608,"and learn a translation model from monolingual text. They translate corpora of Spanish time expressions and subtitles, which both have a limited vocabulary, into English. Their method has not been applied to broader domains of text. Most work on learning translations from monolingual texts only examine small numbers of frequent words. Huang et al. (2005) and Daum´e and Jagarlamudi (2011) are exceptions that improve MT by mining translations for OOV items. A variety of past research has focused on mining parallel or comparable corpora from the web (Munteanu and Marcu, 2006; Smith et al., 2010; Uszkoreit et al., 2010). Others use an existing SMT system to discover parallel sentences within independent monolingual texts, and use them to re-train and enhance the system (Schwenk, 2008; Chen et al., 2008; Schwenk and Senellart, 2009; Rauf and Schwenk, 2009; Lambert et al., 2011). These are complementary but orthogonal to our research goals. 7 Conclusion This paper has demonstrated a novel set of techniques for successfully estimating phrase-based SMT parameters from monolingual corpora, potentially circumventing the need for large bitexts, which are expensive to obtain for new languages and domains. We evaluat"
E12-1014,P03-1041,0,0.0248361,"phrase pair (f, e) is orientated with the preceding phrase pair in the 3 types of orientations (monotone, swapped, and discontinuous). age word translation probabilities, w(ei |fj ), are calculated via phrase-pair-internal word alignments. • Reordering model. Each phrase pair (e, f ) also has associated reordering parameters, po (orientation|f, e), which indicate the distribution of its orientation with respect to the previously translated phrase. Orientations are monotone, swap, discontinuous (Tillman, 2004; Kumar and Byrne, 2004), see Figure 1. • Phrase pairs. Phrase extraction heuristics (Venugopal et al., 2003; Tillmann, 2003; Och and Ney, 2004) produce a set of phrase pairs (e, f ) that are consistent with the word alignments. In this paper we assume that the phrase pairs are given (without any scores), and we induce every other parameter of the phrase-based model from monolingual data. • Phrase translation probabilities. Each phrase pair has a list of associated feature functions (FFs). These include phrase translation probabilities, φ(e|f ) and φ(f |e), which are typically calculated via maximum likelihood estimation. • Lexical weighting. Since MLE overestimates φ for phrase pairs with sparse co"
E12-1014,C98-1066,0,\N,Missing
E12-1014,H05-1021,0,\N,Missing
E12-1014,P07-2045,1,\N,Missing
E12-1014,D08-1076,0,\N,Missing
H01-1035,J95-4004,0,0.0182402,"Missing"
H01-1035,J90-2002,0,0.181864,"Missing"
H01-1035,W99-0612,1,0.296604,"Missing"
H01-1035,C94-2178,0,0.0237077,"Missing"
H01-1035,1994.amta-1.11,0,0.0430671,"Missing"
H01-1035,jones-havrilla-1998-twisted,0,0.0331842,"Missing"
H01-1035,J99-1003,0,0.00579011,"Missing"
H01-1035,N01-1006,1,0.253502,"Missing"
H01-1035,P00-1056,0,0.0545033,"Missing"
H01-1035,P95-1033,0,0.0215018,"Missing"
H01-1035,J97-3002,0,0.0239965,"Missing"
H01-1035,N01-1026,1,0.449195,"Missing"
H01-1035,P00-1027,1,0.236105,"Missing"
H92-1045,P91-1034,0,0.0356017,"rce handtagged materials for use in testing and training. We have achieved considerable progress recently by using a new source of testing and training materials and the application of Bayesian discrimination methods. Rather than depending on small amounts of hand-tagged text, we have been making use of relatively large amounts of parallel text, text such as the Canadian Hansards, which are available in multiple languages. The translation can 233 often be used in lieu of hand-labeling. For example, consider the polysemous word sentence, which has two major senses: (1) a judicial sentence, and (2), a syntactic sentence. We can collect a number of sense (1) examples by extracting instances that are translated as peine, and we can collect a number of sense (2) examples by extracting instances that are translated as phrase. In this way, we have been able to acquire a considerable amount of testing and training material for developing and testing our disambiguation algorithms. The use of bilingual materials for discrimination decisions in machine tranlation has been discussed by Brown and others (1991), and by Dagan, Itai, and Schwall (1991). The use of bilingual materials for an essential"
H92-1045,P91-1017,0,0.00816638,"Missing"
H92-1045,C92-2070,1,\N,Missing
H93-1052,P91-1034,0,0.309011,"Missing"
H93-1052,H92-1045,1,0.800559,"Missing"
H93-1052,H93-1051,0,0.33804,"Missing"
H93-1052,P92-1053,0,0.0285116,"Missing"
H93-1052,C92-2070,1,0.683954,"Missing"
I08-1053,W04-0404,0,0.0147378,"important phase and this is where our work differs significantly from the previous literature. Most of the previous work has been focused on generating compositional translation candidates, that is, the translation candidates of the compound words are lexically composed of the component word translations. This has been done by either just concatenating the translations of component words to form a candidate (Grefenstette, 1999; Cao and Li, 2002), or using syntactic templates such as “E2 in E1 ”, “E1 of E2 ” to form translation candidates from the translation of the component words E2 and E1 (Baldwin and Tanaka, 2004), or using synsets of the component word translations to include synonyms in the compositional candidates (Navigli et al., 2003). The above class of work in compositional-candidate generation fails to translate compounds such as Krankenhaus (hospital) whose component word translations are Kranken (sick) and Haus (hospital), and composing sick and house in any order will not result in the correct translation (hospital). Another problem with using fixed syntactic templates is that they are restricted to the specific patterns occurring in the target language. We show how one can use the gloss pat"
I08-1053,2002.tmi-papers.3,0,0.0576015,"ion is illustrated in Figure 1. Splitting compound words and gloss generation with translation lexicon lookup We first split a given source word, such as the Albanian compound hekurudh¨ e, into a set of component word partitions, such as hekur (iron) and udh¨ e (path). Our initial approach is to consider all possible partitions based on contiguous component words found in a small dictionary for the language, as in zog railroad railway 0.19 0.14 rail 0.05 4.1 Figure 1: Illustration of using cross-language evidence using bilingual dictionaries of different languages for compound translation 404 Brown (2002) and Koehn and Knight (2003)1 . For a given split, we generate its English glosses by using all possible English translations of the component words given in the dictionary of that language2 . 4.2 Using cross-language evidence from different bilingual dictionaries For many compound words (especially for borrowings), the compounding process is identical across several languages and the literal English gloss remains the same across these languages. For example, the English word railway is translated as a compound word in many languages, and the English gloss of those compounds is often “iron pat"
I08-1053,C02-1011,0,0.0902328,"icon lookup and allowing for different splitting options based on corpus frequency (Zhang et al., 2000; Koehn and Knight, 2003). Translation candidate generation is an important phase and this is where our work differs significantly from the previous literature. Most of the previous work has been focused on generating compositional translation candidates, that is, the translation candidates of the compound words are lexically composed of the component word translations. This has been done by either just concatenating the translations of component words to form a candidate (Grefenstette, 1999; Cao and Li, 2002), or using syntactic templates such as “E2 in E1 ”, “E1 of E2 ” to form translation candidates from the translation of the component words E2 and E1 (Baldwin and Tanaka, 2004), or using synsets of the component word translations to include synonyms in the compositional candidates (Navigli et al., 2003). The above class of work in compositional-candidate generation fails to translate compounds such as Krankenhaus (hospital) whose component word translations are Kranken (sick) and Haus (hospital), and composing sick and house in any order will not result in the correct translation (hospital). An"
I08-1053,1999.tc-1.8,0,0.243657,"sing translation lexicon lookup and allowing for different splitting options based on corpus frequency (Zhang et al., 2000; Koehn and Knight, 2003). Translation candidate generation is an important phase and this is where our work differs significantly from the previous literature. Most of the previous work has been focused on generating compositional translation candidates, that is, the translation candidates of the compound words are lexically composed of the component word translations. This has been done by either just concatenating the translations of component words to form a candidate (Grefenstette, 1999; Cao and Li, 2002), or using syntactic templates such as “E2 in E1 ”, “E1 of E2 ” to form translation candidates from the translation of the component words E2 and E1 (Baldwin and Tanaka, 2004), or using synsets of the component word translations to include synonyms in the compositional candidates (Navigli et al., 2003). The above class of work in compositional-candidate generation fails to translate compounds such as Krankenhaus (hospital) whose component word translations are Kranken (sick) and Haus (hospital), and composing sick and house in any order will not result in the correct transla"
I08-1053,E03-1076,0,0.349942,"r 50 languages that were acquired in electronic form over the Internet or via optical character recognition (OCR) on paper dictionaries. Note that no parallel or even monolingual corpora is required, their use described later in the paper is optional. 403 3 Related Work The compound-translation literature typically deals with these steps: 1) Compound splitting, 2) translation candidate generation and 3) translation candidate scoring. Compound splitting is generally done using translation lexicon lookup and allowing for different splitting options based on corpus frequency (Zhang et al., 2000; Koehn and Knight, 2003). Translation candidate generation is an important phase and this is where our work differs significantly from the previous literature. Most of the previous work has been focused on generating compositional translation candidates, that is, the translation candidates of the compound words are lexically composed of the component word translations. This has been done by either just concatenating the translations of component words to form a candidate (Grefenstette, 1999; Cao and Li, 2002), or using syntactic templates such as “E2 in E1 ”, “E1 of E2 ” to form translation candidates from the transl"
I08-1053,P07-2045,0,0.00654243,"Missing"
I08-1053,2005.mtsummit-papers.11,0,0.0164458,"Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. Once in the same language space, source words and their translation hypotheses are compared via cosine similarity using their surrounding context vectors. We performed this experiment for German and Swedish and report average accuracies with and without this addition in Table 7. For monolingual corpora, we used the German and Swedish side of the Europarl corpus (Koehn, 2005) consisting of approximately 15 million and 21 million words respectively. We were able to project context vectors for an average of 4224.5 words in the two languages among all the possible compound words detected in Section 6.4. The poor Eurpoarl coverage could be due to the fact that compound words are generally technical words with low Europarl corpus frequency, especially in parliamentary proceedings. We believe that the small performance gains here are due to these limitations of the monolingual corpora. Method Original ranking Comb. with Context Sim Top1avg 0.196 0.201 Top10avg 0.388 0.3"
I08-1053,C92-4201,0,0.559502,"s along with context similarity from monolingual text and optional combination with traditional bilingual-textbased translation discovery. 1 Goal: To translate new Albanian compounds Hekurudh¨ e Hekur-Udh¨ e iron-path ??? Table 1: Example lexical resources used in this task and their application to translating compound words in new languages. Introduction Compound words such as lighthouse and fireplace are words that are composed of two or more component words and are often a challenge for machine translation due to their potentially complex compounding behavior and ambiguous interpretations (Rackow et al., 1992). For many languages, such words form a significant portion of the lexicon and the compounding process is further complicated by diverse morphological processes (Levi, 1978) and the properties of different compound sequences such as Noun-Noun, Adj-Adj, Adj-Noun, Verb-Verb, etc. Compounds also tend to have a high type frequency but a low token frequency which makes their translation difficult to learn using corpus-based algorithms (Tanaka and Baldwin, 2003). Furthermore, most of the literature on compound translation has been restricted to a few languages dealing with compounding phenomena spec"
I08-1053,P99-1067,0,0.0614234,"Missing"
I08-1053,W02-2026,1,0.924605,"Missing"
I08-1053,P04-3007,1,0.894348,"Missing"
I08-1053,W03-1803,0,0.0691472,"ent words and are often a challenge for machine translation due to their potentially complex compounding behavior and ambiguous interpretations (Rackow et al., 1992). For many languages, such words form a significant portion of the lexicon and the compounding process is further complicated by diverse morphological processes (Levi, 1978) and the properties of different compound sequences such as Noun-Noun, Adj-Adj, Adj-Noun, Verb-Verb, etc. Compounds also tend to have a high type frequency but a low token frequency which makes their translation difficult to learn using corpus-based algorithms (Tanaka and Baldwin, 2003). Furthermore, most of the literature on compound translation has been restricted to a few languages dealing with compounding phenomena specific to the language in question. With these challenges in mind, the primary goal of this work is to improve the coverage of translation lexicons for compounds, as illustrated in Table 1 and Figure 1, in multiple new languages. We show how using cross-language compound evidence obtained from bilingual dictionaries can aid in compound translation. A primary motivating idea for this work is that the literal component glosses for compound words (such as “iron"
I08-1053,W00-1219,0,0.0160385,"onary collections for 50 languages that were acquired in electronic form over the Internet or via optical character recognition (OCR) on paper dictionaries. Note that no parallel or even monolingual corpora is required, their use described later in the paper is optional. 403 3 Related Work The compound-translation literature typically deals with these steps: 1) Compound splitting, 2) translation candidate generation and 3) translation candidate scoring. Compound splitting is generally done using translation lexicon lookup and allowing for different splitting options based on corpus frequency (Zhang et al., 2000; Koehn and Knight, 2003). Translation candidate generation is an important phase and this is where our work differs significantly from the previous literature. Most of the previous work has been focused on generating compositional translation candidates, that is, the translation candidates of the compound words are lexically composed of the component word translations. This has been done by either just concatenating the translations of component words to form a candidate (Grefenstette, 1999; Cao and Li, 2002), or using syntactic templates such as “E2 in E1 ”, “E1 of E2 ” to form translation c"
I08-1061,H05-1071,0,0.0290616,"Missing"
I08-1061,P99-1016,0,0.101473,"Missing"
I08-1061,P06-2007,0,0.0218911,"Missing"
I08-1061,N03-1011,0,0.0203198,"g. companies, locations, time, person-names, etc.) based on bootstrapping from a small set of seed words (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Ravichandran and Hovy, 2002; Hasegawa et al. 2004; Etzioni et al. 2005; Pas¸ca et al. 2006). This framework has been also shown to work for extracting semantic relations between entities: Pantel et al. (2004) proposed an approach based on editpattern “X and Y” is a generic pattern whereas the pattern “Y such as X” is a hyponym-specific pattern distance to learn lexico-POS patterns for is-a and part-of relations. Girju et al. (2003) used 100 seed words from WordNet to extract patterns for part-of relations. While most of the above pattern induction work has been shown to work well for specific relations (such as “birthdates, companies, etc.”), Section 3.1 explains why directly applying seed learning for semantic relations can result in high recall but low precision patterns, a problem also noted by Pantel and Pennacchiotti (2006). Furthermore, much of the semantic relation extraction work has focused on extracting a particular relation independently of other relations. We show how this problem can be solved by combining"
I08-1061,J06-1005,0,0.0259822,"Missing"
I08-1061,P04-1053,0,0.0193709,", “Y such as X”, as in the classic work by Hearst (1992). The problems with using a few fixed patterns is the often low coverage of such patterns; thus there is a need for discovering additional informative patterns automatically. There has been a plethora of work in the area of information extraction using automatically derived patterns contextual patterns for semantic categories (e.g. companies, locations, time, person-names, etc.) based on bootstrapping from a small set of seed words (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Ravichandran and Hovy, 2002; Hasegawa et al. 2004; Etzioni et al. 2005; Pas¸ca et al. 2006). This framework has been also shown to work for extracting semantic relations between entities: Pantel et al. (2004) proposed an approach based on editpattern “X and Y” is a generic pattern whereas the pattern “Y such as X” is a hyponym-specific pattern distance to learn lexico-POS patterns for is-a and part-of relations. Girju et al. (2003) used 100 seed words from WordNet to extract patterns for part-of relations. While most of the above pattern induction work has been shown to work well for specific relations (such as “birthdates, companies, etc.”)"
I08-1061,C92-2082,0,0.126825,"lities of being translations of each other if their hypernyms or hyponyms are translations of one another. 2 Related Work While manually created WordNets for English (Fellbaum, 1998) and Hindi (Narayan, 2002) have been made available, a lot of time and effort is required in building such semantic taxonomies from scratch. Hence several automatic corpus based approaches for acquiring lexical knowledge have been proposed in the literature. Much of this work has been done for English based on using a few evocative fixed patterns including “X and other Ys”, “Y such as X”, as in the classic work by Hearst (1992). The problems with using a few fixed patterns is the often low coverage of such patterns; thus there is a need for discovering additional informative patterns automatically. There has been a plethora of work in the area of information extraction using automatically derived patterns contextual patterns for semantic categories (e.g. companies, locations, time, person-names, etc.) based on bootstrapping from a small set of seed words (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Ravichandran and Hovy, 2002; Hasegawa et al. 2004; Etzioni et al. 2005; Pas¸ca et al."
I08-1061,P06-1102,0,0.0252877,"Missing"
I08-1061,P06-1015,0,0.0196141,"proposed an approach based on editpattern “X and Y” is a generic pattern whereas the pattern “Y such as X” is a hyponym-specific pattern distance to learn lexico-POS patterns for is-a and part-of relations. Girju et al. (2003) used 100 seed words from WordNet to extract patterns for part-of relations. While most of the above pattern induction work has been shown to work well for specific relations (such as “birthdates, companies, etc.”), Section 3.1 explains why directly applying seed learning for semantic relations can result in high recall but low precision patterns, a problem also noted by Pantel and Pennacchiotti (2006). Furthermore, much of the semantic relation extraction work has focused on extracting a particular relation independently of other relations. We show how this problem can be solved by combining evidence from multiple relations in Section 3.2. Snow et al.(2006) also describe a probablistic framework for combining evidence using constraints from hyponymy and cousin relations. However, they use a supervised logistic regression model. Moreover, their features rely on parsing dependency trees which may not be available for most languages. The key contribution of this work is using evidence from mu"
I08-1061,N04-1041,0,0.046848,"Missing"
I08-1061,P02-1006,0,0.0346614,"ns including “X and other Ys”, “Y such as X”, as in the classic work by Hearst (1992). The problems with using a few fixed patterns is the often low coverage of such patterns; thus there is a need for discovering additional informative patterns automatically. There has been a plethora of work in the area of information extraction using automatically derived patterns contextual patterns for semantic categories (e.g. companies, locations, time, person-names, etc.) based on bootstrapping from a small set of seed words (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Ravichandran and Hovy, 2002; Hasegawa et al. 2004; Etzioni et al. 2005; Pas¸ca et al. 2006). This framework has been also shown to work for extracting semantic relations between entities: Pantel et al. (2004) proposed an approach based on editpattern “X and Y” is a generic pattern whereas the pattern “Y such as X” is a hyponym-specific pattern distance to learn lexico-POS patterns for is-a and part-of relations. Girju et al. (2003) used 100 seed words from WordNet to extract patterns for part-of relations. While most of the above pattern induction work has been shown to work well for specific relations (such as “birthda"
I08-1061,W97-0313,0,0.0666946,"Missing"
I08-1061,W02-2026,1,0.589724,"s interesting to see that the incorrect translations seem to be the words that are very general (like “topic”, “stuff”, etc.) and hence their hyponym space is very large and diffuse, resulting in incorrect translations.While the columns 1 and 2 of Table 9 show the standalone application of our translation dictionary induction method, we can also combine our model with existing work on dictionary induction using other translation induction measures such as using relative frequency similarity in multilingual corpora and using cross-language context similarity between word co-occurrence vectors (Schafer and Yarowsky, 2002).We implemented the above dictionary induction measures and combined the taxonomy based dictionary induction model with other measures by just summing the two scores13 . The preliminary results for bidirectional hypernym/hyponym + other features are shown in column 3 of Table 9. The results show that the hypernym/hyponym features can be a useful orthogonal source of lexical similarity in the translation-induction model space. While the model shown in Figure 2 proposes inducing translations of hypernyms, one can also go in the other direction and induce likely translation candidates for hyponym"
I08-1061,P06-1101,0,0.0579702,"Missing"
I08-1061,W02-1028,0,0.0286781,"ew evocative fixed patterns including “X and other Ys”, “Y such as X”, as in the classic work by Hearst (1992). The problems with using a few fixed patterns is the often low coverage of such patterns; thus there is a need for discovering additional informative patterns automatically. There has been a plethora of work in the area of information extraction using automatically derived patterns contextual patterns for semantic categories (e.g. companies, locations, time, person-names, etc.) based on bootstrapping from a small set of seed words (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Thelen and Riloff, 2002; Ravichandran and Hovy, 2002; Hasegawa et al. 2004; Etzioni et al. 2005; Pas¸ca et al. 2006). This framework has been also shown to work for extracting semantic relations between entities: Pantel et al. (2004) proposed an approach based on editpattern “X and Y” is a generic pattern whereas the pattern “Y such as X” is a hyponym-specific pattern distance to learn lexico-POS patterns for is-a and part-of relations. Girju et al. (2003) used 100 seed words from WordNet to extract patterns for part-of relations. While most of the above pattern induction work has been shown to work well for specifi"
I08-1061,N03-1036,0,0.0240118,"Missing"
I17-2076,P15-2044,0,0.143132,"Missing"
I17-2076,S12-1051,0,0.0411913,"portions, which are computed from the consensus. The domain specific paraphrases demonstrate the linguistic variation across the Bible, which can be further analyzed. Fig. 5 explores some of the variations that occur in our specific domain. 4.2 5 Consensus distributions We can compute both the majority values (Fig. 3) and the entire distributions (Fig. 6) of specific features such as POS tags and head words. Aggregating each corpus independently before alignment, Discussion and Related Work While monolingual insights like paraphrases have potential applications in semantic textual similarity (Agirre et al., 2012), there exist bigger corpora for those tasks, such as PPDB (Ganitkevitch et al., 2013). However, as the Bible is often the only significant parallel text for many of the world’s languages, improved 27-way consensus English 451 at the cost of quality – to align and generate the subsequent resources. POS tags Before corpus alignments: TIME: NN (1.00) SECRET : NN (0.54), JJ (0.46) With corpus alignments: TIME: NN (0.94), NNS (0.05) . . . SECRET : JJ (0.51), NN (0.47), NNS (0.01) . . . 6 Head words Before corpus alignments: TIME: at (0.23), for (0.09), in (0.07), is (0.06) . . . SECRET : in (0.28)"
I17-2076,N03-1003,0,0.136993,"these features in the English Biblical multi-parallel corpora. We produce multiway word alignments, complete dependency parses, and POS tag annotations for the English Bible. While our resources and choice of features are catered for our specific domain, the method can be applied more broadly for aligning and establishing consensus in any domain. Introduction Noisy or heterogeneous copies of the same text are prevalent in religious and literary texts (Resnik et al., 1999; Koppel et al., 2016), machine translation n-best lists (Kumar and Byrne, 2004; Papineni et al., 2002), comparable corpora (Barzilay and Lee, 2003), and social media (Xu et al., 2015). While copies can be analyzed independently or together in a pairwise manner, information can be lost by not using them all jointly. We view these copies of text as multi-parallel corpora, which consist of multiple sets of comparable or partially aligned documents. This contrasts with parallel corpora, which are usually between only two. The goal of this work is to produce word alignments for multi-parallel corpora (Fig. 1). We approach this problem by tying the multiparallel corpora together using features such as The English Bible is a literary religious"
I17-2076,W02-1001,0,0.0698268,"to the first iteration: Entire text (#); Old Testament (); New Testament (4); per iteration (+). Since the feature weights are fixed, the absolute score is not meaningful. Dataset and tools The corpus of Bibles were collected by Mayer and Cysouw (2014) and contains 27 English versions. 23 contain just the New Testament (~8K verses, ~200K words), while four also include the Old Testament (~31K verses, ~900K words). We use fast_align (Dyer et al., 2013), a greedy, transition-based dependency parser (Honnibal and Johnson, 2015), and an averaged perceptron POS tagger with Brown cluster features (Collins, 2002; Koo et al., 2008) for feature computation.5 3 Analysis of alignments Fig. 3 shows an example of an alignment produced by our system.6 There are a few misalignments due to both literary divergence and our design choices, such as the sum in Equation 1 and targeting one-toone alignments. The visualization of the matchings simplifies analysis of literary variation. Relations with only a few or different members are easy to spot. These anomalies can be indicators of different choices in translation, unclean source text, or use of older language. For example, unlike “wise men&quot; and “magi,&quot; “astrolo"
I17-2076,N13-1073,0,0.0316542,"ML,t = A LIGN D OCUMENTS(L) Output: {argmaxML,t F (ML,t ) : L ∈ C} 2.3 1.00 1 2 3 4 5 6 7 8 9 10 Iteration Figure 2: Relative change in total score with respect to the first iteration: Entire text (#); Old Testament (); New Testament (4); per iteration (+). Since the feature weights are fixed, the absolute score is not meaningful. Dataset and tools The corpus of Bibles were collected by Mayer and Cysouw (2014) and contains 27 English versions. 23 contain just the New Testament (~8K verses, ~200K words), while four also include the Old Testament (~31K verses, ~900K words). We use fast_align (Dyer et al., 2013), a greedy, transition-based dependency parser (Honnibal and Johnson, 2015), and an averaged perceptron POS tagger with Brown cluster features (Collins, 2002; Koo et al., 2008) for feature computation.5 3 Analysis of alignments Fig. 3 shows an example of an alignment produced by our system.6 There are a few misalignments due to both literary divergence and our design choices, such as the sum in Equation 1 and targeting one-toone alignments. The visualization of the matchings simplifies analysis of literary variation. Relations with only a few or different members are easy to spot. These anomal"
I17-2076,N13-1092,0,0.101065,"Missing"
I17-2076,D15-1162,0,0.0138904,"} 2.3 1.00 1 2 3 4 5 6 7 8 9 10 Iteration Figure 2: Relative change in total score with respect to the first iteration: Entire text (#); Old Testament (); New Testament (4); per iteration (+). Since the feature weights are fixed, the absolute score is not meaningful. Dataset and tools The corpus of Bibles were collected by Mayer and Cysouw (2014) and contains 27 English versions. 23 contain just the New Testament (~8K verses, ~200K words), while four also include the Old Testament (~31K verses, ~900K words). We use fast_align (Dyer et al., 2013), a greedy, transition-based dependency parser (Honnibal and Johnson, 2015), and an averaged perceptron POS tagger with Brown cluster features (Collins, 2002; Koo et al., 2008) for feature computation.5 3 Analysis of alignments Fig. 3 shows an example of an alignment produced by our system.6 There are a few misalignments due to both literary divergence and our design choices, such as the sum in Equation 1 and targeting one-toone alignments. The visualization of the matchings simplifies analysis of literary variation. Relations with only a few or different members are easy to spot. These anomalies can be indicators of different choices in translation, unclean source t"
I17-2076,P16-2091,0,0.037602,"Missing"
I17-2076,P08-1068,0,0.0141626,"eration: Entire text (#); Old Testament (); New Testament (4); per iteration (+). Since the feature weights are fixed, the absolute score is not meaningful. Dataset and tools The corpus of Bibles were collected by Mayer and Cysouw (2014) and contains 27 English versions. 23 contain just the New Testament (~8K verses, ~200K words), while four also include the Old Testament (~31K verses, ~900K words). We use fast_align (Dyer et al., 2013), a greedy, transition-based dependency parser (Honnibal and Johnson, 2015), and an averaged perceptron POS tagger with Brown cluster features (Collins, 2002; Koo et al., 2008) for feature computation.5 3 Analysis of alignments Fig. 3 shows an example of an alignment produced by our system.6 There are a few misalignments due to both literary divergence and our design choices, such as the sum in Equation 1 and targeting one-toone alignments. The visualization of the matchings simplifies analysis of literary variation. Relations with only a few or different members are easy to spot. These anomalies can be indicators of different choices in translation, unclean source text, or use of older language. For example, unlike “wise men&quot; and “magi,&quot; “astrologers&quot; only appears"
I17-2076,W16-0205,0,0.0219552,". pairwise word alignments, dependency parses, and POS tags. Our method jointly learns word alignments and annotations for these features in the English Biblical multi-parallel corpora. We produce multiway word alignments, complete dependency parses, and POS tag annotations for the English Bible. While our resources and choice of features are catered for our specific domain, the method can be applied more broadly for aligning and establishing consensus in any domain. Introduction Noisy or heterogeneous copies of the same text are prevalent in religious and literary texts (Resnik et al., 1999; Koppel et al., 2016), machine translation n-best lists (Kumar and Byrne, 2004; Papineni et al., 2002), comparable corpora (Barzilay and Lee, 2003), and social media (Xu et al., 2015). While copies can be analyzed independently or together in a pairwise manner, information can be lost by not using them all jointly. We view these copies of text as multi-parallel corpora, which consist of multiple sets of comparable or partially aligned documents. This contrasts with parallel corpora, which are usually between only two. The goal of this work is to produce word alignments for multi-parallel corpora (Fig. 1). We appro"
I17-2076,N04-1022,0,0.0337222,"ags. Our method jointly learns word alignments and annotations for these features in the English Biblical multi-parallel corpora. We produce multiway word alignments, complete dependency parses, and POS tag annotations for the English Bible. While our resources and choice of features are catered for our specific domain, the method can be applied more broadly for aligning and establishing consensus in any domain. Introduction Noisy or heterogeneous copies of the same text are prevalent in religious and literary texts (Resnik et al., 1999; Koppel et al., 2016), machine translation n-best lists (Kumar and Byrne, 2004; Papineni et al., 2002), comparable corpora (Barzilay and Lee, 2003), and social media (Xu et al., 2015). While copies can be analyzed independently or together in a pairwise manner, information can be lost by not using them all jointly. We view these copies of text as multi-parallel corpora, which consist of multiple sets of comparable or partially aligned documents. This contrasts with parallel corpora, which are usually between only two. The goal of this work is to produce word alignments for multi-parallel corpora (Fig. 1). We approach this problem by tying the multiparallel corpora toget"
I17-2076,P07-1060,0,0.0218311,"w these copies of text as multi-parallel corpora, which consist of multiple sets of comparable or partially aligned documents. This contrasts with parallel corpora, which are usually between only two. The goal of this work is to produce word alignments for multi-parallel corpora (Fig. 1). We approach this problem by tying the multiparallel corpora together using features such as The English Bible is a literary religious text with multiple authors, disputed authorship structure, and multiple revisions for language modernization.1 While there is existing computational work in Biblical analysis (Lee, 2007), our contribution of automatically generated consensus annotations for all verses allows future research to efficiently investigate across all English Bibles. As the Bible is available in electronic form in over 800 of the world’s languages (Mayer and Cysouw, 2014), the Bible may be the only parallel corpus for low-resource languages, and our in-domain resources can be a valuable reference.2 1 The unresolved Synoptic Problem questions the order and dependencies of the the Synoptic Gospels. 2 Available at github.com/pitrack/monolign. 448 Proceedings of the The 8th International Joint Conferenc"
I17-2076,mayer-cysouw-2014-creating,0,0.423558,"for multi-parallel corpora (Fig. 1). We approach this problem by tying the multiparallel corpora together using features such as The English Bible is a literary religious text with multiple authors, disputed authorship structure, and multiple revisions for language modernization.1 While there is existing computational work in Biblical analysis (Lee, 2007), our contribution of automatically generated consensus annotations for all verses allows future research to efficiently investigate across all English Bibles. As the Bible is available in electronic form in over 800 of the world’s languages (Mayer and Cysouw, 2014), the Bible may be the only parallel corpus for low-resource languages, and our in-domain resources can be a valuable reference.2 1 The unresolved Synoptic Problem questions the order and dependencies of the the Synoptic Gospels. 2 Available at github.com/pitrack/monolign. 448 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 448–453, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 Feature Method IDENTITY PAIRWISE LEMMA POS PARENT NEIGHBORS The consensus consists of aligned tokens between the corpora. Assuming each corpus consists of"
I17-2076,D16-1190,0,0.0128503,"g the scoring function to use entirely language-independent features (e.g. pairwise alignments), our algorithm still maximizes the score of the matching by relearning an improved dictionary between iterations. The corpus alignment may also be desirable over separate alignments for multi-source projection tasks in noisier data because a word or phrase may only align with only a subset of the sources. By generating resources specifically for the Bible, we hope to foster future computational methods for studying religious texts. Current Biblical visualization (Zhang et al., 2016) and authorship (Moritz et al., 2016) works use a small subset of the translations to perform their analysis. Our resources would encourage analysis across all versions of the Bible, which would be less biased than picking a small set. By weighing the votes cast by each token in a relation, it is even possible to emphasize a specific corpus. The algorithms described in Section 2 can be applied to any parallel corpora. The scoring function is simple and accommodates arbitrary features. While our approach specifically assumes the documents (verses) within the corpora are already aligned, knowing which documents are similar (e.g. th"
I17-2076,P02-1040,0,0.0977687,"learns word alignments and annotations for these features in the English Biblical multi-parallel corpora. We produce multiway word alignments, complete dependency parses, and POS tag annotations for the English Bible. While our resources and choice of features are catered for our specific domain, the method can be applied more broadly for aligning and establishing consensus in any domain. Introduction Noisy or heterogeneous copies of the same text are prevalent in religious and literary texts (Resnik et al., 1999; Koppel et al., 2016), machine translation n-best lists (Kumar and Byrne, 2004; Papineni et al., 2002), comparable corpora (Barzilay and Lee, 2003), and social media (Xu et al., 2015). While copies can be analyzed independently or together in a pairwise manner, information can be lost by not using them all jointly. We view these copies of text as multi-parallel corpora, which consist of multiple sets of comparable or partially aligned documents. This contrasts with parallel corpora, which are usually between only two. The goal of this work is to produce word alignments for multi-parallel corpora (Fig. 1). We approach this problem by tying the multiparallel corpora together using features such"
I17-2076,S15-2001,0,0.0482833,"Missing"
I17-2076,H01-1035,1,0.433962,"ignments: TIME: at (0.23), for (0.09), in (0.07), is (0.06) . . . SECRET : in (0.28), is (0.06) kept (0.04) places (0.04) . . . With corpus alignments: TIME: at (0.17), in (0.09), for (0.09), is (0.05) . . . SECRET : in (0.32), place (0.05), mystery (0.04), is (0.04) . . . Figure 6: A comparison of the POS tags (above) and head words (below) distributions for time and secret with and without the consensus alignment. resources created here have value for annotation projection to low-resource languages. The Bible has been productively used as a key resource for cross-lingual knowledge transfer (Yarowsky et al., 2001; Agi´c et al., 2015). Specifically, Johannsen et al. (2016) suggests a method for projecting POS tags and dependency parses onto a target language. Our approach can be modified in a similar way. By restricting the scoring function to use entirely language-independent features (e.g. pairwise alignments), our algorithm still maximizes the score of the matching by relearning an improved dictionary between iterations. The corpus alignment may also be desirable over separate alignments for multi-source projection tasks in noisier data because a word or phrase may only align with only a subset of t"
K17-2001,E14-1060,1,0.855884,"Missing"
K17-2001,P17-1136,0,0.0562959,"Missing"
K17-2001,N15-1107,1,0.861491,"Missing"
K17-2001,chrupala-etal-2008-learning,0,0.152497,"Missing"
K17-2001,W16-2004,0,0.0654088,"Missing"
K17-2001,P14-2102,1,0.910119,"Missing"
K17-2001,Q15-1031,1,0.919531,"Missing"
K17-2001,P16-1156,1,0.87299,"Missing"
K17-2001,K17-2002,0,0.132442,"Missing"
K17-2001,E17-2120,1,0.859022,"Missing"
K17-2001,E17-1049,1,0.894341,"Missing"
K17-2001,N07-1048,0,0.21796,"Missing"
K17-2001,P17-1182,1,0.876831,"Missing"
K17-2001,D09-1011,1,0.888474,"Missing"
K17-2001,D08-1113,1,0.874358,"Missing"
K17-2001,P16-2090,0,0.46072,"Missing"
K17-2001,N13-1138,0,0.147728,"Missing"
K17-2001,P08-1115,0,0.089787,"Missing"
K17-2001,L16-1498,1,0.792213,"Missing"
K17-2001,N16-1077,1,0.812854,"Missing"
K17-2001,W10-2211,0,0.123726,"Missing"
K17-2001,W16-2006,0,0.0673676,"Missing"
K17-2001,P82-1020,0,0.75423,"Missing"
K17-2001,P08-1103,0,0.0541683,"Missing"
K17-2001,D15-1272,1,0.92636,"Missing"
K17-2001,D14-1095,0,0.0938069,"Missing"
K17-2001,K17-2011,0,0.0350181,"Missing"
K17-2001,N15-1093,0,0.19098,"Missing"
K17-2001,K17-2008,0,0.055504,"Missing"
K17-2001,K17-2010,0,0.158704,"Missing"
K17-2001,W16-2007,0,\N,Missing
K17-2001,L16-1497,1,\N,Missing
K17-2001,K17-2012,0,\N,Missing
K17-2001,K17-2003,0,\N,Missing
K17-2001,P17-1029,0,\N,Missing
K18-3001,K18-3001,1,0.103672,"Missing"
K18-3001,P16-2090,1,0.838493,"Missing"
K18-3001,K17-2003,1,0.836665,"Missing"
K18-3001,K17-2010,1,0.734971,"Missing"
K18-3001,W18-6011,1,0.913422,"and a target UniMorph sentence is shown in Figure 3. Since the selection of languages in task 2 is small and we do not attempt to correct annotation errors in the UD source materials, conversion between UD and UniMorph morphosyntactic descriptions is generally straightforward.11 However, UD descriptions are more fine-grained than their UniMorph equivalents. For example, UD denotes lexical features such as noun gender which are inherent features of a lexeme possessed by all of its word forms. Such inherent features are missing from UniMorph which exclusively annotates inflectional morphology (McCarthy et al., 2018). Therefore, UD fea$ → sta$ ti$ → dista$ koti$ → kodista$ i$ → ista$ oti$ → odista$ Such rules are then extracted from each example inflection in the training data. At generation time, the longest matching left hand side of a rule is identified and applied to the citation form. For example, if the Finnish noun luoti ‘bullet’ were to be inflected in the elative (N;IN+ABL;SG) using only the extracted rules given above, the transformation oti$ → odista$ would be triggered, producing the output luodista. In case there are multiple candidate rules of equally long left hand sides that all match, tie"
K18-3001,K18-3012,0,0.30177,"Missing"
K18-3001,K18-3015,0,0.276525,"Missing"
K18-3001,P15-2111,1,\N,Missing
K18-3001,K17-2002,1,\N,Missing
K18-3001,K17-3001,0,\N,Missing
K18-3001,W17-4110,1,\N,Missing
K18-3001,N18-2087,1,\N,Missing
K18-3001,P18-1245,1,\N,Missing
K18-3001,L18-1293,1,\N,Missing
K18-3001,K18-3004,0,\N,Missing
K18-3001,K18-3010,0,\N,Missing
K18-3001,K18-3013,0,\N,Missing
K18-3001,K18-3003,0,\N,Missing
K18-3001,K18-3016,0,\N,Missing
K18-3001,K18-3005,0,\N,Missing
K18-3001,W16-2006,0,\N,Missing
K18-3001,K17-2008,1,\N,Missing
K18-3001,K17-2005,0,\N,Missing
K18-3001,K18-3008,0,\N,Missing
K18-3001,K18-3007,0,\N,Missing
L16-1497,kamholz-etal-2014-panlex,0,0.0278937,"d from a descriptive resource, but this entails additional effort for the linguist. An alternate strategy is to choose lemmas automatically and gather data in sufficient quantities to increase the likelihood that examples of most surface variation will be collected. To do this, two problems must be solved: 1. Choosing lemmas in a way that is likely to maximize variation, and 2. constructing glosses that are intelligible to untrained speakers. Lemmas which maximize lexical variation can be chosen using information on lexical properties available in the entries of lexical resources like PanLex (Kamholz et al., 2014). Given a set of lemmas whose paradigms we would like to elicit and an initial template designed for professional translators, we can automatically generate new prompts for each new lemma to display to untrained speakers. First, we excise the inflected lemmas from the existing prompts, replacing them with a placeholder tag indicating inflection (e.g. ‘I 6 Additional detailed documentation on the UniMorph Schema will be made available at the temporary site for the UniMorph Project: http://ckirov.github.io/UniMorph/ 3117 Figure 1: Excerpt of a blank elicitation template for Sinhalese verbs as pr"
L16-1497,N15-1093,0,0.0560199,"n intransitive gloss template (e.g. ‘I am VBG it’). forms using string edit distance to determine the extent to which they diverge. Divergence can then be penalized, with thresholds established for acceptance, human inspection, and rejection. Speaker output can be compared to humanproduced, gold standard data from reference grammars, professional translators, or a resource such as Wiktionary. It may also be compared to the output predicted by stateof-the-art paradigm completion software that is known to achieve high accuracy, such as Durrett and DeNero (2013), Ahlberg et al. (2014, 2015), and Nicolai et al. (2015). The elicited output of untrained speakers can be scored to determine its accuracy. Scoring consists of comparing the elicited output to either verified or predicted wordThe current corpus of inflectional paradigms gathered via elicitation from professional translators includes data from 15 languages, and is described in detail in Table 1. 3118 Language Akan (Twi) Amharic Azeri Dari Farsi Igbo Indonesian Punjabi (Indian) Sinhala Tagalog Tajik Tigrinya Turkish Uzbek Yoruba TOTAL : (L) (F) L/F L F L F L F L F L F L F L F L F L F L F L F L F L F L F L F 792 25056 Adj 20 101 4 208 15 105 20 180 2"
L16-1497,P15-2111,1,0.937865,"s ready for eliciting data from speakers of 33 additional languages.1 The methods we have developed entail first constructing detailed elicitation templates in which speakers supply desired inflectional forms (§2.). These templates include: 1) A prompt in a high-resource world language (e.g. English) that speakers can translate into the target low-resource language (e.g. Uzbek), 2) a short description of the desired inflectional form in pedagogical or linguistic terminology (e.g. ‘aorist’), and 3) a detailed specification of the inflectional form in terms of the UniMorph Schema, introduced by Sylak-Glassman et al. (2015a, 2015b).2 These elicita1 These 15 languages are listed in Table 1, and the additional 33 languages include: Acholi, Armenian, Balochi, Burmese, Cambodian, Coptic, Dinka, Gujarati, Hausa, Hindi, Kikuyu, Kongo (Fioti), Kurdish, Lao, Lingala, Luganda, Maguindanao, Malay, Mongolian, Nahuatl, Nubian, Nuer, Pashto, Samoan, Somali, Spanish, Surubu (Fiti), Swahili, Tausug, Thai, Vietnamese, Wolof, and Zande. 2 The name used in those works for this schema, the Universal Morphological Feature Schema, is deprecated in favor of the name UniMorph Schema. tion templates are distributed to remotely-located"
L16-1497,E14-1060,0,0.0699827,"add a generic object pronoun to an intransitive gloss template (e.g. ‘I am VBG it’). forms using string edit distance to determine the extent to which they diverge. Divergence can then be penalized, with thresholds established for acceptance, human inspection, and rejection. Speaker output can be compared to humanproduced, gold standard data from reference grammars, professional translators, or a resource such as Wiktionary. It may also be compared to the output predicted by stateof-the-art paradigm completion software that is known to achieve high accuracy, such as Durrett and DeNero (2013), Ahlberg et al. (2014, 2015), and Nicolai et al. (2015). The elicited output of untrained speakers can be scored to determine its accuracy. Scoring consists of comparing the elicited output to either verified or predicted wordThe current corpus of inflectional paradigms gathered via elicitation from professional translators includes data from 15 languages, and is described in detail in Table 1. 3118 Language Akan (Twi) Amharic Azeri Dari Farsi Igbo Indonesian Punjabi (Indian) Sinhala Tagalog Tajik Tigrinya Turkish Uzbek Yoruba TOTAL : (L) (F) L/F L F L F L F L F L F L F L F L F L F L F L F L F L F L F L F 792 2505"
L16-1497,W09-0106,0,0.0178118,"e labeled using a single annotation scheme with consistent, meaningful, non-overlapping features, any application that is designed to take in data from one language can also process data from any other language in the corpus as well as data from any other source annotated using the UniMorph Schema. Similarly, because the UniMorph Schema can be used with any language to capture the meaning encoded by its inflectional morphology, its use in annotating inflected wordform data allows for the design of systems which can be language agnostic or independent without disregarding linguistic diversity (Bender, 2009). 3. Gathering Inflectional Paradigm Data The templates described above were used to gather inflectional paradigm data from professional translators and from untrained native speakers recruited through Amazon’s Mechanical Turk crowdsourcing platform. One of the primary advantages of gathering paradigm data from professional translators was their greater metalinguistic awareness, including their overall knowledge of the language’s structure and their comfort with pedagogical and linguistic terminology. Professional translators can therefore use descriptive information to pinpoint which form is"
L16-1497,N13-1138,0,0.349568,"recated in favor of the name UniMorph Schema. tion templates are distributed to remotely-located speakers, who may be either professional translators or untrained native speakers recruited via crowdsourcing platforms (§3.). Even a very small amount of inflectional paradigm seed data, particularly when it is representative of the surface morphological variation of the language, can serve as effective training data for machine learning, particularly for generating possible inflected forms (§4.). We demonstrate this by assessing the ability of recent morphological paradigm completion software by Durrett and DeNero (2013) to predict inflected word forms using the small amount of training data supplied by elicitation, the full amount of Wiktionary data, and a small random subset of that Wiktionary data. 2. Elicitation Templates Language informants, including professional translators and untrained native speakers, are asked to supply inflectional forms by translating prompts in English or another major world language with which they may be more familiar (e.g. French, Spanish). These prompts are carefully constructed and phrased to make the use of a particular inflectional form obligatory in the given context. Co"
L16-1498,zeman-2008-reusable,0,\N,Missing
L16-1498,N13-1138,0,\N,Missing
L16-1498,P15-2111,1,\N,Missing
L16-1498,P15-2068,0,\N,Missing
L18-1150,W11-2123,0,0.0193788,"nts 1 Note that the training and test sets are in the same domain, i.e. named entities. While it may be interesting to test on an unrelated set of words, the results are not likely to be encouraging unless these words are orthographically/phonologically similar to their English counterparts (e.g. cognates or borrowed words). 2 https://pypi.python.org/pypi/Unidecode Table 1: Bitext format for the OpenNMT Multi experiment. The target word is the English name Pollux. Average Transliteration Performance 0.233 0.25 0.211 1-best Accuracy Moses was trained using a vanilla setup, with a 4-gram KenLM (Heafield, 2011) language model, tuning with MERT (Och, 2003), and setting the distortion limit to 0 to prevent reordering, which does not occur during transliteration. Sequitur was trained iteratively using the --ramp-up flag three times. For OpenNMT, we used the following hyperparameters: a 2 layer GRU for encoder and decoder, optimizer is Adadelta, 0.2 dropout rate, hidden size 200, embedding size 200, no length normalization. We trained each model for 50 epochs and used the model with the lowest validation perplexity. Target pollux pollux pollux pollux pollux pollux pollux pollux 0.20 0.15 0.209 0.211 0.2"
L18-1150,2010.amta-papers.12,0,0.0295594,"gner. The scope of this data accentuates the low-resource setting, which is reasonable for many of the world’s languages; the Bible may be one of the only bilingual resources available for certain languages. 2. Related Work Machine transliteration has been tackled using a variety of methods. For a comprehensive survey, we refer the reader to Karimi et al. (2011). In this paper, we focus on transliteration of named entities across multiple language, especially in a low-resource setting, using the paradigm of monotonic machine translation (Virga and Khudanpur, 2003). Our work is most similar to Irvine et al. (2010), who built character-based machine translation systems using names mined from Wikipedia. Their setting is higher resource, as they use data acquired from the web, and they experiment on a smaller set of languages. Other recent approaches to 4. Experiments We perform three major transliteration experiments: the first, in which we compare several existing machine translation systems in the task of transliteration; the second, in which we evaluate the effectiveness of pre- and postprocessing the data with a baseline transliterator; and the third, in which we employ a single NMT system to transla"
L18-1150,W10-2405,0,0.0358221,"transliterate multiple languages to a single target language significantly outperforms the single-language systems. We release each system’s transliteration output as a dataset for comparative analysis. To our knowledge, this is the first study of such scale that compares such a variety of methods on such small corpora. transliteration of low resource languages include Mayhew et al. (2016), who explore using surrogate languages in place of a language not in Wikipedia, Rosca and Breuel (2016) who showed state of the art transliteration performance using a neural sequence to sequence model, and Jiampojamarn et al. (2010), who explore several methods for language-independent transliteration mining. Qian et al. (2010) also developed a toolkit to extract translation pairs from comparable corpora. We compare several open-source toolkits for translation: Moses (Koehn et al., 2007), a phrase-based statistical machine translation toolkit; Sequitur (Bisani and Ney, 2008), a grapheme to phoneme system; and OpenNMT (Klein et al., 2017), a neural sequence-to-sequence machine translation system. While Sequitur is not commonly used in MT, Sequitur and Moses have been compared for speech recognition tasks (Schlippe et al.,"
L18-1150,P16-2090,0,0.029467,"d help. We expect the preprocessing step to reduce the character set of the source language, which has the effect of losing information if multiple characters can map to a single ASCII character (e.g. in Greek, σ and ς both correspond to the letter s). For postprocessing, we expect that this step will correct characters that were not transliterated, which can occur if they were not seen in the training set, similar to unseen words in MT. In our final experiment, we exploit the multilinguality of the data for transfer learning. Inspired by the winning system in the SIGMORPHON 2016 shared task (Kann and Schütze, 2016), we train a single neural MT system on the concatenation of the entire training set. This allows the system to learn a joint model from multiple source languages to a single target language. In addition, by concatenating training sets of each of the single language-pair systems, the multi-source approach can circumvent the data scarcity problem. Each training example was split into spaces, with a special source language symbol prepended, as shown in Table 1. No target language symbol was used because the target language was only English. Since the training and test data are different (and sub"
L18-1150,P17-4012,0,0.0310326,"e languages in place of a language not in Wikipedia, Rosca and Breuel (2016) who showed state of the art transliteration performance using a neural sequence to sequence model, and Jiampojamarn et al. (2010), who explore several methods for language-independent transliteration mining. Qian et al. (2010) also developed a toolkit to extract translation pairs from comparable corpora. We compare several open-source toolkits for translation: Moses (Koehn et al., 2007), a phrase-based statistical machine translation toolkit; Sequitur (Bisani and Ney, 2008), a grapheme to phoneme system; and OpenNMT (Klein et al., 2017), a neural sequence-to-sequence machine translation system. While Sequitur is not commonly used in MT, Sequitur and Moses have been compared for speech recognition tasks (Schlippe et al., 2014). 3. Data We use the Bible names translation matrix dataset (Wu and Yarowsky, 2018), which contains 1129 person and place names in the Bible aligned across 519 languages. They constructed this name translation matrix using a combination of distance-based, MT transliteration, and string transduction rules to improve the alignments from a baseline aligner. The scope of this data accentuates the low-resourc"
L18-1150,W17-3204,0,0.0434864,"Missing"
L18-1150,P07-2045,0,0.010476,"pares such a variety of methods on such small corpora. transliteration of low resource languages include Mayhew et al. (2016), who explore using surrogate languages in place of a language not in Wikipedia, Rosca and Breuel (2016) who showed state of the art transliteration performance using a neural sequence to sequence model, and Jiampojamarn et al. (2010), who explore several methods for language-independent transliteration mining. Qian et al. (2010) also developed a toolkit to extract translation pairs from comparable corpora. We compare several open-source toolkits for translation: Moses (Koehn et al., 2007), a phrase-based statistical machine translation toolkit; Sequitur (Bisani and Ney, 2008), a grapheme to phoneme system; and OpenNMT (Klein et al., 2017), a neural sequence-to-sequence machine translation system. While Sequitur is not commonly used in MT, Sequitur and Moses have been compared for speech recognition tasks (Schlippe et al., 2014). 3. Data We use the Bible names translation matrix dataset (Wu and Yarowsky, 2018), which contains 1129 person and place names in the Bible aligned across 519 languages. They constructed this name translation matrix using a combination of distance-based"
L18-1150,P03-1021,0,0.0285346,"the same domain, i.e. named entities. While it may be interesting to test on an unrelated set of words, the results are not likely to be encouraging unless these words are orthographically/phonologically similar to their English counterparts (e.g. cognates or borrowed words). 2 https://pypi.python.org/pypi/Unidecode Table 1: Bitext format for the OpenNMT Multi experiment. The target word is the English name Pollux. Average Transliteration Performance 0.233 0.25 0.211 1-best Accuracy Moses was trained using a vanilla setup, with a 4-gram KenLM (Heafield, 2011) language model, tuning with MERT (Och, 2003), and setting the distortion limit to 0 to prevent reordering, which does not occur during transliteration. Sequitur was trained iteratively using the --ramp-up flag three times. For OpenNMT, we used the following hyperparameters: a 2 layer GRU for encoder and decoder, optimizer is Adadelta, 0.2 dropout rate, hidden size 200, embedding size 200, no length normalization. We trained each model for 50 epochs and used the model with the lowest validation perplexity. Target pollux pollux pollux pollux pollux pollux pollux pollux 0.20 0.15 0.209 0.211 0.216 0.214 Maj. Wei. 0.171 0.141 0.098 0.10 0.0"
L18-1150,qian-etal-2010-python,0,0.0118291,"e systems. We release each system’s transliteration output as a dataset for comparative analysis. To our knowledge, this is the first study of such scale that compares such a variety of methods on such small corpora. transliteration of low resource languages include Mayhew et al. (2016), who explore using surrogate languages in place of a language not in Wikipedia, Rosca and Breuel (2016) who showed state of the art transliteration performance using a neural sequence to sequence model, and Jiampojamarn et al. (2010), who explore several methods for language-independent transliteration mining. Qian et al. (2010) also developed a toolkit to extract translation pairs from comparable corpora. We compare several open-source toolkits for translation: Moses (Koehn et al., 2007), a phrase-based statistical machine translation toolkit; Sequitur (Bisani and Ney, 2008), a grapheme to phoneme system; and OpenNMT (Klein et al., 2017), a neural sequence-to-sequence machine translation system. While Sequitur is not commonly used in MT, Sequitur and Moses have been compared for speech recognition tasks (Schlippe et al., 2014). 3. Data We use the Bible names translation matrix dataset (Wu and Yarowsky, 2018), which"
L18-1150,W03-1508,0,0.117044,"tion rules to improve the alignments from a baseline aligner. The scope of this data accentuates the low-resource setting, which is reasonable for many of the world’s languages; the Bible may be one of the only bilingual resources available for certain languages. 2. Related Work Machine transliteration has been tackled using a variety of methods. For a comprehensive survey, we refer the reader to Karimi et al. (2011). In this paper, we focus on transliteration of named entities across multiple language, especially in a low-resource setting, using the paradigm of monotonic machine translation (Virga and Khudanpur, 2003). Our work is most similar to Irvine et al. (2010), who built character-based machine translation systems using names mined from Wikipedia. Their setting is higher resource, as they use data acquired from the web, and they experiment on a smaller set of languages. Other recent approaches to 4. Experiments We perform three major transliteration experiments: the first, in which we compare several existing machine translation systems in the task of transliteration; the second, in which we evaluate the effectiveness of pre- and postprocessing the data with a baseline transliterator; and the third,"
L18-1150,L18-1263,1,0.63447,"ion mining. Qian et al. (2010) also developed a toolkit to extract translation pairs from comparable corpora. We compare several open-source toolkits for translation: Moses (Koehn et al., 2007), a phrase-based statistical machine translation toolkit; Sequitur (Bisani and Ney, 2008), a grapheme to phoneme system; and OpenNMT (Klein et al., 2017), a neural sequence-to-sequence machine translation system. While Sequitur is not commonly used in MT, Sequitur and Moses have been compared for speech recognition tasks (Schlippe et al., 2014). 3. Data We use the Bible names translation matrix dataset (Wu and Yarowsky, 2018), which contains 1129 person and place names in the Bible aligned across 519 languages. They constructed this name translation matrix using a combination of distance-based, MT transliteration, and string transduction rules to improve the alignments from a baseline aligner. The scope of this data accentuates the low-resource setting, which is reasonable for many of the world’s languages; the Bible may be one of the only bilingual resources available for certain languages. 2. Related Work Machine transliteration has been tackled using a variety of methods. For a comprehensive survey, we refer th"
L18-1263,P06-1097,0,0.0238097,"ultilingual learning of name-related affixes and their semantics as well as transliteration of named entities. Keywords: Bible, alignment, named entities, translation, transliteration 1. 2. Introduction In a statistical machine translation (SMT) pipeline, word alignment is important for extracting phrase translations. However, for low-resource languages with very little data, these alignments may be extremely noisy or may not exist at all. Thus, improving the quality of word alignments leads to more accurate phrase pairs, which in turn improves the quality of an SMT system (Och and Ney, 2003; Fraser and Marcu, 2006). For many low-resource languages, the Bible is the only text available, making it a valuable resource to train machine translation (MT) systems. This paper focuses on the translation and transliteration of named entities from the Bible, which are a rich resource for studying lexical borrowing (Tsvetkov and Dyer, 2016), since they are usually borrowed between languages rather than translated1 (Whitney, 1881; Moravcsik, 1978; Myers-Scotton, 2002). Like cognates, names are often phonetically or orthographically similar across languages, which make them suited for training transliteration systems"
L18-1263,W11-2123,0,0.0201672,"Ethnologue. The pivot languages were chosen for their near-complete coverage over the 1129 names, and we utilized the six nearest languages due to the potential for names to be orthographically similar in related languages. For each language pair, treating foreign-English name pairs as bitext, we split the data in half and trained two systems A and B, such that system A decodes the data that system B was trained on, and vice versa; this was done to ensure that the test set was never seen by the system performing decoding. We used a standard Moses (Koehn et al., 2007) setup with 5-gram KenLM (Heafield, 2011) language model and MERT (Och, 2003) for tuning. Each system generated a unique 200-best list of hypotheses. 6 Denotes missing alignment, which in this case is frequent enough to make it into the top 5. 7 For example, a language may not have a translation of the Old Testament, so names appearing only in the Old Testament will have missing alignments. 1660 (a) Issue 1: Incorrect alignments (b) Issue 2: Missing alignments (c) Issue 3: Non-lemma alignments English Foreign Lang English For. Lang English Foreign Lang Boaz Boaz Obed David Eliezer Julia Obedarinchichitam Obed Jeseyrinchichitam Luwiy"
L18-1263,W09-3519,0,0.0305543,"r et al., 2007), using semantic relationships (Songyot and Chiang, 2014), prior distributions (Mermer and Saraclar, 2011; Vaswani et al., 2012), discriminative alignment models (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010) and part-of-speech (POS) tagging (Lee et al., 2006; Sanchis and Sánchez, 2008). Our work uses the assumption that names are often orthographically or phonetically similar across languages. We use MT as an intermediate step to generate hypotheses for citation form alignments. This is akin to training phrase-based MT systems for transliteration (Song et al., 2009; Jia et al., 2009; Dasigi and Diab, 2011). (Dasigi and Diab, 2011) used a character-based Moses system and post-edited the output using linguistic rules, which is similar to our approach of using MT and applying transformation rules. Our approach is unique in that we use cross-language joint models of variant/latent forms to expand and refine the candidate space of citation forms. 3. Translation Matrix The primary contribution of this paper is a translation matrix of 1129 English names aligned and translated into 591 languages. We produced around 14,000 alignments, providing better coverage than Wikipedia for"
L18-1263,P07-2045,0,0.00878062,"in an ordering based on the language tree in Ethnologue. The pivot languages were chosen for their near-complete coverage over the 1129 names, and we utilized the six nearest languages due to the potential for names to be orthographically similar in related languages. For each language pair, treating foreign-English name pairs as bitext, we split the data in half and trained two systems A and B, such that system A decodes the data that system B was trained on, and vice versa; this was done to ensure that the test set was never seen by the system performing decoding. We used a standard Moses (Koehn et al., 2007) setup with 5-gram KenLM (Heafield, 2011) language model and MERT (Och, 2003) for tuning. Each system generated a unique 200-best list of hypotheses. 6 Denotes missing alignment, which in this case is frequent enough to make it into the top 5. 7 For example, a language may not have a translation of the Old Testament, so names appearing only in the Old Testament will have missing alignments. 1660 (a) Issue 1: Incorrect alignments (b) Issue 2: Missing alignments (c) Issue 3: Non-lemma alignments English Foreign Lang English For. Lang English Foreign Lang Boaz Boaz Obed David Eliezer Julia Obedar"
L18-1263,D07-1005,0,0.0296754,"is resource will be of great linguistic importance in studying low resource languages and will be applicable in several areas such as transliteration and morphological analysis of named entities. *Denotes equal contribution. 1 The opposite case is also very interesting, e.g. in many lanRelated Work Due to low frequency words, word alignments can suffer from misalignments, which in turn can be detrimental to downstream tasks like MT. Previously, researchers have worked on improving word alignment to improve MT using a variety of approaches: combining hypotheses generated from bridge languages (Kumar et al., 2007), using semantic relationships (Songyot and Chiang, 2014), prior distributions (Mermer and Saraclar, 2011; Vaswani et al., 2012), discriminative alignment models (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010) and part-of-speech (POS) tagging (Lee et al., 2006; Sanchis and Sánchez, 2008). Our work uses the assumption that names are often orthographically or phonetically similar across languages. We use MT as an intermediate step to generate hypotheses for citation form alignments. This is akin to training phrase-based MT systems for transliteration (Song et al., 2009; Jia et al., 200"
L18-1263,N06-1014,0,0.0669157,"n Ngwan, *gug = Paraguayan Guaraní) are aligned across all languages. Each cell in the translation matrix contains the best guess of the citation form of the English name in the target language. This form is the consensus of four different methods, which are described in the following sections. An excerpt of the name translation matrix is shown in Table 1. 4. Improving Named Entity Alignment The source data from Mayer and Cysouw (2014) contains 24 English editions of the Bible. For 591 target language bibles, we word aligned each verse with each English Bible verse using the Berkeley Aligner (Liang et al., 2006) and performed POS tagging to extracted proper nouns from these alignments. A total of 1129 English named entities were extracted. For each English name, we considered multiple citation hypotheses in all target languages from the following four approaches. 4.1. Most frequent alignment For every target language, the baseline hypothesis for a name’s translation is the most frequent alignment obtained from the aligner. These initial hypotheses contained several alignment problems which we broadly classify into three categories: (1) incorrect alignment, (2) missing alignment, and (3) non-base form"
L18-1263,mayer-cysouw-2014-creating,0,0.189484,"a¯eli d¯avida p¯avils p¯eteri ‘ e¯ gipti jeruzalem¯e gug* jesús cristo israel david pablo pedro egípto jerusalén Table 1: Example translation matrix of Bible named entities (*cnw = Chin Ngwan, *gug = Paraguayan Guaraní) are aligned across all languages. Each cell in the translation matrix contains the best guess of the citation form of the English name in the target language. This form is the consensus of four different methods, which are described in the following sections. An excerpt of the name translation matrix is shown in Table 1. 4. Improving Named Entity Alignment The source data from Mayer and Cysouw (2014) contains 24 English editions of the Bible. For 591 target language bibles, we word aligned each verse with each English Bible verse using the Berkeley Aligner (Liang et al., 2006) and performed POS tagging to extracted proper nouns from these alignments. A total of 1129 English named entities were extracted. For each English name, we considered multiple citation hypotheses in all target languages from the following four approaches. 4.1. Most frequent alignment For every target language, the baseline hypothesis for a name’s translation is the most frequent alignment obtained from the aligner."
L18-1263,P11-2032,0,0.0194372,"pplicable in several areas such as transliteration and morphological analysis of named entities. *Denotes equal contribution. 1 The opposite case is also very interesting, e.g. in many lanRelated Work Due to low frequency words, word alignments can suffer from misalignments, which in turn can be detrimental to downstream tasks like MT. Previously, researchers have worked on improving word alignment to improve MT using a variety of approaches: combining hypotheses generated from bridge languages (Kumar et al., 2007), using semantic relationships (Songyot and Chiang, 2014), prior distributions (Mermer and Saraclar, 2011; Vaswani et al., 2012), discriminative alignment models (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010) and part-of-speech (POS) tagging (Lee et al., 2006; Sanchis and Sánchez, 2008). Our work uses the assumption that names are often orthographically or phonetically similar across languages. We use MT as an intermediate step to generate hypotheses for citation form alignments. This is akin to training phrase-based MT systems for transliteration (Song et al., 2009; Jia et al., 2009; Dasigi and Diab, 2011). (Dasigi and Diab, 2011) used a character-based Moses system and post-edited th"
L18-1263,H05-1011,0,0.0135184,"med entities. *Denotes equal contribution. 1 The opposite case is also very interesting, e.g. in many lanRelated Work Due to low frequency words, word alignments can suffer from misalignments, which in turn can be detrimental to downstream tasks like MT. Previously, researchers have worked on improving word alignment to improve MT using a variety of approaches: combining hypotheses generated from bridge languages (Kumar et al., 2007), using semantic relationships (Songyot and Chiang, 2014), prior distributions (Mermer and Saraclar, 2011; Vaswani et al., 2012), discriminative alignment models (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010) and part-of-speech (POS) tagging (Lee et al., 2006; Sanchis and Sánchez, 2008). Our work uses the assumption that names are often orthographically or phonetically similar across languages. We use MT as an intermediate step to generate hypotheses for citation form alignments. This is akin to training phrase-based MT systems for transliteration (Song et al., 2009; Jia et al., 2009; Dasigi and Diab, 2011). (Dasigi and Diab, 2011) used a character-based Moses system and post-edited the output using linguistic rules, which is similar to our approach of"
L18-1263,J03-1002,0,0.00855461,"on matrix for the multilingual learning of name-related affixes and their semantics as well as transliteration of named entities. Keywords: Bible, alignment, named entities, translation, transliteration 1. 2. Introduction In a statistical machine translation (SMT) pipeline, word alignment is important for extracting phrase translations. However, for low-resource languages with very little data, these alignments may be extremely noisy or may not exist at all. Thus, improving the quality of word alignments leads to more accurate phrase pairs, which in turn improves the quality of an SMT system (Och and Ney, 2003; Fraser and Marcu, 2006). For many low-resource languages, the Bible is the only text available, making it a valuable resource to train machine translation (MT) systems. This paper focuses on the translation and transliteration of named entities from the Bible, which are a rich resource for studying lexical borrowing (Tsvetkov and Dyer, 2016), since they are usually borrowed between languages rather than translated1 (Whitney, 1881; Moravcsik, 1978; Myers-Scotton, 2002). Like cognates, names are often phonetically or orthographically similar across languages, which make them suited for trainin"
L18-1263,P03-1021,0,0.0324273,"sen for their near-complete coverage over the 1129 names, and we utilized the six nearest languages due to the potential for names to be orthographically similar in related languages. For each language pair, treating foreign-English name pairs as bitext, we split the data in half and trained two systems A and B, such that system A decodes the data that system B was trained on, and vice versa; this was done to ensure that the test set was never seen by the system performing decoding. We used a standard Moses (Koehn et al., 2007) setup with 5-gram KenLM (Heafield, 2011) language model and MERT (Och, 2003) for tuning. Each system generated a unique 200-best list of hypotheses. 6 Denotes missing alignment, which in this case is frequent enough to make it into the top 5. 7 For example, a language may not have a translation of the Old Testament, so names appearing only in the Old Testament will have missing alignments. 1660 (a) Issue 1: Incorrect alignments (b) Issue 2: Missing alignments (c) Issue 3: Non-lemma alignments English Foreign Lang English For. Lang English Foreign Lang Boaz Boaz Obed David Eliezer Julia Obedarinchichitam Obed Jeseyrinchichitam Luwiy Yesua Pirorogasomi cbu mnx cbu agu m"
L18-1263,P10-1017,0,0.0259603,"ibution. 1 The opposite case is also very interesting, e.g. in many lanRelated Work Due to low frequency words, word alignments can suffer from misalignments, which in turn can be detrimental to downstream tasks like MT. Previously, researchers have worked on improving word alignment to improve MT using a variety of approaches: combining hypotheses generated from bridge languages (Kumar et al., 2007), using semantic relationships (Songyot and Chiang, 2014), prior distributions (Mermer and Saraclar, 2011; Vaswani et al., 2012), discriminative alignment models (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010) and part-of-speech (POS) tagging (Lee et al., 2006; Sanchis and Sánchez, 2008). Our work uses the assumption that names are often orthographically or phonetically similar across languages. We use MT as an intermediate step to generate hypotheses for citation form alignments. This is akin to training phrase-based MT systems for transliteration (Song et al., 2009; Jia et al., 2009; Dasigi and Diab, 2011). (Dasigi and Diab, 2011) used a character-based Moses system and post-edited the output using linguistic rules, which is similar to our approach of using MT and applying transformation rules. O"
L18-1263,D14-1197,0,0.0135144,"in studying low resource languages and will be applicable in several areas such as transliteration and morphological analysis of named entities. *Denotes equal contribution. 1 The opposite case is also very interesting, e.g. in many lanRelated Work Due to low frequency words, word alignments can suffer from misalignments, which in turn can be detrimental to downstream tasks like MT. Previously, researchers have worked on improving word alignment to improve MT using a variety of approaches: combining hypotheses generated from bridge languages (Kumar et al., 2007), using semantic relationships (Songyot and Chiang, 2014), prior distributions (Mermer and Saraclar, 2011; Vaswani et al., 2012), discriminative alignment models (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010) and part-of-speech (POS) tagging (Lee et al., 2006; Sanchis and Sánchez, 2008). Our work uses the assumption that names are often orthographically or phonetically similar across languages. We use MT as an intermediate step to generate hypotheses for citation form alignments. This is akin to training phrase-based MT systems for transliteration (Song et al., 2009; Jia et al., 2009; Dasigi and Diab, 2011). (Dasigi and Diab, 2011) used a"
L18-1263,H05-1010,0,0.0234988,"*Denotes equal contribution. 1 The opposite case is also very interesting, e.g. in many lanRelated Work Due to low frequency words, word alignments can suffer from misalignments, which in turn can be detrimental to downstream tasks like MT. Previously, researchers have worked on improving word alignment to improve MT using a variety of approaches: combining hypotheses generated from bridge languages (Kumar et al., 2007), using semantic relationships (Songyot and Chiang, 2014), prior distributions (Mermer and Saraclar, 2011; Vaswani et al., 2012), discriminative alignment models (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010) and part-of-speech (POS) tagging (Lee et al., 2006; Sanchis and Sánchez, 2008). Our work uses the assumption that names are often orthographically or phonetically similar across languages. We use MT as an intermediate step to generate hypotheses for citation form alignments. This is akin to training phrase-based MT systems for transliteration (Song et al., 2009; Jia et al., 2009; Dasigi and Diab, 2011). (Dasigi and Diab, 2011) used a character-based Moses system and post-edited the output using linguistic rules, which is similar to our approach of using MT and applying"
L18-1263,P12-1033,0,0.0159743,"such as transliteration and morphological analysis of named entities. *Denotes equal contribution. 1 The opposite case is also very interesting, e.g. in many lanRelated Work Due to low frequency words, word alignments can suffer from misalignments, which in turn can be detrimental to downstream tasks like MT. Previously, researchers have worked on improving word alignment to improve MT using a variety of approaches: combining hypotheses generated from bridge languages (Kumar et al., 2007), using semantic relationships (Songyot and Chiang, 2014), prior distributions (Mermer and Saraclar, 2011; Vaswani et al., 2012), discriminative alignment models (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010) and part-of-speech (POS) tagging (Lee et al., 2006; Sanchis and Sánchez, 2008). Our work uses the assumption that names are often orthographically or phonetically similar across languages. We use MT as an intermediate step to generate hypotheses for citation form alignments. This is akin to training phrase-based MT systems for transliteration (Song et al., 2009; Jia et al., 2009; Dasigi and Diab, 2011). (Dasigi and Diab, 2011) used a character-based Moses system and post-edited the output using linguist"
L18-1263,L18-1150,1,0.63447,"tter than Moses. For these two languages, most of the source and target words were identical. In such cases, Moses would just learn character identity mappings, and we suspect that the language model was biasing the system away from the correct answer. For example, the Ifugao-English system incorrectly transliterated Amminadab as Aminadab, whereas passing the source through unchanged would have achieved a higher accuracy. More investigation is necessary to determine the role of the language model in transliteration, especially of low-resource languages. A followup on this work is presented in Wu and Yarowsky (2018), who compare the performance of several methods of transliteration, including phrase-based and neural machine translation, on our Bible names dataset. 11 For example, Caesar was originally just a name but eventually became a title for the Roman emperor. 0.8 https://pypi.python.org/pypi/Unidecode Transliteration Performance Unidecode Moses 0.6 Accuracy 12 0.4 0.2 0.0 ace agu amx arb aso atg ban bvr cce cni cok dug est gag gaw gng gug huu iba ifb iku ium kac krc kue maj mbb mhr mih mlh mna mpt msa mvp mwf mxb mzw sme tos zul Source Language Figure 1: Comparing the performance of a baseline tran"
L18-1293,E14-1060,1,0.914644,"Missing"
L18-1293,P16-1156,1,0.911216,"Missing"
L18-1293,N07-1048,0,0.136877,"Missing"
L18-1293,N13-1138,0,0.235978,"Missing"
L18-1293,P08-1115,0,0.0456449,"Missing"
L18-1293,N16-1077,1,0.880807,"Missing"
L18-1293,L16-1498,1,0.945257,"The Universal Morphology (UniMorph) project, centered at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University is a collaborative effort to improve how NLP systems handle complex morphology across the world’s languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. Kirov et al. (2016) introduced version 1.0 of the UniMorph morphological database, created by extracting and normalizing the inflectional paradigms included in Wiktionary (www.wiktionary.org), a large, broadly multi-lingual crowd-sourced collection of lexical data. This paper describes UniMorph 2.0. It details improvements in Wiktionary extraction and annotation, as well as normalization of non-Wiktionary resources, leading to a much higher quality morphological database. The new dataset spans 52 languages representing a range of language families. As in UniMorph 1.0, we provide paradigms from highlyinflected op"
L18-1293,D14-1095,0,0.116143,"Missing"
L18-1293,N15-1093,0,0.150689,"Missing"
L18-1293,Q15-1026,0,0.0836395,"Missing"
L18-1293,P15-2111,1,0.852807,"rsing and normalization of Wiktionary. Wiktionary is a broadly multilingual resource with many crowd-sourced morphological paradigms in the form of custom HTML tables. Figure 1 illustrates the challenge associated with extracting this data. Wiktionary is designed for human, rather than machine readability, and authors have extensive freedom in formatting data. This leads to wildly differing table layouts across languages which need to be converted to a consistent tabular format. The extraction process developed for UniMorph 1.0 relied heavily on statistical, visual, and positional heuristics (Sylak-Glassman et al., 2015b) to: 1. Determine which entries in an HTML table are inflected forms and which are grammatical descriptors. 2. Link each inflected form with its appropriate descriptors. 3. Convert each set of linked descriptors into a universal feature annotation schema, described in detail in Sylak-Glassman (2016).1 This led to a large dataset of 952,530 unique noun, verb, and adjective lemmas across 350 languages. Unfortunately, 1 pdf unimorph.github.io/doc/unimorph-schema. Figure 1: Paradigm extraction and normalization. the UniMorph 1.0 dataset was very error-prone due to the inability of our heuristics"
L18-1293,zeman-2008-reusable,0,0.0233658,"Each group represents a different type of paradigm (e.g., regular verb). For each group, a sample table was selected, and an annotator replaced each inflected form in the table with the appropriate UniMorph features. All annotation was compliant with the UniMorph Schema, which was designed to represent the full range of semantic distinctions that can be captured by inflectional morphology in any language (SylakGlassman et al., 2015a). The schema is similar in form and spirit to other tagset universalization efforts, such as the Universal Dependencies Project (Choi et al., 2015) and Interset (Zeman, 2008), but is designed specifically for typological completeness for inflectional morphology, including a focus on the morphology of especially low-resource languages. It includes over 200 base features distributed among 23 dimensions of meaning (i.e., morphological categories), including both common dimensions like tense and aspect as well as rarer dimensions like evidentiality and switch-reference. Despite the high coverage of the UniMorph tagset, for UniMorph 2.0, annotators were allowed to employ additional ‘language specific’ LGSPEC(1, 2, 3, etc.) features to mark any missing distinctions, or"
L18-1293,W16-2002,1,\N,Missing
L18-1538,C10-3010,0,0.078589,"urkic languages are available for research purposes 1 . Using these cognate tables, we construct multi-way bitext and train character-based machine translation systems to transliterate cognates to fill in missing entries in the cognate chains. Finally, we evaluate multiple methods of system combination on the cognate chain completion task, showing improvements over single language-pair systems. For the Romance languages, we find that performance-based weight outperforms combining weights derived from a linguistic phylogeny. 2. Data We begin with lemmas from two free lexical resources, PanLex (Baldwin et al., 2010) and Wiktionary2 . From PanLex, we pivot words on English and extract foreign-English translation pairs, retaining each word’s Meaning IDs,3 and its most common backtranslation in PanLex4 . From Wiktionary, we use translation pairs mined from the info boxes on the English version of the site (Sylak-Glassman et al., 1 github.com/wswu/coglust wiktionary.org 3 An identifier indicating semantic relatedness. A single word may have multiple Meaning IDs, and words in different languages may have the same meaning ID. 4 The most common backtranslation is the most frequent English translation of the for"
L18-1538,I13-1112,0,0.670836,"ds in machine translation, it is often not necessary that these words be true cognates in the linguistic sense, i.e. they are descendants of a common ancestor (Ciobanu and Dinu, 2014). For example, names and loanwords are not technically considered cognates, though they behave as such. Rather, “cognates” only need to meet certain established criteria for cognacy (Kondrak, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014), which include individually or a combination of orthographic, phonetic, and semantic similarity between words. Previous approaches to cognate transliteration (Mulloni, 2007; Beinborn et al., 2013) suffer from the drawback that they require an existing list of cognates, which is infeasible for low-resource languages. In contrast, we automatically generate cognate tables by clustering words from existing lexical resources using a combination of similarity measures. Our produced cognate tables for Romance and Turkic languages are available for research purposes 1 . Using these cognate tables, we construct multi-way bitext and train character-based machine translation systems to transliterate cognates to fill in missing entries in the cognate chains. Finally, we evaluate multiple methods o"
L18-1538,P17-1181,0,0.0180557,"ges. They make extensive use of Levenshtein distance (Levenshtein, 1966) in determining the distance between two cognates. In our work, we employed a weighted edit distance as a major component in determining cognate clusters. Clustering cognates has also been recently explored using different approaches to determine cognacy, e.g. using an SVM which trained to determine cognacy (Hauer and Kondrak, 2011) and accounting for a language family’s phylogeny when constructing cognate groups (Hall and Klein, 2010). We experiment with using phylogenetic information in our system combination. Recently, Bloodgood and Strauss (2017) experimented with global constraints to improve cognate detection. This approach is complementary to ours and could be used to improve our cognate tables. Several methods have also been proposed to generating cognates, e.g. using a POS tagging framework where the tags are actually target language n-grams (Mulloni, 2007). Recently, several approaches to character-based machine translation using cognates have been investigated, although on a small set of language pairs. Beinborn et al. (2013) experiment on English-Spanish with a manual list of cognates. Scherrer and Sagot (2014) perform a task"
L18-1538,P14-2017,0,0.300903,"ing hypotheses for missing cognates has applications in alignment and resolving unknown words in machine translation. In the field of linguistics, examining cognates across multiple related languages can shed light on how words are borrowed between languages. Cognate lists are not widely available for many languages and are time-consuming to create by hand. In many NLP-related applications, including the translating out-ofvocabulary words in machine translation, it is often not necessary that these words be true cognates in the linguistic sense, i.e. they are descendants of a common ancestor (Ciobanu and Dinu, 2014). For example, names and loanwords are not technically considered cognates, though they behave as such. Rather, “cognates” only need to meet certain established criteria for cognacy (Kondrak, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014), which include individually or a combination of orthographic, phonetic, and semantic similarity between words. Previous approaches to cognate transliteration (Mulloni, 2007; Beinborn et al., 2013) suffer from the drawback that they require an existing list of cognates, which is infeasible for low-resource languages. In contrast, we automatically generate"
L18-1538,P10-1105,0,0.0768091,", with Mann and Yarowsky (2001) inducing translation lexicons between cross-family languages via bridge languages. They make extensive use of Levenshtein distance (Levenshtein, 1966) in determining the distance between two cognates. In our work, we employed a weighted edit distance as a major component in determining cognate clusters. Clustering cognates has also been recently explored using different approaches to determine cognacy, e.g. using an SVM which trained to determine cognacy (Hauer and Kondrak, 2011) and accounting for a language family’s phylogeny when constructing cognate groups (Hall and Klein, 2010). We experiment with using phylogenetic information in our system combination. Recently, Bloodgood and Strauss (2017) experimented with global constraints to improve cognate detection. This approach is complementary to ours and could be used to improve our cognate tables. Several methods have also been proposed to generating cognates, e.g. using a POS tagging framework where the tags are actually target language n-grams (Mulloni, 2007). Recently, several approaches to character-based machine translation using cognates have been investigated, although on a small set of language pairs. Beinborn"
L18-1538,I11-1097,0,0.830425,"by running an initial clustering step on each group of words. In this step, the distance function is the unweighted normalized Levenshtein distance LD(x,y) |x|+|y |, and clusters are merged 2 if the distance falls under a generous threshold of 0.5. Treating these clusters as multi-way aligned bitext, we run GIZA++ (Och and Ney, 2000) to extract character-tocharacter substitution probabilities, which are used in a second clustering step. The idea is that a second iteration of clustering should produce better results than a single iteration. This is similar to the two-pass approach employed by (Hauer and Kondrak, 2011). For the second iteration of clustering, we define the distance function d between two words x and y as a linear combination of the following features, chosen specifically to model both the orthographic and semantic relatedness of cognates. 5 A word may have multiple or no POS tags. PanLex also contains POS tags, but they are noisy, so we only use those from Wiktionary (3) The character transition probabilities are obtained from alignment using GIZA++. They are subtracted from 1 to convert them to costs used in the edit distance calculation. We added an addition rule such that the distance be"
L18-1538,W11-2123,0,0.0732505,"pairs (e.g. Beinborn et al. (2013)), an alternative is to evaluate on a downstream task, namely cognate chain completion.6 To do this, we consider all cognate pairs within each cluster as translations of each other and construct bitext for each language pair, where characters are separated with spaces. Intuitively, if a machine translation system can translate well using this data, then the cognate chains have been correctly constructed. We train character-based Moses (Koehn et al., 2007) SMT systems for each language pair, using a standard setup of GIZA++ (Och and Ney, 2000), a 5-gram KenLM (Heafield, 2011) trained with the --discount-fallback option, and 6 This is similar to the task of Scherrer and Sagot (2014). Since we use a different set of languages from a different data source, we cannot directly compare to this work. However, we emphasize that since Scherrer and Sagot (2014) computes a distance between all pairs of words to determine cognacy, our approach of pivoting through English is computationally more efficient. 3413 Average Distance Per Word (Single Linkage) Average Distance Per Word (Single Linkage) 3.5 3.5 3.0 3.0 Average Distance Per Word (Single Linkage) 3.5 3.0 2.5 2.5 2.0 2.0"
L18-1538,P07-2045,0,0.0136827,"ion for Transliteration Although we might ideally evaluate the quality of the cognate clusters against a gold list of cognate pairs (e.g. Beinborn et al. (2013)), an alternative is to evaluate on a downstream task, namely cognate chain completion.6 To do this, we consider all cognate pairs within each cluster as translations of each other and construct bitext for each language pair, where characters are separated with spaces. Intuitively, if a machine translation system can translate well using this data, then the cognate chains have been correctly constructed. We train character-based Moses (Koehn et al., 2007) SMT systems for each language pair, using a standard setup of GIZA++ (Och and Ney, 2000), a 5-gram KenLM (Heafield, 2011) trained with the --discount-fallback option, and 6 This is similar to the task of Scherrer and Sagot (2014). Since we use a different set of languages from a different data source, we cannot directly compare to this work. However, we emphasize that since Scherrer and Sagot (2014) computes a distance between all pairs of words to determine cognacy, our approach of pivoting through English is computationally more efficient. 3413 Average Distance Per Word (Single Linkage) Ave"
L18-1538,C04-1137,0,0.230734,"ours and could be used to improve our cognate tables. Several methods have also been proposed to generating cognates, e.g. using a POS tagging framework where the tags are actually target language n-grams (Mulloni, 2007). Recently, several approaches to character-based machine translation using cognates have been investigated, although on a small set of language pairs. Beinborn et al. (2013) experiment on English-Spanish with a manual list of cognates. Scherrer and Sagot (2014) perform a task similar to our own; they start with a word list and find plausible cognates using the BI-SIM metric (Kondrak and Dorr, 2004), 3416 then perform character-based machine translation on cognates. They experiment with translating cognates from a high-resource language to a low-resource language. Our work differs in that our experiments are on a much larger scale, and we realize improvements by combining the results of multiple MT systems. 6. Conclusion We have presented an automatic clustering method to generate cognate tables from Panlex- and Wiktionary- derived dictionary data, which we release as a resource. Based on these cognate clusters, we then trained multiple Mosesbased models to complete cognate chains by gen"
L18-1538,N03-2016,0,0.116774,"cat mensa mesa mesa ? tabella tabla tabela taula tavolo tavola eng table table azj stol ? tuk tur uig stol ? üstel tablisa tablo ? tat östäl tablis uzn stol tablitsa Figure 1: Each row in the table is a cognate chain. The task of cognate chain completion is to fill in missing cells in the table. 1. Introduction Cognates are words in related languages that share a common origin. For example, the Italian cavallo and French cheval both originated from the Latin caballus. Besides being instrumental in historical linguistics, cognates find uses in many areas of NLP, including machine translation (Kondrak et al., 2003; Nakov and Tiedemann, 2012) and lexicon induction (Mann and Yarowsky, 2001). We define the task of cognate chain completion, shown in Figure 1. Given multi-way aligned cognate table, a cognate “chain” is a group of cognates across a language family (represented as a single row). Chains may have empty cells due to dictionary gaps, denoted by a ?, and the task is to predict these missing entries. Cognate chain completion is related to the task of cognate transliteration, except that words in related languages (within the same row) can contribute to the hypothesis of a cell. For low-resource lan"
L18-1538,N01-1014,0,0.535658,"ed light on how words are borrowed between languages. Cognate lists are not widely available for many languages and are time-consuming to create by hand. In many NLP-related applications, including the translating out-ofvocabulary words in machine translation, it is often not necessary that these words be true cognates in the linguistic sense, i.e. they are descendants of a common ancestor (Ciobanu and Dinu, 2014). For example, names and loanwords are not technically considered cognates, though they behave as such. Rather, “cognates” only need to meet certain established criteria for cognacy (Kondrak, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014), which include individually or a combination of orthographic, phonetic, and semantic similarity between words. Previous approaches to cognate transliteration (Mulloni, 2007; Beinborn et al., 2013) suffer from the drawback that they require an existing list of cognates, which is infeasible for low-resource languages. In contrast, we automatically generate cognate tables by clustering words from existing lexical resources using a combination of similarity measures. Our produced cognate tables for Romance and Turkic languages are available for resear"
L18-1538,N01-1020,1,0.872905,"e table azj stol ? tuk tur uig stol ? üstel tablisa tablo ? tat östäl tablis uzn stol tablitsa Figure 1: Each row in the table is a cognate chain. The task of cognate chain completion is to fill in missing cells in the table. 1. Introduction Cognates are words in related languages that share a common origin. For example, the Italian cavallo and French cheval both originated from the Latin caballus. Besides being instrumental in historical linguistics, cognates find uses in many areas of NLP, including machine translation (Kondrak et al., 2003; Nakov and Tiedemann, 2012) and lexicon induction (Mann and Yarowsky, 2001). We define the task of cognate chain completion, shown in Figure 1. Given multi-way aligned cognate table, a cognate “chain” is a group of cognates across a language family (represented as a single row). Chains may have empty cells due to dictionary gaps, denoted by a ?, and the task is to predict these missing entries. Cognate chain completion is related to the task of cognate transliteration, except that words in related languages (within the same row) can contribute to the hypothesis of a cell. For low-resource languages, generating hypotheses for missing cognates has applications in align"
L18-1538,P07-3005,0,0.705762,"fvocabulary words in machine translation, it is often not necessary that these words be true cognates in the linguistic sense, i.e. they are descendants of a common ancestor (Ciobanu and Dinu, 2014). For example, names and loanwords are not technically considered cognates, though they behave as such. Rather, “cognates” only need to meet certain established criteria for cognacy (Kondrak, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014), which include individually or a combination of orthographic, phonetic, and semantic similarity between words. Previous approaches to cognate transliteration (Mulloni, 2007; Beinborn et al., 2013) suffer from the drawback that they require an existing list of cognates, which is infeasible for low-resource languages. In contrast, we automatically generate cognate tables by clustering words from existing lexical resources using a combination of similarity measures. Our produced cognate tables for Romance and Turkic languages are available for research purposes 1 . Using these cognate tables, we construct multi-way bitext and train character-based machine translation systems to transliterate cognates to fill in missing entries in the cognate chains. Finally, we eva"
L18-1538,P12-2059,0,0.0204519,"tabella tabla tabela taula tavolo tavola eng table table azj stol ? tuk tur uig stol ? üstel tablisa tablo ? tat östäl tablis uzn stol tablitsa Figure 1: Each row in the table is a cognate chain. The task of cognate chain completion is to fill in missing cells in the table. 1. Introduction Cognates are words in related languages that share a common origin. For example, the Italian cavallo and French cheval both originated from the Latin caballus. Besides being instrumental in historical linguistics, cognates find uses in many areas of NLP, including machine translation (Kondrak et al., 2003; Nakov and Tiedemann, 2012) and lexicon induction (Mann and Yarowsky, 2001). We define the task of cognate chain completion, shown in Figure 1. Given multi-way aligned cognate table, a cognate “chain” is a group of cognates across a language family (represented as a single row). Chains may have empty cells due to dictionary gaps, denoted by a ?, and the task is to predict these missing entries. Cognate chain completion is related to the task of cognate transliteration, except that words in related languages (within the same row) can contribute to the hypothesis of a cell. For low-resource languages, generating hypothese"
L18-1538,P03-1021,0,0.0615315,"Language Distance 0.0 rom fra ita cat por spa (c) Distances from Gray and Atkinson (2003) Figure 3: Distance between languages no distortion, since reordering should not occur during transliteration (Karimi et al., 2011). For each language pair, We generate a 10-best list of distinct hypotheses. While MT systems are generally evaluated on BLEU score (Papineni et al., 2002), it is not clear that BLEU is the best metric for evaluating transliterations: Beinborn et al. (2013) find that tuning on BLEU score made almost no difference in their system’s performance. Nevertheless, we tune using MERT (Och, 2003) with the standard Moses scripts. For each experiment in Figure 4, we report 1-best accuracy, 10-best accuracy (is the truth in the top 10 hypotheses?), and mean ∑n 1 reciprocal rank (MRR): MRR = n1 i=1 rank i Due to the way the bitext is constructed (i.e. the cross product of all words in a cognate cluster), the same source word often maps to different output words, e.g. src (por) associação associação tgt (ita) associamento associazione which makes this an inherently hard task for machine translation systems. Thus, to compute accuracy, we consider a hypothesis to be correct if it matches any"
L18-1538,P02-1040,0,0.104463,"ingle Linkage) 3.5 3.5 3.0 3.0 Average Distance Per Word (Single Linkage) 3.5 3.0 2.5 2.5 2.0 2.0 1.5 1.5 1.0 1.0 1.0 0.5 0.5 0.5 0.0 0.0 2.5 2.0 1.5 lat ita fra rom por spa cat (a) Romance Language Distance tur azj uzn uig tuk kaz tat (b) Turkic Language Distance 0.0 rom fra ita cat por spa (c) Distances from Gray and Atkinson (2003) Figure 3: Distance between languages no distortion, since reordering should not occur during transliteration (Karimi et al., 2011). For each language pair, We generate a 10-best list of distinct hypotheses. While MT systems are generally evaluated on BLEU score (Papineni et al., 2002), it is not clear that BLEU is the best metric for evaluating transliterations: Beinborn et al. (2013) find that tuning on BLEU score made almost no difference in their system’s performance. Nevertheless, we tune using MERT (Och, 2003) with the standard Moses scripts. For each experiment in Figure 4, we report 1-best accuracy, 10-best accuracy (is the truth in the top 10 hypotheses?), and mean ∑n 1 reciprocal rank (MRR): MRR = n1 i=1 rank i Due to the way the bitext is constructed (i.e. the cross product of all words in a cognate cluster), the same source word often maps to different output wo"
L18-1538,scherrer-sagot-2014-language,0,0.161819,"7717 Table 1: A translation pair extracted from Panlex and Wiktionary. Ins(a) = 1 − pA→B (NULL → a) (1) Del(a) = 1 − pA→B (a → NULL) (2) Sub(a, b) = 1 − pA→B (a → b) 2015). In addition, we retain a word’s part of speech5 . We preprocess the data by removing words in all caps (abbreviations) and words with spaces and symbols. Table 1 illustrates an example of a single translation pair extracted from the combination of PanLex and Wiktionary. 3. Cognate Clustering To generate multilingual cognate tables, we employ an automatic method of clustering words from our lexical resources. In contrast to Scherrer and Sagot (2014), who compare entire word lists to find possible cognates, we only consider two words to be cognates if they have the same English translation. Pivoting through English removes the need to compute a similarity score between every pair of words in every list, thus reducing the time complexity required to perform alignment. In addition, by introducing a strict semantic similarity constraint, we avoid clustering false cognates, which are orthographically similar by semantically distant. On each group of words with the same English translation, we perform single-linkage clustering, an agglomerativ"
L18-1538,P15-2111,1,0.900416,"Missing"
L18-1612,W14-1207,0,0.0129323,"a much, much larger scale, with the significant benefits of much greater novel semantic pair discovery (both via direct observation and via our transitive cluster and reordering models). In addition, we release a very large 329-language 21,000+ instance large public resource of analyzed compound words and components and statistical analyses of their processes across all languages. In terms of applications, handling compound words well has been shown to improve machine translation, e.g. into English (Koehn and Knight, 2003) and German (Stymne et al., 2013) and has helped simplify medical text (Abrahamsson et al., 2014). We expect that our very large scale publicly distributed compound-based translation dictionaries and associated generative and analytic models will be useful for out-of-vocabulary handling in downstream machine translation systems, especially for low-resource languages. t, b o, d n, s a i, s n, r r, l i, a k, l n, s l, t n, t k, h n, s i, t i, l a, n a, s s, r 6. Conclusion n, r n, p e, d While most languages exhibit broad-scale word formation via compounding, they often differ substantially in terms of the diverse processes by which words compound and novel concepts are realized via these c"
L18-1612,W14-5702,0,0.0158072,"f word compounds in 20 languages. The project seems to have stalled, and we were unable to access the data mentioned in their work. Our work encompasses a much larger set of languages (by a factor of 15x) and a much larger set of derived instances (even if their described database was actually available), and posits compound generation and analysis models absent from their work. While we used straightforward but effective compound splitting algorithms, many more complicated splitting methods have been proposed, e.g. using n-gram counts (Sornlertlamvanich and Tanaka, 1996), supervised methods (Clouet and Daille, 2014), and monolingual and bilingual corpora (Koehn and Knight, 2003; Macherey et al., 2011) and could be productively employed in extensions of our work. In contrast to several of these other works, the approach and analysis in our paper is simple yet effective in that it only 3879 Lang Concat DL Glue Common Glues af br ca co com cop crh cs dbl de dv ee el en eo es esu et eu fa fax ff fi fy ha haw ht hu ia inh io is ist it ja jbo jv ku kum kw ky la lad lb lv pl prg pro ps pt qu raj rap rm ro rup scn sco shn tpi tr uz vai vec vep vi zh 0.75 0.85 0.79 1.0 0.5 0.27 0.81 0.75 1.0 0.8 1.0 0.87 0.65 0.9"
L18-1612,I08-1053,1,0.87632,"of these compounds (e.g. syk = sick and hus = house). Using these dictionaries, we develop a multiiteration method for discovering both compound translation models or “recipes” motivated across multiple languages that can be used to construct or analyze new compound words that may not be in the dictionary. While there are many existing methods for compound splitting (e.g. Koehn and Knight (2003; Macherey et al. (2011)), we concern ourselves with compounds with two components, which can be combined by simple concatenation, optionally using a glue or filler string at the point of concatenation (Garera and Yarowsky, 2008), or by dropping the last character of the left component (henceforth drop-left). These three compounding processes productively cover a wide spectrum of our multilingual data and serve as an efficient foundation for training the semantic models of compounding in the absence of simple concatenation. 3875 1 2 github.com/wswu/worcomal www.wiktionary.org Concept ninety Monday December midnight Frenchman businessman pianist granddaughter queen zh nl fi hu nl th de no hu Left + 九 (nine) maan (moon) joulu (Jule) éj (night) frans (French) นั ก (person) klavier (piano) datter (daughter) király (king)"
L18-1612,guevara-etal-2006-morbo,0,0.0467341,"ter but an important semantic component (that would not normally be associated with a single character in an alphabetic or even syllabic writing system). For instance, the Chinese 杀⼈ murder = 杀害 murder + ⼈ person is reasonable, but ⾳ 乐 music = ⾳ 律 tuning + 乐 music is not. Despite this caveat, single character dropping is still a widely observed and productive compounding process in these languages. 5. Related Work Researchers have explored word compounding, though largely in the monolingual setting or on the order of a couple of languages. One multilingual effort similar to ours is MorBoComp (Guevara et al., 2006), a database of word compounds in 20 languages. The project seems to have stalled, and we were unable to access the data mentioned in their work. Our work encompasses a much larger set of languages (by a factor of 15x) and a much larger set of derived instances (even if their described database was actually available), and posits compound generation and analysis models absent from their work. While we used straightforward but effective compound splitting algorithms, many more complicated splitting methods have been proposed, e.g. using n-gram counts (Sornlertlamvanich and Tanaka, 1996), superv"
L18-1612,E03-1076,0,0.649336,"of constituent orders (e.g. sick+house vs. house+sick) and potentially a variety of compounding processes beyond simple concatenation (e.g. sykehus in Norse (no) is a compound of syk and hus with the insertion of an e). The following paper presents a massively cross-linguistic computational model of both compound morphology compositional processes and compound semantics on a scale of over 300 languages. Furthermore, the paper not only presents a derived analysis of the compounding process and semantics of compounds within a single language (e.g. German), as with much prior related work (e.g. Koehn and Knight (2003)), but does so via a joint model across essentially all the world’s languages with adequate dictionary resources, an unprecedentedly large scale for this class of research, and with significant additional synergistic multilingual power. In addition, the paper successfully applies these models and results to the valuable application of predicting novel translations of compound words, both to English (e.g. kórház → Compound Discovery We begin only with freely available multilingual translation dictionaries extracted from open-source Wiktionary2 , with the hope that they contain both substantial"
L18-1612,P11-1140,0,0.116516,"urce Wiktionary2 , with the hope that they contain both substantial examples of compounding in each language (e.g. sykehus (Norwegian) = hospital (English)) as well as translations of the constituents of these compounds (e.g. syk = sick and hus = house). Using these dictionaries, we develop a multiiteration method for discovering both compound translation models or “recipes” motivated across multiple languages that can be used to construct or analyze new compound words that may not be in the dictionary. While there are many existing methods for compound splitting (e.g. Koehn and Knight (2003; Macherey et al. (2011)), we concern ourselves with compounds with two components, which can be combined by simple concatenation, optionally using a glue or filler string at the point of concatenation (Garera and Yarowsky, 2008), or by dropping the last character of the left component (henceforth drop-left). These three compounding processes productively cover a wide spectrum of our multilingual data and serve as an efficient foundation for training the semantic models of compounding in the absence of simple concatenation. 3875 1 2 github.com/wswu/worcomal www.wiktionary.org Concept ninety Monday December midnight F"
L18-1612,C96-2208,0,0.0819365,"to ours is MorBoComp (Guevara et al., 2006), a database of word compounds in 20 languages. The project seems to have stalled, and we were unable to access the data mentioned in their work. Our work encompasses a much larger set of languages (by a factor of 15x) and a much larger set of derived instances (even if their described database was actually available), and posits compound generation and analysis models absent from their work. While we used straightforward but effective compound splitting algorithms, many more complicated splitting methods have been proposed, e.g. using n-gram counts (Sornlertlamvanich and Tanaka, 1996), supervised methods (Clouet and Daille, 2014), and monolingual and bilingual corpora (Koehn and Knight, 2003; Macherey et al., 2011) and could be productively employed in extensions of our work. In contrast to several of these other works, the approach and analysis in our paper is simple yet effective in that it only 3879 Lang Concat DL Glue Common Glues af br ca co com cop crh cs dbl de dv ee el en eo es esu et eu fa fax ff fi fy ha haw ht hu ia inh io is ist it ja jbo jv ku kum kw ky la lad lb lv pl prg pro ps pt qu raj rap rm ro rup scn sco shn tpi tr uz vai vec vep vi zh 0.75 0.85 0.79 1."
L18-1612,J13-4009,0,0.0245248,"del the compounding phenomenon in more depth as well as on a much, much larger scale, with the significant benefits of much greater novel semantic pair discovery (both via direct observation and via our transitive cluster and reordering models). In addition, we release a very large 329-language 21,000+ instance large public resource of analyzed compound words and components and statistical analyses of their processes across all languages. In terms of applications, handling compound words well has been shown to improve machine translation, e.g. into English (Koehn and Knight, 2003) and German (Stymne et al., 2013) and has helped simplify medical text (Abrahamsson et al., 2014). We expect that our very large scale publicly distributed compound-based translation dictionaries and associated generative and analytic models will be useful for out-of-vocabulary handling in downstream machine translation systems, especially for low-resource languages. t, b o, d n, s a i, s n, r r, l i, a k, l n, s l, t n, t k, h n, s i, t i, l a, n a, s s, r 6. Conclusion n, r n, p e, d While most languages exhibit broad-scale word formation via compounding, they often differ substantially in terms of the diverse processes by"
lin-etal-2010-new,N04-1043,0,\N,Missing
lin-etal-2010-new,sekine-dalwani-2010-ngram,1,\N,Missing
lin-etal-2010-new,C08-3010,1,\N,Missing
lin-etal-2010-new,1999.tc-1.8,0,\N,Missing
lin-etal-2010-new,J93-2004,0,\N,Missing
lin-etal-2010-new,S07-1044,0,\N,Missing
lin-etal-2010-new,N07-2005,1,\N,Missing
lin-etal-2010-new,J92-4003,0,\N,Missing
lin-etal-2010-new,J03-3005,0,\N,Missing
lin-etal-2010-new,P08-1068,0,\N,Missing
lin-etal-2010-new,P03-1059,0,\N,Missing
lin-etal-2010-new,A00-1031,0,\N,Missing
lin-etal-2010-new,P01-1005,0,\N,Missing
lin-etal-2010-new,W05-0603,0,\N,Missing
lin-etal-2010-new,P09-1116,1,\N,Missing
lin-etal-2010-new,Y09-1024,1,\N,Missing
lin-etal-2010-new,U08-1008,0,\N,Missing
lin-etal-2010-new,I05-2018,0,\N,Missing
lin-etal-2010-new,W04-3205,0,\N,Missing
lin-etal-2010-new,J03-3001,0,\N,Missing
lin-etal-2010-new,U07-1008,0,\N,Missing
N01-1020,W99-0626,0,\N,Missing
N01-1020,P97-1057,0,\N,Missing
N01-1020,J93-2003,0,\N,Missing
N01-1020,A00-1002,0,\N,Missing
N01-1020,P00-1037,0,\N,Missing
N01-1020,P98-1036,0,\N,Missing
N01-1020,C98-1036,0,\N,Missing
N01-1020,P98-1043,0,\N,Missing
N01-1020,C98-1043,0,\N,Missing
N01-1020,P93-1002,0,\N,Missing
N01-1020,J98-4003,0,\N,Missing
N01-1026,J99-1003,0,\N,Missing
N01-1026,jones-havrilla-1998-twisted,0,\N,Missing
N01-1026,H01-1035,1,\N,Missing
N01-1026,P00-1027,1,\N,Missing
N01-1026,C94-2178,0,\N,Missing
N01-1026,P00-1035,1,\N,Missing
N01-1026,N01-1006,1,\N,Missing
N01-1026,J90-2002,0,\N,Missing
N01-1026,P95-1033,0,\N,Missing
N01-1026,P93-1001,0,\N,Missing
N01-1026,J95-4004,0,\N,Missing
N01-1026,P93-1003,0,\N,Missing
N01-1026,J97-3002,0,\N,Missing
N01-1026,P94-1012,0,\N,Missing
N01-1026,P00-1056,0,\N,Missing
N03-1006,P98-1080,0,\N,Missing
N03-1006,C98-1077,0,\N,Missing
N03-1006,W02-1030,0,\N,Missing
N03-1006,W02-2006,1,\N,Missing
N03-2026,H01-1033,1,\N,Missing
N12-1033,P06-1005,1,0.825122,"Missing"
N12-1033,P05-1022,0,0.0203883,"pful; this-VBG also occurs in both cases. We need the deeper knowledge that a specific determiner is used as a complete NP. We evaluate three feature types that aim to capture such knowledge. In each case, we aggregate the feature counts over all the parse trees constituting a document. The feature value is the log-count of how often each feature occurs. To remove content information from the features, we preprocess the parse tree terminals: all non-style-word terminals are replaced with their spelling signature (see §6.2). C & J Reranking Features: We also extracted the reranking features of Charniak and Johnson (2005). These features were hand-crafted for reranking the output of a parser, but have recently been used for other NLP tasks (Post, 2011; Wong and Dras, 2011). They include lexicalized features for sub-trees and head-to-head dependencies, and aggregate features for conjunct parallelism and the degree of rightbranching. We get the features using another script from Post.9 While TSG fragments tile a parse tree into a few useful fragments, C & J features can produce thousands of features per sentence, and are thus much more computationally-demanding. CFG Rules: 7 We include a feature for every unique"
N12-1033,2008.amta-papers.4,0,0.0441715,"ART-system stopwords (following Tomokiyo and Jones (2001)). Latin abbreviations are i.e., e.g., etc., c.f., et or al. 7 E.g., signature ‘LC-ing’ means lower-case, ending in ing. These are created via a script included with the Berkeley parser. VP ... NP NP PRP VBD DT VBG this using we did ... Figure 2: Motivating deeper syntactic features: The shaded TSG fragment indicates native English, but is not directly encoded in Bow, Style, nor standard CFG-rules. lometry, Post (2011) uses them to predict sentence grammaticality (i.e. detecting pseudo-sentences following Okanohara and Tsujii (2007) and Cherry and Quirk (2008)). We use Post’s TSG training settings and his public code.8 We parse with the TSG grammar and extract the fragments as features. We also follow Post by having features for aggregate TSG statistics, e.g., how many fragments are of a given size, tree-depth, etc. These syntactic meta-features are somewhat similar to the manually-defined stylometric features of Stamatatos et al. (2001). Style features are likewise unhelpful; this-VBG also occurs in both cases. We need the deeper knowledge that a specific determiner is used as a complete NP. We evaluate three feature types that aim to capture such"
N12-1033,W07-1604,0,0.0280403,"ative vs. Non-Native English We introduce the task of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We 329 then built a list of common first names of English speakers via the top 150 male and female names from the U.S. census.3 If the first author of a paper has an English first name and English-speakingcountry affiliation, we mark as NES.4 If none of the authors have an English first name nor an Englishspeaking-country affi"
N12-1033,W10-4236,0,0.0100164,"ence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content and its meta-data such as author names and publication venues. Johri et al. (2011) use per-author topic models to assess the nature of collaboration in a particular article (e.g., apprenticeship or synergy). One of the tasks in Sarawgi et al. (2011) concerned predicting gender in scientific writing, but they use a corpus of only ten “highly established” authors and make the prediction using twenty papers for each. Finally, Dale and Kilgarriff (2010) initiated a shared task on automatic editing of scientific papers written by non-native speakers, with the objective of developing “tools which can help non-native speakers of English (NNSs) (and maybe some native ones) write academic English prose of the kind that helps a paper get accepted.” Lexical and pragmatic choices in academic writing have also been analyzed within the applied linguistics community (Myers, 1989; Vassileva, 1998). 3 ACL Dataset and Preprocessing We use papers from the ACL Anthology Network (Radev et al., 2009b, Release 2011) and exploit its manually-curated meta-data s"
N12-1033,P11-1137,0,0.0241915,"ttributes of documents from the style of the writing. In some domains, statistical techniques have successfully deduced author identity (Mosteller and Wallace, 1984), gender (Koppel et al., 2003), native language (Koppel et al., 2005), and even whether an author has dementia (Le et al., 2011). Stylometric analysis is important to marketers, analysts and social scientists because it provides demographic data directly from raw text. There has been growing interest in applying stylometry to the content generated by users of Internet applications, e.g., detecting author ethnicity in social media (Eisenstein et al., 2011; Rao et al., 2011), or whether someone is writing deceptive online reviews (Ott et al., 2011). We evaluate stylometric techniques in the novel domain of scientific writing. Science is a difficult domain; authors are encouraged, often explicitly by reviewers/submission-guidelines, to comply with normative practices in style, spelling and grammar. Moreover, topical clues are less salient than in domains like social media. Success in this challenging domain can bring us closer to correctly analyzing the huge volumes of online text that are currently unmarked for useful author attributes such as"
N12-1033,W07-1607,0,0.0665596,"Missing"
N12-1033,C04-1088,0,0.0676934,"r other NLP tasks (Post, 2011; Wong and Dras, 2011). They include lexicalized features for sub-trees and head-to-head dependencies, and aggregate features for conjunct parallelism and the degree of rightbranching. We get the features using another script from Post.9 While TSG fragments tile a parse tree into a few useful fragments, C & J features can produce thousands of features per sentence, and are thus much more computationally-demanding. CFG Rules: 7 We include a feature for every unique, single-level context-free-grammar (CFG) rule application in a paper (following Baayen et al. (1996), Gamon (2004), Hirst and Feiguina (2007), Wong and Dras (2011)). The Figure 2 tree would have features: NP¢PRP, NP¢DT, DT¢this, etc. Such features do capture that a determiner was used as an NP, but they do not jointly encode which determiner was used. This is an important omission; we’ll see that other determiners acting as stand-alone NPs indicate non-native writing (e.g., the word that, see §7.2). TSG Fragments: A tree-substitution grammar is a generalization of CFGs that allow rewriting to tree fragments rather than sequences of non-terminals (Joshi and Schabes, 1997). Figure 2 gives the example NP¢(DT"
N12-1033,N10-1019,0,0.0171364,"entific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We 329 then built a list of common first names of English speakers via the top 150 male and female names from the U.S. census.3 If the first author of a paper has an English first name and English-speakingcountry affiliation, we mark as NES.4 If none of the authors have an English first name nor an Englishspeaking-country affiliation, we mark as NNS. We use this rule to label our development an"
N12-1033,D08-1038,0,0.0300691,"Hill and Provost (2003) predict author identities. They ignore the article body and instead consider (a) potential self-citations and (b) similarity between the article’s citation list and the citation lists of known papers. Radev et al. (2009a) perform a bibliometric analysis of computational linguistics. Teufel and Moens (2002) and Qazvinian and Radev (2008) summarize scientific articles, the latter by automatically finding and filtering sentences in other papers that cite the target article. Our system does not consider citations; it is most 328 similar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content and its meta-data such as author names and publication venues. Johri et al. (2011) use per-author topic models to assess the nature of collaboration in a particular article (e.g., apprenticeship or synergy). One of the tasks in Sarawgi et al. (2011) concerned predicting gender in scient"
N12-1033,W11-1516,0,0.0839927,"ractices in style, spelling and grammar. Moreover, topical clues are less salient than in domains like social media. Success in this challenging domain can bring us closer to correctly analyzing the huge volumes of online text that are currently unmarked for useful author attributes such as gender and native-language. Yet science is more than just a good steppingstone for stylometry; it is an important area in itself. Systems for scientific stylometry would give sociologists new tools for analyzing academic communities, and new ways to resolve the nature of collaboration in specific articles (Johri et al., 2011). Authors might also use these tools, e.g., to help ensure a consistent style in multi-authored papers (Glover and Hirst, 1995), or to determine sections of a paper needing revision. 327 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 327–337, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics The contributions of our paper include: New Stylometric Tasks: We predict whether a paper is written: (1) by a native or non-native speaker, (2) by a male or female, and (3) in the style of"
N12-1033,P07-1010,0,0.0443822,"st is the standard set of 524 SMART-system stopwords (following Tomokiyo and Jones (2001)). Latin abbreviations are i.e., e.g., etc., c.f., et or al. 7 E.g., signature ‘LC-ing’ means lower-case, ending in ing. These are created via a script included with the Berkeley parser. VP ... NP NP PRP VBD DT VBG this using we did ... Figure 2: Motivating deeper syntactic features: The shaded TSG fragment indicates native English, but is not directly encoded in Bow, Style, nor standard CFG-rules. lometry, Post (2011) uses them to predict sentence grammaticality (i.e. detecting pseudo-sentences following Okanohara and Tsujii (2007) and Cherry and Quirk (2008)). We use Post’s TSG training settings and his public code.8 We parse with the TSG grammar and extract the fragments as features. We also follow Post by having features for aggregate TSG statistics, e.g., how many fragments are of a given size, tree-depth, etc. These syntactic meta-features are somewhat similar to the manually-defined stylometric features of Stamatatos et al. (2001). Style features are likewise unhelpful; this-VBG also occurs in both cases. We need the deeper knowledge that a specific determiner is used as a complete NP. We evaluate three feature ty"
N12-1033,P11-1032,0,0.0408499,"uccessfully deduced author identity (Mosteller and Wallace, 1984), gender (Koppel et al., 2003), native language (Koppel et al., 2005), and even whether an author has dementia (Le et al., 2011). Stylometric analysis is important to marketers, analysts and social scientists because it provides demographic data directly from raw text. There has been growing interest in applying stylometry to the content generated by users of Internet applications, e.g., detecting author ethnicity in social media (Eisenstein et al., 2011; Rao et al., 2011), or whether someone is writing deceptive online reviews (Ott et al., 2011). We evaluate stylometric techniques in the novel domain of scientific writing. Science is a difficult domain; authors are encouraged, often explicitly by reviewers/submission-guidelines, to comply with normative practices in style, spelling and grammar. Moreover, topical clues are less salient than in domains like social media. Success in this challenging domain can bring us closer to correctly analyzing the huge volumes of online text that are currently unmarked for useful author attributes such as gender and native-language. Yet science is more than just a good steppingstone for stylometry;"
N12-1033,W02-1011,0,0.0129865,"atures that have been used in stylometry, ranging from early manual selection of potentially discriminative words, to approaches based on automated text categorization (Sebastiani, 2002). We use the following three feature classes; the particular features were chosen based on development experiments. 330 6.1 Bow Features A variety of “discouraging results” in the text categorization literature have shown that simple bag-ofwords (Bow) representations usually perform better than “more sophisticated” ones (e.g. using syntax) (Sebastiani, 2002). This was also observed in sentiment classification (Pang et al., 2002). One key aim of our research is to see whether this is true of scientific stylometry. Our Bow representation uses a feature for each unique lower-case word-type in an article. We also preprocess papers by making all digits ’0’. Normalizing digits and filtering capitalized words helps ensure citations and named-entities are excluded from our features. The feature value is the log-count of how often the corresponding word occurs in the document. 6.2 Style Features While text categorization relies on keywords, stylometry focuses on topic-independent measures like function word frequency (Mostell"
N12-1033,P06-1055,0,0.02213,"mation). In case the text was garbled, we then filtered the first 3 lines from every file and any line with an ’@’ symbol (which might be part of an affiliation). We remove footers like Proceedings of ..., table/figure captions, and any lines with non-ASCII characters (e.g. math equations). Papers are then parsed via the Berke1 2 Via the open-source utility pdftotext Splitter from cogcomp.cs.illinois.edu/page/tools Task NativeL Venue Gender Training Set: Strict Lenient 2127 3963 2484 3991 2125 3497 Dev Set 450 400 400 Test Set 477 421 409 Table 1: Number of documents for each task ley parser (Petrov et al., 2006), and part-of-speech (PoS) tagged using CRFTagger (Phan, 2006). Training sets always comprise papers from 20012007, while test sets are created by randomly shuffling the 2008-2009 portion and then dividing it into development/test sets. We also use papers from 1990-2000 for experiments in §7.3 and §7.4. 4 Stylometric Tasks Each task has both a Strict training set, using only the data for which we are most confident in the labels (as described below), and a Lenient set, which forcibly assigns every paper in the training period to some class (Table 1). All test papers are annotated using a Stric"
N12-1033,P09-2012,1,0.84112,"s is an important omission; we’ll see that other determiners acting as stand-alone NPs indicate non-native writing (e.g., the word that, see §7.2). TSG Fragments: A tree-substitution grammar is a generalization of CFGs that allow rewriting to tree fragments rather than sequences of non-terminals (Joshi and Schabes, 1997). Figure 2 gives the example NP¢(DT this). This fragment captures both the identity of the determiner and its syntactic function as an NP, as desired. Efficient Bayesian procedures have recently been developed that enable the training of large-scale probabilistic TSG grammars (Post and Gildea, 2009; Cohn et al., 2010). While TSGs have not been used previously in sty331 Experiments and Results We take the minority class as the positive class: NES for NativeL, top-tier for Venue and female for Gender, and calculate the precision/recall of these classes. We tune three hyperparameters for F1score on development data: (1) the SVM regularization parameter, (2) the threshold for classifying an instance as positive (using the signed hyperplanedistance as the score), and (3) for transductive training (§5), the fraction of unlabeled data to label as positive. Statistical significance on held-out"
N12-1033,P11-2038,1,0.803867,"ies a noun. The Bow features are clearly unhelpful: this occurs in both cases. The 6 The stopword list is the standard set of 524 SMART-system stopwords (following Tomokiyo and Jones (2001)). Latin abbreviations are i.e., e.g., etc., c.f., et or al. 7 E.g., signature ‘LC-ing’ means lower-case, ending in ing. These are created via a script included with the Berkeley parser. VP ... NP NP PRP VBD DT VBG this using we did ... Figure 2: Motivating deeper syntactic features: The shaded TSG fragment indicates native English, but is not directly encoded in Bow, Style, nor standard CFG-rules. lometry, Post (2011) uses them to predict sentence grammaticality (i.e. detecting pseudo-sentences following Okanohara and Tsujii (2007) and Cherry and Quirk (2008)). We use Post’s TSG training settings and his public code.8 We parse with the TSG grammar and extract the fragments as features. We also follow Post by having features for aggregate TSG statistics, e.g., how many fragments are of a given size, tree-depth, etc. These syntactic meta-features are somewhat similar to the manually-defined stylometric features of Stamatatos et al. (2001). Style features are likewise unhelpful; this-VBG also occurs in both c"
N12-1033,C08-1087,0,0.0181928,"re; citation analysis is a well-known bibliometric approach for ranking authors and papers (Borgman and Furner, 2001). Bibliometry and stylometry can share goals but differ in techniques. For example, in a work questioning the blindness of double-blind reviewing, Hill and Provost (2003) predict author identities. They ignore the article body and instead consider (a) potential self-citations and (b) similarity between the article’s citation list and the citation lists of known papers. Radev et al. (2009a) perform a bibliometric analysis of computational linguistics. Teufel and Moens (2002) and Qazvinian and Radev (2008) summarize scientific articles, the latter by automatically finding and filtering sentences in other papers that cite the target article. Our system does not consider citations; it is most 328 similar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content and its meta-data such as author"
N12-1033,W09-3607,0,0.0268762,"olling for paper venue and origin. 2 Related Work Bibliometrics is the empirical analysis of scholarly literature; citation analysis is a well-known bibliometric approach for ranking authors and papers (Borgman and Furner, 2001). Bibliometry and stylometry can share goals but differ in techniques. For example, in a work questioning the blindness of double-blind reviewing, Hill and Provost (2003) predict author identities. They ignore the article body and instead consider (a) potential self-citations and (b) similarity between the article’s citation list and the citation lists of known papers. Radev et al. (2009a) perform a bibliometric analysis of computational linguistics. Teufel and Moens (2002) and Qazvinian and Radev (2008) summarize scientific articles, the latter by automatically finding and filtering sentences in other papers that cite the target article. Our system does not consider citations; it is most 328 similar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers."
N12-1033,P10-2008,0,0.0633266,"pendent measures like function word frequency (Mosteller and Wallace, 1984), sentence length (Yule, 1939), and PoS (Hirst and Feiguina, 2007). We define a style-word to be: (1) punctuation, (2) a stopword, or (3) a Latin abbreviation.6 We create Style features for all unigrams and bigrams, replacing non-style-words separately with both PoS-tags and spelling signatures.7 Each feature is an N-gram, the value is its log-count in the article. We also include stylistic meta-features such as mean-words-per-sentence and mean-word-length. 6.3 Syntax Features Unlike recent work using generative PCFGs (Raghavan et al., 2010; Sarawgi et al., 2011), we use syntax directly as features in discriminative models, which can easily incorporate arbitrary and overlapping syntactic clues. For example, we will see that one indicator of native text is the use of certain determiners as stand-alone noun phrases (NPs), like this in Figure 2. This contrasts with a proposed non-native phrase, “this/DT growing/VBG area/NN,” where this instead modifies a noun. The Bow features are clearly unhelpful: this occurs in both cases. The 6 The stopword list is the standard set of 524 SMART-system stopwords (following Tomokiyo and Jones (20"
N12-1033,W11-0310,0,0.510526,"ilar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content and its meta-data such as author names and publication venues. Johri et al. (2011) use per-author topic models to assess the nature of collaboration in a particular article (e.g., apprenticeship or synergy). One of the tasks in Sarawgi et al. (2011) concerned predicting gender in scientific writing, but they use a corpus of only ten “highly established” authors and make the prediction using twenty papers for each. Finally, Dale and Kilgarriff (2010) initiated a shared task on automatic editing of scientific papers written by non-native speakers, with the objective of developing “tools which can help non-native speakers of English (NNSs) (and maybe some native ones) write academic English prose of the kind that helps a paper get accepted.” Lexical and pragmatic choices in academic writing have also been analyzed within the applied linguis"
N12-1033,C08-1109,0,0.0119864,"sk of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We 329 then built a list of common first names of English speakers via the top 150 male and female names from the U.S. census.3 If the first author of a paper has an English first name and English-speakingcountry affiliation, we mark as NES.4 If none of the authors have an English first name nor an Englishspeaking-country affiliation, we mark as NNS. We use this rule to label our"
N12-1033,J02-4002,0,0.0214971,"alysis of scholarly literature; citation analysis is a well-known bibliometric approach for ranking authors and papers (Borgman and Furner, 2001). Bibliometry and stylometry can share goals but differ in techniques. For example, in a work questioning the blindness of double-blind reviewing, Hill and Provost (2003) predict author identities. They ignore the article body and instead consider (a) potential self-citations and (b) similarity between the article’s citation list and the citation lists of known papers. Radev et al. (2009a) perform a bibliometric analysis of computational linguistics. Teufel and Moens (2002) and Qazvinian and Radev (2008) summarize scientific articles, the latter by automatically finding and filtering sentences in other papers that cite the target article. Our system does not consider citations; it is most 328 similar to work that uses raw article text. Hall et al. (2008) build per-year topic models over scientific literature to track the evolution of scientific ideas. Gerrish and Blei (2010) assess the influence of individual articles by modeling their impact on the content of future papers. Yogatama et al. (2011) predict whether a paper will be cited based on both its content a"
N12-1033,N01-1031,0,0.794482,"using a Strict rule. While our approaches for automatically-assigning labels can be coarse, they allow us to scale our analysis to a realistic crosssection of academic papers, letting us discover some interesting trends. 4.1 NativeL: Native vs. Non-Native English We introduce the task of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We 329 then built a list of common first names of English speakers via the top 150 male and fema"
N12-1033,W07-0602,0,0.12042,", which forcibly assigns every paper in the training period to some class (Table 1). All test papers are annotated using a Strict rule. While our approaches for automatically-assigning labels can be coarse, they allow us to scale our analysis to a realistic crosssection of academic papers, letting us discover some interesting trends. 4.1 NativeL: Native vs. Non-Native English We introduce the task of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predomina"
N12-1033,D11-1148,0,0.513421,"very paper in the training period to some class (Table 1). All test papers are annotated using a Strict rule. While our approaches for automatically-assigning labels can be coarse, they allow us to scale our analysis to a realistic crosssection of academic papers, letting us discover some interesting trends. 4.1 NativeL: Native vs. Non-Native English We introduce the task of predicting whether a scientific paper is written by a native English speaker (NES) or non-native speaker (NNS). Prior work has mostly made this prediction in learner corpora (Koppel et al., 2005; Tsur and Rappoport, 2007; Wong and Dras, 2011), although there have been attempts in elicited speech transcripts (Tomokiyo and Jones, 2001) and e-mail (Estival et al., 2007). There has also been a large body of work on correcting errors in non-native writing, with a specific focus on difficulties in preposition and article usage (Han et al., 2006; Chodorow et al., 2007; Felice and Pulman, 2007; Tetreault and Chodorow, 2008; Gamon, 2010). We annotate papers using two pieces of associated meta-data: (1) author first names and (2) countries of affiliation. We manually marked each country for whether English is predominantly spoken there. We"
N12-1033,D11-1055,0,0.0516188,"Missing"
N12-1033,J00-4001,0,\N,Missing
N13-1121,W12-2108,1,0.801101,"Our readily-replicable approach and publiclyreleased clusters are shown to be remarkably effective and versatile, substantially outperforming state-of-the-art approaches and human accuracy on each of the tasks studied. 1 Here, we propose and evaluate classifiers that better exploit the attributes that users explicitly provide in their user profiles, such as names (e.g., first names like Mary, last names like Smith) and locations (e.g., Brasil). Such attributes have previously been used as “profile features” in supervised user classifiers (Pennacchiotti and Popescu, 2011; Burger et al., 2011; Bergsma et al., 2012). There are several motivations for exploiting these data. Often the only information available for a user is a name or location (e.g. for a new user account). Profiles also provide an orthogonal or complementary source of information to a user’s social network and textual content; gains based on profiles alone should therefore add to gains based on other data. The decisions of profile-based classifiers could also be used to bootstrap training data for other classifiers that use complementary features. Introduction There is growing interest in automatically classifying users in social media by"
N13-1121,N10-1102,0,0.062149,"ntry: 53 possible countries United States courtland dante United States tinas twin Brazil thamires gomez Denmark marte clason Lang. ID: 9 confusable languages Bulgarian valentina getova Russian borisenko yana Bulgarian NONE Ukrainian andriy kupyna Farsi kambiz barahouei Urdu musadiq sanwal Ethnicity: 13 European ethnicities German dennis hustadt Dutch bernhard hofstede French david coste Swedish mattias bjarsmyr Portuguese helder costa Race: black or white black kerry swain black darrell foskey white ty j larocca black james n jones white sean p farrell of-the-art in detecting name ethnicity (Bhargava and Kondrak, 2010). We add special begin/end characters to the attributes to mark the prefix and suffix positions. We also use a smoothed log-count; we found this to be most effective in preliminary work. Cluster Features (Clus) indicate the soft-cluster memberships of the attributes. We have features for the top-2, 5, and 20 most similar clusters in the C 50 , C 200 , and C 1000 clusterings, respectively. Like Lin and Wu (2009), we “side-step the matter of choosing the optimal value k in k-means” by using features from clusterings at different granularities. Our feature dimensions correspond to cluster IDs; fe"
N13-1121,J92-4003,0,0.280084,"Missing"
N13-1121,D11-1120,0,0.727262,"Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, David Yarowsky Department of Computer Science and Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD 21218, USA shane.a.bergsma@gmail.com, mdredze@cs.jhu.edu, vandurme@cs.jhu.edu, taw@jhu.edu, yarowsky@cs.jhu.edu Abstract and Dredze, 2011), and sociolinguistic phenomena (Eisenstein et al., 2011). Classifiers for user properties often rely on information from a user’s social network (Jernigan and Mistree, 2009; Sadilek et al., 2012) or the textual content they generate (Pennacchiotti and Popescu, 2011; Burger et al., 2011). Hidden properties of social media users, such as their ethnicity, gender, and location, are often reflected in their observed attributes, such as their first and last names. Furthermore, users who communicate with each other often have similar hidden properties. We propose an algorithm that exploits these insights to cluster the observed attributes of hundreds of millions of Twitter users. Attributes such as user names are grouped together if users with those names communicate with other similar users. We separately cluster millions of unique first names, last names, and userprovided locatio"
N13-1121,J90-1003,0,0.0521202,"rm as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to a unique string (target location), corresponding to a structured location object that incl"
N13-1121,D10-1124,0,0.208983,"Missing"
N13-1121,P11-1137,0,0.267211,"Missing"
N13-1121,P09-1080,1,0.806767,"ies, and (2) to predict user properties based on friendships. Friendship prediction systems (e.g. Facebook’s friend suggestion tool) use features such as whether both people are computer science majors (Taskar et al., 2003) or whether both are at the same location (Crandall et al., 2010; Sadilek et al., 2012). The inverse problem has been explored in the prediction of a user’s location given the location of their peers (Backstrom et al., 2010; Cho et al., 2011; Sadilek et al., 2012). Jernigan and Mistree (2009) predict a user’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al."
N13-1121,P90-1034,0,0.466102,"75 70 65 60 10 100 1000 10000 Number of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if fo"
N13-1121,W10-1908,0,0.0181357,"iend suggestion tool) use features such as whether both people are computer science majors (Taskar et al., 2003) or whether both are at the same location (Crandall et al., 2010; Sadilek et al., 2012). The inverse problem has been explored in the prediction of a user’s location given the location of their peers (Backstrom et al., 2010; Cho et al., 2011; Sadilek et al., 2012). Jernigan and Mistree (2009) predict a user’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are"
N13-1121,P08-1068,0,0.0236334,"ased on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes"
N13-1121,P09-1116,0,0.499477,"s). We consider each useruser link as a single event; we count it once no matter how often two specific users interact. We extract 436M user-user links in total. Attribute-Attribute Pairs We use our profile data to map each user-user link to an attribute-attribute pair; we separately count each pair of first names, last names, and locations. For example, the firstname pair (henrik, fredrik) occurs 181 times. Rather than using the raw count, we calculate the association between attributes a1 and a2 via their pointwise mutual information (PMI), following prior work in distributional clustering (Lin and Wu, 2009): PMI(a1 , a2 ) = log P(a1 , a2 ) P(a1 )P(a2 ) PMI essentially normalizes the co-occurrence by what we would expect if the attributes were independently distributed. We smooth the PMI by adding a count of 0.5 to all co-occurrence events. The most highly-associated name attributes reflect similarities in ethnicity and gender (Table 2). The most highly-ranked associates for locations are often nicknames and alternate/misspellings of those locations. For example, the locations charm city, bmore, balto, westbaltimore, b a l t i m o r e, baltimoreee, and balitmore each have the U.S. city of baltimo"
N13-1121,P98-2127,0,0.151694,"of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to a unique str"
N13-1121,N04-1043,0,0.0365937,"er’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explos"
N13-1121,D10-1021,0,0.0278599,"or distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network, a large amount of work has focused on inferring user properties based on the content they generate (e.g. Burger and Henderson (2006), Schler et al. (2006), Rao et al. (2010), Mukherjee and Liu (2010), Pennacchiotti and Popescu (2011), Burger et al. (2011), Van Durme (2012)). 9 Conclusion and Future Work We presented a highly effective and readily replicable algorithm for generating language resources from Twitter communication patterns. We clustered user attributes based on both the communication of users with those attributes as well as substring similarity. Systems using our clusters significantly outperform state-of-the-art algorithms on each of the tasks investigated, and exceed human performance on each task as well. The power and versatility of our clusters is exemplified by the fac"
N13-1121,P93-1024,0,0.430574,"100 1000 10000 Number of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to"
N13-1121,C10-2112,0,0.0212861,"luding namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network, a large amount of work has focused on inferring user properties based on the content they generate (e.g. Burger and Henderson (2006), Schler et al. (2006), Rao et al. (2010), Mukherjee and Liu (2010), Pennacchiotti and Popescu (2011), Burger et al. (2011), Van Durme (2012)). 9 Conclusion and Future Work We presented a highly effective and readily replicable algorithm for generating language resources from Twitter comm"
N13-1121,W09-1119,0,0.00772601,"f their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting u"
N13-1121,D12-1137,0,0.0205995,"ts, and we average their results to give a “Human” performance number. The two humans are experts in 1014 cali baby on the court macapá ap NONE NONE edinburgh blagoevgrad ternopil NONE jammu Table 5: Examples of class (left) and input (names, locations) for some of our evaluation tasks. this domain and have very wide knowledge of global names and locations. 5.2 Twitter Applications Country A number of recent papers have considered the task of predicting the geolocation of users, using both user content (Cheng et al., 2010; Eisenstein et al., 2010; Hecht et al., 2011; Wing and Baldridge, 2011; Roller et al., 2012) and social network (Backstrom et al., 2010; Sadilek et al., 2012). Here, we first predict user location at the level of the user’s location country. To our knowledge, we are the first to exploit user locations and names for this prediction. For this task, we obtain gold data from the portion of Twitter users who have GPS enabled (geocoded tweets). We were able to obtain a very large number of gold instances for this task, so selected only 10K for testing, 10K for development, and retained the remaining 782K for training. Language ID Identifying the language of users is an important prerequisi"
N13-1121,N12-1052,0,0.0235863,"of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network,"
N13-1121,P10-1040,0,0.0155684,"rowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside f"
N13-1121,D12-1005,1,0.555299,"Missing"
N13-1121,P11-1096,0,0.0180694,"s from each of the test sets, and we average their results to give a “Human” performance number. The two humans are experts in 1014 cali baby on the court macapá ap NONE NONE edinburgh blagoevgrad ternopil NONE jammu Table 5: Examples of class (left) and input (names, locations) for some of our evaluation tasks. this domain and have very wide knowledge of global names and locations. 5.2 Twitter Applications Country A number of recent papers have considered the task of predicting the geolocation of users, using both user content (Cheng et al., 2010; Eisenstein et al., 2010; Hecht et al., 2011; Wing and Baldridge, 2011; Roller et al., 2012) and social network (Backstrom et al., 2010; Sadilek et al., 2012). Here, we first predict user location at the level of the user’s location country. To our knowledge, we are the first to exploit user locations and names for this prediction. For this task, we obtain gold data from the portion of Twitter users who have GPS enabled (geocoded tweets). We were able to obtain a very large number of gold instances for this task, so selected only 10K for testing, 10K for development, and retained the remaining 782K for training. Language ID Identifying the language of users is a"
N13-1121,C98-2122,0,\N,Missing
N19-1009,E17-2002,0,0.0341317,"n is conducted in the reading adaptation scenario; language adaptation with unseen target speakers is addressed in Section 8. Beyond answering these questions, this investigation reveals more information about the utility of the proposed auxiliary objectives in different scenarios. Phonology & Geography We test across a number of evaluation languages (c.f. Table 1) by determining, for each evaluation language, groups of pretraining languages that are similar to the evaluation languages in different ways. In order to determine language similarity in a principled way we used URIEL and lang2vec (Littell et al., 2017) to produce feature vectors for each language based on information from several linguistic resources before calculating their cosine similarity. For each language we used two feature vectors. The first is a concatenation of the lang2vec phonology average and inventory average vectors, characterizing phonological properties and phonetic inventory. The second represents geographic location. We denote these two groups P HON /I NV and G EO respectively.9 Geographic proximity may Massively multilingual model As a further point of comparison, we pretrain a model on around 100 languages (denoted 100-"
N19-1009,P07-1015,0,0.0566249,"re very good (VG), good (G), okay (O), not okay (NO). SB Quechua denotes South Bolivian Quechua. We scraped the data that forms the CMU Wilderness dataset, using a freely available script.4 This dataset consists of dramatized readings of the Bible in hundreds of languages. Each reading is ascribed a rating based on alignment quality which fits into one of these classes: very good, good, okay, and not okay. The script used to preprocess the data uses a universal pronunciation module in Festival (Taylor et al., 1998)5 to produce pronunciation lexicons using an approach based on that of UniTran (Yoon et al., 2007), which we use to create phonemic transcriptions. 3.1 Aymara (ayr) SB Quechua (quh) Auxiliary Training Objectives In addition to scaling ASR training to 100 languages, a key contribution of our work is the use of a context-independent phoneme objective paired with a language-adversarial classification objective in a end-to-end grapheme-based neural network, as illustrated in Figure 1. 4.1 Baseline Model Our experiments are conducted within the framework of a hybrid CTC-attention end-to-end neural model using ESPnet (Watanabe et al., 2017b), which uses an encoder-decoder architecture implemente"
N19-1009,L16-1521,0,\N,Missing
P00-1016,W99-0621,0,\N,Missing
P00-1016,J93-2004,0,\N,Missing
P00-1016,E99-1023,0,\N,Missing
P00-1016,A97-1051,0,\N,Missing
P00-1016,A88-1019,0,\N,Missing
P00-1016,C92-3150,0,\N,Missing
P00-1016,J95-4004,0,\N,Missing
P00-1016,P99-1009,1,\N,Missing
P00-1016,P98-1034,0,\N,Missing
P00-1016,C98-1034,0,\N,Missing
P00-1016,P96-1042,0,\N,Missing
P00-1027,C00-1042,0,\N,Missing
P00-1027,A97-1016,0,\N,Missing
P00-1027,P00-1035,1,\N,Missing
P00-1027,J01-2001,0,\N,Missing
P00-1027,W99-0703,0,\N,Missing
P00-1035,W99-0606,0,\N,Missing
P00-1035,J93-2006,0,\N,Missing
P00-1035,W96-0213,0,\N,Missing
P00-1035,A00-2013,0,\N,Missing
P00-1035,A88-1019,0,\N,Missing
P00-1035,P98-2251,0,\N,Missing
P00-1035,C98-2246,0,\N,Missing
P00-1035,J95-4004,0,\N,Missing
P00-1035,P90-1031,0,\N,Missing
P00-1035,A00-1031,0,\N,Missing
P00-1035,P93-1034,0,\N,Missing
P00-1035,P98-1029,0,\N,Missing
P00-1035,C98-1029,0,\N,Missing
P00-1035,W99-0612,1,\N,Missing
P04-3007,W97-0213,0,0.0763477,"ise from hit’s common 2 For our experiments, English words occurring in at least 15 distinct source dictionaries were considered. 3 Again, the threshold for synonyms was 10 and 5 respectively for per-dictionary and per-language coentry counts. Figure 3: 6 Induced sense hierarchy for the word “vital” Related Work There is a distinguished history of research extracting lexical semantic relationships from bilingual dictionaries (Copestake et al., 1995; Chen and Chang, 1998). There is also a longstanding goal of mapping translations and senses in multiple languages in a linked ontology structure (Resnik and Yarowsky, 1997; Risk, 1989; Vossen, 1998). The recent work of Ploux and Ji (2003) has some similarities to the techniques presented here in that it considers topological properties of the graph of synonymy relationships between words. The current paper can be distinguished on a number of dimensions, including our much greater range of participating languages, and the fundamental algorithmic linkage between multilingual translation distributions and monolingual synonymy clusters. 4 In both “vital” and “strike” examples, the rendered hierarchical clusterings were pruned (automatically) in order to fit in this"
P04-3007,J98-1003,0,\N,Missing
P04-3007,J03-2001,0,\N,Missing
P04-3014,J93-2003,0,0.00575353,"or language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distance movement. In contrast, more syntactically informed approaches have been constrained by the often weak syntactic correspondences typical of real-world parallel texts, and by the difficulty of finding or inducing syntactic parsers fo"
P04-3014,dorr-etal-2002-duster,0,0.0198874,"kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai plutonium ’s use nuclear weapons manufacture to is Figure 2: Original Hindi-English sentence pair with gold-standard word-alignments. S NP PP English’: NP NP plutonium of the use nuclear VP VP VP weapons manufacture to is Hindi: plutoniyama kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai plutonium ’s use nuclear weapons manufacture to is Figure 3: Transformed Hindi-English0 sentence pair with gold-standard word-alignments. Rotated nodes are marked with an arc. linguistic structural divergences, such as the DUSTer system (Dorr et al., 2002). While the focus on major classes of structural variation such as manner-ofmotion verb-phrase transformations have facilitated both transfer and generation in machine translation, these divergences have not been integrated into a system that produces automatic word alignments and have tended to focus on more local phrasal variation rather than more comprehensive sentential syntactic reordering. Complementary prior work (e.g. Wu, 1995) has also addressed syntactic transduction for bilingual parsing, translation, and word-alignment. Much of this work depends on high-quality parsing of both targ"
P04-3014,melamed-1998-empirical,0,0.0132153,"sform the target language (e.g. English) into a form more closely resembling the source language, and then by using standard alignment methods to align the transformed bitext. We present experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less"
P04-3014,W03-0301,0,0.111721,"nt experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distance movement. In contrast, more syntactically informed approaches have been constrained by the often weak syntactic correspondences typ"
P04-3014,C00-2163,0,0.168135,"variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distance movement. In contrast, more syntactically informed approaches have been constrained by the often weak syntactic correspondences typical of real-world p"
P04-3014,C02-1002,0,0.0290631,"Missing"
P04-3014,P01-1067,0,0.0429803,"th transfer and generation in machine translation, these divergences have not been integrated into a system that produces automatic word alignments and have tended to focus on more local phrasal variation rather than more comprehensive sentential syntactic reordering. Complementary prior work (e.g. Wu, 1995) has also addressed syntactic transduction for bilingual parsing, translation, and word-alignment. Much of this work depends on high-quality parsing of both target and source sentences, which may be unavailable for many “lower density” languages of interest. Tree-to-string models, such as (Yamada and Knight, 2001) remove this dependency, and such models are well suited for situations with large, cleanly translated training corpora. By contrast, our method retains the robustness of the underlying aligner towards loose translations, and can if necessary use knowledge of syntactic divergences even in the absence of any training corpora whatsoever, using only a translation lexicon. 3 System Figure 1 shows the system architecture. We start by running the Collins parser (Collins, 1999) on the English side of both training and testing data, and apply our source-language-specific heuristics to the Language Eng"
P04-3014,H01-1035,1,0.797935,"e closely resembling the source language, and then by using standard alignment methods to align the transformed bitext. We present experimental results under variable resource conditions. The method improves word alignment performance for language pairs such as English-Korean and English-Hindi, which exhibit longer-distance syntactic divergences. 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion. However, these models have been less successful at modeling syntactic distortions with longer distan"
P04-3014,J03-4003,0,\N,Missing
P05-1060,P99-1071,0,0.0316327,"of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the output of multiple question answering systems in order to rank answers. Dalmas and Webber (2004) use a WordNet cov"
P05-1060,P04-1056,0,0.0686877,"Missing"
P05-1060,2000.bcs-1.23,0,0.513776,"gure 8: Pre-Fusion precision shows slight drops with increased training documents. The data in Figure 9 suggest an alternate possibility that later documents also shift the prior toward a model where it is less likely that a relationship is observed as fewer targets are extracted. 489 11 10 9 8 7 6 5 4 3 2 1 0 0 20 40 60 80 100 120 140 160 Birthyear Birthday Occupation Birthplace Deathyear # Training Documents Per Person Figure 9: Pre-Fusion Pseudo-Recall also drops with increased training documents. 6 Related Work The closest related work to the task of biographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted info"
P05-1060,J98-3005,0,0.0225917,"closest related work to the task of biographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill e"
P05-1060,P02-1006,0,0.24436,"nstrate that a small training set with only the most relevant documents can be as effective as a larger training set with additional, less relevant documents (Section 5). 2 Training by Automatic Annotation Typically, statistical extraction systems (such as HMMs and CRFs) are trained using hand-annotated data. Annotating the necessary data by hand is timeconsuming and brittle, since it may require largescale re-annotation when the annotation scheme changes. For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). 483 Proceedings of the 43rd Annual Meeting of the ACL, pages 483–490, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics Essentially, for any text snippet of the form A1 pA2 qA3 , these systems estimate the probability that a relationship r(p, q) holds between entities p and q, given the interstitial context, as2 P (r(p, q) |pA2 q) = P (r(p, q) |pA2 q) P x,y∈T c(xA2 y) P = x c(xA2 ) That is, the probability of a relationship r(p, q) is the number of times that pattern xA2 y predicts any relationship r(x, y) in the training set T . c(.) is the count. We will refer to x as"
P05-1060,W98-1106,0,0.150823,"f n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the output of multiple question answering systems in order to rank answers. Dalmas and Webber (2004) use a WordNet cover heuristic to choose an appropriate location from a large candidate set of answers. There has been a considerable amount of work in training information extraction systems from annotated data since the mid-90s. The initial work in the field used lexico-syntactic template patterns learned using a variety of different empirical approaches (Riloff and Schmelzenbach, 1998; Huffman, 1995; Soderland et al., 1995). Seymore et al. (1999) use HMMs for information extraction and explore ways to improve the learning process. Nahm and Mooney (2002) suggest a method to learn word-to-word relationships across fields by doing data mining on information extraction results. Prager et al. (2004) uses knowledge of birth year to weed out candidate years of death that are impossible. Using the CRF extractors in our data set, this heuristic did not yield any improvement. More distantly related work for multi-field extraction suggests methods for combining information in graphic"
P05-1060,P01-1059,0,0.183839,"sion shows slight drops with increased training documents. The data in Figure 9 suggest an alternate possibility that later documents also shift the prior toward a model where it is less likely that a relationship is observed as fewer targets are extracted. 489 11 10 9 8 7 6 5 4 3 2 1 0 0 20 40 60 80 100 120 140 160 Birthyear Birthday Occupation Birthplace Deathyear # Training Documents Per Person Figure 9: Pre-Fusion Pseudo-Recall also drops with increased training documents. 6 Related Work The closest related work to the task of biographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform clo"
P05-1060,H01-1054,0,0.0171224,"ographic fact extraction was done by Cowie et al. (2000) and Schiffman et al. (2001), who explore the problem of biographic summarization. There has been rather limited published work in multi-document information extraction. The closest work to what we present here is Masterson and Kushmerick (2003), who perform multi-document information extraction trained on manually annotated training data and use Best Confidence to resolve each particular template slot. In summarizarion, many systems have examined the multi-document case. Notable systems are SUMMONS (Radev and McKeown, 1998) and RIPTIDE (White et al., 2001), which assume perfect extracted information and then perform closed domain summarization. Barzilay et al. (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. In recent question answering research, information fusion has been used to combine multiple candidate answers to form a consensus answer. Clarke et al. (2001) use frequency of n-gram occurrence to pick answers for particular questions. Another example of answer fusion comes in (Brill et al., 2001) which combines the o"
P05-1060,P04-1073,0,\N,Missing
P08-1049,N06-1003,0,0.0300485,"resents many examples. Manual rules are created to expand an abbreviation to its fullform, however, no quantitative results are reported. None of the above work has addressed the Chinese abbreviation issue in the context of a machine translation task, which is the primary goal in this paper. To the best of our knowledge, our work is the first to systematically model Chinese abbreviation expansion to improve machine translation. The idea of using a bridge (i.e., full-form) to obtain translation entries for unseen words (i.e., abbreviation) is similar to the idea of using paraphrases in MT (see Callison-Burch et al. (2006) and references therein) as both are trying to introduce generalization into MT. At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein). 6 Conclusions In this paper, we present a novel method that automatically extracts relations between full-form phrases and their abbreviations from monolingual corpora, and induces translation entries for these abbreviations by using their full-form as a bridge. Our method is scala"
P08-1049,D07-1007,0,0.0234375,"None of these components is trivial to realize. For example, for the first two components, we may need manually annotated data that tags an abbreviation with its full-form. We also need to make sure that the baseline system has at least one valid translation for the full-form phrase. On the other hand, integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation (WSD) into SMT systems: different ways of integration lead to conflicting conclusions on whether WSD helps MT performance (Chan et al., 2007; Carpuat and Wu, 2007). In this paper, we present an unsupervised approach to translate Chinese abbreviations. Our approach exploits the data co-occurrence phenomena and does not require any additional annotated data except the parallel and monolingual corpora that the baseline SMT system uses. Moreover, our approach integrates the abbreviation translation component into the baseline system in a natural way, and thus is able to make use of the minimum-error-rate training (Och, 2003) to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system. We carry out exp"
P08-1049,P07-1005,0,0.0303653,"Missing"
P08-1049,W04-1102,0,0.216421,"lation reverse phrase translation reverse lexical translation phrase penalty word penalty distortion model Baseline 0.137 0.066 0.061 0.059 0.112 -0.150 -0.327 0.089 AAMT 0.133 0.023 0.078 0.103 0.090 -0.162 -0.356 0.055 Table 8: Weights obtained by MERT 5 Related Work Though automatically extracting the relations between full-form Chinese phrases and their abbreviations is an interesting and important task for many natural language processing applications (e.g., machine translation, question answering, information retrieval, and so on), not much work is available in the literature. Recently, Chang and Lai (2004), Chang and Teng (2006), and Lee (2005) have investigated this task. Specifically, Chang and Lai (2004) describes a hidden markov model (HMM) to model the relationship between a full-form phrase and its abbreviation, by treating the abbreviation as the observation and the full-form words as states in the model. Using a set of manually-created fullabbreviation relations as training data, they report experimental results on a recognition task (i.e., given an abbreviation, the task is to obtain its full-form, or the vice versa). Clearly, their method is supervised because it requires the full-abb"
P08-1049,W06-0103,0,0.386143,"tities are extracted from the English monolingual corpora, which has a much larger vocabulary than the English side of the parallel corpora. Therefore, we should remove all the Chinese translations that contain any untranslated English words before proceeding to the next step. Moreover, it is desirable to generate an n-best list instead of a 1-best translation for the English entity. 3.3 Full-abbreviation Relation Extraction from Chinese Monolingual Corpora We treat the Chinese entities obtained in Section 3.2 as full-form phrases. To identify their abbreviations, one can employ an HMM model (Chang and Teng, 2006). Here we propose a much simpler approach, which is based on the data co-occurrence intuition. 3.3.1 Data Co-occurrence In a monolingual corpus, relevant words tend to appear together (i.e., co-occurrence). For example, Bill Gates tends to appear together with Microsoft. The co-occurrence may imply a relationship (e.g., Bill Gates is the founder of Microsoft). By inspection of the Chinese text, we found that the data co-occurrence phenomena also applies to the fullTitle Text ddd dd d dddddddd d d d d d2d9d d(d d d d d d)d20dd d d d d d dddddd dddd10dd8dddddddd ddddddddddddd Table 2: Data Co-oc"
P08-1049,J07-2003,0,0.146119,"e normally abbreviated by either their first letters (i.e. acronyms) or via truncation, the formation of Chinese abbreviations is much more complex. Figure 1 shows two examples for Chinese abbreviations. Clearly, an abbreviated form of a word can be obtained by selecting one or more characters from this word, and the selected characters can be at any position in the word. In an extreme case, there are even re-ordering between a full-form phrase and its abbreviation. While the research in statistical machine translation (SMT) has made significant progress, most SMT systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006) rely on parallel corpora to extract translation entries. The richness and complexness of Chinese abbreviations imposes challenges to the SMT systems. In particular, many Chinese abbreviations may not appear in available parallel corpora, in which case current SMT systems treat them as unknown words and leave them untranslated. This affects the translation quality significantly. To be able to translate a Chinese abbreviation that is unseen in available parallel corpora, one may annotate more parallel data. However, this is very expensive as there are too many possible abb"
P08-1049,P06-1121,0,0.0685392,"reviated by either their first letters (i.e. acronyms) or via truncation, the formation of Chinese abbreviations is much more complex. Figure 1 shows two examples for Chinese abbreviations. Clearly, an abbreviated form of a word can be obtained by selecting one or more characters from this word, and the selected characters can be at any position in the word. In an extreme case, there are even re-ordering between a full-form phrase and its abbreviation. While the research in statistical machine translation (SMT) has made significant progress, most SMT systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006) rely on parallel corpora to extract translation entries. The richness and complexness of Chinese abbreviations imposes challenges to the SMT systems. In particular, many Chinese abbreviations may not appear in available parallel corpora, in which case current SMT systems treat them as unknown words and leave them untranslated. This affects the translation quality significantly. To be able to translate a Chinese abbreviation that is unseen in available parallel corpora, one may annotate more parallel data. However, this is very expensive as there are too many possible abbreviations and new abb"
P08-1049,P07-2045,0,0.0290049,"translate Chinese abbreviations. Our approach exploits the data co-occurrence phenomena and does not require any additional annotated data except the parallel and monolingual corpora that the baseline SMT system uses. Moreover, our approach integrates the abbreviation translation component into the baseline system in a natural way, and thus is able to make use of the minimum-error-rate training (Och, 2003) to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system. We carry out experiments on a state-of-the-art SMT system, i.e., Moses (Koehn et al., 2007), and show that the abbreviation translations consistently improve the translation performance (in terms of BLEU (Papineni et al., 2002)) on various NIST MT test sets. 426 2 Background: Chinese Abbreviations In general, Chinese abbreviations are formed based on three major methods: reduction, elimination and generalization (Lee, 2005; Yin, 1999). Table 1 presents examples for each category. Among the three methods, reduction is the most popular one, which generates an abbreviation by selecting one or more characters from each of the words in the full-form phrase. The selected characters can be"
P08-1049,N03-1017,0,0.0511385,"ile English words are normally abbreviated by either their first letters (i.e. acronyms) or via truncation, the formation of Chinese abbreviations is much more complex. Figure 1 shows two examples for Chinese abbreviations. Clearly, an abbreviated form of a word can be obtained by selecting one or more characters from this word, and the selected characters can be at any position in the word. In an extreme case, there are even re-ordering between a full-form phrase and its abbreviation. While the research in statistical machine translation (SMT) has made significant progress, most SMT systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006) rely on parallel corpora to extract translation entries. The richness and complexness of Chinese abbreviations imposes challenges to the SMT systems. In particular, many Chinese abbreviations may not appear in available parallel corpora, in which case current SMT systems treat them as unknown words and leave them untranslated. This affects the translation quality significantly. To be able to translate a Chinese abbreviation that is unseen in available parallel corpora, one may annotate more parallel data. However, this is very expensive as there are too man"
P08-1049,P06-1011,0,0.0161089,"the best of our knowledge, our work is the first to systematically model Chinese abbreviation expansion to improve machine translation. The idea of using a bridge (i.e., full-form) to obtain translation entries for unseen words (i.e., abbreviation) is similar to the idea of using paraphrases in MT (see Callison-Burch et al. (2006) and references therein) as both are trying to introduce generalization into MT. At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein). 6 Conclusions In this paper, we present a novel method that automatically extracts relations between full-form phrases and their abbreviations from monolingual corpora, and induces translation entries for these abbreviations by using their full-form as a bridge. Our method is scalable enough to handle large amount of monolingual data, and is essentially unsupervised as it does not require any additional annotated data than the baseline translation system. Our method exploits the data co-occurrence phenomena that is very useful for relation extractions. We integrate ou"
P08-1049,P03-1021,0,0.344499,"tems: different ways of integration lead to conflicting conclusions on whether WSD helps MT performance (Chan et al., 2007; Carpuat and Wu, 2007). In this paper, we present an unsupervised approach to translate Chinese abbreviations. Our approach exploits the data co-occurrence phenomena and does not require any additional annotated data except the parallel and monolingual corpora that the baseline SMT system uses. Moreover, our approach integrates the abbreviation translation component into the baseline system in a natural way, and thus is able to make use of the minimum-error-rate training (Och, 2003) to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system. We carry out experiments on a state-of-the-art SMT system, i.e., Moses (Koehn et al., 2007), and show that the abbreviation translations consistently improve the translation performance (in terms of BLEU (Papineni et al., 2002)) on various NIST MT test sets. 426 2 Background: Chinese Abbreviations In general, Chinese abbreviations are formed based on three major methods: reduction, elimination and generalization (Lee, 2005; Yin, 1999). Table 1 presents examples for each catego"
P08-1049,P00-1056,0,0.0146679,"nslation directions. 429 Integration with Baseline Translation System 4 Experimental Results 4.1 Corpora We compile a parallel dataset which consists of various corpora distributed by the Linguistic Data Consortium (LDC) for NIST MT evaluation. The parallel dataset has about 1M sentence pairs, and about 28M words. The monolingual data we use includes the English Gigaword V2 (LDC2005T12) and the Chinese Gigaword V2 (LDC2005T14). 4.2 Baseline System Training Using the toolkit Moses (Koehn et al., 2007), we built a phrase-based baseline system by following the standard procedure: running GIZA++ (Och and Ney, 2000) in both directions, applying refinement rules to obtain a many-to-many word alignment, and then extracting and scoring phrases using heuristics (Och and Ney, 2004). The baseline system has eight feature functions (see Table 8). The feature functions are combined under a log-linear framework, and the weights are tuned by the minimum-error-rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the optimization metric. To handle different directions of translation between Chinese and English, we built two trigram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998"
P08-1049,J04-4002,0,0.0330167,"pora distributed by the Linguistic Data Consortium (LDC) for NIST MT evaluation. The parallel dataset has about 1M sentence pairs, and about 28M words. The monolingual data we use includes the English Gigaword V2 (LDC2005T12) and the Chinese Gigaword V2 (LDC2005T14). 4.2 Baseline System Training Using the toolkit Moses (Koehn et al., 2007), we built a phrase-based baseline system by following the standard procedure: running GIZA++ (Och and Ney, 2000) in both directions, applying refinement rules to obtain a many-to-many word alignment, and then extracting and scoring phrases using heuristics (Och and Ney, 2004). The baseline system has eight feature functions (see Table 8). The feature functions are combined under a log-linear framework, and the weights are tuned by the minimum-error-rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the optimization metric. To handle different directions of translation between Chinese and English, we built two trigram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998) using the SRILM toolkit (Stolcke, 2002). 4.3 Statistics on Intermediate Steps As described in Section 3, our approach involves five major steps. Table 3 reports t"
P08-1049,P02-1040,0,0.0892142,"data except the parallel and monolingual corpora that the baseline SMT system uses. Moreover, our approach integrates the abbreviation translation component into the baseline system in a natural way, and thus is able to make use of the minimum-error-rate training (Och, 2003) to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system. We carry out experiments on a state-of-the-art SMT system, i.e., Moses (Koehn et al., 2007), and show that the abbreviation translations consistently improve the translation performance (in terms of BLEU (Papineni et al., 2002)) on various NIST MT test sets. 426 2 Background: Chinese Abbreviations In general, Chinese abbreviations are formed based on three major methods: reduction, elimination and generalization (Lee, 2005; Yin, 1999). Table 1 presents examples for each category. Among the three methods, reduction is the most popular one, which generates an abbreviation by selecting one or more characters from each of the words in the full-form phrase. The selected characters can be at any position of the word. Table 1 presents examples to illustrate how characters at different positions are selected to generate abb"
P09-1080,P05-1054,0,0.547436,"a small number of manually selected features, and on a small number of formal written texts. Another relevant line of work has been on the blog domain, using a bag of words feature set to discriminate age and gender (Schler et al., 2006; Burger and Henderson, 2006; Nowson and Oberlander, 2006). Conversational speech presents a challenging domain due to the interaction of genders, recognition errors and sudden topic shifts. While prosodic features have been shown to be useful in gender/age classification (e.g. Shafran et al., 2003), their work makes use of speech transcripts along the lines of Boulis and Ostendorf (2005) in order to build a general model that can be applied to electronic conversations as well. While Boulis and Ostendorf (2005) observe that the gender of the partner can have a substantial effect on their classifier accuracy, given that same-gender conversations are easier to classify than mixed-gender classifications, they don’t utilize this observation in their work. In Section 5.3, we show how the predicted gender/age etc. of the partner/interlocutor can be used to improve overall performance via both dyadic modeling and classifier stacking. Boulis and Ostendorf (2005) have also constrained"
P09-1080,cieri-etal-2004-fisher,0,0.0581086,"g and the -in variants of the present participle ending of the verb (Fisher, 1958), and phonological features such as the pronounciation of the “r” sound in words such as far, four, cards, etc. (Labov, 1966). Gender differences has been one of the primary areas of sociolinguistic research, including work such as Coates (1998) and Eckert and McConnell-Ginet (2003). There has also been some work in developing computational models based on linguistically interesting clues suggested 3 Corpus Details Consistent with Boulis and Ostendorf (2005), we utilized the Fisher telephone conversation corpus (Cieri et al., 2004) and we also evaluated performance on the standard Switchboard conversational corpus (Godfrey et al., 1992), both collected and annotated by the Linguistic Data Consortium. In both cases, we utilized the provided metadata 711 Female Fisher Corpus husband -0.0291 my husband -0.0281 oh -0.0210 laughter -0.0186 have -0.0169 mhm -0.0169 so -0.0163 because -0.0160 and -0.0155 i know -0.0152 hi -0.0147 um -0.0141 boyfriend -0.0134 oh my -0.0124 i have -0.0119 but -0.0118 children -0.0115 goodness -0.0114 yes -0.0106 uh huh -0.0105 Switchboard Corpus oh -0.0122 laughter -0.0088 my husband -0.0077 hus"
P09-1080,P07-1131,0,0.0944051,"Missing"
P09-2090,P98-1012,0,0.0744206,"o noteworthy George Bushes: President George H. W. Bush and President George W. Bush. They are both frequently referred to as “George Bush.” If we wish to use a search engine to find documents about one of them, we are likely also to find documents about the other. Improving our ability to find all documents referring to one and none referring to the other in a targeted search is a goal of crossdocument entity coreference detection. Here we describe some results from a system we built to perform this task on Arabic documents. We base our work partly on previous work done by Bagga and Baldwin (Bagga and Baldwin, 1998), which has also been used in later work (Chen and Martin, 2007). Other work such as Lloyd et al. (Lloyd, 2006) focus on techniques specific to English. The main contribution of this work to crossdocument coreference lies in the conditions under which it was done. Even now, there is no largescale resource—in terms of annotated data—for 2.1 The baseline system The baseline system uses a string matching criterion to determine whether two within-document entities are similar enough to be considered as part of the same cross-document entity. Given withindocument entities A and B, the criterion is"
P09-2090,D07-1020,0,0.0139905,"ent George W. Bush. They are both frequently referred to as “George Bush.” If we wish to use a search engine to find documents about one of them, we are likely also to find documents about the other. Improving our ability to find all documents referring to one and none referring to the other in a targeted search is a goal of crossdocument entity coreference detection. Here we describe some results from a system we built to perform this task on Arabic documents. We base our work partly on previous work done by Bagga and Baldwin (Bagga and Baldwin, 1998), which has also been used in later work (Chen and Martin, 2007). Other work such as Lloyd et al. (Lloyd, 2006) focus on techniques specific to English. The main contribution of this work to crossdocument coreference lies in the conditions under which it was done. Even now, there is no largescale resource—in terms of annotated data—for 2.1 The baseline system The baseline system uses a string matching criterion to determine whether two within-document entities are similar enough to be considered as part of the same cross-document entity. Given withindocument entities A and B, the criterion is implemented as follows: 1. Find the mention strings {a1 , a2 , ."
P09-2090,J07-2003,0,0.0567176,"elp in merging those variants, as shown in figure 1. ( ﺍﻟﺴﻴﺪﺓ ﻋﺎﺋﺸﺔMs. Aisha) ( ﻋﺎﺋﺸﺔAisha) Translingual projection We implement a novel cross-language approach for Arabic coreference resolution by expanding the space of exact match comparisons to approximate matches of English translations of the Arabic strings. The intuition for this approach is that often the Arabic strings of the same named entity may differ due to misspellings, titles, or aliases that can be corrected in the English space. The English translations were obtained using a standard statistical machine translation system (Chiang, 2007; Li, 2008) and then compared using an alias match. The algorithm below describes the approach, applied to any Arabic named entities that fail the baseline string-match test: Translate via SMT Aisha Aisha ( ﻛﻠﻨﺘﻮﻥClenton) Clinton ( ﻛﻠﻴﻨﺘﻮﻥClinton) Clinton ( ﻛﻴﻠﻨﺘﻮﻥCilinton) Clinton Figure 1: Illustration of translingual projection method for resolving Arabic named entity strings via English space. The English strings in parentheses indicate the literal glosses of the Arabic strings prior to translation. 2.3 Entity context similarity The context of mentions can play an important role in m"
P09-2090,P08-2067,1,0.879715,"Missing"
P09-2090,W08-0402,0,0.0243127,"Missing"
P09-2090,C98-1012,0,\N,Missing
P11-1135,P05-1074,0,0.0311999,"nd Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreem"
P11-1135,P10-1089,1,0.845045,"ual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new fea1354 tures without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers are reporting impressive numbers these days, but coordination remains an area with room for improvement. We focused on a speci"
P11-1135,D08-1092,0,0.0399049,"a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used m"
P11-1135,W10-2906,0,0.0145568,"word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolv"
P11-1135,C90-3063,0,0.177758,"Missing"
P11-1135,P91-1017,0,0.234031,"s were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both"
P11-1135,2008.amta-srw.2,0,0.171319,"is accuracy carried over to new domains, where bilingual features are not available. We test the robustness of our co-trained monolingual classifier by evaluating it on our labeled WSJ data. The Penn Treebank and the annotations added by Vadas and Curran (2007a) comprise a very special corpus; such data is clearly not available in every domain. We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with in1353 Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4 Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5 Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results"
P11-1135,J93-1005,0,0.58057,"Missing"
P11-1135,P07-1086,0,0.245976,"equire a distinct type of reordering when translated into a foreign language. Since coordination is both complex and productive, parsers and machine translation (MT) systems cannot simply memorize the analysis of coordinate phrases from training text. We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one"
P11-1135,D09-1127,0,0.0370632,"Missing"
P11-1135,2005.mtsummit-papers.11,0,0.116641,"ish text with a corresponding translation in one or more target languages. A variety of mature NLP tools exists in this domain, allowing us to robustly align the parallel text first at the sentence and then at the word level. Given a word-aligned parallel corpus, we can see how the different types of coordinate NPs are translated in the target languages. In Romance languages, examples with ellipsis, such as dairy and meat production (Table 1), tend to correspond to translations with the head in the first position, e.g. “producci´on l´actea y c´arnica” in Spanish (examples taken from Europarl (Koehn, 2005)). When there is no ellipsis, the head-first syntax leads to the “w1 and h w2 ” ordering, e.g. amianto e o cloreto de polivinilo in Portuguese. Another clue for ellipsis is the presence of a dangling hyphen, as in the Finnish maidon- ja lihantuotantoon. We find such hyphens especially common in Germanic languages like Dutch. In addition to language-specific clues, a translation may resolve an ambiguity by paraphrasing the example in the same way it may be paraphrased in English. E.g., we see hard and soft drugs translated into Spanish as drogas blandas y drogas duras with the head, drogas, rep"
P11-1135,P04-1060,0,0.0228937,"and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also"
P11-1135,P95-1007,0,0.0125732,"Missing"
P11-1135,lin-etal-2010-new,1,0.805628,"rove faster. This is desirable because we don’t have unlimited unlabeled examples to draw from, only those found in our parallel text. 5 Data Web-scale text data is used for monolingual feature counts, parallel text is used for classifier co-training, and labeled data is used for training and evaluation. Web-scale N-gram Data We extract our counts from Google V2: a new N-gram corpus (with N-grams of length one-to-five) created from the same one-trillion-word snapshot of the web as the Google 5-gram Corpus (Brants and Franz, 2006), but with enhanced filtering and processing of the source text (Lin et al., 2010, Section 5). We get counts using the suffix array tools described in (Lin et al., 2010). We add one to all counts for smoothing. Parallel Data We use the Danish, German, Greek, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish portions of Europarl (Koehn, 2005). We also use the Czech, German, Spanish and French news commentary data from WMT 2010.1 Word-aligned English-Foreign bitexts are created using the Berkeley aligner. 2 We run 5 iterations of joint IBM Model 1 training, followed by 3to-5 iterations of joint HMM training, and align with the competitive-thresholding heurist"
P11-1135,J93-2004,0,0.0386474,"Missing"
P11-1135,H05-1105,0,0.392611,"phrases from training text. We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other. Starting from only two 1346 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355, c Portland, Oregon, June 19-24, 2011. 20"
P11-1135,C10-1100,1,0.837257,"approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other. Starting from only two 1346 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguis"
P11-1135,N10-1113,0,0.224357,"Missing"
P11-1135,2003.mtsummit-papers.44,0,0.386025,"ld be even better if this accuracy carried over to new domains, where bilingual features are not available. We test the robustness of our co-trained monolingual classifier by evaluating it on our labeled WSJ data. The Penn Treebank and the annotations added by Vadas and Curran (2007a) comprise a very special corpus; such data is clearly not available in every domain. We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with in1353 Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4 Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5 Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 tr"
P11-1135,W04-3207,0,0.0610601,"tler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Ca"
P11-1135,P09-1009,0,0.0314347,"heir count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extrac"
P11-1135,P07-1031,0,0.375712,"(Resnik, 1999; Nakov and Hearst, 2005). Although pairs of adjectives are usually conjoined (and mixed tags are usually not), this is not always true, as in older/younger above. For comparison, we also state accuracy on the noun-only examples (§ 8). Our task is more narrow than the task tackled by full-sentence parsers, but most parsers do not bracket NP-internal structure at all, since such structure is absent from the primary training corpus for statistical parsers, the Penn Treebank (Marcus et al., 1993). We confirm that standard broad-coverage parsers perform poorly on our task (§ 7). 1347 Vadas and Curran (2007a) manually annotated NP structure in the Penn Treebank, and a few custom NP parsers have recently been developed using this data (Vadas and Curran, 2007b; Pitler et al., 2010). Our task is more narrow than the task handled by these parsers since we do not handle other, less-frequent and sometimes more complex constructions (e.g. robot arms and legs). However, such constructions are clearly amenable to our algorithm. In addition, these parsers have only evaluated coordination resolution within base NPs, simplifying the task and rendering the aforementioned older/younger problem moot. Finally,"
P11-1135,P08-1039,0,0.0454381,"Missing"
P11-1135,J97-3002,0,0.114812,"ch copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information"
P11-1135,N01-1026,1,0.757416,"se an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5 Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge"
P11-1135,P95-1026,1,0.30405,"ning Create Lm ← L Create Lb ← L Create a pool Um by choosing um examples randomly from U . Create a pool Ub by choosing ub examples randomly from U . for i = 0 to k do Use Lm to train a classifier hm using only x ¯m , the monolingual features of x ¯ Use Lb to train a classifier hb using only x ¯b , the bilingual features of x ¯ Use hm to label Um , move the nm most-confident examples to Lb Use hb to label Ub , move the nb most-confident examples to Lm Replenish Um and Ub randomly from U with nm and nb new examples end for uncertain, and vice versa. This suggests using a co-training approach (Yarowsky, 1995; Blum and Mitchell, 1998). We train separate classifiers on the labeled data. We use the predictions of one classifier to label new examples for training the orthogonal classifier. We iterate this training and labeling. We outline how this procedure can be applied to bitext data in Algorithm 1 (above). We follow prior work in drawing predictions from smaller pools, Um and Ub , rather than from U itself, to ensure the labeled examples “are more representative of the underlying distribution” (Blum and Mitchell, 1998). We use a logistic regression classifier for hm and hb . Like Blum and Mitchel"
P11-1135,P07-2009,0,\N,Missing
P11-2090,H05-1066,0,0.0377572,"Missing"
P11-2090,D08-1061,0,0.0366451,"Missing"
P13-2090,P10-1060,0,0.141286,"Missing"
P13-2090,P11-2103,0,0.0334507,"2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Dictionary-based methods rely on existing lexical resources to bootstrap sentiment lexicons. Many researchers have explored using relations in WordNet (Miller, 1995), e.g., Esuli and Sabastiani (2006), Andreevskaia and Bergler (2006) for English, Rao and Ravichandran (2009) for Hindi and French, and Perez-Rosas et al. (2012) for Spanish. Mohammad et al. (2009) use a thesaurus to aid in the construction of a sentiment lexicon for English. Other works (Clematide and Klenner, 2010; Abdul-Mageed et al., 2011) automatically expands and evaluates German and Arabic lexicons. However, the lexical resources that dictionary-based methods need, do not yet exist for the majority of languages in social media. There is also a mismatch between the formality of many language resources, such as WordNet, and the extremely informal language of social media. Corpus-based methods extract subjectivity and sentiment lexicons from large amounts of unlabeled data using different similarity metrics to measure the relatedness between words. Hatzivassiloglou and McKeown (1997) were the first to explore automatically lear"
P13-2090,D07-1115,0,0.0540156,"nt lexicons from large amounts of unlabeled data using different similarity metrics to measure the relatedness between words. Hatzivassiloglou and McKeown (1997) were the first to explore automatically learning the polarity of words from corpora. Early work by Wiebe (2000) identifies clusters of subjectivity clues based on their distributional similarity, using a small amount of data to bootstrap the process. Turney (2002) and Velikovich et al. (2010) bootstrap sentiment lexicons for English from the web by using Pointwise Mutual Information (PMI) and graph propagation approach, respectively. Kaji and Kitsuregawa (2007) propose a method for building sentiment lexicon for Japanese from HTML pages. Banea et al. (2008) experiment with Lexical Semantic Analysis (LSA) (Dumais et al., 1988) to bootstrap a subjectivity lexicon for Romanian. Kanayama and Nasukawa (2006) bootstrap subjectivity lexicons for Japanese by generating subjectivity candidates based on word co-occurrence patterns. In contrast to other corpus-based bootstrapping methods, we evaluate our approach on multiple languages, specifically English, Spanish, and Russian. Also, as our approach relies only on the availability of a bilingual dictionary fo"
P13-2090,E06-1027,0,0.0606914,"Work better able to handle the informality and the dynamic nature of social media. It also can be effectively used to bootstrap sentiment lexicons for any language for which a bilingual dictionary is available or can be automatically induced from parallel corpora. Mihalcea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Dictionary-based methods rely on existing lexical resources to bootstrap sentiment lexicons. Many researchers have explored using relations in WordNet (Miller, 1995), e.g., Esuli and Sabastiani (2006), Andreevskaia and Bergler (2006) for English, Rao and Ravichandran (2009) for Hindi and French, and Perez-Rosas et al. (2012) for Spanish. Mohammad et al. (2009) use a thesaurus to aid in the construction of a sentiment lexicon for English. Other works (Clematide and Klenner, 2010; Abdul-Mageed et al., 2011) automatically expands and evaluates German and Arabic lexicons. However, the lexical resources that dictionary-based methods need, do not yet exist for the majority of languages in social media. There is also a mismatch between the formality of many language resources, such as WordNet, and the extremely informal language"
P13-2090,W06-1642,0,0.117862,"Early work by Wiebe (2000) identifies clusters of subjectivity clues based on their distributional similarity, using a small amount of data to bootstrap the process. Turney (2002) and Velikovich et al. (2010) bootstrap sentiment lexicons for English from the web by using Pointwise Mutual Information (PMI) and graph propagation approach, respectively. Kaji and Kitsuregawa (2007) propose a method for building sentiment lexicon for Japanese from HTML pages. Banea et al. (2008) experiment with Lexical Semantic Analysis (LSA) (Dumais et al., 1988) to bootstrap a subjectivity lexicon for Romanian. Kanayama and Nasukawa (2006) bootstrap subjectivity lexicons for Japanese by generating subjectivity candidates based on word co-occurrence patterns. In contrast to other corpus-based bootstrapping methods, we evaluate our approach on multiple languages, specifically English, Spanish, and Russian. Also, as our approach relies only on the availability of a bilingual dictionary for translating an English subjectivity lexicon and crowdsourcing for help in selecting seeds, it is more scalable and 3 Data For the experiments in this paper, we use three sets of data for each language: 1M unlabeled tweets (B OOT) for bootstrappi"
P13-2090,banea-etal-2008-bootstrapping,0,0.171445,"However, such tools and lexical resources are not available for many languages spoken in social media. While English is still the top language in Twitter, it is no longer the majority. Thus, the applicability of these approaches is limited. Any method for analyzing sentiment in microblogs or other social media streams must be easily adapted to (1) many low-resource languages, (2) the dynamic nature of social media, and (3) working in a streaming mode with limited or no supervision. Although bootstrapping has been used for learning sentiment lexicons in other domains (Turney and Littman, 2002; Banea et al., 2008), it has not yet been applied to learning sentiment lexicons for microblogs. In this paper, we present an approach for bootstrapping subjectivity clues from Twitter data, and evaluate our approach on English, Spanish and Russian Twitter streams. Our approach: • handles the informality, creativity and the dynamic nature of social media; • does not rely on language-dependent tools; • scales to the hundreds of new under-explored languages and dialects in social media; • classifies sentiment in a streaming mode. To bootstrap subjectivity clues from Twitter streams we rely on three main assumptions"
P13-2090,P12-4004,0,0.105767,"Missing"
P13-2090,W12-2108,1,0.659222,"Missing"
P13-2090,D09-1063,0,0.199089,"lexicons for any language for which a bilingual dictionary is available or can be automatically induced from parallel corpora. Mihalcea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Dictionary-based methods rely on existing lexical resources to bootstrap sentiment lexicons. Many researchers have explored using relations in WordNet (Miller, 1995), e.g., Esuli and Sabastiani (2006), Andreevskaia and Bergler (2006) for English, Rao and Ravichandran (2009) for Hindi and French, and Perez-Rosas et al. (2012) for Spanish. Mohammad et al. (2009) use a thesaurus to aid in the construction of a sentiment lexicon for English. Other works (Clematide and Klenner, 2010; Abdul-Mageed et al., 2011) automatically expands and evaluates German and Arabic lexicons. However, the lexical resources that dictionary-based methods need, do not yet exist for the majority of languages in social media. There is also a mismatch between the formality of many language resources, such as WordNet, and the extremely informal language of social media. Corpus-based methods extract subjectivity and sentiment lexicons from large amounts of unlabeled data using dif"
P13-2090,perez-rosas-etal-2012-learning,0,0.0818797,"effectively used to bootstrap sentiment lexicons for any language for which a bilingual dictionary is available or can be automatically induced from parallel corpora. Mihalcea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Dictionary-based methods rely on existing lexical resources to bootstrap sentiment lexicons. Many researchers have explored using relations in WordNet (Miller, 1995), e.g., Esuli and Sabastiani (2006), Andreevskaia and Bergler (2006) for English, Rao and Ravichandran (2009) for Hindi and French, and Perez-Rosas et al. (2012) for Spanish. Mohammad et al. (2009) use a thesaurus to aid in the construction of a sentiment lexicon for English. Other works (Clematide and Klenner, 2010; Abdul-Mageed et al., 2011) automatically expands and evaluates German and Arabic lexicons. However, the lexical resources that dictionary-based methods need, do not yet exist for the majority of languages in social media. There is also a mismatch between the formality of many language resources, such as WordNet, and the extremely informal language of social media. Corpus-based methods extract subjectivity and sentiment lexicons from large"
P13-2090,C12-1037,0,0.40101,"Fsubj≥1 0.57 0.71 0.75 Fsubj≥2 0.27 0.48 0.72 (c) Polarity Fpolarity 0.78 0.82 0.78 Figure 1: Precision (x-axis), recall (y-axis) and F-measure (in the table) for English: LE I = initial lexicon, LE = bootstrapped lexicon, SW N = B strongly subjective terms from SentiWordNet. Lexicon Evaluations We evaluate our bootstrapped sentiment lexicons R S English LE B , Spanish LB and Russian LB by comparing them with existing dictionary-expanded lexicons that have been previously shown to be effective for subjectivity and polarity classification (Esuli and Sebastiani, 2006; Perez-Rosas et al., 2012; Chetviorkin and Loukachevitch, 2012). For that we perform subjectivity and polarity classification using rule-based classifiers6 on the test data E-T EST, S-T EST and R-T EST. We consider how the various lexicons perform for rule-based classifiers for both subjectivity and polarity. The subjectivity classifier predicts that a tweet is subjective if it contains a) at least one, or b) at least two subjective terms from the lexicon. For the polarity classifier, we predict a tweet to be positive (negative) if it contains at least one positive (negative) term taking into account negation. If the tweet contains both positive and negat"
P13-2090,E09-1077,0,0.080853,"nd the dynamic nature of social media. It also can be effectively used to bootstrap sentiment lexicons for any language for which a bilingual dictionary is available or can be automatically induced from parallel corpora. Mihalcea et.al (2012) classifies methods for bootstrapping subjectivity lexicons into two types: corpus-based and dictionary-based. Dictionary-based methods rely on existing lexical resources to bootstrap sentiment lexicons. Many researchers have explored using relations in WordNet (Miller, 1995), e.g., Esuli and Sabastiani (2006), Andreevskaia and Bergler (2006) for English, Rao and Ravichandran (2009) for Hindi and French, and Perez-Rosas et al. (2012) for Spanish. Mohammad et al. (2009) use a thesaurus to aid in the construction of a sentiment lexicon for English. Other works (Clematide and Klenner, 2010; Abdul-Mageed et al., 2011) automatically expands and evaluates German and Arabic lexicons. However, the lexical resources that dictionary-based methods need, do not yet exist for the majority of languages in social media. There is also a mismatch between the formality of many language resources, such as WordNet, and the extremely informal language of social media. Corpus-based methods ex"
P13-2090,W03-1014,0,0.466601,"OOTSTRAP (σ, θpr , θf req , θtopK ) ~ ← LI (σ) 1: iter = 0, σ = 0.5, LB (θ) 2: while (stop 6= true) do iter ~ ~ 3: Liter B (θ) ← ∅, ∆LB (θ) ← ∅ ~ do 4: for each new term w ∈ {V  LB (θ)} 5: for each tweet t ∈ T do 6: if w ∈ t then ~ c(w, Lpos (θ)), ~ c(w) 7: UPDATE c(w, LB (θ)), B 8: end if 9: end for ~ B (θ)) 10: psubj (w) ← c(w,L c(w) High-Precision Subjectivity Lexicons For English we seed the bootstrapping process with the strongly subjective terms from the MPQA lexicon3 (Wilson et al., 2005). These terms have been previously shown to be highprecision for recognizing subjective sentences (Riloff and Wiebe, 2003). For the other languages, the subjective seed terms are obtained by translating English seed terms using a bilingual dictionary, and then collecting judgments about term subjectivity from Mechanical Turk. Terms that truly are strongly subjective in translation are used for seed terms in the new language, with term polarity projected from the English. Finally, we expand the lexicons with plurals and inflectional forms for adverbs, adjectives and verbs. 4.2 pos c(w,L 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: Bootstrapping Approach To bootstrap, firs"
P13-2090,D08-1027,0,0.0686327,"Missing"
P13-2090,esuli-sebastiani-2006-sentiwordnet,0,0.00718546,"ns. 5 (a) Subj ≥ 1 Lexicon SW N LE I LE B (b) Subj ≥ 2 Fsubj≥1 0.57 0.71 0.75 Fsubj≥2 0.27 0.48 0.72 (c) Polarity Fpolarity 0.78 0.82 0.78 Figure 1: Precision (x-axis), recall (y-axis) and F-measure (in the table) for English: LE I = initial lexicon, LE = bootstrapped lexicon, SW N = B strongly subjective terms from SentiWordNet. Lexicon Evaluations We evaluate our bootstrapped sentiment lexicons R S English LE B , Spanish LB and Russian LB by comparing them with existing dictionary-expanded lexicons that have been previously shown to be effective for subjectivity and polarity classification (Esuli and Sebastiani, 2006; Perez-Rosas et al., 2012; Chetviorkin and Loukachevitch, 2012). For that we perform subjectivity and polarity classification using rule-based classifiers6 on the test data E-T EST, S-T EST and R-T EST. We consider how the various lexicons perform for rule-based classifiers for both subjectivity and polarity. The subjectivity classifier predicts that a tweet is subjective if it contains a) at least one, or b) at least two subjective terms from the lexicon. For the polarity classifier, we predict a tweet to be positive (negative) if it contains at least one positive (negative) term taking into"
P13-2090,P02-1053,0,0.00653671,"so a mismatch between the formality of many language resources, such as WordNet, and the extremely informal language of social media. Corpus-based methods extract subjectivity and sentiment lexicons from large amounts of unlabeled data using different similarity metrics to measure the relatedness between words. Hatzivassiloglou and McKeown (1997) were the first to explore automatically learning the polarity of words from corpora. Early work by Wiebe (2000) identifies clusters of subjectivity clues based on their distributional similarity, using a small amount of data to bootstrap the process. Turney (2002) and Velikovich et al. (2010) bootstrap sentiment lexicons for English from the web by using Pointwise Mutual Information (PMI) and graph propagation approach, respectively. Kaji and Kitsuregawa (2007) propose a method for building sentiment lexicon for Japanese from HTML pages. Banea et al. (2008) experiment with Lexical Semantic Analysis (LSA) (Dumais et al., 1988) to bootstrap a subjectivity lexicon for Romanian. Kanayama and Nasukawa (2006) bootstrap subjectivity lexicons for Japanese by generating subjectivity candidates based on word co-occurrence patterns. In contrast to other corpus-ba"
P13-2090,W05-0408,0,0.0295599,"ish, Spanish and Russian Twitter streams. Our approach: • handles the informality, creativity and the dynamic nature of social media; • does not rely on language-dependent tools; • scales to the hundreds of new under-explored languages and dialects in social media; • classifies sentiment in a streaming mode. To bootstrap subjectivity clues from Twitter streams we rely on three main assumptions: i. sentiment-bearing terms of similar orientation tend to co-occur at the tweet level (Turney and Littman, 2002); ii. sentiment-bearing terms of opposite orientation do not co-occur at the tweet level (Gamon and Aue, 2005); iii. the co-occurrence of domain-specific and domain-independent subjective terms serves as a signal of subjectivity. We study subjective language in social media and create Twitter-specific lexicons via bootstrapping sentiment-bearing terms from multilingual Twitter streams. Starting with a domain-independent, highprecision sentiment lexicon and a large pool of unlabeled data, we bootstrap Twitter-specific sentiment lexicons, using a small amount of labeled data to guide the process. Our experiments on English, Spanish and Russian show that the resulting lexicons are effective for sentiment"
P13-2090,N10-1119,0,0.181171,"een the formality of many language resources, such as WordNet, and the extremely informal language of social media. Corpus-based methods extract subjectivity and sentiment lexicons from large amounts of unlabeled data using different similarity metrics to measure the relatedness between words. Hatzivassiloglou and McKeown (1997) were the first to explore automatically learning the polarity of words from corpora. Early work by Wiebe (2000) identifies clusters of subjectivity clues based on their distributional similarity, using a small amount of data to bootstrap the process. Turney (2002) and Velikovich et al. (2010) bootstrap sentiment lexicons for English from the web by using Pointwise Mutual Information (PMI) and graph propagation approach, respectively. Kaji and Kitsuregawa (2007) propose a method for building sentiment lexicon for Japanese from HTML pages. Banea et al. (2008) experiment with Lexical Semantic Analysis (LSA) (Dumais et al., 1988) to bootstrap a subjectivity lexicon for Romanian. Kanayama and Nasukawa (2006) bootstrap subjectivity lexicons for Japanese by generating subjectivity candidates based on word co-occurrence patterns. In contrast to other corpus-based bootstrapping methods, we"
P13-2090,P11-2008,0,0.012043,"Missing"
P13-2090,H05-1044,1,0.0581752,"tstrap from the unlabeled data (BOOT) using the labeled development data (DEV) to guide the process. 4.1 Algorithm 1 B OOTSTRAP (σ, θpr , θf req , θtopK ) ~ ← LI (σ) 1: iter = 0, σ = 0.5, LB (θ) 2: while (stop 6= true) do iter ~ ~ 3: Liter B (θ) ← ∅, ∆LB (θ) ← ∅ ~ do 4: for each new term w ∈ {V  LB (θ)} 5: for each tweet t ∈ T do 6: if w ∈ t then ~ c(w, Lpos (θ)), ~ c(w) 7: UPDATE c(w, LB (θ)), B 8: end if 9: end for ~ B (θ)) 10: psubj (w) ← c(w,L c(w) High-Precision Subjectivity Lexicons For English we seed the bootstrapping process with the strongly subjective terms from the MPQA lexicon3 (Wilson et al., 2005). These terms have been previously shown to be highprecision for recognizing subjective sentences (Riloff and Wiebe, 2003). For the other languages, the subjective seed terms are obtained by translating English seed terms using a bilingual dictionary, and then collecting judgments about term subjectivity from Mechanical Turk. Terms that truly are strongly subjective in translation are used for seed terms in the new language, with term polarity projected from the English. Finally, we expand the lexicons with plurals and inflectional forms for adverbs, adjectives and verbs. 4.2 pos c(w,L 12: 13:"
P13-2090,P97-1023,0,0.51814,"or English. Other works (Clematide and Klenner, 2010; Abdul-Mageed et al., 2011) automatically expands and evaluates German and Arabic lexicons. However, the lexical resources that dictionary-based methods need, do not yet exist for the majority of languages in social media. There is also a mismatch between the formality of many language resources, such as WordNet, and the extremely informal language of social media. Corpus-based methods extract subjectivity and sentiment lexicons from large amounts of unlabeled data using different similarity metrics to measure the relatedness between words. Hatzivassiloglou and McKeown (1997) were the first to explore automatically learning the polarity of words from corpora. Early work by Wiebe (2000) identifies clusters of subjectivity clues based on their distributional similarity, using a small amount of data to bootstrap the process. Turney (2002) and Velikovich et al. (2010) bootstrap sentiment lexicons for English from the web by using Pointwise Mutual Information (PMI) and graph propagation approach, respectively. Kaji and Kitsuregawa (2007) propose a method for building sentiment lexicon for Japanese from HTML pages. Banea et al. (2008) experiment with Lexical Semantic An"
P13-2090,D10-1021,0,\N,Missing
P13-2090,W11-0705,0,\N,Missing
P13-2090,baccianella-etal-2010-sentiwordnet,0,\N,Missing
P13-2090,J11-2001,0,\N,Missing
P13-2090,C10-2028,0,\N,Missing
P13-2090,P11-1016,0,\N,Missing
P13-2090,P09-1080,1,\N,Missing
P13-2090,C10-2005,0,\N,Missing
P13-2090,W11-1709,0,\N,Missing
P13-2090,Y12-1013,0,\N,Missing
P13-2090,pak-paroubek-2010-twitter,0,\N,Missing
P13-2090,D11-1120,0,\N,Missing
P13-2090,cieri-etal-2004-fisher,0,\N,Missing
P15-1119,J92-4003,0,0.140167,"n Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: c(wiT ) = arg max k ∑ (i,j)∈AT ∣S ci,j ⋅ 1[c(wjS ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et"
P15-1119,W06-2920,0,0.0171356,"e templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et al. (2013) (McD13), because we’re using Before this dataset was carried out, the CoNLL multilingual dependency treebanks (Buchholz and Marsi, 2006) were often used for evaluation. However, the major problem is that the dependency annotations vary for different languages (e.g. the choice of lexical versus functional head), which makes it impossible to evaluate the LAS. 1239 D ELEX P ROJ P ROJ+Cluster CCA CCA+Cluster Unlabeled Attachment Score (UAS) EN DE ES FR AVG 83.67 57.01 68.05 68.85 64.64 91.96 60.07 71.42 71.36 67.62 92.33 60.35 71.90 72.93 68.39 90.62† 59.42 68.87 69.58 65.96 92.03† 60.66 71.33 70.87 67.62 EN 79.42 90.48 90.91 88.88† 90.49† M C D13 83.33 58.50 68.07 70.14 65.57 78.54 48.11 56.86 58.20 54.39 84.44 90.21 57.30 60.55"
P15-1119,P14-1063,0,0.0232538,"ture templates used in our system are shown in Table 1. Then, feature compositions are performed at the hidden layer via a cube activation function: g(x) = x3 . The cube activation function can be viewed as a special case of low-rank tensor. Formally, g(x) can be expanded as: g(w1 x1 + ... + wm xm + b) = ∑ (wi wj wk )xi xj xk + ∑ b(wi wj )xi xj + ... i,j,k i,j If we treat the bias term as b × x0 where x0 = 1, then the weight corresponding to each feature combination xi xj xk is wi wj wk , which is exactly the same as a rank-1 component tensor in the lowrank form using CP tensor decomposition (Cao and Khudanpur, 2014). Consequently, the cube activation function implicitly derives full feature combinations. An advantage of the cube activation function is that it is flexible for adding extra features to the input. In fact, we can add as many features as possible to the input layer to improve the parsing accuracy. We will show in Section 5.2 that the Brown cluster features can be readily incorporated into our model. Cross-lingual Transfer. The idea of crosslingual transfer using the parser we examined above is straightforward. In contrast to traditional approaches that have to discard rich lexical features (d"
P15-1119,N13-1006,1,0.167482,"(Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao"
P15-1119,D14-1082,0,0.779303,"atures used in traditional dependency parsers, distributed representations map symbolic features into a continuous representation space, that can be shared across languages. Therefore, our model has the ability to utilize both lexical and non-lexical features naturally. Specifically, our framework contains two primary components: • A neural network-based dependency parser. We expect a non-linear model for dependency parsing in our study, because distributed feature representations are shown to be more effective in non-linear architectures than in linear architectures (Wang and Manning, 2013). Chen and Manning (2014) propose a transition-based dependency parser using a neural network architecture, which is simple but works well on several datasets. Briefly, this model simply replaces the predictor in transition-based dependency parser with a well-designed neural network classifier. We will provide explanations for the merits of this model in Section 3, as well as how we adapt it to the cross-lingual task. • Cross-lingual word representation learning. The key to filling the lexical feature gap is to project the representations of these features from different languages into a common vector space, preservin"
P15-1119,P11-1061,0,0.0110737,"ion method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49"
P15-1119,W08-1301,0,0.0204693,"Missing"
P15-1119,de-marneffe-etal-2006-generating,0,0.034227,"Missing"
P15-1119,D12-1001,0,0.0275469,"uding NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-"
P15-1119,P10-4002,0,0.0314634,"r-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units to 400. The dimension of embeddings for different features are shown in Table 2. Dim. Word 50 POS 50 Label 50 Dist. 5 Val. 5 Cluster 8 Table 2: Dimensions of feature embeddings. Adaptive stochastic gradient descent (AdaGrad) (Duchi et al., 2011) is used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer (2014). The dimensions of the monolin"
P15-1119,E14-1049,0,0.441226,"which is typically smaller than the monolingual datasets. Therefore, in order to improve the robustness of projection, we utilize a morphology-inspired mechanism, to propagate embeddings from in-vocabulary words to out-ofvocabulary (OOV) words. Specifically, for each T , we extract a list of candidate OOV word woov words that is similar to it in terms of edit distance, and then set the averaged vector as the embedding T of woov . Formally, T v(woov ) = Avg (v(w′ )) w′ ∈C T where C = {w∣EditDist(woov , w) ≤ τ } 4.3 ?1 Canonical Correlation Analysis The second approach we consider is similar to Faruqui and Dyer (2014), which use CCA to improve monolingual word embeddings with multilingual correlation. CCA is a way of measur?2 Σ ?1 Ω′ Ω ? ? ? ? ?2 ?2 Ω∗ CCA ?1 Σ∗ ? ? Figure 3: CCA for cross-lingual word representation learning. ing the linear relationship between multidimensional variables. For two multidimensional variables, CCA aims to find two projection matrices to map the original variables to a new basis (lowerdimensional), such that the correlation between the two variables is maximized. Let’s treat CCA as a blackbox here, and see how to apply CCA for inducing bilingual word embeddings. Suppose there"
P15-1119,P09-1042,0,0.0255939,"Missing"
P15-1119,P10-1156,0,0.0165341,"46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, whi"
P15-1119,D14-1012,1,0.642138,"major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition actions In this paper, these two terms are used interchangeably. Stack Buffer ROOT has_VBZ good_JJ control_NN ._. nsubj He_PRP Configuration Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially"
P15-1119,P08-1088,0,0.0084253,"projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units"
P15-1119,P14-1006,0,0.0265905,"Missing"
P15-1119,D11-1110,0,0.0181767,"cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the b"
P15-1119,C10-1063,0,0.0172979,".). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingua"
P15-1119,P12-2010,0,0.021569,"oposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES U"
P15-1119,P12-1073,0,0.017508,"tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Klementiev et al., 2012)‡ B IAE (Chandar et al., 2014)‡ B ICVM (Hermann and Blunsom, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison"
P15-1119,P04-1061,0,0.0694958,"ng central problems. The majority of work on dependency parsing has been dedicated to resource-rich languages, such as English and Chinese. For these languages, there exist large-scale ∗ This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features."
P15-1119,C12-1089,0,0.351816,"Missing"
P15-1119,W02-0902,0,0.0119728,"dings for our cross-lingual task. Contrary to the projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network depe"
P15-1119,P13-1105,0,0.0787465,"Missing"
P15-1119,W14-1614,0,0.05557,"-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod"
P15-1119,P14-1126,0,0.19646,"pplied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15 For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod arcs are mostly leftdirected. ‡ FR LAS 58.78 46.66 58.08"
P15-1119,C14-1175,0,0.133083,"dency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel"
P15-1119,N01-1020,1,0.605674,"ual task. Contrary to the projection approach, CCA assigns embeddings for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4 T ∣S A is also worth trying, but we observed slight performance degradation in our experimental setting. 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the"
P15-1119,D07-1013,0,0.00913989,"d cluster features can be effectively embedded into our model, leading to significant additive improvements. 2 2.1 Background Dependency Parsing Given an input sentence x = w0 w1 ...wn , the goal of dependency parsing is to build a dependency tree (Figure 1), which can be denoted by d = {(h, m, l) ∶ 0 ≤ h ≤ n; 0 &lt; m ≤ n, l ∈ L}. (h, m, l) indicates a directed arc from the head word wh to the modifier wm with a dependency label l, and L is the label set. The mainstream models that have been proposed for dependency parsing can be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy d"
P15-1119,D11-1006,0,0.219143,"uages, such as English and Chinese. For these languages, there exist large-scale ∗ This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features. A delexicalized parser makes sense in that POS tag features are significantly predictive for unlabele"
P15-1119,D10-1120,0,0.021069,"Missing"
P15-1119,P12-1066,0,0.0410584,"Missing"
P15-1119,W04-0308,0,0.0244311,"ROOT has_VBZ good_JJ control_NN ._. nsubj He_PRP Configuration Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially derived forest, i.e. a set of dependency arcs A. Given an input word sequence x = w1 w2 , ..., wn , the initial configuration can be represented as a tuple: ⟨[w0 ]S , [w1 w2 , ..., wn ]B , ∅⟩, and the terminal configuration is ⟨[w0 ]S , []B , A⟩, where w0 is a pseudo word indicating the root of the whole dependency tree. We consider the arc-standard algorithm (Nivre, 2004) in this paper, which defines three types of transition actions: L EFT-A RC(l), R IGHT-A RC(l), and S HIFT, l is the dependency label. The typical approach for greedy arc-standard parsing is to build a multi-class classifier (e.g., SVM, MaxEnt) of predicting the transition action given a feature vector extracted from a specific configuration. While conventional feature engineering suffers from the problem of sparsity, incompleteness and expensive feature computation (Chen and Manning, 2014), the neural network model provides a potential solution. The architecture of the neural network-based de"
P15-1119,petrov-etal-2012-universal,0,0.0336392,"Missing"
P15-1119,D09-1086,0,0.00730072,". 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized"
P15-1119,W15-1824,0,0.0443701,"be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B ICVM only uses the bilingual parallel dataset. MTL (Kle"
P15-1119,P12-1068,0,0.00760257,"UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser"
P15-1119,P10-1040,0,0.0337388,"in the NLP research community of learning distributed representations for different natural language units, from morphemes, words and phrases, to sentences and documents. Using distributed representations, these symbolic units are embedded into a lowdimensional and continuous space, thus it is often referred to as embeddings.1 In general, there are two major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Su"
P15-1119,P08-1086,0,0.0144334,"). We use the same alignment dictionary as described in Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: c(wiT ) = arg max k ∑ (i,j)∈AT ∣S ci,j ⋅ 1[c(wjS ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower p"
P15-1119,I13-1183,0,0.0132607,"Missing"
P15-1119,Q14-1005,0,0.032741,"om, 2014) B ILBOWA (Gouws et al., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cr"
P15-1119,W14-1613,0,0.186749,"2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. T"
P15-1119,H01-1035,1,0.652706,"ks. It is worth noting that we don’t assume/require bilingual parallel data in CCA and P ROJ. What we need in practice is a bilingual lexicon for each paired languages. This is especially important for generalizing our approaches to lower-resource languages, where parallel texts are not available. 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorp"
P15-1119,P11-2033,0,0.022,"an be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy decoding and local training is not as accurate as graph-based parsers or transition-based parsers with beam-search and 1235 global training (Zhang and Clark, 2011). Recently, Chen and Manning (2014) show that greedy transition-based parsers can be greatly improved by using a well-designed neural network architecture. This approach can be considered as a new paradigm of parsing, in that it is based on pure distributed feature representations. In this study, we choose Chen and Manning’s architecture to build our basic depe"
P15-1119,P09-1007,0,0.00764621,"ting approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 1241 13 14 The MTL embeddings are normalized before training. B"
P15-1119,D10-1030,0,0.0157323,"l., 2014) CCA P ROJ DE UAS LAS 57.70 47.13 53.74 43.68 56.30 46.99 51.65 41.83 59.42 49.32 60.07 49.94 ES UAS 68.04 58.81 67.78 65.02 68.87 71.42 Table 8: Comparison with existing bilingual word embeddings. released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings"
P15-1119,N12-1052,0,0.445863,"Missing"
P15-1119,J11-1005,0,\N,Missing
P15-1119,P13-2017,0,\N,Missing
P15-2111,D11-1057,0,0.034462,"al morphemes are able to be represented either through single features or through multiple features in combina674 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 674–680, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tion. These features capture only the semantic content of morphemes, but can be integrated into existing frameworks that precisely indicate morpheme form (Sagot and Walther, 2013) or automatically discover it (Dreyer and Eisner, 2011; Hammarstr¨om, 2006; Goldsmith, 2001). The fact that the schema is meant to capture only the meanings of overt, non-root affixal morphemes restricts the semantic-conceptual space that must be captured by its features and renders an interlingual approach to representing inflectional morphology feasible. The universal morphological feature schema is most similar to tagset systematization efforts across multiple languages, such as the Universal Dependencies Project (Choi et al., 2015) and Interset (Zeman, 2008). While these efforts encode similar morphological features to the current schema, the"
P15-2111,N13-1138,0,0.185301,"Missing"
P15-2111,J01-2001,0,0.0221461,"r through single features or through multiple features in combina674 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 674–680, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tion. These features capture only the semantic content of morphemes, but can be integrated into existing frameworks that precisely indicate morpheme form (Sagot and Walther, 2013) or automatically discover it (Dreyer and Eisner, 2011; Hammarstr¨om, 2006; Goldsmith, 2001). The fact that the schema is meant to capture only the meanings of overt, non-root affixal morphemes restricts the semantic-conceptual space that must be captured by its features and renders an interlingual approach to representing inflectional morphology feasible. The universal morphological feature schema is most similar to tagset systematization efforts across multiple languages, such as the Universal Dependencies Project (Choi et al., 2015) and Interset (Zeman, 2008). While these efforts encode similar morphological features to the current schema, their goal is different, namely to system"
P15-2111,zeman-2008-reusable,0,0.0602602,"ing algorithm and feature mapping algorithms, yielding 883,965 instantiated paradigms in 352 languages. These data are shown to be effective for training morphological analyzers, yielding significant accuracy gains when applied to Durrett and DeNero’s (2013) paradigm learning framework. 1 Introduction Semantically detailed and typologically-informed morphological analysis that is broadly crosslinguistically applicable and interoperable has the potential to improve many NLP applications, including machine translation (particularly of morphologically rich languages), parsing (Choi et al., 2015; Zeman, 2008; Mikulov´a et al., 2006), ngram language models, information extraction, and co-reference resolution. To do large-scale cross-linguistic analysis and translation, it is necessary to be able to compare the meanings of morphemes using a single, welldefined framework. Haspelmath (2010) notes that while morphological categories will never map with perfect precision across languages and can only be exhaustively defined within a single language, practitioners of linguistic typology have typically recognized that there is sufficient similarity in these categories across languages to do meaningful co"
P19-1172,P15-2044,0,0.198831,"Missing"
P19-1172,P16-1184,0,0.0512551,"(2015) exploit the parallel nature of the Bible to project POS tags and train taggers in the target languages, leveraging the signal from multiple languages to improve the tagger accuracy. We focus, instead, on the induction of detailed morphological categories. Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner. While this is analogous to lemmatization, part of our motivation is to also produce detailed morphological features that might be useful to train lowresource taggers, or to more richly annotate morphologically sparse languages such as English. Buys and Botha (2016) train morphological taggers in morphologically rich languages from an English projection. However, their method is dependent upon an English corpus tagged with more morphologically aware tags than are typically produced by an off-the-shelf English POS tagger. We instead argue that much of this information is recoverable from syntactic and semantic parses, allowing us to use massively-parallel corpora such as the Bible. Kirov et al. (2017) notes the morphological sparsity of English, and reverses our setup, projecting morphologically rich tags from Czech into English. Rather than add another p"
P19-1172,I05-1075,0,0.717276,"and translations of religious texts. Thus, documents such as the Christian Bible are among the most translated documents in the world (Mayer and Cysouw, 2014). Furthermore, the Bible consists of short, numbered chapters and verses consisting of a small number of sentences. Although not parallel to the standard required in fields such as machine translation, the structure of the Bible means that different Bibles are approximately parallel across verses. We follow a tradition of projecting POS tags from a high-resource language onto a language with fewer available tools (Yarowsky et al., 2001; Fossum and Abney, 2005; Agi´c et al., 2015; Buys 1765 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1765–1774 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Botha, 2016). Our contributions, however, lie on the level of morphology and morphosyntax. With no further resources in the target language than a Bible translation and a dictionary, we project English POS tags, dependency relations, and semantic labels across the alignment. Leveraging the alignment and a collaboration of annotations, we are able to hypothesize bot"
P19-1172,P17-1044,0,0.0254566,"he data and tools that we use to label our English Bibles and generate our morphological analyses. We also outline our evaluation metrics and describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al., 2018). Using the URIEL typological database (Littel et al., 2016), we limit the language"
P19-1172,N10-1103,0,0.275632,"Missing"
P19-1172,N07-1047,0,0.0475751,"r morphological analyses. We also outline our evaluation metrics and describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al., 2018). Using the URIEL typological database (Littel et al., 2016), we limit the languages to those that include affixing verbal and nominal inflection, and that distinctly"
P19-1172,kamholz-etal-2014-panlex,0,0.0208061,"d describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al., 2018). Using the URIEL typological database (Littel et al., 2016), we limit the languages to those that include affixing verbal and nominal inflection, and that distinctly mark plurality and temporality.5 Our evaluation set consists"
P19-1172,L18-1293,1,0.681645,"et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al., 2018). Using the URIEL typological database (Littel et al., 2016), we limit the languages to those that include affixing verbal and nominal inflection, and that distinctly mark plurality and temporality.5 Our evaluation set consists of 26 languages belonging to several language families such as Semitic, Germanic, Italic, Slavic, Uralic, and Bantu. For each of these languages, we randomly select a validation set of 5000 instances, and 1000 heldout instances.6 For our declension experiments, we approximate case from a majority of higher-resource morphological dictionaries, as described in Section 3.3"
P19-1172,N06-1014,0,0.090606,"d that one iteration of supplementing the training data was beneficial across our languages; subsequent iterations led to little further gain. 4 Experiments In this section, we describe the data and tools that we use to label our English Bibles and generate our morphological analyses. We also outline our evaluation metrics and describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lex"
P19-1172,W16-2701,0,0.0266243,"instead argue that much of this information is recoverable from syntactic and semantic parses, allowing us to use massively-parallel corpora such as the Bible. Kirov et al. (2017) notes the morphological sparsity of English, and reverses our setup, projecting morphologically rich tags from Czech into English. Rather than add another potentially noisy projection step (i.e., Czech to English to LRL), we instead leverage dependency and semantic parses to more richly tag English. In the area of contraint-based discovery, our methodology most closely resembles the constrained discovery systems of Lin et al. (2016) and particularly Upadhyay et al. (2018). Starting from a high-quality seed, a learning algorithm generalizes observed patterns, iteratively increasing the seed data with confident examples, while discarding examples that fail to pass certain heuristics. However, unlike previous work, we assume no gold seed annotations for our system - our seed is extracted exclusively from a noisy bitext word alignment. 3 Methods In this section, we describe our methods for inducing lemmas and morphological features pertaining to plurality, temporality, and case from aligned English-target Bibles. Our process"
P19-1172,C18-1008,0,0.0874434,"Missing"
P19-1172,P14-5010,0,0.00324872,"subsequent iterations led to little further gain. 4 Experiments In this section, we describe the data and tools that we use to label our English Bibles and generate our morphological analyses. We also outline our evaluation metrics and describe our experimental results. Our Bible data is obtained from the corpus of Mayer and Cysouw (2014), which consists of verse-parallel Bible data across 591 languages, including 27 English Bibles. The English and target Bibles are aligned using the Berkeley aligner (Liang et al., 2006), and POS tagged and syntactically parsed using the Stanford NLP toolkit (Manning et al., 2014). We semantically parse the Bibles using the Deep Semantic Role Labeler (He et al., 2017). The alignment filter is implemented using M2M aligner (Jiampojamarn et al., 2007), and our dictionaries come from PanLex (Kamholz et al., 2014); statistics concerning dictionary and training sizes are contained in the appendix. 4 For languages such as Arabic and Hebrew, where the citation form is not an attested word, we use the unmarked nominative singular form, instead. 1769 To evaluate the quality of the lexica that are produced, we extract gold validation and heldout sets from UniMorph (Kirov et al.,"
P19-1172,mayer-cysouw-2014-creating,0,0.134548,"otate the French words. Note that the projection is not lossless: the aligner could not find a French translation of “doubtless”, and has thus been unable to project the RB tag or advmod relation into French. Parallel corpora are rare, and even when they do exist, they often only exist between specific pairs of languages. However, the documentation of a language often begins with the creation of several important documents, including a dictionary of key terms, and translations of religious texts. Thus, documents such as the Christian Bible are among the most translated documents in the world (Mayer and Cysouw, 2014). Furthermore, the Bible consists of short, numbered chapters and verses consisting of a small number of sentences. Although not parallel to the standard required in fields such as machine translation, the structure of the Bible means that different Bibles are approximately parallel across verses. We follow a tradition of projecting POS tags from a high-resource language onto a language with fewer available tools (Yarowsky et al., 2001; Fossum and Abney, 2005; Agi´c et al., 2015; Buys 1765 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1765–1774"
P19-1172,N15-1186,0,0.062909,"al morphology, and deploy a new set of machine learning techniques to do so. Futhermore, we significantly expand the languages included in our test set, from 3 to 26 typologically diverse languages, substantially increasing the range of morphosyntactic phenomena covered and assessed. Similarly, Fossum and Abney (2005) and Agi´c et al. (2015) exploit the parallel nature of the Bible to project POS tags and train taggers in the target languages, leveraging the signal from multiple languages to improve the tagger accuracy. We focus, instead, on the induction of detailed morphological categories. Soricut and Och (2015) induce morphological transformation rules in an unsupervised manner. While this is analogous to lemmatization, part of our motivation is to also produce detailed morphological features that might be useful to train lowresource taggers, or to more richly annotate morphologically sparse languages such as English. Buys and Botha (2016) train morphological taggers in morphologically rich languages from an English projection. However, their method is dependent upon an English corpus tagged with more morphologically aware tags than are typically produced by an off-the-shelf English POS tagger. We i"
P19-1172,D18-1046,0,0.0232662,"formation is recoverable from syntactic and semantic parses, allowing us to use massively-parallel corpora such as the Bible. Kirov et al. (2017) notes the morphological sparsity of English, and reverses our setup, projecting morphologically rich tags from Czech into English. Rather than add another potentially noisy projection step (i.e., Czech to English to LRL), we instead leverage dependency and semantic parses to more richly tag English. In the area of contraint-based discovery, our methodology most closely resembles the constrained discovery systems of Lin et al. (2016) and particularly Upadhyay et al. (2018). Starting from a high-quality seed, a learning algorithm generalizes observed patterns, iteratively increasing the seed data with confident examples, while discarding examples that fail to pass certain heuristics. However, unlike previous work, we assume no gold seed annotations for our system - our seed is extracted exclusively from a noisy bitext word alignment. 3 Methods In this section, we describe our methods for inducing lemmas and morphological features pertaining to plurality, temporality, and case from aligned English-target Bibles. Our process is outlined in Figure 2. After annotati"
P19-1172,H01-1035,1,0.847298,"ictionary of key terms, and translations of religious texts. Thus, documents such as the Christian Bible are among the most translated documents in the world (Mayer and Cysouw, 2014). Furthermore, the Bible consists of short, numbered chapters and verses consisting of a small number of sentences. Although not parallel to the standard required in fields such as machine translation, the structure of the Bible means that different Bibles are approximately parallel across verses. We follow a tradition of projecting POS tags from a high-resource language onto a language with fewer available tools (Yarowsky et al., 2001; Fossum and Abney, 2005; Agi´c et al., 2015; Buys 1765 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1765–1774 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Botha, 2016). Our contributions, however, lie on the level of morphology and morphosyntax. With no further resources in the target language than a Bible translation and a dictionary, we project English POS tags, dependency relations, and semantic labels across the alignment. Leveraging the alignment and a collaboration of annotations, we are"
P19-1172,E17-2018,0,0.0710215,"led morphological features that might be useful to train lowresource taggers, or to more richly annotate morphologically sparse languages such as English. Buys and Botha (2016) train morphological taggers in morphologically rich languages from an English projection. However, their method is dependent upon an English corpus tagged with more morphologically aware tags than are typically produced by an off-the-shelf English POS tagger. We instead argue that much of this information is recoverable from syntactic and semantic parses, allowing us to use massively-parallel corpora such as the Bible. Kirov et al. (2017) notes the morphological sparsity of English, and reverses our setup, projecting morphologically rich tags from Czech into English. Rather than add another potentially noisy projection step (i.e., Czech to English to LRL), we instead leverage dependency and semantic parses to more richly tag English. In the area of contraint-based discovery, our methodology most closely resembles the constrained discovery systems of Lin et al. (2016) and particularly Upadhyay et al. (2018). Starting from a high-quality seed, a learning algorithm generalizes observed patterns, iteratively increasing the seed da"
P92-1032,P90-1032,0,0.01869,"Missing"
P92-1032,C90-2067,0,0.022795,"Missing"
P92-1032,C92-2070,1,0.732359,"uation, but fails to offer any very satisfying solutions that we might adopt to quantify the performance of our two disambiguation algorithms. 3 At first, we thought that the method was completely dependent on the availability of parallel corpora for training. This has been a problem since parallel text remains somewhat difficult to obtain in large quantity, and what little is available is often fairly unbalanced and unrepresentative of general language. Moreover, the assumption that differences in translation correspond to differences in word-sense has always been somewhat suspect. Recently, Yarowsky (1992) has found a way to extend our use of the Bayesian techniques by training on the Roget's Thesaurus (Chapman, 1977) 2 and G-rolier's Encyclopedia (1991) instead of the Canadian Hansards, thus circumventing many of the objections to our use of the Hansards. Yarowsky (1992) inputs a 100-word context surrounding a polysemous word and scores each of the 1042 Roget Categories by: 1-[ Pr(wlRoget Perhaps the most common evaluation technique is to select a small sample of words and compare the results of the machine with those of a human judge. This method has been used very effectively by Kelly and St"
P92-1032,P91-1034,0,0.135016,"Missing"
P92-1032,A88-1019,1,0.397426,"Missing"
P92-1032,P91-1017,0,0.0511046,"Missing"
P92-1032,H92-1045,1,\N,Missing
P94-1013,H89-1054,0,0.0358142,"Missing"
P94-1013,H93-1051,0,0.185811,"Missing"
P94-1013,J94-2001,0,0.0630884,"Missing"
P94-1013,C90-3049,0,0.0140732,"Missing"
P94-1013,C92-2070,1,0.346708,"Missing"
P94-1013,H93-1052,1,0.705596,"Missing"
P94-1013,P91-1034,0,0.121484,"Missing"
P94-1013,P92-1023,0,0.0316725,"Missing"
P94-1013,A88-1019,0,0.171575,"Missing"
P95-1026,P91-1034,0,0.111387,"Missing"
P95-1026,P94-1020,0,0.0795683,"Missing"
P95-1026,J94-4003,0,0.146338,"Missing"
P95-1026,P91-1019,0,0.0812462,"Missing"
P95-1026,H93-1051,0,0.0823684,"Missing"
P95-1026,C90-2067,0,0.0799115,"Missing"
P95-1026,C92-2070,1,0.109578,"y words provide strong and consistent clues to the sense of a target word, conditional on relative distance, order and syntactic relationship. 2. One sense p e r discourse: The sense of a target word is highly consistent within any given document. for each sense, This procedure is robust and selfcorrecting, and exhibits many strengths of supervised approaches, including sensitivity to word-order information lost in earlier unsupervised algorithms. 2 The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quantified in Gale, Church and Yarowsky (1992). Yet to date, the full power of this property has not been exploited for sense disambiguation. The work reported here is the first to take advantage of this regularity in conjunction with separate models of local context for each word. Importantly, I do not use one-sense-per-discourse as a hard constraint; it affects the classification probabilistically and can be overridden when local evidence is strong. In this current work, the one-sense-per-discourse hypothesis was tested on a set of 37,232 examples (hand-tagged over a period of 3 years), the same data studied in the disambiguation experi"
P95-1026,H93-1052,1,0.200368,"Missing"
P95-1026,P94-1013,1,0.130706,"fe ? company manufacturing plant is i n O r l a n d o ... ? ... g r o w t h o f a q u a t i c plant life i n w a t e r ... ? a u t o m a t e d m a n u f a c t u r i n g plant in F r e m o n t , ? ... A n i m a l a n d plant life a r e d e l i c a t e l y ? d i s c o v e r e d a t a S t . L o u i s plant m a n u f a c t u r i n g ? c o m p u t e r m a n u f a c t u r i n g plant a n d a d j a c e n t ... ? ... t h e p r o l i f e r a t i o n o f plant a n d a n i m a l life ? °Including variants of the EM algorithm (Bantu, 1972; Dempster et al., 1977), especially as applied in Gale, Church and Yarowsky (1994). 7Indeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here. These include Bayesian classifiers (Mosteller and Wallace, 1964) and some implementations of neural nets, but not BrK! rules (Brill, 1993). 190 S T E P 2: For each possible sense of the word, identify a relatively small number of training examples representative of that sense, s This could be accomplished by hand tagging a subset of the training sentences. However, I avoid this laborious procedure by identifying a small number of seed collocations representative"
P99-1022,J90-2002,0,0.16842,"Missing"
P99-1022,P98-1035,0,0.0314858,"ce wi... wj ; a common size for m is 3 (trigram language models). Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context. Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al. (1993)), even more general, try to capture the interrelationships between words. Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al. (1993)) effectively estimate intra-sentence syntactic word dependencies. The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics. We propose to compute the conditional language model probability as a dynamic mixture model of K topicspecific language models: 167 2 The Data The data used in this research is the Broadcast News (BN94) corpus, consisting of radio and TV news transcripts form the year 1994. From the total of 30226 documents, 20226 were used for training and the other 10000 were u"
P99-1022,H94-1014,0,0.0481707,"Missing"
P99-1022,H94-1013,0,0.0183234,"peace given major news topic distinctions (e.g. BUSINESS and INTERNATIONAL news) as illustrated in Figure 1. There is substantial subtopic probability variation for peace within INTERNATIONAL news (the word usage is 50-times more likely where w{ denotes the sequence wi... wj ; a common size for m is 3 (trigram language models). Even if n-grams were proved to be very powerful and robust in various tasks involving language models, they have a certain handicap: because of the Markov assumption, the dependency is limited to very short local context. Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al. (1993)), even more general, try to capture the interrelationships between words. Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al. (1993)) effectively estimate intra-sentence syntactic word dependencies. The approach we present here is based on the observation that certain words tend to have different probability distributions in different topics. We propose to compute the conditional language model probability as a dynamic mixture model"
P99-1022,C98-1035,0,\N,Missing
S01-1040,P00-1035,1,0.889368,"Missing"
S01-1040,P98-1080,0,0.0285784,"d context, local bigram and trigram collocations and several syntactic relationships based on predicate-argument structure (described in Section 1.2). Their use is illustrated on a sample English sentence for train in Figure 1. 1.1 Part-of-Speech Tagging and Lemmatization Part-of-speech tagger availability varied across the languages included in this sense-disambiguation system evaluation. Transformation-based taggers (Ngai and Florian, 2001) were trained on standard data for English (Penn Treebank), Swedish (SUC-1 corpus) and Estonian (MultextEast corpus). For Czech, an available POS tagger (Hajic and Hladka, 1998), which includes lemmatization, was used. The remaining languages - Spanish, Italian and Basque were tagged using an unsupervised tagger (Cucerzan 163 Syntactic {predicate-argument) features Object Prep ObjPrep children until age NNS IN NN child/N until/I age/N Ngram collocational features and Yarowsky, 2000). Lemmatization was performed using a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000), and using existing trie-based supervised models for English. 1.2 Syntactic Features Extracted syntactic relationships in the feature space depended on the keyword's pa"
S01-1040,P98-1081,0,0.0696734,"Missing"
S01-1040,P00-1027,1,0.878851,"Missing"
S01-1040,N01-1006,1,\N,Missing
S01-1040,C98-1077,0,\N,Missing
S01-1040,C98-1078,0,\N,Missing
S07-1042,S07-1012,0,0.650599,"Missing"
S07-1042,W03-0405,1,\N,Missing
W02-1004,P98-1029,0,0.407309,"Missing"
W02-1004,J95-4004,0,0.0345552,"Missing"
W02-1004,P00-1035,1,0.921822,"Missing"
W02-1004,W02-1005,1,0.880993,"Missing"
W02-1004,S01-1001,0,0.0215764,"Missing"
W02-1004,W99-0623,0,0.111867,"Missing"
W02-1004,N01-1006,1,0.325593,"Missing"
W02-1004,A00-2009,0,0.16041,"Missing"
W02-1004,C00-2124,0,0.0656361,"Missing"
W02-1004,J01-3001,0,0.0297114,"Missing"
W02-1004,P98-1081,0,0.107306,"Missing"
W02-1004,J01-2002,0,0.0972506,"Missing"
W02-1004,P00-1027,1,0.886884,"Missing"
W02-1004,C98-1078,0,\N,Missing
W02-1004,C98-1029,0,\N,Missing
W02-1005,P98-1029,0,0.179273,"Missing"
W02-1005,J95-4004,0,0.327789,"Missing"
W02-1005,W96-0210,0,0.0517602,"Missing"
W02-1005,P00-1035,1,0.91969,"Missing"
W02-1005,W02-1004,1,0.881845,"Missing"
W02-1005,P96-1010,0,0.141012,"Missing"
W02-1005,W95-0104,0,0.730587,"Missing"
W02-1005,W96-0208,0,0.133603,"Missing"
W02-1005,N01-1006,0,0.150006,"Missing"
W02-1005,C98-1029,0,\N,Missing
W02-1005,S01-1001,0,\N,Missing
W02-2006,W99-0613,0,0.010463,"al gender is performed via global modeling of contextwindow feature agreement. Using a combination of these and other evidence sources, interactive training of context and lexical prior models are accomplished for fine-grained POS tag spaces. Experiments show high accuracy, fine-grained tag resolution with minimal new human effort. 1 Introduction Previous work in minimally supervised language learning has defined minimal using several different criteria. Some have assumed only partially tagged training corpora (Merialdo, 1994), while others have begin with small tagged seed wordlists (such as Collins and Singer (1999) and Cucerzan and Yarowsky (1999) for named-entity tagging). Others have exploited the automatic transfer of some already existing annotated resource in a different medium or language (such as the translingual projection of part-of-speech tags, syntactic bracketing and inflectional morphology in Yarowsky et al. (2001), requiring no direct supervision in the foreign language). Ngai and Yarowsky (2000) observed that an often more practical measure of the degree of supervision is not simply the quantity of annotated words, but the total weighted human labor and resource costs of different modes o"
W02-2006,W99-0612,1,0.249222,"bal modeling of contextwindow feature agreement. Using a combination of these and other evidence sources, interactive training of context and lexical prior models are accomplished for fine-grained POS tag spaces. Experiments show high accuracy, fine-grained tag resolution with minimal new human effort. 1 Introduction Previous work in minimally supervised language learning has defined minimal using several different criteria. Some have assumed only partially tagged training corpora (Merialdo, 1994), while others have begin with small tagged seed wordlists (such as Collins and Singer (1999) and Cucerzan and Yarowsky (1999) for named-entity tagging). Others have exploited the automatic transfer of some already existing annotated resource in a different medium or language (such as the translingual projection of part-of-speech tags, syntactic bracketing and inflectional morphology in Yarowsky et al. (2001), requiring no direct supervision in the foreign language). Ngai and Yarowsky (2000) observed that an often more practical measure of the degree of supervision is not simply the quantity of annotated words, but the total weighted human labor and resource costs of different modes of supervision (allowing manual ru"
W02-2006,P00-1035,1,0.800959,"e of parts of speech with little human effort. We then perform a noise-robust combination of model estimation and re-estimation techniques for the syntagmatic trigram models       and lexical priors      using the word cooccurrence information from a raw corpus.  A   suffix-based part-of-speech probability model   suffix   using hierarchically smoothed tries is trained on the raw initial tag distributions, yielding coverage to unseen words and smoothing of low-confidence initial tag assignments.  Paradigmatic cross-context tag modeling is performed as in Cucerzan and Yarowsky (2000) when sufficiently large unannotated corpora are available.  Sub-part-of-speech contextual agreement for features such as gender is performed as described in Section 4.1.  The  part-of-speech tag sequence models       utilize a weighted backoff between fine-grained and coarse-grained tags.  Both the tag-sequence and lexical prior models are iteratively retrained using these additional evidence sources and first-pass probability distributions. 2 For processing efficiency, one additional constraint is that potential hypothesized observed string pair candidates must exactly ma"
W02-2006,J94-2001,0,0.0386011,"ve model using weighted Levenshtein alignments. Unsupervised induction of grammatical gender is performed via global modeling of contextwindow feature agreement. Using a combination of these and other evidence sources, interactive training of context and lexical prior models are accomplished for fine-grained POS tag spaces. Experiments show high accuracy, fine-grained tag resolution with minimal new human effort. 1 Introduction Previous work in minimally supervised language learning has defined minimal using several different criteria. Some have assumed only partially tagged training corpora (Merialdo, 1994), while others have begin with small tagged seed wordlists (such as Collins and Singer (1999) and Cucerzan and Yarowsky (1999) for named-entity tagging). Others have exploited the automatic transfer of some already existing annotated resource in a different medium or language (such as the translingual projection of part-of-speech tags, syntactic bracketing and inflectional morphology in Yarowsky et al. (2001), requiring no direct supervision in the foreign language). Ngai and Yarowsky (2000) observed that an often more practical measure of the degree of supervision is not simply the quantity o"
W02-2006,W00-0712,0,0.00748702,"Because all dictionary entries are equally weighted, errors on rare words such as mythological characters or kinship terms can substantially downgrade performance. But for the purposes of providing seed POS distributions to context-sensitive taggers, performance is quite adequate for this follow-on task. 3 Inducing Morphological Analyses There has been extensive previous work in the supervised and minimally supervised induction of both affix paradigms (e.g. Goldsmith, 2000; Snover and Brent, 2001) and diverse models of regular and irregular concatenative and non-concatenative morphology (e.g. Schone and Jurafsky, 2000; van den Bosch and Daelemans, 1999; Yarowsky and Wicentowski, 2000). While such approaches are important from the perspective of learning theory or broad coverage handling of irregular forms, another possible paradigm for minimal supervision is to begin with whatever knowledge can be efficiently manually entered from the grammar book in several hours work. We defined such grammar-based “supervision” as entry of regular inflectional affix changes and their associated part of speech in standardized ordering of fine-grained attributes, as in Table 2 for Spanish and Romanian. The full tables have"
W02-2006,P01-1063,0,0.0833373,"y hindered by frequent long phrasal translations which often included an explanation or definition in their translation. Because all dictionary entries are equally weighted, errors on rare words such as mythological characters or kinship terms can substantially downgrade performance. But for the purposes of providing seed POS distributions to context-sensitive taggers, performance is quite adequate for this follow-on task. 3 Inducing Morphological Analyses There has been extensive previous work in the supervised and minimally supervised induction of both affix paradigms (e.g. Goldsmith, 2000; Snover and Brent, 2001) and diverse models of regular and irregular concatenative and non-concatenative morphology (e.g. Schone and Jurafsky, 2000; van den Bosch and Daelemans, 1999; Yarowsky and Wicentowski, 2000). While such approaches are important from the perspective of learning theory or broad coverage handling of irregular forms, another possible paradigm for minimal supervision is to begin with whatever knowledge can be efficiently manually entered from the grammar book in several hours work. We defined such grammar-based “supervision” as entry of regular inflectional affix changes and their associated part"
W02-2006,P99-1037,0,0.016596,"Missing"
W02-2006,P00-1027,1,0.443621,"rare words such as mythological characters or kinship terms can substantially downgrade performance. But for the purposes of providing seed POS distributions to context-sensitive taggers, performance is quite adequate for this follow-on task. 3 Inducing Morphological Analyses There has been extensive previous work in the supervised and minimally supervised induction of both affix paradigms (e.g. Goldsmith, 2000; Snover and Brent, 2001) and diverse models of regular and irregular concatenative and non-concatenative morphology (e.g. Schone and Jurafsky, 2000; van den Bosch and Daelemans, 1999; Yarowsky and Wicentowski, 2000). While such approaches are important from the perspective of learning theory or broad coverage handling of irregular forms, another possible paradigm for minimal supervision is to begin with whatever knowledge can be efficiently manually entered from the grammar book in several hours work. We defined such grammar-based “supervision” as entry of regular inflectional affix changes and their associated part of speech in standardized ordering of fine-grained attributes, as in Table 2 for Spanish and Romanian. The full tables have approximately 200 lines each and required roughly 1.5-2 person-hour"
W02-2006,H01-1035,1,\N,Missing
W02-2006,N01-1026,1,\N,Missing
W02-2006,J01-2001,0,\N,Missing
W02-2007,W99-0612,1,0.95717,"a language independent model for named entity recognition based on iterative learning in a co-training fashion, using word-internal and contextual information as independent evidence sources. Its bootstrapping process begins with only seed entities and seed contexts extracted from the provided annotated corpus. F-measure exceeds 77 in Spanish and 72 in Dutch. 1. Introduction Our aim has been to build a maximally languageindependent system for named-entity recognition using minimal supervision or knowledge of the source language. The core model utilized, extended and evaluated here is based on Cucerzan and Yarowsky (1999). It assumes that only an entity exemplar list is provided as a bootstrapping seed set. For the particular task of CoNLL-2002, the seed entities are extracted from the provided annotated corpus. As a consequence, the seed examples may be ambiguous and the system must therefore handle seeds with probability distribution over entity classes rather than unambiguous seeds. Another consequence is that this approach of extracting only the entity seeds from the annotated text does not use the full potential of the training data, ignoring contextual information. For example, Bosnia appears labeled 9 t"
W02-2007,W02-2006,1,0.874722,"Missing"
W02-2007,H92-1045,1,0.742321,"L KK L KK L KK L KK L KK L KK LLLLLLLLLLLLLLLLLLLL Topic boundary Topic boundary Soft boundary Soft boundary Topic boundary MNM MNM MNM MNM NNNN Other occurences of w Topic boundary Topic boundary Word position in the corpus Figure 3: Using contextual clues from all instances of an entity candidate in the corpus. Each instance is depicted as a disc with the diameter representing the confidence of the classification of that instance using word-internal and local contextual information. emphasis and clarity”. We use this property in conjunction with the one sense per discourse tendency noted by Gale et al. (1992). The later paradigm is not directly usable when analyzing a large corpus in which there are no document boundaries, like the one provided for Spanish. Therefore, a segmentation process needs to be employed, so that all the instances of a name in a segment have a high probability of belonging to the same class. Our approach is to consider a ’soft’ segmentation, which is worddependent and does not compute topic/document boundaries but regions for which the contextual information for all instances of a word can be used jointly when making a decision. This is viewed as an alternative to the class"
W02-2007,W97-0305,0,0.0281831,"h. Therefore, a segmentation process needs to be employed, so that all the instances of a name in a segment have a high probability of belonging to the same class. Our approach is to consider a ’soft’ segmentation, which is worddependent and does not compute topic/document boundaries but regions for which the contextual information for all instances of a word can be used jointly when making a decision. This is viewed as an alternative to the classical topic segmentation approach and can be used in conjunction with a language-independent segmentation system (Figure 3) like the one presented by Richmond et al. (1997). After estimating the class probability distributions for all instances of entity candidates in the corpus, a re-estimation step is employed. The probability of an entity class O&apos;P given an entity candidate Q at position RSUTV is re-computed using the formula: WXZY[ X$[ O&apos;[ P] Q_^`RSUTV acbefh d gjikm[ l don O&apos;P] Qp^`RqST k a0r T s7t RSUTV ^`RSUT k aurOvSwyx Qp^`RSUT k a (2) where RSUT ^mzzz^`RSUT i are the positions of all ind stances of Q in the corpus, T5s<t is the positional similarity, encoding the physical distance and topic (if topic or document boundary information"
W02-2007,W96-0102,0,\N,Missing
W02-2026,P00-1027,1,\N,Missing
W02-2026,N01-1020,1,\N,Missing
W02-2026,P99-1067,0,\N,Missing
W03-0405,P01-1059,0,0.00435174,"d for word sense disambiguation (Gale et al., 1992). We then present corroborating evidence from real personal name polysemy to show that this technique works in practice. birth day birth year occupation birth place birth year occupation birth place Miles Davis May 26(5), May 25(5) 1926(82), 1967(18), 1969(9)... trumpeter(38), artist(10), player(5)... Alton(7), Illinois(6) Joerg Haider 1950(6) leader(198) politician(93) chairman(6)... Austria(1) Table 1: Extracted Biographical Information from 1000 Web Pages Another topic of recent interest is in producing biographical summaries from corpora (Schiffman et al., 2001). Along with disambiguation, our system simultaneously collects biographic information (Table 1). The relevant biographical attributes are depicted along with a clustering which shows the distinct referents (Section 4.1). 2 Robust Extraction of Categorical Biographic Data Past work on this task (e.g. Bagga and Baldwin, 1998) has primarily approached personal name disambiguation using document context profiles or vectors, which recognize and distinguish identical name instances based on partially indicative words in context such as computer or car in the Clark case. However, in the specialized"
W03-0405,A97-1030,0,0.0418412,"ambiguity to be a major stumbling block in personal name web searches. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. Jim Clark - Race car driver from Scotland Jim Clark - Clockmaker from Colorado Jim Clark - Film Editor Jim Clark - Netscape Founder Jim Clark - Disaster Survivor Jim Clark - Car Salesman in Kansas Jim Clark - Fishing Instructor in Canada Jim Clark - Computer Science student in Hong Kong Jim Clark - Professor at McGill Jim Clark - Gun Dealer in Louisiana In this paper, we present a method for distinguishing the real world referent of a given name in context. Approaches to this problem include Wacholder et al. (1997), focusing on the variation of surface name for a given referent, and Smith and Crane (2002), resolving geographic name ambiguity. We present preliminary evaluation on pseudonames: conflations of multiple personal names, constructed in the same way pseudowords are used for word sense disambiguation (Gale et al., 1992). We then present corroborating evidence from real personal name polysemy to show that this technique works in practice. birth day birth year occupation birth place birth year occupation birth place Miles Davis May 26(5), May 25(5) 1926(82), 1967(18), 1969(9)... trumpeter(38), art"
W03-0405,A00-1039,0,0.0106644,"tring Patterns with <person> and <birth year> Extractions of <person> with potential <birth years> Extractions where <birth year> is a year Extraction graded by #correct extractions/#total extractions <person> ( <birth year> − dddd) <person> <birthyear>−dddd <person> was born in <birth year> <person> ( b.<birth year> Figure 1: Learning Extraction Patterns from Filled Templates and Web Pages In the late 90s, there was a substantial body of research on learning information extraction patterns from templates (Huffman, 1995; Brin, 1998; Califf and Mooney, 1998; Freitag and McCallum, 1999; Yangarber et al., 2000; Ravichandran and Hovy, 2002). These techniques provide a way to bootstrap information extraction patterns from a set of example extractions or seed facts, where a tuple with the filled roles for the desired pattern are given. For the task of extracting biographical information, each example would include the personal name and the biographic feature. For example, training data for the pattern born in might be (“Wolfgang Amadeus Mozart”,1756). Given this set of examples, each method generates patterns differently. In this paper, we employ and extend the method described by Ravichandran and Hov"
W03-0405,P98-1012,0,\N,Missing
W03-0405,C98-1012,0,\N,Missing
W03-0405,P02-1006,0,\N,Missing
W03-1002,J00-1004,0,0.0394348,"un swapping, is adequately modeled by the relative-position-based distortion models of the classic IBM approach. Unfortunately, these distortion models are less effective for languages such as Japanese or Arabic, which have substantially different top-level sentential word orders from English, and hence longer distance constituent motion. Wu (1997) and Jones and Havrilla (1998) have sought to more closely tie the allowed motion of constituents between languages to those syntactic transductions supported by the independent rotation of parse tree constituents. Yamada and Knight (2000, 2001) and Alshawi et al. (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a limitation shared with probabilistic context free grammars that the preferred ordering of subtrees is insufficiently constrained by their embedding context, which is especially problematic for very deep syntactic parses. In contrast, Och et al. (1999) have avoided the constraints of"
W03-1002,W00-0508,0,0.0162845,"flat embedded noun-phrases in a flat sentential constituent-based chunk sequence that can be driven by syntactic bracketers and POS tag models rather than a full parser, facilitating its transfer to lower density languages. The flatter 2-level structures also better support transductions conditioned to full sentential context than do deeply embedded tree models, while retaining the empirically observed advantages of translation ordering independence of nounphrases. Another improvement over Och et al. and Yamada and Knight is the use of the finite state machine (FSM) modelling framework (e.g. Bangalore and Riccardi, 2000), which offers the considerable advantage of a flexible framework for decoding, as well as a representation which is suitable for the fixed two-level phrasal modelling employed here. Finally, the original cross-part-of-speech lexical coercion models presented in Section 4.3.3 have related work in the primarily-syntactic coercion models utilized by Dorr and Habash (2002) and Habash and Door (2003), although their induction and modelling are quite different from the approach here. 3 Resources As in other SMT approaches, the primary training resource is a sentence-aligned parallel bilingual corpu"
W03-1002,J93-2003,0,0.0129159,"ce-level order of Verb-SubjectObject, which means that translation into English (with a standard ordering of Subject-Verb-Object) commonly requires motion of entire phrasal constituents, which is not true of French-to-English translation, to cite one language pair whose characteristics have wielded great influence in the history of work on statistical machine translation. A key motivation for and objective of this work was to build a translation model and feature space to handle the above-described phenomenon effectively. 2 Prior Work Statistical machine translation, as pioneered by IBM (e.g. Brown et al., 1993), is grounded in the noisy channel model. And similar to the related channel problems of speech and handwriting recognition, the original SMT language pair French-English exhibits a relatively close linear correlation in source and target sequence. Much common local motion that is observed for French, such as adjective-noun swapping, is adequately modeled by the relative-position-based distortion models of the classic IBM approach. Unfortunately, these distortion models are less effective for languages such as Japanese or Arabic, which have substantially different top-level sentential word ord"
W03-1002,W02-2006,1,0.816279,"re. Finally, the original cross-part-of-speech lexical coercion models presented in Section 4.3.3 have related work in the primarily-syntactic coercion models utilized by Dorr and Habash (2002) and Habash and Door (2003), although their induction and modelling are quite different from the approach here. 3 Resources As in other SMT approaches, the primary training resource is a sentence-aligned parallel bilingual corpus. We further require that each side of the corpus be partof-speech (POS) tagged and phrase chunked; our lab has previously developed techniques for rapid training of such tools (Cucerzan and Yarowsky, 2002). Our translation experiments were carried out on two languages: Arabic and French. The Arabic training corpus was a subset of the United Nations (UN) parallel corpus which is being made available by the Linguistic Data Consortium. For French-English training, we used a portion of the Canadian Hansards. Both corpora utilized sentencelevel alignments publicly distributed by the Linguistic Data Consortium. POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit (Ngai and Florian, 2001); both were trained from the annotated Penn Treebank corp"
W03-1002,P91-1023,0,0.0651479,"Missing"
W03-1002,N03-1013,0,0.0208581,"Missing"
W03-1002,J93-2004,0,0.0514913,"r translation experiments were carried out on two languages: Arabic and French. The Arabic training corpus was a subset of the United Nations (UN) parallel corpus which is being made available by the Linguistic Data Consortium. For French-English training, we used a portion of the Canadian Hansards. Both corpora utilized sentencelevel alignments publicly distributed by the Linguistic Data Consortium. POS tagging and phrase chunking in English were done using the trained systems provided with the fnTBL Toolkit (Ngai and Florian, 2001); both were trained from the annotated Penn Treebank corpus (Marcus et al., 1993). French POS tagging was done using the trained French lexical tagger also provided with the fnTBL software. For Arabic, we used a colleague’s POS tagger and tokenizer (clitic separation was also performed prior to POS tagging), which was rapidly developed in our laboratory. Simple regular-expression-based phrase chunkers were developed by the authors for both Arabic and French, requiring less than a person-day each using existing multilingual learning tools. A further input to our system is a set of word alignment links on the parallel corpus. These are used to compute word translation probab"
W03-1002,P00-1056,0,0.435153,"rmed prior to POS tagging), which was rapidly developed in our laboratory. Simple regular-expression-based phrase chunkers were developed by the authors for both Arabic and French, requiring less than a person-day each using existing multilingual learning tools. A further input to our system is a set of word alignment links on the parallel corpus. These are used to compute word translation probabilities and phrasal alignments. The word alignments can in principle come from any source: a dictionary, a specialized alignment program, or another SMT system. We used alignments generated by Giza++ (Och and Ney, 2000) by running it in both directions (e.g., Arabic → English and English → Arabic) on our parallel corpora. The union of these bidirectional alignments was used to compute cross-language phrase correspondences by simple majority voting, and for purposes of estimating word translation probabilities, each link in this union was treated as an independent instance of word translation. 4 Translation Model Now we turn to a detailed description of the proposed translation model. The exposition will give a formal specification and also will follow a running example throughout, using one of the actual Ara"
W03-1002,W99-0604,0,0.145082,"and Knight (2000, 2001) and Alshawi et al. (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a limitation shared with probabilistic context free grammars that the preferred ordering of subtrees is insufficiently constrained by their embedding context, which is especially problematic for very deep syntactic parses. In contrast, Och et al. (1999) have avoided the constraints of tree-based syntactic models and allow the relatively flat motion of empirically derived phrasal chunks, which need not adhere to traditional constituent boundaries. Our current paper takes a middle path, by grounding motion in syntactic transduction, but in a much flatter 2level model of syntactic analysis, based on flat embedded noun-phrases in a flat sentential constituent-based chunk sequence that can be driven by syntactic bracketers and POS tag models rather than a full parser, facilitating its transfer to lower density languages. The flatter 2-level struc"
W03-1002,2001.mtsummit-papers.68,0,0.0642416,"Missing"
W03-1002,J97-3002,0,0.0448955,"ated channel problems of speech and handwriting recognition, the original SMT language pair French-English exhibits a relatively close linear correlation in source and target sequence. Much common local motion that is observed for French, such as adjective-noun swapping, is adequately modeled by the relative-position-based distortion models of the classic IBM approach. Unfortunately, these distortion models are less effective for languages such as Japanese or Arabic, which have substantially different top-level sentential word orders from English, and hence longer distance constituent motion. Wu (1997) and Jones and Havrilla (1998) have sought to more closely tie the allowed motion of constituents between languages to those syntactic transductions supported by the independent rotation of parse tree constituents. Yamada and Knight (2000, 2001) and Alshawi et al. (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a limitation s"
W03-1002,P01-1067,0,0.353681,"Missing"
W03-1002,P02-1039,0,0.0253483,"Missing"
W03-1002,J93-1004,0,\N,Missing
W03-1002,jones-havrilla-1998-twisted,0,\N,Missing
W03-1002,N01-1006,0,\N,Missing
W03-1002,P02-1040,0,\N,Missing
W03-1002,2003.mtsummit-systems.9,0,\N,Missing
W05-0807,J99-2004,0,0.0138788,"ited to languages such as Czech (e.g. Hajiˇc and Hladk´a, 1998) or French (e.g. Foster etc.) where finer-grained tagset distinctions are morphologically marked and hence natural for the language. In support of supervised tagger learning of these languages, fine-trained tagset inventories have been developed by the teams above at Charles University (Czech) and Universit´e de Montr´eal (French). The tagset developed by Hajiˇc forms the basis of the distinctions used in this paper. The other major approach to fine-grained tagging involves using tree-based tags that capture grammatical structure. Bangalore and Joshi (1999) have utilized “supertags” based on tree-structures of various complexity in the tree-adjoining grammar model. Using such tags, Brants (2000) has achieved the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a treebank of German. 2.2 Classifier combination for part-of-speech tagging There has been broad work in classifier combination at the tag-level for supervised POS tagging models. For example, M`arquez and Rodr´ıguez (1998) have performed voting over an ensemble of decision"
W05-0807,A00-1031,0,0.0178121,"arked and hence natural for the language. In support of supervised tagger learning of these languages, fine-trained tagset inventories have been developed by the teams above at Charles University (Czech) and Universit´e de Montr´eal (French). The tagset developed by Hajiˇc forms the basis of the distinctions used in this paper. The other major approach to fine-grained tagging involves using tree-based tags that capture grammatical structure. Bangalore and Joshi (1999) have utilized “supertags” based on tree-structures of various complexity in the tree-adjoining grammar model. Using such tags, Brants (2000) has achieved the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a treebank of German. 2.2 Classifier combination for part-of-speech tagging There has been broad work in classifier combination at the tag-level for supervised POS tagging models. For example, M`arquez and Rodr´ıguez (1998) have performed voting over an ensemble of decision tree and HMM-based taggers for supervised En50 glish tagging. Murata et al. (2001) have combined neural networks, support vector machines, de"
W05-0807,N03-1006,1,0.921228,"ech POS taggers based on these projected tags. Their projected tagsets, however, were limited to those distinctions captured in the English Penn treebank inventory, and hence failed to make many of the finer grained distinctions traditionally assumed for French and Czech POS tagging, such as verb person, number, and polarity and noun/adjective case. Probst (2003) pursued a similar methodology for the purposes of tag projection, using a somewhat expanded tagset inventory (e.g. including adjective number but not case), and focusing on targetlanguage monolingual modeling using morpheme analysis. Cucerzan and Yarowsky (2003) addressed the problem of grammatical gender projection via the use of small seed sets based on natural gender. Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al. 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). Word Core POS Prsn Num. Case The books were provoking DT NN VB VB 3 3 3 3 PL. PL. PL. PL. N OM . N OM . laughter with their curious titles NN IN DT JJ NN 3 S. ACC . 3 3 3 PL. PL. PL. ‘ WITH ’ ‘ WITH ’ ‘ WITH ’ Tns/ Asp. Pol. PAST PASTP ROG . + + VB Voi. Person Numb"
W05-0807,P98-1080,0,0.0799729,"Missing"
W05-0807,P93-1016,0,0.0404708,"ures are those whose detection requires some degree of syntactic analysis. These include case, which summarizes the relation of each noun with its governor, and the agreementbased features: we define person, number, and case 52 for attributive adjectives by agreement with their head nouns, number and person for verbs and predicate adjectives by agreement with their subjects, and tense for some verbs by agreement with their inflected auxiliaries. We investigated four individual approaches for the syntax-features – a regular-expression-based quasi-parser, a system based on Dekang Lin’s MiniPar (Lin, 1993), a system based on the Collins parser (Collins, 1999), and one based on the CMU Link Grammar Parser (Sleator and Temperley, 1993), as well as a family of voting-based combination schemes. 4.1 Regular-expression Quasi-parser The regular-expression ‘quasi-parser’ takes a direct approach, using several dozen heuristics based on regular-expression-like patterns over words, Penn part-of-speech tags, and the output of the fnTBL noun chunker. Use of the noun chunker facilitates identification of noun/dependent relationships within chunks, and extends the range of patterns identifying noun/governor r"
W05-0807,N01-1006,0,0.0286102,"POS tags Word Core POS Prsn Num. Case Les livres provoquaient DT NN VB 3 3 3 PL. PL. PL. N OM . N OM . des rires avec ses titres curieux DT NN IN DT NN JJ 3 3 PL. PL. ACC . ACC . 3 3 3 PL. PL. PL. ‘ WITH ’ ‘ WITH ’ ‘ WITH ’ Tns/ Asp. Pol. PASTP ROGR . + Degree Voice Tense ACT. Perfectivity Progressivity Polarity Voice Figure 2: Example of fined-grained POS tags projected onto a French translation 3 Tagsets We use Penn treebank-style part-of-speech tags as a substrate for further enrichment (for all of the experiments described here, text was first tagged using the fnTBL part-of-speech tagger (Ngai and Florian, 2001)). Each Penn tag is mapped to a core part-of-speech tag, which determines the set of finegrained tags further applicable to each word. The fine-grained tags applicable to nouns, verbs, and adjective are shown in Table 1. This paper concentrates on these most important core parts-of-speech. The example English sentence in Figure 1 illustrates several key points about our tagset. Some of the information we are interested in is already expressed by the Penn-style tags – the NN titles is plural; the VBD were is in the past tense. For these, our goal is simply to make these facts explicit. On the o"
W05-0807,P00-1056,0,0.0140485,"f these models condition their probabilities first on the core part-of-speech of a word. We used the methods of Yarowsky et al. (2001) to develop a core part-of-speech tagger for French, based only on the projected core tags, and used this as a basis for fine-grained tags. We also ran experiments isolating the question of fine-grained tagging, assuming as input externally supplied core tags from the goldstandard data. Table 4 shows results under both of these assumptions. For French, the training data was 15 million words from the Canadian Hansards. Word alignments were produced using GIZA++ (Och and Ney, 2000) set to produce a maximum of one English word link for each French word (i.e., a French-toEnglish model). The test data was 111,000 words of text from the Laboratoire de Recherche Appliqu´ee en Linguistique Informatique at the Universit´e de Montr´eal, annotated with person, number, and tense.                55 Suffix none -s -is -ais Pr(P LURAL suffix) 32.5 66.5 35.3 16.2  Pr(S INGULAR suffix) 67.5 33.5 64.7 83.8 Figure 6: Example smoothed suffix trie probabilities for French noun number Several factors contributed to a fairly successful set of results. Th"
W05-0807,W03-0414,0,0.01635,"ork in the cross-lingual projection of part-of-speech tag annotations from English to French and Czech, by way of word-aligned parallel bilingual corpora. They also used noise-robust supervised training techniques to train stand-alone French and Czech POS taggers based on these projected tags. Their projected tagsets, however, were limited to those distinctions captured in the English Penn treebank inventory, and hence failed to make many of the finer grained distinctions traditionally assumed for French and Czech POS tagging, such as verb person, number, and polarity and noun/adjective case. Probst (2003) pursued a similar methodology for the purposes of tag projection, using a somewhat expanded tagset inventory (e.g. including adjective number but not case), and focusing on targetlanguage monolingual modeling using morpheme analysis. Cucerzan and Yarowsky (2003) addressed the problem of grammatical gender projection via the use of small seed sets based on natural gender. Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al. 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). Wo"
W05-0807,1993.iwpt-1.22,0,0.0725835,"the relation of each noun with its governor, and the agreementbased features: we define person, number, and case 52 for attributive adjectives by agreement with their head nouns, number and person for verbs and predicate adjectives by agreement with their subjects, and tense for some verbs by agreement with their inflected auxiliaries. We investigated four individual approaches for the syntax-features – a regular-expression-based quasi-parser, a system based on Dekang Lin’s MiniPar (Lin, 1993), a system based on the Collins parser (Collins, 1999), and one based on the CMU Link Grammar Parser (Sleator and Temperley, 1993), as well as a family of voting-based combination schemes. 4.1 Regular-expression Quasi-parser The regular-expression ‘quasi-parser’ takes a direct approach, using several dozen heuristics based on regular-expression-like patterns over words, Penn part-of-speech tags, and the output of the fnTBL noun chunker. Use of the noun chunker facilitates identification of noun/dependent relationships within chunks, and extends the range of patterns identifying noun/governor relationships across chunks. The output of the quasi-parser consists of two parts: a case tag for each noun in a sentence, and a se"
W05-0807,W04-3207,0,0.0226452,"ective case. Probst (2003) pursued a similar methodology for the purposes of tag projection, using a somewhat expanded tagset inventory (e.g. including adjective number but not case), and focusing on targetlanguage monolingual modeling using morpheme analysis. Cucerzan and Yarowsky (2003) addressed the problem of grammatical gender projection via the use of small seed sets based on natural gender. Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al. 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). Word Core POS Prsn Num. Case The books were provoking DT NN VB VB 3 3 3 3 PL. PL. PL. PL. N OM . N OM . laughter with their curious titles NN IN DT JJ NN 3 S. ACC . 3 3 3 PL. PL. PL. ‘ WITH ’ ‘ WITH ’ ‘ WITH ’ Tns/ Asp. Pol. PAST PASTP ROG . + + VB Voi. Person Number ACT. ACT. Case Figure 1: Example of fine-grained English POS tags Word Core POS Prsn Num. Case Les livres provoquaient DT NN VB 3 3 3 PL. PL. PL. N OM . N OM . des rires avec ses titres curieux DT NN IN DT NN JJ 3 3 PL. PL. ACC . ACC . 3 3 3 PL. PL. PL. ‘ WITH ’ ‘ WITH ’ ‘ WITH ’ Tns/ Asp. Pol. PASTP ROGR . + Degree Voice Tense"
W05-0807,H01-1035,1,0.936109,"ive, etc. variants), increasing both the standalone effectiveness of lexical prior models and wordsuffix models for part-of-speech tagging. However, for many multilingual applications, including feature-based word alignment in bilingual corpora and machine translation into morphologically richer languages, it is helpful to extract finergrained lexical analyses on the English side that more closely parallel the morphologically realized tagset of the second (source or target) language. In particular, prior work on translingual part-ofspeech tagger projection via parallel bilingual corpora (e.g. Yarowsky et al., 2001) has been limited to inducing part-of-speech taggers in second languages (such as French or Czech) that only assign tags at the granularity of their source language (i.e. This paper presents an original approach to part-of-speech tagging of fine-grained features (such as case, aspect, and adjective person/number) in languages such as English where these properties are generally not morphologically marked. The goals of such rich lexical tagging in English are to provide additional features for word alignment models in bilingual corpora (for statistical machine translation), and to provide an in"
W05-0807,J03-4003,0,\N,Missing
W05-0807,C98-1077,0,\N,Missing
W06-2906,P99-1008,0,0.0858481,"oaches to build a WordNet like resource automatically (Hearst, 1992; Cara1 The test examples were selected as follows: First, all the sentences containing definite NP “The Y” were extracted from the corpus. Then, the sentences containing instances of anaphoric definite NPs were kept and other cases of definite expressions (like existential NPs “The White House”,“The weather”) were discarded. From this anaphoric set of sentences, 177 sentence instances covering 13 distinct hypernyms were randomly selected as the test set and annotated for the correct antecedent by human judges. 38 ballo, 1999; Berland and Charniak, 1999). Also, several researchers have applied it to resolving different types of bridging anaphora (Clark, 1975). Poesio et al. (2002) have proposed extracting lexical knowledge about part-of relations using Hearst-style patterns and applied it to the task of resolving bridging references. Poesio et al. (2004) have suggested using Google as a source of computing lexical distance between antecedent and definite NP for mereological bridging references (references referring to parts of an object already introduced). Markert et al. (2003) have applied relations extracted from lexicosyntactic patterns s"
W06-2906,P99-1016,0,0.0490318,"Missing"
W06-2906,T75-2034,0,0.799841,"st, all the sentences containing definite NP “The Y” were extracted from the corpus. Then, the sentences containing instances of anaphoric definite NPs were kept and other cases of definite expressions (like existential NPs “The White House”,“The weather”) were discarded. From this anaphoric set of sentences, 177 sentence instances covering 13 distinct hypernyms were randomly selected as the test set and annotated for the correct antecedent by human judges. 38 ballo, 1999; Berland and Charniak, 1999). Also, several researchers have applied it to resolving different types of bridging anaphora (Clark, 1975). Poesio et al. (2002) have proposed extracting lexical knowledge about part-of relations using Hearst-style patterns and applied it to the task of resolving bridging references. Poesio et al. (2004) have suggested using Google as a source of computing lexical distance between antecedent and definite NP for mereological bridging references (references referring to parts of an object already introduced). Markert et al. (2003) have applied relations extracted from lexicosyntactic patterns such as ’X and other Ys’ for OtherAnaphora (referential NPs with modifiers other or another) and for bridgin"
W06-2906,N01-1008,0,0.492705,"Missing"
W06-2906,C92-2082,0,0.0116978,"Missing"
W06-2906,J05-3004,0,0.773569,"ing a clearly defined task for our evaluation purposes. We describe an unsupervised approach to this task that extracts examples containing definite NPs from a large corpus, considers all head words appearing before the definite NP as potential antecedents and then filters the noisy <antecedent, definite-NP> pair using Mutual Information space. The co-occurence statistics of such pairs can then be used as a mechanism for detecting a hypernym relation between the definite NP and its potential antecedents. We compare this approach with a WordNet-based algorithm and with an approach presented by Markert and Nissim (2005) on resolving definite NP coreference that makes use of lexico-syntactic patterns such as ’X and Other Ys’ as utilized by Hearst (1992). 2 Related work There is a rich tradition of work using lexical and semantic resources for anaphora and coreference resolution. Several researchers have used WordNet as a lexical and semantic resource for certain types of bridging anaphora (Poesio et al., 1997; Meyer and Dale, 2002). WordNet has also been used as an important feature in machine learning of coreference resolution using supervised training data (Soon et al., 2001; Ng and Cardie, 2002). However,"
W06-2906,W03-2606,0,0.0617205,"or the correct antecedent by human judges. 38 ballo, 1999; Berland and Charniak, 1999). Also, several researchers have applied it to resolving different types of bridging anaphora (Clark, 1975). Poesio et al. (2002) have proposed extracting lexical knowledge about part-of relations using Hearst-style patterns and applied it to the task of resolving bridging references. Poesio et al. (2004) have suggested using Google as a source of computing lexical distance between antecedent and definite NP for mereological bridging references (references referring to parts of an object already introduced). Markert et al. (2003) have applied relations extracted from lexicosyntactic patterns such as ’X and other Ys’ for OtherAnaphora (referential NPs with modifiers other or another) and for bridging involving meronymy. There has generally been a lack of work in the existing literature for automatically building lexical resources for definite anaphora resolution involving hyponyms relations such as presented in Example (1). However, this issue was recently addressed by Markert and Nissim (2005) by extending their work on Other-Anaphora using lexico syntactic pattern ’X and other Y’s to antecedent selection for definite"
W06-2906,P02-1014,0,0.114987,"ed by Markert and Nissim (2005) on resolving definite NP coreference that makes use of lexico-syntactic patterns such as ’X and Other Ys’ as utilized by Hearst (1992). 2 Related work There is a rich tradition of work using lexical and semantic resources for anaphora and coreference resolution. Several researchers have used WordNet as a lexical and semantic resource for certain types of bridging anaphora (Poesio et al., 1997; Meyer and Dale, 2002). WordNet has also been used as an important feature in machine learning of coreference resolution using supervised training data (Soon et al., 2001; Ng and Cardie, 2002). However, several researchers have reported that knowledge incorporated via WordNet is still insufficient for definite anaphora resolution. And of course, WordNet is not available for all languages and is missing inclusion of large segments of the vocabulary even for covered languages. Hence researchers have investigated use of corpus-based approaches to build a WordNet like resource automatically (Hearst, 1992; Cara1 The test examples were selected as follows: First, all the sentences containing definite NP “The Y” were extracted from the corpus. Then, the sentences containing instances of a"
W06-2906,W97-1301,0,0.039903,"sed as a mechanism for detecting a hypernym relation between the definite NP and its potential antecedents. We compare this approach with a WordNet-based algorithm and with an approach presented by Markert and Nissim (2005) on resolving definite NP coreference that makes use of lexico-syntactic patterns such as ’X and Other Ys’ as utilized by Hearst (1992). 2 Related work There is a rich tradition of work using lexical and semantic resources for anaphora and coreference resolution. Several researchers have used WordNet as a lexical and semantic resource for certain types of bridging anaphora (Poesio et al., 1997; Meyer and Dale, 2002). WordNet has also been used as an important feature in machine learning of coreference resolution using supervised training data (Soon et al., 2001; Ng and Cardie, 2002). However, several researchers have reported that knowledge incorporated via WordNet is still insufficient for definite anaphora resolution. And of course, WordNet is not available for all languages and is missing inclusion of large segments of the vocabulary even for covered languages. Hence researchers have investigated use of corpus-based approaches to build a WordNet like resource automatically (Hear"
W06-2906,poesio-etal-2002-acquiring,0,0.0198607,"ntences containing definite NP “The Y” were extracted from the corpus. Then, the sentences containing instances of anaphoric definite NPs were kept and other cases of definite expressions (like existential NPs “The White House”,“The weather”) were discarded. From this anaphoric set of sentences, 177 sentence instances covering 13 distinct hypernyms were randomly selected as the test set and annotated for the correct antecedent by human judges. 38 ballo, 1999; Berland and Charniak, 1999). Also, several researchers have applied it to resolving different types of bridging anaphora (Clark, 1975). Poesio et al. (2002) have proposed extracting lexical knowledge about part-of relations using Hearst-style patterns and applied it to the task of resolving bridging references. Poesio et al. (2004) have suggested using Google as a source of computing lexical distance between antecedent and definite NP for mereological bridging references (references referring to parts of an object already introduced). Markert et al. (2003) have applied relations extracted from lexicosyntactic patterns such as ’X and other Ys’ for OtherAnaphora (referential NPs with modifiers other or another) and for bridging involving meronymy."
W06-2906,P04-1019,0,0.168199,"xpressions (like existential NPs “The White House”,“The weather”) were discarded. From this anaphoric set of sentences, 177 sentence instances covering 13 distinct hypernyms were randomly selected as the test set and annotated for the correct antecedent by human judges. 38 ballo, 1999; Berland and Charniak, 1999). Also, several researchers have applied it to resolving different types of bridging anaphora (Clark, 1975). Poesio et al. (2002) have proposed extracting lexical knowledge about part-of relations using Hearst-style patterns and applied it to the task of resolving bridging references. Poesio et al. (2004) have suggested using Google as a source of computing lexical distance between antecedent and definite NP for mereological bridging references (references referring to parts of an object already introduced). Markert et al. (2003) have applied relations extracted from lexicosyntactic patterns such as ’X and other Ys’ for OtherAnaphora (referential NPs with modifiers other or another) and for bridging involving meronymy. There has generally been a lack of work in the existing literature for automatically building lexical resources for definite anaphora resolution involving hyponyms relations suc"
W06-2906,J01-4004,0,0.158019,"an approach presented by Markert and Nissim (2005) on resolving definite NP coreference that makes use of lexico-syntactic patterns such as ’X and Other Ys’ as utilized by Hearst (1992). 2 Related work There is a rich tradition of work using lexical and semantic resources for anaphora and coreference resolution. Several researchers have used WordNet as a lexical and semantic resource for certain types of bridging anaphora (Poesio et al., 1997; Meyer and Dale, 2002). WordNet has also been used as an important feature in machine learning of coreference resolution using supervised training data (Soon et al., 2001; Ng and Cardie, 2002). However, several researchers have reported that knowledge incorporated via WordNet is still insufficient for definite anaphora resolution. And of course, WordNet is not available for all languages and is missing inclusion of large segments of the vocabulary even for covered languages. Hence researchers have investigated use of corpus-based approaches to build a WordNet like resource automatically (Hearst, 1992; Cara1 The test examples were selected as follows: First, all the sentences containing definite NP “The Y” were extracted from the corpus. Then, the sentences con"
W06-2906,W02-1040,0,0.208153,"Missing"
W06-2906,J00-4003,0,0.467544,"Missing"
W06-2906,P03-1023,0,0.143385,"Missing"
W08-2006,C04-1046,0,0.0141334,", j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC (i, j) is a function of the commute time between vertex i and vertex j. 4 Graph construction Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this In this paper we consider uniform weights on all edges as our main aim is to illustrate the different random walk measures rather than fine tune the gr"
W08-2006,J06-1003,0,0.119811,"sures • Propose a new similarity measure based on commute time. • An improvement to the above measure by eliminating noisy features via singular value decomposition. c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 41 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41–48 Manchester, August 2008 3 Problem setting problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lexical relatedness measures on WordNet. Our focus in this paper is not to derive the best similarity measure for WordNet but to use WordNet and the lexical relatedness task as a method to evaluate the various random walk based similarity measures. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. This data consists of 30 word-pairs along with human judgements which is a real value between 1 and 4. For every measure we consider, we derive similarity scores and compare with the human judgements using the Spearm"
W08-2006,P07-1054,0,0.0237167,"Time Given a graph with the transition probability matrix P as defined above, the hitting time between vertices i and j, denoted as h(i, j), is defined as the expected number of steps taken by a random walker to first encounter vertex j starting from vertex i. This can be recursively defined as follows: X  pik h(k, j) if i 6= j  1+ h(i, j) = k : wik > 0  0 if i = j (1) 7.1.3 Similarity via pagerank Pagerank (Page et al., 1998) is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization (Erkan and Radev, 2004) to opinion mining (Esuli and Sebastiani, 2007) to our task of lexical relatedness (Hughes and Ramage, 2007). Pagerank is yet another random walk model with a difference that it allows the random walk to “jump” to its initial state with a nonzero probability (α). Given the probability transition matrix P as defined above, a stationary distribution vector for any vertex (say i) could be derived as follows: 4 The Jensen-Shannon divergence between two distributions p and q is defined as D(p k a) + D(q k a), where D(. k .) is the Kullback-Liebler divergence and a = (p + q)/2. Note that unlike KL-divergence this measure is symmetric. See (Lin,"
W08-2006,D07-1061,0,0.274246,"choice of m depends on Armed with the stationary distribution vectors for vertices i and j, we define pagerank similarity either as the cosine of the stationary distribution vectors or the reciprocal Jensen-Shannon (JS) divergence4 between them. Table 3. shows results on the Miller-Charles data. We use α = 0.1, the best value on this data. Observe that these results are Method Pagerank JS-Divergence Pagerank Cosine Spearman correlation 0.379 0.393 Table 3: Similarity via pagerank (α = 0.1). better than the best bounded walk result. We further note that our results are different from that of (Hughes and Ramage, 2007) as they use extensive feature engineering and weight tuning during the graph generation process that we have not been able to reproduce. Hence for simplicity we stuck to a simpler graph generation process. Nevertheless, the result in Table 3. is still useful as we are interested in the performance of the various spectral similarity measures rather than achieving the best performance on the lexical relatedness task. The graphs we use in all methods are identical making comparisons across methods possible. Figure 2: Effect of m in Bounded walk the amount of computation available. A reasonably l"
W08-2006,N06-1058,0,0.0211135,"of its hyponyms – For each word sense, add edges to all of its hypernyms recursively. • simP (i, j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC (i, j) is a function of the commute time between vertex i and vertex j. 4 Graph construction Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this In this paper we consider uniform weights on all edges as our m"
W08-2006,P05-3019,0,0.0147224,"– For each word sense, add edges to all of its hyponyms – For each word sense, add edges to all of its hypernyms recursively. • simP (i, j) is the probability of a random walk from vertex i to vertex j defined via a pagerank model. • simC (i, j) is a function of the commute time between vertex i and vertex j. 4 Graph construction Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this In this paper we con"
W08-2006,N04-3012,0,0.0193334,"rious random walk based measures • Propose a new similarity measure based on commute time. • An improvement to the above measure by eliminating noisy features via singular value decomposition. c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 41 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 41–48 Manchester, August 2008 3 Problem setting problem has generated copious literature in the past – see (Pedersen et al., 2004) or (Budanitsky and Hirst, 2006) for a detailed review of various lexical relatedness measures on WordNet. Our focus in this paper is not to derive the best similarity measure for WordNet but to use WordNet and the lexical relatedness task as a method to evaluate the various random walk based similarity measures. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. This data consists of 30 word-pairs along with human judgements which is a real value between 1 and 4. For every measure we consider, we derive similarity scores and compare with the h"
W08-2006,P93-1024,0,0.265483,"nstruction Data and Evaluation We evaluate each of the similarity measure we consider by using a linguistically motivated task of finding lexical similarity. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004) to name a few. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al., 1993). WordNet is an interesting graph-structured thesaurus where the vertices are the words and the edges represent relations between the words. For the purpose of this work, we only consider relations like hypernymy, hyponymy, and synonymy. The importance of this In this paper we consider uniform weights on all edges as our main aim is to illustrate the different random walk measures rather than fine tune the graph construction process. 6 Shortest path based measure The most obvious measure of distance in a graph is the shortest path between the vertices which is defined as the minimum number of"
W09-1117,W06-2920,0,0.0111348,"tive model from monolingual corpora and a seed lexicon. All of the aforementioned work defines context similarity in terms of the adjacent words over a window of some arbitary size (usually 2 to 4 words), as initially proposed by Rapp (1999). We show that the model for surrounding context can be improved by using dependency information rather than strictly relying on adjacent words, based on the success of dependency trees for monolingual clustering and disambiguation tasks (Lin and Pantel, 2002; Pado and Lapata, 2007) and the recent developments in multilingual dependency parsing literature (Buchholz and Marsi, 2006; Nivre et al., 2007). We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types, rather than just nouns. While the straightforward application of context-based model gives a lower overall accuracy than nouns alone, we show how learning a mapping of part-of-speech tagsets between the source and target language can result in comparable performance to that of noun translation. 3 Translation by Context Vector Projection This section details how translations are discovered from monolingual corpora through conte"
W09-1117,C02-1011,0,0.0213356,"Missing"
W09-1117,J07-2002,0,0.0141048,"text. Haghighi et al., (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. All of the aforementioned work defines context similarity in terms of the adjacent words over a window of some arbitary size (usually 2 to 4 words), as initially proposed by Rapp (1999). We show that the model for surrounding context can be improved by using dependency information rather than strictly relying on adjacent words, based on the success of dependency trees for monolingual clustering and disambiguation tasks (Lin and Pantel, 2002; Pado and Lapata, 2007) and the recent developments in multilingual dependency parsing literature (Buchholz and Marsi, 2006; Nivre et al., 2007). We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types, rather than just nouns. While the straightforward application of context-based model gives a lower overall accuracy than nouns alone, we show how learning a mapping of part-of-speech tagsets between the source and target language can result in comparable performance to that of noun translation. 3 Translation by Context Vector Pr"
W09-1117,J07-2003,0,0.021105,"Missing"
W09-1117,P99-1067,0,0.83134,"h Garera, Chris Callison-Burch, David Yarowsky Department of Computer Science, Johns Hopkins University Baltimore MD, USA {ngarera,ccb,yarowsky}@cs.jhu.edu Abstract researchers to develop methods for automatically learning bilingual lexicons, either by using monolingual corpora (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Haghighi et al., 2008) or by exploiting the cross-language evidence of closely related “bridge” languages that have more resources (Mann and Yarowsky, 2001). This paper investigates new ways of learning translations from monolingual corpora. We extend the Rapp (1999) model of context vector projection using a seed lexicon. It is based on the intuition that translations will have similar lexical context, even in unrelated corpora. For example, in order to translate the word “airplane”, the algorithm builds a context vector which might contain terms such as “passengers”, “runway”, “airport”, etc. and words in target language that have their translations (obtained via seed lexicon) in surrounding context can be considered as likely translations. We extend the basic approach by formulating a context model that uses dependency trees. The use of dependencies ha"
W09-1117,P98-1069,0,0.448521,"Missing"
W09-1117,P08-1088,0,0.665773,"the board, and better performance than statistical translation models on Top-10 accuracy for noun translation when trained on identical data. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 129–137, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics We further show that an extension based on partof-speech clustering can give similar accuracy gains for learning translations of all word-types, deepening the findings of previous literature which mainly focused on translating nouns (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). 2 Related Work The literature on translation lexicon induction for low-density languages falls in to two broad categories: 1) Effectively utilizing similarity between languages by choosing a high-resource “bridge” language for translation (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002) and 2) Extracting noisy clues (such as similar context) from monolingual corpora with help of a seed lexicon (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002, Haghighi et al., 2008). The latter category is more relevant to this work and is explained in detail below. The idea of words with"
W09-1117,W02-0902,0,0.498206,"ne context models across the board, and better performance than statistical translation models on Top-10 accuracy for noun translation when trained on identical data. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 129–137, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics We further show that an extension based on partof-speech clustering can give similar accuracy gains for learning translations of all word-types, deepening the findings of previous literature which mainly focused on translating nouns (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). 2 Related Work The literature on translation lexicon induction for low-density languages falls in to two broad categories: 1) Effectively utilizing similarity between languages by choosing a high-resource “bridge” language for translation (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002) and 2) Extracting noisy clues (such as similar context) from monolingual corpora with help of a seed lexicon (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002, Haghighi et al., 2008). The latter category is more relevant to this work and is explained in detail below."
W09-1117,N03-1017,0,0.011337,"Missing"
W09-1117,P07-2045,1,0.00989501,"glish and English-to-Spanish) using their sum as the final translation score. We contrasted the accuracy of the above methods, which use monolingual corpora, with a statistical camino Depposn Cntxt Model Adjbow Cntxt Model way 0.124 intentions 0.22 solution 0.097 way 0.21 steps 0.094 idea 0.20 path 0.093 thing 0.20 debate 0.085 faith 0.18 account 0.082 steps 0.17 means 0.080 example 0.17 work 0.079 news 0.16 approach 0.074 work 0.16 issue 0.073 attitude 0.15 model trained on bilingual parallel corpora. We refer to that model as Mosesen-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007). 4.1 Training Data All context models were trained on a Spanish corpus containing 100,000 sentences with 2.13 million words and an English corpus containing 100,000 sentences with 2.07 million words. The Spanish corpus was parsed using the MST dependency parser (McDonald et al., 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al., 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus"
W09-1117,2005.mtsummit-papers.11,0,0.0135649,"4.1 Training Data All context models were trained on a Spanish corpus containing 100,000 sentences with 2.13 million words and an English corpus containing 100,000 sentences with 2.07 million words. The Spanish corpus was parsed using the MST dependency parser (McDonald et al., 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al., 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). The fact that our two monolingual corpora are taken from a parallel corpus ensures that the assumption that similar contexts are a good indicator of translation holds. This assumption underlies in all work of translation lexicon induction from comparable monolingual corpora, and here we strongly bias toward that assumption. Despite the bias, the comparison of different context models holds, since all models are trained on the same data. 4.2 Evaluation Criterion The models were evaluated in terms of exact-match translation accuracy of the 1000 most frequent nouns in a English-Spanish dictiona"
W09-1117,N01-1020,1,0.834359,"proving Translation Lexicon Induction from Monolingual Corpora via Dependency Contexts and Part-of-Speech Equivalences Nikesh Garera, Chris Callison-Burch, David Yarowsky Department of Computer Science, Johns Hopkins University Baltimore MD, USA {ngarera,ccb,yarowsky}@cs.jhu.edu Abstract researchers to develop methods for automatically learning bilingual lexicons, either by using monolingual corpora (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Haghighi et al., 2008) or by exploiting the cross-language evidence of closely related “bridge” languages that have more resources (Mann and Yarowsky, 2001). This paper investigates new ways of learning translations from monolingual corpora. We extend the Rapp (1999) model of context vector projection using a seed lexicon. It is based on the intuition that translations will have similar lexical context, even in unrelated corpora. For example, in order to translate the word “airplane”, the algorithm builds a context vector which might contain terms such as “passengers”, “runway”, “airport”, etc. and words in target language that have their translations (obtained via seed lexicon) in surrounding context can be considered as likely translations. We"
W09-1117,J93-2004,0,0.0431806,"le 0.17 work 0.079 news 0.16 approach 0.074 work 0.16 issue 0.073 attitude 0.15 model trained on bilingual parallel corpora. We refer to that model as Mosesen-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007). 4.1 Training Data All context models were trained on a Spanish corpus containing 100,000 sentences with 2.13 million words and an English corpus containing 100,000 sentences with 2.07 million words. The Spanish corpus was parsed using the MST dependency parser (McDonald et al., 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al., 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). The fact that our two monolingual corpora are taken from a parallel corpus ensures that the assumption that similar contexts are a good indicator of translation holds. This assumption underlies in all work of translation lexicon induction from comparable monolingual corpora, and here we strongly bias toward that assumption. Despite the bias, the comparison of differ"
W09-1117,H05-1066,0,0.00505969,"4 idea 0.20 path 0.093 thing 0.20 debate 0.085 faith 0.18 account 0.082 steps 0.17 means 0.080 example 0.17 work 0.079 news 0.16 approach 0.074 work 0.16 issue 0.073 attitude 0.15 model trained on bilingual parallel corpora. We refer to that model as Mosesen-es-100k , because it was trained using the Moses toolkit (Koehn et al., 2007). 4.1 Training Data All context models were trained on a Spanish corpus containing 100,000 sentences with 2.13 million words and an English corpus containing 100,000 sentences with 2.07 million words. The Spanish corpus was parsed using the MST dependency parser (McDonald et al., 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al., 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). The fact that our two monolingual corpora are taken from a parallel corpus ensures that the assumption that similar contexts are a good indicator of translation holds. This assumption underlies in all work of translation lexicon induction from comparable monolingual cor"
W09-1117,W02-2026,1,0.863378,"ssociation for Computational Linguistics We further show that an extension based on partof-speech clustering can give similar accuracy gains for learning translations of all word-types, deepening the findings of previous literature which mainly focused on translating nouns (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). 2 Related Work The literature on translation lexicon induction for low-density languages falls in to two broad categories: 1) Effectively utilizing similarity between languages by choosing a high-resource “bridge” language for translation (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002) and 2) Extracting noisy clues (such as similar context) from monolingual corpora with help of a seed lexicon (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002, Haghighi et al., 2008). The latter category is more relevant to this work and is explained in detail below. The idea of words with similar meaning having similar contexts in the same language comes from the Distributional Hypothesis (Harris, 1985) and Rapp (1999) was the first to propose using context of a given word as a clue to its translation. Given a German word with an unknown translation, a German context vector is"
W09-1117,C98-1066,0,\N,Missing
W09-1117,D07-1096,0,\N,Missing
W09-3209,C04-1046,0,0.00773696,"k (α = 0.1) Parallel Label Propagation (β = 9) reduce(key, values): begin Emit(key, serialize(values)); end Table 2: Lexical-relatedness results: Comparision of PageRank and β-corrected Parallel Label Propagation labeled and would like to predict labels for the remaining vertices. 8.1 8.2 To evaluate ranking, we consider the problem of deriving lexical relatedness between terms. This has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004), to name a few. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. We compare our rankings with the human judegments using the Spearman rank correlation coefficient. The graph for this task is derived from WordNet, an electronic lexical database. We compare Algorithm 2 with results from using geodesic similarity as a baseline. As observed in Table 1, the parallel implementation in Algorithm 2 performs better than ranking using geodesic similarity derived from shortest path lengths. This reinforces the motivation of using random walks as descri"
W09-3209,N06-1058,0,0.0123278,"n map(key, value): begin edgeEntry = value; Node n(edgeEntry); Emit(n.id, n); end Method PageRank (α = 0.1) Parallel Label Propagation (β = 9) reduce(key, values): begin Emit(key, serialize(values)); end Table 2: Lexical-relatedness results: Comparision of PageRank and β-corrected Parallel Label Propagation labeled and would like to predict labels for the remaining vertices. 8.1 8.2 To evaluate ranking, we consider the problem of deriving lexical relatedness between terms. This has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004), to name a few. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. We compare our rankings with the human judegments using the Spearman rank correlation coefficient. The graph for this task is derived from WordNet, an electronic lexical database. We compare Algorithm 2 with results from using geodesic similarity as a baseline. As observed in Table 1, the parallel implementation in Algorithm 2 performs better than ranking using geodesic similarity derived fr"
W09-3209,N06-1026,0,0.0586442,"n the PageRank algorithm and set β = (1−α) = 9 in the β-corrected Parallel Laα 2 62 http://www.wjh.harvard.edu/∼inquirer/ (a) (b) Figure 3: Scalability results: (a) Scaleup (b) Speedup this case, we progressively increase the number of nodes in the cluster. Again, the speedup achieved is linear in the number of processing elements (CPUs). An appealing factor of Algorithm 2 is that the memory used by each mapper process is fixed regardless of the size of the graph. This makes the algorithm feasible for use with large-scale graphs. (F-scores) with another scalable previous work by Kim and Hovy (Kim and Hovy, 2006) in Table 2 for the same seed set. Their approach starts with a few seeds of positive and negative terms and bootstraps the list by considering all synonyms of positive word as positive and antonyms of positive words as negative. This procedure is repeated mutatis mutandis for negative words in the seed list until there are no more words to add. Method Kim & Hovy Parallel Label Propagation Nouns 34.80 58.53 Verbs 53.36 83.40 10 Historically, there is an abundance of work in parallel and distributed algorithms for graphs. See Grama et al. (2003) for survey chapters on the topic. In addition, th"
W09-3209,P05-3019,0,0.0188497,"Table 2. Algorithm 3: Graph Construction map(key, value): begin edgeEntry = value; Node n(edgeEntry); Emit(n.id, n); end Method PageRank (α = 0.1) Parallel Label Propagation (β = 9) reduce(key, values): begin Emit(key, serialize(values)); end Table 2: Lexical-relatedness results: Comparision of PageRank and β-corrected Parallel Label Propagation labeled and would like to predict labels for the remaining vertices. 8.1 8.2 To evaluate ranking, we consider the problem of deriving lexical relatedness between terms. This has been a topic of interest with applications in word sense disambiguation (Patwardhan et al., 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al., 2001), and machine translation (Blatz et al., 2004), to name a few. Following the tradition in previous literature we evaluate on the Miller and Charles (1991) dataset. We compare our rankings with the human judegments using the Spearman rank correlation coefficient. The graph for this task is derived from WordNet, an electronic lexical database. We compare Algorithm 2 with results from using geodesic similarity as a baseline. As observed in Table 1, the parallel implementation in Algorithm 2 performs better than r"
W16-2002,P14-2102,1,0.883884,"Missing"
W16-2002,W16-2007,0,0.0605324,"eral tack as the previous two systems—they used a pipelined approach that first discovered an alignment between the string pairs and then discriminatively trained a transduction. The alignment algorithm employed is the same as that of the baseline system, which relies on a rich-get-richer scheme based on the Chinese restaurant process (Sudoh et al., 2013), as discussed in §5. After obtaining the alignments, they extracted edit operations based on the alignments and used a semi-Markov CRF to apply the edits in a manner very similar to the work of Durrett and DeNero (2013). BIU-MIT The BIU-MIT (Aharoni et al., 2016) team submitted two systems. Their first model, like LMU, built upon the sequence-to-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2014; Faruqui et al., 2016), but with several im16 System LMU-1 LMU-2 BIU/MIT-1 BIU/MIT-2 HEL MSU CU EHU COL/NYU OSU UA O RACLE.E Task 1 1.0 (95.56) 2.0 (95.56) — — — 3.8 (84.06) 4.6 (81.02) 5.5 (79.24) 6.5 (67.86) — 4.6 (81.83) 97.49 Standard Task 2 1.0 (96.35) 2.0 (96.23) — — — 3.6 (86.06) 5.0 (72.98) — 4.7 (75.59) — 4.7 (74.06) 98.15 Task 3 1.0 (95.83) 2.0 (95.83) — — — 3.8 (84.87) 5.0 (71.75) — 4.8 (67.61) — 4.4 (71.23) 97.97 Task 1 1.0 (95.56"
W16-2002,Q15-1031,1,0.850026,"nd then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models with string-valued variables to model the paradigm jointly. In such models it is possible to predict values for cells in the paradigm conditioned on sets of other cells, which are not required to include the lemma. 2 a a 3 t t 4 t 5 o o 6 ssa - In general, each edit has the form copy, insert(string), delete(number), or subst(string), where subst(w) has the same effect as delete(|w|) followed by insert(w). The system treats edit sequence prediction as a sequential decision-making problem, greedily choosing each edit action given the previously chosen actions. This choice"
W16-2002,E14-1060,1,0.267684,"nd suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply to a stem, and attaching appropriate affixes to the result. Moscow State The Moscow State system (Sorokin, 2016) is derived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki system (Ostling, 2016), like LMU and BIU-MIT, built"
W16-2002,D09-1011,1,0.861261,"tions at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models with string-valued variables to model the paradigm jointly. In such models it is possible to predict values for cells in the paradigm conditioned on sets of other cells, which are not required to include the lemma. 2 a a 3 t t 4 t 5 o o 6 ssa - In general, each edit has the form copy, insert(string), delete(number), or subst(string), where subst(w) has the same effect as delete(|w|) followed by insert(w). The system treats edit sequence prediction as a sequential decision-making problem, greedily choosing each edit action given the previously"
W16-2002,N15-1107,1,0.936397,"work on computational approaches to inflectional morphology has focused on a special case of reinflection, where the input form is always the lemma (i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completio"
W16-2002,D11-1057,1,0.266047,"for the addition of bound morphemes, but also incorporation, which involves combining lexical stems that are often used to form independent words (Mithun, 1984). Such languages combine the need to decompound, generate derivational alternatives, and accurately inflect any resulting words. implemented as finite state transducers (Beesley and Karttunen, 2003), often return all morphologically plausible analyses if there is ambiguity. Learning to mimic the behavior of a hand-written analyzer in this respect could offer a more challenging task, and one that is useful within unsupervised learning (Dreyer and Eisner, 2011) as well as parsing. Existing wide-coverage morphological analyzers could be leveraged in the design of a more interactive shared task, where handcoded models or approximate surface rules could serve as informants for grammatical inference algorithms. The current task design did not explore all potential inflectional complexities in the languages included. For example, cliticization processes were generally not present in the language data. Adding such inflectional elements to the task can potentially make it more realistic in terms of real-world data sparsity in L1 learning scenarios. For exa"
W16-2002,W16-2004,0,0.0554931,"em. Like the Colorado system, they followed Durrett and DeNero (2013) and used a semi-Markov CRF to apply the edit operations. In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the model is quadratic in the size of the state space (the number of edit operations), this created a significant slowdown with over 15 days required to train in some cases. CRF (Sarawagi and Cohen, 2004). EHU EHU (Alegria and Etxeberria, 2016) took an approach based on standard grapheme-tophoneme machinery. They extend the Phonetisaurus (Novak et al., 2012) toolkit, based on the OpenFST WFST library (Allauzen et al., 2007), to the task of morphological reinflection. Their system is organized as a pipeline. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicol"
W16-2002,D08-1113,1,0.900508,"Missing"
W16-2002,C04-1022,0,0.0394056,"inct word forms for any given 1 This latter figure rises to 52 if the entire imperfectiveperfective pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of systems that could accuratel"
W16-2002,N13-1138,0,0.0757217,"06 1.05 Table 3: Descriptive statistics on data released to shared task participants. Figures represent averages across tasks. Abbreviations in the headers: ‘Lem’ = lemmas, ‘Full’ = number of full tags, T2T = average occurrences of tag-to-tag pairs, I-Tag & O-Tag = average occurrences of each input or output tag, resp., and ‘Sync’ = average forms per tag (syncretism). • Standard Release: Arabic, Finnish, Georgian, German, Navajo, Russian, Spanish, and Turkish • Surprise: Hungarian and Maltese Finnish, German, and Spanish have been the subject of much recent work, due to data made available by Durrett and DeNero (2013), while the other datasets used in the shared task are released here for the first time. For all languages, the word forms in the data are orthographic (not phonological) strings in the native script, except in the case of Arabic, where we used the romanized forms available from Wiktionary. An accented letter is treated as a single character. Descriptive statistics of the data are provided in Table 3. The typological character of these languages varies widely. German and Spanish inflection generation has been studied extensively, and the morphological character of the languages is similar: Bot"
W16-2002,P15-1033,0,0.00992926,"the context and they represent individual morphosyntactic attributes as well. In addition, they include template-inspired components to better cope with the templatic morphology of Arabic and Maltese. The second architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al"
W16-2002,D13-1105,0,0.0212324,"r et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the submitted systems in that the first step in the pipeline is segmentation of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply t"
W16-2002,N16-1077,0,0.544713,"(i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many of the approaches taken by the shared task participants drew inspiration from work on paradigm completion. Some work, however, has considered full reinflection. For example, Dreyer and Eisner (2009) and Cotterell et al. (2015) apply graphical models wi"
W16-2002,W14-4012,0,0.0716839,"Missing"
W16-2002,chrupala-etal-2008-learning,0,0.161107,"Missing"
W16-2002,L16-1498,1,0.655386,"Missing"
W16-2002,H05-1085,0,0.0534859,"pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of systems that could accurately generate morphologically inflected words for a set of 10 languages based on a range of training parame"
W16-2002,J10-4005,0,0.0133976,"al of 10 distinct word forms for any given 1 This latter figure rises to 52 if the entire imperfectiveperfective pair (e.g. govorit’/skazat’ ‘speak, tell’) is considered to be a single lemma. 10 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 10–22, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics forms, the probability of encountering any single word form decreases, reducing the effectiveness of frequency-based techniques in performing tasks like word alignment and language modeling (Koehn, 2010; Duh and Kirchhoff, 2004). Techniques like lemmatization and stemming can ameliorate data sparsity (Goldwater and McClosky, 2005), but these rely on morphological knowledge, particularly the mapping from inflected forms to lemmas and the list of morphs together with their ordering. Developing systems that can accurately learn and capture these mappings, overt affixes, and the principles that govern how those affixes combine is crucial to maximizing the crosslinguistic capabilities of most human language technology. The goal of the 2016 SIGMORPHON Shared Task2 was to spur the development of sy"
W16-2002,W16-2006,0,0.0709506,"Missing"
W16-2002,N07-1047,0,0.0429849,"e. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to that of EHU—an unsupervised alignment model is first applied to the training pairs to impute an alignment. In this case, they employ the M2M-aligner (Jiampojamarn et al., 2007). In contrast to EHU, Nicolai et al. (2016) do allow many-to-many alignments. After computing the alignments, they discriminatively learn a stringto-string mapping using the DirectTL+ model (Jiampojamarn et al., 2008). This model is stateof-the-art for the grapheme-to-phoneme task and is very similar to the EHU system in that it assumes a monotonic alignment and could therefore be encoded as a WFST. Despite the similarity to the EHU system, the model performs much better overall. This increase in performance may be attributable to the extensive use of language-specific heuristics, detailed in"
W16-2002,D15-1272,1,0.145283,"Missing"
W16-2002,N15-1093,0,0.142748,"te(3). This results in the output katto, via the following alignment: 1 k k Previous Work Much previous work on computational approaches to inflectional morphology has focused on a special case of reinflection, where the input form is always the lemma (i.e. the citation form). Thus, the task is to generate all inflections in a paradigm from the lemma and often goes by the name of paradigm completion in the literature. There has been a flurry of recent work in this vein: Durrett and DeNero (2013) heuristically extracted transformational rules and learned a statistical model to apply the rules, Nicolai et al. (2015) tackled the problem using standard tools from discriminative string transduction, Ahlberg et al. (2015) used a finite-state construction to extract complete candidate inflections at the paradigm level and then train a classifier, Faruqui et al. (2016) applied a neural sequence-to-sequence architecture (Sutskever et al., 2014) to the problem. In contrast to paradigm completion, the task of reinflection is harder as it may require both morphologically analyzing the source form and transducing it to the target form. In addition, the training set may include only partial paradigms. However, many"
W16-2002,P08-1103,0,0.0216551,"s, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to that of EHU—an unsupervised alignment model is first applied to the training pairs to impute an alignment. In this case, they employ the M2M-aligner (Jiampojamarn et al., 2007). In contrast to EHU, Nicolai et al. (2016) do allow many-to-many alignments. After computing the alignments, they discriminatively learn a stringto-string mapping using the DirectTL+ model (Jiampojamarn et al., 2008). This model is stateof-the-art for the grapheme-to-phoneme task and is very similar to the EHU system in that it assumes a monotonic alignment and could therefore be encoded as a WFST. Despite the similarity to the EHU system, the model performs much better overall. This increase in performance may be attributable to the extensive use of language-specific heuristics, detailed in the paper, or the application of a discriminative reranker. 6.2 Camp 2: Revenge of the RNN A surprising result of the shared task is the large performance gap between the top performing neural models and the rest of t"
W16-2002,W16-2005,0,0.0679067,"Missing"
W16-2002,W16-2010,0,0.168052,"Missing"
W16-2002,W12-6208,0,0.0181364,"In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the model is quadratic in the size of the state space (the number of edit operations), this created a significant slowdown with over 15 days required to train in some cases. CRF (Sarawagi and Cohen, 2004). EHU EHU (Alegria and Etxeberria, 2016) took an approach based on standard grapheme-tophoneme machinery. They extend the Phonetisaurus (Novak et al., 2012) toolkit, based on the OpenFST WFST library (Allauzen et al., 2007), to the task of morphological reinflection. Their system is organized as a pipeline. Given pairs of input and output strings, the first step involves an unsupervised algorithm to extract an alignment (many-to-one or one-to-many). Then, they train the weights of the WFSTs using the imputed alignments, introducing morphological tags as symbols on the input side of the transduction. Alberta The Alberta system (Nicolai et al., 2016) is derived from the earlier work by Nicolai et al. (2015) and is methodologically quite similar to"
W16-2002,W16-2003,0,0.0522292,"rived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki system (Ostling, 2016), like LMU and BIU-MIT, built off of the sequenceto-sequence architecture, augmenting the system with several innovations. First, a single decoder was used, rather than a unique one for all possible morphological tags, which allows for additional parameter sharing, similar to LMU. More LSTM layers were also added to the decoder, creating a deeper network. Finally, a convolutional layer over the character inputs was used, which was found to significantly increase performance over models without the convolutional layers. 17 trained a neural model to predict edit operations, consistently ranked b"
W16-2002,W16-2008,0,0.0168371,"eline system are given in Table 5. Most participants in the shared task were able to outperform the baseline, often by a significant margin. 6.1 Camp 1: Align and Transduce Most of the systems in this camp drew inspiration from the work of Durrett and DeNero (2013), who extracted a set of edit operations and applied the transformations with a semi-Markov 8 Note that at training time, we know the correct lemma for S thanks to the task 1 data, which is permitted for use by task 2 in the standard track. This is also why task 2 is permitted to use the trained task 1 system. 15 OSU The OSU system (King, 2016) also used a pipelined approach. They first extracted sequences of edit operations using Hirschberg’s algorithm (Hirschberg, 1975). This reduces the string-to-string mapping problem to a sequence tagging problem. Like the Colorado system, they followed Durrett and DeNero (2013) and used a semi-Markov CRF to apply the edit operations. In contrast to Durrett and DeNero (2013), who employed a 0th-order model, the OSU system used a 1st-order model. A major drawback of the system was the cost of inference. The unpruned set of edit operations had over 500 elements. As the cost of inference in the mo"
W16-2002,Q16-1023,0,0.0186037,"ey represent individual morphosyntactic attributes as well. In addition, they include template-inspired components to better cope with the templatic morphology of Arabic and Maltese. The second architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the"
W16-2002,N16-1076,1,0.132588,"nd architecture, while also neural, more radically departs from previously proposed sequence-to-sequence models. The aligner from the baseline system is used to create a series of edit actions, similar to the systems in Camp 1. Rather than use a CRF, the BIU-MIT team predicted the sequence of edit actions using a neural model, much in the same way as a transition-based LSTM parser does (Dyer et al., 2015; Kiperwasser and Goldberg, 2016). The architectural consequence of this is that it replaces the soft alignment mechanism of (Bahdanau et al., 2014) with a hard attention mechanism, similar to Rastogi et al. (2016). Camp 3: Time for Some Linguistics The third camp relied on linguistics-inspired heuristics to reduce the problem to multi-way classification. This camp is less unified than the other two, as both teams used very different heuristics. Columbia – New York University Abu Dhabi The system developed jointly by Columbia and NYUAD (Taji et al., 2016) is based on the work of Eskander et al. (2013). It is unique among the submitted systems in that the first step in the pipeline is segmentation of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with mo"
W16-2002,W16-2009,0,0.0638163,"of the input words into prefixes, stems, and suffixes. Prefixes and suffixes are directly associated with morphological features. Stems within paradigms are further processed, using either linguistic intuitions or an empirical approach based on string alignments, to extract the stem letters that undergo changes across inflections. The extracted patterns are intended to capture stem-internal changes, such as vowel changes in Arabic. Reinflection is performed by selecting a set of changes to apply to a stem, and attaching appropriate affixes to the result. Moscow State The Moscow State system (Sorokin, 2016) is derived from the work of Ahlberg et al. (2014) and Ahlberg et al. (2015). The general idea is to use finite-state techniques to compactly model all paradigms in an abstract form called an ‘abstract paradigm’. Roughly speaking, an abstract paradigm is a set of rule transformations that derive all slots from the shared string subsequences present in each slot. Their method relies on the computation of longest common subsequence (Gusfield, 1997) to derive the abstract paradigms, which is similar to its use in the related task of lemmatization (Chrupała et al., 2008; ¨ Helsinki The Helsinki sy"
W16-2002,D13-1021,0,0.118112,"Missing"
W16-2002,P15-2111,1,0.178102,"Missing"
W18-1813,C10-3010,0,0.0348286,"V 1 PL PST IPFV) The feature vectors output by all analysis methods were manually mapped into the UniMorph feature schema standard (Sylak-Glassman et al., 2015). As the total set of of unique feature vectors remaining after this mapping was limited for both Russian (569 vectors) and Spanish (239 vectors), we were able to manually produce one or more gloss templates for each vector (e.g., V;1;PL;PST;IPFV → ‘(we) were VBG.’). Source lemmas in Russian and Spanish were converted to English lemmas via lookup in, preferably, Wiktionary-derived lemma translation data (Kirov et al., 2016), or PanLex (Baldwin et al., 2010). English lemmas were then inﬂected using the tools provided by Smedt and Daelemans (2012) and inserted into the corresponding gloss templates. We further post-processed the glosses by removing anything in parenthesis or brackets, and then removed entries containing non-English words in the translation, after which we were left with 3,122,470 Russian glosses, and 589,188 Spanish glosses. 5 Conditions We trained three baseline models and ﬁve additional experimental setups. Total sizes of training datasets for each condition, in number of paired sentences, are shown in Table 1. For Baseline 1, i"
W18-1813,K15-1017,0,0.0685017,"tly superior. Broadly, the goal of our substitution approach is to transform the source language into a form that is more similar to the target. A number of previous strategies used in MT have fallen under this umbrella. Compound splitting (Koehn and Knight, 2003; Macherey et al., 2011) of, for example, German source-side words increases similarity with English as English doesn’t use nearly as many compounds as German. Fraser and Marcu (2005) use stemming to reduce Romanian source side vocabulary size to improve Romanian-English word alignment. Ding et al. (2016) compare supervised (ChipMunk (Cotterell et al., 2015)) and unsupervised (Morfessor (Virpioja et al., 2013), and Byte-Pair encoding (Sennrich et al., 2015)) morphological segmentation methods on the source side of the PBMT system for the WMT Turkish-English Translation Task. 4 4.1 Experiments Model We use Moses (Koehn et al., 2007) as the Phrase Based Machine Translation (PBMT) system to run all our translation experiments. Data is tokenized and truecased using standard Moses scripts. We use GIZA++ (Och and Ney, 2003) for alignment with the grow-diag-ﬁnal-and setting. We set the maximum sentence length to 80 and the maximum phrase length to 5. Fo"
W18-1813,W16-2310,1,0.851047,"h), but did not ﬁnd that one method was consistently superior. Broadly, the goal of our substitution approach is to transform the source language into a form that is more similar to the target. A number of previous strategies used in MT have fallen under this umbrella. Compound splitting (Koehn and Knight, 2003; Macherey et al., 2011) of, for example, German source-side words increases similarity with English as English doesn’t use nearly as many compounds as German. Fraser and Marcu (2005) use stemming to reduce Romanian source side vocabulary size to improve Romanian-English word alignment. Ding et al. (2016) compare supervised (ChipMunk (Cotterell et al., 2015)) and unsupervised (Morfessor (Virpioja et al., 2013), and Byte-Pair encoding (Sennrich et al., 2015)) morphological segmentation methods on the source side of the PBMT system for the WMT Turkish-English Translation Task. 4 4.1 Experiments Model We use Moses (Koehn et al., 2007) as the Phrase Based Machine Translation (PBMT) system to run all our translation experiments. Data is tokenized and truecased using standard Moses scripts. We use GIZA++ (Och and Ney, 2003) for alignment with the grow-diag-ﬁnal-and setting. We set the maximum senten"
W18-1813,W05-0814,0,0.045378,"ending their synthetic translations to the parallel text as well as using an additional phrase table (and the combination of both), but did not ﬁnd that one method was consistently superior. Broadly, the goal of our substitution approach is to transform the source language into a form that is more similar to the target. A number of previous strategies used in MT have fallen under this umbrella. Compound splitting (Koehn and Knight, 2003; Macherey et al., 2011) of, for example, German source-side words increases similarity with English as English doesn’t use nearly as many compounds as German. Fraser and Marcu (2005) use stemming to reduce Romanian source side vocabulary size to improve Romanian-English word alignment. Ding et al. (2016) compare supervised (ChipMunk (Cotterell et al., 2015)) and unsupervised (Morfessor (Virpioja et al., 2013), and Byte-Pair encoding (Sennrich et al., 2015)) morphological segmentation methods on the source side of the PBMT system for the WMT Turkish-English Translation Task. 4 4.1 Experiments Model We use Moses (Koehn et al., 2007) as the Phrase Based Machine Translation (PBMT) system to run all our translation experiments. Data is tokenized and truecased using standard Mo"
W18-1813,W11-2123,0,0.0463608,"segmentation methods on the source side of the PBMT system for the WMT Turkish-English Translation Task. 4 4.1 Experiments Model We use Moses (Koehn et al., 2007) as the Phrase Based Machine Translation (PBMT) system to run all our translation experiments. Data is tokenized and truecased using standard Moses scripts. We use GIZA++ (Och and Ney, 2003) for alignment with the grow-diag-ﬁnal-and setting. We set the maximum sentence length to 80 and the maximum phrase length to 5. For decoding, we use Cube Pruning (Huang and Chiang, 2007). We also weigh potential translations using a 5-gram KenLM (Heaﬁeld, 2011) language model. 4.2 Data Bitext. For our base bitext, we use the Russian-English Corpus from the LORELEI Russian Representative Language Pack (LDC2016E95 V1.1), and the Spanish-English Corpus from the LORELEI Spanish Representative Language Pack (LDC2016E97). The corpora primarily consist of news and web forums. While they are included in the LORELEI corpora, we exclude Tweets from these experiments. We also remove sentences longer than 80 words. For the Russian baselines, we randomly split the remaining data into train (46,746), tune (2,233), and test (2,462) sentence pairs. For the Spanish"
W18-1813,2016.amta-researchers.14,1,0.897446,"t to in-situ translations of a word, and often contain additional descriptive text. Glosses, as we deﬁne them, are intended to remedy these problems. A gloss is a mapping between an inﬂected form of a word, and an in-situ translation. In many cases, English uses syntactic constructions to express distinctions made by inﬂectional morphology in a source language. As a result, single source words are often glossed as multi-word expressions in English. бегут, бежать,V;IPFV;PRS;3;PL,(they/NNS) are running; (they/NNS) run Generating a gloss for an inﬂected word follows a general process outlined in Hewitt et al. (2016). In this work, however, we simplify many of the steps. Our implementation is fully described in the Experiments section below. 1. Apply morphological analysis to an input inﬂected word to recover its base lemma and morphological features, e.g., comprábamos → comprar, V;1;PL;PST;IPFV 2. Using a separate lemma-to-lemma dictionary, recover a target lemma for the source word: comprar → buy 3. Specify a conversion from each vector of source morphological features to a target gloss template. For many language pairs, this can be done manually: V;1;PL;PST;IPFV → ‘(we) were VBG.’ Here, VBG is a Penn T"
W18-1813,P07-1019,0,0.0614756,"rpioja et al., 2013), and Byte-Pair encoding (Sennrich et al., 2015)) morphological segmentation methods on the source side of the PBMT system for the WMT Turkish-English Translation Task. 4 4.1 Experiments Model We use Moses (Koehn et al., 2007) as the Phrase Based Machine Translation (PBMT) system to run all our translation experiments. Data is tokenized and truecased using standard Moses scripts. We use GIZA++ (Och and Ney, 2003) for alignment with the grow-diag-ﬁnal-and setting. We set the maximum sentence length to 80 and the maximum phrase length to 5. For decoding, we use Cube Pruning (Huang and Chiang, 2007). We also weigh potential translations using a 5-gram KenLM (Heaﬁeld, 2011) language model. 4.2 Data Bitext. For our base bitext, we use the Russian-English Corpus from the LORELEI Russian Representative Language Pack (LDC2016E95 V1.1), and the Spanish-English Corpus from the LORELEI Spanish Representative Language Pack (LDC2016E97). The corpora primarily consist of news and web forums. While they are included in the LORELEI corpora, we exclude Tweets from these experiments. We also remove sentences longer than 80 words. For the Russian baselines, we randomly split the remaining data into trai"
W18-1813,E17-2059,0,0.0204914,"p table: ‘(we) were VBG’ + buy → ‘(we) were buying’ This completes the gloss generation process: comprábamos → ‘(we) were buying’ 1 https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 133 3 Related Work Prior work has both explored ways of generating morphological information, and incorporating such morphological information into Phrase Based Machine Translation. While there is work on generating rich morphology on the target side of translations (for example: (Toutanova et al., 2008; Huck et al., 2017), we focus on rich source side morphology in this work. Hewitt et al. (2016) created glosses by re-purposing instructional prompts found in a special corpus designed to elicit inﬂectional paradigms from bilingual speakers (Sylak-Glassman et al., 2016). For example, the sample prompt ‘(The apple) has been eaten.’ was designed to elicit third person present perfect verb forms from bilingual Spanish speakers. They heuristically interpolated between multiple prompts to generate new gloss templates for each possible feature vector and lemma. They experiment with both appending their synthetic trans"
W18-1813,P16-2090,0,0.032694,"a tokenization of the monolingual corpus released by the LDC as part of LORELEI language packs for Russian and Spanish. For each word in the list, additional morphological analyses were obtained. For Russian, we applied the PyMorphy2 package (Korobov, 2015)3 to each word, while for Spanish we used the Freeling package (Padró and Stanilovsky, 2012).4 For both Russian and Spanish, we also applied a custom sequence-2-sequence neural network analyzer trained on the raw data in the UniMorph database. The network used an architecture, training scheme, and hyperparameters identical to that used in (Kann and Schütze, 2016). It mapped sequences of characters representing an inﬂected word directly to a sequence representing the its underlying lemma and features (c o m p r á b a m o s → c o m p r a r V 1 PL PST IPFV) The feature vectors output by all analysis methods were manually mapped into the UniMorph feature schema standard (Sylak-Glassman et al., 2015). As the total set of of unique feature vectors remaining after this mapping was limited for both Russian (569 vectors) and Spanish (239 vectors), we were able to manually produce one or more gloss templates for each vector (e.g., V;1;PL;PST;IPFV → ‘(we) were V"
W18-1813,L16-1498,1,0.7585,"p r á b a m o s → c o m p r a r V 1 PL PST IPFV) The feature vectors output by all analysis methods were manually mapped into the UniMorph feature schema standard (Sylak-Glassman et al., 2015). As the total set of of unique feature vectors remaining after this mapping was limited for both Russian (569 vectors) and Spanish (239 vectors), we were able to manually produce one or more gloss templates for each vector (e.g., V;1;PL;PST;IPFV → ‘(we) were VBG.’). Source lemmas in Russian and Spanish were converted to English lemmas via lookup in, preferably, Wiktionary-derived lemma translation data (Kirov et al., 2016), or PanLex (Baldwin et al., 2010). English lemmas were then inﬂected using the tools provided by Smedt and Daelemans (2012) and inserted into the corresponding gloss templates. We further post-processed the glosses by removing anything in parenthesis or brackets, and then removed entries containing non-English words in the translation, after which we were left with 3,122,470 Russian glosses, and 589,188 Spanish glosses. 5 Conditions We trained three baseline models and ﬁve additional experimental setups. Total sizes of training datasets for each condition, in number of paired sentences, are s"
W18-1813,P07-2045,0,0.0068491,"011) of, for example, German source-side words increases similarity with English as English doesn’t use nearly as many compounds as German. Fraser and Marcu (2005) use stemming to reduce Romanian source side vocabulary size to improve Romanian-English word alignment. Ding et al. (2016) compare supervised (ChipMunk (Cotterell et al., 2015)) and unsupervised (Morfessor (Virpioja et al., 2013), and Byte-Pair encoding (Sennrich et al., 2015)) morphological segmentation methods on the source side of the PBMT system for the WMT Turkish-English Translation Task. 4 4.1 Experiments Model We use Moses (Koehn et al., 2007) as the Phrase Based Machine Translation (PBMT) system to run all our translation experiments. Data is tokenized and truecased using standard Moses scripts. We use GIZA++ (Och and Ney, 2003) for alignment with the grow-diag-ﬁnal-and setting. We set the maximum sentence length to 80 and the maximum phrase length to 5. For decoding, we use Cube Pruning (Huang and Chiang, 2007). We also weigh potential translations using a 5-gram KenLM (Heaﬁeld, 2011) language model. 4.2 Data Bitext. For our base bitext, we use the Russian-English Corpus from the LORELEI Russian Representative Language Pack (LDC2"
W18-1813,E03-1076,0,0.116254,"Missing"
W18-1813,W17-3204,0,0.0130806,"ncidents) program as our source of parallel training data. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 132 These packs typically contain less than 50,000 bilingual sentence pairs in total, orders of magnitude below the amount used to train most state-of-the-art MT systems. We run our experiments using traditional phrase-based statistical machine translation models (PBMT). While neural machine translation offers state-of-the-art performance when training data is plentiful, PBMT remains competitive or superior in the low resource conditions we focus on (Koehn and Knowles, 2017). 2 Multilingual Dictionaries Versus Glosses We deﬁne an entry in a multilingual dictionary as a mapping between a lemma form in the source language to one or more deﬁnitions in the target language. бежать,VERB,to run, to be running While useful, these types of entries have several notable drawbacks when used as bitext for a translation system. First, on the source side, the dictionary forms of words, or lemmas, are typically uninﬂected, and may not be in common usage. For example, the dictionary form of verbs in many languages is the inﬁnitive, but in actual text tensed forms are much more co"
W18-1813,P11-1140,0,0.024093,"heuristically interpolated between multiple prompts to generate new gloss templates for each possible feature vector and lemma. They experiment with both appending their synthetic translations to the parallel text as well as using an additional phrase table (and the combination of both), but did not ﬁnd that one method was consistently superior. Broadly, the goal of our substitution approach is to transform the source language into a form that is more similar to the target. A number of previous strategies used in MT have fallen under this umbrella. Compound splitting (Koehn and Knight, 2003; Macherey et al., 2011) of, for example, German source-side words increases similarity with English as English doesn’t use nearly as many compounds as German. Fraser and Marcu (2005) use stemming to reduce Romanian source side vocabulary size to improve Romanian-English word alignment. Ding et al. (2016) compare supervised (ChipMunk (Cotterell et al., 2015)) and unsupervised (Morfessor (Virpioja et al., 2013), and Byte-Pair encoding (Sennrich et al., 2015)) morphological segmentation methods on the source side of the PBMT system for the WMT Turkish-English Translation Task. 4 4.1 Experiments Model We use Moses (Koeh"
W18-1813,J03-1002,0,0.0117066,"nian source side vocabulary size to improve Romanian-English word alignment. Ding et al. (2016) compare supervised (ChipMunk (Cotterell et al., 2015)) and unsupervised (Morfessor (Virpioja et al., 2013), and Byte-Pair encoding (Sennrich et al., 2015)) morphological segmentation methods on the source side of the PBMT system for the WMT Turkish-English Translation Task. 4 4.1 Experiments Model We use Moses (Koehn et al., 2007) as the Phrase Based Machine Translation (PBMT) system to run all our translation experiments. Data is tokenized and truecased using standard Moses scripts. We use GIZA++ (Och and Ney, 2003) for alignment with the grow-diag-ﬁnal-and setting. We set the maximum sentence length to 80 and the maximum phrase length to 5. For decoding, we use Cube Pruning (Huang and Chiang, 2007). We also weigh potential translations using a 5-gram KenLM (Heaﬁeld, 2011) language model. 4.2 Data Bitext. For our base bitext, we use the Russian-English Corpus from the LORELEI Russian Representative Language Pack (LDC2016E95 V1.1), and the Spanish-English Corpus from the LORELEI Spanish Representative Language Pack (LDC2016E97). The corpora primarily consist of news and web forums. While they are included"
W18-1813,padro-stanilovsky-2012-freeling,0,0.0263739,"r Spanish. Glosses. Glosses for Russian and Spanish were created as follows: Lists of inﬂected wordforms were obtained via a union of the UniMorph database (unimorph.githbub.io), which provides a mapping from inﬂected forms to their lemmas and morphological feature vectors, and a tokenization of the monolingual corpus released by the LDC as part of LORELEI language packs for Russian and Spanish. For each word in the list, additional morphological analyses were obtained. For Russian, we applied the PyMorphy2 package (Korobov, 2015)3 to each word, while for Spanish we used the Freeling package (Padró and Stanilovsky, 2012).4 For both Russian and Spanish, we also applied a custom sequence-2-sequence neural network analyzer trained on the raw data in the UniMorph database. The network used an architecture, training scheme, and hyperparameters identical to that used in (Kann and Schütze, 2016). It mapped sequences of characters representing an inﬂected word directly to a sequence representing the its underlying lemma and features (c o m p r á b a m o s → c o m p r a r V 1 PL PST IPFV) The feature vectors output by all analysis methods were manually mapped into the UniMorph feature schema standard (Sylak-Glassman e"
W18-1813,L16-1497,1,0.844818,"MT Research Track Boston, March 17 - 21, 2018 |Page 133 3 Related Work Prior work has both explored ways of generating morphological information, and incorporating such morphological information into Phrase Based Machine Translation. While there is work on generating rich morphology on the target side of translations (for example: (Toutanova et al., 2008; Huck et al., 2017), we focus on rich source side morphology in this work. Hewitt et al. (2016) created glosses by re-purposing instructional prompts found in a special corpus designed to elicit inﬂectional paradigms from bilingual speakers (Sylak-Glassman et al., 2016). For example, the sample prompt ‘(The apple) has been eaten.’ was designed to elicit third person present perfect verb forms from bilingual Spanish speakers. They heuristically interpolated between multiple prompts to generate new gloss templates for each possible feature vector and lemma. They experiment with both appending their synthetic translations to the parallel text as well as using an additional phrase table (and the combination of both), but did not ﬁnd that one method was consistently superior. Broadly, the goal of our substitution approach is to transform the source language into"
W18-1813,P08-1059,0,0.0429118,"generation tool or lookup table: ‘(we) were VBG’ + buy → ‘(we) were buying’ This completes the gloss generation process: comprábamos → ‘(we) were buying’ 1 https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 133 3 Related Work Prior work has both explored ways of generating morphological information, and incorporating such morphological information into Phrase Based Machine Translation. While there is work on generating rich morphology on the target side of translations (for example: (Toutanova et al., 2008; Huck et al., 2017), we focus on rich source side morphology in this work. Hewitt et al. (2016) created glosses by re-purposing instructional prompts found in a special corpus designed to elicit inﬂectional paradigms from bilingual speakers (Sylak-Glassman et al., 2016). For example, the sample prompt ‘(The apple) has been eaten.’ was designed to elicit third person present perfect verb forms from bilingual Spanish speakers. They heuristically interpolated between multiple prompts to generate new gloss templates for each possible feature vector and lemma. They experiment with both appending t"
W18-6011,P17-1080,0,0.0204505,"ph represents both categories as a single templatic value. 2. We discard any values that UniMorph doesn’t annotate for a particular part of speech, like gender and number in French verb participles, or German noun genders. Our approach to converting UD MSDs to UniMorph MSDs begins with the attribute-value lookup, then amends it on a language-specific basis. Alterations informed by the MSD and the word form, like insertion, substitution, and deletion, increase the number of agreeing annotations. They are critical for work that examines the MSD monolithically instead of feature-by-feature (e.g. Belinkov et al., 2017; Cotterell and Heigold, 2017): Without exact matches, converting the individual tags becomes hollow. 3. We make MSD additions when they are unambiguously implied by the resources, like PFV to accompany PST in Spanish “pasado simple”, but PST to accompany IPFV in Spanish “pasado continuo”. 4. We also incorporate fixes using information outside of the MSD like the L G S PEC 1 tag for Spanish’s “-ra” forms, as described in §4, and other language-specific corrections, like mapping the various dative cases to the crosslingually comparable case annotations used in UniMorph. Beginning our process, w"
W18-6011,J93-2003,0,0.0985893,"ords to be a match if their form and lemma are present in both resources. Syncretism allows a single surface form to realize multiple MSDs (Spanish “mandaba” can be first- or third-person), so we define success as the computed MSD matching any of the word’s UniMorph MSDs. This gives rise to an equation for recall: of the word–lemma pairs found in both resources, how many of their UniMorph-converted MSDs are present in the UniMorph tables? Why not a learned mapping? One can imagine learning the UniMorph MSD corresponding to a UD dataset’s MSD by a set-to-set translation model like IBM Model 1 (Brown et al., 1993). Unfortunately, statistical (and especially neural) machine translation generalizes in unreliable ways. Our goal is a straightforward, easily manipulable and extensible conversion that prioritizes correctness over coverage. 6 Intrinsic evaluation Experiments Why no held-out test set? Our problem here is not a learning problem, so the question is ill-posed. There is no training set, and the two resources for a given language make up a test set. The quality of our model—the conversion tool—comes from how well we encode prior knowledge about the relationship between the UD and UniMorph corpora."
W18-6011,D17-1078,1,0.850043,"gories as a single templatic value. 2. We discard any values that UniMorph doesn’t annotate for a particular part of speech, like gender and number in French verb participles, or German noun genders. Our approach to converting UD MSDs to UniMorph MSDs begins with the attribute-value lookup, then amends it on a language-specific basis. Alterations informed by the MSD and the word form, like insertion, substitution, and deletion, increase the number of agreeing annotations. They are critical for work that examines the MSD monolithically instead of feature-by-feature (e.g. Belinkov et al., 2017; Cotterell and Heigold, 2017): Without exact matches, converting the individual tags becomes hollow. 3. We make MSD additions when they are unambiguously implied by the resources, like PFV to accompany PST in Spanish “pasado simple”, but PST to accompany IPFV in Spanish “pasado continuo”. 4. We also incorporate fixes using information outside of the MSD like the L G S PEC 1 tag for Spanish’s “-ra” forms, as described in §4, and other language-specific corrections, like mapping the various dative cases to the crosslingually comparable case annotations used in UniMorph. Beginning our process, we relied on documentation of t"
W18-6011,K18-3001,1,0.882683,"Missing"
W18-6011,K17-2001,1,0.892721,"Missing"
W18-6011,W17-0405,0,0.0486603,"Missing"
W18-6011,W05-0807,1,0.760167,"Missing"
W18-6011,L18-1293,1,0.652734,"Missing"
W18-6011,E17-2018,1,0.849189,"Morph’s schema does not indicate the type of pronoun (demonstrative, interrogative, etc.), and when lexical information is not recorded in UniMorph, we delete it from the MSD during transformation. On the other hand, UniMorph’s atomic tags have more parts to guess, but they are often related. (E.g. I PFV always entails P ST in Spanish.) Altogether, these forces seem to have little impact on tagging performance. 8 In addition to using the number of extra rules as a proxy for harmony between resources, one could perform cross-lingual projection of morphological tags (Dr´abek and Yarowsky, 2005; Kirov et al., 2017). Our approach succeeds even without parallel corpora. 9 Conclusion and Future Work We created a tool for annotating Universal Dependencies CoNLL-U files with UniMorph annotations. Our tool is ready to use off-the-shelf today, requires no training, and is deterministic. While under-specification necessitates a lossy and imperfect conversion, ours is interpretable. Patterns of mistakes can be identified and ameliorated. The tool allows a bridge between resources annotated in the Universal Dependencies and Universal Morphology (UniMorph) schemata. As the Universal Dependencies project provides a"
W18-6011,L16-1498,1,0.848261,"directly comparable across languages. Its features are informed by a distinction between universal categories, which are widespread and psychologically “real” to speakers; and comparative concepts, only used by linguistic typologists to compare languages (Haspelmath, 2010). Additionally, it strives for identity of meaning across languages, not simply similarity of terminology. As a prime example, it does not regularly label a dative case for nouns, for reasons explained in depth by Haspelmath (2010).4 The UniMorph resources for a language contain complete paradigms extracted from Wiktionary (Kirov et al., 2016, 2018). Word types are annotated to form a database, mapping a lemma–tag pair to a surface form. The schema is explained in detail in Sylak-Glassman (2016). It has been used in the SIGMORPHON shared task (Cotterell et al., 2016) and the CoNLL–SIGMORPHON shared tasks (Cotterell et al., 2017, 2018). Several components of the UniMorph schema have been adopted by UD.5 Universal Dependencies The Universal Dependencies morphological schema comprises part of speech and 23 additional attributes (also called features in UD) annotating meaning or syntax, as well as language-specific attributes. In orde"
W18-6011,P18-1247,0,0.200895,"tated datasets. UD’s v2.1 release (Nivre et al., 2017) has 102 treebanks in 60 languages. The large resource, constructed by independent parties, evinces problems in the goal of a universal inventory of annotations. Annotators may choose to omit certain values (like the coerced gender of refrescante in Figure 1), and they may disagree on how a linguistic concept is encoded. (See, e.g., Haspelmath’s (2010) description of the dative case.) Additionally, many of the treebanks “were created by fully- or semi-automatic conversion from treebanks with less comprehensive annotation schemata than UD” (Malaviya et al., 2018). For instance, the Spanish word “vas” “you go” is incorrectly labeled G ENDER : F EM|N UMBER : P L because it ends in a character sequence which is common among feminine plural nouns. (Nevertheless, the part of speech field for “vas” is correct.) UniMorph’s development is more centralized and pipelined.7 Inflectional paradigms are scraped from Wiktionary, annotators map positions in the scraped data to MSDs, and the mapping is automatically applied to all of the scraped paradigms. Because annotators handle languages they are familiar with (or related ones), realization of the schema is also d"
W18-6011,J93-2004,0,0.0604809,"a given lemma and part of speech gives a paradigm: a mapping from slots to surface forms. Regular English verbs have five slots in their paradigm (Long, 1957), which we illustrate for the verb prove, using simple labels for the forms in Table 1. A morphosyntactic schema prescribes how language can be annotated—giving stricter categories than our simple labels for prove—and can vary in the level of detail provided. Part of speech tags are an example of a very coarse schema, ignoring details of person, gender, and number. A slightly finer-grained schema for English is the Penn Treebank tagset (Marcus et al., 1993), which includes signals for English morphology. For instance, its VBZ tag pertains to the specially inflected 3rd-person singular, present-tense verb form (e.g. “proves” in Table 1). If the tag in a schema is detailed enough that it exactly specifies a slot in a paradigm, it is • We detail a deterministic mapping from UD morphological annotations to UniMorph. Language-specific edits of the tags in 31 languages increase harmony between converted UD and existing UniMorph data (§5). • We provide an implementation of this mapping and post-editing, which replaces the UD features in a CoNLL-U file"
W18-6011,W17-0419,0,0.0599591,"Missing"
W18-6011,petrov-etal-2012-universal,0,0.0495453,"nguage. Our approach, by contrast, is a direct flight from the source to the target.) Because UniMorph corpora are noisy, the encoding from the interlingua would have to be rewritten for each target. Further, decoding the UD MSD into the interlingua cannot leverage external information like the lemma and form. The creators of HamleDT sought to harmonize dependency annotations among treebanks, similar to our goal of harmonizing across resources (Zeman et al., 2014). The treebanks they sought to harmonize used multiple diverse annotation schemes, which the authors unified under a single scheme. Petrov et al. (2012) present mappings into a coarse, “universal” part of speech for 22 languages. Working with POS tags rather than morphological tags (which have far more dimensions), their space of options to harmonize is much smaller than ours. Our extrinsic evaluation is most in line with the paradigm of Wisniewski and Lacroix (2017) (and similar work therein), who compare syntactic parser performance on UD treebanks annotated with two styles of dependency representation. Our problem differs, though, in that the dependency representations express different relationships, while our two schemata vastly overlap."
W18-6011,W17-0412,0,0.0275171,"ject releases type-level annotated tables, the newfound compatibility opens up new experiments. A prime example of exploiting tokenand type-level data is T¨ackstr¨om et al. (2013). That work presents a part-of-speech (POS) dictionary built from Wiktionary, where the POS tagger is also constrained to options available in their typelevel POS dictionary, improving performance. Our transformation means that datasets are prepared for similar experiments with morphological tagging. It would also be reasonable to incorporate this tool as a subroutine to UDPipe (Straka and Strakov´a, 2017) and Udapi (Popel et al., 2017). We leave open the task of converting in the opposite direction, turning UniMorph MSDs into Universal Dependencies MSDs. Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources. These findings will harden both resources and better align them with their goal of universal, crosslingual annotation. Related Work The goal of a tagset-to-tagset mapping of morpholo"
W18-6011,K17-3009,0,0.0457031,"Missing"
W18-6011,P15-2111,1,0.851167,"Missing"
W18-6011,K17-3001,0,0.0560233,"alled a morphosyntactic description (MSD).2 These descriptions require varying amounts of detail: While the English verbal paradigm is small enough to fit on a page, the verbal paradigm of the Northeast Caucasian language Archi can have over 1,500,000 slots (Kibrik, 1998). 3 is an annotated treebank, making it a resource of token-level annotations. The schema is guided by these treebanks, with feature names chosen for relevance to native speakers. (In §3.2, we will contrast this with UniMorph’s treatment of morphosyntactic categories.) The UD datasets have been used in the CoNLL shared tasks (Zeman et al., 2017, 2018 to appear). Two Schemata, Two Philosophies Unlike the Penn Treebank tags, the UD and UniMorph schemata are cross-lingual and include a fuller lexicon of attribute-value pairs, such as P ER SON : 1. Each was built according to a different set of principles. UD’s schema is constructed bottomup, adapting to include new features when they’re identified in languages. UniMorph, conversely, is top-down: A cross-lingual survey of the literature of morphological phenomena guided its design. UniMorph aims to be linguistically complete, containing all known morphosyntactic attributes. Both schemat"
W99-0612,A97-1028,0,0.0577277,"Missing"
W99-0612,H92-1045,1,0.804419,"Missing"
W99-0612,P95-1026,1,0.287958,"Missing"
W99-0612,W97-0305,0,0.0537616,"Missing"
W99-0612,W97-0312,0,0.0147031,"Missing"
