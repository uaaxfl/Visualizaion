2011.jeptalnrecital-long.6,2010.jeptalnrecital-long.39,0,0.0910184,"Missing"
2011.jeptalnrecital-long.6,C10-2030,0,0.0255174,"Missing"
2011.jeptalnrecital-long.6,W08-0615,0,0.0669835,"Missing"
2011.jeptalnrecital-long.6,P09-3003,0,0.0664102,"Missing"
2015.jeptalnrecital-court.40,moriceau-tannier-2014-french,1,0.880988,"Missing"
2015.jeptalnrecital-long.3,P10-1052,0,0.066151,"Missing"
2015.jeptalnrecital-long.3,moriceau-tannier-2014-french,1,0.877846,"Missing"
2015.jeptalnrecital-long.3,S10-1071,0,0.0564766,"Missing"
2015.jeptalnrecital-long.3,P05-3021,0,0.110123,"Missing"
2016.jeptalnrecital-poster.7,D14-1187,0,0.0468592,"Missing"
2018.jeptalnrecital-court.32,W09-3002,0,0.0881323,"Missing"
2018.jeptalnrecital-court.32,D17-2017,0,0.0330491,"Missing"
2018.jeptalnrecital-court.32,W10-1807,0,0.0489928,"Missing"
2018.jeptalnrecital-court.32,W07-1509,0,0.0728015,"Missing"
2018.jeptalnrecital-court.32,L16-1643,1,0.869191,"Missing"
2018.jeptalnrecital-court.32,C12-2082,0,0.069741,"Missing"
2018.jeptalnrecital-court.32,D14-1162,0,0.0891903,"Missing"
2018.jeptalnrecital-court.32,U13-1012,0,0.037588,"Missing"
2018.jeptalnrecital-court.32,voutilainen-2012-improving,0,0.0314545,"Missing"
2018.jeptalnrecital-court.32,P14-5016,0,0.0540445,"Missing"
2018.jeptalnrecital-deft.1,2018.jeptalnrecital-deft.12,0,0.102356,"(negative, neutral, positive, mixed), to identify clues of sentiment and target, and to annotate each tweet in terms of source and target concerning all expressed sentiments. Twelve teams participated, mainly on the two first tasks. On the identification of tweets about public transportation, micro F-measure values range from 0.827 to 0.908. On the identification of the global polarity, micro F-measure values range from 0.381 to 0.823. M OTS - CLÉS : Classification automatique, Analyse de sentiments, Fouille de texte. K EYWORDS: Automatic Classification, Sentiment Analysis, Text Mining. c ATALA 2018 219 1 Introduction Dans la continuité de l’édition 2015 (Hamon et al., 2015), la treizième édition du DÉfi Fouille de Textes (DEFT) porte sur l’extraction d’information et l’analyse de sentiments dans des tweets rédigés en français sur la thématique des transports. La campagne s’est déroulée sur une période limitée avec une ouverture des inscriptions le 31 janvier, la diffusion des données d’entraînement à partir du 19 février, et le déroulement de la phase de test entre le 28 mars et le 5 avril, sur une durée de trois jours fixée par chacun des participants. Quinze équipes se sont inscrites,"
2019.jeptalnrecital-long.5,W18-5614,1,0.847588,"Missing"
2019.jeptalnrecital-long.5,W15-2604,1,0.853082,"Missing"
2019.jeptalnrecital-long.5,X96-1019,0,0.802383,"Missing"
2019.jeptalnrecital-long.5,P10-1052,0,0.116292,"Missing"
2019.jeptalnrecital-long.5,W08-0606,0,0.0536337,"Missing"
2020.jeptalnrecital-deft.1,2020.lrec-1.851,1,0.759904,"Missing"
2020.jeptalnrecital-deft.1,2020.jeptalnrecital-deft.4,0,0.0657627,"Missing"
2020.jeptalnrecital-deft.1,W18-7002,1,0.875702,"Missing"
2020.jeptalnrecital-deft.1,W18-5614,1,0.866685,"Missing"
2020.jeptalnrecital-deft.1,W19-5029,1,0.892526,"Missing"
2020.jeptalnrecital-deft.1,E12-2021,0,0.0586855,"Missing"
2021.eval4nlp-1.1,Q17-1010,0,0.00626055,"token is associated to one true label and named entities are encoded according to the BIO (begin, inside, outside) scheme. In the present work we deal with tokens rather than entities, so that we can apply the presented method directly. We consider that ‘O’ labels are negatives and that all other labels are positives. A true positive system prediction is an association between an input token and a non-‘O’ label that is the gold-standard label for this token. We are comparing entity detection systems that rely on word embeddings based upon CharacterBert (El Boukkouri et al., 2020) or fastText (Bojanowski et al., 2017), pre-trained on different corpora, either as-is or concatenated with knowledge embeddings learned using node2vec (Grover and Leskovec, 2016) on two biomedical vocabularies (the Medical Suject Headings (MeSH), and SNOMED CT). Moreover, we also consider a variant of CharacterBert where the node2vec embeddings are injected within the model architecture. The fastText embeddings are either randomly initialized, which we note “fastTextRandom”; pre-trained on a newswire corpus (Gigaword (Graff et al., 2007)), which we note “fastTextGigaword”; or on medical corpora (PubMed Central3 and MIMIC-III (Joh"
2021.eval4nlp-1.1,2020.coling-main.609,1,0.733787,"Missing"
2021.eval4nlp-1.1,2020.emnlp-main.393,0,0.03534,"sign choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as promoted by leaderboards) overlooks aspects such as utility, prediction cost, and robustness of models. They recommend considering the point of view of the user of models rather than just performance scores to estimate their relevance. Trying to provide a finer understanding of the issues raised by the input text and of the limitations of the evaluated systems, we propose a new qualitative analysis method that takes into account the observed relative difficulty of predicting gold labels for each input. This difficulty is assessed pragmatically bas"
2021.eval4nlp-1.1,C96-1079,0,0.860475,"ipating in a multi-label text classification task (CLEF eHealth 2018 ICD-10 coding), and a comparison of neural models trained for biomedical entity detection (BioCreative V chemical-disease relations dataset). 1 Introduction The analysis of NLP system results has mainly focused on evaluation scores meant to rank systems and feed leaderboards. In tasks such as information extraction, text classification, etc., evaluation generally relies on the comparison of a hypothesis (typically a system output) with a gold standard, generally produced through manual annotation. Since the MUC-6 conference (Grishman and Sundheim, 1996), the metrics used were created for information retrieval (Cleverdon, 1960): recall (true positive rate), precision (positive predictive value) and their harmonic (possibly weighted) mean, the F1score. Evaluation scripts are widely available nowadays, for instance those of the CoNLL shared tasks (Tjong Kim Sang and De Meulder, 2003). These scripts rely on an annotation scheme based on the 1 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 1–10 c November 10, 2021. 2021 Association for Computational Linguistics Figure 1: Example input file for a"
2021.eval4nlp-1.1,W16-2703,0,0.017816,"r outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as pr"
2021.eval4nlp-1.1,W04-1013,0,0.0384163,"o Processing Lucie Gianola, Hicham El Boukkouri, Cyril Grouin, Thomas Lavergne, Patrick Paroubek, Pierre Zweigenbaum Université Paris-Saclay, CNRS, LISN, 91405, Orsay, France firstname.lastname@lisn.fr Abstract BIO prefix used to specify whether a token is at the beginning, inside or outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scor"
2021.eval4nlp-1.1,P02-1040,0,0.112402,"on: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing Lucie Gianola, Hicham El Boukkouri, Cyril Grouin, Thomas Lavergne, Patrick Paroubek, Pierre Zweigenbaum Université Paris-Saclay, CNRS, LISN, 91405, Orsay, France firstname.lastname@lisn.fr Abstract BIO prefix used to specify whether a token is at the beginning, inside or outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrate"
2021.eval4nlp-1.1,2021.acl-long.179,0,0.0278283,"machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as promoted by leaderboards) overlooks aspects such as utility, prediction cost, and robustness of models. They recommend considering the point of view of the user of models rather than just performance scores to estimate their relevance. Trying to"
2021.jeptalnrecital-deft.1,2021.jeptalnrecital-deft.2,0,0.0632114,"Missing"
2021.jeptalnrecital-deft.1,Q13-1032,0,0.0263255,"Missing"
2021.jeptalnrecital-deft.1,S13-2045,0,0.0237811,"Missing"
2021.jeptalnrecital-deft.1,2021.jeptalnrecital-deft.3,0,0.0903058,"Missing"
2021.jeptalnrecital-deft.1,W18-5614,1,0.84626,"Missing"
2021.jeptalnrecital-deft.1,W19-5029,1,0.900129,"Missing"
2021.jeptalnrecital-deft.1,horbach-etal-2014-finding,0,0.0465509,"Missing"
2021.jeptalnrecital-deft.1,W19-4433,0,0.0440774,"Missing"
2021.jeptalnrecital-deft.1,E09-1065,0,0.145203,"Missing"
2021.jeptalnrecital-deft.1,E12-2021,0,0.147089,"Missing"
C12-2079,J08-4004,0,0.650981,"us (a “gold-standard”) can obviously bias an evaluation performed using this corpus as a reference. Finally, a bad quality annotation would lead to misleading clues in a linguistic analysis used to create rule-based systems. However, it is not possible to directly evaluate the validity of manual annotations. Instead, interannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can be of many types and the well-known Kappa-family is described in details in (Artstein and Poesio, 2008). However, as pointed out by the authors of this article, the obtained results are difficult to interpret. Kappa coefficients, for example, are difficult to compare, even within the same annotation task, as they imply a definition of the markables that can vary from one campaign to the other (Grouin et al., 2011). More generally, we lack clues to know if a Kappa of 0.75 is a “good” result, or if a Kappa of 0.8 is twice as good as one of 0.4 or if a result of 0.6 obtained using one coefficient is better than 0.5 with another one, and for which annotation task. We first briefly present the state"
C12-2079,J11-4004,0,0.0379275,"e obtained results. However, their analyses lack robustness, as they only apply to similar campaigns. Other studies concerning the evaluation of the quality of manual annotation identified some factors that influence inter- and intra-annotator agreements, thereby giving clues on their behavior. Gut and Bayerl (2004) thus demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated: the larger the number of categories, the lower the inter-annotator agreement. However, categories prone to confusion are in limited number. The meta-analysis presented by Bayerl and Paul (2011) extends this research on the factors influencing agreement results, identifying 8 such factors and proposing useful recommendations to improve manual annotation reliability. However, neither of these studies provides a clear picture of the behavior of the agreement coefficients 0 This work has been partially financed by OSEO, the French State Agency for Innovation, under the Quaero program. 810 or of their meanings. The experiments detailed in (Reidsma and Carletta, 2008) constitute an interesting step in this direction, focusing on the effect of annotation errors on machine learning systems"
C12-2079,fort-etal-2012-analyzing,1,0.700025,"present the pros and cons of these methods, from the statistical and mathematical points of view, with some hints about specific issues raised in some annotation campaigns, like the prevalence of one category. A section of their article is dedicated to various attempts at providing an interpretation scale for the Kappa family coefficients and how they failed to converge. Works such as (Gwet, 2012) are also to be mentioned. They present various inter-rater reliability coefficients and insist on benchmarking issues related to their interpretation. Many authors, among whom (Grouin et al., 2011; Fort et al., 2012), tried to obtain a more precise assessment of the quality of the annotation in their campaigns by computing different coefficients and analyzing the obtained results. However, their analyses lack robustness, as they only apply to similar campaigns. Other studies concerning the evaluation of the quality of manual annotation identified some factors that influence inter- and intra-annotator agreements, thereby giving clues on their behavior. Gut and Bayerl (2004) thus demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated: the larger the number o"
C12-2079,W11-0411,1,0.934731,"nterannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can be of many types and the well-known Kappa-family is described in details in (Artstein and Poesio, 2008). However, as pointed out by the authors of this article, the obtained results are difficult to interpret. Kappa coefficients, for example, are difficult to compare, even within the same annotation task, as they imply a definition of the markables that can vary from one campaign to the other (Grouin et al., 2011). More generally, we lack clues to know if a Kappa of 0.75 is a “good” result, or if a Kappa of 0.8 is twice as good as one of 0.4 or if a result of 0.6 obtained using one coefficient is better than 0.5 with another one, and for which annotation task. We first briefly present the state of the art (Section 2), then detail the principles of our method to benchmark measures (Section 3) and show on some examples how different coefficients can be compared (Section 4). We finally discuss current limitations and point out future developments. 2 State of the art A quite detailed analysis of the most c"
C12-2079,J02-1002,0,0.272818,"on the contrary, a reference element is missing (false negative). All of these error paradigms tend to damage the annotations, so each of them should be taken into account by agreement measures. We propose here to apply each measure to a set of corpora, each of which embeds errors from one or more paradigms, and with a certain magnitude (the higher the magnitude, the higher the number of errors). This experiment should allow us to observe how the measures behave w.r.t. the different paradigms, and with a full range of magnitudes. The idea of creating artificial damaged corpora is inspired by Pevzner and Hearst (2002), then Bestgen (2009) in thematic segmentation, but our goal (giving meaning to measures) and our method (e.g. applying progressive magnitudes) are very different. 3.2 Protocol Reference. A reference annotation set (called reference) is provided to the system: a true Gold Standard or an automatically generated set based on a statistical model. It is assumed to correspond exactly to what annotations should be, with respect to the annotation guidelines. Shuffling. A shuffling process is an algorithm that automatically generates a multi-annotated corpus given three parameters: a reference annotat"
C12-2079,J08-3001,0,0.379365,", and the obtained results are used to model the behavior of these measures and understand their actual meaning. KEYWORDS: inter-annotator agreement, manual corpus annotation, evaluation. Proceedings of COLING 2012: Posters, pages 809–818, COLING 2012, Mumbai, December 2012. 809 1 Introduction The quality of manual annotations has a direct impact on the applications using them. For example, it was demonstrated that machine learning tools learn to make the same mistakes as the human annotators, if these mistakes follow a certain regular pattern and do not correspond to simple annotation noise (Reidsma and Carletta, 2008; Schluter, 2011). Furthermore, errors in a manually annotated reference corpus (a “gold-standard”) can obviously bias an evaluation performed using this corpus as a reference. Finally, a bad quality annotation would lead to misleading clues in a linguistic analysis used to create rule-based systems. However, it is not possible to directly evaluate the validity of manual annotations. Instead, interannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can"
chatzimina-etal-2014-use,N04-1043,0,\N,Missing
chatzimina-etal-2014-use,J92-4003,0,\N,Missing
chatzimina-etal-2014-use,P08-2026,0,\N,Missing
chatzimina-etal-2014-use,P06-1043,0,\N,Missing
chatzimina-etal-2014-use,P06-1055,0,\N,Missing
chatzimina-etal-2014-use,N13-1090,0,\N,Missing
chatzimina-etal-2014-use,P10-1040,0,\N,Missing
chatzimina-etal-2014-use,P10-1052,0,\N,Missing
deleger-etal-2014-annotation,W12-4304,0,\N,Missing
deleger-etal-2014-annotation,W12-2411,0,\N,Missing
deleger-etal-2014-annotation,E12-2021,0,\N,Missing
F13-1036,P11-2087,0,0.0575878,"Missing"
F13-1036,E99-1042,0,0.183517,"Missing"
F13-1036,W12-2202,0,0.0436198,"Missing"
F13-1036,S12-1066,0,0.0236179,"Missing"
F13-1036,S12-1068,1,0.888679,"Missing"
F13-1036,quasthoff-etal-2006-corpus,0,0.0705231,"Missing"
F13-1036,D11-1038,0,0.0383291,"Missing"
F13-1036,N10-1056,0,0.0755902,"Missing"
galibert-etal-2012-extended,grover-etal-2008-named,0,\N,Missing
galibert-etal-2012-extended,A00-1044,0,\N,Missing
galibert-etal-2012-extended,C02-1130,0,\N,Missing
galibert-etal-2012-extended,W11-0411,1,\N,Missing
galibert-etal-2012-extended,I11-1058,1,\N,Missing
galibert-etal-2012-extended,doddington-etal-2004-automatic,0,\N,Missing
galibert-etal-2012-extended,bick-2004-named,0,\N,Missing
galibert-etal-2012-extended,galibert-etal-2010-named,1,\N,Missing
galibert-etal-2012-extended,C96-1079,0,\N,Missing
goryainova-etal-2014-morpho,vasilescu-etal-2012-cross,1,\N,Missing
goryainova-etal-2014-morpho,luzzati-etal-2014-human,1,\N,Missing
goryainova-etal-2014-morpho,gravier-etal-2012-etape,0,\N,Missing
grappy-etal-2010-corpus,cramer-etal-2006-building,0,\N,Missing
grappy-etal-2010-corpus,rosset-petel-2006-ritel,0,\N,Missing
grappy-etal-2010-corpus,varasai-etal-2008-building,0,\N,Missing
grouin-2008-certification,W04-2104,0,\N,Missing
grouin-2008-certification,J06-3001,0,\N,Missing
grouin-2014-biomedical,J96-1002,0,\N,Missing
grouin-2014-biomedical,J92-4003,0,\N,Missing
grouin-2014-biomedical,W13-2011,0,\N,Missing
grouin-2014-biomedical,W13-2022,1,\N,Missing
grouin-2014-biomedical,W13-2024,0,\N,Missing
grouin-2014-biomedical,W13-2001,0,\N,Missing
grouin-2014-biomedical,W11-1801,0,\N,Missing
grouin-2014-biomedical,P10-1052,0,\N,Missing
I11-1058,P98-1031,0,0.164757,"(2009) and many others have focused on speech data. Named Entity detection evaluation over French spoken data has been proposed within the Ester II project, as described by Galliano et al. (2009). Within the framework of the Quaero project,we proposed an extended named entity definition with compositional and hierarchical structure. This extension raises new issues and challenges in NER evaluation. First, as we shall explain below in more detail, the usual evaluation methods cannot compute the Slot Error Rate (SER) metric when named entities are compositional and recursive. Second, following Burger et al. (1998) and Hirschman et al. (1999), we consider that the evaluation of named entity recognition on noisy text output by automatic speech recognition (ASR) systems should take as reference the named entities found in the human annotation of a humantranscribed text: what should have been there in the ASR output. This requires to project the clean reference to the noisy text, which is made all the more difficult because of the compositional and hierarchical structure of the named entities. Introduction Named Entity Detection has been studied since the MUC conferences in 1987. The notion has been extend"
I11-1058,C02-1130,0,0.144389,"are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date, time, and amounts (money and percents in most cases). Specific entities have been proposed and handled for some tasks, e.g. language and shape by Rosset et al. (2007), or email address and phone number (Maynard et al., 2001). In specific domains, entities such as g"
I11-1058,galibert-etal-2010-named,1,0.873075,"Missing"
I11-1058,moreau-etal-2010-evaluation,1,0.810616,"ed to project the clean reference on the noisy text in order to build a new reference. That new reference then allows us to apply the clean text methodology. This projection method consists in finding new positions for the frontiers through either a dynamic programming alignment (standard sclitetype ASR evaluation alignment) or a phone-level dynamic programming alignment using canonical phonetizations. They noticed the result was too strict frontier-wise and required reducing the weight of frontier errors to obtain significant results. In Question Answering from speech transcripts evaluation, Moreau et al. (2010) required that QA systems extract answers to natural language questions from ASR outputs of broadcast news shows. The inherent application was to replay the sound segment containing the answer, with a time interval as an answer; it tolerated a time interval around the boundaries. The results were satisfactory. We thus decided to project the clean reference on the noisy text following five steps: pers.ind name.first recevrons Benoît name.last Majimel 167.5s recevrons l' acteur 168s Benoît magie name.first 168.5s mais l' acteur name.last pers.ind Figure 4: Example of a fuzzy reference built by t"
I11-1058,C96-1079,0,0.792768,"al nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date"
I11-1058,W11-0411,1,0.350134,"om news data, we chose to support new kinds of entities (time, function, etc.) in order to extract a maximum of information from the corpus we processed. Compared to existing named entity structuration, our approach is more general than the extensions that have been done for specific domains, and is simpler than the complete hierarchy defined by Sekine (2004). This structure allows us to cover a large amount of named entities with a basic categorization so as to be quickly suitable for all further annotation work. The extended named entities we defined are both hierarchical and compositional (Grouin et al., 2011). This hierarchical and compositional nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and compositional (Section 2.4). Section 2.5 provides a discussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Co"
I11-1058,W03-0419,0,0.0376895,"Missing"
I11-1058,sekine-nobata-2004-definition,0,0.68687,"of proper names (e.g., phrases built around substantives). In this work, we decided to extend the coverage of the named entities rather than sub-dividing the existing classes as it has been done in previous work. As we aimed to build a fact database from news data, we chose to support new kinds of entities (time, function, etc.) in order to extract a maximum of information from the corpus we processed. Compared to existing named entity structuration, our approach is more general than the extensions that have been done for specific domains, and is simpler than the complete hierarchy defined by Sekine (2004). This structure allows us to cover a large amount of named entities with a basic categorization so as to be quickly suitable for all further annotation work. The extended named entities we defined are both hierarchical and compositional (Grouin et al., 2011). This hierarchical and compositional nature of the extended named entities imply a specific method when evaluating system outputs (see Section 3). In this section, we present our extension to named entities, starting with related work (Section 2.1) and specifying their scope (Section 2.2). Our entities are hierarchical (Section 2.3) and c"
I11-1058,I05-1058,0,0.0287484,"ussion of the issues they raise in the evaluation of named entity recognition from speech transcripts. 2.1 Named Entity Types Named Entity recognition was initially defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities are proper names categorized into three major classes: persons, locations and organizations. Proposals have been made to sub-divide these entities into finer-grained classes. For example, politicians for the person class by Fleischman and Hovy (2002) or cities for the location class by Fleischman (2001) as well as Lee and Lee (2005). The CONLL conference added a miscellaneous type which includes proper names outside the previous classes. Some classes are sometimes added, e.g. product by Bick (2004). Some numerical types are also often described and used in the literature: date, time, and amounts (money and percents in most cases). Specific entities have been proposed and handled for some tasks, e.g. language and shape by Rosset et al. (2007), or email address and phone number (Maynard et al., 2001). In specific domains, entities such as gene, protein, DNA etc. are also addressed (Ohta, 2002) and campaigns are organized f"
I11-1058,W04-1213,0,\N,Missing
I11-1058,bonneau-maynard-etal-2006-results,0,\N,Missing
I11-1058,C98-1031,0,\N,Missing
I11-1058,bick-2004-named,0,\N,Missing
I17-1101,P14-2111,0,0.159126,"cons (de Does and Depuydt, 2013) and characterbased errors statistics obtained from a corrected training set (Kumar and Lehal, 2016). However, they have some drawbacks that limit their usefulness for specific, low-resource domains. Such resources are expensive to create and for highly specialized texts (e.g., medical domain) not always possible to obtain. The recent advances in neural network models, based on textual context and needing no external resources, provide new opportunities for OCR post-correction. Character-level sequence modeling architectures are especially suited for this task (Chrupała, 2014; Schmaltz et al., 2016), as they reduce the complexity at output time. Moreover, current systems are often limited to processing texts with a limited degree of OCR corruption, i.e., so-called single-error word corrections (Kissos and Dershowitz, 2016) and correction of OCRed corpora that have been generated by older OCR engines can prove too challenging. The correct recognition of historical texts remains an open challenge (Kluzner et al., 2009). A general-purpose OCR post-correction tool should be adaptable to the ratio of error that is present in the OCR output in order to deal with both ty"
I17-1101,W16-6108,1,0.886811,"Missing"
I17-1101,H05-1109,0,0.0554927,"into a Bayesian model (Tong and Evans, 1996) or a HMM model (Borovikov et al., 2004) to select the optimal word candidate. These systems are explicitly or implicitly limited to cases in which an erroneous word appears in an otherwise clean context. For serious degrees of corruption (e.g., historical texts), the common approach aims to optimally combine an ensemble of multiple OCR engines (Nakano et al., 2004; Lund and Ringger, 2009). ‘Noisy channel paradigm’ aims to learn error models describing the OCR output generation from the reference text, and as such combine error and language models. Kolak and Resnik (2005) used finite state machines on a small set of training material while Llobet et al. (2010) combined all OCR process hypotheses for each recognized character. Such models need a large amount of training material which is costly and not always easily available. In response, the TextInduced Corpus Clean-up (TICCL) system (Reynaert, 2011) was developed to run with no annotated training data. It takes noisy texts and extracts the high-frequency word variants through statistical analysis and clusters typographical word variants within a user-defined Levenshtein distance. Recently, Neural Network Lan"
I17-1101,W11-4114,0,0.0497845,"Missing"
I17-1101,W16-0528,0,0.0315064,"d Depuydt, 2013) and characterbased errors statistics obtained from a corrected training set (Kumar and Lehal, 2016). However, they have some drawbacks that limit their usefulness for specific, low-resource domains. Such resources are expensive to create and for highly specialized texts (e.g., medical domain) not always possible to obtain. The recent advances in neural network models, based on textual context and needing no external resources, provide new opportunities for OCR post-correction. Character-level sequence modeling architectures are especially suited for this task (Chrupała, 2014; Schmaltz et al., 2016), as they reduce the complexity at output time. Moreover, current systems are often limited to processing texts with a limited degree of OCR corruption, i.e., so-called single-error word corrections (Kissos and Dershowitz, 2016) and correction of OCRed corpora that have been generated by older OCR engines can prove too challenging. The correct recognition of historical texts remains an open challenge (Kluzner et al., 2009). A general-purpose OCR post-correction tool should be adaptable to the ratio of error that is present in the OCR output in order to deal with both types of errors. In this p"
I17-1101,W96-0108,0,0.388647,"lexicon. 2 Background The problem of OCR post-correction has been studied since the seventies (Kukich, 1992). While traditional OCR error detection systems focused on constructing ‘confusion matrices’ of likely character (pairs) to detect corruptions of existing words into non-words, recent systems improve accuracy using information on the language context in which the error appears (Evershed and Fitch, 2014), using bigrams (Kissos and Dershowitz, 2016), large-scale word n-grams and character ngrams from the web (Bassil and Alwani, 2012) or associating confusion scores into a Bayesian model (Tong and Evans, 1996) or a HMM model (Borovikov et al., 2004) to select the optimal word candidate. These systems are explicitly or implicitly limited to cases in which an erroneous word appears in an otherwise clean context. For serious degrees of corruption (e.g., historical texts), the common approach aims to optimally combine an ensemble of multiple OCR engines (Nakano et al., 2004; Lund and Ringger, 2009). ‘Noisy channel paradigm’ aims to learn error models describing the OCR output generation from the reference text, and as such combine error and language models. Kolak and Resnik (2005) used finite state mac"
L16-1570,J92-4003,0,0.0954695,"Missing"
L16-1570,P10-1052,0,0.1031,"Missing"
L16-1570,P99-1057,0,0.0975187,"t al. (2010) proposed an algorithm for ancient machine-printed documents (e.g., old books from the 18th century) based on the run length smoothing algorithm (RLSA). They improved the results achieved by the RLSA algorithm for both line, word and character recognitions. Kaur et al. (2013) demonstrated that a pre-processing stage can improve the page decomposition task. Alternatively, post-processing techniques are also used to correct OCR errors which constitute obstacles to many further NLP processes. Those post-processing techniques aim at identifying tables within the text (Kieninger, 1998; Ng et al., 1999) or correcting tokenization errors (Furrer, 2013). Since performances of page decomposition also depends on the metrics used, Shafait et al. (2007) compared six metrics to evaluate the page decomposition functions from OCR systems in order to select the metric that provides the best decomposition outputs. 1 XY Cut or top-down algorithms represent the whole document as the root of a tree. Each final segmentation is represented as a leaf in this tree. Recovering the original document layout Nevertheless, the global layout of digitized documents can be lost, especially frontiers of text columns,"
L16-1570,E12-2021,0,0.0334222,"Missing"
L16-1643,swift-etal-2004-semi,0,\N,Missing
L16-1643,voutilainen-2012-improving,0,\N,Missing
L16-1643,E12-2021,0,\N,Missing
L18-1025,D15-1301,0,0.0261746,"to reproducibility is a problem because without them, we cannot compare studies of reproducibility. A number of such studies have appeared very recently, and in general, the results have been depressing. Multiple studies over the course of the past two years have reported widespread failures of reproducibility (Collaboration and others, 2015; Collberg et al., 2015). They range from unusually large-scale studies in psychology (Collaboration and others, 2015), to surprisingly large ones in computer science (Collberg et al., 2015), to case studies in natural language processing (Schwartz, 2010; Borgholt et al., 2015; Cohen et al., 2016; Gomes et al., 2016; N´ev´eol et al., 2016; Cassidy and Estival, 2017; Kilicoglu, 2017; Mieskes, 2017). Yet, it is still quite difficult to get even a rough sense of the actual scale of the problem in natural language processing, because the lack of agreement about what exactly is being assessed makes it difficult to compare findings across papers on reproducibility issues. 156 To address this problem of a lack of consensus definitions, this paper proposes a set of dimensions of reproducibility. Perhaps counter-intuitively, we first give the definition of replicability or"
L18-1025,J92-1002,0,0.025332,"anguage-related value that stimulated an enormous amount of academic work, some of which has been evaluated with respect to the extent to which it does or does not reproduce the values reported in (Shannon, 1951). For example, (Cover and King, 1978) used a very different method from Shannon’s original one and found a value of 1.3 bits for the entropy of written English. The paper explicitly states that this value “agrees well with Shannon’s estimate,” suggesting that the authors considered their value to have reproduced Shannon’s original value in (Shannon, 1951)3 . In a very different tone, (Brown et al., 1992) reported an upper bound of exactly 1.75 bits, but did not explicitly compare that to previous findings, although it is clear from the paper that they considered it different from—and better than—previously reported values. As the authors put it: We see this paper as a gauntlet thrown down before the computational linguistics community. A relevant value from our papers that was not reproduced is the mean value for the frequency of negation. We reported this in our papers (Cohen et al., 2010) and (Cohen et al., 2017a). They were different by roughly a factor of 2, even though we used the same c"
L18-1025,daelemans-hoste-2002-evaluation,0,0.0820242,"as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics and natural language processing that integrates these strengths of our field and also explores the relationships between natural language processing; computational and corpus linguistics; artificia"
L18-1025,P13-1166,0,0.316125,"Missing"
L18-1025,H90-1013,0,0.482748,", it can be quite difficult to achieve (Fokkens et al., 2013; N´ev´eol et al., 2016), and the causes of reproducibility problems can be well-hidden—see (Johnson et al., 2007; Cohen et al., 2017b), as well as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computati"
L18-1025,H94-1017,0,0.0985049,"difficult to achieve (Fokkens et al., 2013; N´ev´eol et al., 2016), and the causes of reproducibility problems can be well-hidden—see (Johnson et al., 2007; Cohen et al., 2017b), as well as the bug that we report in this paper. 4.3. Definitions of dimensions of reproducibility in the larger context of natural language processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics"
L18-1025,W16-6110,1,0.905358,"Missing"
L18-1025,J05-1004,0,0.0161754,"e processing The bigger picture in which this work is situated is that of a lack of a fully developed epistemology of computational linguistics and natural language processing. Enormous advancements in this area have come from the shared task model of evaluation (Hirschman, 1990; Hirschman, 1994; Jones and Galliers, 1995; Resnik and Lin, 2010; Hirschman, 1998; Chapman et al., 2011; Huang and Lu, 2015), from the development of a science of evaluation in our field (Daelemans and Hoste, 2002; Voorhees et al., 2005; Buckley and Voorhees, 2017), and from the development of a science of annotation (Palmer et al., 2005; Ide, 2007; Wilcock, 2009; Pustejovsky and Stubbs, 2012; Stubbs, 2012; Styler IV et al., 2014; Bonial et al., 2017; Green et al., 2017; Ide and Pustejovsky, 2017; Savova et al., 2017). But, large holes remain in our development of an epistomology of computational linguistics and natural language processing that integrates these strengths of our field and also explores the relationships between natural language processing; computational and corpus linguistics; artificial intelligence, theoretical linguistics, and cognitive science (Cori et al., 2002). (See also (Cori and L´eon, 2002) for a dis"
L18-1025,W10-1726,0,0.114498,"initions related to reproducibility is a problem because without them, we cannot compare studies of reproducibility. A number of such studies have appeared very recently, and in general, the results have been depressing. Multiple studies over the course of the past two years have reported widespread failures of reproducibility (Collaboration and others, 2015; Collberg et al., 2015). They range from unusually large-scale studies in psychology (Collaboration and others, 2015), to surprisingly large ones in computer science (Collberg et al., 2015), to case studies in natural language processing (Schwartz, 2010; Borgholt et al., 2015; Cohen et al., 2016; Gomes et al., 2016; N´ev´eol et al., 2016; Cassidy and Estival, 2017; Kilicoglu, 2017; Mieskes, 2017). Yet, it is still quite difficult to get even a rough sense of the actual scale of the problem in natural language processing, because the lack of agreement about what exactly is being assessed makes it difficult to compare findings across papers on reproducibility issues. 156 To address this problem of a lack of consensus definitions, this paper proposes a set of dimensions of reproducibility. Perhaps counter-intuitively, we first give the definiti"
luzzati-etal-2014-human,nemoto-etal-2008-speech,1,\N,Missing
luzzati-etal-2014-human,gravier-etal-2012-etape,0,\N,Missing
R19-1089,L18-1025,1,0.896139,"Missing"
R19-1089,dakota-kubler-2017-towards,0,0.0480764,"Missing"
R19-1089,Q17-1033,0,0.0541077,"Missing"
R19-1089,P13-1166,0,0.272847,"Missing"
R19-1089,D17-1076,0,0.0438822,"Missing"
R19-1089,E99-1046,0,0.432502,"Missing"
R19-1089,E17-4003,0,0.0452396,"Missing"
R19-1089,W17-1603,1,0.897332,"Missing"
R19-1089,C18-1097,0,0.0384908,"Missing"
R19-1089,W16-6110,1,0.903417,"Missing"
S12-1068,C10-1152,1,0.84074,"utions performed by non-native English speakers, we tried to use linguistic resources that best fit this kind of data. In this way, we made the hypothesis that training our system on documents written by or written for nonnative English speakers would be useful. The use of the Simple English version from Wikipedia seems to be a good solution as it is targeted at people who do not have English as their mother tongue. Our hypothesis seems to be correct due to the results we obtained. Morevover, the Simple English Wikipedia has been used previously in work on automatic text simplification, e.g. (Zhu et al., 2010). 1 http://infolingu.univ-mlv.fr/ DonneesLinguistiques/Dictionnaires/ telechargement.html First, we produced a plain text version of the Simple English Wikipedia. We downloaded the dump dated February 27, 2012 and extracted the textual contents using the wikipedia2text tool.2 The final plaintext file contains approximately 10 million words. We extracted word n-grams (n ranging from 1 to 3) and their frequencies from this corpus thanks to the Text-NSP Perl module 3 and its count.pl program, which produces the list of n-grams of a document, with their frequencies. Table 1 gives the number of n-g"
S12-1068,S12-1046,0,\N,Missing
S16-1190,S16-1165,0,0.0333094,"d, the HeidelTime tool identifies DOCTIME and TIMEX3 elements, and computes DocTimeRel for each EVENT identified by the CRF. Third, another CRF system computes DocTimeRel for each previously identified EVENT, based on DocTimeRel computed by HeidelTime. In the first submission, all EVENTS and TIMEX3 are identified through one general CRF model while in the second submission, we combined two CRF models (one for both EVENT and TIMEX3, and one only for TIMEX3) and we applied post-processing rules on the outputs. 1 Task description Presentation The 2016 Clinical TempEval track3 proposed six tasks (Bethard et al., 2016). We participated in the first five tasks which concern the identification of: (i) spans of time expressions (TS task), (ii) spans of event expressions (ES task), (iii) the attribute of time expressions (TA task), (iv) attributes of event expressions (EA task), and (v) the relation between each event and the document creation time (DR task). We did not participate in the narrative container relation task (CR). Introduction In this paper, we present the methods we used while participating in the 2016 Clinical TempEval task as part of the SemEval-2016 challenge. A few recent NLP challenges focus"
S16-1190,J92-4003,0,0.189152,"Missing"
S16-1190,P10-1052,0,0.0513262,"Missing"
S16-1190,S10-1071,0,0.0624188,"Missing"
W08-1204,2005.iwslt-1.1,0,0.014556,"nder the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 See http://deft.limsi.fr/ for a presentation in French. 2 http://trec.nist.gov http://www.clef-campaign.org 4 http://www.technolangue.net 3 17 Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 17–23 Manchester, August 2008 ISLE6 projects. For cost-efficiency reasons, automatic evaluation is relevant, and its results have sometimes been compared to results from human judges. For instance, Eck and Hori (2005) compare results of evaluation measurements used in automatic translation with human judgments on the same corpus. In (Burstein and Wolska, 2003), the authors describe an experiment in the evaluation of writing style and find a better agreement between the automatic evaluation system and one human judge, than between two human judges. Returning to the DEFT campaign, once the task is chosen, the corpora are collected, and evaluation measurements are defined, there can remain some necessity of adjusting parameters, according to the expected difficulty of the task. This could be, for instance, th"
W08-1204,W01-1614,0,0.032139,"ree judges who took part in the test, the first and third one agree well with themselves, while for the second one, the agreement is only moderate. • 5 values for film and book reviews8 (a mark between 0 and 4, from bad to excellent) ; • 20 values for video game reviews9 (a global mark calculated from a set of advices about various aspects of the game: graphics, playability, life span, sound track and scenario). In order to, first, assess the feasibility of the task, and to, secondly, define the scale of values to be used in the evaluation campaign, we submitted human judges to several tests (Paek, 2001): they were instructed to assign a mark on two kinds of scale, a wide one with the original values, and a restricted one with 2 or 3 values, depending on the corpus it was applying to. The results from various judges were evaluated in terms of precision and recall, and matched to each other by way of the Kappa coefficient (Carletta, 1996) (Cohen, 1960). We present hereunder the values of the κ coefficient between pairs of human judges, and with the reference, on the video game corpus. The wide scale (Table 1) uses the original values (marks between 0 and 20), while the restricted scale (Table"
W08-1204,J96-2004,0,0.241768,"pects of the game: graphics, playability, life span, sound track and scenario). In order to, first, assess the feasibility of the task, and to, secondly, define the scale of values to be used in the evaluation campaign, we submitted human judges to several tests (Paek, 2001): they were instructed to assign a mark on two kinds of scale, a wide one with the original values, and a restricted one with 2 or 3 values, depending on the corpus it was applying to. The results from various judges were evaluated in terms of precision and recall, and matched to each other by way of the Kappa coefficient (Carletta, 1996) (Cohen, 1960). We present hereunder the values of the κ coefficient between pairs of human judges, and with the reference, on the video game corpus. The wide scale (Table 1) uses the original values (marks between 0 and 20), while the restricted scale (Table 2) relies upon 3 values with following definitions: class 0 for original marks between 0 and 10, class 1 for marks between 11 and 14, and class 2 for marks between 15 and 20. Judge 1 2 3 1 0.74 2 3 0.46 0.70 Table 3: Video game corpus: agreement of each judge with himself when scales change. 7 http://www.assemblee-nationale.fr/12/ debats/"
W08-1204,W01-0902,0,\N,Missing
W11-0411,W10-1804,0,0.10543,"ation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact database through the extraction of named entities from texts, we defined a richer taxonomy than those used in other information extraction works. 93 Following (Bonneau-Maynard et al., 2005; Alex et al., 2010), the annotation guidelines were first written from December 2009 to May 2010 by three researchers managing the manual annotation campaign. During guidelines production, we evaluated the feasibility of this specific annotation task and the usefulness of the guidelines by annotating a small part of the target corpus. Then, these guidelines were delivered to the annotators. They consist of a description of the objects to annotate, general annotation rules and principles, and more than 250 prototypical and real examples extracted from the corpus (Rosset et al., 2010). Rules are important to set t"
W11-0411,J08-4004,0,0.16077,"rics Because human annotation is an interpretation process (Leech, 1997), there is no “truth” to rely on. It is therefore impossible to really evaluate the validity of an annotation. All we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (IAA). The best way to compute it is to use one of the Kappa family coefficients, namely Cohen’s Kappa (Cohen, 1960) or Scott’s Pi (Scott, 1955), also known as Carletta’s Kappa (Carletta, 1996),11 as they take chance into account (Artstein and Poesio, 2008). However, these coefficients imply a comparison with a “random baseline” to establish whether the correlation between annotations is statistically significant. This baseline depends on the number of “markables”, i.e. all the units that could be annotated. In the case of named entities, as in many others, this “random baseline” is known to be difficult—if not impossible—to identify (Alex et al., 2010). We wish to analyze this in more detail, to see how we could actually compute these coefficients and what information it would give us about the annotation. Markables Annotators Both institutes F"
W11-0411,bick-2004-named,0,0.312622,"undheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard et al., 2001). Numeric types are also often described and used. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as"
W11-0411,J96-2004,0,0.23313,"inally, we merged the results with the anno97 5.2 Metrics Because human annotation is an interpretation process (Leech, 1997), there is no “truth” to rely on. It is therefore impossible to really evaluate the validity of an annotation. All we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (IAA). The best way to compute it is to use one of the Kappa family coefficients, namely Cohen’s Kappa (Cohen, 1960) or Scott’s Pi (Scott, 1955), also known as Carletta’s Kappa (Carletta, 1996),11 as they take chance into account (Artstein and Poesio, 2008). However, these coefficients imply a comparison with a “random baseline” to establish whether the correlation between annotations is statistically significant. This baseline depends on the number of “markables”, i.e. all the units that could be annotated. In the case of named entities, as in many others, this “random baseline” is known to be difficult—if not impossible—to identify (Alex et al., 2010). We wish to analyze this in more detail, to see how we could actually compute these coefficients and what information it would give"
W11-0411,W09-3002,0,0.0410686,"we presented an extension of the traditional named entity categories to new types (functions, civilizations) and new coverage (expressions built over a substantive). We created guidelines that were used by graduate annotators to annotate a broadcast news corpus. The organizers also annotated a small part of the corpus to build a mini reference corpus. We evaluated the human annotations with our mini-reference corpus: the actual computed κ is between 0.71 et 0.85 which, given the complexity of the task, seems to indicate a good annotation quality. Our results are consistent with other studies (Dandapat et al., 2009) in demonstrating that human annotators’ training is a key asset to produce quality annotations. 99 We also saw that guidelines are never fixed, but evolve all along the annotation process due to feedback between annotators and organizers; the relationship between guidelines producers and human annotators evolved from “parent” to “peer” (Akrich and Boullier, 1991). This evolution was observed during the annotation development, beyond our expectations. These data have been used for the 2011 Quaero Named Entity evaluation campaign. Extensions and revisions are planned. Our first goal is to add a"
W11-0411,desmet-hoste-2010-towards,0,0.0216477,"rent classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain better agreement. Desmet and Hoste (2010) described the Named Entity annotation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact database through the extraction of named entities from texts, we defined a richer taxonomy than those used in other information extraction works. 93 Followin"
W11-0411,doddington-etal-2004-automatic,0,0.157033,", some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain better agreement. Desmet and Hoste (2010) described the Named Entity annotation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact databa"
W11-0411,C02-1130,0,0.0613942,"cations and organizations), we decided to extend the coverage of our campaign to new types of entities and to broaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific do"
W11-0411,W09-3025,1,0.778972,"ed. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as gene, protein, are also handled (Ohta, 2002), and campaigns are organized for gene detection (Yeh et al., 2005). At the same time, extensions of named entities have been proposed: (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. 2.2 Named Entities and Annotation As for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain bette"
W11-0411,C96-1079,0,0.902879,"luation campaign on named entity extraction aiming at building a fact database in the news domain, the first step being to define what kind of entities are needed. This campaign focused on broadcast news corpora in French. While traditional named entities include three major classes (persons, locations and organizations), we decided to extend the coverage of our campaign to new types of entities and to broaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; G"
W11-0411,I05-1058,0,0.0948165,"roaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard"
W11-0411,sekine-nobata-2004-definition,0,0.926265,"entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard et al., 2001). Numeric types are also often described and used. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as gene, protein, are also handled (Ohta, 2002), and campaigns are organized for gene detection (Yeh et al., 2005). At the same time, extensions of named entities have been proposed: (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. 2.2 Named Entities and Annotation As for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be a"
W11-0411,vilnat-etal-2010-passage,0,\N,Missing
W11-2840,C08-1022,0,0.0257963,"Missing"
W11-2840,grouin-2008-certification,1,0.881677,"Missing"
W11-2840,P11-1093,0,0.0245815,"Missing"
W11-2840,W11-2838,0,\N,Missing
W11-2840,W10-4236,0,\N,Missing
W12-1101,S10-1004,0,0.173659,"Missing"
W12-1101,W12-1102,0,0.0186244,"Missing"
W12-3606,bick-2004-named,0,0.0325063,"(CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the definition of named entity follows the classical definition. Nevertheless, in some cases, new categories were added. For example, the Virginia Banks project (Crane and Jones, 20"
W12-3606,doddington-etal-2004-automatic,0,0.0405426,"self (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location"
W12-3606,C02-1130,0,0.0265156,"et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the de"
W12-3606,galibert-etal-2010-named,1,0.859086,"can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub"
W12-3606,I11-1058,1,0.836346,"tities on spoken data and the resulting corpus. The training part of the corpus is only composed of broadcast news data while the test corpus is composed of both broadcast news and broadcast conversations data. In order to build a minireference corpus for this annotation campaign (a “gold” corpus), we randomly extracted a sub-corpus from the training one. This sub-corpus was annotated by 6 different annotators following a 4-step procedure. Table 1 gives statistics about training, test and gold corpora. These corpora (“BN” in the remainder of the paper) has been used in an evaluation campaign (Galibert et al., 2011). PP PP Data Training PP Inf. PP P # shows 188 # lines 43,289 # tokens 1,291,225 # entity types 113,885 # distinct types 41 # components 146,405 # distinct comp. 29 Test Gold 18 5,637 108,010 5,523 32 8,902 22 398 11,532 1,161 29 1,778 22 Table 1: Statistics on the annotated BN corpora Structured Named Entities in Old Newspapers We performed the same annotations on a corpus composed of OCRed press archives, henceforth the Old Press (OP) corpus. Human annotation was subcontracted to the same team of annotators as for the BN corpus, thus facilitating the consistency of annotations across corpora"
W12-3606,galibert-etal-2012-extended,1,0.588515,"née e t a p p a r e i l l e r a e n s u i t e nour &lt; loc.adm.town &gt; Toulon. &lt; / loc.adm.town &gt; Figure 4: Example annotated text block Component noisy-entities. When a character recognition error involves an entity boundary, a segmentation error occurs, either between an entity and other tokens, or between several entities and possibly other tokens. To allow the annotators to annotate the entity in that character span, we defined a new component noisy-entities which indicates that an entity is present in the noisy span of characters. A complete description of these adaptations can be found in (Galibert et al., 2012). 3.3 Global corpus extraction Unannotated sub-corpus Global annotated corpus Scientist 1 Scientist 2 Scientist 4 Scientist 3 Adjudication extraction Inter-Annotator Agreement To evaluate the manual annotations of the annotation team (“Global annotated corpus” in Figure 5), we built a mini reference corpus by selecting 255 blocks from the training corpus. We followed the same procedure as the one used for the BN corpus, as illustrated in Figure 5: Adjudication Institute 1 Institute 2 Adjudication Annotated sub-corpus IAA Institutes Adjudication IAA Mini-reference 1. The corpus is annotated ind"
W12-3606,C96-1079,0,0.645168,"he point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of"
W12-3606,W11-0411,1,0.930226,", as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and"
W12-3606,grover-etal-2008-named,0,0.179562,"corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since"
W12-3606,A00-1044,0,0.0706869,"ora: the pre-existing broadcast news corpus and this new corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described"
W12-3606,sekine-nobata-2004-definition,0,0.07436,"992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the definition of named entity follows the classical definition. Nevertheless, in some cases, new categories were added. For example, the Virginia Banks project (Crane and Jones, 2006) added categories"
W13-2022,J92-4003,0,0.0481205,"Missing"
W13-2022,W11-1801,0,0.0982212,"Missing"
W13-2022,P10-1052,0,0.16587,"Missing"
W13-2022,W11-1814,0,0.129041,"calization” relation is the link between a bacterium and the place where it lives while the “part-of” relation is the relation between hosts and host parts (bacteria) (Bossy et al., 2012a). Three teams participated in this task. All systems followed the same process: in a first stage, they detected bacteria names, detected and typed locations; then, they used co-reference to link the extracted entities; the last stage focused on the event extraction. Bj¨orne et al. (2012) adapted an SVM-based Named Entity Recognition system and used the list of Prokaryotic Names with Standing in Nomenclature. Nguyen and Tsuruoka (2011) used a CRF-based system and used the NCBI web page about the genomic BLAST. Ratkovic et al. (2012) designed an ad hoc rule-based system based on the NCBI Taxonomy. The participants obtained poor results (Table 1) which underlines the complexity of this task. • Sub-task 3 aims at extracting all bacteria and biotopes names (including both habitat and geographical names), and then identifying relations between these concepts. In this paper, we present the methods we designed as first time participant to the BioNLP Bacteria Biotopes Shared Task. 144 Proceedings of the BioNLP Shared Task 2013 Work"
W13-2022,J96-1002,0,0.0378633,"o process, we used two distinct formalisms implemented in the Wapiti tool (Lavergne et al., 2010) to build our models: • Conditional Random Fields (CRF) (Lafferty et al., 2001; Sutton and McCallum, 2006) to identify bacteria and biotopes names (subtasks 1 and 3). tax id name txt name class 346 Xanthomonas citri (ex authority Hasse 1915) Gabriel et al. 1989 scientific 346 Xanthomonas citri name 346 Xanthomonas axonopodis synonym pv. citri 346 Xanthomonas campestris synonym (pv. citri) 346 Xanthomonas campestris synonym pv. Citri (A group) • Maximum Entropy (MaxEnt) (Guiasu and Shenitzer, 1985; Berger et al., 1996) to process the relationships between entities (subtasks 2 and 3). 4.2.2 Bacteria biotopes features set We used several sets of features, including “classical” internal features (columns #4 to #7 in Table 4: typographic, digit, punctuation, length) and a few semantic features. In table 4, we present a sample tabular file produced in order to train the CRF model. Table 3: Bacteria names from the NCBI taxonomy • Presence of the token in the NCBI taxonomy (column #9); 4.1.3 The Cocoa annotations Cocoa is a WebAPI annotator tool for biological text.4 We used the Cocoa annotations provided by the o"
W13-2022,W00-0726,0,0.0568132,".46 to 0.49. Our system achieved high precision (0.62) but low recall (0.35). It produced two false positives and 144 false negatives. Out of 283 predicted habitats, 175.34 are correct. There was also a high number of substitutions (187.66). In this subsection, we present the results we achieved on the development corpus (Table 2) to identify bacteria and biotopes names without linking those names to the concept in the OntoBiotope ontology. We built the model on the training corpus and applied it on the development corpus. The evaluation has been done using the conlleval.pl script6 (Tjong Kim Sang and Buchholz, 2000) that has been created to evaluate the results in the CoNLL-2000 Shared Task. We chose this script because it takes as input a tabular file which is commonly used in the machine-learning process. Nevertheless, the script does not take into account the offsets to evaluate the annotations, which is the official way to evaluate the results. We give in Table 7 the results we achieved. Those results show our system succeed to correctly identify the bacteria and biotopes names. Nevertheless, the biotopes names are more difficult to process than the bacteria names. Similarly, Kolluru et al. (2011) ac"
W13-2321,J93-2004,0,0.0481147,"n all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome. 1 • can a system trained on data from one specific domain be useful on data from another domain in a pre-annotation task? Introduction • does this pre-annotation help human annotators or bias them? Human corpus annotation is a difficult, timeconsuming, and hence costly process. This motivates research into methods which reduce this cost (Leech, 1997). One such method consists of automatically pre-annotating the corpus (Marcus et al., 1993; Dandapat et al., 2009) using an existing system, e.g., a POS tagger, syntactic parser, named entity recognizer, according to the task for which the annotations aim to provide a gold standard. The pre-annotations are then corrected by the human annotators. The underlying hypothesis is that this should reduce annotation time while possibly at the same time increasing annotation completeness and consistency. We study here corpus pre-annotation in a specific setting, out-of-domain named entity annotation, in which we examine specific questions that we present below. We produced corpora and annot"
W13-2321,W09-3002,0,0.126125,"speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome. 1 • can a system trained on data from one specific domain be useful on data from another domain in a pre-annotation task? Introduction • does this pre-annotation help human annotators or bias them? Human corpus annotation is a difficult, timeconsuming, and hence costly process. This motivates research into methods which reduce this cost (Leech, 1997). One such method consists of automatically pre-annotating the corpus (Marcus et al., 1993; Dandapat et al., 2009) using an existing system, e.g., a POS tagger, syntactic parser, named entity recognizer, according to the task for which the annotations aim to provide a gold standard. The pre-annotations are then corrected by the human annotators. The underlying hypothesis is that this should reduce annotation time while possibly at the same time increasing annotation completeness and consistency. We study here corpus pre-annotation in a specific setting, out-of-domain named entity annotation, in which we examine specific questions that we present below. We produced corpora and annotation guidelines for nam"
W13-2321,I11-1142,1,0.822824,"Leixa , Olivier Galibertγ , Pierre Zweigenbaumα . α LIMSI–CNRS β Universit´e Paris-Sud γ LNE δ LPP, Universit´e Sorbonne Nouvelle  ELDA {rosset,grouin,lavergne,ben-jannet,pz}@limsi.fr leixa@elda.org, olivier.galibert@lne.fr 2011),1 and which we used in contrastive studies of news texts in French (Rosset et al., 2012). We want to rely on the same named entity definitions for studies on two types of data we did not cover: parliament debates (Europarl corpus) and regional, contemporary written news (L’Est R´epublicain), both in French. To help the annotation process we could reuse our system (Dinarelli and Rosset, 2011), but needed first to examine whether a system trained on one type of text (our first Broadcast News data) could be used to produce a useful pre-annotation for different types of text (our two corpora). We therefore set up the present study in which we aim to answer the following questions linked to this point and to related annotation issues: Abstract Automatic pre-annotation is often used to improve human annotation speed and accuracy. We address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneficial in this setting. Our study design incl"
W13-2321,W10-1807,0,0.523869,"want to answer these questions taking into account these two levels. We first examine related work on pre-annotation (Section 2), then present our corpora and annotation task (Section 3). We describe and discuss experiments in Section 4, and make subjective and 1 Corpora, guidelines and tools are available through ELRA under references ELRA-S0349 and ELRA-W0073. 168 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168–177, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics the pre-annotation (Rehbein et al., 2009; Fort and Sagot, 2010; South et al., 2011). In their framesemantic argument structure annotation, Rehbein et al. (2009) addressed a specific question considering a two-level annotation scheme: is the preannotation of frame assignment (low-level annotation) useful for annotating semantic roles (highlevel annotation)? Although for the low-level annotation task they observed a significant difference in quality of final annotation, for the high-level task they found no difference. Most of these studies used a pre-annotation system trained on the same kind of data as those which were to be annotated manually. Neverthel"
W13-2321,C12-1055,1,0.796018,"ould you say that one has been easier to annotate than the other? The Europarl corpus is more difficult to annotate in the sense that the existing types and components do not always match the realities found in the corpus, either because their definitions 4. Concerning the annotation manual, are there topics that you would like to change, or correct? In the same way, which named entities caused you the most difficulties to deal with? All 8 annotators answered these questions. We summarize below what we found in their answers. 2 This feeling is supported by results about ambiguity presented in Fort et al. (2012). 172 there has been a difference between novice and expert annotators. Both groups agreed on the same difficulties, pointed at the same errors, and criticized the same entities, saying that their definitions needed to be clarified. cannot apply exactly, or because the required types and components are missing (mainly for frequencies: “five times per year”). The other half of the annotators did not feel any specific difficulties in annotating one corpus or the other. According to them, both corpora are the same in terms of register and sentence structure. 6 In this section we provide results o"
W13-2321,W09-3003,0,0.0351183,"es and components), we want to answer these questions taking into account these two levels. We first examine related work on pre-annotation (Section 2), then present our corpora and annotation task (Section 3). We describe and discuss experiments in Section 4, and make subjective and 1 Corpora, guidelines and tools are available through ELRA under references ELRA-S0349 and ELRA-W0073. 168 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168–177, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics the pre-annotation (Rehbein et al., 2009; Fort and Sagot, 2010; South et al., 2011). In their framesemantic argument structure annotation, Rehbein et al. (2009) addressed a specific question considering a two-level annotation scheme: is the preannotation of frame assignment (low-level annotation) useful for annotating semantic roles (highlevel annotation)? Although for the low-level annotation task they observed a significant difference in quality of final annotation, for the high-level task they found no difference. Most of these studies used a pre-annotation system trained on the same kind of data as those which were to be annotat"
W13-2321,I11-1058,1,0.890374,"Missing"
W13-2321,W07-1516,0,0.0591516,"Missing"
W13-2321,W11-0411,1,0.821738,"eement and that full pre-annotation yields the best result. We observe that, as expected, pre-annotation leads human annotators to obtain higher consistency. Table 2: F-measure and Slot Error Rate achieved by the automatic system on each kind of annotation and on in-domain broadcast data We also computed inter-annotator agreement (IAA) for each corpus considering two groups of annotators, experts and novices. We consider that the inter-annotator agreement is somewhere between the F-measure and the standard IAA considering as markables all the units annotated by at least one of the annotators (Grouin et al., 2011). We computed Scott’s Pi (Scott, 1955), and Cohen’s Kappa (Cohen, 1960). The former considers 5 Subjective assessment An important piece of information in any annotation campaign is the feelings of the annotators about the task. This can give interesting clues about the expected quality of their work and on the usefulness of the pre-annotation step. We asked the annotators a few questions concerning several features of this project, such as the annotation 171 1 5.1.1 Press: Cohen&apos;s kappa Press: F-measure Europarl: Cohen&apos;s kappa Europarl: F-measure 0.9 Most of the annotators preferred the corpo"
W13-2321,W12-3606,1,0.874172,"Missing"
W14-4907,J08-4004,0,0.11081,"tion quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotator modeling. Overall, past work shows that creating high-quality manual annotations is time-consuming and often requires the work of experts. The time burden is distributed between the sheer creation of the annotations, the act of producing multiple annotations for the same data and the subsequent analysis of multiple annotations to resolve conflicts, viz. the creation o"
W14-4907,J92-4003,0,0.100316,"Missing"
W14-4907,W11-0408,0,0.0224984,"s quality and affordable cost. Inter-annotator agreement (IAA) has been used as an indicator of annotation quality. Early work showed that the use of automatic preannotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006). Efforts have investigated methods to reduce the human workload while annotating corpora. In particular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and"
W14-4907,W10-1807,0,0.0164962,"r, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotat"
W14-4907,P10-1052,1,0.893732,"Missing"
W14-4907,J93-2004,0,0.0462052,"tasks such as part-of-speech tagging or named entity recognition by relying on large annotated text corpora. As a result, developping highquality annotated corpora representing natural language phenomena that can be processed by statistical tools has become a major challenge for the scientific community. Several aspects of the annotation task have been studied in order to ensure corpus quality and affordable cost. Inter-annotator agreement (IAA) has been used as an indicator of annotation quality. Early work showed that the use of automatic preannotation tools improved annotation consistency (Marcus et al., 1993). Careful and detailed annotation guideline definition was also shown to have positive impact on IAA (Wilbur et al., 2006). Efforts have investigated methods to reduce the human workload while annotating corpora. In particular, active learning (Settles et al., 2008) sucessfully selects portions of corpora that yield the most benefit when annotated. Alternatively, (Dligach and Palmer, 2011) investigated the need for double annotation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a c"
W14-4907,W13-2321,1,0.84682,"otation and found that double annotation could be limited to carefully selected portions of a corpus. They produced an algorithm that automatically selects portions of a corpus for double annotation. Their approach allowed to reduce the amount of work by limiting the portion of doubly annotated data and maintained annotation quality to the standard of a fully doubly annotated corpus. The use of automatic pre-annotations was shown to increase annotation consistency and result in producing quality annotation with a time gain over annotating raw data (Fort and Sagot, 2010; N´ev´eol et al., 2011; Rosset et al., 2013). With the increasing use of crowdsourcing for obtaining annotated data, (Fort et al., 2011) show that there are ethic aspects to consider in addition to technical and monetary cost when using a microworking platform for annotation. While selecting the adequate methods for computing IAA is important (Artstein and Poesio, 2008) for interpreting the IAA for a particular task, annotator disagreement is inherent to all annotation tasks. To address this situation (Rzhetsky et al., 2009) designed a method to estimate annotation confidence based on annotator modeling. Overall, past work shows that cr"
W14-4907,J11-2010,0,\N,Missing
W14-6301,W14-6306,0,0.0610287,"Missing"
W14-6301,W14-6304,0,0.0536407,"Missing"
W14-6301,W14-6308,0,0.035181,"Missing"
W15-2604,J08-4004,0,\N,Missing
W16-3008,W13-2025,0,0.0650173,"Missing"
W16-3008,W13-2021,0,0.0274355,"Missing"
W16-3008,P10-1052,0,0.100002,"Missing"
W16-3008,J92-4003,0,0.651514,"Missing"
W16-3008,W13-2027,0,0.196667,"he entity detection and categorization task, the best results were obtained using either machine-learning approaches, as done by Bannour et al. (2013) who ranked first (Slot Error Rate (SER) of 0.661), or using syntactic hand-coded ¨ ur (2013) rules, as done by Karadeniz and Ozg¨ who ranked second (SER=0.676). We ranked third (SER=0.678) using CRF and normalization rules. On the localization relation extraction task, the best results were obtained through machinelearning approaches. Bj¨orne and Salakoski (2013) ranked first (F=0.42), using a system based on Support Vector Machine (SVM), while Claveau (2013) ranked second (F=0.40) using a lazy machine learning (kNN) approach. This paper presents our participation in the Bacteria/Biotope track from the 2016 BioNLP Shared-Task. Our methods rely on a combination of distinct machinelearning and rule-based systems. We used CRF and post-processing rules to identify mentions of bacteria and biotopes, a rulebased approach to normalize the concepts in the ontology and the taxonomy, and SVM to identify relations between bacteria and biotopes. On the test datasets, we achieved similar results to those obtained on the development datasets: on the categorizat"
W16-3008,deleger-etal-2014-annotation,1,0.887973,"Missing"
W16-3008,W16-3002,0,0.0975588,"Missing"
W16-3008,W13-2022,1,0.841512,"ion task (event), and a knowledge-base population task (kb) which combines categorization and relation identification. Additionally, each task is composed of a named entity recognition sub-task: categorization and relation identification are based on predictions of entities (cat+ner, event+ner, and kb+ner tasks) instead of gold standard annotations. Introduction In this paper, we present the methods we used while participating in the Bacteria/Biotope track from the 2016 BioNLP Shared-Task. We partially reused the method we designed while participating in the previous edition of the challenge (Grouin, 2013), and we updated afterwards while designing new experiments (Lavergne et al., 2015). 2 Task description Background Four teams participated in the Bacteria/Biotope track (Bossy et al., 2015) from the 2013 BioNLP Shared-Task. 1 http://2016.bionlp-st.org/ 64 Proceedings of the 4th BioNLP Shared Task Workshop, pages 64–72, c Berlin, Germany, August 13, 2016. 2016 Association for Computational Linguistics 3.2 3.2.1 Material Mentions of bacteria are normalized into only one category while mentions of habitat can be normalized into several categories. The categorization into one or several categories"
W16-5107,N13-1073,0,0.0169323,"), where the suicide must be coded by taking into account the specific circumstance that lead to it (here, strangulation). In such cases we kept the input statements separate. The most generic statement (e.g. suicide) was considered inconclusive and did not receive a code assignment while the ‘head’ statement (e.g. ligature strangulation, which provided the defining information for code assignment) was aligned with the output code. To align the statements, we used a model originally intended for bilingual word alignment in parallel sentences: a log-linear reparameterization of the IBM2 model (Dyer et al., 2013). The alignments were produced from the computed clauses without allowing for null alignment in order to satisfy our constraints, and with a Dirichlet prior to favor diagonal alignments. The model underperforms on multi-word segments as it relies on co-occurrence counts of raw and computed causes, which are very sparse. To overcome this problem, both causes were pre-processed by removing stopwords and applying stemming. Next, the Damerau-Levenshtein distance between two segments was linearly combined with the occurrence count to act as a prior on the alignment probabilities. 4 Results We appli"
W16-5107,W11-1801,0,0.0175283,". It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task. 1 Introduction Over the past decade, biomedical named entity recognition (NER) and concept normalization have been widely covered in NLP challenges. Different types of texts were explored: clinical texts were used in the CMC (Pestian et al., 2007) and the i2b2 NLP Challenges (Uzuner et al., 2007; Uzuner et al., 2011) while the biomedical literature provided material for the BioNLP-Shared Tasks (Kim et al., 2011; Nédellec et al., 2015). Few challenges offered datasets in more than one languages, such as the CLEF ER (RebholzSchuhmann et al., 2013) and CLEF eHealth Challenges (Goeuriot et al., 2015) The assignment of codes from the International Classification of Diseases (ICD) to clinical texts is primarily used for billing purposes but also has a wide range of applications including epidemiological studies (Woodfield et al., 2015), monitoring disease activity (Koopman et al., 2015a), or predicting cancer incidence through retrospective and prospective studies (Bedford et al., 2014). Nevertheless, use"
W16-5107,W07-1013,0,0.573981,"ed the coder’s decision for each code. The dataset comprises 93,694 death certificates totalling 276,103 statements and 377,677 ICD-10 code assignments (3,457 unique codes). It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task. 1 Introduction Over the past decade, biomedical named entity recognition (NER) and concept normalization have been widely covered in NLP challenges. Different types of texts were explored: clinical texts were used in the CMC (Pestian et al., 2007) and the i2b2 NLP Challenges (Uzuner et al., 2007; Uzuner et al., 2011) while the biomedical literature provided material for the BioNLP-Shared Tasks (Kim et al., 2011; Nédellec et al., 2015). Few challenges offered datasets in more than one languages, such as the CLEF ER (RebholzSchuhmann et al., 2013) and CLEF eHealth Challenges (Goeuriot et al., 2015) The assignment of codes from the International Classification of Diseases (ICD) to clinical texts is primarily used for billing purposes but also has a wide range of applications including epidemiological studies (Woodfield et al., 2015), moni"
W16-5109,W06-1632,0,0.0262479,"ly a small piece of preprocessing in a larger natural language processing pipeline whose adaptation to a given clinical task generally already requires some human annotation effort, a supervised method requiring more human annotation is not desirable. We therefore endeavored to investigate methods which require no human annotation to perform this task. 2 Related work The problem of &lt;EOL> classification seems to be little explored in natural language processing (NLP), and the section that Smith (2011) dedicates to segmentation does not mention it. Some NLP research (Sporleder and Lapata, 2006; Filippova and Strube, 2006) has addressed paragraph segmentation from a quite different perspective: given a text split into sentences, determine paragraph boundaries. However, they started from texts where sentence boundaries were given, and the input texts were assumed to be “clean” from the point of view of &lt;EOL> marks (i.e., either sentence boundaries are deterministically marked by &lt;EOL>s or by XML markup). A few papers on clinical NLP have recently addressed it and proposed methods based upon heuristics and knowledge about the usual format of the texts (Zweigenbaum and Grouin, 2014) or supervised machine learning"
W16-5109,tepper-etal-2012-statistical,0,0.0278098,"of MA or MAB to an automatically detected +wrap subset. A most interesting perspective is the study of the interaction of &lt;EOL> classification with sentence segmentation. On the one hand, as suggested by one of the reviewers, sentence segmentation might be used as a baseline for &lt;EOL> classification, all the more in texts where paragraphs typically end with a period. On the other hand, the study of the impact of &lt;EOL> classification on sentence segmentation is one of the motivations for the present work, and constitutes our next step. As suggested by another reviewer, section title detection (Tepper et al., 2012) can also help paragraph segmentation. As a matter of fact, it was part of the heuristics used in (Zweigenbaum and Grouin, 2014), where it helped to avoid pasting a title (possibly with no final period) to the next line. 6 Conclusion We presented a method which uses self-training and co-training to classify &lt;EOL>s with no human annotation, based on available token and line length features. It achieves high &lt;EOL> classification Fmeasures on i2b2 clinical texts which incur paragraph folding, and can also detect texts which are not subject to this phenomenon. In future work, we plan to test MB as"
W16-5109,D14-1187,0,0.0139509,"lows them to re-train the classifier then to iterate until convergence, according to the expectation-maximization algorithm (EM). The method we propose below to train an &lt;EOL> classifier is related to this principle, but does not need an initial human annotation. Elkan and Noto (2008) propose a non-iterative method for this purpose, but it assumes that the annotated examples are drawn randomly from the positive examples, which is not the case in our situation. Yet another path would consist in considering the &lt;EOL> annotations as ambiguous (both &lt;SP> and &lt;TUB>) and in applying the methods of (Wisniewski et al., 2014). However, this would create a systematic dependency between these two classes in these annotations, a situation in which learning is not guaranteed (Bordes et al., 2010). 3 3.1 Material and methods Corpora We target here clinical texts with a complex mixture of formats. However, we also test our methods on more controlled corpora which we have in several formats. The controlled-format corpora are made of six plain text e-books by Jules Verne in four languages from the Gutenberg project (http://www. gutenberg.net), which we split into chapters. Each of their paragraphs is split into multiple l"
W16-5112,S12-1008,0,0.0184955,"ssing will occur for that document pair and the information is effectively lost for the information extraction process. In this paper we present and evaluate the text reuse detection tool in isolation and discuss its strengths and weaknesses. 2 Background A traditional approach for the detection of verbatim copying1 is to compute the similarity between the source and target text as the proportion of substring sequences that the two texts have in common. These substring sequences can either be defined as character n-grams (Cohen et al., 2013), words (Wrenn et al., 2010), or word n-grams (Adeel Nawab et al., 2012). These methods are mainly based on fingerprinting and hashing techniques, i.e. the documents are represented as sets of unique digital signatures, and are highly precise but are not robust to much surface variation. Some methods, however, are adapted to deal with insertions and deletion of words or characters. For example, as an extension of the ‘longest common substring’ algorithm (Gusfield, 1997), which calculated text similarity as the length of the longest continuous sequence of characters normalized by the sum of the document lengths, Wise et al. (1996) developed the ‘Greedy String Tilin"
W16-5112,P02-1020,0,0.0151576,"f subsequent text mining processes, such as encoding errors, missing files, OCR errors, etc. One interesting issue in cumulatively constructed text corpora is the problem of ‘text reuse’. Text reuse is defined here as the intentional or unintentional reusing of existing text (fragments) to create a new text, for example, by copy-pasting text fragments from one document to fit into a new document; or by adapting a report and saving both the old and the new version as separate documents. Text reuse is a complex phenomenon which has been studied in multiple settings such as newspaper journalism (Clough et al., 2002), programming code (Ohno and Murao, 2009), the analysis of text reuse in blogs and web pages (Abdel Hamid et al., 2009), etc. It is quite prevalent in the medical domain (Wrenn et al., 2010) and often seen as a negative factor: Cohen et al. (2013) found that copy-pasting practices in US hospitals have a significant negative impact on the accuracy of the subsequent text mining systems on the clinical notes. However, when text reuse is considered as a diachronic phenomenon, it has some interesting aspects. By identifying which text (fragments) have been reused we can follow the flow of informati"
W16-5112,W15-2603,1,0.887071,"Missing"
W16-6108,W11-4114,0,0.160936,"Missing"
W16-6110,P13-1166,0,0.22125,"Missing"
W19-5029,J92-4003,0,0.506649,"Missing"
W19-5029,P10-1052,0,0.120129,"Missing"
W19-5029,L18-1201,0,0.0614524,"Missing"
W19-5029,W18-5614,1,0.510736,"ized domains (e.g., clinical notes or justice decisions) are not easily accessible unless authorization (Chapman et al., 2011). 273 Proceedings of the BioNLP 2019 workshop, pages 273–282 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 2.1 Corpus and annotation guidelines work, we only focus on the clinical case description. This set has been manually annotated with general and fine-grained information, which is described in the two following sections. This corpus is part of a larger and yet growing corpus, which currently contains over 4,100 clinical cases (Grabar et al., 2018). Corpus In the clinical domain, in order to overcome the privacy and ethical issues when working on electronic health records, one solution consists in using clinical case reports. Indeed, it is quite common to find freely available publications from scientific journals which report in clinical cases of real de-identified or fake patients. Such clinical cases are usually published and discussed to improve medical knowledge (Atkinson, 1992) of colleagues and medical students. One may find scientific journals specifically dedicated to case reports, such as the Journal of Medical Case Reports la"
W19-5029,W11-0411,1,0.539433,"Recall and Fmeasure values (Sebastiani, 2002). General information We computed interannotator agreement scores on the normalized values for general information: Age, Gender and Outcome, and on the annotated text spans for Origin. We achieved excellent agreements for Age and Gender (κ=0.939), differences being due to omissions; poor agreement for Outcome (κ=0.369) due to differences of interpretation between close values (e.g., recovery vs. improvement for long-term diseases); and very low agreement for Origin (κ=-0.762) since spans of text were often distinct between annotators. As stated by Grouin et al. (2011), the κ metric is not well suited for annotations of text since it relies on a random baseline for which the number of units that may be annotated is hard to define. As a consequence, the classical F-measure is often used as an approximation of inter-annotator agreement. In the following experiments, we present the inter-annotator agreements through Precision, Recall, and F-measure. P 0.5660 0.5714 0.7042 0.3151 0.3744 0.7500 0.4260 0.5135 0.5597 0.4328 0.5563 0.2596 0.5567 0.3077 0.5950 0.5378 0.4426 R 0.8511 0.2857 0.2747 0.8519 0.8913 0.5816 0.8267 0.2879 0.8824 0.8056 0.8778 0.6116 0.6888"
