2004.iwslt-evaluation.8,P00-1056,0,0.237361,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,J03-1002,0,0.0116263,"own that if the process succeeds in generating a triple (φl0 , τ0l , π0l ), then there is exactly one corresponding pair (f , a), and viceversa. This property justifies the following decomposition of Model 4: pθ (f , a |e) = p(φl0 , τ0l , π0l |el0 ) = p(φ, τ , π |e) p(τi |φi , ei ) k=1 a a φi ) (10) i=1 with where λi ’s represent scaling factors of factors. In eq. (5), English strings e are ranked on the basis of the weighted product of the language model probability Pr(e), usually computed through an n-gram language model [13], and the marginal of the translation probability Pr(f , a |e). In [8, 14] six translation models (Model 1 to 6) of increasing complexity are introduced. These alignment models are usually estimated through the Expectation Maximization algorithm [15], or approximations of it, by exploiting a suitable parallel corpus of translation pairs. For computational reasons, the optimal translation of f is computed with the approximated search criterion: e l Y l X i=0 exploiting eq. (3), eq. (2) can be rewritten as: e∗ p(φi |ei ) p(φ0 | i=1 Unfortunately, a closed-form solution of (4) does not exist. An iterative procedure converging to the solution was proposed by [10]; an im"
2004.iwslt-evaluation.8,P01-1067,0,0.114159,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,W02-1018,0,0.0226266,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,2003.mtsummit-papers.53,0,0.0486164,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,N03-1017,0,0.0501189,"d by the many papers on the subject, which followed its first introduction. Of course, there have been also attempts to overcome some of its shortcomings, e.g. the use of limited context within the foreign string to guess word translations and word positions. Recently, several research labs have reported improvements in translation accuracy by shifting from word- to phrase-based SMT. In particular, statistical phrasebased translation models have recently emerged, which rely on statistics of phrase pairs. Phrase pairs statistics can be automatically extracted from word-aligned parallel corpora [5]. In the following subsections, we introduce the SMT framework and the Model 4. Then, we briefly describe a method for extracting phrase pairs. Finally, a novel phrase-based translation framework is presented which is tightly related to Model 4. Focus of this paper is the system for statistical machine translation developed at ITC-irst. It has been employed in the evaluation campaign of the International Workshop on Spoken Language Translation 2004 in all the three data set conditions of the Chinese-English track. Both the statistical model underlying the system and the system architecture are"
2004.iwslt-evaluation.8,2004.iwslt-papers.2,1,0.890764,"by means of the GIZA++ toolkit [1]. Phrase pairs are then extracted taking into account both direct and inverse alignments (see section 2.3), and the phrase-based distributions are estimated (section 2.4). In the second phase the scaling factors of the log-linear model are estimated by the so-called minimum error training procedure. This iterative method searches for a set of factors that minimizes a given error measure on a development corpus. The simplex method [17] is used to explore the space of scaling factors. A detailed description of the minimum error training approach is reported in [18]. BLEU NIST MWER MPER 0.3001 0.3509 0.3466 0.3460 0.4311 0.4574 7.0157 7.5099 7.4475 7.4427 8.5336 8.7890 50.8 47.2 47.6 47.1 42.0 39.7 41.5 38.1 38.3 38.3 33.3 30.5 baseline system, provided these data are close enough to the domain of the test set. However, an exhaustive exploration of corpora available for the IWSLT evaluation for finding the best combination for training the system is unfeasible. Hence, first we searched for the best monolingual resources consisting of the English part of parallel corpora. Successively, we tried the effectiveness of additional bilingual resources. Note tha"
2004.iwslt-evaluation.8,W03-1001,0,0.125553,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,takezawa-etal-2002-toward,0,0.115361,"Missing"
2004.iwslt-evaluation.8,P01-1050,0,0.0488447,"ineseEnglish track. 2.1. Log-linear Model As originally proposed by [8], the most likely translation of a foreign source sentence f into English is obtained by searching for the sentence with the highest posterior probability: The ITC-irst SMT system implements an extension of the IBM Model 4 as a log-linear interpolation of statistical models, which estimate probabilities in terms of phrases. The use of phrases rather than words has recently emerged as a mean to cope with the limited context that Model 4 exploits to guess word translation (lexicon model) and word positions (distortion model) [1, 2, 3, 4, 5, 6, 7]. e∗ = arg max Pr(e |f ) e (1) Usually, the hidden variable a is introduced: While parameters of the models are estimated exploiting statistics of phrase pairs extracted from word alignments, the weights of the interpolation are optimized through a training procedure which directly aims at minimizing translation errors on a development set. e∗ = arg max e X Pr(e, a |f ) (2) a which represents an alignment from source to target positions. The framework of maximum entropy [9] provides a mean to directly estimate the posterior probability Pr(e, a |f ). It is determined through suitable real value"
2004.iwslt-evaluation.8,P02-1038,0,0.215643,"These alignment models are usually estimated through the Expectation Maximization algorithm [15], or approximations of it, by exploiting a suitable parallel corpus of translation pairs. For computational reasons, the optimal translation of f is computed with the approximated search criterion: e l Y l X i=0 exploiting eq. (3), eq. (2) can be rewritten as: e∗ p(φi |ei ) p(φ0 | i=1 Unfortunately, a closed-form solution of (4) does not exist. An iterative procedure converging to the solution was proposed by [10]; an improved version is given in [11]. If the following feature functions are chosen [12]: h1 (e, f , a) h2 (e, f , a) l Y 2.3. Phrase-pair Extraction The here used method exploits the so called union alignments between sentence pairs of the training corpus [5]. Given strings f = f1 , . . . , fm and e = e1 , . . . , el , a direct alignment a (from f to e) and an inverted alignment b (from e to f ), the union alignment is defined as: c = {(j, i) : aj = i ∨ bi = j} (15) It is easy to verify that while a and b are many-to-one alignments, c is a many-to-many alignment. Moreover, the union 1 A cept is a target word (including e ) with positive fertility. A not-cept 0 word may only gene"
2004.iwslt-evaluation.8,J93-2003,0,\N,Missing
2004.iwslt-evaluation.8,J96-1002,0,\N,Missing
2004.iwslt-papers.2,P03-1021,0,0.144581,"the addition of four scaling factors: 2. Log-Linear Model for SMT a arg max Pr(e, a |f ) (e,f ,a)∈T An interesting log-linear model results if the following feature functions derived from Model 4 [6] are used: Log-linear interpolation models, which can be formally derived within the maximum entropy framework [1], have been only recently applied to statistical machine translation (SMT) [2]. In addition, and similarly to what proposed for speech recognition [3], optimization of interpolation parameters can directly address translation quality, rather than the usual maximum likelihood criterion [4]. This paper goes along the direction of [4], and proposes an alternative and more direct training procedure, but computationally more intensive. Moreover, a subtle relationship between the parameter optimization and the beam search algorithm is pointed out, which might have an important impact on the choice of optimal parameters. ≈ (2) The maximum entropy criterion suggests to compute values λi , which maximize the log-likelihood over a training sample T : X λ∗ = arg max log pλ (e, a |f ) (3) 1. Introduction e P exp{ i λi hi (e, f , a)} P e,a exp{ i λi hi (e, f , a)} (1) 103 is deployed, too."
2004.iwslt-papers.2,J96-1002,0,0.0776359,"mentally ex˜ ) of the source tends partial translation hypotheses (˜ e, a string, until an optimal complete translation is found. A translation is said partial if its corresponding alignment ˜ does not cover all positions in f . The complexity of a the search algorithm mainly depends on the number of possible translations and of target positions to be considered for each source word. To avoid exponential complexity, constraints on both factors are generally introduced. Moreover, the so-called pruning of hypotheses Given a source string f and a target string e, the framework of maximum entropy [5] provides a mean to directly address the posterior probability Pr(e |f ). By introducing the hidden alignment variable a, the usual SMT optimization criterion is expressed by: X e∗ = arg max Pr(e, a |f ) e,a = log Pr(e) = log Pr(φ |e) = log Pr(τ |e, φ) = log Pr(π |e, φ, τ ), which explain f and a for e in terms of fertilities φ, tablets τ and permutations π. In fact, after simple manipulations, the usual decoding criterion for Model 4 results, with the addition of four scaling factors: 2. Log-Linear Model for SMT a arg max Pr(e, a |f ) (e,f ,a)∈T An interesting log-linear model results if the"
2004.iwslt-papers.2,J93-2003,0,0.0211432,"r probability Pr(e |f ). By introducing the hidden alignment variable a, the usual SMT optimization criterion is expressed by: X e∗ = arg max Pr(e, a |f ) e,a = log Pr(e) = log Pr(φ |e) = log Pr(τ |e, φ) = log Pr(π |e, φ, τ ), which explain f and a for e in terms of fertilities φ, tablets τ and permutations π. In fact, after simple manipulations, the usual decoding criterion for Model 4 results, with the addition of four scaling factors: 2. Log-Linear Model for SMT a arg max Pr(e, a |f ) (e,f ,a)∈T An interesting log-linear model results if the following feature functions derived from Model 4 [6] are used: Log-linear interpolation models, which can be formally derived within the maximum entropy framework [1], have been only recently applied to statistical machine translation (SMT) [2]. In addition, and similarly to what proposed for speech recognition [3], optimization of interpolation parameters can directly address translation quality, rather than the usual maximum likelihood criterion [4]. This paper goes along the direction of [4], and proposes an alternative and more direct training procedure, but computationally more intensive. Moreover, a subtle relationship between the paramet"
2004.iwslt-papers.2,N04-1033,0,0.0381245,"and abbreviations, number extraction, case normalization, etc. In the following, we will refer to the baseline system when uniform parameters are assumed, which can be possibly scaled up or down to modify the beam-search pruning. (7) λ Unlike the log-likelihood criterion (3), the objective function ED (·) might have many local minima. Hence, finding an optimal solution can be very hard. In this work, we use the simplex method [7], an algorithm for multivariate function minimization which requires relatively few function evaluations. The same algorithm was already applied for the same task in [8] and for training log-linear language models in [1]. 3.1. Interaction with Beam Search The optimization process, besides tuning the parameters of the statistical model, may also interfere with the beam search. The reason is in the following property of the scoring function (4): ˜ ; αλ) = Q(˜ ˜ ; λ) Q(˜ e, a e, a α 4.2. Data We evaluated our approach on two Chinese-English translation tasks: the Nist 2003 MT evaluation task1 , large-data case-insensitive conditions, and the C-Star 2003 evaluation campaign 2 . The first task concerns with translation of new agencies, while the second task concer"
2004.iwslt-papers.2,2004.iwslt-evaluation.8,1,0.890938,"t among the top N best scoring ones are pruned. 3. Minimum Error Training 4. Experiments In place of the criterion (3), [4] recently proposed to estimate parameters by minimizing the number of translation errors. We assume that a function ED (λ) is available, which measures the translation errors made by running a model defined by parameter values λ on a development set D. Hence, parameters are searched by: λ∗ = arg min ED (λ) 4.1. Baseline System The core of the translation system is a statistical model, based on the IBM Model 4 and extended to deal with phrases rather than with single words [9]. The corresponding log-linear model is similar to that shown in eq. (5) with the addition of two terms, which explicitly scale the fertility and distortion probabilities of the null word. Search is performed by a decoder based on dynamic programming. Both in training and testing, sentences are pre-processed in order to reduce data sparseness. Pre-processing includes: Chinese word segmentation, separation of words from punctuation, handling of acronyms and abbreviations, number extraction, case normalization, etc. In the following, we will refer to the baseline system when uniform parameters a"
2004.iwslt-papers.2,takezawa-etal-2002-toward,0,0.150824,"e models in [1]. 3.1. Interaction with Beam Search The optimization process, besides tuning the parameters of the statistical model, may also interfere with the beam search. The reason is in the following property of the scoring function (4): ˜ ; αλ) = Q(˜ ˜ ; λ) Q(˜ e, a e, a α 4.2. Data We evaluated our approach on two Chinese-English translation tasks: the Nist 2003 MT evaluation task1 , large-data case-insensitive conditions, and the C-Star 2003 evaluation campaign 2 . The first task concerns with translation of new agencies, while the second task concerns with basic traveling expressions [10]. Test sentences are provided with 4 and 16 human translations, respectively. Tables 1 and 2 report detailed statistics about the used training and test data. For parameter optimization, the Nist 2002 MT evaluation data and 1,000 sentences extracted from the C-Star training data were used, respectively. It is worth noticing that the C-Star 2003 test set has been used as development set for the IWSLT-2004 evalu(8) for any positive real number α. As a consequence, the threshold criterion (6) is affected by any change of the parameter vector λ which corresponds to a scaling transformation. For in"
2004.iwslt-papers.2,2001.mtsummit-papers.68,0,0.070638,"Missing"
2004.iwslt-papers.2,P02-1040,0,\N,Missing
2004.iwslt-papers.2,P02-1038,0,\N,Missing
2005.iwslt-1.11,2004.iwslt-evaluation.8,1,\N,Missing
2005.iwslt-1.11,J93-2003,0,\N,Missing
2005.iwslt-1.11,J96-1002,0,\N,Missing
2005.iwslt-1.11,J00-2004,0,\N,Missing
2005.iwslt-1.11,J03-1005,0,\N,Missing
2005.iwslt-1.11,takezawa-etal-2002-toward,0,\N,Missing
2005.iwslt-1.11,P00-1056,0,\N,Missing
2005.iwslt-1.11,2004.iwslt-papers.2,1,\N,Missing
2006.iwslt-evaluation.7,W06-3110,0,0.0903759,"ments. Briefly, the CLA computes an association score between all possible word pairs within the parallel corpus, and then applies a greedy algorithm to compute the best word-alignment for each sentence pair • question feature, i.e. a binary feature which triggers when text ends with a question mark and starts with one of the typical starting words of question sentences found in training data • frequency of its n-grams (n=1,2,3,4) within the N-best translations • ratio between the target and source length • 2,3,5-grams target LMs • n-gram posterior probabilities within the N-best translations [4] • sentence length posterior probabilities [4] • word/block reordering probabilities (Section 2.2) 2. System Description • ratio of the source length and the number of source phrases (Section 2.3) ITC-irst SMT system [1] implements a log-linear model and features a two-step decoding strategy. In the first pass, a dynamic programming beam search algorithm generates N-best translation hypotheses for each source sentence. In the secThe first six feature functions were used in our system in IWSLT-2005. The two posterior probabilities represent a 53 Table 1: Statistics of training, development and"
2006.iwslt-evaluation.7,2006.iwslt-papers.4,1,0.889045,". Preprocessing tokenization txt-to-digit lower-casing Chi-to-Eng Chinese English x x x x – x Jpn-to-Eng Japanese English x x – – – x refinement of the counts of n-grams in N-best lists we employed for the 2005 campaign. The last two features are new and are presented in the following. Ara-to-Eng Arabic English x x – – – x Ita-to-Eng Italian English x x x x x x hrules (˜ e, f , a) = K 1 X log Pr(ri ) K i=1 (1) where ri is a matching rule, Pr(ri ) its probability and K the number of the reordering patterns matching the given source/target pair. 2.2. Word/Block Reordering In the companion paper [5], the use of rules for modeling word reordering phenomena in phrase-based SMT is proposed. Reordering rules consist of two sides: the left-handside (lhs), which is a word-based pattern, and the right-handside (rhs), which corresponds to a possible reordering of that pattern. Different rules can share the lhs, because the same pattern can be reordered in more than one way. Rules are automatically extracted from word aligned training data and weighted according to observed statistics. Rules can reorder sequences of single words or a pair of blocks of words. A block is a sequence of source words"
2006.iwslt-evaluation.7,takezawa-etal-2002-toward,0,0.137694,"Missing"
2006.iwslt-evaluation.7,P00-1056,0,0.124776,"2 20.04 5.445 21.07 5.465 Ita-to-Eng BLEU NIST 35.24 7.779 35.59 7.859 – – – – 37.47 8.075 tuation, etc. For both the modules we use the disambig tool,1 as suggested in the instructions supplied by the evaluation organizers. The English training data have been employed to train the language models of the two modules. 3.3. Development: Baseline and Improvements The setup of baselines includes the use of phrases up to 8 words and monotonic search. We extracted phrases and estimated the phrase translation models from the intersection of direct and inverse IBM alignments, expanded as suggested in [7]. Improvements were carried out by introducing novelties in an incremental way, monitoring performance on development sets. Upgrades involve: (i) the addition of CLA wordalignments and (ii) of IBM union word-alignments [3], (iii) the execution of non-monotonic search, and (iv) the application of the rescoring module. Non-monotonic search uses constraints on word reordering defined by means of the maximum vacancy number (MVN) and maximum vacancy distance (MVD) parameters. The following setting was used for experiments: 3.1. Preprocessing The most important stage of preprocessing is tokenization"
2006.iwslt-evaluation.7,W05-0835,0,0.122059,"lations is generated for each source sentence by means of a beam-search decoder; in the second pass, N-best lists are rescored and reranked exploiting additional feature functions. Main updates brought to the 2005 system involve novel additional features which are here described. Results on development sets are analyzed and commented. 2.1. Rescoring Models The feature functions and search constraints adopted for decoding are quite standard: phrase and word translation models, 4-gram language model, fertility model, IBM reordering constraints, beam search. A detailed description is provided in [1, 2]. On the other hand, SMT systems often differ a lot in the models employed for rescoring N-best candidates. Here the list of those we apply: 1. Introduction • direct and inverse IBM model 1 and 3 lexicons, over all possible alignments In this paper, we report on the participation of ITC-irst to the evaluation campaign of the International Workshop on Spoken Language Translation 2006. We submitted runs under the Open Data conditions for all the language pairs: Arabic-to-English, Chinese-to-English, Japanese-to-English and Italian-to-English. For each language pair, we performed translations of"
2006.iwslt-evaluation.7,2005.iwslt-1.11,1,\N,Missing
2006.iwslt-papers.4,J99-4005,0,0.203932,"slation task. On other language pairs which differ a lot in the word order, the use of lexicalized rules allows to observe significant improvements as well. 1. Introduction In Machine Translation (MT), one of the main problems to handle is word reordering. Informally, a word is “reordered” when it and its translation occupy different positions within the corresponding sentences. In Statistical Machine Translation (SMT) [1], word reordering is faced from two points of view: constraints and modeling. If arbitrary word-reorderings are permitted, the exact decoding problem was shown to be NP-hard [2]; it can be made polynomial-time by introducing proper constraints, such as IBM constraints [3] and Inversion Transduction Grammars (ITG) constraints [4, 5]. It is worth to notice that both types of constraints are linguistically blind, i.e. they are unable to tune the number of allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausib"
2006.iwslt-papers.4,J97-3002,0,0.157164,"ll. 1. Introduction In Machine Translation (MT), one of the main problems to handle is word reordering. Informally, a word is “reordered” when it and its translation occupy different positions within the corresponding sentences. In Statistical Machine Translation (SMT) [1], word reordering is faced from two points of view: constraints and modeling. If arbitrary word-reorderings are permitted, the exact decoding problem was shown to be NP-hard [2]; it can be made polynomial-time by introducing proper constraints, such as IBM constraints [3] and Inversion Transduction Grammars (ITG) constraints [4, 5]. It is worth to notice that both types of constraints are linguistically blind, i.e. they are unable to tune the number of allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target"
2006.iwslt-papers.4,N04-1021,0,0.0472691,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,N04-4026,0,0.0427816,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,P05-1069,0,0.0683208,"allowed word reorderings according to the actual portion of the input sentence under process. Whatever the constraint, among the allowed wordreorderings it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-s"
2006.iwslt-papers.4,2004.iwslt-evaluation.6,0,0.0148907,"s. The aim of reordering models, known also as distortion models, is just that of providing a measure of the plausibility of reorderings. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. Some lexicalized block re-ordering models were presented in [6, 7, 8], where each block is associated with an orientation with respect to its predecessor. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. In [9] and [10], the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. Part-of-Speech (POS) rules are applied for preprocessing the source side both in translation model training and in decoding. In this work we present a novel method for extracting reordering rules from word-aligned training data. The units in the left-hand-side of rules can be plain words or POS’s; moreover, rules can reorder sequences of single units or a pair of unit blocks. In a two-stage SMT system like our one, reordering rules could be exploited"
2006.iwslt-papers.4,P02-1038,0,0.332166,"we have worked with. Section 3 introduces the new reordering method. Sections 4 and 5 present the experimental results and future application, respectively. Some conclusions are drawn in Section 6. 2. The Phrase-based SMT System Given a string f in the source language, the goal of statistical machine translation is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrase-based translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model [11, 12] and by introducing the concept of word alignment [1], the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e 182 a R X r=1 λr hr (˜ e, f , a), source string decoder target string WG N−best Rescoring : extractor (from WG) a b c d b a b Figure 1: Architecture of the ITC-irst SMT system. The decoder produces a word-graph (WG) of translation hypotheses. In single-stage translation the most probable string is output. In two-stage decoding, N-best translations are extracted, re-scored, and re-ranked by applying additional feature functions. where ˜ e represents a str"
2006.iwslt-papers.4,takezawa-etal-2002-toward,0,0.0601658,"ORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER BlockREORDER /rr •/vmodal /rr /vmodal /rr /vmodal * * * * [/rr /vmodal] [/rr /vmodal] [/rr /vmodal] [/rr /vmodal] * * * * * * /v * * * * [/v] [/v] * * [/v] [/v] * where K is the number of the reordering patterns matching the given source/target pair. If for a matching POS pattern there is no reordering suggestion matching the actual translation, a small probability is used in the sum (2). 4. Experiments 4.1. Translation Tasks and Data Experiments were carried out on the Basic Traveling Expression Corpus (BTEC) [19]. BTEC is a multilingual speech corpus that contains translation pairs taken from phrase books for tourists. We conducted experiments on three language pairs: Chinese-to-English, Japanese-to-English and Arabic-to-English. On the Chinese-to-English direction, both POS and lexicalized rules were tested. On the contrary, for the other three language pairs, only lexicalized rules were experimented: lexicalized rules were extracted and applied in the same way as POS rules, with the only obvious difference that here the lhs is defined on words instead of POS’s. Detailed statistics on BTEC training d"
2006.iwslt-papers.4,W03-1709,0,0.0362555,"that here the lhs is defined on words instead of POS’s. Detailed statistics on BTEC training data are reported in Table 2. Data sets distributed for the CSTAR 2003 Evaluation Campaign were employed for development, while testing was performed on sets of IWSLT 2004 and IWSLT 2005 Evaluation Campaigns, and on one of the development sets (devset4) distributed for the IWSLT 2006 Evaluation Campaign. For each source sentence of those sets, 16 or 7 references are available. Detailed statistics are reported in Table 3. The Chinese word segmentation and POS tagging were performed by means of ICTCLAS [20] for the experiments involving POS rules. In the experiments using lexicalized rules, Chinese and Japanese were re-segmented with an inhouse word segmentation tool. We found that this permits to smooth inconsistencies of the manual segmentation. All texts were finally tokenized and put in lower case. In the following, translation performance are provided in terms of BLEU [21] and NIST1 scores, computed in the 1 http://www.nist.gov/speech/tests/mt/ /ng * * /ng /ng * * [/ng [/ng [/ng [/ng [/ng ¡/v * * /v /v * * /v] /v] /v] /v] /v] /wfullstop * * * * * * * * * * [/wfullstop] #21: #12: #12: #10: #"
2006.iwslt-papers.4,W05-0835,0,0.105966,"ed to model different aspects of the translation process. Figure 1 illustrates how the translation of an input string is performed by the ITC-irst SMT system. In the first stage, a beam search algorithm (decoder) computes a word graph of translation hypotheses. Hence, either the best translation hypothesis is directly extracted from the word graph and output, or an N-best list of translations is computed by means of an exact algorithm [13]. The N-best translations are then reranked by applying additional feature functions and the top ranking translation is finally output. The search algorithm [14] exploits dynamic programming, i.e. the optimal solution is computed by expanding and recombining previously computed partial theories. The target string is extended step by step by covering new source positions until all of them are covered. For each added target phrase, a source phrase within the source string is chosen, and the corresponding score is computed on the basis of its position and phrase-to-phrase translation probabilities. The fluency of the added target phrase with respect to its left context is evaluated by a 4-gram language model. Some exceptions are also managed: target phra"
2006.iwslt-papers.4,P03-1021,0,0.0875236,"e less promising and constraints are set to possible word re-ordering. Word re-ordering constraints are applied during translation each time a new source position is covered, by limiting the number of vacant positions on the left and the distance from the left most vacant position. The log-linear model on which both the search algorithm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3"
2006.iwslt-papers.4,2004.iwslt-papers.2,1,0.770981,"e less promising and constraints are set to possible word re-ordering. Word re-ordering constraints are applied during translation each time a new source position is covered, by limiting the number of vacant positions on the left and the distance from the left most vacant position. The log-linear model on which both the search algorithm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3"
2006.iwslt-papers.4,P00-1056,0,0.0950438,"hm and the rescoring stage work embeds feature functions whose parameters are either estimated from data or empirically fixed. The scaling factors λ of the log-linear model are instead estimated on a development set, by applying a minimum error training procedure [15, 16]. The language model feature function is estimated on unsegmented monolingual texts. c a c d b Figure 2: Example illustrating the concept of block. The phrase-to-phrase probability feature is estimated from phrase-pair statistics extracted from word-aligned parallel texts. Alignments are computed with the GIZA++ software tool [17]. Phrase pairs are extracted from the segment pairs by means of the algorithm described in [18]. The distortion model feature function is a fixed negative exponential function computed on the distance between the current and the previously translated source phrases. 3. Reordering Rules In order to overcome the limitations of our simple distortion model, we propose to enrich the translation process with reordering rules as defined in the following. Units on which rules work can be words (lexicalized rules) or POS’s (POS rules). Rules can suggest/constraint reorderings at the level of either sin"
2006.iwslt-papers.4,J93-2003,0,\N,Missing
2006.iwslt-papers.4,J96-1002,0,\N,Missing
2006.iwslt-papers.4,P02-1040,0,\N,Missing
2006.iwslt-papers.4,2005.iwslt-1.11,1,\N,Missing
2007.iwslt-1.11,P07-2045,1,0.0120612,"dels, and finally the use of multiple phrase-tables. By working on top of a state-of-the art baseline, experiments showed that the above methods accounted for significant BLEU score improvements. 1. Introduction This paper presents work carried out at FBK (formerly ITCirst) to develop speech translation systems for three translation tasks of the IWSLT 2007 Evaluation, namely Chineseto-English (CE), Japanese-to-English (JE), and Italian-toEnglish (IE). All three systems are based on different set-ups of the same translation engine, namely the Moses statistical machine translation (SMT) Toolkit [1].1 The FBK team joined the Moses open source project in 2006 and indeed discontinued the development of its own decoding software [2]. This paper is organized as follows. Section 2 gives a short introduction of the general architecture of the systems, which basically takes advantage of experience gained in the past IWSLT evaluations. Section 3 focuses on the novel aspects that were investigated specifically for IWSLT 2007, namely the management of punctuation and the use of additional language resources. Sections 4 to 6 discuss details related to the development and experimentation of each sin"
2007.iwslt-1.11,W07-0712,1,0.913023,"rrently available release of Moses features a multistack, phrase-based, beam-search decoder able to process a CN as well as plain text. It implements a log-linear translation model including as feature functions: direct and inverse phrase-based and word-based lexicons, multiple wordbased n-gram target language models (LMs), phrase and word penalties, and distance-based (possibly lexicalized) reordering model. Computational efficiency is obtained through prefetching and early recombining the translation alternatives of the source phrases. On-demand loading of lexicon, distortion models and LMs [6] and quantization of LMs [7] allow a big reduction of run-time memory usage. 2 Available from http://www.speech.sri.com/projects/srilm A detailed description of the CN decoder can be found in [8]. 2.3. Rescoring After running Moses, a second decoding step can possibly be applied, which rescores the 5000-best list of translation hypotheses with the following 7 additional features: • direct and inverse IBM Model 1 lexicon scores which should capture lexical co-occurrences in the source and target strings; • weighted sum of n-grams relative frequencies (n from 1 to 4) in N -best list, which favor"
2007.iwslt-1.11,W06-3113,1,0.833384,"Moses features a multistack, phrase-based, beam-search decoder able to process a CN as well as plain text. It implements a log-linear translation model including as feature functions: direct and inverse phrase-based and word-based lexicons, multiple wordbased n-gram target language models (LMs), phrase and word penalties, and distance-based (possibly lexicalized) reordering model. Computational efficiency is obtained through prefetching and early recombining the translation alternatives of the source phrases. On-demand loading of lexicon, distortion models and LMs [6] and quantization of LMs [7] allow a big reduction of run-time memory usage. 2 Available from http://www.speech.sri.com/projects/srilm A detailed description of the CN decoder can be found in [8]. 2.3. Rescoring After running Moses, a second decoding step can possibly be applied, which rescores the 5000-best list of translation hypotheses with the following 7 additional features: • direct and inverse IBM Model 1 lexicon scores which should capture lexical co-occurrences in the source and target strings; • weighted sum of n-grams relative frequencies (n from 1 to 4) in N -best list, which favors hypotheses containing popu"
2007.iwslt-1.11,W06-3110,0,0.0169392,"rescores the 5000-best list of translation hypotheses with the following 7 additional features: • direct and inverse IBM Model 1 lexicon scores which should capture lexical co-occurrences in the source and target strings; • weighted sum of n-grams relative frequencies (n from 1 to 4) in N -best list, which favors hypotheses containing popular n-grams of higher order; • the reciprocal of the rank (log), which prefers high-ranked hypotheses; • counts of hypothesis duplicates (log), which awards translations occurring several times; • n-gram posterior probabilities within the N-best translations [9]; • sentence length posterior probabilities [9]. 2.4. Capitalization In the IE task both human and automatic transcripts do not contain case information. We decided to perform translation with models trained on lower-cased texts, and restore capitalization as postprocessing step, by means of the disambig tool of SRILM toolkit, fed with a n-gram case sensitive target LM. Instead, in the CE and JE tasks this issue is not present because the source languages do not represent capitalization explicitly. Hence, case information is automatically introduced during translation by using case-sensitive m"
2007.iwslt-1.11,2005.iwslt-1.8,0,0.0285146,"Missing"
2007.iwslt-1.11,P03-1010,0,0.02494,"s, also in this case performance well compare with the best official results of the 2006 IWSLT evaluation campaign. 5. Japanese-to-English System FBK also developed a system for the JE classic translation task, that is the translation of read speech in the travel domain. We exploited most of the outcomes of the CE system development, with few adjustments as specified in the following. Statistics of corpora utilized here are given in Table 5. It can be noted the symmetry with respect to the resources used for the CE task, with the LDC parallel corpus replaced by the much smaller Reuters corpus [15], one of the shared resources.4 Performance of the various system configurations tested for the JE task are collected in Table 6. The first row refers to the baseline system (Section 2 and Table 2), that is the 1TM/1-LM system corresponding to the entry +inter. of Table 4. In this task, the attempt of adding the 5-gram web75nc LM did not give any improvement (second row). On the contrary, the use of a second TM trained on the Reuters corpus yielded to the best performing system (third row), which was also tested on the best automatic transcription of the Japanese speech (last row). Even for th"
2007.iwslt-1.11,J03-1002,0,\N,Missing
2007.mtsummit-papers.15,J96-1002,0,0.0149963,"organized as follow. Section 2 presents the phrase-based SMT system we work with. Section 3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides t"
2007.mtsummit-papers.15,J93-2003,0,0.0168755,"3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides the decoder, Moses provides tools for training translation and lexicalized reordering model"
2007.mtsummit-papers.15,E06-1032,0,0.0127946,"er’s constraints. For instance, it is easy to verify that a low-order LM (e.g. a bigram LM) permits long word movements and the creation of phrases which are not contained in the phrase-table. The algorithm we have proposed for expanding N-best lists is simple and intuitive, but nevertheless effective in improving the quality of a two-pass SMT system. In fact, small but consistent improvements were measured on various evaluation sets on the challenging Chinese-to-English NIST MT task over a state-of-the-art performing baseline. We are aware of the criticisms about the BLEU score expressed in (Callison-Burch et al., 2006), which could particularly apply to our technique. In their paper, Burch et. al showed that for a translation output there are many possible variants, based on word permutations, that would each receive a similar BLEU score. Some variations could even correspond to higher scores but not to any genuine improvement in translation quality. The same could indeed apply to the translations computed by the expansion step, which in practice generates new word arrangements from the N-best list. We cannot reject this claim, as we did not manually inspect all the translations. However, from one hand our"
2007.mtsummit-papers.15,2006.iwslt-papers.4,1,0.898932,"Missing"
2007.mtsummit-papers.15,W05-0835,0,0.0183761,"ist indeed among the N-best ones. In this paper we present a technique to expand existing N-best lists in order to increase their potential of containing better translations. New entries are generated by means of a word-based n-gram language model estimated on the N-best entries. Experimental results on the NIST Chinese-to-English task show that better N-best lists can be obtained which also result in systematic BLEU score improvements in the re-scoring step. 1. Introduction In Statistical Machine Translation (SMT), performance improvements are often reported by applying two processing steps (Federico and Bertoldi, 2005; Koehn et al., 2003). In the first step, a decoding algorithms is applied that generates an N-best list of translation hypotheses. In the second step, the final translation is computed by re-ranking the Nbest translations through additional scores, computed with more sophisticated feature functions. Clearly, a fundamental assumption of the two step approach is that the generated N-best list contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The r"
2007.mtsummit-papers.15,N03-1017,0,0.00526933,"nes. In this paper we present a technique to expand existing N-best lists in order to increase their potential of containing better translations. New entries are generated by means of a word-based n-gram language model estimated on the N-best entries. Experimental results on the NIST Chinese-to-English task show that better N-best lists can be obtained which also result in systematic BLEU score improvements in the re-scoring step. 1. Introduction In Statistical Machine Translation (SMT), performance improvements are often reported by applying two processing steps (Federico and Bertoldi, 2005; Koehn et al., 2003). In the first step, a decoding algorithms is applied that generates an N-best list of translation hypotheses. In the second step, the final translation is computed by re-ranking the Nbest translations through additional scores, computed with more sophisticated feature functions. Clearly, a fundamental assumption of the two step approach is that the generated N-best list contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The reason for applying tw"
2007.mtsummit-papers.15,2005.iwslt-1.8,0,0.0157904,"coding: besides “large” and “giga” LMs, a third 4-gram LM was trained on the English side of the development sets (“dev”). MT performance are provided in terms of case-insensitive BLEU and NIST scores, as computed with the NIST scoring tool. 4.2. Setup For the generation of N-best translation lists, we run Moses with the maximum reordering distance set to 6 and the following feature functions: • phrase translation model, with phrases including at most 7 words • 3 LMs, namely “dev”, “large” and “giga” • lexicalized distortion model, trained specifying the option “orientation-bidirectional-fe” (Koehn et al., 2005) • word and phrase penalties, for balancing the length of translations with that of input sentences. Once N-best lists are available, they are expanded through the algorithm described in Section 3 by setting n = 4. Then, original (1) and updated (2) lists are re-scored and then re-ranked by applying some of the following feature functions, as specified in brackets: • n-gram posterior probabilities within the expanded N+M-best translations (2) • sentence length posterior probabilities within the original N-best translations (Zens and Ney, 2006) (1, 2) • sentence length posterior probabilities w"
2007.mtsummit-papers.15,J00-2004,0,0.245198,"Missing"
2007.mtsummit-papers.15,P02-1038,0,0.0289396,"Section 2 presents the phrase-based SMT system we work with. Section 3 introduces the new hypotheses generation algorithm. Section 4 describes experiments and analyzes results. A discussion and conclusions end the paper. 2. SMT Process Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996; Och and Ney, 2002) and by introducing the concept of word alignment (Brown et al., 1993), the optimal translation can be searched for with the criterion: ˜ e∗ = arg max max ˜ e a R X λr hr (˜ e, f , a), r=1 where ˜ e represents a string of phrases in the target language, a an alignment from the words in f to the phrases in ˜ e, and hr (˜ e, f , a) r = 1, . . . , R are feature functions, designed to model different aspects of the translation process. We performed the “argmax” operation of the above equation by means of the decoder available in Moses,1 an open source toolkit for SMT. Besides the decoder, Moses pr"
2007.mtsummit-papers.15,W06-3110,0,0.460989,"t contains better translations than the best one found by the decoder. The aim of the additional feature functions is indeed to reward better translations found among the N-best entries of the decoder. The reason for applying two steps instead of one is that not all available feature functions can be efficiently implemented into the decoder. In fact, not all of them can be decomposed into local scores that can be computed on partial translation hypotheses. Moreover, recently feature functions have been proposed that are estimated directly on the N-best list. In particular, (Chen et al., 2005; Zens and Ney, 2006) have recently reported performance improvements by computing posterior probabilities through n-gram language models (LMs) estimated on the N-best translations. This paper proposes an intermediate step in the chain. Before applying re-scoring, the N-best list is further expanded by applying a generative statistical n-gram LM, estimated on the N-best list itself. In particular, the LM is used to generate M new and different target strings, that do not occur in the N-best list. We applied this technique to a well performing baseline for Chinese-to-English translation, trained under the largedata"
2007.mtsummit-papers.28,P06-1067,0,0.0371199,"Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and inverted. Any block reordering is allowed, no matter whether it was obser"
2007.mtsummit-papers.28,J96-1002,0,0.0249367,"language point of view, namely English. Moreover, differently from what can happen in lexicalized models, our model does not suffer from data sparseness, since statistics are collected for POS classes instead of plain words. 4. The Phrase-based SMT System Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996b; Och and Ney, 2002), the optimal translation can be searched for by exploiting a set of feature functions, designed to model different aspects of the translation process. Our translation system works in two steps. In the first stage, the beam search decoder available in Moses (Koehn et al., 2007),1 computes an N-best list of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. I"
2007.mtsummit-papers.28,P05-1033,0,0.0426713,"unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model local reorderings, short idioms, insertions and deletions that are sensitive to local context. They are a simple mechanism but powerful enough to really improve performance (Koehn et al., 2003; Och and Ney, 2004). Nevertheless, they are able to capture only local phenomena. In (Chiang, 2005) an interesting extension toward hierarchical phrases was proposed, which allows one to predict long-span reordering phenomena, too. In this work we present a novel word reordering model. In particular, our goal is to model reorderings concerning three major part-of-speech (POS) classes, namely nouns, verbs and adjectives. Relevant statistics are collected from wordaligned parallel texts regarding the distance between target words and the distance between the corresponding source words. The model was applied as a set of additional feature functions for re-scoring N-best lists generated by a ph"
2007.mtsummit-papers.28,W05-0835,0,0.0174814,"roach should have over them. 3. Related Work One of the main research areas in SMT is word/phrase reordering models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai , bi−1 ), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and"
2007.mtsummit-papers.28,J99-4005,0,0.294462,"Missing"
2007.mtsummit-papers.28,N03-1017,0,0.277583,"iding a measure of the plausibility of word movements. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model local reorderings, short idioms, insertions and deletions that are sensitive to local context. They are a simple mechanism but powerful enough to really improve performance (Koehn et al., 2003; Och and Ney, 2004). Nevertheless, they are able to capture only local phenomena. In (Chiang, 2005) an interesting extension toward hierarchical phrases was proposed, which allows one to predict long-span reordering phenomena, too. In this work we present a novel word reordering model. In particular, our goal is to model reorderings concerning three major part-of-speech (POS) classes, namely nouns, verbs and adjectives. Relevant statistics are collected from wordaligned parallel texts regarding the distance between target words and the distance between the corresponding source words. The mode"
2007.mtsummit-papers.28,P07-2045,1,0.0139298,"he goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996b; Och and Ney, 2002), the optimal translation can be searched for by exploiting a set of feature functions, designed to model different aspects of the translation process. Our translation system works in two steps. In the first stage, the beam search decoder available in Moses (Koehn et al., 2007),1 computes an N-best list of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. In the second stage, the N-best translations are re-scored by applying additional feature functions and re-ranked: the top-ranked translation is finally output. The log-linear models used in both steps have interpolation parameters which are estimated from a development set by applying a minimum err"
2007.mtsummit-papers.28,2005.mtsummit-papers.11,0,0.027414,"Missing"
2007.mtsummit-papers.28,H05-1021,0,0.0173776,"t phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are consi"
2007.mtsummit-papers.28,2004.iwslt-evaluation.6,0,0.0171378,"dering. It predicts four types of reordering patterns: monotone adjacent, monotone gap, reverse adjacent and reverse gap. By collapsing into the same neutral class monotone gaps and reverse gaps, it models only three possible events similarly to local reordering models (Tillmann and Zhang, 2005). The distortion model proposed in (Al-Onaizan and Papineni, 2006) assigns a probability distribution over possible relative jumps conditioned on source words. It consists of three components: outbound, inbound and pair distortion. The model’s parameters are directly estimated from word alignments. In (Lee and Roukos, 2004) and (Lee, 2006), the aim is to capture particular syntactic phenomena occurring in the source language which are not preserved by the target language. POS rules are applied for preprocessing the source side both in translation model training and in decoding. All models referred to above were tested on different language pairs, including Arabic, Chinese, English, German and Japanese languages. Apart Chinese, which is typologically inconsistent (Newmeyer, 2004), each one of other languages has its own grammatical properties which are peculiar but nevertheless comparable. Hence, the reordering m"
2007.mtsummit-papers.28,P06-1090,0,0.0571521,", 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and inverted. Any block reordering is allowed"
2007.mtsummit-papers.28,P02-1038,0,0.0451674,"w, namely English. Moreover, differently from what can happen in lexicalized models, our model does not suffer from data sparseness, since statistics are collected for POS classes instead of plain words. 4. The Phrase-based SMT System Given a string f in the source language, the goal of SMT is to select the string e in the target language which maximizes the posterior distribution Pr(e |f ). In phrasebased translation, words are no longer the only units of translation, but they are complemented by strings of consecutive words, the phrases. By assuming a log-linear model (Berger et al., 1996b; Och and Ney, 2002), the optimal translation can be searched for by exploiting a set of feature functions, designed to model different aspects of the translation process. Our translation system works in two steps. In the first stage, the beam search decoder available in Moses (Koehn et al., 2007),1 computes an N-best list of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. In the second stage, t"
2007.mtsummit-papers.28,J04-4002,0,0.398776,"he plausibility of word movements. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model local reorderings, short idioms, insertions and deletions that are sensitive to local context. They are a simple mechanism but powerful enough to really improve performance (Koehn et al., 2003; Och and Ney, 2004). Nevertheless, they are able to capture only local phenomena. In (Chiang, 2005) an interesting extension toward hierarchical phrases was proposed, which allows one to predict long-span reordering phenomena, too. In this work we present a novel word reordering model. In particular, our goal is to model reorderings concerning three major part-of-speech (POS) classes, namely nouns, verbs and adjectives. Relevant statistics are collected from wordaligned parallel texts regarding the distance between target words and the distance between the corresponding source words. The model was applied as a s"
2007.mtsummit-papers.28,N04-1021,0,0.0203889,"eas in SMT is word/phrase reordering models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai , bi−1 ), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orient"
2007.mtsummit-papers.28,P03-1021,0,0.0164861,"ist of translations. Moses is an open source toolkit for statistical machine translation which includes, besides the decoder, tools for training translation and lexicalized reordering models, and a minimum error training procedure for estimating optimal interpolation weights. In the second stage, the N-best translations are re-scored by applying additional feature functions and re-ranked: the top-ranked translation is finally output. The log-linear models used in both steps have interpolation parameters which are estimated from a development set by applying a minimum error training procedure (Och, 2003). The reordering model presented in the following section is the only additional feature function applied for re-scoring the N-best lists. 5. The POS-based Reordering Model We assume that we have a parallel training corpus provided with inverted word alignments, that is alignments from target to source positions. Let (f , e) be a source-target sentence pair, and let a be an inverted alignment which maps target positions i into source positions ai = j. For any target position i, we look for its predecessor i∗ that is aligned to the rightmost source position. Our interest is indeed in the differ"
2007.mtsummit-papers.28,takezawa-etal-2002-toward,0,0.0773993,"Missing"
2007.mtsummit-papers.28,P05-1069,0,0.0861631,"ng models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai , bi−1 ), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finit"
2007.mtsummit-papers.28,N04-4026,0,0.0424606,"/phrase reordering models. Many reordering models have recently been proposed in the literature. The simplest but effective way to capture movements of target phrases is the use of a relative distortion probability distribution d(ai , bi−1 ), where ai denotes the start position of the source phrase that is translated into the i-th target phrase, while bi−1 denotes the end position of the source phrase translated into the i − 1-th target phrase. Systems described in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is impleme"
2007.mtsummit-papers.28,J97-3002,0,0.0838103,"1. Introduction In machine translation (MT), one of the main problems to handle is word reordering. A word is “reordered” when it and its translation occupy different positions within the corresponding sentence. In Statistical MT (SMT) (Brown et al., 1993), word reordering is faced from two points of view: constraints and modeling. If arbitrary wordreorderings are permitted, the exact decoding problem is NP-hard (Knight, 1999); it can be made polynomialtime by introducing proper constraints, such as IBM constraints (Berger et al., 1996a) and Inversion Transduction Grammars (ITG) constraints (Wu, 1997). Among all the allowed word-reorderings, it is expected that some are more likely than others. The aim of reordering models, known also as distortion models, is that of providing a measure of the plausibility of word movements. Most of the distortion models developed so far are unable to exploit linguistic context to score reorderings: they just predict target positions on the basis of other (source and target) positions. A few years ago SMT moved from words to phrases as basic units of translation. Phrases are sequences of words, not necessarily with a syntactic meaning, that allow to model"
2007.mtsummit-papers.28,P06-1066,0,0.0475135,", 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and inverted. Any block"
2007.mtsummit-papers.28,W06-3108,0,0.053813,"ibed in (Och and Ney, 2004; Koehn et al., 2003; Federico and Bertoldi, 2005), and many others, adopt this strategy. In (Och et al., 2004; Tillmann, 2004; Tillmann and Zhang, 2005), reordering models work on the concept of block, which is a pair of source and target phrases. Each block is associated with an orientation with respect to its predecessor block. During decoding, the probability of a sequence of blocks with the corresponding orientations is computed. Many recent papers on reordering models are inspired by the block orientation idea introduced by Tillman, like (Kumar and Byrne, 2005; Zens and Ney, 2006; Xiong et al., 2006; Nagata et al., 2006; Al-Onaizan and Papineni, 2006). In (Kumar and Byrne, 2005) the block orientation is implemented through weighted finite state transducers. Unfortunately, that model cannot capture all possible phrase movements. Discriminative lexicalized reordering models are presented in (Zens and Ney, 2006). Several types of features are tested: word-based, word class-based, POS-based and based on local context. Also (Xiong et al., 2006) exploit a discriminative model to predict reordering of consecutive blocks. Two kinds of reorderings are considered: straight and"
2007.mtsummit-papers.28,J93-2003,0,\N,Missing
2007.mtsummit-papers.28,2005.iwslt-1.8,0,\N,Missing
2009.iwslt-evaluation.5,N06-2013,0,0.0238153,"e-Processing for Morphologically Rich Languages Indeed linguistic preprocessing plays a fundamental role in any NLP application involving morphologically rich languages, such as Arabic and Turkish. This is particularly true for SMT into English where differences in word granularity between languages reflects on much higher data sparseness on the source side and on the difficulty to properly model word-level alignments. We approached these problems through morphological segmentation of the source languages, referring partly to the work of [1] on an EnglishTurkish task, and partly to the one of [2] on an ArabicEnglish task. Secondly, as this was shown to have a positive effect on some Arabic-English SMT systems of previous IWSLT editions [3, 4], we developed two simple languagespecific techniques of lexical approximation, which consists in replacing the OOVs of the test set by words of the training that are morphologically close to them. 1. Introduction FBK submitted runs at the IWSLT 2009 Evaluation for the Arabic-English and Turkish-English BTEC tasks, and for the Challenge Task involving Chinese and English languages in both directions. This paper reports on efforts we made in the de"
2009.iwslt-evaluation.5,2009.iwslt-papers.1,1,0.811638,"eaning absence of possessive suffixes is also removed; • copula is split off; • person suffixes are split off from finite verb forms and from copula. The following example shows an analyzed Turkish word before and after segmentation. The number of tokens increases from 1 to 5 as the word is split into noun, possessive, instrumental case, copula and verbal person: 2 More precisely: score = match × 20 − diff × 2 − diff × 5, 1 2 where match, diff1 and diff2 are respectively the numbers of shared contiguous tags, different tags in the OOV word, different tags in the replacer candidate. 1 Refer to [8] for a more detailed and linguistically motivated description. - 38 - Proceedings of IWSLT 2009, Tokyo - Japan where pi ’s are target LMs built on clusters which the training data are split in. With the help of Figure 1, the basic adaptation procedure is described in the following. Let us assume that the parallel training data have been partitioned into a set of M bilingual clusters, according to some criterion. On each cluster, language specific LMs are estimated, which are then organized into two language specific mixture models. All operations described so far are performed off-line. Now le"
2009.iwslt-evaluation.5,P05-1071,0,0.015153,"o language specific mixture models. All operations described so far are performed off-line. Now let us consider a source text or sentence to be translated. Before translation, the input is used to estimate optimal weights of the source language mixture through Expectation-Maximization. The resulting weights are then transferred to the target language mixture, which is finally used as LM feature function by the SMT system. 2.2.1. Morphological Segmentation Several state-of-the-art tools are available to perform morphological segmentation of Arabic text. For the evaluation we have compared MADA [9] and AMIRA [10], two softwares that differ both on their decision strategy and on the segmentation scheme they apply to the words. While the first is a morphological disambiguator based on linguistic features produced by the Buckwalter analyzer [11], the second is a much lighter-weight SVM classifier based on a -5/+5 character context. As for the segmentation schemes (see Table 2) MADA (scheme “D2”) only splits proclitics – namely conjunctions (w+ ‘and’, f+ ‘then’), prepositions (b+ ‘by’, k+ ‘as’, l+ ‘to’) and the future tense (s+) – whereas AMIRA also separates enclitics, i.e. object and poss"
2009.iwslt-evaluation.5,N04-4038,0,0.0110197,"cific mixture models. All operations described so far are performed off-line. Now let us consider a source text or sentence to be translated. Before translation, the input is used to estimate optimal weights of the source language mixture through Expectation-Maximization. The resulting weights are then transferred to the target language mixture, which is finally used as LM feature function by the SMT system. 2.2.1. Morphological Segmentation Several state-of-the-art tools are available to perform morphological segmentation of Arabic text. For the evaluation we have compared MADA [9] and AMIRA [10], two softwares that differ both on their decision strategy and on the segmentation scheme they apply to the words. While the first is a morphological disambiguator based on linguistic features produced by the Buckwalter analyzer [11], the second is a much lighter-weight SVM classifier based on a -5/+5 character context. As for the segmentation schemes (see Table 2) MADA (scheme “D2”) only splits proclitics – namely conjunctions (w+ ‘and’, f+ ‘then’), prepositions (b+ ‘by’, k+ ‘as’, l+ ‘to’) and the future tense (s+) – whereas AMIRA also separates enclitics, i.e. object and possessive pronouns"
2009.iwslt-evaluation.5,P02-1038,0,0.0119197,"ng the possible data sparseness issue that can affect the sentence specific weight estimation. 4.2. Turkish-English System 4.2.1. Data For training our Turkish-English system we exclusively used the provided BTEC training corpus. Parameters were tuned on IWSLT09’s devset1 using the gold reference translation only. Evaluation during development was performed on devset2. 4. Evaluation results 4.1. Baseline System Given a string f in the source language, the goal of statistical machine translation [12] is to select the most probable string e in the target language. By assuming a log-linear model [13, 14], the optimal translation can be searched for with the criterion: e∗ = arg max max e a R X 4.2.2. Baseline Setup The baseline preprocessing consists in simple tokenization (IWSLT09’s released script) and lowercasing of the source side data. Due to the severe mismatch in word order between the languages, we set the distortion limit (DL) to 10. Moses option -drop-unknown was active in all submitted runs. λr hr (e, f , a), 4.2.3. Results r=1 where a represents a word- or phrase-based alignment between f and e, and hr (e, f , a) r = 1, . . . , R are feature functions, designed to model different a"
2009.iwslt-evaluation.5,P07-2045,1,0.0126036,"t of separating the training corpus into several subsets yields wi pi (e) i=1 3 AMIRA splits the same proclitics as MADA except for the future tense. 4 Available - 39 - from http://glaros.dtc.umn.edu/gkhome/views/cluto Proceedings of IWSLT 2009, Tokyo - Japan SRC TRAINING PARALLEL TEXTS SRC SRC TGT CLSTR 1 LM ESTIMATION CLSTR 2 TGT CLSTR M TGT LM 1 LM 1 LM 2 LM 2 LM M LM M CLUSTERING OPTIMIZATION of SRC LMs INTERPOLATION SRC wi OFF−LINE ON−LINE INTERPOLATION of TGT LMs TEXT SMT TRANSLATION Figure 1: Basic procedure for LM adaptation. Our systems are built upon the open-source MT toolkit Moses [15]. The decoder features a statistical log-linear model including a phrase-based translation model, a language model, a distortion model and word and phrase penalties. The weights λr of the log-linear combination are optimized by means of a minimum error training (MERT) procedure [16]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [17]. better results, since the EM procedure is allowed complete fre"
2009.iwslt-evaluation.5,P03-1021,0,0.0149284,"EXTS SRC SRC TGT CLSTR 1 LM ESTIMATION CLSTR 2 TGT CLSTR M TGT LM 1 LM 1 LM 2 LM 2 LM M LM M CLUSTERING OPTIMIZATION of SRC LMs INTERPOLATION SRC wi OFF−LINE ON−LINE INTERPOLATION of TGT LMs TEXT SMT TRANSLATION Figure 1: Basic procedure for LM adaptation. Our systems are built upon the open-source MT toolkit Moses [15]. The decoder features a statistical log-linear model including a phrase-based translation model, a language model, a distortion model and word and phrase penalties. The weights λr of the log-linear combination are optimized by means of a minimum error training (MERT) procedure [16]. The phrase-based translation model provides direct and inverted frequency-based and lexical-based probabilities for each phrase pair included in a given phrase table. Phrase pairs are extracted from symmetrized word alignments generated by GIZA++ [17]. better results, since the EM procedure is allowed complete freedom in assigning the LM weights. However, weights computed in such a manner may be less reliable, since the estimation is performed on few data (one single sentence). 3.3.3. Two-step weight estimation This approach merges the previous two in the attempt of keeping their advantages"
2009.iwslt-evaluation.5,J93-2003,0,\N,Missing
2009.iwslt-evaluation.5,J96-1002,0,\N,Missing
2009.iwslt-evaluation.5,J03-1002,0,\N,Missing
2009.iwslt-evaluation.5,W07-0704,0,\N,Missing
2009.iwslt-evaluation.5,2007.iwslt-1.27,0,\N,Missing
2009.iwslt-evaluation.5,2008.iwslt-evaluation.10,0,\N,Missing
2009.iwslt-papers.5,C04-1059,0,\N,Missing
2009.iwslt-papers.5,J93-2003,0,\N,Missing
2009.iwslt-papers.5,J96-1002,0,\N,Missing
2009.iwslt-papers.5,P07-2045,1,\N,Missing
2009.iwslt-papers.5,W04-3225,0,\N,Missing
2009.iwslt-papers.5,D07-1036,0,\N,Missing
2009.iwslt-papers.5,W07-0733,0,\N,Missing
2009.iwslt-papers.5,N07-1064,0,\N,Missing
2009.iwslt-papers.5,P02-1038,0,\N,Missing
2009.iwslt-papers.5,J03-1002,0,\N,Missing
2009.iwslt-papers.5,W07-0722,0,\N,Missing
2009.iwslt-papers.5,2005.iwslt-1.8,0,\N,Missing
2009.iwslt-papers.5,P03-1021,0,\N,Missing
2010.eamt-1.30,J93-2003,0,0.0193991,"ent or the test set. Different mixture weight estimation schemes are proposed and compared, at the level of either single or all source sentences. Experimental results show that, by training different specific language models weighted according to the actual input instead of using a single target language model, translation quality is improved, as measured by BLEU and TER. 1 exp K k=1 λk hk (f , e) Pr(e|f ) = P PK 0 k=1 λk hk (f , e ) e0 exp P ˆ = argmax e e The grounds of modern Statistical Machine Translation (SMT), a pattern recognition approach to machine translation, were established in (Brown et al., 1993), where the problem of machine translation was defined as follows: given a sentence f from a certain source language, an equivalent senˆ in a given target language that maximizes tence e the posterior probability is to be found. Such a statement can be formalized as: ˆ = argmax Pr(e|f ) e e (1) e where Pr(f |e) stands for the translation probability and Pr(e) accounts for penalizing ill-formed sentences of the target language. c 2010 European Association for Machine Translation. (2) and the decision rule is given by the expression Introduction = argmax Pr(f |e) · Pr(e) Mauro Cettolo FBK Fondaz"
2010.eamt-1.30,W07-0722,0,0.0249224,"model (TM) is implemented as an unsupervised multinomial mixture of TMs and each component is supposed to concentrate most of its probability mass on a certain topic. Slightly later, (Nepveu et al., 2004) applied other adaptation techniques to interactive MT, following the ideas in (Kuhn and De Mori, 1990) and adding cache LMs and cache TMs to their system. In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. In (Civera and Juan, 2007), alignment model mixtures were explored as a way of performing topic-specific adaptation, the alignments being used to extract phrases. A work that resembles the one presented here is (Zhao et al., 2004), where each source sentence was used to build a query and retrieve similar sentences from a larger corpus. Then, a specific LM was trained and interpolated with a generic LM. This combination was used to translate the original sentence. In (L¨u et al., 2007), each sentence was used to select similar data within the same corpus by means of TF-IDF, and then prepare specific LMs and TMs ready to"
2010.eamt-1.30,W06-3114,0,0.0244781,"ords. OoV stands for “Out of Vocabulary” words, Dev. for Development, K/M for thousands/millions. En Fr En Training Es Sentences 751K 731K 688K Run. words 15.3M 16.1M 15.7M 15.2M 15.6M 13.8M Voc. 195K 66K 103K 64K 80K 62K Dev. En Sentences 2000 Run. words 55K 59K OoV 432 125 2000 61K 59K 208 127 2000 67K 59K 144 138 Test De Sentences 2000 Run. words 54K 58K OoV 377 127 2000 60K 58K 207 125 2000 66K 58K 139 133 4 4.1 Experiments Corpora Experiments were conducted on the Europarl corpus (Koehn, 2005), in the setup established in the Workshop on Statistical Machine Translation of the NAACL 2006 (Koehn and Monz, 2006). The Europarl corpus consists of the transcriptions of European Parliament speeches and includes versions in eleven European languages. Here, we will focus on the German–English, Spanish–English and French–English tasks, the same language pairs selected for the cited workshop. Although we tested our systems on both translation directions, in this paper we will only report experiments having English as source language for the sake of brevity, given that the behavior in the opposite direction was similar. The corpus is divided into three separate sets, for training, development and testing purp"
2010.eamt-1.30,W07-0733,0,0.0523234,"ion (De Mori and Federico, 1999; Bellagarda, 2001). Nowadays, also in the SMT community the interest for adaptation is continuously growing. One of the first approaches was proposed by (Lagarda and Juan, 2003), in which the translation model (TM) is implemented as an unsupervised multinomial mixture of TMs and each component is supposed to concentrate most of its probability mass on a certain topic. Slightly later, (Nepveu et al., 2004) applied other adaptation techniques to interactive MT, following the ideas in (Kuhn and De Mori, 1990) and adding cache LMs and cache TMs to their system. In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. In (Civera and Juan, 2007), alignment model mixtures were explored as a way of performing topic-specific adaptation, the alignments being used to extract phrases. A work that resembles the one presented here is (Zhao et al., 2004), where each source sentence was used to build a query and retrieve similar sentences from a larger corpus. Then, a specific LM was trained and interpolated with a"
2010.eamt-1.30,P07-2045,0,0.00751365,"Missing"
2010.eamt-1.30,2005.mtsummit-papers.11,0,0.00638585,"imation technique. Table 1: Europarl corpus statistics. Average sentence length is always between 20 and 30 words. OoV stands for “Out of Vocabulary” words, Dev. for Development, K/M for thousands/millions. En Fr En Training Es Sentences 751K 731K 688K Run. words 15.3M 16.1M 15.7M 15.2M 15.6M 13.8M Voc. 195K 66K 103K 64K 80K 62K Dev. En Sentences 2000 Run. words 55K 59K OoV 432 125 2000 61K 59K 208 127 2000 67K 59K 144 138 Test De Sentences 2000 Run. words 54K 58K OoV 377 127 2000 60K 58K 207 125 2000 66K 58K 139 133 4 4.1 Experiments Corpora Experiments were conducted on the Europarl corpus (Koehn, 2005), in the setup established in the Workshop on Statistical Machine Translation of the NAACL 2006 (Koehn and Monz, 2006). The Europarl corpus consists of the transcriptions of European Parliament speeches and includes versions in eleven European languages. Here, we will focus on the German–English, Spanish–English and French–English tasks, the same language pairs selected for the cited workshop. Although we tested our systems on both translation directions, in this paper we will only report experiments having English as source language for the sake of brevity, given that the behavior in the oppo"
2010.eamt-1.30,D07-1036,0,0.0450041,"Missing"
2010.eamt-1.30,W04-3225,0,0.027173,"ically, one of the features described in eq. 3 may be h(e, f ) = log p(e) Related Work LM adaptation has been deeply explored since at least mid 90s in the ambit of speech recognition (De Mori and Federico, 1999; Bellagarda, 2001). Nowadays, also in the SMT community the interest for adaptation is continuously growing. One of the first approaches was proposed by (Lagarda and Juan, 2003), in which the translation model (TM) is implemented as an unsupervised multinomial mixture of TMs and each component is supposed to concentrate most of its probability mass on a certain topic. Slightly later, (Nepveu et al., 2004) applied other adaptation techniques to interactive MT, following the ideas in (Kuhn and De Mori, 1990) and adding cache LMs and cache TMs to their system. In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. In (Civera and Juan, 2007), alignment model mixtures were explored as a way of performing topic-specific adaptation, the alignments being used to extract phrases. A work that resembles the one presented here"
2010.eamt-1.30,P02-1038,0,0.194667,"Missing"
2010.eamt-1.30,P03-1021,0,0.141768,"in this paper we will only report experiments having English as source language for the sake of brevity, given that the behavior in the opposite direction was similar. The corpus is divided into three separate sets, for training, development and testing purposes, respectively. Statistics are provided in Table 1. 4.2 Baseline system The baseline system is built upon the open-source MT toolkit Moses (Koehn et al., 2007)2 in its default setup. Following standard practice, the weights of the log-linear combination (eq. 3) are optimized by means of the Minimum Error Rate Training (MERT) procedure (Och, 2003). MERT was only performed for the baseline system, and its weights were re-used for all other systems. Although there could be reasons for re-running MERT when the LM changes, we did not do so in order to better isolate the effects of including different LMs into the SMT system. As baseline LM, a 5-gram word-based LM was estimated on the target side of the training corpus, smoothed according to the improved Kneser-Ney technique (Chen and Goodman, 1999). 4.3 Results Adaptation procedures presented in Section 3 have been experimentally assessed by performing automatic translation whose quality i"
2010.eamt-1.30,2001.mtsummit-papers.68,0,0.0487725,"Missing"
2010.eamt-1.30,2009.iwslt-papers.5,1,0.68223,"hts λk are optimized during the tuning stage with the use of a development set. In this paper, we deal with the problem of adaptation of SMT models. Specifically, we focus on augmenting the Language Model (LM) component by introducing parameters that are adapted dynamically to the input text. With this purpose, the LM is implemented as a mixture of specialized sub-LMs, which are conveniently estimated through some bilingual clustering of the training data and then combined following different weighting schemes. The work described here represents an important extension of what is presented in (Sanchis-Trilles et al., 2009): in fact, there the methods were tested on a small task like IWSLT; on the contrary, here the approach is assessed on the medium-sized Europarl task. Moreover, the clustering of training data does not exploit any supervised annotation of texts. The paper is organized as follows. Section 2 briefly lists other papers dealing related issues. Our adaptation procedure is described in Section 3, together with the different clustering techniques and weighting schemes we have investigated. In Section 4 the experimental setup is described and results provided and commented. Possible extensions of the"
2010.eamt-1.30,2006.amta-papers.25,0,0.0364164,"Missing"
2010.eamt-1.30,D07-1054,0,0.0230508,"ures were explored as a way of performing topic-specific adaptation, the alignments being used to extract phrases. A work that resembles the one presented here is (Zhao et al., 2004), where each source sentence was used to build a query and retrieve similar sentences from a larger corpus. Then, a specific LM was trained and interpolated with a generic LM. This combination was used to translate the original sentence. In (L¨u et al., 2007), each sentence was used to select similar data within the same corpus by means of TF-IDF, and then prepare specific LMs and TMs ready to be interpolated. In (Yamamoto and Sumita, 2007), the bilingual corpus is clustered so as to minimize the entropy of each subset, and then independent LMs and TMs are trained from these smaller bilingual corpora, which are in turn recombined in translation time by performing domain prediction. Differently, in our work the final combination of target LMs is obtained by re-using the weights estimated by maximizing the probability of generating Language Model Adaptation which provides the log score of the target LM. Typically, p(e) is given by a single LM; this configuration will represent our baseline. However, that distribution can be expres"
2010.eamt-1.30,C04-1059,0,0.0612091,"lied other adaptation techniques to interactive MT, following the ideas in (Kuhn and De Mori, 1990) and adding cache LMs and cache TMs to their system. In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. In (Civera and Juan, 2007), alignment model mixtures were explored as a way of performing topic-specific adaptation, the alignments being used to extract phrases. A work that resembles the one presented here is (Zhao et al., 2004), where each source sentence was used to build a query and retrieve similar sentences from a larger corpus. Then, a specific LM was trained and interpolated with a generic LM. This combination was used to translate the original sentence. In (L¨u et al., 2007), each sentence was used to select similar data within the same corpus by means of TF-IDF, and then prepare specific LMs and TMs ready to be interpolated. In (Yamamoto and Sumita, 2007), the bilingual corpus is clustered so as to minimize the entropy of each subset, and then independent LMs and TMs are trained from these smaller bilingual"
2010.eamt-1.30,P02-1040,0,\N,Missing
2010.eamt-1.30,W04-3250,0,\N,Missing
2010.iwslt-evaluation.5,N03-1017,0,0.00258058,"ish-English. The combination of several Turkish segmentation schemes into a lattice input led to an improvement wrt to last year. The use of additional training data was explored for Arabic-English, while on the English to French task improvement was achieved over a strong baseline by automatically selecting relevant and high quality data from the available training corpora. 1. BTEC task Turkish and Arabic are morphologically rich languages. When dealing with a small scale task such as the BTEC, this characteristic can have a particularly negative impact on phrase-based statistical MT methods [1]. Following last year’s findings [2] we decided to continue working on the problem of out-of-vocabulary words (OOVs) using different strategies. In the Arabic-English pair we tested the usefulness of additional resources by decoding with multiple phrase-tables. As for Turkish-English, we enriched our morphological segmentation rule set and combined several segmentation schemes inside a word lattice. In order to further improve the coverage of the models on the test, we then tried to refine the lexical approximation technique developed last year. 1.1. Arabic-English The experience of last year"
2010.iwslt-evaluation.5,P08-1115,0,0.0847327,"fixes replaced by tags as explained in [4]. The rules for suffix splitting or removal are then applied according to the selected segmentation scheme. It can be seen that in some cases the new rules allow for a better correspondence at the level of words between the Turkish sentence and its English translation. However, this doesn’t always corresponds to an improvement in translation quality (see Table 3). It was shown in [7] that the choice of the optimal segmentation scheme for Arabic-English SMT is not a trivial problem and may depend on several factors such as the training data size. Later [8] obtained considerable gains in translation quality by combining unsegmented and segmented Arabic test sentences into a lattice. Given these findings and given that the segmentation space of Turkish is even richer 1o ¨ ks¨ur¨uk: ‘cough’, P1sg: 1st person singular possessive, Acc: accusative, dur-: ‘stop’, Caus: causative, Able: ability, Neg: negation, Prog1: present progressive, A1sg: 1st person singular subject suffix. segmentation MS11 MS13 MS14 MS15 MS11+13+15 BLEU – NIST 60.30 – 9.367 58.98 – 9.357 57.76 – 9.373 60.32 – 9.575 60.41 – 9.650 1.3. Evaluation results and discussion All our sys"
2010.iwslt-evaluation.5,P07-2045,1,0.0268624,"Missing"
2010.iwslt-evaluation.5,P03-1021,0,0.0310921,"Missing"
2010.iwslt-evaluation.5,N04-4038,0,0.322573,"Missing"
2010.iwslt-evaluation.5,2009.iwslt-papers.1,1,0.928397,"ide) of newswire parallel text. Multiple phrase-table decoding was handled by the Moses decoder [9] in the ‘either’ mode, that is for each phrase the union of translation options coming from all the tables is considered. 1.2. Turkish-English Turkish morphology is agglutinative, which implies that the vocabulary is built by a wide range of basic suffix combinations. Thus it often occurs that a Turkish word is aligned with an English phrase, and sometimes even to a whole sentence as in the following example: oda odam odamda odamdayım ‘room’ ‘my room’ ‘in my room’ ‘I am in my room’ Previous work [4] has shown that selectively splitting and removing suffixes from the Turkish text used to train a phrase-base SMT system considerably boosts performances 53 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 in a small scale task like the BTEC. The best segmentation scheme reported in that paper (MS11) mainly includes rules for nominal case and possessive suffixes, plus a few rules on verbal suffixation, namely the splitting of the copula and of the person subject suffixes. In order to better address the rich verbal morphology we adde"
2010.iwslt-evaluation.5,N06-2013,0,0.0267942,"make my cough stop) Table 21 illustrates the segmentation process: the Turkish text is first morphologically analysed and disambiguated ([5], [6]) and the surface form of suffixes replaced by tags as explained in [4]. The rules for suffix splitting or removal are then applied according to the selected segmentation scheme. It can be seen that in some cases the new rules allow for a better correspondence at the level of words between the Turkish sentence and its English translation. However, this doesn’t always corresponds to an improvement in translation quality (see Table 3). It was shown in [7] that the choice of the optimal segmentation scheme for Arabic-English SMT is not a trivial problem and may depend on several factors such as the training data size. Later [8] obtained considerable gains in translation quality by combining unsegmented and segmented Arabic test sentences into a lattice. Given these findings and given that the segmentation space of Turkish is even richer 1o ¨ ks¨ur¨uk: ‘cough’, P1sg: 1st person singular possessive, Acc: accusative, dur-: ‘stop’, Caus: causative, Able: ability, Neg: negation, Prog1: present progressive, A1sg: 1st person singular subject suffix. s"
2010.iwslt-evaluation.5,2010.iwslt-papers.3,1,0.882252,"Missing"
2010.iwslt-evaluation.5,J03-1002,0,0.00410318,"Missing"
2010.iwslt-evaluation.5,P07-1019,0,0.0308228,"Missing"
2010.iwslt-papers.3,W04-3208,0,0.167575,"Missing"
2010.iwslt-papers.3,J05-4003,0,0.17025,"al approaches have recently been proposed in the literature to extract parallel excerpts. Most of the techniques, if not all, share the stages of splitting documents into sentences and of pairing sentences across documents. Methods significantly differ in the successive filtering steps, that can be clustered into two main groups: procedures aiming at deciding if paired sentences are mutual translations or at extracting parallel sub-sentential fragments. In the following, we briefly describe a few works tightly related to our approach, which also report significant performance improvements. In [2] words in the source documents are translated through a bilingual lexicon; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In [3] a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered"
2010.iwslt-papers.3,E09-1003,0,0.197294,"clustered into two main groups: procedures aiming at deciding if paired sentences are mutual translations or at extracting parallel sub-sentential fragments. In the following, we briefly describe a few works tightly related to our approach, which also report significant performance improvements. In [2] words in the source documents are translated through a bilingual lexicon; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In [3] a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered pairs can be tuned by varying the acceptance threshold. The approach in [4] resembles [2] up to the definition of the set of candidate sentence pairs. Instead of deciding whether the two sentences are mutual translations, now they search for parallel fragments using an approach inspired by signal p"
2010.iwslt-papers.3,P06-1011,0,0.49035,"n; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In [3] a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered pairs can be tuned by varying the acceptance threshold. The approach in [4] resembles [2] up to the definition of the set of candidate sentence pairs. Instead of deciding whether the two sentences are mutual translations, now they search for parallel fragments using an approach inspired by signal processing. Using a set of parameters derived from Log-Likelihood-Ratio statistics, each word is annotated with values in [−1, +1] indicating the likelihood that the word has some translations in the other sentence by performing a greedy alignment. This stream of values is then treated as a signal and passed through an averaging filter. Spans that have only positive signal v"
2010.iwslt-papers.3,2007.mtsummit-papers.50,0,0.391793,"ameters derived from Log-Likelihood-Ratio statistics, each word is annotated with values in [−1, +1] indicating the likelihood that the word has some translations in the other sentence by performing a greedy alignment. This stream of values is then treated as a signal and passed through an averaging filter. Spans that have only positive signal values and are longer than a threshold (3 words) are considered more likely to have a translation on the other side. The same process is repeated on the other translation direction, and the resulting fragment pair is assumed to be parallel. Quirk et al. [5] try to overcome some of the limitation of the approach described in [4] in the way parallel fragments are identified. In particular, they propose two generative models for generating noisy target sentences from the source sentences. One model is employed to align words in candidate sentence pairs; fragments are then extracted from alignments by applying simple heuristics. Another model tries to directly generate fragments; in the process, three generation options are competing: source-only fragment, targetonly fragment, or joint source-target fragment. The rational behind this model is that “"
2010.iwslt-papers.3,W04-3225,0,0.225101,"Missing"
2010.iwslt-papers.3,W07-0733,0,0.108535,"Missing"
2010.iwslt-papers.3,2009.mtsummit-posters.17,0,0.0274716,"Missing"
2010.iwslt-papers.3,D07-1103,0,0.0280528,"eam search, histogram pruning...) are omitted, but implemented. Some aspects of the algorithm deserve to be highlighted and commented. First of all, translation probabilities are not used at all. This allows the exploitation of any phrase pair repository, even lacking of probabilities (like multiwordnets), and prevents hypotheses built on low probability phrase pairs from cutting by the beam search. In our specific case, the SMT phrase table used as repository is likely to be noisy. In order to have the repository as clean as possible, the phrase table is pruned via the algorithm described in [10]. Secondly, the use of a phrase-based translation model allows us to cover phrases instead of single words, differently from what is done in similar approaches (e.g. [5] but also [4]). This way the job done in SMT training for discovering reliable phrase pairs is exploited: if such pairs occur in the bilingual input text, they represent a solid anchor for the fragment extraction. Finally, it is worth to noticing that the algorithm permits partial alignment: portions of either source or target texts can remain unaligned. This is achieved by (i) adding dummy translation options (i.e. a target ph"
2010.iwslt-papers.3,2005.mtsummit-papers.11,0,0.0449093,"ble 1: Statistics of the De-En/Ar-En translation/reordering models: size of training texts (running words), dictionary size, and number of phrase pairs in the baselines. from a corpus already cleaned at the sentence level. 5.1. Data Experiments were conducted on two different tasks and language pairs. In the first task, German news are translated into English (De-En) according to the setup established in the Workshop on Statistical Machine Translation of the ACL 2010.2 Parallel training data consist of a small in-domain corpus (News Commentary - NC) and a larger out-of-domain corpus (Europarl [11], version 5 - EP). News-test2008 has been used for development, while news-test2009 (TST09) and news-test2010 (TST10) for testing purposes. As comparable data, we used a set of 25,517 bilingual documents downloaded from the multilingual and pan-European television news channel EuroNews (EN),3 for a total of 4.3 million and 4.7 million German and English words, respectively. The second task involves the translation of news from Arabic into English (Ar-En) in the framework defined by the 2009 NIST evaluation campaign.4 In this case, for development and testing purposes the portions containing ne"
2010.iwslt-papers.3,P07-2045,1,0.010614,"and of both 2008 and 2009 evaluation sets have been employed. We used only one of the parallel resources allowed for the constrained training condition, namely the ISI Arabic-English Automatically Extracted Parallel Text (LDC2007T08). It consists of sentence pairs extracted automatically from the Arabic and English monolingual Gigaword corpora by means of the method described in [2]. For each sentence pair, a confidence score (between 0.5 and 1.0) is provided, which is indicative of its degree of parallelism. 5.2. Baselines The baseline systems are built upon the open-source MT toolkit Moses [12].5 The translation and the lexicalized reordering models have been trained on NC for De-En, and on the subset of ISI corpus containing sentences with confidence score larger than 0.993 (ISI-0.993), for Ar-En. Phrase tables are pruned according to [10]. In all experiments, 6gram LMs have been employed, smoothed with the improved Kneser-Ney technique [13] and computed with the IRSTLM 2 www.statmt.org/wmt10/ 3 www.euronews.net 4 www.itl.nist.gov/iad/mig/tests/mt/2009/ 5 www.statmt.org/moses/ 231 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3"
2010.iwslt-papers.3,P96-1041,0,0.144766,"Missing"
2011.eamt-1.34,E09-1003,0,0.0205869,"entences across documents. Methods significantly differ in the successive filtering steps, and can be clustered into two main groups: procedures aiming at deciding if paired sentences are mutual translations or at detecting and extracting parallel sub-sentential fragments. In (Munteanu and Marcu, 2005) words in the source documents are translated through a bilingual lexicon; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In (Abdul-Rauf and Schwenk, 2009) a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered pairs can be tuned by varying the acceptance threshold. The approach in (Munteanu and Marcu, 2006) resembles (Munteanu and Marcu, 2005) up to the definition of the set of candidate sentence pairs. Instead of deciding whether the two sentences are mutual translations, now they search for parallel fr"
2011.eamt-1.34,C10-2010,0,0.0308503,"the estimation of SMT models. type web parallel sent. web parallel frag. total total clean web monol. sent. |W | ar it 1.4M 1.4M 1.8M 1.6M 3.0M 3.0M 2.8M 1.06G trained models LM TM RM LM Table 5: Training data for ArIt models. English-to-Italian (EnIt) Unlike in the case of the Arabic–Italian pair, many Web sites publish news in English and Italian which are (almost) parallel. We have been able to collect so far sentences for a total of about 24M words per side. Document pairs have been split into sentences on strong punctuation and sentence alignment has been performed by means of Gargantua (Braune and Fraser, 2010). On the contrary, from sites where documents are at most comparable, fragments (2.8M words) have been mined via the procedures described in Sections 4.1 and 4.2. In addition, Europarl8 and JRC-Acquis9 parallel corpora (70M words) have been employed. The LM of the ArIt system has been re-used. Table 6 provides some statistics on texts employed for the estimation of these specific SMT models. type web parallel sent. web parallel frag. total total clean ep5+acquis clean web monol. sent. |W | en it 24.2M 24.1M 2.7M 2.8M 27.0M 23.3M 23.5M 70.0M 70.0M 1.06G trained models LM TM RM TM RM LM Devsets"
2011.eamt-1.34,2010.iwslt-papers.3,1,0.845686,"te sentence pairs; fragments are then extracted from alignments by applying simple heuristics. Another model tries to directly generate fragments; in the process, three generation options are competing: source-only fragment, target-only fragment, or joint source-target fragment. The rational behind this model is that “the probability of generating source and target fragments jointly should be more likely than generating them independently if and only if they are parallel”. The latter model is definitely more complex than the former one, although their impact on SMT performance is similar. In (Cettolo et al., 2010), a method is defined for extracting fragments which was new in some aspects and that is summarized in Section 4.2. 3 Evaluation Sets Evaluation and comparison of MT systems are still open issues to which much attention and many efforts have been directed in the last years. One of the few certainties is the use of publicly available evaluation sets. Nowadays, several MT evaluation campaigns are held every year which release evaluation sets. Unfortunately, no evaluation set built on news texts has been made available so far for Arabic–Italian translation. To fill this gap, we decided to add two"
2011.eamt-1.34,P07-2045,1,0.0102695,"Missing"
2011.eamt-1.34,J05-4003,0,0.0790055,"t al., 2010). Once document pairs are available, the problem is the detection of actual parallel text inside them. Several approaches have recently been proposed in the literature to extract such parallel excerpts. Most of the techniques, if not all, share the stages of splitting documents into sentences and of pairing sentences across documents. Methods significantly differ in the successive filtering steps, and can be clustered into two main groups: procedures aiming at deciding if paired sentences are mutual translations or at detecting and extracting parallel sub-sentential fragments. In (Munteanu and Marcu, 2005) words in the source documents are translated through a bilingual lexicon; all possible sentence pairs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In (Abdul-Rauf and Schwenk, 2009) a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target si"
2011.eamt-1.34,P06-1011,0,0.0293232,"rs of the two documents are passed first through a word-overlap filter and then classified as parallel or not by a maximum entropy classifier trained on (a small amount of) parallel sentences. In (Abdul-Rauf and Schwenk, 2009) a SMT system, trained on a small amount of parallel data, is used to directly translate the source side of the documents. Then, instead of the maximum entropy classifier, WER and TER scores are computed at the sentence level by comparing the translations with the target side; the amount of filtered pairs can be tuned by varying the acceptance threshold. The approach in (Munteanu and Marcu, 2006) resembles (Munteanu and Marcu, 2005) up to the definition of the set of candidate sentence pairs. Instead of deciding whether the two sentences are mutual translations, now they search for parallel fragments using an approach inspired by signal processing. Using a set of parameters derived from Log-Likelihood-Ratio statistics, each word is annotated with values in [−1, +1] indicating the likelihood that the word has some translations in the other sentence by performing a greedy alignment. This stream of values is then treated as a signal and passed through an averaging filter. Spans that have"
2011.eamt-1.34,2007.mtsummit-papers.50,0,0.0176098,"meters derived from Log-Likelihood-Ratio statistics, each word is annotated with values in [−1, +1] indicating the likelihood that the word has some translations in the other sentence by performing a greedy alignment. This stream of values is then treated as a signal and passed through an averaging filter. Spans that have only positive signal values and are longer than a threshold (3 words) are considered more likely to have a translation on the other side. The same process is repeated on the other translation direction, and the resulting fragment pair is assumed to be parallel. Quirk et al. (Quirk et al., 2007) try to overcome some of the limitations of the approach described in (Munteanu and Marcu, 2006) in the way paralset #sentences eval08-NW eval09-NW 813 586 Arabic |W | |V | 21.9k 7.8k 17.5k 6.4k English |W | |V | 29.1k 4.9k 23.1k 3.9k French |W | |V | 33.2k 4.9k 26.7k 4.4k Italian |W | |V | 32.0k 5.7k 25.1k 4.8k Table 1: Statistics of the dev/test sets for the pivoting task. Texts are tokenized. For the English side, 4 manual translations (references) are available: average values are reported. |W |stands for “running words”, |V |for “vocabulary size”. lel fragments are identified. In particul"
2011.eamt-1.34,C10-1124,0,0.0120521,"ng the same content in different languages. Although these documents are not parallel, it often happens that some portions of them are mutual translations to some extent. In the last years, much effort has been devoted by the research community to the effective exploitation of such data for SMT. The first problem to face is the alignment of multilingual documents reporting the same news. This problem has been rarely investigated systematically, as the usual pairing strategies aim to keep low the missing rate, that is to reward the recall. One of the most valuable methods is that presented in (Uszkoreit et al., 2010). There, all non-English documents are translated into English through a initial, even low-quality, translation system. Documents are then paired in two steps: the first generates a set of candidate pairs of documents sharing at least a certain number of rare features. This step is made linear in the number of input documents by setting a threshold defining such rare features. In the second step, a computationally more expensive and fine grained comparison is performed for deciding whether such document pairs are comparable or not. Pairing multilingual documents is also the goal of (Steinberge"
2011.eamt-1.34,P09-1018,0,0.145031,"el; on the target side, specific LMs were estimated as well; on the contrary, a single reordering model was built on such texts. Concerning the exploitation of monolingual data, again specific LMs were built on each of the 6 provided sets (English Gigaword 3rd Edition); also in this case, a single model has been defined by linearly interpolating the total of 11 LMs. Table 7 provides some statistics on texts actually employed for the estimation of SMT models. |W | ar/en 0.6M 0.9M 6.2M 24.3M 115.2M 142.2M 3.6G 5.2 The pivoting technique employed here is the composition, also called transfer in (Wu and Wang, 2009), consisting in the translation of the source language into the pivot language, and of this into the target language. Table 8 shows the performance obtained by composing the proper direct systems presented above, together with that by composing the two corresponding Google Translate systems. Table 6: Training data for EnIt models. corpus of-the-art SMT systems is possible (English-toItalian and Arabic-to-English directions); otherwise (Arabic-to-Italian) the need arises for alternative approaches to achieve acceptable quality. www.statmt.org/europarl wt.jrc.it/lt/Acquis/ 254 ArEn ⊗ EnIt ArEn-g"
2011.eamt-1.34,2010.iwslt-evaluation.1,1,\N,Missing
2011.mtsummit-papers.1,W08-0304,0,0.0462071,"rload, as multiple translations of the input are needed. On the other side, the average system well competes with the system combination, without any computational additional cost. 2 Related Work Since its ﬁrst appearance, the MERT procedure (Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from origi"
2011.mtsummit-papers.1,2004.iwslt-papers.2,1,0.704303,"he system combination, without any computational additional cost. 2 Related Work Since its ﬁrst appearance, the MERT procedure (Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e"
2011.mtsummit-papers.1,D08-1024,0,0.0531715,"Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavi"
2011.mtsummit-papers.1,P11-2031,0,0.108123,"ion of models. Interpolation weights are typically computed by means of iterative procedures which aim at maximizing a given scoring function. Unfortunately, such a function is deﬁnitely non-convex; hence, only local optima can be reached. Moreover, it has been observed that the commonly used optimization procedure, the N-best minimum error rate training (simply MERT hereafter) (Och, 2003), is quite unstable. In the last years, many efforts have been devoted for making the procedure or its results more reliable. Recently, a deep investigation of the optimizer instability has been presented by Clark et al. (2011). In that work, experimental evidence of the instability problems affecting optimizers is shown; then, statistical tools 32 are selected for making possible both a quantitative evaluation of the optimization process of single systems and a fair comparison of two systems, somehow independent from the optimization process. The work ends with some recommendations: for instance, run optimization at least three times; use additional held-out test sets for manual analysis of translations in order to select the best optimization (better: “more reliable”) among those available. By taking that investig"
2011.mtsummit-papers.1,W02-1001,0,0.0979044,"MERT procedure (Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (200"
2011.mtsummit-papers.1,P08-2010,0,0.0192258,"ct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weights. BoostedMERT (Duh and Kirchhoff, 2008) uses one single set of models, with different sets of interpolation weights. Each log-linear model is treated as a “weak learner”, and boosting is used to combine such weak learners for N-best re-ranking. On the contrary, we either produce a new log-linear model"
2011.mtsummit-papers.1,W09-0439,0,0.0317607,"ive function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti"
2011.mtsummit-papers.1,W11-2130,0,0.0127143,"cognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et a"
2011.mtsummit-papers.1,D08-1011,0,0.015922,", Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weights. BoostedMERT (Duh and Kirchhoff, 2008) uses one single set"
2011.mtsummit-papers.1,W10-1744,0,0.0182485,"lso to high values of the test set score function. The centroid should then fall inside that region both for the development and for the test set, increasing the chance of stabilizing the performance on the test set. It is worth noticing that this approach requires additional computation effort only in tuning stage, as multiple optimization runs have to be performed. System combination (sysComb) System combination is the second solution we propose for smoothing the outputs of various optimizer samples. For combining the outputs of the available systems, we used the software1 developed at CMU (Heaﬁeld and Lavie, 2010). It works at the word level and smartly allows “the synthesis of new word orderings”. The scheme includes several stages. Hypotheses are aligned in pairs using the publicly available METEOR (Banerjee and Lavie, 2005) aligner. Then, on these alignments a search space is deﬁned, which is explored by a beam search decoding. Hypotheses are scored using a linear interpolation of features, including the LM score, the hypothesis length and the average n-gram length found in the LM. Interpolation weights are tuned using Z-MERT (Zaidan, 2009). 1 Available at http://kheaﬁeld.com/code/mt/ 34 This approa"
2011.mtsummit-papers.1,2008.amta-srw.3,0,0.0156974,"x, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained"
2011.mtsummit-papers.1,P05-3026,0,0.0182981,"iang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combine"
2011.mtsummit-papers.1,P07-2045,1,0.0123236,"se alignments a search space is deﬁned, which is explored by a beam search decoding. Hypotheses are scored using a linear interpolation of features, including the LM score, the hypothesis length and the average n-gram length found in the LM. Interpolation weights are tuned using Z-MERT (Zaidan, 2009). 1 Available at http://kheaﬁeld.com/code/mt/ 34 This approach requires the translation of the test set by each optimizer sample to be combined, besides the computational cost of the combination of translations itself. 4 Experiments The SMT systems are built upon the open-source MT toolkit Moses2 (Koehn et al., 2007). The translation and the lexicalized reordering models have been trained on parallel data. The LMs have been estimated via the IRSTLM toolkit (Federico et al., 2008) either on the monolingual data, when available, or on the target side of the parallel data; they are smoothed through the improved Kneser-Ney technique (Chen and Goodman, 1999). The weights of the log-linear interpolation model have been optimized on the development sets by means of the standard MERT procedure provided within the Moses toolkit: different realizations of tuned systems (optimizer samples) have been obtained by mult"
2011.mtsummit-papers.1,D07-1105,0,0.0173459,"evel combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weights. BoostedMERT (Duh and Kirchhoff, 2008) uses one single set of models, with different sets of interpolation weights. Each log-linear model is treated as a “weak learner”, and boosting is used to combine such weak learners for N-best re-ranking. On the contrary, we either"
2011.mtsummit-papers.1,E06-1005,0,0.0215326,"t al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of"
2011.mtsummit-papers.1,C08-1074,0,0.0488716,"for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008),"
2011.mtsummit-papers.1,P03-1021,0,0.396757,"ffective in smoothing instability, but also that the average system well competes with the more expensive system combination. 1 Introduction Statistical machine translation (SMT) systems feature a log-linear interpolation of models. Interpolation weights are typically computed by means of iterative procedures which aim at maximizing a given scoring function. Unfortunately, such a function is deﬁnitely non-convex; hence, only local optima can be reached. Moreover, it has been observed that the commonly used optimization procedure, the N-best minimum error rate training (simply MERT hereafter) (Och, 2003), is quite unstable. In the last years, many efforts have been devoted for making the procedure or its results more reliable. Recently, a deep investigation of the optimizer instability has been presented by Clark et al. (2011). In that work, experimental evidence of the instability problems affecting optimizers is shown; then, statistical tools 32 are selected for making possible both a quantitative evaluation of the optimization process of single systems and a fair comparison of two systems, somehow independent from the optimization process. The work ends with some recommendations: for insta"
2011.mtsummit-papers.1,N07-1029,0,0.020768,"hoice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weigh"
2011.mtsummit-papers.1,P07-1040,0,0.0185003,"hoice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weigh"
2011.mtsummit-papers.1,W08-0329,0,0.0192641,"(2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct selection from original outputs of single SMT systems (Sim et al. (2007), Hildebrand and Vogel (2008)), or phrase- or word-level combination, i.e. the synthesis of a (possibly) new output joining portions of the original outputs (Bangalore (2001), Jayaraman and Lavie (2005), Matusov et al. (2006), Rosti et al. (2007a), Rosti et al. (2007b), Ayan et al. (2008), He et al. (2008), Rosti et al. (2008)). These works focus on the combination of multiple machine translation systems based on different models and paradigms. Investigation similar to ours has been done by Macherey and Och (2007), and Duh and Kirchhoff (2008). Macherey and Och (2007) analyzed the combination of multiple outputs obtained by using one decoding engine with different models, trained on different conditions. Instead, we employ the same engine and the same models; the variable part of the combined systems is the set of interpolation weights. BoostedMERT (Duh and Kirchhoff, 2008) uses one single set of models, with diffe"
2011.mtsummit-papers.1,2009.iwslt-evaluation.12,0,0.0351409,"Missing"
2011.mtsummit-papers.1,D07-1055,0,0.0210922,"nt computational overload, as multiple translations of the input are needed. On the other side, the average system well competes with the system combination, without any computational additional cost. 2 Related Work Since its ﬁrst appearance, the MERT procedure (Och, 2003) has been recognized as unstable. The author analyzed the problem and proposed a way for smoothing the objective function to be optimized. Thereafter, many works tried to handle and reduce instability of the MERT procedure, following different approaches including regularization/smoothing of the objective error surface (e.g. Zens et al. (2007), Cer et al. (2008)), improvement in the optimization algorithms – Simplex, Powell, Coordinate Descent, etc. – (e.g. Cettolo and Federico (2004), Cer et al. (2008)), usage of online learning techniques – Perceptron, MIRA – (e.g. Collins (2002), Chiang et al. (2008), Haddow et al. (2011)), better choice of the random starting points (e.g. Moore and Quirk (2008), Foster and Kuhn (2009)). Also system combination has been already applied to SMT. One research line takes n-best translations of single systems, and produces the ﬁnal output by means of either sentence-level combination, i.e. a direct s"
2012.eamt-1.60,2003.mtsummit-papers.9,0,0.00847487,"Missing"
2012.eamt-1.60,N04-4038,0,0.00519852,"e site will also release unofficial benchmarks for many other language pairs. 4 4.1 4.1.1 265 Data Experiments were performed on data supplied by the organizers of the IWSLT 2011 evaluation campaign for the MT track,4 who asked participants to automatically translate talks from Arabic to English, from Chinese to English and from English to French. For developing the baselines, only texts from the TED domain were employed, i.e. no additional out-of-domain resources were used. Different preprocessings were performed depending to the language: Arabic and Chinese were segmented by means of AMIRA (Diab et al., 2004) and the Stanford Chinese Segmenter (Tseng et al., 2005), respectively; the tokenizer script released together with the Europarl corpus (Koehn, 2005) was applied to other languages. The same partitioning of the evaluation campaign in terms of parallel training data, development (dev2010, tst2010) and test (tst2011) sets has been adopted: Tables 4 and 5 report some statistics of such texts. Baselines In this section, we present results on some benchmarks that we obtained by training MT baseline systems on the available TED Talks data. The aim is to provide MT scientists with reference results t"
2012.eamt-1.60,eisele-chen-2010-multiun,0,0.0802012,"e the main source of information to infer parameter values of the employed mathematical model. In statistical machine translation (SMT), learning is performed on parallel texts, i.e. documents, sentences or even fragments of sentences with their translation(s). Large amounts of in-domain parallel data are usually required to properly train translation and reordering models. Unfortunately, parallel data are a scarce resource, which are freely available only for some language pairs and for few, very specific domains. c 2012 European Association for Machine Translation. 261 For example, MultiUN (Eisele and Chen, 2010) provides large parallel texts (300 million words) but for only 6 languages; Europarl (Koehn, 2005) consists of the translation into most European languages of the proceedings of the European Parliament (at most 50 million words); JRC-Acquis1 comprises the total body of European Union law applicable to the Member States, written in 22 European languages (35 million words); other smaller parallel corpora in specific domains are included in OPUS (Tiedemann, 2009) for various languages. On the other hand, it is unfeasible for research laboratories to cover all possible needs in terms of parallel"
2012.eamt-1.60,P07-2045,1,0.0608402,"Missing"
2012.eamt-1.60,2005.mtsummit-papers.11,0,0.528374,"machine translation (SMT), learning is performed on parallel texts, i.e. documents, sentences or even fragments of sentences with their translation(s). Large amounts of in-domain parallel data are usually required to properly train translation and reordering models. Unfortunately, parallel data are a scarce resource, which are freely available only for some language pairs and for few, very specific domains. c 2012 European Association for Machine Translation. 261 For example, MultiUN (Eisele and Chen, 2010) provides large parallel texts (300 million words) but for only 6 languages; Europarl (Koehn, 2005) consists of the translation into most European languages of the proceedings of the European Parliament (at most 50 million words); JRC-Acquis1 comprises the total body of European Union law applicable to the Member States, written in 22 European languages (35 million words); other smaller parallel corpora in specific domains are included in OPUS (Tiedemann, 2009) for various languages. On the other hand, it is unfeasible for research laboratories to cover all possible needs in terms of parallel texts by resorting to professional translators, given their high cost. The data available at the TE"
2012.eamt-1.60,P11-2031,0,\N,Missing
2012.eamt-1.60,O07-5005,0,\N,Missing
2012.eamt-1.60,I05-3027,0,\N,Missing
2012.eamt-1.60,2011.iwslt-evaluation.1,1,\N,Missing
2012.eamt-1.60,2010.iwslt-evaluation.1,1,\N,Missing
2013.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.863584,"al of 217 primary runs submitted. All runs were evaluated with objective metrics on a current test set and two progress test sets, in order to compare the progresses against systems of the previous years. In addition, submissions of one of the official machine translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been now running for a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9]. The 2013 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Machine translation (MT), i.e. the translation of a polished transcript into another language, • Spoken language translation (SLT), that addressed the conversion and translation"
2013.iwslt-evaluation.1,2012.eamt-1.60,1,0.897031,"on tracks, many other optional translation directions were also offered. Optional SLT directions were from English to Spanish, Portuguese (B), Italian, Chinese, Polish, Slovenian, Arabic, and Persian. Optional MT translation directions were: English from/to Arabic, Spanish, Portuguese (B), Italian, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, and Russian. For each official and optional translation direction, training and development data were supplied by the organizers through the workshop’s website. Major parallel collections made available to the participants were the WIT3 [10] corpus of TED talks, all data from the WMT 2013 workshop [11], the MULTIUN corpus [12], and the SETIMES parallel corpus [13]. A list of monolingual resources was provided too, that includes both freely available corpora and corpora available from the LDC. Test data were released at the begin of each test period, requiring participants to return one primary run and optional contrastive runs within one week. The schedule of the evaluation was organized as follows: June 8, release of training data; Sept 2-8, ASR test of period; Sept 9-15, SLT test period; Oct 7-13, MT test period; Oct 7-20, test"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.21,0,0.0238082,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.11,0,0.0818098,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.5,0,0.0447354,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.6,0,0.0540129,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.9,0,0.0348889,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.23,0,0.0935135,"Missing"
2013.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.125368,"y, the standard dev2010 and tst2010 development sets have been released as well. Tables 2 and 3 provide statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear inte"
2013.iwslt-evaluation.1,N04-4038,0,0.0999494,"tion purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official languag"
2013.iwslt-evaluation.1,P07-2045,1,0.010973,"This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official language pairs task MTEnF r MTDeEn MTEnDe data set train dev2010 tst2010 tst2011 tst2012 tst2013 train dev2010 tst2010 dev2012 tst2013 train dev2010 tst2010 tst20"
2013.iwslt-evaluation.1,2012.amta-papers.22,1,0.822673,"Missing"
2013.iwslt-evaluation.1,2006.amta-papers.25,0,0.207086,"Missing"
2013.iwslt-evaluation.1,J93-3001,0,0.516669,"Missing"
2013.iwslt-evaluation.1,W05-0908,0,0.0754408,"Missing"
2013.iwslt-evaluation.1,1993.eamt-1.1,0,0.4595,"Missing"
2013.iwslt-evaluation.16,2012.eamt-1.60,1,0.8976,"neous speech and heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been v"
2013.iwslt-evaluation.16,2005.mtsummit-papers.11,1,0.078384,"nd heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful"
2013.iwslt-evaluation.16,eisele-chen-2010-multiun,0,0.0452925,"ous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful in contribut"
2013.iwslt-evaluation.16,E06-1005,1,0.921596,"e-scale evaluation campaigns like IWSLT and WMT in recent years, thereby demonstrating their ability to continuously enhance their systems and promoting progress in machine translation. Machine translation research within EU-BRIDGE has a strong focus on translation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. The work described here is an attempt to attain translation quality beyond strong single system performance via system combination [11]. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in other projects, e.g. in the Quaero programme [12, 13]. Within EU-BRIDGE, we built combined system setups for text translation of talks from English to French as well as from German to English. We found that the combined translation engines of RWTH, UEDIN, KIT, and FBK systems are very effective. In the rest of the paper we will give some insight into the technology behind the combined engines which have been used to produce the joint EU-BRIDGE submission to the IWSLT 2013 MT track"
2013.iwslt-evaluation.16,P02-1040,0,0.0892795,"-BRIDGE submission to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SR"
2013.iwslt-evaluation.16,2006.amta-papers.25,0,0.15922,"sion to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [2"
2013.iwslt-evaluation.16,W10-1738,1,0.880309,"iques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment w"
2013.iwslt-evaluation.16,popovic-ney-2006-pos,1,0.929216,"ll available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram"
2013.iwslt-evaluation.16,P03-1021,0,0.129032,"ns from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all avai"
2013.iwslt-evaluation.16,P12-1031,0,0.10415,"For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discri"
2013.iwslt-evaluation.16,P10-2041,0,0.0805135,"setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 41 of the French Gigaword Second Edition corpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a se"
2013.iwslt-evaluation.16,D08-1089,0,0.117877,"orpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of"
2013.iwslt-evaluation.16,P07-2045,1,0.0125349,"EU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a"
2013.iwslt-evaluation.16,W13-2212,1,0.868834,"m and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation"
2013.iwslt-evaluation.16,W11-2123,0,0.0545914,"tion by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequen"
2013.iwslt-evaluation.16,P11-1105,1,0.916011,"d on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev"
2013.iwslt-evaluation.16,D09-1022,1,0.892821,"n for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resu"
2013.iwslt-evaluation.16,2012.iwslt-papers.17,1,0.860668,"em [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate resu"
2013.iwslt-evaluation.16,D13-1138,1,0.815085,"based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running w"
2013.iwslt-evaluation.16,N04-1022,0,0.487773,"atistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence m"
2013.iwslt-evaluation.16,W12-2702,0,0.051709,"cribed in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RW"
2013.iwslt-evaluation.16,P07-1019,0,0.222647,"ranslation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown wo"
2013.iwslt-evaluation.16,E03-1076,1,0.900834,"data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective"
2013.iwslt-evaluation.16,N12-1047,0,0.148125,"penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown word clusters, these setups were not finished in time for the contribution to the EU-BRIDGE system combination. language models trained on WIT3 , Europarl, News Commentary, 109 , and Common Crawl by minimizing the perplexity on the development data. For the class-based language model, KIT utilized in-domain WIT3 data with 4grams and 50 clusters. In addition, a 9-gram POS-based language model derived fr"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.9,1,0.925869,"ge model derived from LIA POS tags [55] on all monolingual data was applied. KIT optimized the log-linear combination of all these models on the provided development data using Minimum Error Rate Training [20]. 4. Karlsruhe Institute of Technology The KIT translations have been generated by an in-house phrase-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, K"
2013.iwslt-evaluation.16,2007.tmi-papers.21,0,0.422618,"e-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED"
2013.iwslt-evaluation.16,W09-0435,1,0.918776,"Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection"
2013.iwslt-evaluation.16,W13-0805,1,0.889592,"ra for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase tabl"
2013.iwslt-evaluation.16,W08-1006,0,0.169177,"SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a"
2013.iwslt-evaluation.16,2005.iwslt-1.8,1,0.888473,"t. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context feat"
2013.iwslt-evaluation.16,W08-0303,1,0.796238,"word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to bet"
2013.iwslt-evaluation.16,2012.amta-papers.19,1,0.890564,"and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-doma"
2013.iwslt-evaluation.16,W11-2124,1,0.885306,"ated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-"
2013.iwslt-evaluation.16,W13-2264,1,0.887808,"se trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, K"
2013.iwslt-evaluation.16,2012.iwslt-papers.3,1,0.886049,"criminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a"
2013.iwslt-evaluation.16,E99-1010,0,0.0594124,"ed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two F"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.18,1,0.928081,"el, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two French language models (LMs), as well as distortion, word, and phrase penalties. In order to focus it on TED specific domain and genre, and to reduce the size of the system, data selection by means of IRSTLM toolkit [57] was performed on the whole parallel English→French corpus, using the WIT3 training data as in-domain data. Different amount of data are selected from each available corpora but the WIT3 data, for a total of 66 M English running words. Two TMs and two RMs were trained on WIT3 and selected data, separately, and combined using the fil"
2013.iwslt-evaluation.16,W05-0909,0,0.0593782,"es which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. [60]. This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR [61]. A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses. Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm. The translation with the best total score within the"
2013.iwslt-evaluation.16,W12-3140,1,\N,Missing
2013.iwslt-evaluation.16,J03-1002,1,\N,Missing
2013.iwslt-evaluation.16,C12-3061,1,\N,Missing
2013.iwslt-evaluation.16,federico-etal-2012-iwslt,1,\N,Missing
2013.iwslt-evaluation.16,2011.iwslt-evaluation.1,1,\N,Missing
2013.iwslt-evaluation.16,W13-2223,1,\N,Missing
2013.iwslt-evaluation.16,N13-1073,0,\N,Missing
2013.mtsummit-papers.4,P11-2031,0,0.0525206,"Missing"
2013.mtsummit-papers.4,D10-1044,0,0.0660693,"Missing"
2013.mtsummit-papers.4,P07-2045,1,0.00719976,"er; in fact: – there is a significant mismatch between IT-docs and the TM – the customer specific project TM-prjct matches the TM, not generic IT-docs • the IT-docs/TM mismatch can be reduced by properly selecting a portion of the TM. 4 Lab Test Results Lab tests have been performed on data sets described in Section 3. Performance are given in terms of BLEU and TER, computed by means of the MultEval script (Clark et al., 2011) that also provides the standard deviation σ, and of GTM.5 4.1 Baseline SMT for the IT domain An IT baseline system has been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the parallel training data available (Table 1); a 6-gram LM smoothed through the improved Kneser-Ney technique (Chen and Goodman, 1999) is estimated on the target side via the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model are optimized by means of the standard MERT procedure provided within the Moses toolkit. For the experiments, the set of IT-docs has been split into three equally sized blocks: the first is used for data selection (employed for adaptation, not for baselines), the"
2013.mtsummit-papers.4,D09-1074,0,0.0594647,"Missing"
2013.mtsummit-papers.4,P10-2041,0,0.0437632,"Missing"
2013.mtsummit-papers.4,W08-0320,0,0.0588141,"Missing"
2013.mtsummit-papers.4,2012.amta-papers.19,0,0.0312502,"nt sizes and content is rather typical. One way to face this combination issue is the fill-up technique, initially proposed by Nakov (2008) and then refined by Bisazza et al. (2011). Fill-up effectively exploits background knowledge to improve translation and distortion models coverage, while preserving the more reliable information coming from the foreground corpus. In practice, the background phrase table is merged with the foreground phrase table by adding only phrase pairs that do not appear in the foreground table. While performing at least as good as other popular adaptation techniques (Niehues and Waibel, 2012), fill-up approach builds models that are more compact and easier to tune by means of the minimum error rate training procedure. 2.3 Mixture LMs As concerns the LM adaptation, we employed the mixture of LMs since it is a well-established and good-performing method. The mixture model can be used to combine one or more background LMs with a foreground LM representing new features of the language we want to include (Federico and Bertoldi, 2004). The technique consists of the convex combination of the LMs; the mixture weights are estimated on the training data of the foreground LM by applying a cr"
2013.mtsummit-papers.4,steinberger-etal-2006-jrc,0,0.0700722,"n of the LMs; the mixture weights are estimated on the training data of the foreground LM by applying a cross-validation scheme that simulates the occurrence of new n-grams. The method is available in the IRSTLM toolkit (Federico et al., 2008). 28 3 #seg Data for Development For our experiments we relied on existing language resources, including parallel corpora and translation memories. For the IT domain, in addition to small publicly available corpora, proprietary data sets were employed (software documentation in general). For the Legal domain, the publicly available JRC-Acquis collection (Steinberger et al., 2006) was used, which mostly includes EU legislative texts translated into 22 languages. More details are provided in the next sections. 3.1 #tgt wrds TM+OPUS all entries (wd) 5.5M 63.8M 66.6M no duplicates (wod) 1.9M 27.8M 29.0M IT-docs 4.1k 56.0k 60.5k data-sel 1.4k 18.0k 19.3k dev 1.4k 21.1k 22.9k test 1.4k 16.9k 18.3k TM-prjct 1.8k 18.0k 18.7k dev 800 5.1k 5.4k test 989 12.9k 13.3k IT domain Most of text corpora for this domain were provided by the industrial partner of the MateCat project. In particular, we employed the following resources: • Translation Memory (TM): a large collection of para"
2013.mtsummit-papers.4,D11-1033,0,0.0376481,"Missing"
2013.mtsummit-papers.4,2011.iwslt-evaluation.18,1,0.885137,"of the background corpus. This score is the difference between the cross-entropy calculated with the foreground LM and the crossentropy calculated with the background LM. The background sentences are then ordered according to this score. The selection of useful sentences Fill-up for phrase-based SMT adaptation Given the scarcity of parallel linguistic resources, in SMT training, the need of combining data of parallel corpora of different sizes and content is rather typical. One way to face this combination issue is the fill-up technique, initially proposed by Nakov (2008) and then refined by Bisazza et al. (2011). Fill-up effectively exploits background knowledge to improve translation and distortion models coverage, while preserving the more reliable information coming from the foreground corpus. In practice, the background phrase table is merged with the foreground phrase table by adding only phrase pairs that do not appear in the foreground table. While performing at least as good as other popular adaptation techniques (Niehues and Waibel, 2012), fill-up approach builds models that are more compact and easier to tune by means of the minimum error rate training procedure. 2.3 Mixture LMs As concerns"
2013.mtsummit-papers.4,I08-2088,0,0.0718824,"Missing"
2013.mtsummit-papers.5,W12-3155,1,0.837488,"eover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one of them or to translate the source segment from scratch. Completed segments represent"
2013.mtsummit-papers.5,2011.iwslt-evaluation.18,1,0.249691,"e matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one of them or to translate the source segment f"
2013.mtsummit-papers.5,2010.iwslt-papers.3,1,0.832754,"-pairs can be inserted, and (ii) scores of all entries are modified when new pairs are added. All entries are associated with an age, corresponding to the time they were actually 37 inserted, and scored accordingly. Each new insertion causes the ageing of the existing phrase pairs and hence their rescoring; in case of re-insertion of a phrase pair, the old value is overwritten. Phrase pairs are scored based on a negative exponential decaying function. For each segment pair, a set of phrase pairs are extracted from the partial alignment provided by the constrained search algorithm described by Cettolo et al. (2010). The procedure, detailed in (W¨aschle et al., 2013), extracts both already “known” and “new” pairs; the latter can provide translation options for OOVs and phrases including OOVs. All the extracted phrase pairs are simultaneously added to the local translation model by feeding the decoder with an XML-tag like: &lt;dlt cbtm=“The crude face of domination . Le visage rustre de la domination . |crude rustre |· · · |domination la domination |face visage”/&gt; The pair (x, y) consisting of the whole segments is also added to mimic the behaviour of a translation memory. During decoding, translation altern"
2013.mtsummit-papers.5,2012.eamt-1.60,1,0.823386,"(Federico et al., 2012) were considered, i.e. the translation of talks from Arabic to English and from English to French. 4.1.1 Training Data The training data for the IT domain mostly consist of data extracted from a translation memory built during the execution of translation projects commissioned by several commercial companies. In addition, parallel texts from the OPUS corpus (Tiedemann, 2012) were are also included. Training data of the Legal domain come from Version 3.0 of the JRC-Acquis collection (Steinberger et al., 2006), while TED training data are those released through the WIT3 (Cettolo et al., 2012) website for the IWSLT 2012 evaluation campaign (Federico et al., 2012). The reader may again refer to Table 1 for statistics on the actual corpora employed for training. Evaluations were performed on both proprietary and publicly available data, involving the translation of documents from three domains, namely Information Technology (IT), Legal (LGL) and TED talks,3 and for different language pairs. Table 1 summarizes the experimental framework in terms of data type. Commercial data were provided by the industrial partner of the MateCat project and were collected during the real use of CAT to"
2013.mtsummit-papers.5,2013.mtsummit-papers.4,1,0.805,"nguage pairs. Table 1 summarizes the experimental framework in terms of data type. Commercial data were provided by the industrial partner of the MateCat project and were collected during the real use of CAT tools by professional translators. Public data, which allow to 4.1.2 Evaluation Data Evaluation on IT and Legal domains has been performed on data collected in a two-day field test. During the first day (D0), MT suggestions came from baseline SMT systems (Section 4.4); during the second day (D1), they came from static SMT systems adapted to the post-edits of the first day as described in (Cettolo et al., 2013). For the IT domain, the text to be translated was taken from a software user manual. Concerning the Legal domain, the text was taken from a recent motion for a European Parliament resolution published on the EUR-Lex platform. Statistics on the test documents translated during the field test are reported in Table 2 (rows D0 and D1); they refer to tokenized texts. Figures on the source side (English) refer to the texts the users are requested to translate; figures on the target side (Italian) refer to the references, whatever they are actual post-edited texts or new translations. The additional"
2013.mtsummit-papers.5,P11-2031,0,0.0254423,"systems were developed with the Moses toolkit (Koehn et al., 2007). Translation and lexicalized reordering models were trained on the parallel training data; 6-gram (for IT and Legal systems) and 5-gram (for TED systems) LMs with improved Kneser-Ney smoothing (Chen and Goodman, 1999) were estimated on the target side of the training parallel data with the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model were optimized via the MERT procedure provided with Moses. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script (Clark et al., 2011) that also provides the standard deviation σ, and of GTM8 Here some specific features of systems developed for each task: IT/Legal baselines: different weights were used in the two days of the field test, 8 nlp.cs.nyu.edu/GTM 40 task IT LGL system baseline +cache +context baseline +cache +context BLEU (σ) 44.69 (1.67) 47.25 (1.81) 47.89 (1.82) 47.66 (2.13) 47.69 (2.05) 46.58 (2.05) D0 TER (σ) 36.34 (1.30) 34.83 (1.34) 34.61 (1.44) 34.69 (1.71) 33.94 (1.63) 34.74 (1.65) GTM 74.59 76.36 76.99 73.88 75.04 74.49 BLEU (σ) 41.06 (1.57) 44.61 (1.72) 48.04 (1.93) 47.42 (2.11) 47.57 (2.09) 48.07 (2.16)"
2013.mtsummit-papers.5,W07-0717,0,0.0370258,"ptions whose source side matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one of them or to translat"
2013.mtsummit-papers.5,W07-0733,0,0.0223156,"h, fed by all translation options whose source side matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one"
2013.mtsummit-papers.5,P07-2045,1,0.0105799,"ce segment xt is received . . . [optional step] . . . a translation yˆt is computed with Mt a post-edited translation yt is received a new system Mt+1 is created by adapting Mt with features extracted from (xt , yt ) Step 5 is implemented via a caching mechanism that allows us to define and dynamically adapt local (with respect to the document) models that are combined during decoding with the global SMT models estimated on the training data. In the following, we present how the local cache-based models are defined and adapted under the wellknown phrase-based SMT setting of the Moses toolkit (Koehn et al., 2007). They will be included in a future release of the toolkit. The optional step 2 is additional with respect to the basic online learning procedure and comprises the updating of Mt with context features, as described in more detail in Section 3.3. 3.1 TM Adaptation The pair (x, y) composed of a source segment and its post-edited translation, is exploited to update a local translation model. This model is intended for integrating new translation alternatives suggested by the user and for rewarding those approved, with the ultimate goal of translating the successive segments more consistently with"
2013.mtsummit-papers.5,N10-1062,0,0.161242,"-edition; explicit, possibly partial, phrase-alignment is obtained via an efficient constrained search, fed by all translation options whose source side matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translat"
2013.mtsummit-papers.5,W11-2122,0,0.0186151,"ibly partial, phrase-alignment is obtained via an efficient constrained search, fed by all translation options whose source side matches the sentence to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on"
2013.mtsummit-papers.5,D12-1037,0,0.0337503,"to translate. Moreover, we propose a further enhancement of the basic caching mechanism for rewarding cached items related to the current sentence to translate. Our work also deals with MT adaptation in general, and online learning more specifically. A thorough overview of the literature on these topics can be found in the companion paper (W¨aschle et al., 2013). Here, we just list some of the most representative papers: (Levenberg et al., 2010; Levenberg et al., 2011) for online learning methods in streaming scenarios; (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Bisazza et al., 2011; Liu et al., 2012; Bertoldi et al., 2012) for incremental adaptation. 3 Online Adaptation in SMT In the well established CAT work flow, source documents are split into chunks, typically corresponding to sentences and called segments, that are in general translated sequentially. When the translator opens a segment, the CAT tool tries to propose possible translation suggestions, originating from the translation memory and/or from a machine translation engine. Depending on the quality of the suggestions, the translator decides whether to post-edit one of them or to translate the source segment from scratch. Compl"
2013.mtsummit-papers.5,W04-3225,0,0.675866,"and CasmaCat2 European projects, which are jointly developing a new generation CAT tool integrating novel interaction modalities and MT functions. In this paper we deal with SMT models which dynamically learn from the user feedback by means of a caching mechanism. The main idea behind cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items observed in the history of the input stream. In (Kuhn and De Mori, 1990), the caching mechanism was applied to language models, and only very later its use was extended to translation models (Nepveu et al., 2004). In lab tests, cache-based language and translation models have already proven to be effective in interactive SMT (Nepveu et al., 2004); in adapting generic SMT models, (Tiedemann, 2010) obtained interesting but not definitive gains, the “main obstacle” being “the invalid assumption that initial translations are correct”. In a real CAT framework like ours, the cached items are correct by definition. The road map of our work is defined through a list of research questions addressed in this paper. Clearly, the first questions we are interested in is: Q1 - Is the effectiveness of cache-based ada"
2013.mtsummit-papers.5,steinberger-etal-2006-jrc,0,0.032325,"ency of observed feature. To mimic the way translation memory works, we also decide to reward target ngram and phrase-pair features observed in similar and already translated segments of the document. This implies adding the following lazy learning step to the previous on-line learning algorithm: 2. Mt is updated with features extracted from the pair (x, y) in {(x1 , y1 ), . . . , (xt−1 , yt−1 )} such that x is most similar to xt . replicate and cross-assess our outcomes, were chosen to cover a wide range of linguistic conditions. For the Legal domain, we worked with the JRCAcquis collection (Steinberger et al., 2006), on the English to Italian direction. The same direction is also investigated for the IT domain. As concerns TED, the two official MT tasks of the IWSLT 2012 evaluation campaign (Federico et al., 2012) were considered, i.e. the translation of talks from Arabic to English and from English to French. 4.1.1 Training Data The training data for the IT domain mostly consist of data extracted from a translation memory built during the execution of translation projects commissioned by several commercial companies. In addition, parallel texts from the OPUS corpus (Tiedemann, 2012) were are also includ"
2013.mtsummit-papers.5,W10-2602,0,0.0475392,"dynamically learn from the user feedback by means of a caching mechanism. The main idea behind cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items observed in the history of the input stream. In (Kuhn and De Mori, 1990), the caching mechanism was applied to language models, and only very later its use was extended to translation models (Nepveu et al., 2004). In lab tests, cache-based language and translation models have already proven to be effective in interactive SMT (Nepveu et al., 2004); in adapting generic SMT models, (Tiedemann, 2010) obtained interesting but not definitive gains, the “main obstacle” being “the invalid assumption that initial translations are correct”. In a real CAT framework like ours, the cached items are correct by definition. The road map of our work is defined through a list of research questions addressed in this paper. Clearly, the first questions we are interested in is: Q1 - Is the effectiveness of cache-based adaptation confirmed in a real CAT environment? As mentioned in (Tiedemann, 2010), there are two types of important properties in natural language and translation that are often ignored in s"
2013.mtsummit-papers.5,tiedemann-2012-parallel,0,0.0187782,"collection (Steinberger et al., 2006), on the English to Italian direction. The same direction is also investigated for the IT domain. As concerns TED, the two official MT tasks of the IWSLT 2012 evaluation campaign (Federico et al., 2012) were considered, i.e. the translation of talks from Arabic to English and from English to French. 4.1.1 Training Data The training data for the IT domain mostly consist of data extracted from a translation memory built during the execution of translation projects commissioned by several commercial companies. In addition, parallel texts from the OPUS corpus (Tiedemann, 2012) were are also included. Training data of the Legal domain come from Version 3.0 of the JRC-Acquis collection (Steinberger et al., 2006), while TED training data are those released through the WIT3 (Cettolo et al., 2012) website for the IWSLT 2012 evaluation campaign (Federico et al., 2012). The reader may again refer to Table 1 for statistics on the actual corpora employed for training. Evaluations were performed on both proprietary and publicly available data, involving the translation of documents from three domains, namely Information Technology (IT), Legal (LGL) and TED talks,3 and for di"
2013.mtsummit-papers.5,2013.mtsummit-papers.2,1,0.847457,"Missing"
2013.mtsummit-papers.5,federico-etal-2012-iwslt,1,\N,Missing
2013.mtsummit-wptp.13,D11-1033,0,0.0228773,"o adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is in general true only if"
2013.mtsummit-wptp.13,N09-2038,0,0.0242009,"the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine translation tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in impr"
2013.mtsummit-wptp.13,W12-3155,1,0.851259,"ently homogeneous, the language is sufficiently complex, and there is sufficient multilingual data available to train and tune MT models. The paper is organized as follows. Section 2 lists some of the related works. Section 3 introduces methods used for project adaptation. Section 4 briefly describes the conduct of the field test. Section 5 and Section 6, respectively, introduce the set-up and results of experiments. Section 7 concludes the paper with a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (e"
2013.mtsummit-wptp.13,2011.iwslt-evaluation.18,1,0.827144,"elated Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is pe"
2013.mtsummit-wptp.13,P11-2031,0,0.0146555,"Dntgt , n=0,1) or the concatenation of the source side of both the development and test set (D01src ); we name FGtgt and FGsrc the selected corpus and the models trained on it in the two cases. The table also provides the percentage of data selected, computed with respect to the target side. The optimal splitting was performed by minimizing the perplexity of the target side of the development set. 6 Experiments Lab test experiments have been performed on data sets described in Section 5. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script implemented by Clark et al. (2011), and of GTM.4 For statistical significance, p-values were calculated via approximate randomization for adapted systems with respect to the baselines and are reported in Tables 5 and 6 whenever not larger than 0.10. The SMT systems have been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the available parallel training data (Table 1); 5-gram LMs smoothed through the improved Kneser-Ney tech4 http://nlp.cs.nyu.edu/GTM nique (Chen and Goodman, 1999) are estimated on the target side via the IRSTLM toolkit (Fed"
2013.mtsummit-wptp.13,2012.amta-papers.22,1,0.534373,"MT suggestions for the second half of the document came from a system adapted to the text of the first day by means of one of the adaptation methods tested in our experiments (Section 6). Translators post-edited machine-generated translations for correcting mistakes and making them stylistically appropriate. The document was selected such that the size of its halves corresponds approximately to the daily productivity of professional translators, that is three to five thousand words. A report on the field test including an analysis of the productivity of translators has already been published (Federico et al., 2012). Moreover, we performed a preliminary measure of the performance of MT outputs versus the post-edition of each translator. In both cases, pretty large inter-translator differences were observed. Since the limited number of subjects would have led to scores with large variances, we decided to choose segments en→de up, called backoff, in which the indicator feature is discarded. Again, the backoff method proposed by Niehues and Waibel (2012) differs slightly in the way the scores of the phrase pairs stemming from the background phrase table are computed. Language model: As concerns the LM adapt"
2013.mtsummit-wptp.13,P02-1023,0,0.0361356,"Missing"
2013.mtsummit-wptp.13,W07-0733,0,0.0352986,"t-up and results of experiments. Section 7 concludes the paper with a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performan"
2013.mtsummit-wptp.13,P07-2045,1,0.00690725,"plitting was performed by minimizing the perplexity of the target side of the development set. 6 Experiments Lab test experiments have been performed on data sets described in Section 5. Performance are provided in terms of BLEU and TER, computed by means of the MultEval script implemented by Clark et al. (2011), and of GTM.4 For statistical significance, p-values were calculated via approximate randomization for adapted systems with respect to the baselines and are reported in Tables 5 and 6 whenever not larger than 0.10. The SMT systems have been built upon the open-source MT toolkit Moses (Koehn et al., 2007). The translation and the lexicalized reordering models are trained on the available parallel training data (Table 1); 5-gram LMs smoothed through the improved Kneser-Ney tech4 http://nlp.cs.nyu.edu/GTM nique (Chen and Goodman, 1999) are estimated on the target side via the IRSTLM toolkit (Federico et al., 2008). The weights of the log-linear interpolation model have been optimized by means of the Margin Infused Relaxed Algorithm (MIRA) process (Hasler et al., 2011) provided within the Moses toolkit. Various models have been built by means of the methods described in Section 3. Here the list o"
2013.mtsummit-wptp.13,D12-1037,0,0.0226802,"and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine transla"
2013.mtsummit-wptp.13,D09-1074,0,0.0233102,"Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram"
2013.mtsummit-wptp.13,P10-2041,0,0.0161319,"of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is in general true only if the new data is enough relevant to the task at hand, a condition"
2013.mtsummit-wptp.13,W08-0320,0,0.0387515,"Missing"
2013.mtsummit-wptp.13,2012.amta-papers.19,0,0.127366,"rien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the MT module involves only the LM and is performed on the MT outputs. On standard machine translation tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et a"
2013.mtsummit-wptp.13,W12-3147,1,0.900227,"Missing"
2013.mtsummit-wptp.13,steinberger-etal-2006-jrc,0,0.0170217,"Missing"
2013.mtsummit-wptp.13,I08-2088,0,0.0276276,"on tasks, Niehues and Waibel (2012) compare different approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistic"
2013.mtsummit-wptp.13,W07-0717,0,0.0387197,"a discussion on the overall results. 2 Related Work Our work deals with MT adaptation in general, and incremental adaptation more specifically. Bertoldi et al. (2012) present an adaptation scenario where foreground translation and reordering models (TM) and language model (LM) of a phrase-based SMT system are incrementally trained on batches of fresh data and then paired to static background models. Similarly, the use of local and global models for incremental learning was previously proposed through a log-linear combination (Koehn and Schroeder, 2007), a mixture model (linear or log-linear) (Foster and Kuhn, 2007), the filling-up (Bisazza et al., 2011), or via ultraconservative updating (Liu et al., 2012). Sharon O’Brien, Michel Simard and Lucia Specia (eds.) Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, September 2, 2013, p. 111–118. c 2013 The Authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Bach et al. (2009) investigate how a speechto-speech translation system can adapt day-to-day from collected data on day one to improve performance on day two, similarly to us. However, the adaptation of the M"
2013.mtsummit-wptp.13,D10-1044,0,0.0217311,"ifferent approaches to adapt a SMT system towards a target domain using small amounts of parallel in-domain data, namely the backoff, the factored, and the already mentioned log-linear and fill-up techniques; the general outcome is that each of them is effective in improving non-adapted models and none is definitely better than each other, which is the best depending on how well the test data matches the indomain training data. This work deals with data selection as well, which is a problem widely investigated by the SMT community, see for example (Yasuda et al., 2008; Matsoukas et al., 2009; Foster et al., 2010; Axelrod et al., 2011). We apply a standard selection technique (Moore and Lewis, 2010), but in a quite different scenario where the task-specific data is extremely small and the generic corpus is actually close to the domain of the task. 3 Adaptation Methods In this section we describe the techniques employed to adapt our SMT systems, namely data selection and translation, distortion and language model combination. 3.1 Data selection It has been believed for a long time that just adding more training data always improves performance of a statistical model, e.g. a n-gram LM. However, this is"
2014.amta-researchers.12,2013.mtsummit-papers.5,1,0.785139,"e the multi-task scenario. Legal domain data has been release as a part of JRC-acquis corpus (Steinberger et al., 2006). The dataset contains translation of legal documents from English to Italian. This dataset was also a part of the field test carried out under the same MateCat project, so essentially we have post-edited data from 4 different translators on a test set of 90 sentences. Since our methods regard the adaptation of MT models, the potential impact strictly depends on how much the considered text is repetitive. For measuring that text feature, we use the repetition rate proposed by Bertoldi et al. (2013). Equation 9 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different 2 In this paper we use the terms tasks and translators interchangeably as the tasks are translators in the CAT scenario. 3 To keep the notation light we again drop the dependency of h from x. 4 http://www.matecat.com Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 155 n-grams and nr is the number of different n-grams occurring exactly r times. Statistics of the parallel sets on source and target sides al"
2014.amta-researchers.12,C14-1040,1,0.88094,"Missing"
2014.amta-researchers.12,J93-3001,0,0.0669408,"Missing"
2014.amta-researchers.12,P13-1004,0,0.0276447,"e use a matrix-based regularization approach on the core features for online adaptation of all the translation models. Multi-Task learning has also been used in re-ranking the N-best list by Duh et al. (2010). Each N-Best list is considered as a different task and the weights are jointly learnt over a large set of sparse features. Simianer et al. (2011) trained a discriminative model using multi-task learning over a set of k documents belonging to different topics but with strong commonalities. Recent application of multi-task learning has been in quality estimation for machine translation by Cohn and Specia (2013) where the authors model annotator bias using multi-task Gaussian processes. Their model outperforms the annotator specific model and thus boosting the use of Multi-Task learning in NLP applications. Another application of MTL has been in supervised domain adaptation for quality estimation (C. de Souza et al., 2014). In this work the authors leverage all available training labels from different domains in order to learn a robust model for a target domain with very little labeled data. The approach proposed outperforms independent models trained separetely on each domain. 7 Conclusion We addres"
2014.amta-researchers.12,E14-1042,0,0.101531,"Missing"
2014.amta-researchers.12,W10-1757,0,0.0236818,"explored in SMT in the context of tuning the sparse log linear weights by Simianer et al. (2012) where they split the training set in random shards and perform a joint feature selection over these shards using `1 /`2 regularization. In this way after each epoch, the size of feature vector decreases and only the important features are taken into account. In our paper instead of `1 /`2 regularization we have use a matrix-based regularization approach on the core features for online adaptation of all the translation models. Multi-Task learning has also been used in re-ranking the N-best list by Duh et al. (2010). Each N-Best list is considered as a different task and the weights are jointly learnt over a large set of sparse features. Simianer et al. (2011) trained a discriminative model using multi-task learning over a set of k documents belonging to different topics but with strong commonalities. Recent application of multi-task learning has been in quality estimation for machine translation by Cohn and Specia (2013) where the authors model annotator bias using multi-task Gaussian processes. Their model outperforms the annotator specific model and thus boosting the use of Multi-Task learning in NLP"
2014.amta-researchers.12,1993.eamt-1.1,0,0.313734,"Missing"
2014.amta-researchers.12,2012.amta-papers.22,1,0.685493,"nd on a popular benchmark with multiple references, respectively. 1 Introduction In a professional localization environment, a document is post-edited by several professional translators with assistance of tools such as translation memory, dictionary, spell checkers etc. To speed up the process, lately localization companies have started using computer assisted translation (CAT) tools with statistical machine translation (SMT) systems in the backend. The role played by the SMT engine is to provide a translation hypothesis that the translator can post edit to produce high quality translations (Federico et al., 2012). In recent works on online adaptation by Mathur et al. (2013) and Denkowski et al. (2014), the SMT is fed with the post edited sentence, allowing the models to adapt to the corrections made by the translators. These kind of systems works well if the document is being post edited by a single translator because models can adapt to the style of that translator. Problems arise when a document is being post edited by a group of translators which is usually the case with big size documents. In fact, if the SMT system adapts to the corrections of all translators together, it will likely mix or overl"
2014.amta-researchers.12,P07-2045,1,0.0108768,"target side is 22.18, as reported in Table 1; this could be due to the fact that translators tend to be not consistent among themselves, yielding less repetitions in each post-edited test set than in the shuffled test set. #Sentence 1 2 3 4 Sentence Input Date # 0 Input Date # Data di input # 0 Evaluates conditionally # 1 Evaluates conditionally # Valuta in modo condizionale # 1 OnlineLearning Not Activated Activated Not Activated Activated Translator ID 0 0 1 1 Table 2: Excerpt from IT development set tagged with meta data. 5.2 Experiments The SMT systems were built using the Moses toolkit (Koehn et al., 2007). Domain specific training data was used to create translation and lexical reordering models. 5-gram language models for each task were estimated by means of IRSTLM toolkit (Federico et al., 2008), with improved Kneser-Ney smoothing (Chen and Goodman, 1998), on the target side of the training parallel corpora. After the training of MT models, the log linear weights were optimized using Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 156 MERT (Och, 2003) implementation provided in the Moses toolkit. Performance is computed not with corpus"
2014.amta-researchers.12,C04-1072,0,0.0308242,"learning algorithm which can be applied in CAT scenario. Experiments and results are shown in Section 5. We conclude the paper with a preview of interesting related works and few words about the future work. 2 Background: Online Large Margin Training Previous work by Mathur et al. (2013) applies an online large margin algorithm (MIRA), that updates the weights w of a phrase-based SMT model according to the loss that is occurred due to an incorrect translation. The margin is coupled with the following loss function based on the complement of the sentence level BLEU (BLEU+1, henceforth sBLEU) (Lin and Och, 2004; Nakov et al., 2012): lj = sBLEU (y ∗ ) − sBLEU (yj ) (1) where y ∗ is the oracle (closest translation to the reference) and yj is the j-th candidate being processed inside an N -best list. According to (Watanabe et al., 2007), weights are updated so that the loss is not larger than the difference between the scores given by the model: lj ≤ wT ∆hj (2) where ∆hj is the difference between the feature vectors of the oracle and the candidate, and w is the weight vector. Hence, the size of the weight update is: arg min ||w − w0 ||+ C w X ξj j subject to T w ∆hj + ξj ≥ lj ξj ≥ 0 ∀j ∈ {1 . . . N } A"
2014.amta-researchers.12,W13-2237,1,0.915445,". 1 Introduction In a professional localization environment, a document is post-edited by several professional translators with assistance of tools such as translation memory, dictionary, spell checkers etc. To speed up the process, lately localization companies have started using computer assisted translation (CAT) tools with statistical machine translation (SMT) systems in the backend. The role played by the SMT engine is to provide a translation hypothesis that the translator can post edit to produce high quality translations (Federico et al., 2012). In recent works on online adaptation by Mathur et al. (2013) and Denkowski et al. (2014), the SMT is fed with the post edited sentence, allowing the models to adapt to the corrections made by the translators. These kind of systems works well if the document is being post edited by a single translator because models can adapt to the style of that translator. Problems arise when a document is being post edited by a group of translators which is usually the case with big size documents. In fact, if the SMT system adapts to the corrections of all translators together, it will likely mix or overlap stylistic features of the post-editors and thus not learn t"
2014.amta-researchers.12,C12-1121,0,0.0234231,"Missing"
2014.amta-researchers.12,J03-1002,0,0.00383573,"di produzione nel volume di backup . you create a backup and after it completes # 1 possibile creare una copia di riserva e dopo il completamento creare un backup e dopo il completamento possibile creare un backup e , al suo completamento Table 8: Example from the IT test set. Here Multi-Task refers to MTL-pearson system and K-ind is K-independent system. 6 Related Works Despite several online adaptation strategies have been proposed in the past, only a few deal with adaptation of post-edited/evaluation data while most works are on adaptation over development data during tuning of parameters (Och and Ney, 2003). Cesa-Bianchi et al. (2008) proposed an online learning approach during decoding. They construct a layer of online weights over the regular feature weights and update these weights at sentence level using margin infused relaxed algorithm (Crammer and Singer, 2003); to our knowledge, this is the first work on online adaptation during decoding. Mart´ınez-G´omez et al. (2011, 2012) presented a comparison of online adaptation techniques in post editing scenario. They compared different adaptation strategies on feature weights and features itself. Multi-Task learning has been explored in SMT in th"
2014.amta-researchers.12,P03-1021,0,0.0421863,"eta data. 5.2 Experiments The SMT systems were built using the Moses toolkit (Koehn et al., 2007). Domain specific training data was used to create translation and lexical reordering models. 5-gram language models for each task were estimated by means of IRSTLM toolkit (Federico et al., 2008), with improved Kneser-Ney smoothing (Chen and Goodman, 1998), on the target side of the training parallel corpora. After the training of MT models, the log linear weights were optimized using Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 156 MERT (Och, 2003) implementation provided in the Moses toolkit. Performance is computed not with corpus level metrics but with sentence level metrics. We decided to do this to avoid a metric mismatch between the evaluation and actual optimization where the margin is calculated by the sentence level BLEU scores (refer to Section 2). Therefore, we computed sBLEU scores and sentence level TER (Snover et al., 2006) scores and reported their average over the whole documents. We call them avg-sBLEU and avg-sTER. Calculating A−1 matrix: Interaction matrix can be computed in different ways. It basically conveys the re"
2014.amta-researchers.12,W05-0908,0,0.0636609,"Missing"
2014.amta-researchers.12,P12-1002,0,0.0191377,"ing approach during decoding. They construct a layer of online weights over the regular feature weights and update these weights at sentence level using margin infused relaxed algorithm (Crammer and Singer, 2003); to our knowledge, this is the first work on online adaptation during decoding. Mart´ınez-G´omez et al. (2011, 2012) presented a comparison of online adaptation techniques in post editing scenario. They compared different adaptation strategies on feature weights and features itself. Multi-Task learning has been explored in SMT in the context of tuning the sparse log linear weights by Simianer et al. (2012) where they split the training set in random shards and perform a joint feature selection over these shards using `1 /`2 regularization. In this way after each epoch, the size of feature vector decreases and only the important features are taken into account. In our paper instead of `1 /`2 regularization we have use a matrix-based regularization approach on the core features for online adaptation of all the translation models. Multi-Task learning has also been used in re-ranking the N-best list by Duh et al. (2010). Each N-Best list is considered as a different task and the weights are jointly"
2014.amta-researchers.12,2006.amta-papers.25,0,0.0694535,"ing parallel corpora. After the training of MT models, the log linear weights were optimized using Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 156 MERT (Och, 2003) implementation provided in the Moses toolkit. Performance is computed not with corpus level metrics but with sentence level metrics. We decided to do this to avoid a metric mismatch between the evaluation and actual optimization where the margin is calculated by the sentence level BLEU scores (refer to Section 2). Therefore, we computed sBLEU scores and sentence level TER (Snover et al., 2006) scores and reported their average over the whole documents. We call them avg-sBLEU and avg-sTER. Calculating A−1 matrix: Interaction matrix can be computed in different ways. It basically conveys the relatedness/correlation between the translators who are post-editing a particular document. Usually a localization company keeps a ranking of the hired translators with them; either we can use the ranking to exploit the relatedness between the translators or we can calculate their correlation based on a known previous post-edited data set. Here, we assume that the relatedness between the translat"
2014.amta-researchers.12,steinberger-etal-2006-jrc,0,0.102397,"Missing"
2014.amta-researchers.12,D07-1080,0,0.0241727,"Online Large Margin Training Previous work by Mathur et al. (2013) applies an online large margin algorithm (MIRA), that updates the weights w of a phrase-based SMT model according to the loss that is occurred due to an incorrect translation. The margin is coupled with the following loss function based on the complement of the sentence level BLEU (BLEU+1, henceforth sBLEU) (Lin and Och, 2004; Nakov et al., 2012): lj = sBLEU (y ∗ ) − sBLEU (yj ) (1) where y ∗ is the oracle (closest translation to the reference) and yj is the j-th candidate being processed inside an N -best list. According to (Watanabe et al., 2007), weights are updated so that the loss is not larger than the difference between the scores given by the model: lj ≤ wT ∆hj (2) where ∆hj is the difference between the feature vectors of the oracle and the candidate, and w is the weight vector. Hence, the size of the weight update is: arg min ||w − w0 ||+ C w X ξj j subject to T w ∆hj + ξj ≥ lj ξj ≥ 0 ∀j ∈ {1 . . . N } Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC (3) © The Authors 153 C is an aggressiveness parameter which controls the size of the update and ξ are slack variables. Following (Watanab"
2014.amta-researchers.13,D11-1033,0,0.0121256,"d, which mostly includes EU legislative texts translated into 22 languages. Table 2 provides detailed statistics on the actual bitexts used for training purposes. In particular, the train entries refer to the whole generic training texts, while development set entries to additional data on which the parameters of the phrase-based MT model were optimized. The domain selection entry of the IT en→fr task refers to data selected from out-ofdomain texts (Giga English-French, United Nation, and Common Crawl corpora6 (Bojar et al., 2013)) by using the in-domain text as seed in the method proposed by Axelrod et al. (2011) and available within the XenC toolkit (Rousseau, 2013); this was done to augment the amount of training data, since the size of in-domain text available for that language pair (15.4/17.9 million words) is about four times smaller than for the other tasks. domain pair en→it IT en→fr en→it Legal en→fr en→es corpus segments train development set train domain selection development set train development set train development set train development set 5.4 M 2,156 1.1 M 1.2 M 4,755 2.7 M 181 2.8 M 600 2.3 M 700 tokens source target 57.2M 59.9M 26,080 28,137 15.4M 17.9M 20.0M 22.2M 26,747 30,100 61.4"
2014.amta-researchers.13,2013.mtsummit-papers.5,1,0.943654,"ed for capturing the repetitiveness of text. Indeed, in the case of MT related tasks, like the quality estimation of MT output, in addition to features computed on the source text, features have been proposed which involve the translated/target text or even the MT models: although they can be really effective, we focus our investigation to the source side only, since we are interested in deciding what kind of MT system is most suitable for translating a given text before having any MT engine at disposal. In this paper we experimentally assess the repetition rate, that we recently proposed in (Bertoldi et al., 2013) where no support to its effectiveness was provided, as a single light measure to characterize a full document to be translated. Roughly, the repetition rate computes the rate of event types (single words and n-grams) that occur more than once in a text; for making this statistics independent from the size of the document, it is computed on a fixed-size sliding window. We measured the prediction power of the repetition rate on several MT adaptation tasks and compared it against other text features that were proposed for very related NLP tasks. The comparison was carried out through a regressio"
2014.amta-researchers.13,I13-1185,0,0.045335,"Missing"
2014.amta-researchers.13,J10-4005,0,0.0180421,"analysis between feature values and MT performance gains by dynamically adapted versus non-adapted MT engines, on five different translation tasks. The main outcome of experiments is that the repetition rate correlates better than any other considered feature with the MT gains yielded by the online adaptation, although using all features jointly results in better predictions than with any single feature. 1 Introduction Language and content repetitiveness1 is a key factor for the successful deployment of translation memories (TMs) (Somers, 2003) as well as statistical machine translation (MT) (Koehn, 2010). The capability of a TM to provide useful translation suggestions for a text segment relies on the chance of finding segments with very similar content – i.e. with a significant percentage of overlapping words – inside a large repository of already translated texts. On the other hand, statistical MT also relies on the assumption that the segment to be translated shares with the training data a significant amount of patterns, from single words to groups of words. Advances on the integration of human post-editing into MT have recently revealed the potential of incremental and online MT adaptati"
2014.amta-researchers.13,P07-2045,1,0.0161214,"Missing"
2014.amta-researchers.13,P11-1132,0,0.0267725,"oportion of simple/complex sentences, ambiguity as the average of senses per word, word length as the proportion of syllables per word, lexical richness, and information load as the proportion of lexical words to tokens in (Ilisei et al., 2010); most frequent words and 3 Obfuscation is the strategy adopted by real plagiarists to rewrite their source passages in order to make detection more difficult. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 168 a list of some hundred function words are instead used in (Islam and Hoenen, 2013) and (Koppel and Ordan, 2011), respectively. 3 Repetition Rate We recently introduced the repetition rate (Bertoldi et al., 2013) as a way to measure the repetitiveness inside a text, by looking at the rate of non-singleton n-gram types (n=1. . .4) it contains. As shown there, this rate decays exponentially with n. For combining values with exponential decay, a reasonable scheme is to average their logarithms, or equivalently to compute their geometric mean. Furthermore, in order to make the measure comparable across different sized documents, statistics are collected on a sliding window of one thousand words, and properl"
2014.amta-researchers.13,P02-1040,0,0.0900417,"s iterates over all sentences of the document to be translated. In our systems, the local model is implemented by a caching mechanism. The caching regards both translation and language models: phrase pairs extracted from the alignment of the source and post-edit are extracted and inserted into the cache-based translation model, while n-grams of the post-edit fill the cache-based language model. More details are provided in (Bertoldi et al., 2013). Note that in our experiments the post-editing is simulated by using human references. 5.5 Results and comments First of all, Table 3 provides BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and GTM (Turian et al., 2003) scores computed on the evaluation documents with respect to human references for each of the five considered translation tasks. In both domains, but especially for IT, the improvements over the static systems yielded by the dynamic adaptation technique are remarkable. Focusing on the BLEU score, in the IT domain it improves by more than 7 absolute points for English-to-Italian (57.5 to 64.6), and almost 15 absolute points for English-to-French (41.4 to 56.3); in the LGL domain, the gain is quite limited in English-to-Italian (1.5 absolu"
2014.amta-researchers.13,2006.amta-papers.25,0,0.0546353,"of the document to be translated. In our systems, the local model is implemented by a caching mechanism. The caching regards both translation and language models: phrase pairs extracted from the alignment of the source and post-edit are extracted and inserted into the cache-based translation model, while n-grams of the post-edit fill the cache-based language model. More details are provided in (Bertoldi et al., 2013). Note that in our experiments the post-editing is simulated by using human references. 5.5 Results and comments First of all, Table 3 provides BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and GTM (Turian et al., 2003) scores computed on the evaluation documents with respect to human references for each of the five considered translation tasks. In both domains, but especially for IT, the improvements over the static systems yielded by the dynamic adaptation technique are remarkable. Focusing on the BLEU score, in the IT domain it improves by more than 7 absolute points for English-to-Italian (57.5 to 64.6), and almost 15 absolute points for English-to-French (41.4 to 56.3); in the LGL domain, the gain is quite limited in English-to-Italian (1.5 absolute points), but definitely"
2014.amta-researchers.13,steinberger-etal-2006-jrc,0,0.0939907,"Missing"
2014.amta-researchers.13,tiedemann-2012-parallel,0,0.0298076,"Missing"
2014.amta-researchers.13,2003.mtsummit-papers.51,0,0.043002,"ted. In our systems, the local model is implemented by a caching mechanism. The caching regards both translation and language models: phrase pairs extracted from the alignment of the source and post-edit are extracted and inserted into the cache-based translation model, while n-grams of the post-edit fill the cache-based language model. More details are provided in (Bertoldi et al., 2013). Note that in our experiments the post-editing is simulated by using human references. 5.5 Results and comments First of all, Table 3 provides BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and GTM (Turian et al., 2003) scores computed on the evaluation documents with respect to human references for each of the five considered translation tasks. In both domains, but especially for IT, the improvements over the static systems yielded by the dynamic adaptation technique are remarkable. Focusing on the BLEU score, in the IT domain it improves by more than 7 absolute points for English-to-Italian (57.5 to 64.6), and almost 15 absolute points for English-to-French (41.4 to 56.3); in the LGL domain, the gain is quite limited in English-to-Italian (1.5 absolute points), but definitely notable – almost 5 points – fo"
2014.amta-researchers.13,W12-2019,0,0.129424,"ts over the use of single tokens, especially by introducing not too long n-grams (F¨urnkranz, 1998), outcome that we exploit by defining the repetition rate over 1- to 4-grams. Readability assessment is a form of text classification aiming at retrieving texts that suit a particular target reading level. In a school setting, it can help teachers to find texts appropriate to their students; other real-life contexts where it can play an important role are those involving people with intellectual disabilities, dyslexics, immigrant populations, and second or foreign language learners. Commendably, Vajjala and Meurers (2012) present dozens of features used in previous research on text readability and complexity and group them into three broad categories: lexical, syntactic and traditional features. Examples of features from the first group are the type-token ratio (see Section 4) and the lexical density, defined as the ratio of the number of lexical word tokens (nouns, adjectives, verbs, adverbs) and the number of all tokens (total number of words) in the analysed text. Syntactic features include mean length of clauses and sentences, and co-ordinate phrases and complex nominals per clause. The average sentence le"
2014.amta-researchers.13,W13-2201,0,\N,Missing
2014.amta-workshop.4,2013.mtsummit-papers.5,1,0.841263,"Italian and has been used in the field test recently carried out under the MateCat project.5 In the Legal domain, experiments involved the translation of English documents into either Spanish or French; training and evaluation sets belong to the JRC-Acquis corpus (Steinberger et al., 2006) so that the effectiveness of the proposed approaches is assessed also on publicly available data. Since our methods regard the adaptation of MT models, the potential impact strictly depends on how much the considered text is repetitive. For measuring that text feature, we use the repetition rate proposed by Bertoldi et al. (2013). Statistics of the parallel sets of both source and target side along with the repetition rates are reported in Table 1. Domain ITen→it Legalen→es Legalen→fr Set Train Dev Test Train Dev Test Train Dev Test #srcTok 57M 3.7K 3.4K 56M 3K 11K 63M 3K 11K srcRR na 7.65 34.33 na 24.09 20.67 na 23.52 20.67 #tgtTok 60M 4K 3.7K 62M 3.5K 12.5K 70M 3.7K 13K tgtRR na 7.61 33.90 na 24.47 20.07 na 23.42 20.92 Table 1: Statistics of the parallel data along with the corresponding repetition rate (RR). 5.2 Reference Systems The SMT systems are built using the Moses toolkit (Koehn et al., 2007). Domain specifi"
2014.amta-workshop.4,W12-3159,0,0.215053,"ds. The optimal weights are computed by minimizing the error on a held-out parallel development set by means of MIRA which operates on the N -best list and re-ranks it by changing the loglinear weights. Since hyperparameters do not affect this N -best list once it is created, there is no direct way to optimize the hyperparameters on the development set via traditional tuning methods. An alternative solution needs to be found. Hyperparameters in SMT models, such as distortion limit and beam size, have been typically optimized using derivative free optimization (DFO) techniques such as Simplex (Chung and Galley, 2012) and Hill Climbing (Stymne et al., 2013). Analogously, once we have tuned the log-linear weights, we keep them fixed and optimize our hyperparameters by means of DFO. This cascade approach prevents joint optimization over 18 parameters2 which is not feasible using the DFO techniques because they tend not to converge with so many parameters. In this paper we focus on three hyperparameters, namely the feature learning rate (FLR), the weight learning rate (WLR) and the slack variable (SLK). FLR determines the rate of learning of the additional online feature; WLR and SLK control respectively the"
2014.amta-workshop.4,E14-1042,0,0.0241365,"2 points). Summarizing, we see consistent improvements of TER, but not of BLEU. A possible explanation is the use of TER as the error metric for finding the optimal iterations of online learning for the blocks and the clusters.In fact, the size of clusters is too small to allow the reliable computation of the BLEU, but optimizing TER favors short sentences, which lower BLEU through the brevity penalty. 7 Related Work Online learning for SMT has emerged as a hot topic over the last decade (Nepveu et al., 2004; Hardt and Elming, 2010; Ortiz-Mart´ınez et al., 2010; Mart´ınez-G´omez et al., 2012; Denkowski et al., 2014). Nevertheless, to our knowledge, optimizing the number of iterations of online learning has not been previously studied in the context of SMT integrated in CAT tools. Therefore, this is the first work towards a fully optimized MT online learning system for CAT. The most notable work in the field of optimization of hyperparameters in MT is that by Chung and Galley (2012) where the decoder is integrated with a minimizer so that they can optimize the values of free parameters such as beam size and distortion limit. This minimizer runs derivative free optimization techniques, such as Powell and N"
2014.amta-workshop.4,2012.amta-papers.22,0,0.0164941,"Assisted Translation (CAT) tools are used which provide access to translation memories, terminology, built-in spell checkers, dictionaries. A translation memory is a good source of previously translated segments; however, for new translation tasks, they are often obsolete. Due to the generalization capability of Machine Translation (MT) systems, they are employed in the back end of the CAT tools, for providing translation suggestions to the humans in cases where the translation memory fails. In fact, a seamless integration of SMT engines in CAT has shown to increase translator’s productivity (Federico et al., 2012). In state-of-the-art CAT tools, the SMT systems are generally static in nature and cannot adapt to the corrections posted by the translators. On the contrary, an adaptable SMT system would be preferable which can learn from the corrections of the post editor and modifies the statistical models accordingly. The task of learning from user corrections at the sentence level fits well in the online learning scenario. Online learning is a machine learning task where a predictor iteratively: (1) receives an input, (2) predicts an output label, (3) receives the correct output label from a human and,"
2014.amta-workshop.4,P13-4033,0,0.0183939,"grated with a minimizer so that they can optimize the values of free parameters such as beam size and distortion limit. This minimizer runs derivative free optimization techniques, such as Powell and Nelder-Mead methods, to optimize log-linear weights as well as the hyperparameters. They also argue that this integrated minimizer measures the true error rate whereas MERT minimizes the artificial error rate computed on a N -best list. Their use of DFO methods pushed us to adopt the same. Later, Stymne et al. (2013) focused on using the distortion limit (DL) in the document level decoder Docent (Hardmeier et al., 2013). Their system provides better BLEU scores when there is a soft constraint on the DL (i.e. DL is tunable) rather than a hard constraint (DL not tunable). This experiment further supports the optimization of MT parameters in order to gain performance. Learning of hyperparameters has been a widely studied topic in machine learning. Grid search, random search (Bergstra and Bengio, 2012), Gaussian process (Snoek et al., 2012) are only a few methods that have been used in the past for hyperparameter optimization. Gradient39 based hyperparameter learning algorithms have been proposed for a variety o"
2014.amta-workshop.4,2010.amta-papers.21,0,0.0159696,"small BLEU improvements (no more than 0.7 points), larger TER gains (even more than 2 points). Summarizing, we see consistent improvements of TER, but not of BLEU. A possible explanation is the use of TER as the error metric for finding the optimal iterations of online learning for the blocks and the clusters.In fact, the size of clusters is too small to allow the reliable computation of the BLEU, but optimizing TER favors short sentences, which lower BLEU through the brevity penalty. 7 Related Work Online learning for SMT has emerged as a hot topic over the last decade (Nepveu et al., 2004; Hardt and Elming, 2010; Ortiz-Mart´ınez et al., 2010; Mart´ınez-G´omez et al., 2012; Denkowski et al., 2014). Nevertheless, to our knowledge, optimizing the number of iterations of online learning has not been previously studied in the context of SMT integrated in CAT tools. Therefore, this is the first work towards a fully optimized MT online learning system for CAT. The most notable work in the field of optimization of hyperparameters in MT is that by Chung and Galley (2012) where the decoder is integrated with a minimizer so that they can optimize the values of free parameters such as beam size and distortion li"
2014.amta-workshop.4,P07-2045,0,0.00352161,"proposed by Bertoldi et al. (2013). Statistics of the parallel sets of both source and target side along with the repetition rates are reported in Table 1. Domain ITen→it Legalen→es Legalen→fr Set Train Dev Test Train Dev Test Train Dev Test #srcTok 57M 3.7K 3.4K 56M 3K 11K 63M 3K 11K srcRR na 7.65 34.33 na 24.09 20.67 na 23.52 20.67 #tgtTok 60M 4K 3.7K 62M 3.5K 12.5K 70M 3.7K 13K tgtRR na 7.61 33.90 na 24.47 20.07 na 23.42 20.92 Table 1: Statistics of the parallel data along with the corresponding repetition rate (RR). 5.2 Reference Systems The SMT systems are built using the Moses toolkit (Koehn et al., 2007). Domain specific training data is used to create translation and lexical reordering models. 5-gram language models for each task, smoothed through the improved Kneser-Ney technique (Chen and Goodman, 1998), are estimated by means of the IRSTLM toolkit (Federico et al., 2008) on the target side of the training parallel corpora. The weights of the log-linear interpolation of MT models are optimized using the MIRA (Watanabe et al., 2007) implementation provided in the Moses toolkit. 3 The Cosine distance was also tested: it performed similarly to the Euclidean distance, but Euclidean distance ga"
2014.amta-workshop.4,W13-2237,1,0.847421,"match, (4) learns from the mistake. However, the introduction of online learning itself brings two main issues. The first regards the tuning of the rate of learning, which is typically determined by the value of a number of parameters of the algorithm, hereafter referred to as hyperparameters;1 optimizing them is then the first issue. The second problem is the selection of the optimal number of iterations of the online learning algorithm, i.e. an optimal stopping criterion. In this paper, we focus on these issues and propose solutions in the context of SMT and CAT. Our work is an extension of Mathur et al. (2013), where three different hyperparameters are considered. Here, we are going to investigate techniques for optimizing the same , but in principle this work could be applied to any arbitrary number of hyperparameters. 1 They are so called to distinguish them from the parameters of the models under analysis. 32 Proceedings of the Workshop on Interactive and Adaptive Machine Translation, pages 32–41 AMTA Workshop. Vancouver, Canada. September 22, 2014 The organization of the paper is as follows. Section 2 gives an insight on the background needed to understand the concepts in the paper. Sections 3"
2014.amta-workshop.4,W04-3225,0,0.0434852,"of the cluster size, small BLEU improvements (no more than 0.7 points), larger TER gains (even more than 2 points). Summarizing, we see consistent improvements of TER, but not of BLEU. A possible explanation is the use of TER as the error metric for finding the optimal iterations of online learning for the blocks and the clusters.In fact, the size of clusters is too small to allow the reliable computation of the BLEU, but optimizing TER favors short sentences, which lower BLEU through the brevity penalty. 7 Related Work Online learning for SMT has emerged as a hot topic over the last decade (Nepveu et al., 2004; Hardt and Elming, 2010; Ortiz-Mart´ınez et al., 2010; Mart´ınez-G´omez et al., 2012; Denkowski et al., 2014). Nevertheless, to our knowledge, optimizing the number of iterations of online learning has not been previously studied in the context of SMT integrated in CAT tools. Therefore, this is the first work towards a fully optimized MT online learning system for CAT. The most notable work in the field of optimization of hyperparameters in MT is that by Chung and Galley (2012) where the decoder is integrated with a minimizer so that they can optimize the values of free parameters such as bea"
2014.amta-workshop.4,N10-1079,0,0.0353576,"Missing"
2014.amta-workshop.4,P02-1040,0,0.0890337,"e target side of the training parallel corpora. The weights of the log-linear interpolation of MT models are optimized using the MIRA (Watanabe et al., 2007) implementation provided in the Moses toolkit. 3 The Cosine distance was also tested: it performed similarly to the Euclidean distance, but Euclidean distance gave better quality of clusters than Cosine. 4 In real texts, we can assume that bunches of some tens of segments (e.g. 10-30) are linguistically coherent such that an adaptation scheme can be effectively applied. 5 http://www.matecat.com 35 Performance is reported in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Details on the tested SMT systems follow: Baseline The static baseline system does not perform any online learning, hence there are no hyperparameters involved in the system. Def-Param-*x Online learning systems using default values of hyperparameters and running 1, 5 and 10 iterations of online learning. These systems provide a reference for assessing the usefulness of estimation of the optimal number of iterations vs. the use of a pre-defined number of iterations. The default values of the hyperparameters are FLR=0.1, WLR=0.05 and SLK=0.001, which yield reason"
2014.amta-workshop.4,2006.amta-papers.25,0,0.0161308,"rallel corpora. The weights of the log-linear interpolation of MT models are optimized using the MIRA (Watanabe et al., 2007) implementation provided in the Moses toolkit. 3 The Cosine distance was also tested: it performed similarly to the Euclidean distance, but Euclidean distance gave better quality of clusters than Cosine. 4 In real texts, we can assume that bunches of some tens of segments (e.g. 10-30) are linguistically coherent such that an adaptation scheme can be effectively applied. 5 http://www.matecat.com 35 Performance is reported in terms of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). Details on the tested SMT systems follow: Baseline The static baseline system does not perform any online learning, hence there are no hyperparameters involved in the system. Def-Param-*x Online learning systems using default values of hyperparameters and running 1, 5 and 10 iterations of online learning. These systems provide a reference for assessing the usefulness of estimation of the optimal number of iterations vs. the use of a pre-defined number of iterations. The default values of the hyperparameters are FLR=0.1, WLR=0.05 and SLK=0.001, which yield reasonable performance in preliminar"
2014.amta-workshop.4,steinberger-etal-2006-jrc,0,0.031758,"Missing"
2014.amta-workshop.4,W13-2229,0,0.0645383,"imizing the error on a held-out parallel development set by means of MIRA which operates on the N -best list and re-ranks it by changing the loglinear weights. Since hyperparameters do not affect this N -best list once it is created, there is no direct way to optimize the hyperparameters on the development set via traditional tuning methods. An alternative solution needs to be found. Hyperparameters in SMT models, such as distortion limit and beam size, have been typically optimized using derivative free optimization (DFO) techniques such as Simplex (Chung and Galley, 2012) and Hill Climbing (Stymne et al., 2013). Analogously, once we have tuned the log-linear weights, we keep them fixed and optimize our hyperparameters by means of DFO. This cascade approach prevents joint optimization over 18 parameters2 which is not feasible using the DFO techniques because they tend not to converge with so many parameters. In this paper we focus on three hyperparameters, namely the feature learning rate (FLR), the weight learning rate (WLR) and the slack variable (SLK). FLR determines the rate of learning of the additional online feature; WLR and SLK control respectively the learning rate and the size of the update"
2014.amta-workshop.4,D07-1080,0,0.0532368,"we use to enhance the performance of the adaptable SMT system. Section 5 and 6 present experiments and results, respectively. Section 7 mentions a few related works and is followed by the conclusions section. 2 Background Mathur et al. (2013) described an online learning approach for SMT integrated in CAT. In that paper, a twofold adaptation is proposed: 1) feature adaptation, in which an additional feature is added to the phrase table for rewarding the recently seen phrase pairs; 2) weight adaptation, where the log-linear interpolation weights of SMT model are adapted on the fly using MIRA (Watanabe et al., 2007). The online learning in the CAT framework is performed on the pair of source sentence and post edit, once the latter is provided by the human translator. From the implementation point of view, a particular structure where the source sentence is paired with its post edited translation is passed to the decoder: this activates a single online learning iteration. To perform multiple iterations, a corresponding number of copies of that structure has to be passed as input to the decoder. The aforementioned paper does not deeply investigate the tuning of free parameters involved in that online learn"
2014.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.792952,"pants were also asked to submit runs on the 2013 test set (progress test set), in order to measure the progress of systems with respect to the previous year. All runs were evaluated with objective metrics, and submissions for two of the official text translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the 2014 evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been running now for over a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. The 2014 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Spoken language translation (SLT), that addressed the conversion and translation of a speech signal into a transcript in another language, • Machine translation (MT), i.e. the tr"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.15,0,0.0679155,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.14,0,0.0614042,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.7,1,0.877515,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.13,0,0.0364543,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.5,1,0.82235,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.12,0,0.0879094,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.21,0,0.047922,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.10,0,0.0535615,"Missing"
2014.iwslt-evaluation.1,1993.eamt-1.1,0,0.523227,"Missing"
2014.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.0265085,"time that Italian is involved in ASR/SLT tracks, therefore no evaluation set is available for assessing progress. A single TEDx based development set was released for each pair, together with standard TED based development sets dev2010, tst2010, tst2011 and tst2012 sets. Tables 2 and 3 provides statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [33] was applied to all languages, with the exception of Chinese and Arabic languages, which were sent 179k 887 1,664 818 1,124 1,026 1,305 172k 887 1,565 1,433 1,700 993 1,305 1,165 1,363 1,414 182k 887 1,529 1,433 1,704 1,402 1,183 1,056 883 tokens talks En Fr 3.63M 3.88M 1415 20,1k 20,2k 8 32,0k 33,9k 11 14,5k 15,6k 8 21,5k 23,5k 11 21,7k 23,3k 16 24,8k 27,5k 15 En De 3.46M 3.24M 1361 20,1k 19,1k 8 32,0k 30,3k 11 26,9k 26,3k 16 30,7k 29,2k 15 20,9k 19,7k 16 24,8k 23,8k 15 21,6k 20,8k 7 23,3k 22,4k 9 28,1k 27,6k 10 En It 3.68M 3.44M 1434 20,1k 17,9k 8 31,0k 28,7k 10 26,9k 24,5k 16 30,7k 28,2k 15"
2014.iwslt-evaluation.1,2012.amta-papers.22,1,0.779032,"amely the MT English-German (EnDe) track and MT English-French (EnF r) track. Following the methodology introduced last year, human evaluation was based on PostEditing, and HTER (Human-mediated Translation Edit Rate) was adopted as the official evaluation metric to rank the systems. Post-Editing, i.e. the manual correction of machine translation output, has long been investigated by the translation industry as a form of machine assistance to reduce the costs of human translation. Nowadays, Computer-aided translation (CAT) tools incorporate post-editing functionalities, and a number of studies [35, 36] demonstrate the usefulness of MT to increase professional translators’ productivity. The MT TED task offered in IWSLT can be seen as an interesting application scenario to test the utility of MT systems in a real subtitling task. 5.4. Results Table 4: BLEU and TER scores of baseline SMT systems on all tst2014 sets. († ) TEDx test set. (⋆ ) Char-level scores. pair En Fr De It Ar Es Fa He Nl Pl Pt Ro Ru Sl Tr Zh direction BLEU 32.07 18.33 27.15 11.13 31.31 11.31 15.91 22.77 9.63 31.25 18.05 11.74 8.46 7.75 ⋆ 16.49 → TER 48.62 62.11 53.19 73.01 48.29 71.20 65.62 58.38 82.81 47.25 65.25 71.99 73."
2014.iwslt-evaluation.1,2006.amta-papers.25,0,0.397152,"paign, our goal was to adopt a human evaluation framework able to maximize the benefit to the research community, both in terms of information about MT systems and data and resources to be reused. With respect to other types of human assessment, such as judgments of translation quality (i.e. adequacy/fluency and ranking tasks), the post-editing task has the double advantage of producing (i) a set of edits pointing to specific translation errors, and (ii) a set of additional reference translations. Both these byproducts are very useful for MT system development and evaluation. Furthermore, HTER[37] - which consists of measuring the minimum edit distance between the machine translation and its manually post-edited version - has been shown to correlate quite well with human judgments of MT quality. First of all, for reference purposes Table 4 shows BLEU and TER scores on the tst2014 evaluation sets of the baseline systems we developed as described in Section 5.1. The results on the official test set for each participant are shown in Appendix A.1. For most languages, we show the case-sensitive and case-insensitive BLEU and TER scores. The human evaluation setup and the collection of posted"
2014.iwslt-evaluation.1,J93-3001,0,0.560866,"Missing"
2014.iwslt-evaluation.7,D14-1003,1,0.921764,"slation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. In this work, we describe the EU-BRIDGE submissions to the 2014 IWSLT translation task. This year, we combined several single systems of RWTH, UEDIN, KIT, and FBK for the German→English SLT, German→English MT, English→German MT, and English→French MT tasks. Additionally to the standard system combination pipeline presented in [1, 2], we applied a recurrent neural network rescoring step [3] for the English→French MT task. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in previous joint submissions, e.g. [4, 5]. 2. RWTH Aachen University RWTH applied the identical training pipeline and models on both language pairs: The state-of-the-art phrase-based baseline systems were augmented with a hierarchical reordering model, several additional language models (LMs) and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translat"
2014.iwslt-evaluation.7,W10-1738,1,0.885248,"and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-"
2014.iwslt-evaluation.7,P03-1021,0,0.488353,"employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing."
2014.iwslt-evaluation.7,popovic-ney-2006-pos,1,0.798687,"th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selectio"
2014.iwslt-evaluation.7,P13-2121,1,0.819366,"mplemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWT"
2014.iwslt-evaluation.7,P10-2041,0,0.0916594,"rdering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU ob"
2014.iwslt-evaluation.7,E99-1010,0,0.0737032,"them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for"
2014.iwslt-evaluation.7,D13-1138,1,0.85854,"RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumv"
2014.iwslt-evaluation.7,P12-1031,0,0.0125863,"lection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and"
2014.iwslt-evaluation.7,P10-1049,1,0.833909,"the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LST"
2014.iwslt-evaluation.7,D14-1132,0,0.157332,"M. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficienc"
2014.iwslt-evaluation.7,2011.iwslt-papers.7,1,0.944851,"ort-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficiency as described in [20]. All neural networks were trained on the TED portion of the data with 2000 word classes. In addition to the recurrent language model (RNN-LM), RWTH applied the deep bidirectional word-based translation model (RNN-BTM) described in [3], which is capable of taking the full source context into account for each translation decision. Spoken Language Translation For the SLT task, RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the f"
2014.iwslt-evaluation.7,2014.iwslt-papers.17,1,0.734908,"RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided"
2014.iwslt-evaluation.7,P07-2045,1,0.0190208,"n hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26]"
2014.iwslt-evaluation.7,N04-1035,0,0.0565459,"equences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [2"
2014.iwslt-evaluation.7,W08-0509,0,0.192359,"[24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six"
2014.iwslt-evaluation.7,N13-1073,0,0.0453396,"e syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sp"
2014.iwslt-evaluation.7,C14-1041,1,0.839592,"ndividual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable"
2014.iwslt-evaluation.7,N12-1047,0,0.0681194,"them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is"
2014.iwslt-evaluation.7,P02-1040,0,0.0918061,"d to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by trai"
2014.iwslt-evaluation.7,2006.iwslt-papers.1,1,0.862433,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,2012.iwslt-papers.15,1,0.927241,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,P05-1066,1,0.733044,"ferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the"
2014.iwslt-evaluation.7,E03-1076,1,0.858704,"xt before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE sy"
2014.iwslt-evaluation.7,2012.amta-papers.9,1,0.84942,"arallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE system combination. Both comprise Brown clusters with 200 classes as additional factors on source and target"
2014.iwslt-evaluation.7,D08-1089,0,0.176922,"ign [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation a"
2014.iwslt-evaluation.7,W14-3324,1,0.784121,"ical tag. UEDIN-A was trained with all corpora, whereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house ph"
2014.iwslt-evaluation.7,2012.iwslt-papers.17,1,0.881764,"ain 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic diff"
2014.iwslt-evaluation.7,C04-1024,0,0.0400394,"ereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models wer"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.18,1,0.873679,"ined on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standa"
2014.iwslt-evaluation.7,W14-3362,1,0.610881,"N-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for"
2014.iwslt-evaluation.7,W14-4018,1,0.774295,"ptimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In t"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.9,1,0.861968,"m with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were"
2014.iwslt-evaluation.7,2007.tmi-papers.21,0,0.0614729,"ta. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabiliti"
2014.iwslt-evaluation.7,W09-0413,1,0.842557,"pora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the tra"
2014.iwslt-evaluation.7,W13-0805,1,0.85195,"ifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual langu"
2014.iwslt-evaluation.7,W08-1006,0,0.0150981,"k, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the ta"
2014.iwslt-evaluation.7,2012.amta-papers.19,1,0.839901,"e rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WI"
2014.iwslt-evaluation.7,W11-2124,1,0.902739,"for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cl"
2014.iwslt-evaluation.7,W13-2264,1,0.835602,"ed by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cluster-based 4-gram LM was trained on 500 clusters. For English→German"
2014.iwslt-evaluation.7,2012.eamt-1.60,1,0.892622,"Missing"
2014.iwslt-evaluation.7,D11-1033,0,0.167316,"Missing"
2014.iwslt-evaluation.7,W05-0909,0,0.085167,"m multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University [1]. In Fig. 1 an overview is illustrated. We first address the generation of a confusion network (CN) from I input translations. For that we need a pairwise alignment between all input hypotheses. This alignment is calculated via METEOR [60]. The hypotheses are then reordered to match the word order of a selected skeleton hypothesis. Instead of using only one of the input hypothesis as skeleton, we generate I different CNs, each having one of the input systems as skeleton. The final lattice is the union of all I previous generated CNs. In Fig. 2 an example confusion network of I = 4 input translations with one skeleton translation is illustrated. Between two adjacent nodes, we always have a choice between the I different system output words. The confusion network decoding step involves determining the shortest path through the ne"
2014.iwslt-evaluation.7,2006.amta-papers.25,0,0.0356913,"andard set of models is a word penalty, a 3-gram language model trained on the input hypotheses, and for each system one binary voting feature. During decoding the binary voting feature for system i (1 ≤ i ≤ I) is 1 iff the word is from system i, otherwise 0. The M different model weights λm are trained with MERT [8]. the red cab the a a red blue green train car car Figure 2: System A: the red cab ; System B: the red train ; System C: a blue car ; System D: a green car ; Reference: the blue car . 7. Results In this section, we present our experimental results. All reported B LEU [34] and T ER [61] scores are case-sensitive with one reference. All system combination results have been generated with RWTH’s open source system combination implementation Jane [1]. German→English SLT For the German→English SLT task, we combined three different individual systems generated by UEDIN, KIT, and RWTH. Experimental results are given in Table 1. The final system combination yields improvements of 1.5 points in B LEU and 1.2 points in T ER compared to the best single system (KIT). All single systems as well as the system combination parameters were tuned on dev2012. For this year’s IWSLT SLT track,"
2014.iwslt-evaluation.7,E06-1005,1,\N,Missing
2014.iwslt-evaluation.7,P11-1105,1,\N,Missing
2014.iwslt-evaluation.7,W10-1711,1,\N,Missing
2014.iwslt-evaluation.7,2010.iwslt-evaluation.22,1,\N,Missing
2014.iwslt-evaluation.7,E14-2008,1,\N,Missing
2014.iwslt-evaluation.7,2014.iwslt-evaluation.6,1,\N,Missing
2014.iwslt-evaluation.7,J03-1002,1,\N,Missing
2014.iwslt-evaluation.7,C12-3061,1,\N,Missing
2014.iwslt-evaluation.7,2013.iwslt-evaluation.16,1,\N,Missing
2014.iwslt-evaluation.7,W14-3310,1,\N,Missing
2015.iwslt-evaluation.1,2005.iwslt-1.19,0,\N,Missing
2015.iwslt-evaluation.1,2007.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2004.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,J93-3001,0,\N,Missing
2015.iwslt-evaluation.1,W05-0908,0,\N,Missing
2015.iwslt-evaluation.1,2005.mtsummit-papers.11,0,\N,Missing
2015.iwslt-evaluation.1,2015.iwslt-evaluation.16,0,\N,Missing
2015.iwslt-evaluation.1,2006.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2008.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2011.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,2009.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2012.eamt-1.60,1,\N,Missing
2015.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,\N,Missing
2021.acl-long.224,D11-1033,0,0.0217744,"Missing"
2021.acl-long.224,2020.iwslt-1.3,0,0.734347,"ith ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al., 2017) integrating the encoder with: i) convolutional layers to reduce input length, and ii) penalties bias"
2021.acl-long.224,N19-1006,0,0.0282767,"The problem has been mainly tackled with data augmentation and knowledge transfer techniques. Data augmentation consists in producing artificial training corpora by altering existing datasets or by generating (audio, translation) pairs through speech synthesis or MT (Bahar et al., 2019b; Nguyen et al., 2020; Ko et al., 2015; Jia et al., 2019). Knowledge transfer (Gutstein et al., 2008) consists in passing (here to ST) the knowledge learnt by a neural network trained on closely related tasks (here, ASR and MT). Existing ASR models have been used for encoder pre-training (B´erard et al., 2018; Bansal et al., 2019; Bahar et al., 2019a) and multi-task learning (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Indurthi et al., 2020). Existing neural MT models have been used for decoder pre-training (Bahar et al., 2019a; Inaguma et al., 2020), joint learning (Indurthi et al., 2020; Liu et al., 2020) and knowledge distillation (Liu et al., 2019). Previous comparisons. Most of the works on direct ST also evaluate the proposed solutions against a cascade counterpart. The conclusions, however, are discordant. Looking at recent works, Pino et al. (2019) show similar scores, Indurthi et al. (2020) report hi"
2021.acl-long.224,D19-5304,0,0.0221619,"ffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first dire"
2021.acl-long.224,W04-3250,0,0.391428,"ww.cs.umd.edu/˜snover/tercom 6 BLEU8 (Post, 2018) and TER scores computed only on the official MuST-C Common references. C D C es D C it D de HTER 28.65 30.22 29.96 28.19∗ 25.69 26.14 PE Set mTER BLEU 24.41 28.96 25.60 28.46 25.30 34.05∗ 24.02∗ 32.17 23.29 30.04∗ 23.26 28.81 TER 53.23 52.56 50.75 51.08 54.01 54.06 M. Common BLEU TER 28.86 53.93 29.05 52.77∗ 32.93∗ 53.21∗ 31.98 54.00 28.56 56.29 28.56 55.35∗ Table 1: Performance of (C)ascade and (D)irect systems on the PE-sets and MuST-C Common test sets. Statistically significant differences (∗ ) are computed with Paired Bootstrap Resampling (Koehn, 2004). A bird’s-eye view of the results shows that, in more than half of the cases, performance differences between cascade and direct systems are not statistically significant. When they are, the raw count of wins for the two approaches is the same (4), attesting their substantial parity. Looking at our primary metrics (HTER and mTER), systems are on par on en-it and en-de, while for en-es the direct approach significantly outperforms the cascade one. This difference, however, does not emerge with the other metrics. Indeed, BLEU and TER scores computed against the official references are less cohe"
2021.acl-long.224,C96-1075,0,0.130421,"bility across languages and domains. At the same time, however, they suffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models,"
2021.acl-long.224,W18-1818,0,0.0340255,"Missing"
2021.acl-long.224,N19-4009,0,0.01971,"Missing"
2021.acl-long.224,2012.iwslt-papers.18,0,0.0208779,"hey suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and P"
2021.acl-long.224,2006.amta-papers.25,0,0.267854,", we manually checked the selected samples and kept only those segments for which the audio-transcripttranslation alignment was correct. Each of the three resulting test sets – henceforth PE-sets – is composed of 550 segments, corresponding to about 10,000 English source words. Post-editing. A key element of our multi-faceted analysis is human post-editing (PE), which consists in manually correcting systems’ output according to the input (the source audio in our case). In PEbased evaluation, the original output is compared against its post-edited version using distance-based metrics like TER (Snover et al., 2006). This allows for counting only the true errors made by a system, without penalising differences due to linguistic variation as it happens when exploiting independent references. This makes PE-based evaluation one of the most prominent methodologies used for translation quality assessment (Snover et al., 2006, 2009; Denkowski and Lavie, 2010; Cettolo et al., 2013; Bojar et al., 2015; Graham et al., 2016; Bentivogli et al., 2018b). To collect the post-edits for our study, we strictly followed the methodology of the IWSLT 20132017 evaluation campaigns (Cettolo et al., 2013), which offered us a c"
2021.acl-long.224,W09-0441,0,0.0747003,"Missing"
2021.acl-long.224,P19-1115,0,0.0127146,"n problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data structures (e.g. ASR n-best, lattices or confusion networks) which are more informative than the 1-best output (Lavie et al., 1996; Matusov et al., 2005; Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019), and ii) making the MT robust to ASR errors, for instance by training it on parallel data incorporating real or emulated ASR errors as in (Peitz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing"
2021.acl-long.224,W18-6319,0,0.0184861,"based7 metrics: i) human-targeted TER (HTER) computed between the automatic translation and its human post-edited version, and ii) multi-reference TER (mTER) computed against the closest reference among the three available ones (two post-edits and the official reference from MuST-C). The latter metric better accounts for post-editors’ variability, making the evaluation more reliable and informative. For the sake of completeness, in Table 1 we also report Sacre5 www.matecat.com The ad-hoc ST PE guidelines given to translators are included in Appendix B. 7 www.cs.umd.edu/˜snover/tercom 6 BLEU8 (Post, 2018) and TER scores computed only on the official MuST-C Common references. C D C es D C it D de HTER 28.65 30.22 29.96 28.19∗ 25.69 26.14 PE Set mTER BLEU 24.41 28.96 25.60 28.46 25.30 34.05∗ 24.02∗ 32.17 23.29 30.04∗ 23.26 28.81 TER 53.23 52.56 50.75 51.08 54.01 54.06 M. Common BLEU TER 28.86 53.93 29.05 52.77∗ 32.93∗ 53.21∗ 31.98 54.00 28.56 56.29 28.56 55.35∗ Table 1: Performance of (C)ascade and (D)irect systems on the PE-sets and MuST-C Common test sets. Statistically significant differences (∗ ) are computed with Paired Bootstrap Resampling (Koehn, 2004). A bird’s-eye view of the results sh"
2021.acl-long.224,2020.iwslt-1.9,0,0.44219,"tz et al., 2012; Ruiz et al., 2015; Sperber et al., 2017; Cheng et al., 2019; Di Gangi et al., 2019a). Although the former solutions are effective to some extent, state-of-theart cascade architectures (Pham et al., 2019; Bahar et al., 2020) prefer the latter, as they are simpler to implement and maintain. Direct ST. To overcome the limitations of cascade models, B´erard et al. (2016) and Weiss et al. (2017) proposed the first direct solutions bypassing intermediate representations by means of encoderdecoder architectures based on recurrent neural networks. Currently, more effective solutions (Potapczyk and Przybysz, 2020; Bahar et al., 2020; Gaido et al., 2020) rely on ST-oriented adaptations of Transformer (Vaswani et al., 2017) integrating the encoder with: i) convolutional layers to reduce input length, and ii) penalties biasing attention to local context in the encoder self-attention layers (Povey et al., 2018; Sperber et al., 2018; Di Gangi et al., 2019b). Though effective, these architectures have to confront with training data paucity, a critical bottleneck for neural solutions. The problem has been mainly tackled with data augmentation and knowledge transfer techniques. Data augmentation consists in p"
2021.acl-long.224,2020.acl-main.661,0,0.0171621,"ences 5 240 191 45 74 215 234 54 47 231 212 55 52 6.15 6.00 6.68 2.71 5.92 6.28 6.47 3.09 6.03 6.06 6.93 2.96 14.43 14.52 14.31 15.53 12.20 12.09 12.01 13.14 12.31 12.21 11.94 12.68 19.75 18.88 22.07 9.64 19.52 20.39 20.26 10.23 19.41 19.33 21.73 10.33 16.30 44.85 40.74 0 16.09 46.45 40.22 0 14.82 37.65 35.39 0 40.53 17.89 40.18 0 38.76 21.14 40.37 0 36.40 15.80 35.37 0 Table 2: Comparison of (C)ascade and (D)irect performance based on different audio properties. In particular, although suffering from the wellknown scarcity of sizeable training corpora, direct solutions come with the promise (Sperber and Paulik, 2020) of: i) higher robustness to error propagation, and ii) reduced loss of speech information (e.g. prosody). Our next qualitative analysis tries to delve into these aspects by looking at audio understanding and prosody issues. Audio understanding. Errors due to wrong audio understanding are easy to identify for cascade systems – since they are evident in the intermediate ASR transcripts – but harder to spot for direct systems, whose internal representations are by far less accessible. In this case, errors can still be identified in mistranslations corresponding to words which are phonetically si"
2021.acl-long.224,2016.eamt-2.8,0,0.113294,"Missing"
2021.acl-long.224,1991.mtsummit-papers.18,0,0.578059,"ng errors (§6). We finally explore whether, due to latent characteristics overlooked by all previous investigations, the output of cascade and direct systems can be distinguished either by a human or by an automatic classifier (§7). Together with a comparative study attesting the parity of the two paradigms on our test data, another contribution of this paper is the release of the manual post-edits that rendered our investigation possible. The data is available at: https://ict.fbk.eu/mustc-post-edits. 2 Background Cascade ST. By concatenating ASR and MT components (Stentiford and Steer, 1988; Waibel et al., 1991), cascade ST architectures represent an intuitive solution to achieve reasonable performance and high adaptability across languages and domains. At the same time, however, they suffer from well-known problems related to the concatenation of multiple systems. First, they require ad-hoc training and maintenance procedures for the ASR and MT modules; second, they suffer from error propagation and from the loss of speech information (e.g. prosody) that might be useful to improve final translations. Research has focused on mitigating error propagation by: i) feeding the MT system with ASR data stru"
2021.eacl-main.57,2020.amta-research.13,1,0.844364,"Missing"
2021.eacl-main.57,W19-6603,1,0.853997,"Missing"
2021.eacl-main.57,N19-4009,0,0.0221702,"ennrich et al., 2016). As having more encoder layers than decoder layers has been shown to be beneficial (Potapczyk and Przybysz, 2020; Gaido et al., 2020), we use 8 Transformer encoder layers and 6 decoder layers for ASR and 11 encoder and 4 decoder layers for ST unless stated otherwise. We train until the 692 1 2 https://lowerquality.com/gentle/ http://www.voxforge.org/home model does not improve on the validation set for 5 epochs and we average the last 5 checkpoints. Trainings were performed on K80 GPUs and lasted ~48 hours (~50 minutes per epoch). Our implementation3 is based on Fairseq (Ott et al., 2019). We evaluate performance with WER for ASR and with BLEU (Papineni et al., 2002)4 and SacreBLEU (Post, 2018)5 for ST. Baseline - 8L EN 8L PH 2L PH AVG 4L PH AVG 8L PH AVG 8L PH W/O POS. AVG 8L EN AVG WER (↓) 16.0 15.6 21.2 17.5 16.3 16.4 16.3 RAM (MB) 6929 (1.00) 6661 (0.96) 3375 (0.49) 4542 (0.66) 6286 (0.91) 6565 (0.95) 6068 (0.88) Table 1: Results on ASR using the CTC loss with transcripts and phones as target. AVG indicates that sequence is compressed averaging the vectors. 5 5.1 Results ASR We first tested whether ASR benefits from the usage of phones and sequence compression. Table 1 sho"
2021.eacl-main.57,2020.iwslt-1.8,1,0.895243,"Missing"
2021.eacl-main.57,2020.conll-1.22,0,0.0350439,"input sequence assume that samples carry the same amount of information. This does not necessarily hold true, as phonetic features vary at a different speed in time and frequency in the audio signals. Consequently, researchers have studied how to reduce the input length according to dynamic criteria based on the audio content. Salesky et al. (2019) demonstrated that a phoneme-based compression of the input frames yields significant gains compared to fixed length reduction. Phone-based and linguistically-informed compression also proved to be useful in the context of visually grounded speech (Havard et al., 2020). However, Salesky and Black (2020) questioned the approach, claiming that the addition of phone features without segmentation and compression of the input is more effective. None of these works is a direct ST solution, as they all require a separate model for phone recog690 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 690–696 April 19 - 23, 2021. ©2021 Association for Computational Linguistics nition and intermediate representations. So, they: i) are affected by error propagation (Salesky and Black 2020 show in fact that lo"
2021.eacl-main.57,2020.acl-demos.34,0,0.142221,"Missing"
2021.eacl-main.57,P02-1040,0,0.113243,"s been shown to be beneficial (Potapczyk and Przybysz, 2020; Gaido et al., 2020), we use 8 Transformer encoder layers and 6 decoder layers for ASR and 11 encoder and 4 decoder layers for ST unless stated otherwise. We train until the 692 1 2 https://lowerquality.com/gentle/ http://www.voxforge.org/home model does not improve on the validation set for 5 epochs and we average the last 5 checkpoints. Trainings were performed on K80 GPUs and lasted ~48 hours (~50 minutes per epoch). Our implementation3 is based on Fairseq (Ott et al., 2019). We evaluate performance with WER for ASR and with BLEU (Papineni et al., 2002)4 and SacreBLEU (Post, 2018)5 for ST. Baseline - 8L EN 8L PH 2L PH AVG 4L PH AVG 8L PH AVG 8L PH W/O POS. AVG 8L EN AVG WER (↓) 16.0 15.6 21.2 17.5 16.3 16.4 16.3 RAM (MB) 6929 (1.00) 6661 (0.96) 3375 (0.49) 4542 (0.66) 6286 (0.91) 6565 (0.95) 6068 (0.88) Table 1: Results on ASR using the CTC loss with transcripts and phones as target. AVG indicates that sequence is compressed averaging the vectors. 5 5.1 Results ASR We first tested whether ASR benefits from the usage of phones and sequence compression. Table 1 shows that having phones instead of English transcripts (Baseline - 8L EN) as targe"
2021.eacl-main.57,W18-6319,0,0.0241895,"k and Przybysz, 2020; Gaido et al., 2020), we use 8 Transformer encoder layers and 6 decoder layers for ASR and 11 encoder and 4 decoder layers for ST unless stated otherwise. We train until the 692 1 2 https://lowerquality.com/gentle/ http://www.voxforge.org/home model does not improve on the validation set for 5 epochs and we average the last 5 checkpoints. Trainings were performed on K80 GPUs and lasted ~48 hours (~50 minutes per epoch). Our implementation3 is based on Fairseq (Ott et al., 2019). We evaluate performance with WER for ASR and with BLEU (Papineni et al., 2002)4 and SacreBLEU (Post, 2018)5 for ST. Baseline - 8L EN 8L PH 2L PH AVG 4L PH AVG 8L PH AVG 8L PH W/O POS. AVG 8L EN AVG WER (↓) 16.0 15.6 21.2 17.5 16.3 16.4 16.3 RAM (MB) 6929 (1.00) 6661 (0.96) 3375 (0.49) 4542 (0.66) 6286 (0.91) 6565 (0.95) 6068 (0.88) Table 1: Results on ASR using the CTC loss with transcripts and phones as target. AVG indicates that sequence is compressed averaging the vectors. 5 5.1 Results ASR We first tested whether ASR benefits from the usage of phones and sequence compression. Table 1 shows that having phones instead of English transcripts (Baseline - 8L EN) as target of the CTC loss (8L PH) wi"
2021.eacl-main.57,2020.iwslt-1.9,0,0.0991299,"Missing"
2021.eacl-main.57,2020.acl-main.217,0,0.027471,"ples carry the same amount of information. This does not necessarily hold true, as phonetic features vary at a different speed in time and frequency in the audio signals. Consequently, researchers have studied how to reduce the input length according to dynamic criteria based on the audio content. Salesky et al. (2019) demonstrated that a phoneme-based compression of the input frames yields significant gains compared to fixed length reduction. Phone-based and linguistically-informed compression also proved to be useful in the context of visually grounded speech (Havard et al., 2020). However, Salesky and Black (2020) questioned the approach, claiming that the addition of phone features without segmentation and compression of the input is more effective. None of these works is a direct ST solution, as they all require a separate model for phone recog690 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 690–696 April 19 - 23, 2021. ©2021 Association for Computational Linguistics nition and intermediate representations. So, they: i) are affected by error propagation (Salesky and Black 2020 show in fact that lower quality in phone recognition si"
2021.eacl-main.57,P19-1179,0,0.0969688,"Missing"
2021.eacl-main.57,W04-3250,0,0.668023,"Missing"
2021.eacl-main.57,1991.mtsummit-papers.18,0,0.666171,"Missing"
C14-2028,2013.mtsummit-papers.5,1,0.880457,"Missing"
C14-2028,2013.mtsummit-wptp.13,1,0.900099,"Missing"
C14-2028,2012.amta-papers.22,1,0.865102,"Missing"
C14-2028,2013.mtsummit-wptp.10,0,0.135035,"Missing"
C14-2028,W13-2231,1,0.678754,"Missing"
C14-2028,P14-1067,1,0.806908,"Missing"
C14-2028,2013.mtsummit-wptp.7,1,\N,Missing
C18-1054,D16-1025,1,0.920078,"valuates different implementations of the attention mechanism, the comprehension of what a model can learn and the errors it makes has been drawing much attention of the research community, as This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 641 Proceedings of the 27th International Conference on Computational Linguistics, pages 641–652 Santa Fe, New Mexico, USA, August 20-26, 2018. evidenced by the number of recent publications aiming at comparing the behavior of neural vs. phrasebased systems (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017; Bentivogli et al., 2018). However, understanding the capability of multilingual NMT models in general and zero-shot translation, in particular, has not been thoroughly analyzed yet. By taking the bilingual model as the reference, this work quantitatively analyzes the translation outputs of multilingual and zero-shot models, aiming at answering the following research questions: • How do bilingual, multilingual, and zero-shot systems compare in terms of general translation quality? Is there any translation aspect better modeled by each specific system? • How"
C18-1054,P11-2031,0,0.0243927,"eferences of the official IWSLT test sets. In addition, two TER-based scores are reported, namely the multiple-reference TER (mTER) and a lemma-based TER (lmmTER), which are instead computed on the nine post-edits of the IWSLT 2017 human evaluation set. In mTER, TER is computed by counting, for each segment of the MT output, the minimum number of edits across all the references and dividing by the average length of references. lmmTER is computed similarly to mTER but looking for matches at the lemma level instead of surface forms. Significance tests for all scores are reported using Multeval (Clark et al., 2011) tool. Systems are also compared in terms of three well known and widely used error categories, that is lexical, morphological, and word order errors, exploiting TER and post-edits as follows. First, the MT outputs 2 3 https://github.com/OpenNMT/OpenNMT-py A script from the Moses SMT toolkithttp://www.statmt.org/moses 645 Direction Recurrent System Transformer BLEU TER mTER lmmTER Nl→De NMT M-NMT ZST 18.05 17.79 17.06 64.61 66.18 65.73 23.70 21.75 26.35 20.60 18.28 22.29 Ro→It NMT M-NMT ZST 22.16 21.69 18.72 59.35 59.50 62.08 22.99 21.12 29.66 20.39 18.46 26.15 BLEU TER mTER lmmTER 18.37 19.95"
C18-1054,W17-3203,0,0.0199033,"reprocessing The experimental setting comprises seven languages; for each language pair, we use the ≈200,000 parallel sentences made publicly available by the IWSLT 2017 evaluation campaign (Cettolo et al., 2017), partitioned in training, development, and test sets. In the preprocessing pipeline, the raw data is first tokenized and cleaned by removing empty lines. Then, a shared byte pair encoding (BPE) model (Sennrich et al., 2015) is trained using the union of the source and target sides of the training data. The number of BPE segmentation rules is set to 8, 000, following the suggestion of Denkowski and Neubig (2017) for experiments in small training data condition. For the case of Transformer training, the internal sub-word segmentation (Wu et al., 2016) provided by the Tensor2Tensor library1 is used. Note that prepending the “language-flag” on the source side of the corpus is specific to the multilingual and zero-shot models. 4.2 Evaluation data For our investigation, we exploit the nine post-edits available from the IWSLT 2017 evaluation campaign. Post-editing regarded the bilingual, multilingual, and zero-shot runs of three different participants to the two tasks Dutch (Nl)→German (De) and Romanian (R"
C18-1054,P15-1166,0,0.0797078,"lo et al., 2017), WMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurrent neural networks, initially introduced in Sutskever et al. (2014; Cho et al."
C18-1054,N16-1101,0,0.0607578,"), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurrent neural networks, initially introduced in Sutskever et al. (2014; Cho et al. (2014), convolutional neural networks, prop"
C18-1054,P17-4012,0,0.0416632,"ing also De↔Ro and Nl↔It data Zero-shot, trained as ZST A but adding En↔Fr/Es data Table 2: The training setting of 4*bilingual, 1*multilingual, and 3*zero-shot systems. 4.3 Training setting Each of the three system types, namely bilingual, multilingual and zero-shot, is trained using both Recurrent and Transformer architectures, with the proper training data provided in the IWSLT 2017 evaluation campaign. Meta training parameters were set in a preliminary stage with the aim of maximizing the quality of each approach. Recurrent NMT experiments are carried out using the open source OpenNMTpy2 (Klein et al., 2017), whereas the Transformer models are trained using the Tensor2Tensor toolkit. Hence, we took the precaution of selecting the optimal training and inference parameters for both approaches and toolkits. For instance, for our low-resource setting characterized by a high data sparsity, the dropout (Srivastava et al., 2014) is set to 0.3 (Gal and Ghahramani, 2016) in Recurrent models and to 0.2 in Transformer models to prevent over-fitting. Similarly, Adam (Kingma and Ba, 2014) optimizer with an initial learning rate of either 0.001 (RNN) or 0.2 (Transformer) is used. If the perplexity does not dec"
C18-1054,W17-3204,0,0.0351809,"setting (§4.3), models (§4.4) and the evaluation methods (§4.5). In Section (§5), we analyze the overall translation quality for related and unrelated language directions. Before the summary and conclusion, we will focus on lexical, morphological and word-order error types for the fine-grained analysis (§6). 2 Related Work Recent trends in NMT evaluation show that post-editing helps to identify and address the weakness of systems (Bentivogli et al., 2018). Furthermore, the use of multiple post-edits in addition to the manual reference is gaining more and more ground (Bentivogli et al., 2016; Koehn and Knowles, 2017; Toral and S´anchez-Cartagena, 2017; Bentivogli et al., 2018). For our investigation, we follow the error analysis approach defined in Bentivogli et al. (2018), where multiple post-edits are exploited in order to quantify morphological, lexical, and word order errors, a simplified error classification with respect to that proposed in Vilar et al. (2006), which settles two additional classes, namely missing and extra words. The first work that compares bilingual, multilingual, and zero-shot systems comes from the IWSLT 2017 evaluation campaign (Cettolo et al., 2017). The authors analyze the ou"
C18-1054,P16-1160,0,0.023201,"evaluation campaigns (IWSLT (Cettolo et al., 2017), WMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurrent neural networks, initially introduced in"
C18-1054,D15-1166,0,0.53415,"As witnessed by recent machine translation evaluation campaigns (IWSLT (Cettolo et al., 2017), WMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurren"
C18-1054,P02-1040,0,0.101105,"T) trained in all directions in the set {En,De,Nl,It,Ro}. Then, we test zero-shot translation (ZST) between related languages, namely Nl→De and Ro→It, by training a multilingual NMT without any data for these language pairs. We also test zero-shot translation between unrelated languages (ZST A), namely Ro→De and Nl→It, by excluding parallel data between these languages. Finally, for the same unrelated zero-shot directions we also train multi-lingual systems (ZST B) that include data related to Romanian and Italian, namely En↔Fr/Es. 4.5 Evaluation methods Systems are compared in terms of BLEU (Papineni et al., 2002) (as implemented in multi-bleu.perl 3 ) and TER (Snover et al., 2006) scores, on the single references of the official IWSLT test sets. In addition, two TER-based scores are reported, namely the multiple-reference TER (mTER) and a lemma-based TER (lmmTER), which are instead computed on the nine post-edits of the IWSLT 2017 human evaluation set. In mTER, TER is computed by counting, for each segment of the MT output, the minimum number of edits across all the references and dividing by the average length of references. lmmTER is computed similarly to mTER but looking for matches at the lemma le"
C18-1054,R13-1079,0,0.0131769,"18.46 26.15 BLEU TER mTER lmmTER 18.37 19.95 ↑ 19.13 63.74 61.90 62.69 27.95 23.62 25.19 23.86 20.05 21.53 22.48 22.12 ↑ 21.29 57.34 57.51 59.08 26.60 25.05 26.93 23.36 21.57 23.33 ↑ ↑ Table 3: Automatic scores on tasks involving related languages. BLEU and TER are computed on test2017, while mTER and lmmTER are reported for human evaluation sets. Best scores of the Transformer model against the Recurrent are highlighted in bold, whereas arrow ↑ indicates statistically significant differences (p &lt; 0.05). and the corresponding post-edits are lemmatized and POS-tagged; for that, we used ParZu (Sennrich et al., 2013) for German and TreeTagger (Schmid, 1994) for Italian. Then, the lemmatized outputs are evaluated against the corresponding post-edits via a variant of the tercom implementation4 of TER: in addition to computing TER, the tool provides complete information about matching lemmas, as well as shift (matches after displacements), insertion, deletion, and substitution operations. Since for each lemma the tool keeps track of the corresponding original word form and POS tag, we are able to measure the number of errors falling in the three error categories, following the scheme described in detail in B"
C18-1054,N18-2074,0,0.0191071,"at are found in the previous approach.In other words, the attention mechanism is repurposed to compute the latent space representation of both the encoder and the decoder sides. However, with the absence of recurrence, positional-encoding is added to the input and output embeddings. Similarly, as the time-step in a recurrent network, the positional information provides the Transformer network with the order of input and output sequences. In our work, we use the absolute positional encoding, but very recently the use of the relative positional information has been shown to improve performance (Shaw et al., 2018). The model is organized as a stack of encoder-decoder networks that works in an auto-regressive way, using the previously generated symbol as input for the next prediction. Both the decoder and encoder can be composed of uniform layers, each built of two sublayers, i.e., a multi-head self-attention layer and a position wise feed-forward network (FFN) layer. The multi-head sub-layer enables the use of multiple attention functions with a similar cost of utilizing attention, while the FFN sub-layer is a fully connected network used to process the attention sublayers; as such, FFN applies two lin"
C18-1054,2006.amta-papers.25,0,0.109971,"zero-shot translation (ZST) between related languages, namely Nl→De and Ro→It, by training a multilingual NMT without any data for these language pairs. We also test zero-shot translation between unrelated languages (ZST A), namely Ro→De and Nl→It, by excluding parallel data between these languages. Finally, for the same unrelated zero-shot directions we also train multi-lingual systems (ZST B) that include data related to Romanian and Italian, namely En↔Fr/Es. 4.5 Evaluation methods Systems are compared in terms of BLEU (Papineni et al., 2002) (as implemented in multi-bleu.perl 3 ) and TER (Snover et al., 2006) scores, on the single references of the official IWSLT test sets. In addition, two TER-based scores are reported, namely the multiple-reference TER (mTER) and a lemma-based TER (lmmTER), which are instead computed on the nine post-edits of the IWSLT 2017 human evaluation set. In mTER, TER is computed by counting, for each segment of the MT output, the minimum number of edits across all the references and dividing by the average length of references. lmmTER is computed similarly to mTER but looking for matches at the lemma level instead of surface forms. Significance tests for all scores are r"
C18-1054,E17-1100,0,0.0382274,"Missing"
C18-1054,vilar-etal-2006-error,0,0.0926287,"ow that post-editing helps to identify and address the weakness of systems (Bentivogli et al., 2018). Furthermore, the use of multiple post-edits in addition to the manual reference is gaining more and more ground (Bentivogli et al., 2016; Koehn and Knowles, 2017; Toral and S´anchez-Cartagena, 2017; Bentivogli et al., 2018). For our investigation, we follow the error analysis approach defined in Bentivogli et al. (2018), where multiple post-edits are exploited in order to quantify morphological, lexical, and word order errors, a simplified error classification with respect to that proposed in Vilar et al. (2006), which settles two additional classes, namely missing and extra words. The first work that compares bilingual, multilingual, and zero-shot systems comes from the IWSLT 2017 evaluation campaign (Cettolo et al., 2017). The authors analyze the outputs of several systems through two human evaluation methods: direct assessment which focuses on the generic assessment of overall translation quality, and post-editing which directly measures the utility of a given MT output to translators. Post-edits are also exploited to run a fine-grained analysis of errors made by the systems. The main findings are"
C18-1054,N16-1004,0,0.0449802,"nt machine translation evaluation campaigns (IWSLT (Cettolo et al., 2017), WMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have been proposed and tested in neural machine translation (NMT). NMT models were mostly employed in conventional single language-pair settings, where the training process exploits a parallel corpus from a source language to a target language, and the inference involves only those two languages in the same direction. However, there have also been attempts to incorporate multiple languages in the source (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al., 2016), in the target (Dong et al., 2015), or in both sides like Firat et al. (2016) which combines a shared attention mechanism and multiple encoder-decoder layers. Regardless, the simple approach proposed in Johnson et al. (2016) and Ha et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and attention modules, and manages multilinguality by introducing an artificial token at the beginning of the input sentence to specify the requested target language. The current NMT state-of-the-art includes the use of recurrent neural networks, initi"
C18-1054,Q17-1026,0,\N,Missing
C18-1054,W17-4717,0,\N,Missing
C18-1054,P16-1162,0,\N,Missing
D16-1025,W13-2257,1,0.870008,"Missing"
D16-1025,P11-1059,0,0.0237569,"Missing"
D16-1025,2012.eamt-1.60,1,0.116875,"not applicable to NMT, which does not rely on a fixed inventory of translation units extracted from the parallel data. Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi´c et al., 2013). We are the first to conduct this kind of study on the output of a neural MT system. 3 Experimental Setting We perform a number of analyses on data and results of the IWSLT 2015 MT En-De task, which consists in translating manual transcripts of English TED talks into German. Evaluation data are publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated into many languages by volunteers worldwide. Besides representing a popular benchmark for spoken language technology, TED Talks embed interesting research challenges. Translating TED Talks implies dealing with spoken rather than written language, which is hence expected to be structurally less complex, formal and fluent (Ruiz and Federico, 2014). Moreover, as human translations of the talk"
D16-1025,W14-4012,0,0.0937737,"Missing"
D16-1025,D14-1179,0,0.0584623,"Missing"
D16-1025,daems-etal-2014-origin,0,0.0125712,"aspects are better modeled by different MT frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphologica"
D16-1025,N13-1073,0,0.0838252,"Missing"
D16-1025,D14-1172,1,0.84939,"modeled by different MT frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphological, reordering, missing"
D16-1025,fishel-etal-2012-terra,0,0.038989,"Missing"
D16-1025,1994.amta-1.9,0,0.196451,"s vocabulary and the variety of subject matter in a text. For the first two features we did not find any correlation; on the contrary, we found a moderate Pearson correlation (R=0.7332) between TTR and the mTER gains of NMT over its closest competitor in each talk. This result suggests that NMT is able to cope with lexical diversity better than any other considered approach. 5 Analysis of Translation Errors We now turn to analyze which types of linguistic errors characterize NMT vs. PBMT. In the literature, various error taxonomies covering different levels of granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014). We focus on three error categories, namely (i) morphology errors, (ii) lexical errors, and (iii) word order errors. As for lexical errors, a number of existing taxonomies further distinguish among translation errors due to missing words, extra words, or incorrect lexical choice. However, given the proven difficulty of disambiguating between these three subclasses (Popovi´c and Ney, 2011; Fishel et al., 2012), we prefer to rely on a more coarse-grained linguistic error classification where le"
D16-1025,D12-1043,0,0.0167871,"Missing"
D16-1025,2015.iwslt-evaluation.9,0,0.093503,"Missing"
D16-1025,2015.iwslt-evaluation.4,0,0.120054,"rephrasing and reordering is expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a representative subset of the official 2015 test set. The Human Evaluation (HE) set includes the first half of each of the 12 test 3 4 System PBSY (Huck and Birch, 2015) HPB (Jehl et al., 2015) SPB (Ha et al., 2015) NMT (Luong & Manning, 2015) Data 175M/ 3.1B 166M/ 854M 117M/ 2.4B 120M/ – Table 1: MT systems’ overview. Data column: size of parallel/monolingual training data for each system in terms of English and German tokens. talks, for a total of 600 sentences and around 10K words. Five professional translators were asked to post-edit the MT output by applying the minimal edits required to transform it into a fluent sentence with the same meaning as the source sentence. Data were prepared so that all translators equally post-edited the five MT outputs, i.e"
D16-1025,Q13-1035,0,0.0418296,"Missing"
D16-1025,P15-1001,0,0.0782404,"tationally costly and resource demanding to compete with state-of-the-art Phrase-Based MT (PBMT)1 , the situation changed in 2015. For the first time, in the latest edition of IWSLT2 (Cettolo et 1 We use the generic term phrase-based MT to cover standard phrase-based, hierarchical and syntax-based SMT approaches. 2 International Workshop on Spoken Language Translation (http://workshop2015.iwslt.org/) This impressive improvement follows the distance reduction previously observed in the WMT 2015 shared translation task (Bojar et al., 2015). Just few months earlier, the NMT systems described in (Jean et al., 2015b) ranked on par with the best phrase-based models on a couple of language pairs. Such rapid progress stems from the improvement of the recurrent neural network encoderdecoder model, originally proposed in (Sutskever et al., 2014; Cho et al., 2014b), with the use of the attention mechanism (Bahdanau et al., 2015). This evolution has several implications. On one side, NMT represents a simplification with respect to previous paradigms. From a management point of view, similar to PBMT, it allows for a more efficient use of human and data resources with respect to rulebased MT. From the architectu"
D16-1025,W15-3014,0,0.0509369,"tationally costly and resource demanding to compete with state-of-the-art Phrase-Based MT (PBMT)1 , the situation changed in 2015. For the first time, in the latest edition of IWSLT2 (Cettolo et 1 We use the generic term phrase-based MT to cover standard phrase-based, hierarchical and syntax-based SMT approaches. 2 International Workshop on Spoken Language Translation (http://workshop2015.iwslt.org/) This impressive improvement follows the distance reduction previously observed in the WMT 2015 shared translation task (Bojar et al., 2015). Just few months earlier, the NMT systems described in (Jean et al., 2015b) ranked on par with the best phrase-based models on a couple of language pairs. Such rapid progress stems from the improvement of the recurrent neural network encoderdecoder model, originally proposed in (Sutskever et al., 2014; Cho et al., 2014b), with the use of the attention mechanism (Bahdanau et al., 2015). This evolution has several implications. On one side, NMT represents a simplification with respect to previous paradigms. From a management point of view, similar to PBMT, it allows for a more efficient use of human and data resources with respect to rulebased MT. From the architectu"
D16-1025,2015.iwslt-evaluation.6,0,0.129399,"s expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a representative subset of the official 2015 test set. The Human Evaluation (HE) set includes the first half of each of the 12 test 3 4 System PBSY (Huck and Birch, 2015) HPB (Jehl et al., 2015) SPB (Ha et al., 2015) NMT (Luong & Manning, 2015) Data 175M/ 3.1B 166M/ 854M 117M/ 2.4B 120M/ – Table 1: MT systems’ overview. Data column: size of parallel/monolingual training data for each system in terms of English and German tokens. talks, for a total of 600 sentences and around 10K words. Five professional translators were asked to post-edit the MT output by applying the minimal edits required to transform it into a fluent sentence with the same meaning as the source sentence. Data were prepared so that all translators equally post-edited the five MT outputs, i.e. 120 sentences for each"
D16-1025,W12-3123,0,0.0131782,"(see also Section 3.4). Irvine et al. (2013) propose another word-level error analysis technique specifically focused on lexical choice and aimed at understanding the effects of domain differences on MT. Their error classification is strictly related to model coverage and insensitive to word order differences. The technique requires access to the system’s phrase table and is thus not applicable to NMT, which does not rely on a fixed inventory of translation units extracted from the parallel data. Previous error analyses based on manually postedited translations were presented in (Bojar, 2011; Koponen, 2012; Popovi´c et al., 2013). We are the first to conduct this kind of study on the output of a neural MT system. 3 Experimental Setting We perform a number of analyses on data and results of the IWSLT 2015 MT En-De task, which consists in translating manual transcripts of English TED talks into German. Evaluation data are publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated in"
D16-1025,2014.eamt-1.38,0,0.0817661,"Missing"
D16-1025,2015.iwslt-evaluation.11,0,0.0570707,"e. 120 sentences for each evaluated system. The resulting evaluation data consist of five new reference translations for each of the sentences in the HE set. Each one of these references represents the targeted translation of the system output from which it was derived, but the other four additional translations can also be used to evaluate each MT system. We will see in the next sections how we exploited the available post-edits in the more suitable way depending on the kind of analysis carried out. 3.3 MT Systems Our analysis focuses on the first four top-ranking systems, which include NMT (Luong and Manning, 2015) and three different phrase-based approaches: standard phrase-based (Ha et al., 2015), hierarchical (Jehl et al., 2015) and a combination of phrasebased and syntax-based (Huck and Birch, 2015). Table 1 presents an overview of each system, as well as figures about the training data used.5 The phrase+syntax-based (PBSY) system combines the outputs of a string-to-tree decoder, trained with the GHKM algorithm, with those of two stan5 wit3.fbk.eu http://www.ted.com/ Approach Combination: Phrase+Syntax-based GHKM string-to-tree; hierarchical + sparse lexicalized reordering models Hierarchical Phrase"
D16-1025,D15-1166,0,0.088644,"nvestigate how MT systems’ quality varies with specific characteristics of the input, i.e. sentence length and type of content of each talk (Section 4). Then, we focus on differences among MT systems with respect to morphology, lexical, and word order errors (Section 5). Finally, based on the finding that word reordering is the strongest aspect of NMT compared to the other systems, we carry out a finegrained analysis of word order errors (Section 6). 2 Previous Work To date, NMT systems have only been evaluated by BLEU in single-reference setups (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015a; G¨ulc¸ehre et al., 2015). Ad258 ditionally, the Montreal NMT system submitted to WMT 2015 (Jean et al., 2015b) was part of a manual evaluation experiment where a large number of non-professional annotators were asked to rank the outputs of multiple MT systems (Bojar et al., 2015). Results for the Montreal system were very positive – ranked first in English-German, third in GermanEnglish, English-Czech and Czech-English – which confirmed and strengthened the BLEU results published so far. Unfortunately neither BLEU nor manual ranking judgements tell us which translation as"
D16-1025,W15-5003,0,0.016732,"frameworks. To this end, a detailed and systematic error analysis of NMT vs. PBMT output is required. Translation error analysis, as a way to identify systems’ weaknesses and define priorities for their improvement, has received a fair amount of attention in the MT community. In this work we opt for the automatic detection and classification of translation errors based on manual post-edits of the MT output. We believe this choice provides an optimal trade-off between fully manual error analysis (Farr´us Cabeceran et al., 2010; Popovi´c et al., 2013; Daems et al., 2014; Federico et al., 2014; Neubig et al., 2015), which is very costly and complex, and fully automatic error analysis (Popovi´c and Ney, 2011; Irvine et al., 2013), which is noisy and biased towards one or few arbitrary reference translations. Existing tools for translation error detection are either based on Word Error Rate (WER) and Position-independent word Error Rate (PER) (Popovi´c, 2011) or on output-reference alignment (Zeman et al., 2011). Regarding error classification, Hjerson (Popovi´c, 2011) detects five main types of word-level errors as defined in (Vilar et al., 2006): morphological, reordering, missing words, extra words, an"
D16-1025,J11-4002,0,0.200619,"Missing"
D16-1025,2013.mtsummit-posters.5,0,0.0498368,"Missing"
D16-1025,W14-4009,0,0.0473704,"Missing"
D16-1025,2014.eamt-1.39,1,0.718619,"publicly available through the WIT3 repository (Cettolo et al., 2012).3 3.1 Task Data TED Talks4 are a collection of rather short speeches (max 18 minutes each, roughly equivalent to 2,500 words) covering a wide variety of topics. All talks have captions, which are translated into many languages by volunteers worldwide. Besides representing a popular benchmark for spoken language technology, TED Talks embed interesting research challenges. Translating TED Talks implies dealing with spoken rather than written language, which is hence expected to be structurally less complex, formal and fluent (Ruiz and Federico, 2014). Moreover, as human translations of the talks are required to follow the structure and rhythm of the English captions, a lower amount of rephrasing and reordering is expected than in the translation of written documents. As regards the English-German language pair, the two languages are interesting since, while belonging to the same language family, they have marked differences in levels of inflection, morphological variation, and word order, especially long-range reordering of verbs. 3.2 Evaluation Data Five systems participated in the MT En-De task and were manually evaluated on a represent"
D16-1025,stymne-ahrenberg-2012-practice,0,0.0488173,"rst two features we did not find any correlation; on the contrary, we found a moderate Pearson correlation (R=0.7332) between TTR and the mTER gains of NMT over its closest competitor in each talk. This result suggests that NMT is able to cope with lexical diversity better than any other considered approach. 5 Analysis of Translation Errors We now turn to analyze which types of linguistic errors characterize NMT vs. PBMT. In the literature, various error taxonomies covering different levels of granularity have been developed (Flanagan, 1994; Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014). We focus on three error categories, namely (i) morphology errors, (ii) lexical errors, and (iii) word order errors. As for lexical errors, a number of existing taxonomies further distinguish among translation errors due to missing words, extra words, or incorrect lexical choice. However, given the proven difficulty of disambiguating between these three subclasses (Popovi´c and Ney, 2011; Fishel et al., 2012), we prefer to rely on a more coarse-grained linguistic error classification where lexical errors include all of them (Farr´us Cabeceran et al., 2010). 6 The type-to"
D16-1025,vilar-etal-2006-error,0,0.431417,"Missing"
D16-1025,2010.iwslt-evaluation.11,0,\N,Missing
D16-1025,W15-3001,0,\N,Missing
D16-1025,R13-1079,0,\N,Missing
D16-1025,2015.iwslt-evaluation.1,1,\N,Missing
E06-2002,2005.mtsummit-posters.19,0,0.0175623,"del-4. Starting from the parallel training corpus, provided with direct and inverted alignments, the socalled union alignment (Och and Ney, 2003) is computed. Phrase-pairs are extracted from each sentence pair which correspond to sub-intervals of the source and target positions, J and I, such that the union alignment links all positions of J into I and all positions of I into J. In general, phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax . All such phrase-pairs are efficiently computed by an 2 algorithm with complexity O(lImax Jmax ) (Cettolo et al., 2005). Given all phrase-pairs extracted from the training corpus, lexicon probabilities and fertility probabilities are estimated. Target language models (LMs) used by the decoder and rescoring modules are, respectively, estimated from 3-gram and 4-gram statistics by applying the modified Kneser-Ney smoothing method (Goodman and Chen, 1998). LMs are estimated with an in-house software toolkit which also provides a compact binary representation of the LM which is used by the decoder. 3 Demo Architecture Figure 1 shows the two-layer architecture of the demo. At the bottom lie the programs that provid"
E06-2002,W05-0835,0,0.0137666,"interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target word (null) according to specific fertility and permutatio"
E06-2002,N03-1017,0,0.00726484,"ion 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special target word (null) according to spe"
E06-2002,J03-1002,0,0.00280669,"of generated theories some approximations are introduced during the search: less promising theories are pruned off (beam search) and a new source position is selected by limiting the number of vacant positions on the left-hand and the distance from the left most vacant position (re-ordering constraints). 2.3 Phrase extraction and model training Training of the phrase-based translation model requires a parallel corpus provided with wordalignments in both directions, i.e. from source to target positions, and viceversa. This preprocessing step can be accomplished by applying the GIZA++ toolkit (Och and Ney, 2003) that provides Viterbi alignments based on IBM Model-4. Starting from the parallel training corpus, provided with direct and inverted alignments, the socalled union alignment (Och and Ney, 2003) is computed. Phrase-pairs are extracted from each sentence pair which correspond to sub-intervals of the source and target positions, J and I, such that the union alignment links all positions of J into I and all positions of I into J. In general, phrases are extracted with maximum length in the source and target defined by the parameters Jmax and Imax . All such phrase-pairs are efficiently computed b"
E06-2002,takezawa-etal-2002-toward,0,0.0141419,"concerned, all the languages are encoded in UTF8: this allows to manage the processing phase in an uniform way and to render graphically different character sets. 4 The supported language-pairs Although there is no theoretical limit to the number of supported language-pairs, the current version of the demo provides translations to English from three source languages: Arabic, Chinese and The Arabic-to-English system has been trained with the data provided by the International Workshop on Spoken Language Translation 2005 The context is that of the Basic Traveling Expression Corpus (BTEC) task (Takezawa et al., 2002). BTEC is a multilingual speech corpus which contains sentences coming from phrase books for tourists. Training set includes 20k sentences containing 159K Arabic and 182K English running words; vocabulary size is 18K for Arabic, 7K for English. Chinese-to-English (Newswire) The Chinese-to-English system has been trained with the data provided by the NIST MT Evaluation Campaign 2005 , large-data condition. In this case parallel data are mainly news-wires provided by news agencies. Training set includes 71M Chinese and 77M English running words; vocabulary size is 157K for Chinese, 214K for Engl"
E06-2002,J96-1002,0,0.0194882,"se-based Statistical Machine Translation system which can be accessed by means of a Web page. Section 2 presents the general log-linear framework to SMT and gives an overview of our phrase-based SMT system. In section 3 the software architecture of the demo is outlined. Section 4 focuses on the currently supported language-pairs: Arabic-to-English, Chinese-toEnglish and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is"
E06-2002,J93-2003,0,0.00522389,"and Spanish-to-English. In section 5 the Web-based interface of the demo is described. 1 http://www.nist.gov/speech/tests/mt/ http://www.slt.atr.jp/IWSLT2004/ 3 http://www.is.cs.cmu.edu/iwslt2005/ 2 Exploiting the maximum entropy (Berger et al., 1996) framework, the conditional distribution Pr(e, a |f ) can be determined through suitable real valued functions (called features) hr (e, f , a), r = 1 . . . R, and takes the parametric form: pλ (e, a |f ) ∝ exp{ R X λr hr (e, f , a)} r=1 The ITC-irst system (Chen et al., 2005) is based on a log-linear model which extends the original IBM Model 4 (Brown et al., 1993) to phrases (Koehn et al., 2003; Federico and Bertoldi, 2005). In particular, target strings e are built from sequences of phrases e˜1 . . . e˜l . For each target phrase e˜ the corresponding source phrase within the source string is identified through three random quantities: the fertility φ, which establishes its length; the permutation πi , which sets its first position; the tablet f˜, which tells its word string. Notice that target phrases might have fertility equal to zero, hence they do not translate any 91 source word. Moreover, uncovered source positions are associated to a special targ"
E06-2002,2005.iwslt-1.11,1,\N,Missing
E06-2002,P03-1021,0,\N,Missing
federico-etal-2012-iwslt,niessen-etal-2000-evaluation,0,\N,Missing
federico-etal-2012-iwslt,N04-4038,0,\N,Missing
federico-etal-2012-iwslt,P02-1040,0,\N,Missing
federico-etal-2012-iwslt,W07-0734,0,\N,Missing
federico-etal-2012-iwslt,2005.mtsummit-papers.11,0,\N,Missing
federico-etal-2012-iwslt,O07-5005,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.10,0,\N,Missing
federico-etal-2012-iwslt,I05-3027,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.1,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.5,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.1,1,\N,Missing
L16-1562,J90-2002,0,0.264106,"Missing"
L16-1562,P02-1033,0,0.10194,"Missing"
L16-1562,N13-1073,0,0.251905,"rucial aspect for WA and – more generally – machine translation. WAGS is publicly released under a Creative Commons Attribution license (CC BY 4.0) and is available at: http://hlt-mt.fbk.eu/technologies/wags In addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized a"
L16-1562,W14-0313,1,0.899664,"ligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned, and sample word alignment, where a set of test words are selected and only those words are manually aligned (V´eronis and Langlais, 2000; Merkel, 1999; Ahrenberg et al., 2002). One of the most challenging issues for current state-ofthe-art word aligners is that they show poor generalization capability and are prone to errors when infrequent or unknown words (with respect to the training data) occur in new sentence pairs to be aligned (Farajian et al., 2014). Thus, WA research would highly benefit from gold standard data specifically tailored to assess WA systems on this issue. However, to our knowledge, none of the available WA benchmarks specifically focuses on the problem of out-ofvocabulary (OOV) and rare words. The main contribution of our work is to provide the research community with WAGS (Word Alignment Gold Standard), a novel benchmark which allows extensive evaluation of WA tools on OOV and rare words. WAGS is a subset of the Common Test section of the Europarl English-Italian parallel corpus (Koehn et al., 2003; Koehn, 2005), and is sp"
L16-1562,J07-3002,0,0.0845348,"Missing"
L16-1562,W08-0509,0,0.188606,"under a Creative Commons Attribution license (CC BY 4.0) and is available at: http://hlt-mt.fbk.eu/technologies/wags In addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,17"
L16-1562,C14-2026,1,0.822664,"adopted in other available word alignment benchmarks to cope with alignment ambiguity, namely all links in disagreement were included in the final reference alignment as P-links. In Example 4, both annotators linked “orfana” (“orphan” in English) and “deprived”, but one labeled it as S-link while the other as P-link. Since the adjudicator did not prefer one solution to the other, that link was labeled as Possible in the gold standard. Example 4: • Ita: ...questa Unione e` stata un po’ orfana. • Eng: ...the Union was somewhat deprived. Annotations were accomplished using the MT-EQuAl toolkit5 (Girardi et al., 2014). In addition to the traditional matrix-based alignment, MT-EQuAl allows a more user friendly text-based alignment procedure, where mouse clicks on words are used directly to establish alignment links. This alignment method was particularly useful in our task, since annotators were presented with long sentences and only few words were to be annotated. A screenshot of the alignment interface is presented in Figure 3, which shows the alignment of Example 1 above. In the figure, the P-link between “you” and “domanderete” has already been created: the words are underlined in the text (light blue)"
L16-1562,graca-etal-2008-building,0,0.0494955,"Missing"
L16-1562,W11-4615,0,0.0489247,"Missing"
L16-1562,N03-1017,0,0.0140844,"ds. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,178 #Eng tokens 55,141,541 1,266, 968 Table 1: Europarl v7 statistics: number of segments and Italian/English tokens in Training and Common Test sets. 2.1. Data selection The length of segments in the Europarl Common Test set ranges from one (single word segments) to more than two hundreds. It is a matter of fact that the automatic WA of either too short o"
L16-1562,2005.mtsummit-papers.11,0,0.0472688,"addition to the gold standard data, the release includes the annotation guidelines and an evaluation package that allows to compute Alignment Error Rate (AER) on customizable subsets of WAGS links, for example those aligning only OOV words. In the following, we describe the characteristics of WAGS 3535 and provide results from relevant WA state-of-the-art technology, namely fast align (Dyer et al., 2013) – a variant of IBM model 2 – and IBM model 4 as implemented in mgiza++ (Gao and Vogel, 2008). 2. Dataset Description To create WAGS, we used the publicly available Europarl parallel corpus1 (Koehn, 2005), which contains the proceedings of the European Parliament in the various official languages. A Common Test set, made of the texts from the 4th quarter of year 2000, was defined to be used for machine translation evaluation (Koehn et al., 2003). Table 1 shows some statistics about the Italian and English portions of Europarl v7 release. WAGS is a selection from the Common Test set, realized as described below. Training Common Test #seg 1,908,966 42,753 #Ita tokens 54,848,640 1,224,178 #Eng tokens 55,141,541 1,266, 968 Table 1: Europarl v7 statistics: number of segments and Italian/English tok"
L16-1562,kruijff-korbayova-etal-2006-annotation,0,0.0671595,"Missing"
L16-1562,P04-1060,0,0.0831318,"Missing"
L16-1562,macken-2010-annotation,0,0.0586513,"Missing"
L16-1562,W05-0809,0,0.0461213,"l Machine Translation (Och and Ney, 2004; Fraser and Marcu, 2007), but also other applications rely on WA, such as extraction of bilingual lexica (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), projection of linguistic information between languages (Yarowsky and Ngai, 2001; Kuhn, 2004; Bentivogli and Pianta, 2005). WA gold standards represent a crucial resource to evaluate and analyse WA systems’ performance, and nowadays various benchmarks for different language pairs are available (Melamed, 1998; Och and Ney, 2000; Mihalcea and Pedersen, 2003; Lambert et al., 2005; Martin et al., 2005; Kruijff-Korbayov´a et al., 2006; Grac¸a et al., 2008; Macken, 2010; Holmqvist and Ahrenberg, 2011). Besides the languages addressed, existing benchmarks differ in various respects – also depending on the final application to be evaluated – such as the parallel data used, the annotation scheme adopted (and related guidelines), the selection of words to be manually aligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned, and sample word alignment, where a set of test words are selected and"
L16-1562,W03-0301,0,0.129144,"Missing"
L16-1562,P00-1056,0,0.752067,"tence pair (Brown et al., 1990). WA is a basic component of Statistical Machine Translation (Och and Ney, 2004; Fraser and Marcu, 2007), but also other applications rely on WA, such as extraction of bilingual lexica (Smadja et al., 1996), word sense disambiguation (Diab and Resnik, 2002), projection of linguistic information between languages (Yarowsky and Ngai, 2001; Kuhn, 2004; Bentivogli and Pianta, 2005). WA gold standards represent a crucial resource to evaluate and analyse WA systems’ performance, and nowadays various benchmarks for different language pairs are available (Melamed, 1998; Och and Ney, 2000; Mihalcea and Pedersen, 2003; Lambert et al., 2005; Martin et al., 2005; Kruijff-Korbayov´a et al., 2006; Grac¸a et al., 2008; Macken, 2010; Holmqvist and Ahrenberg, 2011). Besides the languages addressed, existing benchmarks differ in various respects – also depending on the final application to be evaluated – such as the parallel data used, the annotation scheme adopted (and related guidelines), the selection of words to be manually aligned. Regarding this latter issue, two main approaches were followed in previous works: full text alignment, where all words in the text are manually aligned"
L16-1562,J04-4002,0,0.0713423,"Missing"
L16-1562,J96-1001,0,0.58811,"Missing"
L16-1562,steinberger-etal-2006-jrc,0,0.0981094,"Missing"
L16-1562,N01-1026,0,0.22907,"Missing"
L16-1562,ahrenberg-etal-2002-system,0,\N,Missing
N10-1064,P00-1037,0,0.264804,"ht affect text mining is in (Subramaniam et al., 2009), while an analysis of how noise phenomena, commonly occurring in blogs, affect an opinion mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based"
N10-1064,E95-1010,0,0.187162,"Missing"
N10-1064,fossati-di-eugenio-2008-saw,0,0.0471505,"Missing"
N10-1064,P07-2045,1,0.011029,"Missing"
N10-1064,reynaert-2006-corpus,0,0.0152725,"rch and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsky, 2005). More sophisticated approaches have been proposed by (Fossati and Di Eugenio, 2008), that mixes surface and Part-Of-Speech Information, and (Schaback and Li, 2007), which combines similarity measures at the character, phonetic, word, syntax, and semantic levels into one global feature-based framework. 413 a) *W *w had just come in from Australia [Australia] b) good service we *staid one week. [Tahiti] c) The room was *exellent but the hallway was *filty . [NJ] d) is a good place to stay, if you are looking for a hotel *arrou"
N10-1064,P02-1019,0,0.149611,"(Subramaniam et al., 2009), while an analysis of how noise phenomena, commonly occurring in blogs, affect an opinion mining application is in (Dey and Haque, 2009). Concerning spelling correction literature, many works apply the noisy channel model which consists of two components: a source model (prior of word probabilities) and a channel (error) model, that accounts for spelling transformations on letter sequences. Several approaches have been proposed under this framework, that mainly differ in the employed error model; see for example: (Church and Gale, 1991), (Brill and Moore, 2000) and (Toutanova and Moore, 2002). Comprehensive surveys on methods to model and recover spelling errors can be found in (Kukich, 1992) and (Pedler, 2007); in particular, the latter work is specifically centered on methods for correcting so-called real-word errors (cf. Section 3). The detection of errors and the suggestion of corrections typically rely on the availability of text corpora or human-made lexical resources. Search for correct alternatives can be based on word similarity measures, such as the edit distance (Mitton, 1995), anagram hashing (Reynaert, 2006), and semantic distance based on WordNet (Hirst and Budanitsk"
N10-1064,E03-1050,0,\N,Missing
W06-2601,W00-0729,0,0.0196059,"theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functions 1 , popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each model are reported. ME estimation with real-valued features is accomplished by combining GIS with the leave-one-out method (Manning and Schutze, 1999). 1 Introduction The Maximum Entropy (ME) statistical framework (D"
W06-2601,P98-2140,0,0.024674,"he current class, i.e. ct = c, the identity and offset of the word in the context, i.e. wt+d = w. Formally, the feature is computed by: Lex c,w,d (x, y) = ˆ δ(ct = c) · δ(wt+d = w). For example, the lexical feature for word Verona, at position t with tag loc (location) is: Lexloc,Verona,0 (x, y) = δ(ct = loc) · ·δ(wt = Verona). The n feature functions fi (x, y) represent any kind of information about the event (x, y) which can be useful for the classification task. Typically, binary features are employed which model the verification of simple events within the target class and the context. In Mikheev (1998), binary features for text tagging are classified into two broad classes: atomic and complex. Atomic features tell information about the current tag and one single item (word or tag) of the context. Complex features result as a combination of two or more atomic features. In this way, if the grouped events are not independent, complex features should capture higher correlations or dependencies, possibly useful to discriminate. Lexical features might introduce data sparseness in the model, given that in real texts an important fraction of words occur only once. In other words, many words in the"
W06-2601,P02-1038,0,0.0600425,"into the standard ME training algorithm. Experimental results on two tagging tasks show statistically significant performance gains after augmenting standard binaryfeature models with real-valued features. Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functions 1 , popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each mode"
W06-2601,W03-0420,0,0.0949571,"with real-valued features. Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functions 1 , popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A direct comparison of ME models based on binary, realvalued, and mixed features is presented. Besides, performance on the tagging tasks, complexity and training time by each model are reported. ME estimation with real-valued features is accomplished by combining GIS with the leave-one-out method (Manning and Schutze, 1999). 1 Introduction The Maximum Entr"
W06-2601,J96-1002,0,0.0519929,"lting ME models have orders of magnitude fewer parameters. Effective use of training data to estimate features and parameters is achieved by integrating a leaving-one-out method into the standard ME training algorithm. Experimental results on two tagging tasks show statistically significant performance gains after augmenting standard binaryfeature models with real-valued features. Despite ME theory and its related training algorithm (Darroch and Ratcliff, 1972) do not set restrictions on the range of feature functions 1 , popular NLP text books (Manning and Schutze, 1999) and research papers (Berger et al., 1996) seem to limit them to binary features. In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). This paper focuses on ME models for two texttagging tasks: Named Entity Recognition (NER) and Text Chuncking (TC). By taking inspiration from the literature (Bender et al., 2003; Borthwick, 1999; Koeling, 2000), a set of standard binary features is introduced. Hence, for each feature type, a corresponding real-valued feature is developed in terms of smoothed probability distributions estimated on the training data. A d"
W06-2601,C98-2135,0,\N,Missing
W07-0712,W06-3113,1,0.766207,"opted by the CMU-Cambridge LM Toolkit (Clarkson and Rosenfeld, 1997) and well analyzed in (Whittaker and Raj, 2001). Briefly, n-grams are stored in a data structure which privileges memory saving rather than access time. In particular, single components of each n-gram are searched, via binary search, into blocks of successors stored contiguously (Figure 2). Further improvements in memory savings are obtained by quantizing both back-off weights and probabilities. 2.4 LM Quantization Quantization provides an effective way of reducing the number of bits needed to store floating point variables. (Federico and Bertoldi, 2006) showed that best results were achieved with the so-called binning method. This method partitions data points into uniformly populated intervals or bins. Bins are filled in in a greedy manner, starting from the lowest value. The center of each bin corresponds to the mean value 90 1-gr 2-gr 3-gr 3 1 w |pr 3 4 1 1 w |bo |pr |idx Figure 2: Static data structure for LMs. Number of bytes are shown used to encode single words (w), quantized back-off weights (bo) and probabilities (pr), and start index of successors (idx). of all its points. Quantization is applied separately at each n-gram level and"
W12-3155,P02-1040,0,\N,Missing
W12-3155,P07-2045,1,\N,Missing
W12-3155,W07-0717,0,\N,Missing
W12-3155,macklovitch-2006-transtype2,0,\N,Missing
W13-2237,2013.mtsummit-papers.5,1,0.522852,"l., 2006) computed using the MultEval script (Clark et al., 2011). Since the implementations of standard Giza and of incremental Giza combined with dynamic suffix arrays are not comparable, we constructed two baselines, a standard phrase based SMT system and an incremental Giza baseline (§5). Details on experimental SMT systems we built follow. actually generated from scratch. In our experiments, the extent of usefulness of online learning highly depends on the amount of repetition of text. A reasonable way to measure the quantity of repetition in each document is through the repetition rate (Bertoldi et al., 2013). It computes the rate of non-singleton n-grams, n=1...4, averaging the values over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean. Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times: RR = !1/4 4 P Y dict(n) − n 1 S P S dict(n) Baseline This system was built on the parallel training data for each domain. We run 5 iterations of model 1, 5 of HMM (Vog"
W13-2237,J93-2003,0,0.0390959,"s, n=1...4, averaging the values over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean. Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times: RR = !1/4 4 P Y dict(n) − n 1 S P S dict(n) Baseline This system was built on the parallel training data for each domain. We run 5 iterations of model 1, 5 of HMM (Vogel et al., 1996), 3 of model 3, 3 of model 4 (Brown et al., 1993) using MGiza (Gao and Vogel, 2008) toolkit to align the parallel corpus at word level. Translation and reordering models were built using Moses, while log-linear weights were optimized with MERT on the corresponding development sets. The same IT baseline system was used in the field test of MateCat and the references in the IT data are actual postedits of its translation. (7) n=1 Statistics of the parallel sets and their repetition rate on both sides are reported in Table 1. Domain ITen→it TEDen→f r Set #srcTok srcRR #tgtTok Train 57M na 60M Dev 3.3k 12.03 3.5k Test 3.3k 15.00 3.3k Train 2.6M"
W13-2237,P05-1032,0,0.011504,"e aggressive j subject to lj ≤ ∆hj · w + ξj 303 ∀j ∈ J ⊆ {1 . . . N } (5) word alignments and language models (Levenberg and Osborne, 2009). To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts. Since we work at the sentence level, on-thefly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements. To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008). They are used to efficiently store the source and the target corpus and alignments in efficient data structure, namely the suffix array. When a phrase translation is asked by the decoder, the corpus is searched, the counts are collected and its probabilities are computed on the fly. However, the current implementation in Moses of the stream based MT relying on the suffix arrays is severely limited as it allows the computation of only three translation features, namely the two direct translation probabilities and the phrase penalty. This results in a significant degradation of p"
W13-2237,2012.eamt-1.60,1,0.460333,"j = Optimize(lP j , hj , w, C); wi = wi−1 + η · j αj ∆hj ; end end end end 6 Experiments 6.1 Datasets We compared our online learning approaches (Sections 3 and 4) and the stream based adaptation method (Section 5) on two datasets from different domains, namely Information Technology (IT) and TED talks, and two different language pairs. The IT domain dataset is proprietary, it involves the translation of technical documents from English to Italian and has been used in the field test carried out under the MateCat project2 . Experiments are also conducted on English to French TED talks dataset (Cettolo et al., 2012) to assess the robustness of the proposed approaches in a different scenario and to provide results on a publicly available dataset for the sake of reproducibility. The training, development (dev2010) and evaluation (tst20103 ) sets are the same as used in the last IWSLT last evaluation campaigns. In experiments on TED data, we considered the human reference translations as post edits, even if they were In the following section we overview a stream based adaptation method with which we experimentally compared our two online learning approaches as it well fits the framework we are working in. 5"
W13-2237,C08-1064,0,0.0103257,"≤ ∆hj · w + ξj 303 ∀j ∈ J ⊆ {1 . . . N } (5) word alignments and language models (Levenberg and Osborne, 2009). To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts. Since we work at the sentence level, on-thefly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements. To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008). They are used to efficiently store the source and the target corpus and alignments in efficient data structure, namely the suffix array. When a phrase translation is asked by the decoder, the corpus is searched, the counts are collected and its probabilities are computed on the fly. However, the current implementation in Moses of the stream based MT relying on the suffix arrays is severely limited as it allows the computation of only three translation features, namely the two direct translation probabilities and the phrase penalty. This results in a significant degradation of performance. wh"
W13-2237,P11-2031,0,0.0118167,"as it well fits the framework we are working in. 5 Stream based adaptation Continuously updating an SMT system to an incoming stream of parallel data comes under stream based adaptation. Levenberg et. al. (2010) proposed an incremental adaptation technique for the core generative component of the SMT system, 2 www.matecat.com As the size of evaluation set in TED data is too large with respect to the current implementation of our algorithms, we performed evaluation on the first 200 sentences only. 3 304 measured in terms of BLEU and TER (Snover et al., 2006) computed using the MultEval script (Clark et al., 2011). Since the implementations of standard Giza and of incremental Giza combined with dynamic suffix arrays are not comparable, we constructed two baselines, a standard phrase based SMT system and an incremental Giza baseline (§5). Details on experimental SMT systems we built follow. actually generated from scratch. In our experiments, the extent of usefulness of online learning highly depends on the amount of repetition of text. A reasonable way to measure the quantity of repetition in each document is through the repetition rate (Bertoldi et al., 2013). It computes the rate of non-singleton n-g"
W13-2237,W02-1001,0,0.0910054,"following idea. For each N-best translation (candidate) in the search space, we compute a similarity score against the postedit using the sentence-level BLEU metric (Lin and Och, 2004), a smoothed variant of the popular BLEU metric (Papineni et al., 2001). We hence compare the similarity score of each candidate against the similarity score achieved by the bestHyp, that was also computed against the postedit. If the candidate 2.1 Online Adaptation during Tuning Liang et. al. (2006) improved SMT performance by online adaptation of scaling factors (λ in (1)) using averaged perceptron algorithm (Collins, 2002). They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) update towards the oracle translation in N-Best list, local update; (3) a hybrid approach in which a bold update is performed when the reference is reachable, otherwise a local update is performed. Liang and Klein (2009) compared two online EM algorithms, stepwise online EM (Sato and Ishii, 2000; Capp´e and Moulines, 2007) and incremental EM (Neal and Hinton, 1998) which they use to update the alignment models (the generative comp"
W13-2237,2012.amta-papers.22,1,0.705074,"uctivity of a human translator. A CAT tool comes as a package of a Translation Memory (TM), builtin spell checkers, a dictionary, a terminology list etc. which help the translator while translating a sentence. Recent research has led to the integration of CAT tools with statistical machine translation (SMT) engines. SMT makes use of a large available parallel corpus to generate statistical models for translation. Due to their generalization capability, SMT systems are a good fit in this scenario and a seamless integration of SMT engines in CAT have shown to increase translator’s productivity (Federico et al., 2012). Although automatic systems generate reliable translations they are not accurate enough to be used directly and need postedition by human translators. In state-of-the-art CAT tools, the SMT systems are static in nature and so they cannot adapt In this paper, we implemented two online learning methods through which a phrase-based SMT system evolves over time, sentence after sentence, by taking advantage of the post-edition or translation of the previous sentence by the user.1 In the first approach, we focus on the translation model aspect of SMT which is represented by five conventional featur"
W13-2237,J03-1002,0,0.0193141,"for 1 Moses code is available in the github repository. https://github.com/mtresearcher/ mosesdecoder/tree/moses_onlinelearning 301 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 301–308, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics on the fly. However, stepwise EM is prone to failure if mini-batch size and stepsize parameters are not chosen correctly, while incremental EM requires substantial storage costs because it has to store sufficient statistics for each sample. Other works on online minimum error rate training in SMT (Och and Ney, 2003) that deserve mentioning are (Hopkins and May, 2011; Hasler et al., 2011). the translation hypothesis as shown in Equation 1. score(e∗ , f ) = Σi λi hi (e∗ , f ) (1) where hi (·) are the feature functions representing the models and λi are the linear weights. The highest scored translation is the best hypothesis e∗ output by the system. We extend the translation model with a new feature which provides extra phrase-pair scores changing according to the user feedback. The scores of the new feature are adapted in a discriminative fashion, by rewarding phrase-pairs observed in the search space and"
W13-2237,W08-0509,0,0.0318654,"over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean. Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times: RR = !1/4 4 P Y dict(n) − n 1 S P S dict(n) Baseline This system was built on the parallel training data for each domain. We run 5 iterations of model 1, 5 of HMM (Vogel et al., 1996), 3 of model 3, 3 of model 4 (Brown et al., 1993) using MGiza (Gao and Vogel, 2008) toolkit to align the parallel corpus at word level. Translation and reordering models were built using Moses, while log-linear weights were optimized with MERT on the corresponding development sets. The same IT baseline system was used in the field test of MateCat and the references in the IT data are actual postedits of its translation. (7) n=1 Statistics of the parallel sets and their repetition rate on both sides are reported in Table 1. Domain ITen→it TEDen→f r Set #srcTok srcRR #tgtTok Train 57M na 60M Dev 3.3k 12.03 3.5k Test 3.3k 15.00 3.3k Train 2.6M na 2.8M Dev 20k 3.43 20k Test 32k"
W13-2237,P03-1021,0,0.026349,"update), and a learning rate for feature weights used by MIRA. These additional parameters were optimized by maximizing the BLEU 6.2 Systems The SMT systems were built using the Moses toolkit (Koehn et al., 2007). Training data in each domain was used to create translation and lexical reordering models. We created a 5-gram LM for TED talks and a 6-gram LM for the IT domain using IRSTLM (Federico et al., 2008) with improved Kneser-Ney smoothing (Chen and Goodman, 1996) on the target side of the training parallel corpora. The log linear weights for the baseline systems are optimized using MERT (Och, 2003) provided in the Moses toolkit. To counter the instability of MERT, we averaged the weights of three MERT runs in each case. Performance is 4 305 http://code.google.com/p/inc-giza-pp/ probability distribution. We empirically proved that this method is robust and works for different domain datasets be it Information Technology or TED talks. In addition, if the repetition rate is high in the text, online learning works much better than if the rate is low. We tested both with an unbounded and a bounded range on the online feature and found out that bounded values produce more stable and consisten"
W13-2237,2001.mtsummit-papers.68,0,0.0121164,"ce segment, post-edits it and finally approves it. From the SMT point of view, for each source segment the decoder explores a search space of possible translations and finally returns the best scoring one (bestHyp) to the user. The user possibly corrects this suggestion thus generating the final translation (postedit). Our online learning procedure is based on the following idea. For each N-best translation (candidate) in the search space, we compute a similarity score against the postedit using the sentence-level BLEU metric (Lin and Och, 2004), a smoothed variant of the popular BLEU metric (Papineni et al., 2001). We hence compare the similarity score of each candidate against the similarity score achieved by the bestHyp, that was also computed against the postedit. If the candidate 2.1 Online Adaptation during Tuning Liang et. al. (2006) improved SMT performance by online adaptation of scaling factors (λ in (1)) using averaged perceptron algorithm (Collins, 2002). They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) update towards the oracle translation in N-Best list, local update; (3) a hy"
W13-2237,D11-1125,0,0.0271454,"sitory. https://github.com/mtresearcher/ mosesdecoder/tree/moses_onlinelearning 301 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 301–308, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics on the fly. However, stepwise EM is prone to failure if mini-batch size and stepsize parameters are not chosen correctly, while incremental EM requires substantial storage costs because it has to store sufficient statistics for each sample. Other works on online minimum error rate training in SMT (Och and Ney, 2003) that deserve mentioning are (Hopkins and May, 2011; Hasler et al., 2011). the translation hypothesis as shown in Equation 1. score(e∗ , f ) = Σi λi hi (e∗ , f ) (1) where hi (·) are the feature functions representing the models and λi are the linear weights. The highest scored translation is the best hypothesis e∗ output by the system. We extend the translation model with a new feature which provides extra phrase-pair scores changing according to the user feedback. The scores of the new feature are adapted in a discriminative fashion, by rewarding phrase-pairs observed in the search space and in the reference, and penalizing phrase-pairs obse"
W13-2237,P07-2045,1,0.0128033,"+O+NS as (1) but with the online feature normalized with the sigmoid function; (3) +W weights updated (Section 4) with Algorithm 2; (4) +O+W combination of online feature and weight update; (5) +O+NS+W as system (4) with normalized online feature score. In the online learning system we have three additional parameters: a weight for the online feature, a learning rate for features (used in the perceptron update), and a learning rate for feature weights used by MIRA. These additional parameters were optimized by maximizing the BLEU 6.2 Systems The SMT systems were built using the Moses toolkit (Koehn et al., 2007). Training data in each domain was used to create translation and lexical reordering models. We created a 5-gram LM for TED talks and a 6-gram LM for the IT domain using IRSTLM (Federico et al., 2008) with improved Kneser-Ney smoothing (Chen and Goodman, 1996) on the target side of the training parallel corpora. The log linear weights for the baseline systems are optimized using MERT (Och, 2003) provided in the Moses toolkit. To counter the instability of MERT, we averaged the weights of three MERT runs in each case. Performance is 4 305 http://code.google.com/p/inc-giza-pp/ probability distri"
W13-2237,D09-1079,0,0.0191269,"; end end end end end ∆h · w ˆ = score(y ∗ ) − score(ˆ y) (4) MIRA is an ultraconservative algorithm, meaning that the update of the current weight vector is the smallest possible value satisfying the constraint that the variation incurred by the objective function must not be larger than the variation incurred by the model (plus a non-negative slack variable ξ). Formally, weight update at ith iteration is defined as: X 1 wi = arg min C ||w − wi−1 ||2 + |{z} ξj w 2η | {z } conservative aggressive j subject to lj ≤ ∆hj · w + ξj 303 ∀j ∈ J ⊆ {1 . . . N } (5) word alignments and language models (Levenberg and Osborne, 2009). To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts. Since we work at the sentence level, on-thefly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements. To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008). They are used to efficiently store the source and the target corpus and alignments in efficient"
W13-2237,2006.amta-papers.25,0,0.102455,"perimentally compared our two online learning approaches as it well fits the framework we are working in. 5 Stream based adaptation Continuously updating an SMT system to an incoming stream of parallel data comes under stream based adaptation. Levenberg et. al. (2010) proposed an incremental adaptation technique for the core generative component of the SMT system, 2 www.matecat.com As the size of evaluation set in TED data is too large with respect to the current implementation of our algorithms, we performed evaluation on the first 200 sentences only. 3 304 measured in terms of BLEU and TER (Snover et al., 2006) computed using the MultEval script (Clark et al., 2011). Since the implementations of standard Giza and of incremental Giza combined with dynamic suffix arrays are not comparable, we constructed two baselines, a standard phrase based SMT system and an incremental Giza baseline (§5). Details on experimental SMT systems we built follow. actually generated from scratch. In our experiments, the extent of usefulness of online learning highly depends on the amount of repetition of text. A reasonable way to measure the quantity of repetition in each document is through the repetition rate (Bertoldi"
W13-2237,N10-1062,0,0.368748,"w 2η | {z } conservative aggressive j subject to lj ≤ ∆hj · w + ξj 303 ∀j ∈ J ⊆ {1 . . . N } (5) word alignments and language models (Levenberg and Osborne, 2009). To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts. Since we work at the sentence level, on-thefly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements. To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008). They are used to efficiently store the source and the target corpus and alignments in efficient data structure, namely the suffix array. When a phrase translation is asked by the decoder, the corpus is searched, the counts are collected and its probabilities are computed on the fly. However, the current implementation in Moses of the stream based MT relying on the suffix arrays is severely limited as it allows the computation of only three translation features, namely the two direct translation probabilities and the phrase penalty. This results in a"
W13-2237,C96-2141,0,0.0450371,"13). It computes the rate of non-singleton n-grams, n=1...4, averaging the values over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean. Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times: RR = !1/4 4 P Y dict(n) − n 1 S P S dict(n) Baseline This system was built on the parallel training data for each domain. We run 5 iterations of model 1, 5 of HMM (Vogel et al., 1996), 3 of model 3, 3 of model 4 (Brown et al., 1993) using MGiza (Gao and Vogel, 2008) toolkit to align the parallel corpus at word level. Translation and reordering models were built using Moses, while log-linear weights were optimized with MERT on the corresponding development sets. The same IT baseline system was used in the field test of MateCat and the references in the IT data are actual postedits of its translation. (7) n=1 Statistics of the parallel sets and their repetition rate on both sides are reported in Table 1. Domain ITen→it TEDen→f r Set #srcTok srcRR #tgtTok Train 57M na 60M Dev"
W13-2237,N09-1069,0,0.0336169,"lso computed against the postedit. If the candidate 2.1 Online Adaptation during Tuning Liang et. al. (2006) improved SMT performance by online adaptation of scaling factors (λ in (1)) using averaged perceptron algorithm (Collins, 2002). They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) update towards the oracle translation in N-Best list, local update; (3) a hybrid approach in which a bold update is performed when the reference is reachable, otherwise a local update is performed. Liang and Klein (2009) compared two online EM algorithms, stepwise online EM (Sato and Ishii, 2000; Capp´e and Moulines, 2007) and incremental EM (Neal and Hinton, 1998) which they use to update the alignment models (the generative component of SMT) 302 scores better than the bestHyp, then we promote the building blocks, i.e. phrase-pairs, of candidate that were not used in bestHyp and demote the phrase-pairs used in bestHyp that were not used for candidate. On the contrary, if the candidate scores worse than the bestHyp, we promote the building blocks of bestHyp that are not in candidate and demote those of candid"
W13-2237,P06-1096,0,0.0610588,"Missing"
W13-2237,C04-1072,0,0.0423539,"scenario, the user receives a translation suggestion for each source segment, post-edits it and finally approves it. From the SMT point of view, for each source segment the decoder explores a search space of possible translations and finally returns the best scoring one (bestHyp) to the user. The user possibly corrects this suggestion thus generating the final translation (postedit). Our online learning procedure is based on the following idea. For each N-best translation (candidate) in the search space, we compute a similarity score against the postedit using the sentence-level BLEU metric (Lin and Och, 2004), a smoothed variant of the popular BLEU metric (Papineni et al., 2001). We hence compare the similarity score of each candidate against the similarity score achieved by the bestHyp, that was also computed against the postedit. If the candidate 2.1 Online Adaptation during Tuning Liang et. al. (2006) improved SMT performance by online adaptation of scaling factors (λ in (1)) using averaged perceptron algorithm (Collins, 2002). They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) updat"
W13-2237,P02-1040,0,\N,Missing
W15-2501,W15-2508,1,0.700494,"all submissions, both primary and secondary, in terms of macro-averaged F-score, several systems performed better in terms of accuracy. The other systems did not use explicit anaphora resolution, but attempted to gather relevant information about possible antecedents by considering a certain number of preceding, or preceding and following, noun phrases. They differed in the type of classifier and in the information sources used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED U"
W15-2501,E06-1032,0,0.027114,"itself achieves the best scores, but considering the inadequacy of BLEU for pronoun evaluation, we do not see this as a major concern in itself. The other submissions fall behind in terms of automatic MT metrics. The UU - HARDMEIER system is similar to the other SMT systems, but uses different language and translation models, which evidently do not yield the same level of raw MT performance as the baseline system. ITS 2 is a rule-based system. Since it is well known that n-gram-based evaluation metrics do not always do full justice to rule-based MT approaches not using n-gram language models (Callison-Burch et al., 2006), it is difficult to draw definite conclusions from this system’s lower scores. Finally, the extremely low scores for the A 3-108 system indicate serious problems with translation quality, an impression that we easily confirmed by examining the system output. 8 5 The low scores for the ITS 2 system were partly due to a design decision. The anaphora prediction component of ITS 2 only generated the personal pronouns il, elle, ils and elles; this led to zero recall for ce and ça/cela and, as a consequence, to a large number of misses that would have been comparatively easy to predict with an n-gr"
W15-2501,P14-1065,1,0.749966,"Missing"
W15-2501,2012.eamt-1.60,1,0.298652,"est data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare"
W15-2501,2010.iwslt-papers.10,1,0.800853,"g such a setup makes it possible to explore a variety of approaches for solving the problem at hand since the participating groups independently come up with various ways to address it. All of this is highly beneficial for continued research as it creates a well-defined benchmark with a low entry barrier, a set of results to compare to, and a collection of properly evaluated ideas to start from. We decided to base this shared task on the problem of pronoun translation. Historically, this was one of the first discourse problems to be considered in the context of SMT (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010); yet, it is still far from being solved. For an overview of the existing work on pronoun translation, we refer the reader to Hardmeier (2014, Section 2.3.1). The typical case is an anaphoric pronoun – one that refers to an entity mentioned earlier in the discourse, its antecedent. Many languages have agreement constraints between pronouns and their antecedents. In translation, these constraints must be satisfied in the target language. Note that source language information is not enough for this task. To see why, consider the following example for English– French:1 We describe the design, the"
W15-2501,P13-4033,1,0.934301,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,chrupala-etal-2008-learning,0,0.0467164,"Missing"
W15-2501,W11-2107,0,0.0222412,"Missing"
W15-2501,D13-1037,1,0.793439,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,N13-1073,0,0.0272151,"inting characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoot"
W15-2501,W15-2510,1,0.750725,"hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), they do not specifically focus on pronoun translation. 5 Machine Translation Evaluation http://stp.lingfil.uu.se/~ch/DiscoMT2015.maneval/index.php Machine Translation Eva"
W15-2501,J07-3002,0,0.00746111,"n order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all three models, the results on pronoun links are better than those on all links. More"
W15-2501,P13-2121,0,0.00705082,"Missing"
W15-2501,D07-1103,0,0.0077006,"00 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200"
W15-2501,guillou-etal-2014-parcor,1,0.687529,"g requirements: 1. The talks have been transcribed (in English) and translated into French. 2. They were not included in the training, development, and test datasets of any IWSLT evaluation campaign, so DiscoMT.tst2015 can be used as held-out data with respect to those. 3. They contain a sufficient number of tokens of the English pronouns it and they translated into the French pronouns listed in Table 1. 4. They amount to a total number of words suitable for evaluation purposes (e.g., tens of thousands). 2 http://www.ted.com 3 The following overview of text characteristics is based on work by Guillou et al. (2014). 3 To meet requirement 3, we selected talks for which the combined count of the rarer classes ça, cela, elle, elles and on was high. The resulting distribution of pronoun classes, according to the extraction procedure described in Section 5.1, can be found in Table 8 further below. We aimed to have at least one pair of talks given by the same speaker and at least one pair translated by the same translator. These two features are not required by the DiscoMT shared task, but could be useful for further linguistic analysis, such as the influence of speakers and translators on the use of pronouns"
W15-2501,W15-2509,0,0.0482925,"gy of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T sy"
W15-2501,W14-3352,1,0.899484,"Missing"
W15-2501,D14-1027,1,0.902972,"Missing"
W15-2501,P07-2045,0,0.00942963,"2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million w"
W15-2501,2005.mtsummit-papers.11,0,0.0282203,"e resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For compl"
W15-2501,W15-2514,0,0.067491,"Missing"
W15-2501,W10-1737,0,0.740286,"Missing"
W15-2501,W11-1902,0,0.185739,"sions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with"
W15-2501,W15-2512,0,0.121441,"glish-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For completeness, we also provided a recasing model that was trained on the same dataset to render it straightforward to produce case-sensitive output, which we required as the final submission. 4.2 ITS 2 (Loáiciga and Wehrli, 2015) was a rulebased machine translation system using syntaxbased transfer. For the shared task, it was extended with an anaphora resolution component influenced by Binding Theory (Chomsky, 1981). For the sixth submission, A 3-108, no system description paper was submitted. Its output seemed to have been affected by problems at the basic MT level, yielding very bad translation quality. 4.3 Evaluation Methods Evaluating machine translations for pronoun correctness automatically is difficult because standard assumptions fail. In particular, it is incorrect to assume that a pronoun is translated corr"
W15-2501,2006.amta-papers.25,0,0.144804,"Missing"
W15-2501,W15-2511,0,0.0430542,"s used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED UEDIN MALTA 2 MALTA WHATELLES UEDIN 2 UU - TIED 2 GENEVA GENEVA 2 IDIAP IDIAP 2 A 3-108 ( WITHDRAWN ) Macro-F Accuracy ce cela elle elles F-score il 0.584 0.579 0.571 0.565 0.561 0.553 0.550 0.539 0.437 0.421 0.206 0.164 0.129 0.122 0.663 0.742 0.723 0.740 0.732 0.721 0.714 0.734 0.592 0.579 0.307 0.407 0.240 0.325 0.817 0.862 0.823 0.875 0.853 0.862 0.823 0.849 0.647 0.611 0.282 0.152 0.225 0.220 0.346 0.235 0.213 0.1"
W15-2501,W14-3334,1,0.85254,"ed the same bitext as for the MT baseline in the first task (Section 4.1); we pre-processed it like before, except for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignme"
W15-2501,W15-2513,0,0.169961,"ompared to the perhaps more obvious methodology of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline"
W15-2501,tiedemann-2012-parallel,1,0.765503,"ic or human processing. Table 3 shows some statistics and metadata about the TED talks that are part of the DiscoMT.tst2015 set. talk id segs 205 1756 1819 1825 1894 1935 1938 1950 1953 1979 2043 2053 189 186 147 120 237 139 107 243 246 160 175 144 4,188 4,320 2,976 2,754 5,827 3,135 2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment"
W15-2501,W15-2515,1,0.722711,"and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphor"
W15-2501,C96-2141,0,0.0650665,"for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . Fo"
W15-2501,P00-1056,0,0.0244007,"reated automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all t"
W15-2501,W15-2516,0,0.0553089,"air, and (ii) the sums for each row/column; • accuracy; All six groups with system description papers used some form of machine learning. The main difference was whether or not they explicitly attempted to resolve pronominal coreference. Two systems relied on explicit anaphora resolution: UEDIN and MALTA. They both applied the Stanford coreference resolver (Lee et al., 2011) on the source language text, then projected the antecedents to the target language through the word alignments, and finally obtained morphological tags with the Morfette software (Chrupała et al., 2008). The UEDIN system (Wetzel et al., 2015) was built around a maximum entropy classifier. In addition to local context and antecedent information, it used the NADA tool (Bergsma and Yarowsky, 2011) to identify nonreferring pronouns and included predictions by a standard n-gram language model as a feature. The MALTA system (Pham and van der Plas, 2015) was based on a feed-forward neural network combined with word2vec continuous-space word embeddings (Mikolov et al., 2013). It used local context and antecedent information. • precision (P), recall (R), and F-score for each label; • micro-averaged P, R, F-score (note that in our setup, mi"
W15-2501,J03-1002,0,0.00752171,"ing two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and s"
W15-2501,P03-1021,0,0.00636797,"filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version"
W15-2501,P02-1040,0,\N,Missing
W15-2501,W14-3302,0,\N,Missing
W16-2345,D12-1133,0,0.253709,"set of the provided training data that has well-defined document boundaries in order to allow for meaningful extraction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddi"
W16-2345,W16-2348,0,0.0249438,"urce word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER class. The difference between the primary and contrastive systems is small. In the primary system, the feature val"
W16-2345,P06-1005,0,0.150956,"res based on the target-language model estimates provided by the baseline system, linguistic features concerning the source word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER clas"
W16-2345,2012.eamt-1.60,1,0.85967,"predicting all of the other pronouns, the system relied solely on the scores coming from the proposed PLM model. This target-side PLM model uses a large target-language training dataset to learn a probabilistic relation between each target pronoun and the distribution of the gender-number of its preceding nouns and pronouns. For prediction, given each source pronoun “it” or “they”, the system uses the PLM to score all possible candidates and to select the one with the highest score. In addition to the PoS-tagged lemmatised data that was provided for the shared task, the WIT3 parallel corpus (Cettolo et al., 2012), provided as part of the training data at the DiscoMT 2015 workshop, was used to train the PLM model. Furthermore, a French PoS-tagger, Morfette (Chrupala et al., 2008), was employed for gendernumber extraction. Before extracting the examples as feature vectors, the data is linguistically preprocessed usˇ ing the Treex framework (Popel and Zabokrtsk´ y, 2010). The source-language texts undergo a thorough analysis and are enriched with PoS tags, dependency syntax, as well as semantic roles and coreference for English. On the other hand, only grammatical genders are assigned to nouns in the tar"
W16-2345,chrupala-etal-2008-learning,0,0.0898214,"Missing"
W16-2345,W16-2350,1,0.838182,"on. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trai"
W16-2345,W11-2123,0,0.0192239,"e classifier is trained on a combination of semantic, based on lexical resources such as VerbNet (Schuler, 2005) and WordNet (Miller, 1995), and frequencies computed over the annotated Gigaword corpus (Napoles et al., 2012), syntactic, from the dependency parser in the Mate tools (Bohnet et al., 2013), and contextual features. The event classification results are modest, reaching only 54.2 F-score for the event class. The translation model, into which the classifier is integrated, is a 6-gram language model computed over target lemmata using modified KneserNey smoothing and the KenLM toolkit (Heafield, 2011). In addition to the pure target lemma context, it also has access to the identity of the sourcelanguage pronoun, used as a concatenated label to each REPLACE item. This provides information about the number marking of the pronouns in the source, and also allows for the incorporation of the output of the ‘it’-label classifier. To predict classes for an unseen test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels p"
W16-2345,W16-2349,0,0.0373816,"sing the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun translation decisions. The model performs reasonably well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features"
W16-2345,W10-1737,0,0.434398,"Missing"
W16-2345,guillou-etal-2014-parcor,1,0.910739,"the fact that all talks are originally given in English, which means that French–English translation is in reality a back-translation. • she: feminine singular subject pronoun; 3 1 We explain below in Section 3.3.3 how non-subject pronouns are filtered out from the data. 528 TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. As shown in analysis presented by Guillou et al. (2014), TED talks differ from other text types with respect to pronoun usage. TED speakers frequently use first- and second-person pronouns (singular and plural): first-person to refer to themselves and their colleagues or to themselves and the audience, second-person to refer to the audience, the larger set of viewers, or people in general. TED speakers often use the pronoun “they” without a specific textual antecedent, in sentences such as “This is what they think.” They also use deictic and third-person pronouns to refer to things in the spatio-temporal context shared by the speaker and the audie"
W16-2345,W16-2351,1,0.900928,"Missing"
W16-2345,E12-3001,1,0.880326,"it is required by syntax to fill the subject position. An event reference pronoun may refer to a verb phrase (VP), a clause, an entire sentence, or a longer passage of text. Examples of each of these pronoun functions are provided in Figure 1. It is clear that instances of the English pronoun “it” belonging to each of these functions would have different translation requirements in French and German. Introduction Pronoun translation poses a problem for current state-of-the-art Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). 525 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 525–542, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 The problem of pronouns in machine translation has long been studied. In particular, for SMT systems, the recent previous studies cited above have focused on the translation of anaphoric pronouns. In this case, a well-known constraint of languages with grammatical gender is that agreement must hold between an anaphoric pronoun and the NP with which it corefers, called its antecede"
W16-2345,W16-2352,1,0.881771,"Missing"
W16-2345,W16-2353,0,0.0435664,"s useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed sequences of these embeddings within a certain window to the left and to the right of the target pronoun. The window size used by the system is 50 tokens or until the end of the sentence boundary. All of these inputs are read"
W16-2345,2010.iwslt-papers.10,1,0.888921,"Missing"
W16-2345,D13-1037,1,0.883273,"3 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trained 6gram language model identical to the contrastive system of the UUPPSALA submission described above. The"
W16-2345,S16-1001,1,0.795211,"d 69.76 in macro-averaged recall. This is very much above the performance of baseline0 and baseline-1.5, which are in the low-mid 40s. It is also well above the majority/random baseline (not shown) at 11.11, which is outperformed by far by all systems. Note that the top-3 systems in terms of macro-averaged recall are also the top-3 in terms of accuracy, but in different order. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we adopted macro-averaged recall, which was also recently adopted by some other competitions, e.g., by SemEval-2016 Task 4 (Nakov et al., 2016). Moreover, as in 2015, we also report accuracy as a secondary evaluation measure. Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. 8 If the test data did not have any instances of some of the classes, we excluded these classes from the macro-a"
W16-2345,W15-2501,1,0.657407,"ould replace a placeholder value (represented by the token REPLACE) in the target-language text. It requires no specific Machine Translation (MT) expertise and is interesting as a machine learning task in its own right. Within the context of SMT, one could think of the task of cross-lingual pronoun prediction as a component of an SMT system. This component may take the form of a decoder feature or it may be used to provide “corrected” pronoun translations in a post-editing scenario. The design of the WMT 2016 shared task has been influenced by the design and the results of a 2015 shared task (Hardmeier et al., 2015) organised at the EMNLP workshop on Discourse in MT (DiscoMT). The first intuition about evaluating pronoun translation is to require participants to submit MT systems — possibly with specific strategies for pronoun translation — and to estimate the correctness of the pronouns they output. This estimation, however, cannot be performed with full reliability only by comparing pronouns across candidate and reference translations because this would miss the legitimate variation of certain pronouns, as well as variations in gender or number of the antecedent itself. Human judges are thus required f"
W16-2345,W16-2354,0,0.0469939,"Missing"
W16-2345,H05-1108,0,0.0601982,"Missing"
W16-2345,W14-3334,1,0.800608,"he OTHER class. For the DiscoMT 2015 shared task, we explored this issue for English–French and found that GIZA++ model 4 and HMM with grow-diag-final-and symmetrisation gave the best results. For pronoun– pronoun links, we had an F-score of 0.96, with perfect recall and precision of 0.93 (Hardmeier et al., 2015). This was slightly higher than for other links, which had an F-score of 0.92. For German–English, we explored this issue this year since it is a new language pair. We used an aligned gold standard of 987 sentences from (Pad´o and Lapata, 2005), which has been extensively evaluated by Stymne et al. (2014). We used the same methodology as in 2015, and performed an evaluation on the subset of links between the pronouns we are interested in. We report precision and recall of links both for the pronoun subset and for all links, shown in Table 4. The alignment quality is considerably worse than for French–English both for all links and for pronouns, but again the results for pronouns is better than for all links in both precision and recall. 6 https://github.com/slavpetrov/ universal-pos-tags 530 Alignment Symmetrisation Model 4 fast-align gdfa HMM gd gdf ∪ ∩ All links P R Pronouns P R .75 .69 .80"
W16-2345,W16-2355,1,0.832701,"the test dataset is imbalanced. Thus, one cannot interpret the absolute value of accuracy (e.g., is 0.7 a good or a bad value?) without comparing it to a baseline that must be computed for each specific test dataset. In contrast, for macro-averaged recall, it is clear that a value of, e.g., 0.7, is well above the majority-class and the random baselines, which are both always 1/C (e.g., 0.5 with two classes, 0.33 with three classes, etc.). Standard F1 and macro-averaged F1 are also sensitive to class imbalance for the same reason; see Sebastiani (2015) for more detail. The UU-S TYMNE systems (Stymne, 2016) use linear SVM classifiers for all language pairs. A number of different features were explored, but anaphora is not explicitly modelled. The features used can be grouped in the following way: source pronouns, local context words/lemmata, preceding nouns, target PoS n-grams with two different PoS tag-sets, dependency heads of pronouns, target LM scores, alignments, and pronoun position. A joint tagger and dependency parser on the source text is used for some of the features. The primary system is a 2-step classifier where a binary classifier is first used to distinguish between the OTHER clas"
W16-2345,petrov-etal-2012-universal,0,0.0937891,"Missing"
W16-2345,W16-2356,1,0.48149,"networks, except for the embedding for the aligned pronoun. All outputs of the recurrent layers are concatenated to a single vector along with the embedding of the aligned pronoun. This vector is then used to make the pronoun prediction by a dense neural network layer. The primary systems are trained to optimise macro-averaged recall and the contrastive systems are optimised without preference towards rare classes. The system is trained only on the shared task data and all parts of the data, in-domain and out-of-domain, are used for training the system. 5.5 5.6 UHELSINKI The UHELSINKI system (Tiedemann, 2016) implements a simple linear classifier based on LibSVM with its L2-loss SVC dual solver. The system applies local source-language and target-language context using the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun tra"
W16-2345,W16-2357,0,0.0259257,"well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features: tokens and their PoS tags are extracted from a context window around source- and targetside pronouns. N -gram combinations of these features are included by concatenating adjacent tokens or PoS tags. Furthermore, the pleonastic use of a pronoun is detected with NADA (Bergsma and Yarowsky, 2011) on the source side. 534 This CRF approach has been applied only to German, but there are plans to extend it to other languages. This indicates that the NN mechanism is quite effective. Th"
W16-2345,sagot-2010-lefff,0,0.0184156,"traction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed seq"
W16-2345,schmid-etal-2004-smor,0,0.0349386,"test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the pr"
W16-2345,W12-3018,0,\N,Missing
W16-2345,2015.iwslt-evaluation.1,1,\N,Missing
W16-2345,W14-6111,0,\N,Missing
W17-4801,D12-1133,0,0.165034,"16 (Guillou et al., 2016), but the differences in the resulting evaluation scores are actually minor. As we have explained above, the shared task focused primarily on subject pronouns. However, in English and German, some pronouns are ambiguous between subject and object position, e.g., the English it and the German es and sie. In order to address this issue, in 2016 we introduced filtering of object pronouns based on dependency parsing. This filtering removed all pronoun instances that did not have a subject dependency label.6 For joint dependency parsing and POS-tagging, we used Mate Tools (Bohnet and Nivre, 2012), with default models. Since in 2016 we found that this filtering was very accurate, this year we performed only automatic filtering for the training and the development, and also for the test datasets. Note that since only subject pronouns can be realized as prodropped pronouns in Spanish, subject filtering was not necessary. 4 Baseline Systems The baseline system is based on an n-gram language model (LM). The architecture is the same as that used for the WMT 2016 cross-lingual pronoun prediction task.7 In 2016, most systems outperformed this baseline, and for the sake of comparison, we thoug"
W17-4801,W16-2350,1,0.857196,"Missing"
W17-4801,W17-4807,1,0.863641,"Missing"
W17-4801,2010.iwslt-papers.10,1,0.907647,"is is hard as selecting the correct pronoun may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-au"
W17-4801,W15-2501,1,0.855394,"gradually raised interest in the research community for a shared task that would allow to compare various competing proposals and to quantify the extent to which they improve the translation of different pronouns for different language pairs and different translation directions. However, evaluating pronoun translation comes with its own challenges, as reference-based evaluation, which is standard for machine translation in general, cannot easily take into account legitimate variations of translated pronouns or their placement in the sentence. Thus, building upon experience from DiscoMT 2015 (Hardmeier et al., 2015) and WMT 2016 (Guillou et al., 2016), this year’s cross-lingual pronoun prediction shared task has been designed to test the capacity of the participating systems for translating pronouns correctly, in a framework that allows for objective evaluation, as we will explain below. 2 ce OTHER ce|PRON qui|PRON It ’s an idiotic debate . It has to stop . REPLACE 0 eˆ tre|VER un|DET d´ebat|NOM idiot|ADJ REPLACE 6 devoir|VER stopper|VER .|. 0-0 1-1 2-2 3-4 4-3 6-5 7-6 8-6 9-7 10-8 Figure 2: English→French example from the development dataset. First come the gold class labels, followed by the pronouns (t"
W17-4801,N13-1073,0,0.0460377,"raka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED talks, one for each subtask. However, we chose the current setup as using a smaller set of documents r"
W17-4801,W17-4806,0,0.0618776,"ial anaphora) or in different sentences (inter-sentential anaphora). Most MT systems translate sentences in isolation, and thus inter-sentential anaphoric pronouns will be translated without knowledge of their antecedent, and thus pronoun-antecedent agreement cannot be guaranteed. NMT yields generally higher-quality translation, but is harder to analyze, and thus little is known about how well it handles pronoun translation. Yet, it is clear that it has access to larger context compared to phrase-based SMT models, potentially spanning multiple sentences, which can improve pronoun translation (Jean et al., 2017a). Motivated by these challenges, the DiscoMT 2017 workshop on Discourse in Machine Translation offered a shared task on cross-lingual pronoun prediction. This was a classification task, asking the participants to make predictions about which pronoun should replace a placeholder in the target-language text. The task required no MT expertise and was designed to be interesting as a machine learning task on its own right, e.g., for researchers working on co-reference resolution. Source Target POS tags Reference The above constraints start playing a role in pronoun translation in situations where"
W17-4801,E12-3001,0,0.0230275,"may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the sou"
W17-4801,2005.mtsummit-papers.11,0,0.0869283,"teresting research challenges from the perspective of both speech recognition and machine translation. Therefore, both research communities are making increased use of them in building benchmarks. TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. 3.1.2 Europarl and News For training purposes, in addition to TED talks, we further made available the Europarl3 (Koehn, 2005) and News Commentary4 corpora for all language pairs but Spanish-English, for which only TED talks and Europarl were available. We used the alignments provided by OPUS, including the document boundaries from the original sources. For Europarl, we used ver. 7 of the data release, and for News Commentary we used ver. 9. 3.2 Test Set Selection We selected the test data from talks added recently to the TED repository such that: 1. The talks have been transcribed (in English) and translated into both German and French. 2. They were not used in the IWSLT evaluation campaigns, nor in the DiscoMT 2015"
W17-4801,2005.iwslt-1.8,0,0.121419,"OS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English dir"
W17-4801,W10-1737,0,0.13529,"Missing"
W17-4801,J03-1002,0,0.0103978,"rted the TreeTagger’s POS tags to the target coarse POS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction,"
W17-4801,P14-2050,0,0.026726,"that is aligned to the pronoun to be predicted. All input sequences are fed in an embedding layer followed by two layers of GRUs. The values in the last layer form a vector, which is further concatenated to the pronoun alignment embeddings, to form a larger vector, which is then used to make the final prediction using a dense neural network. The pretraining is a modification of the skip-gram model of WORD 2 VEC (Mikolov et al., 2013), in which along with the skip-gram token context, all target sentence pronouns are predicted as well. The process of pretraining is performed using WORD 2 VECF (Levy and Goldberg, 2014). 5.2 NYU The NYU system (Jean et al., 2017b) uses an attention-based neural machine translation model and three variants that incorporate information from the preceding source sentence. The sentence is added as an auxiliary input using additional encoder and attention models. The systems are not specifically designed for pronoun prediction and may be used to generate complete sentence translations. They are trained exclusively on the data provided for the task, using the text only and ignoring the provided POS tags and alignments. 5.4 UU-Hardmeier The UU- HARDMEIER system (Hardmeier, 2017) is"
W17-4801,petrov-etal-2012-universal,0,0.0455296,"Missing"
W17-4801,W16-2351,1,0.891541,"Missing"
W17-4801,S17-2088,1,0.872336,"Missing"
W17-4801,W16-2202,0,0.0276957,"Missing"
W17-4801,D15-1166,0,0.0502002,"nt. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the targetlanguage lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of cl"
W17-4801,W16-2353,0,0.0406223,"/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a different tag: EP. Still, we decided to use the same filtering this year, to keep the task sta"
W17-4801,L16-1680,0,0.054481,"Missing"
W17-4801,W17-4808,0,0.0205662,"mata constructed from news texts, parliament debates, and the TED talks of the training/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a differe"
W17-4801,W16-2355,1,0.796565,"the data is used in each epoch. For the primary system, all classes are sampled equally, as long as there are enough instances for each class. Although this sampling method biases the system towards macro-averaged recall, on the test data the system performed very well in terms of both macro-averaged recall and accuracy. The secondary system uses a sampling method in which the samples are proportional to the class distribution in the development dataset. 5.5 UU-Stymne16 The UU-S TYMNE 16 system uses linear SVM classifiers, and it is the same system that was submitted for the 2016 shared task (Stymne, 2016). It is based mainly on local features, and anaphora is not explicitly modeled. The features used include source pronouns, local context words/lemmata, target POS n-grams with two different POS tagsets, dependency heads of pronouns, alignments, and position of the pronoun. A joint tagger and dependency parser (Bohnet and Nivre, 2012) is used on the source text in order to produce some of the features. Overall, the source pronouns, the local context and the dependency features performed best across all language pairs. 8 7 Stymne (2016) describes several variations of the method, including both"
W17-4801,W17-4805,1,0.882705,"Missing"
W17-4801,S16-1001,1,0.80372,"erforming system here is T URKU NLP with a macro-averaged recall of 58.82. However, it is nearly tied with U PPSALA, and both are somewhat close to NYU. Noteworthy, though, is that the highest-scoring system on macro-average recall is the contrastive system of NYU; NYU also has the second-best accuracy, outperformed only by U PPSALA. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we followed the setup of 2016, where we switched to macroaveraged recall, which was also recently adopted by some other competitions, e.g., by SemEval2016/2017 Task 4 (Nakov et al., 2016; Rosenthal et al., 2017). Moreover, as in 2015 and 2016, we also report accuracy as a secondary evaluation measure (but we abandon F1 altogether). Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. The advantage of macro-averaged recall over acc"
W17-4801,C96-2141,0,0.453266,"c information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED t"
