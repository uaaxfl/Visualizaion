2020.aacl-main.91,J08-4004,0,0.016822,"analysis over the dataset to better understand its properties. Inter7 https://www.ibge.gov.br/en/home-eng. html 917 Table 1: Annotators demographic information. 8 6 count 3.2 Categories Male Female 4 2 0 18 19 20 21 22 23 24 25 26 27 29 30 35 37 age Figure 1: Annotators age distribution. α LGBTQ+phobia Insult Xenophobia Misogyny Obscene Racism 0.68 0.56 0.57 0.52 0.49 0.48 Mean 0.55 Table 2: Krippendorff ’s α for each label. annotator agreement is calculated in terms of Krippendorf ’s α (Table 2), since α is robust to multiple annotators, different degrees of disagreement and, missing values (Artstein and Poesio, 2008). The LGBTQ+phobia class shows the highest agreement, which may indicate that comments in this class have a more distinctive lexicon than other classes. The lowest agreement is seem in obscene and racism classes. Besides, we observed in the annotations many cases in which some examples were labelled as separate classes, although they intend o fdp do filho dela nao parava de tocar auto pra c*****o [...] Ann 1 Ann 2 Ann 3 Insult None Obscene her sob son did not stop to play loud as f**k [...] ˜ VC NAO ˜ E´ FELIZ PQ NAO QUER [...] VAI SE F***R IRMAO Obscene Insult Insult [...] f**k you brother yo"
2020.aacl-main.91,W19-3510,0,0.0195581,"large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data. Although there is some work on this topic for other languages – e.g. Arabic (Mubarak et al., 2017) and German (Wiegand et al., 2018) –, most of the resources and studies available are for English (Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019b).3 For Portuguese, only two previous works are available (Fortuna et al., 2019; de Pelle and Moreira, 2017) and their datasets are considerably small, mainly when compared to resources available for English. We present ToLD-Br (Toxic Language Dataset for Brazilian Portuguese), a new dataset with Twitter posts in the Brazilian Portuguese language.4 1 https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge/ overview 2 This is also similar to the usage of offensive comments in OffensEval (Zampieri et al., 2019b, 2020). 3 A large list of resources is available at http:// hatespeechdata.com. 4 It is important to distinguish the language variant, since 914 Pro"
2020.aacl-main.91,S19-2007,0,0.0330831,"lassification, where each different type of toxicity is automatically classified. This is a considerably harder problem than binary classification, where BERTbased models do not outperform the baseline. Section 2 presents an overview of relevant previous work. Section 3 shows details about the ToLDBr dataset. Material and methods are presented in Section 4, whilst results are discussed in Section 5. Finally, Section 6 shows a final discussion and future work. 2 Related Work Although multiple researchers have addressed the topic of hate speech (e.g. Waseem and Hovy (2016), Chung et al. (2019), Basile et al. (2019)), we focus the literature review on previous work related to toxic comments detection, the topic of our paper. Due to space constraints, we only describe papers that create and use Twitter-based datasets and/or focus on the Brazilian Portuguese language. English Davidson et al. (2017) present a dataset with around 25K tweets annotated by crowdworkers as containing hate, offensive language, or neither. They build a feature-based classifier with TF-IDF transformation over n-grams, part-ofspeech information, sentiment analysis, network information (e.g., number of replies), among other features."
2020.aacl-main.91,P19-1271,0,0.0199259,"nt with multi-label classification, where each different type of toxicity is automatically classified. This is a considerably harder problem than binary classification, where BERTbased models do not outperform the baseline. Section 2 presents an overview of relevant previous work. Section 3 shows details about the ToLDBr dataset. Material and methods are presented in Section 4, whilst results are discussed in Section 5. Finally, Section 6 shows a final discussion and future work. 2 Related Work Although multiple researchers have addressed the topic of hate speech (e.g. Waseem and Hovy (2016), Chung et al. (2019), Basile et al. (2019)), we focus the literature review on previous work related to toxic comments detection, the topic of our paper. Due to space constraints, we only describe papers that create and use Twitter-based datasets and/or focus on the Brazilian Portuguese language. English Davidson et al. (2017) present a dataset with around 25K tweets annotated by crowdworkers as containing hate, offensive language, or neither. They build a feature-based classifier with TF-IDF transformation over n-grams, part-ofspeech information, sentiment analysis, network information (e.g., number of replies),"
2020.aacl-main.91,N19-1423,0,0.0943486,"Missing"
2020.aacl-main.91,W18-3504,0,0.0673115,"Missing"
2020.aacl-main.91,W17-3008,0,0.0284079,"eful for platform’s moderators and to select content for specific users (e.g. children). Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms; (ii) there is a large spectrum of types of toxicity (e.g. sexism, racism, insult); (iii) toxic comments correspond to a minority of comments, which is fortunate, but means that automatic data-driven approaches need to deal with highly unbalanced data. Although there is some work on this topic for other languages – e.g. Arabic (Mubarak et al., 2017) and German (Wiegand et al., 2018) –, most of the resources and studies available are for English (Davidson et al., 2017; Wulczyn et al., 2017; Founta et al., 2018; Mandl et al., 2019; Zampieri et al., 2019b).3 For Portuguese, only two previous works are available (Fortuna et al., 2019; de Pelle and Moreira, 2017) and their datasets are considerably small, mainly when compared to resources available for English. We present ToLD-Br (Toxic Language Dataset for Brazilian Portuguese), a new dataset with Twitter posts in the Brazilian Portuguese language.4 1 https://www.kaggle.com/c/ jigsaw-toxic-c"
2020.aacl-main.91,W19-3512,0,0.0327688,"Missing"
2020.aacl-main.91,N16-2013,0,0.205525,"oxicity. 1 Introduction Social media can be a powerful tool that enables virtual human interactions, connecting people and enhancing businesses’ presence. On the other hand, since users feel somehow protected under their virtual identities, social media has also become a platform for hate speech and use of toxic language. Although hate speech is a crime in most countries, identifying cases in social media is not an easy task, given the massive amounts of data posted every day. Therefore, automatic approaches for detecting online hate speech have received significant attention in recent years (Waseem and Hovy, 2016; Davidson et al., 2017; Zampieri et al., 2019b). In this paper, we focus on the analysis and automatic detection of toxic comments. Our definition of toxic is similar to the one used by the Jigsaw competition,1 where comments containing insults and obscene language are also considered, besides hate speech.2 Systems capable of automatically identifying toxic comments are useful for platform’s moderators and to select content for specific users (e.g. children). Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explic"
2020.aacl-main.91,N19-1144,0,0.218122,"powerful tool that enables virtual human interactions, connecting people and enhancing businesses’ presence. On the other hand, since users feel somehow protected under their virtual identities, social media has also become a platform for hate speech and use of toxic language. Although hate speech is a crime in most countries, identifying cases in social media is not an easy task, given the massive amounts of data posted every day. Therefore, automatic approaches for detecting online hate speech have received significant attention in recent years (Waseem and Hovy, 2016; Davidson et al., 2017; Zampieri et al., 2019b). In this paper, we focus on the analysis and automatic detection of toxic comments. Our definition of toxic is similar to the one used by the Jigsaw competition,1 where comments containing insults and obscene language are also considered, besides hate speech.2 Systems capable of automatically identifying toxic comments are useful for platform’s moderators and to select content for specific users (e.g. children). Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms;"
2020.aacl-main.91,S19-2010,0,0.385439,"powerful tool that enables virtual human interactions, connecting people and enhancing businesses’ presence. On the other hand, since users feel somehow protected under their virtual identities, social media has also become a platform for hate speech and use of toxic language. Although hate speech is a crime in most countries, identifying cases in social media is not an easy task, given the massive amounts of data posted every day. Therefore, automatic approaches for detecting online hate speech have received significant attention in recent years (Waseem and Hovy, 2016; Davidson et al., 2017; Zampieri et al., 2019b). In this paper, we focus on the analysis and automatic detection of toxic comments. Our definition of toxic is similar to the one used by the Jigsaw competition,1 where comments containing insults and obscene language are also considered, besides hate speech.2 Systems capable of automatically identifying toxic comments are useful for platform’s moderators and to select content for specific users (e.g. children). Nevertheless, there are multiple challenges specific to process toxic comments automatically, e.g. (i) toxic language may not be explicit, i.e. may not contain explicit toxic terms;"
2020.aacl-main.91,2020.semeval-1.188,0,0.0939088,"Missing"
2020.aacl-main.92,S17-2080,0,0.0455375,"Missing"
2020.aacl-main.92,S19-2193,0,0.0343048,"Missing"
2020.aacl-main.92,S17-2081,0,0.0640373,"Missing"
2020.aacl-main.92,C18-1284,1,0.836142,"ammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but also two minority classes (deny and support) are the most important to classify well. Based on prior research on imbalanced datasets in areas other that NLP (e.g. Yijing et al. (201"
2020.aacl-main.92,S17-2082,0,0.0669704,"Missing"
2020.aacl-main.92,S19-2192,0,0.0304242,"Missing"
2020.aacl-main.92,S19-2196,0,0.0209881,"Missing"
2020.aacl-main.92,S17-2084,0,0.056951,"Missing"
2020.aacl-main.92,S19-2197,0,0.026033,"Missing"
2020.aacl-main.92,S19-2147,1,0.891312,"Missing"
2020.aacl-main.92,S19-2195,0,0.0290328,"Missing"
2020.aacl-main.92,C18-1288,0,0.017542,"emEval 2016 task 6 (Mohammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but also two minority classes (deny and support) are the most important to classify well. Based on prior research on imbalanced datasets in areas other that NLP (e."
2020.aacl-main.92,P19-1498,0,0.0145585,"not only an imbalanced, multi-class problem, but it also has classes with different importance. This is different from standard stance classification tasks (e.g. SemEval 2016 task 6 (Mohammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but"
2020.aacl-main.92,S19-2148,0,0.0840143,"tion tasks (e.g. SemEval 2016 task 6 (Mohammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but also two minority classes (deny and support) are the most important to classify well. Based on prior research on imbalanced datasets in"
2020.aacl-main.92,P19-1113,0,0.0451716,"tion tasks (e.g. SemEval 2016 task 6 (Mohammad et al., 2016)), where classes have arguably the same importance. It also differs from the veracity task (RumourEval sub-task B), where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy (ACC), which is not sufficiently robust on imbalanced datasets (Huang and Ling, 2005). This prompted the adoption of macro-F 1 in the 2019 evaluation. Kumar and Carley (2019) also argue that macro-F 1 is a more reliable evaluation metric for RSC. Previous work on RSC also adopted these metrics (Li et al., 2019b; Kochkina et al., 2018; Dungs et al., 2018). This paper re-evaluates the sub-task A results of RumourEval 2017 and 2019.2 It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro-F 1, that is robust for evaluating binary classification on imbalanced datasets, fails to reliably evaluate the performance on RSC. This is particularly critical in RumourEval where not only is data imbalanced, but also two minority classes (deny and support) are the most important to classify well. Based on prior research on imbalanced datasets in"
2020.aacl-main.92,S19-2194,0,0.0262947,"Missing"
2020.aacl-main.92,S16-1003,0,0.0800136,"Missing"
2020.aacl-main.92,S17-2087,0,0.0655003,"Missing"
2020.aacl-main.92,S17-2085,0,0.0557501,"Missing"
2020.aacl-main.92,S17-2086,0,0.0477101,"Missing"
2020.aacl-main.92,S19-2191,0,0.0254569,"Missing"
2020.iwltp-1.15,W16-3503,1,0.91547,"PI Manager Workflow Manager Preprocessing Provisioning of Datasets and Content Language Identification Storage Knowledge Graph File Storage Duplicate Detection Document Structure Analysis Semantic Analysis Content Generation Summarization Named Entity Recognition and Linking Paraphrasing Temporal Expression Analysis Machine Translation Relation Extraction Semantic Storytelling Event Detection Security Figure 5: Technical architecture of the QURATOR platform velops a curation technology platform, which is also being populated with services, simplifying and accelerating the curation of content (Bourgonje et al., 2016a; Rehm et al., 2019a; Schneider and Rehm, 2018a; Schneider and Rehm, 2018b). The project develops, evaluates and integrates services for preprocessing, analyzing and generating content, spanning use cases from the sectors of culture, media, health and industry. To process and transform incoming data, text or multimedia streams into device-adapted, publishable content, various groups of components, services and technologies are applied. These include adapters to data, content and knowledge sources, as well as infrastructural tools and AI methods for the acquisition, analysis and generation of"
2020.iwltp-1.15,2020.lrec-1.696,1,0.735511,"nt annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 8 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every annotation model, a linking model defines subclass-relationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other community-maintained vocabularies are linked with OLiA, e. g., the CLARIN Concept Registry (Chiarcos et al., 2020). OLiA was developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Witt et al., 2009; Rehm et al., 2009). Its field of application included the formalization of annotation schemes and concept-based querying over heterogeneously annotated corpora (Rehm et al., 2008a). As several institutions and resources from various disciplines were involved, no holistic annotation standard could be enforced onto the contributors. 101 3.4. Figure 8: Modular OLiA ontologies 3.2. Level 1: Simple Cross-Platform"
2020.iwltp-1.15,W12-5201,0,0.0160466,"tral hub for linguistic annotation terminology in the web of data. OLiA was designed for mediating between various terminology repositories on the one hand and annotated resources (i. e., their annotation schemes), on the other. Four different types of ontologies are distinguished (Fig. 8): (1) The OLiA Reference Model is an OWL ontology that specifies the common terminology that different annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 8 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every annotation model, a linking model defines subclass-relationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other community-maintained vocabularies are linked with OLiA, e. g., the CLARIN Concept Registry (Chiarcos et al., 2020). OLiA was developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Witt et al., 2009; Rehm et al., 2009"
2020.iwltp-1.15,2020.lrec-1.420,1,0.820145,"Missing"
2020.iwltp-1.15,W17-4212,1,0.852914,". g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data Documents Figure 6: The Lynx technology platform The platform’s microservice architecture is a variant of the service-oriented architecture (SOA), in which an application is structured as a collection of loosely coupled services. It uses Docker containers ho"
2020.iwltp-1.15,2020.iwltp-1.12,1,0.914049,"ments, yet others provide a user interface. The Document Manager provides the storage and annotation of documents with an emphasis on keeping them synchronized, providing read and write access, as well as updates of documents and annotations. It can be queried in terms of annotations and documents, through REST APIs. The interface includes a set of create, read, update, and delete APIs to manage collections, documents and annotations. The orchestration and execution of services involved in more complex tasks is addressed by a Workflow Manager. It defines combinations of services as workflows (Moreno-Schneider et al., 2020b; Bourgonje et al., 2016a; Schneider and Rehm, 2018a; Schneider and Rehm, 2018b). Workflows are described using BPMN and executed using Camunda.8 Interoperability is addressed at the following levels: Since the QURATOR platform is a closed ecosystem, the platform can be thought of as an experimental toolbox with services customised by the partners for their own use cases. As the platform is used only by the QURATOR partners, it does not contain a catalogue or any kind or structured metadata. However, two of the ten QURATOR projects have a focus on service composition and workflows with protot"
2020.iwltp-1.15,2020.lrec-1.284,1,0.886537,"Missing"
2020.iwltp-1.15,piperidis-2012-meta,1,0.875093,"t least upon a certain (obligatory) subset (Labropoulou et al., 2020; McCrae et al., 2015). Such a more detailed, semantics-driven approach enables more efficient and more user-friendly search results from multiple platforms that can be visually aggregated and also easily ranked. The actual search can be performed through publicly available APIs but returned objects would be semantically richer. Alternatively, the metadata records of external repositories can be harvested using standard protocols such as OAI-PMH, which allow the construction of a master index out of decentralised inventories (Piperidis, 2012). A known issue that needs to be addressed using such an approach involves the detection of duplicate resources. Figure 9: A cross-platform workflow example A similar approach was implemented in the project OpenMinTeD (OMTD) (Labropoulou et al., 2018) using the Galaxy workflow management system.10 Three types of LT components are supported: (1) components packaged in Docker images that follow the OMTD specifications; (2) components wrapped with UIMA or GATE, available in a Maven repository; (3) Text and Data Mining web services that run outside the OMTD platform and that follow the OMTD specif"
2020.iwltp-1.15,L18-1519,1,0.396376,"ed in AI4EU Experiments. 2.2. European Language Grid (ELG) Multilingualism and cross-lingual communication in Europe can only be enabled through Language Technologies (LTs) (Rehm et al., 2016). The European LT landscape is fragmented (Vasiljevs et al., 2019), holding back its impact. Another crucial issue is that many languages are underresourced and, thus, in danger of digital extinction (Rehm and Uszkoreit, 2012; Kornai, 2013; Rehm et al., 2014). There is an enormous need for an European LT platform as a unifying umbrella (Rehm and Uszkoreit, 2013; Rehm et al., 2016; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018; Rehm et al., 2020c). The project European Language Grid (2019-2021) attempts to establish the primary platform and marketplace for the European LT community, both industry and research (Rehm et al., 2020a). This scalable cloud platform will provide access to hundreds of LTs for all European languages, including running services as well as data sets. ELG will enable the European LT community to upload their technologies and data sets, to deploy them, and to connect with other resources. ELG caters for commercial and non-commercial LTs (i. e., LTs with a high Technol"
2020.iwltp-1.15,rehm-etal-2008-ontology,1,0.781144,"Missing"
2020.iwltp-1.15,W17-2707,1,0.837412,"road groups: (1) Preprocessing encompasses services for obtaining and processing information from different content sources so that they can be used in the platform and integrated into other services (Schneider et al., 2018), e. g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data Documents Figure 6:"
2020.iwltp-1.15,2020.lrec-1.413,1,0.802365,"Missing"
2020.iwltp-1.15,2016.tc-1.14,1,0.737947,"n be divided into three broad groups: (1) Preprocessing encompasses services for obtaining and processing information from different content sources so that they can be used in the platform and integrated into other services (Schneider et al., 2018), e. g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data"
2020.lrec-1.163,W16-0310,1,0.269202,"s, the EHR can be analysed at different levels of granularity (Velupillai et al., 2018). Considering these levels of granularity for detection of suicidal behaviour, previous approaches can be categorised into patient level - a set of EHRs that belong to a certain patient who has exhibited or exhibits suicidal behaviour (Barak-Corren et al., 2017; Tran et al., 2015; Walsh et al., 2017; Fernandes et al., 2018; Choi et al., 2018; Cook et al., 2016); mention level - word or sub-sentences that mention suicidal thoughts or behaviour (Haerian et al., 2012; Anderson et al., 2015; Downs et al., 2017; Gkotsis et al., 2016b); and combinations of these into document and patient level - aggregating mention level annotations to derive document- and patient-level labels (Velupillai et al., 2019). To the best of our knowledge, none of these previous approaches have addressed this problem on a sentence level. There are several benefits to modeling the problem of classifying suicidal information in EHRs on a sentence level: • Sentences are relatively shorter than patient/document level, so researchers/clinicians can easily verify or identify the suicidal thought. • Compared with the mention level, data labelling is re"
2020.lrec-1.163,D14-1181,0,0.00311443,"x classifier – blue For details of the C-LSTM-CNN architecture please refer our previous paper (Song et al., 2018). We briefly describe its motivation here. The input words of the focus sentence are first transformed into vector space representations in order to capture the semantic representation of the words. In this paper, we use the Word2Vec(w2v) embedding model (Mikolov et al., 2013). The bi-directional LSTM layer (Hochreiter and Schmidhuber, 1997) is used to enrich the word vector representation with sentence level sequential information. This is followed by CNN with max-pooling layers (Kim, 2014), which extract local features at specific points from the LSTM outputs. In addition to processing the focus sentence with the LSTM and CNN layers, the input words of the left and right context are encoded by an adapted FOFE encoder. All sentences prior to the focus are considered part of the left context, and all sentences following the focus to be part of the right context. Each sentence i in the context can be represented as Si = {x0 , x1 , ...xU }, a sequence of U vector representations xu , one for each word in the sentence. The context representation z for S can then be calculated recurs"
2020.lrec-1.163,N16-1062,0,0.0197113,"al. When combined with context (Sentence 2), Example 1 expresses a clear suicidal thought, whereas Example 2 does not necessarily convey suicidality. In order to address the importance of context, we propose a sentence level suicidal behaviour classification approach based on a C-LSTM-CNN (Song et al., 2018) algorithm. C stands for context. We encode intra-sentence context using a bi-directional LSTM. We encode the surrounding intersentence context using a Fixed Size Ordinally Forgetting Encoding (FOFE (Zhang et al., 2015)) based algorithm. Compared with previous LSTM based context encodings (Lee and Dernoncourt, 2016), a FOFE can be generated before training, to massively reduce computational cost in both training and application. This reduction in computational cost satisfies an important requirement that our method be usable in the typically compute resource constrained environments of hospitals, where additionally, the ethically sensitive nature of the data means that it cannot be processed elsewhere. Whilst we present a mental health use case for our method, the importance of inter-sentence context and of computationally efficient deep learning are not restricted to this domain, as discussed in (Song e"
2020.lrec-1.163,W19-3005,0,0.278978,"ceptron, and logistic regression deep learning 2 based research on suicidal behaviour detection, to date. This may be because of the challenges in data access, where access to EHR data is typically restricted due to governance regulations. However, in non-restricted data (e.g. social media data), deep learning approaches have dominated work on detecting suicide-related information in recent years. Approaches have included CNN (Shing et al., 2018; Gaur et al., 2019), LSTM (Ji et al., 2018), attention LSTM (Coppersmith et al., 2018), combined CNN and LSTM (Sawhney et al., 2018). Most recently, (Matero et al., 2019) applied context word embeddings (BERT) to improved performance. 3. C-LSTM-CNN The C-LSTM-CNN architecture is shown in Figure 1. It takes three inputs: 1. the focus sentence - the sentence we are aiming to classify; 2. left context - all document text to the left of the focus sentence; 3. right context all document text to the right of the focus sentence. The architecture contains five major parts, represented in five different colors in Figure 1: 1. Word embedding – purple 2. Bi-directional LSTM – pink 3. Multiple window CNN – yellow 4. Context encoder – green 5. Softmax classifier – blue For"
2020.lrec-1.163,W18-6223,0,0.0273046,"n rules, decision trees, multilayer perceptron, and logistic regression deep learning 2 based research on suicidal behaviour detection, to date. This may be because of the challenges in data access, where access to EHR data is typically restricted due to governance regulations. However, in non-restricted data (e.g. social media data), deep learning approaches have dominated work on detecting suicide-related information in recent years. Approaches have included CNN (Shing et al., 2018; Gaur et al., 2019), LSTM (Ji et al., 2018), attention LSTM (Coppersmith et al., 2018), combined CNN and LSTM (Sawhney et al., 2018). Most recently, (Matero et al., 2019) applied context word embeddings (BERT) to improved performance. 3. C-LSTM-CNN The C-LSTM-CNN architecture is shown in Figure 1. It takes three inputs: 1. the focus sentence - the sentence we are aiming to classify; 2. left context - all document text to the left of the focus sentence; 3. right context all document text to the right of the focus sentence. The architecture contains five major parts, represented in five different colors in Figure 1: 1. Word embedding – purple 2. Bi-directional LSTM – pink 3. Multiple window CNN – yellow 4. Context encoder –"
2020.lrec-1.163,W18-0603,0,0.0200863,"Missing"
2020.lrec-1.163,D18-1107,1,0.865624,"adjacent sentences. Consider the following example: Example 1 1. He says that he wants to jump out of the window. 2. He feels life is not worth continuing. Example 2 1. He says that he wants to jump out of the window. 2. He wants to escape from his family. 1303 Sentence 1 in both Example 1 and 2 are identical. When combined with context (Sentence 2), Example 1 expresses a clear suicidal thought, whereas Example 2 does not necessarily convey suicidality. In order to address the importance of context, we propose a sentence level suicidal behaviour classification approach based on a C-LSTM-CNN (Song et al., 2018) algorithm. C stands for context. We encode intra-sentence context using a bi-directional LSTM. We encode the surrounding intersentence context using a Fixed Size Ordinally Forgetting Encoding (FOFE (Zhang et al., 2015)) based algorithm. Compared with previous LSTM based context encodings (Lee and Dernoncourt, 2016), a FOFE can be generated before training, to massively reduce computational cost in both training and application. This reduction in computational cost satisfies an important requirement that our method be usable in the typically compute resource constrained environments of hospita"
2020.lrec-1.163,P15-2081,0,0.0240187,"2. He wants to escape from his family. 1303 Sentence 1 in both Example 1 and 2 are identical. When combined with context (Sentence 2), Example 1 expresses a clear suicidal thought, whereas Example 2 does not necessarily convey suicidality. In order to address the importance of context, we propose a sentence level suicidal behaviour classification approach based on a C-LSTM-CNN (Song et al., 2018) algorithm. C stands for context. We encode intra-sentence context using a bi-directional LSTM. We encode the surrounding intersentence context using a Fixed Size Ordinally Forgetting Encoding (FOFE (Zhang et al., 2015)) based algorithm. Compared with previous LSTM based context encodings (Lee and Dernoncourt, 2016), a FOFE can be generated before training, to massively reduce computational cost in both training and application. This reduction in computational cost satisfies an important requirement that our method be usable in the typically compute resource constrained environments of hospitals, where additionally, the ethically sensitive nature of the data means that it cannot be processed elsewhere. Whilst we present a mental health use case for our method, the importance of inter-sentence context and of"
2020.lrec-1.176,W10-1001,1,0.747351,"Missing"
2020.lrec-1.176,W13-4829,1,0.814744,"Missing"
2020.lrec-1.176,W11-2308,0,0.076365,"Missing"
2020.lrec-1.176,E17-1090,0,0.0641901,"Missing"
2020.lrec-1.176,W11-4504,1,0.434882,"Missing"
2020.lrec-1.176,D15-1133,0,0.0708394,"Missing"
2020.lrec-1.176,P18-1022,0,0.0659801,"Missing"
2020.lrec-1.176,N10-2011,1,0.782921,"Missing"
2020.lrec-1.176,W16-4123,0,0.0222544,"Missing"
2020.lrec-1.176,P17-2102,0,0.0484565,"Missing"
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.lrec-1.413,cassidy-etal-2014-alveo,0,0.0760054,"Missing"
2020.lrec-1.413,W03-0810,0,0.137563,"Missing"
2020.lrec-1.413,gavrilidou-etal-2012-meta,1,0.915791,"Missing"
2020.lrec-1.413,hinrichs-krauwer-2014-clarin,0,0.182909,"Missing"
2020.lrec-1.413,P10-4005,0,0.0528445,"Missing"
2020.lrec-1.413,2020.lrec-1.420,1,0.821941,"Missing"
2020.lrec-1.413,L18-1213,1,0.812955,"0b) and plan to integrate experimental workflow functionality into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 addition"
2020.lrec-1.413,2020.iwltp-1.12,1,0.789706,"ercial services, written in disparate programming languages (Java/Spring, .NET, Python) with just a few days work in the first iteration, falling to a few hours once developers became more familiar with the infrastructure and required formats. 14 These requests are received and handled by the LT Service Execution Server (Section 3.1). 3370 The composition of individual services offered by ELG directly or other cloud platforms is not addressed by ELG itself. However, we experiment with workflow composition and platform interoperability in other contexts (Rehm et al., 2020a; Rehm et al., 2020b; Moreno-Schneider et al., 2020a; Moreno-Schneider et al., 2020b) and plan to integrate experimental workflow functionality into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent"
2020.lrec-1.413,2020.lrec-1.284,1,0.363908,"Missing"
2020.lrec-1.413,piperidis-etal-2014-meta,1,0.89681,"metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 additional repositories have been located so far, which will increase the numbers in Table"
2020.lrec-1.413,L18-1205,1,0.945027,"tities of interest to users (Section 3.6), appropriately indexed and described so that they can easily search, find and select the resources that meet their requirements and deploy them, as well as visualise the LT domain activities, stakeholders and resources with specific criteria (e. g., service type, language, etc.). All entities are described in compliance with the ELG-SHARE metadata schema (Labropoulou et al., 2019; Labropoulou et al., 2020).7 The schema builds upon, consolidates and updates previous activities, especially the META-SHARE schema and its profiles (Gavrilidou et al., 2012; Piperidis et al., 2018; Labropoulou et al., 2018), taking into account the ELG user requirements (Melnika et al., 2019a), recent developments in the (meta)data domain (e. g., FAIR8 , data and software citation recommendations9 , Open Science movement, etc.), and the need for establishing a common pool of resources through exchange mechanisms with collaborating projects and initiatives (Rehm et al., 2020c), cf. Section 3.6. The schema caters for the description of the ELG core entities (Figure 2), i. e., Language Technologies (tools/services), including functional services and nonfunctional ones (e. g., downloadable"
2020.lrec-1.413,piperidis-2012-meta,1,0.90149,"y into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 additional repositories have been located so far, which will incr"
2020.lrec-1.413,L18-1519,1,0.634981,"any are world-class, with technologies that outperform the global players. However, European LT business is also fragmented – by nation states, languages, domains and sectors (Vasiljevs et al., 2019) –, significantly holding back its impact. In addition, many European languages are severely under-resourced and, thus, in danger of digital language exinction (Rehm and Uszkoreit, 2012; Kornai, 2013; Rehm et al., 2014; Rehm et al., 2016a), which is why there is an enormous need for a European LT platform as a unifying umbrella (Rehm and Uszkoreit, 2013; Rehm et al., 2016b; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018). The project European Language Grid (ELG; 2019-2021) addresses this fragmentation by establishing the ELG as the primary platform and marketplace for the European LT community, both industry and research.1 The ELG is developed to be a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and 1 https://www.european-language-grid.eu resources. Once fully operational, it will enable the commercial and non-commercial E"
2020.lrec-1.413,L16-1251,1,0.856739,"Missing"
2020.lrec-1.413,P14-5010,0,\N,Missing
2021.eacl-demos.26,hinrichs-krauwer-2014-clarin,0,0.0190659,"arts in February 2021 with a duration of 912 months. In the first call, 110 proposals were accepted for evaluation with applicants from 29 countries. We received more proposals from SMEs (62) than re search organisations (48). While 79 proposals fo 2.10 Legal Entity 3 Related Work ELG builds upon previous work of the ELG con sortium and the wider European LT community, es pecially METANET/META and ELRC. In addition, we have collected more than 30 plat forms, projects or initiatives that can be considered relevant for ELG including, among others, UIMA (Ferrucci and Lally, 2003), CLARIN (Hinrichs and Krauwer, 2014), DKPro (Gurevych et al., 2007); Rehm et al. (2020a) provide an exhaustive com parison. They share at least one of the following goals with ELG, i. e., they provide: 1) a collec tion of LT/NLP tools or data sets; 2) a platform, which harvests metadata records from distributed sources, 3) a platform for the sharing of tools or data sets. While related projects do exist, the ap proach of ELG is unique. The platform that most closely resembles ELG is the National Platform for LT, operated by the Ministry of Electronics and In formation Technology in India.16 Several global technology enterpri"
2021.eacl-demos.26,2020.iwltp-1.12,1,0.888601,"Missing"
2021.eacl-demos.26,piperidis-2012-meta,1,0.736597,"four TTS and two text categori sation services. Further services are being added on a regular basis with 200+ additional IE and text analysis services, 21 MT, eight ASR and nine TTS scheduled to be included by the time of ELG Re lease 2 in February 2021. We aim to make it as simple as possible for LT providers to integrate their services, but in a Already now ELG provides access to more than 2700 language resources. We ingested substan tial resources from existing repositories, especially ELDA/ELRA, ELRCSHARE (Lösch et al., 2018; Piperidis et al., 2018; Smal et al., 2020) and META SHARE (Piperidis, 2012; Piperidis et al., 2014). We have also been working on ‘external’ reposito ries, about 220 of which have been identified so far. Some (e. g., Zenodo, Quantum Stat) are al ready being ingested together with two reposito ries related to ELG, LINDAT/CLARIAHCZ and ELRASHARELRs (LRs published at LREC). 2.6 Access Methods and User Interfaces Our main groups of users are: (1) LT/LR providers – companies or research organisations with tools, services or data that can be provided through the ELG; (2) Developers and integrators – companies and research institutions interested in using LT; (3) Gen"
2021.eacl-demos.26,L18-1205,1,0.930475,"ps://www.postgresql.org 8 https://www.keycloak.org 9 https://prometheus.io 10 https://helm.sh 5 2.3 Catalogue The metadata records stored in the catalogue en able access to services and data resources. They are described using the ELG metadata schema (Labropoulou et al., 2020) and can be browsed and explored. The catalogue also includes a registry of stakeholders who develop LT services or products, and relevant projects, thus providing an overview of the whole European LT landscape. The ELG metadata schema builds upon, consolidates and updates the METASHARE schema (Gavrilidou et al., 2012; Piperidis et al., 2018; Labropoulou et al., 2018), taking into account ELG’s require ments, recent developments in the metadata do main (e. g., FAIR11 ), and the need for creating a common pool of resources through exchange mechanisms with collaborating initiatives. The metadata schema caters for the descrip tion of the ELG core entities, i. e., Language Technologies (tools/services), including functional services and nonfunctional ones, and Data Lan guage Resources, comprising data sets (corpora), language descriptions (i. e., models) and lexical/ conceptual resources (e. g., gazetteers, ontologies, etc.). I"
2021.eacl-demos.26,piperidis-etal-2014-meta,1,0.869104,"text categori sation services. Further services are being added on a regular basis with 200+ additional IE and text analysis services, 21 MT, eight ASR and nine TTS scheduled to be included by the time of ELG Re lease 2 in February 2021. We aim to make it as simple as possible for LT providers to integrate their services, but in a Already now ELG provides access to more than 2700 language resources. We ingested substan tial resources from existing repositories, especially ELDA/ELRA, ELRCSHARE (Lösch et al., 2018; Piperidis et al., 2018; Smal et al., 2020) and META SHARE (Piperidis, 2012; Piperidis et al., 2014). We have also been working on ‘external’ reposito ries, about 220 of which have been identified so far. Some (e. g., Zenodo, Quantum Stat) are al ready being ingested together with two reposito ries related to ELG, LINDAT/CLARIAHCZ and ELRASHARELRs (LRs published at LREC). 2.6 Access Methods and User Interfaces Our main groups of users are: (1) LT/LR providers – companies or research organisations with tools, services or data that can be provided through the ELG; (2) Developers and integrators – companies and research institutions interested in using LT; (3) General LT information seeke"
2021.eacl-demos.26,L16-1251,1,0.869271,"Missing"
2021.eacl-demos.26,L18-1519,1,0.849444,"et al., 2019; Rehm et al., 2020d). We describe Release 2 of the European Lan guage Grid (ELG) cloud platform.1 This scal able system is targeted to evolve into the primary 1 https://www.europeanlanguagegrid.eu. We provide a screencast demo video at https://youtu.be/LD6QadkkZiM. platform for LT in Europe. It will provide one umbrella platform for all LTs developed by the European LT landscape, including research and industry, addressing a gap that has been repeat edly raised by the European LT community for many years (Rehm and Uszkoreit, 2013; Rehm et al., 2016b; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018). ELG is meant to be a virtual home and marketplace for all products, services and organisations active in the LT space in Europe (Rehm et al., 2020a). The platform can be used by all stakeholders to show case, share and distribute their products, services, tools and resources. At the end of the EU project ELG (20192022), which will establish a legal en tity in early 2022, the platform will provide access to approx. 1300 commercial and noncommercial tools and services for all European languages, as well as thousands of language resources (LRs). ELG will enable t"
2021.eacl-demos.26,2020.lrec-1.422,0,0.0498058,"Missing"
aker-etal-2017-simple,P12-1091,0,\N,Missing
aker-etal-2017-simple,D14-1082,0,\N,Missing
aker-etal-2017-simple,P14-1023,0,\N,Missing
aker-etal-2017-simple,D13-1170,0,\N,Missing
aker-etal-2017-simple,S16-1003,0,\N,Missing
aker-etal-2017-simple,P16-2064,1,\N,Missing
aker-etal-2017-simple,S17-2087,0,\N,Missing
aker-etal-2017-simple,S17-2085,0,\N,Missing
aker-etal-2017-simple,S17-2080,0,\N,Missing
aker-etal-2017-simple,S17-2081,0,\N,Missing
aker-etal-2017-simple,D11-1147,0,\N,Missing
bontcheva-2004-open,ide-romary-2002-standards,0,\N,Missing
C16-1111,R13-1011,1,0.323241,"ial media, creating topic shifts of both greater magnitude and higher frequency than other text types. We focus on Twitter, using this as the “model organism” of social media text (Tufekci, 2014), to assemble a corpus that is capable of catching these variances. 2.1 Annotation Scheme The BTC corpus is divided into segments, where the documents within each segment share a common theme (see Table 2). The documents consist of the social media message – i.e. tweet – complete with its JSON metadata. The text of the tweet is then annotated with into sentences and tokens, using the TwitIE tokenizer (Bontcheva et al., 2013). Tokenisation presents some issues in tweets. Classic schemas like PTB do not work well with constructs like smilies or URLs. To address this, we use the TwitIE tokeniser (Bontcheva et al., 2013) which is roughly based on TweetMotif and the twokeniser tool (O’Connor et al., 2010). Of note, we separate the preceding symbol in mentions and hashtags (the @ or # characters) as a distinct token, but still include this in entity spans. The main question was which entity classes should be covered in the corpus. Some Twitter corpora have used ten top-level Freebase categories (Bollacker et al., 2008;"
C16-1111,E14-2025,1,0.661946,"le 1). For expert-based annotation methodologies Hovy (2010) recommend at most ten, ideally seven, categories. In crowdsourcing, successful tasks tend to present even fewer choices – in most cases between two and five categories (Sabou et al., 2014). Therefore, we chose to focus on the three most widely used entity categories: Person, Location, and Organization (Table 1). Apart from being well understood by annotators, these three categories offer straightforward mapping to the other existing Twitter datasets, so all could be used in combination, as training data, if needed. An initial pilot (Bontcheva et al., 2014a) also included a fourth class, “Product”, but crowd workers struggled to annotate these correctly and with good recall, so they were dropped. For polysemous entities, our guidelines instructed annotators to assign the entity class that corresponds to the correct entity class in the given context. For example, in “We’re driving to Manchester”, Manchester is a location, but in “Manchester are in the final tonight”, it is a sports club – an organization. Special attention is given to username mentions. Where other corpora have blocked these out (Rowe et al., 2013) or classified them universally"
C16-1111,W10-0701,0,0.0426189,"luded. Segment H is the most varied. To balance the UK bias of segment B, this segment excludes tweets of UK origin (according to the Twitter metadata). The segment is stratified for month of year, time of day, and day of week, giving an even spread over many temporal cycle types in the collection period. 3 Annotation Process To make annotation scalable and of high quality, while ensuring sufficient annotator variety, corpus annotation was carried out using a mix of NLP experts and paid-for crowdsourcing. The annotation process follows general best practices in crowdsourced corpus annotation (Callison-Burch and Dredze, 2010; Alonso and Lease, 2011; Sabou et al., 2014). For example, task design is kept clean (a critical factor, more important than e.g. interface language – (Khanna et al., 2010)), and the process developed over pilot and refinement iterations. Tasks were built in GATE and jobs automatically managed through through Crowdflower (Bontcheva et al., 2014b). First segments were entirely annotated and adjudicated by experts as calibration. To maximize annotator focus, images attached to tweets or featuring in content (e.g. news stories) linked to from tweets are shown alongside the task, for worker primi"
C16-1111,W14-3207,0,0.0326691,"esses, governments, and communities increasingly need real-time information from dynamic, large-volume media data streams, such as blogs, Facebook, and Twitter. In particular, the automatic detection of mentions of people, organizations, locations, and other entities (i.e. Named Entity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter context; and lack of sufficiently large NE annotated social media datasets. In more detail, T"
C16-1111,N13-1037,0,0.0669534,"peaking world. Finally, it is partially socially segmented, including reactions to news stories, non-professional content, and text from the “twitterati”. The corpus is made freely available in various formats; the source text is included under Twitter’s revised 2015 licensing guidelines, as are the intermediate annotations. 2 Corpus Construction The goal of the corpus is to provide a representative example of named entities in social media. Social media is said to contain more variance than some other text types, like newswire. It certainly is authored by a broader demographic than newswire (Eisenstein, 2013), and contains a variety of styles and formality registers, unlike other user-generated content such as Youtube comments or SMS (Hu et al., 2013). Changes in general discourse subject are also said to present more quickly in social media, creating topic shifts of both greater magnitude and higher frequency than other text types. We focus on Twitter, using this as the “model organism” of social media text (Tufekci, 2014), to assemble a corpus that is capable of catching these variances. 2.1 Annotation Scheme The BTC corpus is divided into segments, where the documents within each segment share"
C16-1111,W10-0713,0,0.0210569,"Missing"
C16-1111,fromreide-etal-2014-crowdsourcing,0,0.0745323,"Missing"
C16-1111,N13-1132,0,0.0179668,"origin tweet had better recall on these entities. Conversely, annotators working on documents from other countries had lower recall. As matched geographic contexts tend to produce better results, during annotation, the geographically stratified parts of corpora were issued only to crowd workers in the same region, in order to maximize recall and local knowledge. 3.2 Adjudication Adjudication is an important step in refining annotator data. In the case of crowdsourced annotations, further economies of scale can be afforded by automating adjudication; tools already exist for this, such as MACE (Hovy et al., 2013). However, auto-adjudication is poorly equipped to handle exceptional circumstances; further, it is hard to judge its impact without human intervention. The construction of the BTC involved a combination of automatic and human adjudication. Primarily, we found there were problems with recall. Often, only a single crowd worker or expert would annotate a given (correct) entity. Under traditional agreement-based measures, this singleton annotation would be in the minority, and so likely removed in a typical adjudication step. Given our and others’ experiences with annotator recall and diversity ("
C16-1111,P15-1155,0,0.0676664,"sed openly, including source text and intermediate annotations. 1 Introduction Businesses, governments, and communities increasingly need real-time information from dynamic, large-volume media data streams, such as blogs, Facebook, and Twitter. In particular, the automatic detection of mentions of people, organizations, locations, and other entities (i.e. Named Entity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter cont"
C16-1111,P11-1037,0,0.0289405,"ntity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter context; and lack of sufficiently large NE annotated social media datasets. In more detail, Table 1 shows that there are less than 90 thousand tokens of publicly available NEannotated tweet datasets, and even those have shortcomings in terms of annotation methodology (e.g. singly annotated), low inter-annotator agreement, and stripping of important entity-bearing"
C16-1111,P12-3005,0,0.0172216,"inds of Twitter users. For instance, verbal communication behaviors such as g-dropping are often copied into typed social media messages (Eisenstein et al., 2010). To try to capture these, the corpus collects data from different segments, explicitly taking in content from well-known public figures, news outlets, wellknown social media figures, plus a large volume of randomly-selected posts. 2.3 Corpus Segmentation The dataset is organized into multiple segments (Table 2) to reflect the diversity criteria and annotation approach (expert vs. crowd). English tweets were filtered using langid.py (Lui and Baldwin, 2012). Segment A comprises a random sample of UK tweets, collected after New Year, annotated by multiple NLP experts. This data was used for calibration, so includes both expert input and crowd correction.4 Segment B is similar to segment A. In this segment we focused on non-directed tweets – i.e. those that are not private replies and so do not begin with a username mention. These were found to be more likely to contain a named entity based on sampling the Ritter et al. (2011) corpus. Segment E is a small sample focused on a specific event, the crash of flight MH17 over Ukraine. It contains commen"
C16-1111,I11-1108,0,0.0650068,"source text and intermediate annotations. 1 Introduction Businesses, governments, and communities increasingly need real-time information from dynamic, large-volume media data streams, such as blogs, Facebook, and Twitter. In particular, the automatic detection of mentions of people, organizations, locations, and other entities (i.e. Named Entity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter context; and lack of suffi"
C16-1111,D11-1141,0,0.0829466,"mentions of people, organizations, locations, and other entities (i.e. Named Entity Recognition) is a key step in numerous social media analysis applications, e.g. competitor and brand monitoring (Mostafa, 2013), debate and election analysis (Mascaro and Goggins, 2012; Tumasjan et al., 2010), disaster response (Kedzie et al., 2015; Neubig et al., 2011), and health- and well-being applications (Coppersmith et al., 2014; Choudhury et al., 2013). NER methods (typically trained on longer texts, such as news articles), have been shown to perform poorly on shorter and noisier social media content (Ritter et al., 2011). Therefore, recent Twitter NER work (Ritter et al., 2011; Liu et al., 2011; Derczynski et al., 2015) has focused on improving the state-ofthe-art, through new methods. The challenges come from named entities (NEs) typically being out-ofvocabulary (OOV) as compared to the training newswire data; the shorter context; and lack of sufficiently large NE annotated social media datasets. In more detail, Table 1 shows that there are less than 90 thousand tokens of publicly available NEannotated tweet datasets, and even those have shortcomings in terms of annotation methodology (e.g. singly annotated)"
C16-1111,rose-etal-2002-reuters,0,0.0453131,"picture of globally common entities to make inter-regional comparisons meaningful. The side effect a disproportionate novel part. 5 Entities in Social Media Having examined diversity in the underlying text, we next analyze characteristics of entities. We qualitatively examine surface forms, and compare entity distribution in social media to that in newswire. 5.1 Common Surface Forms Table 12 presents the most frequent surface forms in our corpus and also in the CoNLL’03 NER annotated data. The latter comes from news, based on the RCV1 corpus, which is largely US-based newswire from the 1990s (Rose et al., 2002) written by white working-age men (Eisenstein, 2013). Temporal concept drift (Masud et al., 2010) is evident here. For example, the most frequentlymentioned person entities have different surface forms, while referring to the same concept. The lexical representation of “the President of the US” has changed from Clinton to Obama. Similarly, the leader of Russia is present but with a different word; Yeltsin in the older newswire, Putin in modern social media. The top locations mentioned remain largely the same level and granularity, being countries that are major actors on the global scale or in"
C16-1111,sabou-etal-2014-corpus,1,0.491815,"ote, we separate the preceding symbol in mentions and hashtags (the @ or # characters) as a distinct token, but still include this in entity spans. The main question was which entity classes should be covered in the corpus. Some Twitter corpora have used ten top-level Freebase categories (Bollacker et al., 2008; Ritter et al., 2011)1 or have included products (see Table 1). For expert-based annotation methodologies Hovy (2010) recommend at most ten, ideally seven, categories. In crowdsourcing, successful tasks tend to present even fewer choices – in most cases between two and five categories (Sabou et al., 2014). Therefore, we chose to focus on the three most widely used entity categories: Person, Location, and Organization (Table 1). Apart from being well understood by annotators, these three categories offer straightforward mapping to the other existing Twitter datasets, so all could be used in combination, as training data, if needed. An initial pilot (Bontcheva et al., 2014a) also included a fourth class, “Product”, but crowd workers struggled to annotate these correctly and with good recall, so they were dropped. For polysemous entities, our guidelines instructed annotators to assign the entity"
C16-1111,P11-1137,0,\N,Missing
C16-1111,P10-5004,0,\N,Missing
C18-1284,aker-etal-2017-simple,1,0.861026,"1 As described in Section 2, prior work on tweet veracity classification investigated a wide range of features and classification methods. Most influential is the feature set proposed by Castillo et al. (2011), which we have adopted for training a classification model. We experimented with various classifiers such as simple decision trees, k-nn, etc. but achieved best performance with Random Forests, more precisely, we use a range of 33 different features varying from syntactical, semantic, indicator, user-specific and message-specific categories. Details of our features can be obtained from (Aker et al., 2017b). 5.2 Stance Aware Baseline: B2 Following the approach of Liu et al. (2015), we integrate stance as several features, additional to those used in the first baseline. Since there are four different stance classes, the following additional feature(s) are used: 3364 Relative-Stance-Score Percentage of supporting, denying, questioning, and commenting stances extracted from tweets within a rumour. To obtain this feature we count e.g. how many individual tweets express support for the rumour and divide it by the total number of tweets. We have in total four features, one of each class. Similar to"
C18-1284,S17-2082,0,0.50435,"in the stance distribution over time. Therefore, after training models for true and false rumours, we can build a binary veracity classifier by comparing sequence occurrence probabilities for the two cases. This paper investigates whether and to what extent rumour veracity classification can be predicted on the basis of crowd stance. While there is a body of work on automatic veracity classification such as (Castillo et al., 2011; Kwon et al., 2013; Vosoughi, 2015; Wu et al., 2015; Ma et al., 2015; Lukasik et al., 2016), only few have applied stance information as a feature (Liu et al., 2015; Enayet and El-Beltagy, 2017), and none of these studies investigated the collective power of the crowd as a source of stance information, which can then be used as a feature in rumour veracity classification. Here we argue that the content of the posts is not necessarily helpful towards determining the veracity of a story, as the content can be either misleading or inaccurate. We believe it is the aggregation of stances which can provide useful information to determine the veracity. Despite the fact that some of the users will be inevitably mistaken, and sharing the wrong stance, we argue that the aggregation of stances"
C18-1284,D15-1311,1,0.856867,"t of the rumour until a replying tweet occurred. Accordingly, observation alphabet of MSHMM λ0 is defined as a random vector o = (X, x), where X is a space index (i.e. a stance) and x ∈ R1 the processed timestamp. Multi spaced observation probability of o is defined as X b(o) = wg Ng (x) (8) g∈X Additionally, state set S, state transition matrix A and starting state probability vector π of system λ0 are defined analogue to system λ. 5 Evaluation Settings To evaluate the performance of the classification framework leave-one-event-out cross validation was performed, following the methodology of Lukasik et al. (2015). This means that we train on n − 1 events (on all rumours within these events) and test it on an unseen nth event (on all rumours within this event). We use F1 score to measure classifier performance and compare our results against two baselines. The first one does not make use of stance, whereas the second one integrates stance as one of a set of features. These two baselines have been selected to simulate the impact of collective stance i.e., that collective stance have positive impact on rich traditional feature engineered approaches. However, our results (see Section 6) show that it is im"
C18-1284,P16-2064,1,0.900532,"Missing"
C18-1284,D11-1147,0,0.668644,"Missing"
C18-1284,S17-2087,0,0.0357009,"The algorithm aimed at producing positive seeds to be shown to users first. 3361 Rumour veracity classification has also been studied in the RumourEval shared task at SemEval 2017 (Derczynski et al., 2017a). Subtask B consisted in determining if each of the rumours in the dataset were true, false or remained unverified. It considered two different settings, one closed where participants could not make use of external knowledge bases and another open where use of external resources was allowed. Participants viewed the task either as a three-way (Enayet and El-Beltagy, 2017; Wang et al., 2017; Singh et al., 2017) or two-way (Chen et al., 2017; Srivastava et al., 2017), single tweet classification task. The winning system (Enayet and El-Beltagy, 2017) added features more specific to the distribution of stance labels in the tweets replying to the source tweet (percentage of reply tweets classified as either support, deny or query). The survey paper of Zubiaga et al. (2018) provides an extensive summary of current work on rumour verification but also related task such as detection of rumours as well as stance classification of messages involved in rumours. In this work we propose to use stance only to ta"
C18-1284,S17-2085,0,0.0369485,"be shown to users first. 3361 Rumour veracity classification has also been studied in the RumourEval shared task at SemEval 2017 (Derczynski et al., 2017a). Subtask B consisted in determining if each of the rumours in the dataset were true, false or remained unverified. It considered two different settings, one closed where participants could not make use of external knowledge bases and another open where use of external resources was allowed. Participants viewed the task either as a three-way (Enayet and El-Beltagy, 2017; Wang et al., 2017; Singh et al., 2017) or two-way (Chen et al., 2017; Srivastava et al., 2017), single tweet classification task. The winning system (Enayet and El-Beltagy, 2017) added features more specific to the distribution of stance labels in the tweets replying to the source tweet (percentage of reply tweets classified as either support, deny or query). The survey paper of Zubiaga et al. (2018) provides an extensive summary of current work on rumour verification but also related task such as detection of rumours as well as stance classification of messages involved in rumours. In this work we propose to use stance only to tackle the rumour verification task. As highlighted earlie"
C18-1284,S17-2086,0,0.0415238,"m to block rumours. The algorithm aimed at producing positive seeds to be shown to users first. 3361 Rumour veracity classification has also been studied in the RumourEval shared task at SemEval 2017 (Derczynski et al., 2017a). Subtask B consisted in determining if each of the rumours in the dataset were true, false or remained unverified. It considered two different settings, one closed where participants could not make use of external knowledge bases and another open where use of external resources was allowed. Participants viewed the task either as a three-way (Enayet and El-Beltagy, 2017; Wang et al., 2017; Singh et al., 2017) or two-way (Chen et al., 2017; Srivastava et al., 2017), single tweet classification task. The winning system (Enayet and El-Beltagy, 2017) added features more specific to the distribution of stance labels in the tweets replying to the source tweet (percentage of reply tweets classified as either support, deny or query). The survey paper of Zubiaga et al. (2018) provides an extensive summary of current work on rumour verification but also related task such as detection of rumours as well as stance classification of messages involved in rumours. In this work we propose to"
C96-2177,C90-1011,0,0.0439818,"Missing"
C96-2177,1994.bcs-1.16,0,0.118723,"Missing"
C96-2177,C92-3146,0,\N,Missing
cunningham-etal-2000-software,C92-2123,0,\N,Missing
cunningham-etal-2000-software,bird-etal-2000-atlas,0,\N,Missing
cunningham-etal-2000-software,W99-0301,0,\N,Missing
cunningham-etal-2000-software,C96-1082,0,\N,Missing
cunningham-etal-2000-software,C94-1070,0,\N,Missing
cunningham-etal-2000-software,A97-1034,0,\N,Missing
cunningham-etal-2000-software,W94-0319,0,\N,Missing
cunningham-etal-2000-software,A97-1035,1,\N,Missing
cunningham-etal-2000-software,W98-1310,0,\N,Missing
cunningham-etal-2000-software,A97-1054,0,\N,Missing
cunningham-etal-2000-software,P98-2126,0,\N,Missing
cunningham-etal-2000-software,C98-2121,0,\N,Missing
cunningham-etal-2000-software,P98-1099,0,\N,Missing
cunningham-etal-2000-software,C98-1096,0,\N,Missing
cunningham-etal-2000-software,P98-1024,0,\N,Missing
cunningham-etal-2000-software,C98-1024,0,\N,Missing
cunningham-etal-2000-software,C98-2126,0,\N,Missing
cunningham-etal-2000-software,W97-1500,0,\N,Missing
cunningham-etal-2000-software,W98-1102,0,\N,Missing
cunningham-etal-2000-software,M93-1009,0,\N,Missing
cunningham-etal-2000-software,A97-2017,1,\N,Missing
D15-1028,P15-2085,1,0.900329,"#39 2h (b) rumour #60 Figure 1: Intensity functions and corresponding predicted arrival times for different methods across example Ferguson rumours. Arrival times predicted by LGCP are denoted by red pluses, LGCPTXT by blue dots, and ground truth by black crosses. Light regions denote uncertainty of predictions. mours around 2014 Ferguson unrest. It consists of conversational threads that have been manually labeled by annotators to correspond to rumours1 . Since some rumours have few posts, we consider only those with at least 15 posts in the first hour as they express interesting behaviour (Lukasik et al., 2015). This results in 114 rumours consisting of a total of 4098 tweets. inter-arrival times as independent and exponentially distributed with a constant rate parameter. A similar model is used by Sakaki et al. (2010) to monitor the tweets related to earthquakes. The renewal process model used by Esteban et al. (2012) assumes the inter-arrival times to be independent and identically distributed. Gonzalez et al. (2014) attempts to model arrival times of tweets using a Gaussian process but assumes the tweet arrivals to be independent every hour. These approaches do not take into account the varying c"
D15-1028,N13-1039,0,0.099375,"Missing"
D15-1311,W11-0705,0,0.0568982,"The first setting (denoted GP) considers only target rumour data for training. The second (GPPooled) additionally considers tweets from reference rumours (i.e. other than the target rumour). The third setting is GPICM, where an ICM kernel is used to weight influence from tweets from reference rumours. 6 Features We conducted a series of preprocessing steps in order to address data sparsity. All words were lowercased; stopwords removed; all emoticons were replaced with words2 ; and stemming was performed. In addition, multiple occurrences of a character were replaced with a double occurrence (Agarwal et al., 2011), to correct for misspellings and lengthenings, e.g., looool. All punctuation was also removed, except for ., ! and ?, which we hypothesize to be important for expressing emotion. Lastly, usernames were removed as they tend to be rumour-specific, i.e., very few users comment on more than one rumour. After preprocessing the text data, we use either the resulting bag of words (BOW) feature representation or replace all words with their Brown cluster ids (Brown), using 1000 clusters acquired from a large scale Twitter corpus (Owoputi et al., 2013). In all cases, simple re-tweets are removed from"
D15-1311,D14-1190,1,0.658627,"Missing"
D15-1311,P13-1004,1,0.421575,"discussed and the attitude expressed towards that. This information can be acquired either via manual annotation as part of expert analysis, as is the case with our dataset, or automatically, e.g. using pattern-based rumour detection (Zhao et al., 2015). Afterwards, our method can be used to classify the attitudes expressed in each new tweet from outside the training set. 5 Gaussian Processes for Classification Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods (Cohn and Specia, 2013; Lampos et al., 2014; Beck et al., 2014; Preotiuc-Pietro et al., 2015). We use Gaussian Processes as this probabilistic kernelised framework avoids the need for expensive crossvalidation for hyperparameter selection.1 1 There exist frequentist kernel methods, like SVMs, which additionally require extensive heldout parameter tuning. The central concept of Gaussian Process Classification (GPC; (Rasmussen and Williams, 2005)) is a latent function f over inputs x: f (x) ∼ GP(m(x), k(x, x0 )), where m is the mean function, assumed to be 0 and k is the kernel function, specifying the degree to whic"
D15-1311,E14-1043,1,0.828062,"tude expressed towards that. This information can be acquired either via manual annotation as part of expert analysis, as is the case with our dataset, or automatically, e.g. using pattern-based rumour detection (Zhao et al., 2015). Afterwards, our method can be used to classify the attitudes expressed in each new tweet from outside the training set. 5 Gaussian Processes for Classification Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods (Cohn and Specia, 2013; Lampos et al., 2014; Beck et al., 2014; Preotiuc-Pietro et al., 2015). We use Gaussian Processes as this probabilistic kernelised framework avoids the need for expensive crossvalidation for hyperparameter selection.1 1 There exist frequentist kernel methods, like SVMs, which additionally require extensive heldout parameter tuning. The central concept of Gaussian Process Classification (GPC; (Rasmussen and Williams, 2005)) is a latent function f over inputs x: f (x) ∼ GP(m(x), k(x, x0 )), where m is the mean function, assumed to be 0 and k is the kernel function, specifying the degree to which the outputs covary"
D15-1311,N13-1039,0,0.0153413,"character were replaced with a double occurrence (Agarwal et al., 2011), to correct for misspellings and lengthenings, e.g., looool. All punctuation was also removed, except for ., ! and ?, which we hypothesize to be important for expressing emotion. Lastly, usernames were removed as they tend to be rumour-specific, i.e., very few users comment on more than one rumour. After preprocessing the text data, we use either the resulting bag of words (BOW) feature representation or replace all words with their Brown cluster ids (Brown), using 1000 clusters acquired from a large scale Twitter corpus (Owoputi et al., 2013). In all cases, simple re-tweets are removed from the training set to prevent bias (Llewellyn et al., 2014). 2 We used the dictionary from: http://bit.ly/ 1rX1Hdk and extended it with: :o, : |, =/, :s, :S, :p. method acc Majority GPPooled Brown GPPooled BOW 0.68 0.72 0.69 Table 3: Accuracy taken across all rumours in the LOO setting. 7 Experiments and Discussion Table 3 shows the mean accuracy in the LOO scenario following the GPPooled method, which pools all reference rumours together ignoring their task identities. ICM can not use correlations to target rumour in this case and so can not be"
D15-1311,P15-1169,0,0.0454968,"ation can be acquired either via manual annotation as part of expert analysis, as is the case with our dataset, or automatically, e.g. using pattern-based rumour detection (Zhao et al., 2015). Afterwards, our method can be used to classify the attitudes expressed in each new tweet from outside the training set. 5 Gaussian Processes for Classification Gaussian Processes are a Bayesian non-parametric machine learning framework that has been shown to work well for a range of NLP problems, often beating other state-of-the-art methods (Cohn and Specia, 2013; Lampos et al., 2014; Beck et al., 2014; Preotiuc-Pietro et al., 2015). We use Gaussian Processes as this probabilistic kernelised framework avoids the need for expensive crossvalidation for hyperparameter selection.1 1 There exist frequentist kernel methods, like SVMs, which additionally require extensive heldout parameter tuning. The central concept of Gaussian Process Classification (GPC; (Rasmussen and Williams, 2005)) is a latent function f over inputs x: f (x) ∼ GP(m(x), k(x, x0 )), where m is the mean function, assumed to be 0 and k is the kernel function, specifying the degree to which the outputs covary as a function of the inputs. We use a linear kerne"
D15-1311,D11-1147,0,0.740962,"Missing"
D15-1311,llewellyn-etal-2014-using,0,0.0253452,"lengthenings, e.g., looool. All punctuation was also removed, except for ., ! and ?, which we hypothesize to be important for expressing emotion. Lastly, usernames were removed as they tend to be rumour-specific, i.e., very few users comment on more than one rumour. After preprocessing the text data, we use either the resulting bag of words (BOW) feature representation or replace all words with their Brown cluster ids (Brown), using 1000 clusters acquired from a large scale Twitter corpus (Owoputi et al., 2013). In all cases, simple re-tweets are removed from the training set to prevent bias (Llewellyn et al., 2014). 2 We used the dictionary from: http://bit.ly/ 1rX1Hdk and extended it with: :o, : |, =/, :s, :S, :p. method acc Majority GPPooled Brown GPPooled BOW 0.68 0.72 0.69 Table 3: Accuracy taken across all rumours in the LOO setting. 7 Experiments and Discussion Table 3 shows the mean accuracy in the LOO scenario following the GPPooled method, which pools all reference rumours together ignoring their task identities. ICM can not use correlations to target rumour in this case and so can not be used. The majority baseline simply assigns the most frequent class from the training set. We can observe th"
D15-1311,P15-2085,1,0.716506,"Missing"
D16-1084,W15-1516,0,0.0260991,"ork mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applie"
D16-1084,S16-1063,1,0.714724,"models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a"
D16-1084,D15-1075,0,0.0850398,"tweets on targets outperform all baselines on the test set. It is further worth noting that the Bag-of-WordVectors baseline achieves results comparable with TweetOnly, Concat and one of the conditional encoding models, TarCondTweet, on the dev set, even though it achieves significantly lower performance on the test set. This indicates that the pre-trained word embeddings on their own are already very useful for stance detection. This is consistent with findings of other works showing the usefulness of such a Bag-of-Word-Vectors baseline for the related tasks of recognising textual entailment Bowman et al. (2015) and sentiment analysis Eisner et al. (2016). Our best result in the test setup with BiCond is the second highest reported result on the Twitter Stance Detection corpus, however the first, third and fourth best approaches achieved their results by automatically labelling Donald Trump training data. BiCond for the unseen target setting outperforms the third and fourth best approaches by a large margin (5 and 7 points in Macro F1, respectively), as can be seen in Table 7. Results for weakly supervised stance detection are discussed in Section 6. Pre-Training Table 4 shows the effect of unsupervi"
D16-1084,S16-1061,0,0.0879263,"0.5078 TweetOnly FAVOR AGAINST Macro 0.5284 0.5774 0.6284 0.4615 0.5741 0.5130 0.5435 Concat FAVOR AGAINST Macro 0.5506 0.5794 0.5878 0.4883 0.5686 0.5299 0.5493 TarCondTweet FAVOR AGAINST Macro 0.5636 0.5947 0.6284 0.4515 0.5942 0.5133 0.5538 TweetCondTar FAVOR AGAINST Macro 0.5868 0.5915 0.6622 0.4649 0.6222 0.5206 0.5714 BiCond FAVOR AGAINST Macro 0.6268 0.6057 0.6014 0.4983 0.6138 0.5468 0.5803 Table 6: Stance Detection test results for weakly supervised setup, trained on automatically labelled pos+neg+neutral Trump data, and reported on the official test set. Marsh, 2016) and INF-UFRGS (Dias and Becker, 2016) considered a different experimental setup. They automatically annotated training data for the test target Donald Trump, thus converting the task into weakly supervised seen target stance detection. The pkudblab system uses a deep convolutional neural network that learns to make 2-way predictions on automatically labelled positive and negative training data for Donald Trump. The neutral class is predicted according to rules which are applied at test time. Since the best performing systems which participated in the shared task consider a weakly supervised setup, we further compare our proposed"
D16-1084,W16-6208,1,0.849006,"Missing"
D16-1084,N16-1138,1,0.817213,"tioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applied to the related task of recognising textual entailment (Rockt¨aschel et al., 2016), using a dataset of half a million training examples (Bowman et al., 2015) and numerous different hypotheses. Our experiments here show that conditional encoding is also successful on a relatively small training set and when applied to an unseen testing target. Moreover, we augment conditional encoding with bidirectional encoding and demonstrate"
D16-1084,I13-1191,0,0.15284,"es which also learn representations for the targets (BoWV, Concat). By training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results. The best result (F1 of 0.5803) is achieved with the bi-directional conditional encoding model (BiCond). This shows that 3 Method Note that “|” indiates “or”, ( ?) indicates optional space 883 in Dias and Becker (2016) such models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell"
D16-1084,D13-1171,0,0.0742182,"Missing"
D16-1084,S16-1003,0,0.327289,"Missing"
D16-1084,D11-1147,0,0.135482,"tection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applied to the related task of recognising textual entailment (Rockt¨aschel et al., 2016), using a dataset of half a million training examples (Bowman et al., 2015) and numerous different hypotheses. Our experiments her"
D16-1084,D13-1170,0,0.00386016,"la and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis (Pang and Lee, 2008; Socher et al., 2013). However, such an approach cannot capture the stance of a tweet with respect to a particular target, unless training data is available for each of the test targets. In such cases, we could learn that a tweet mentioning Donald Trump in a positive manner expresses a negative stance towards Hillary Clinton. Despite this limitation, we use two such baselines, one implemented with a Support Vector Machine (SVM) classifier and one with an LSTM network, in order to assess whether we are successful in incorporating the target in stance prediction. A naive approach to incorporate the target in stance"
D16-1084,N12-1072,0,0.115068,"representations for the targets (BoWV, Concat). By training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results. The best result (F1 of 0.5803) is achieved with the bi-directional conditional encoding model (BiCond). This shows that 3 Method Note that “|” indiates “or”, ( ?) indicates optional space 883 in Dias and Becker (2016) such models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et"
D16-1084,S16-1062,0,0.392335,"data is not provided. Systems need to classify the stance of each tweet as “positive” (FAVOR), “negative” (AGAINST) or “neutral” (NONE) towards the target. The official metric reported for the shared task is F1 macroaveraged over the classes FAVOR and AGAINST. Although the F1 of NONE is not considered, systems still need to predict it to avoid precision errors for the other two classes. Even though participants were not allowed to manually label data for the test target Donald Trump, they were allowed to label data automatically. The two best-performing systems submitted to Task B, pkudblab (Wei et al., 2016) and LitisMind (Zarrella and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis"
D16-1084,S16-1074,0,0.326839,"ed to classify the stance of each tweet as “positive” (FAVOR), “negative” (AGAINST) or “neutral” (NONE) towards the target. The official metric reported for the shared task is F1 macroaveraged over the classes FAVOR and AGAINST. Although the F1 of NONE is not considered, systems still need to predict it to avoid precision errors for the other two classes. Even though participants were not allowed to manually label data for the test target Donald Trump, they were allowed to label data automatically. The two best-performing systems submitted to Task B, pkudblab (Wei et al., 2016) and LitisMind (Zarrella and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis (Pang and Lee, 2008; Socher et al., 2013"
D16-1084,D15-1073,0,0.0469489,"Missing"
D19-3020,S17-2082,0,0.0594781,"Missing"
D19-3020,N16-1138,0,0.0326165,"limited number of production quality tools are currently available to support them in the individual steps of this process, with much of the technology still in research and experimental phases. In more detail, existing projects and tools are mostly focused on images/video forensics and verification (e.g. InVID (Teyssou et al., 2017), REVEAL3 ), crowdsourced verification (e.g. CheckDesk4 , Veri.ly5 ), fact-checking claims made by politicians (e.g. Politifact6 , FactCheck.org7 , FullFact8 ), citizen journalism (e.g. Citizen Desk), repositories of checked facts/rumours/websites (e.g. Emergent (Ferreira and Vlachos, 2016), • Hoaxy (Shao et al., 2016) is a recent open-source tool focused on visualising and searching over claims and fact checks. Such sophisticated visualisations are out of scope of our system, but relevant open-source visualisation tools, e.g. from Hoaxy, could be integrated in the future. • CrossCheck10 was a collaborative rumour checking project led by First Draft and Google News Lab, during the French elections. Its output was a useful dataset of false or unverified rumours. • Meedan’s Check4 is an open-source breaking news verification platform, which however does not support continuously up"
D19-3020,C18-1288,0,0.0162241,"the model using the available annotated rumours dataset prepared by the data pro11 Rumour Classification Model https://pytorch.org 117 Figure 1: Data flow diagram of the rumour classification service. denying, questioning and commenting. It is split into training, development and test set as in the RumourEval2017 challenge with 272, 25 and 28 rumours respectively. The dataset has a majority class baseline of 0.429. Evaluation is performed by comparing against several state-of-the-art approaches – NileTMRG (Enayet and El-Beltagy, 2017), Branch LSTM (Zubiaga et al., 2018b), Multi-task Learning (Kochkina et al., 2018), vanilla LSTM (without inner-attention), LSTM with soft attention (Bahdanau et al., 2014) and our Inner-Attention algorithm. The algorithms achieved F-1 scores of 0.539, 0.558, 0.491, 0.528, 0.496, 0.616 and accuracy of 0.571, 0.571, 0.5, 0.537, 0.5, and 0.607 respectively. We expect the results should improve as more data is added to the training set through the Rumour Annotation Service. Further analysis and discussion of the algorithm and its evaluation can be found in Aker et al. (2019). Figure 2: Network diagram of the rumour veracity model. 5 The Rumour Annotation Service part provides"
dalli-etal-2004-web,A97-2017,1,\N,Missing
dalli-etal-2004-web,dalli-2002-creation,1,\N,Missing
damljanovic-etal-2008-text,tablan-etal-2006-user,1,\N,Missing
damljanovic-etal-2008-text,W06-1414,0,\N,Missing
E03-2009,baker-etal-2002-emille,1,0.827736,"enabled graphical user interface (GUI) needs to address two main issues: the capability to display text and the ability to enter text in other languages than the default one. It also provides a means of entering text in a variety of languages and scripts, using virtual keyboards where the language is not supported by the underlying operating platform (Java itself does not support input in many languages covered by Unicode, although it supports Unicode representation). Figure 1 depicts text in various scripts displayed in GATE. The facilities have been developed as part of the EMILLE project (Baker et al., 2002), designed to construct a 63 million word corpus of South Asian languages. There are currently 28 languages supported in GATE, and more are planned for the future. Since GATE is an open architecture, new virtual keyboards can easily be defined by users and added to the system. Apart from the input methods, GUK also provides a simple Unicode-aware text editor which is important because not all platforms provide one by default or the users may not know which one of the already installed editors is Unicode-aware. Besides providing text visualisation and editing facilities, the GUK editor also per"
E03-2009,W02-2025,0,0.0384946,"Missing"
E03-2009,pastra-etal-2002-feasible,1,0.699012,"e development and testing of GATE in a cross-platform environment, while the ability to handle Unicode enables applications developed 220 within GATE to be easily ported to new languages. 4 The future isn't English Robust tools for multilingual information extraction are becoming increasingly sought after now that we have capabilities for processing texts in different languages and scripts. While the default IE system is English-specific, some of the modules can be reused directly (e.g. the Unicode-based tokeniser can handle IndoEuropean languages), and/or easily customised for new languages (Pastra et al., 2002). So far, ANNIE has been adapted to do IE in Bulgarian, Romanian, Bengali, Greek, Spanish, Swedish, German, Italian, and French, and we are currently porting it to Arabic, Chinese and Russian, as part of the MUSE project 2 . IMMFis AnnotatiooidA Din surnarui Nr.336/1111110.2002 • Gnidul calatoriilorfa4a vita in ball Bilandul activitadii poobor buzodani in anul 111.(partea b•Polidistii au posibOtatea legislativa de aid indeplini mai blue sarcinileii misnimle cc le revin• A incep-ut cnn nou an de lupta cu infractorii: Printre primii la calatorli fara vita, 5eribedn Nadine din a font prinsa pa ae"
E03-2013,W03-2805,1,0.791342,"entences to a set of &quot;correct&quot; extracted sentences, then co-selection is measured by precision, recall and F-score. Gate&apos;s AnnotationDiff tool enables two sets of annotations on a document to be quantitative compared (i,e. two summaries produced by two summarisation configurations). We are making use of human annotated corpus (source documents and sets of extracts) (Saggion et al., 2002b) in order to evaluate different system configurations and to identify experimentally the best feature combination. Processing resources for content-based evaluation have already been integrated in the system (Pastra and Saggion, 2003). Future work will include the use of document-summary (non extractive) pairs (from the Document Understanding Conferences Corpus as well as from the HKNews Corpus (Saggion et al., 2002a)) and machine learning algorithms to obtain the best combination of the summarisation features, where &apos;extracts&apos; will be learn based on the automatic alignment between the non-extractive summaries and their source documents. The summarisation 238 system presented here provides a framework for experimentation in text summarisation research. The summariser combines two orthogonal approaches in a simple way takin"
E03-2013,J02-4005,1,0.748078,"summarisation addressing the need for user adaptation. 1 Introduction Two approaches are generally considered in automatic text summarisation research: the shallow sentence extraction approach and the deep, understand and generate approach (Mani, 2000). Sentence extraction methods are quite robust, but sentence extracts suffer from lack of cohesion and coherence. Methods that identify the essential information of the document by either information extraction or text understanding and that use the key information to produce a new text, lead to high-quality summarisation (Paice and Jones, 1993; Saggion and Lapalme, 2002) but suffer from the knowledge-bottleneck problem: adapting information extraction rules, templates, and generation grammars to new tasks or domains is time consuming. An alternative to these approaches is to use combination of robust techniques for semantic tagging together with statistical methods (Saggion, 2002). Here, we present a summarisation system that makes use of robust components for semantic tagging and coreference resolution provided by GATE (Cunningham et al., 2002). Our system combines GATE components with well established statistical techniques developed for the purpose of text"
E03-2013,C02-1073,1,0.68697,"Missing"
E03-2013,saggion-etal-2002-developing,1,0.791192,"rch projects make use of in-house evaluation, making it difficult to replicate experiments, to compare results, or to use evaluation data for training purposes. When text summarisation systems are evaluated by comparing extracted sentences to a set of &quot;correct&quot; extracted sentences, then co-selection is measured by precision, recall and F-score. Gate&apos;s AnnotationDiff tool enables two sets of annotations on a document to be quantitative compared (i,e. two summaries produced by two summarisation configurations). We are making use of human annotated corpus (source documents and sets of extracts) (Saggion et al., 2002b) in order to evaluate different system configurations and to identify experimentally the best feature combination. Processing resources for content-based evaluation have already been integrated in the system (Pastra and Saggion, 2003). Future work will include the use of document-summary (non extractive) pairs (from the Document Understanding Conferences Corpus as well as from the HKNews Corpus (Saggion et al., 2002a)) and machine learning algorithms to obtain the best combination of the summarisation features, where &apos;extracts&apos; will be learn based on the automatic alignment between the non-e"
E14-2025,W10-0701,0,0.0146216,"t of this section discusses in more detail where reusable components and infrastructural support for automatic data mapping and user interface generation are necessary, in order to reduce the overhead of crowdsourcing NLP corpora. Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not made available, despite their important role in result quality (Khanna et al., 2010). A big outstanding challenge for crowdsourcing projects is that the cost to define a single 2.1 Project Definition An important part of project definition is the mapping of the NLP problem into one or more crowdsourcing tasks, which are sufficiently"
E14-2025,P02-1022,1,0.483231,"ists of three kinds of tasks: task workflow and management, contributor management (including profiling and retention), and quality control. Paid-for marketplaces like Amazon Mechanical Turk and CrowdFlower already provide this support. As with conventional corpus annotation, quality control is particularly challenging, and additional NLP-specific infrastructural support can help. 2.4 3.2 Data Evaluation and Aggregation The plugin expects documents to be presegmented into paragraphs, sentences and word tokens, using a tokeniser, POS tagger, and sentence splitter – e.g. those built in to GATE (Cunningham et al., 2002). The GATE Crowdsourcing plugin allows choice between these of which to use as the crowdsourcing task unit; e.g., to show one sentence per unit or one paragraph. In the demonstration we will show both automatic mapping at sentence level (for named entity annotation) and at paragraph level (for named entity disambiguation). In this phase, additional NLP-specific, infrastructural support is needed for evaluating and aggregating the multiple contributor inputs into a complete linguistic resource, and in assessing the resulting overall quality. Next we demonstrate how these challenges have been ad"
E14-2025,J11-2010,0,0.00831923,"re detail where reusable components and infrastructural support for automatic data mapping and user interface generation are necessary, in order to reduce the overhead of crowdsourcing NLP corpora. Introduction Annotation science (Hovy, 2010; Stede and Huang, 2012) and general purpose corpus annotation tools (e.g. Bontcheva et al. (2013)) have evolved in response to the need for creating highquality NLP corpora. Crowdsourcing is a popular collaborative approach that has been applied to acquiring annotated corpora and a wide range of other linguistic resources (Callison-Burch and Dredze, 2010; Fort et al., 2011; Wang et al., 2012). Although the use of this approach is intensifying, especially paid-for crowdsourcing, the reuse of annotation guidelines, task designs, and user interfaces between projects is still problematic, since these are generally not made available, despite their important role in result quality (Khanna et al., 2010). A big outstanding challenge for crowdsourcing projects is that the cost to define a single 2.1 Project Definition An important part of project definition is the mapping of the NLP problem into one or more crowdsourcing tasks, which are sufficiently simple to be carri"
E14-2025,ide-etal-2000-xces,0,0.0552745,"ion This stage, in particular, can benefit significantly from infrastructural support and reusable components, in order to collect the data (e.g. crawl the web, download samples from Twitter), preprocess it with linguistic tools (e.g. tokenisation, POS tagging, entity recognition), and then map automatically from documents and sentences to crowdsourcing micro-tasks. 2.3 Figure 1: Classification UI Configuration et al., 2002), which was chosen for its support for overlapping annotations and the wide range of automatic pre-processing tools available. GATE also has support for the XCES standard (Ide et al., 2000) and others (e.g. CoNLL) if preferred. Annotations are grouped in separate annotation sets: one for the automatically pre-annotated annotations, one for the crowdsourced judgements, and a consensus set, which can be considered as the final resulting corpus annotation layer. In this way, provenance is fully tracked, which makes it possible to experiment with methods that consider more than one answer as potentially correct. Running the Crowdsourcing Project This is the main phase of each crowdsourcing project. It consists of three kinds of tasks: task workflow and management, contributor manage"
E14-2025,P10-5004,0,\N,Missing
E14-4014,I13-1041,0,0.0369275,"Missing"
E14-4014,P02-1022,1,0.66554,"Missing"
E14-4014,P11-1037,0,0.0243396,"this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder tokens in MSM data (i.e. MENTION , HASHTAG , URL ) are replaced with @Mention, #hashtag, and http://url/, respectively, to give case and character n-grams more similar to the original values. The total corpus has 4 285 tweets, around a third the size of that in Liu et al. (2011). This dataset contains 86 352 tokens with 1 741 entity mentions. Person entity recognition was chosen as it is a challenging entity type. Names of persons popular on Twitter change more frequently than e.g. locations. Person names also tend to have a long tail, not being confined to just public figures. Lastly, although all three corpora cover different entity types, they all have Person annotations. 3.2 Figure 1: Training curve for lem. Diagonal cross (blue) is CRF/PA, vertical cross (red) SVM/UM. lem: with added lemmas, lower-case versions of tokens, word shape, and neighbouring lemmas (in"
E14-4014,N13-1037,0,0.07521,"Missing"
E14-4014,W10-0713,0,0.0556158,"Computational Linguistics, pages 69–73, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics LDA and vocabularies: Ritter et al. (2011)’s T-NER system uses 2,400 labelled tweets, unlabelled data and Linked Data vocabularies (Freebase), as well as co-training. These techniques helped but did not bring person recognition accuracy above the supervised MaxEnt baseline in their experiments. We use this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder tokens in MSM data (i.e. MENTION , HASHTAG , URL ) are replaced with @Mention, #hashtag, and http://url/, respectively, to give case and character n-grams more similar to the original values. The total corpus has 4 285 tweets, around a third the size of that in Liu et al. (2011). This dataset contains 86 352 tokens with 1 741 entity mentions. Person entity recognition was chosen as it is a challenging entity type. Names of persons popular on Twitt"
E14-4014,W09-1119,0,0.372934,"Missing"
E14-4014,D11-1141,0,0.196657,"trates that entity recognition using noise-resistant sequence labeling outperforms state-of-the-art Twitter NER, although we find that recall is consistently lower than precision. Secondly, to remedy this, we introduce a method for automatically post-editing the resulting entity annotations by using a discriminative classifier. This improves recall and precision. 69 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 69–73, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics LDA and vocabularies: Ritter et al. (2011)’s T-NER system uses 2,400 labelled tweets, unlabelled data and Linked Data vocabularies (Freebase), as well as co-training. These techniques helped but did not bring person recognition accuracy above the supervised MaxEnt baseline in their experiments. We use this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder t"
E14-4014,P05-1045,0,0.0344035,"Missing"
E14-4014,W03-0419,0,0.154486,"Missing"
funk-bontcheva-2010-ontology,W02-1011,0,\N,Missing
guthrie-etal-2004-large,H93-1051,0,\N,Missing
guthrie-etal-2004-large,A97-1035,1,\N,Missing
guthrie-etal-2004-large,J01-3001,0,\N,Missing
guthrie-etal-2004-large,C96-1079,0,\N,Missing
I05-3023,W03-0422,0,0.0415066,"Missing"
I05-3023,W03-1721,0,0.0312292,"ne particular context should have the same segmentation. 2 Character Based Chinese Word Segmentation We adopted the character based methodology for Chinese word segmentation, in which every character in a sentence was checked one by one to see if it was a word on its own or it was beginning, middle, or end character of a multi-character word. In contrast, another commonly used strategy, the word based methodology segments a Chinese sentence into the words in a pre-defined word list possibly with probability information about each word, according to some maximum probability criteria ( see e.g. Chen (2003)). The performance of word based segmentation is dependent upon the quality of word list used, while the character based method does not need any word list – it segments a sentence only based on the characters in the sentence. Using character based methodology, we transform the word segmentation problem into four binary classification problems, corresponding to single-character word, the beginning, middle and end character of multi-character word, respectively. For each of the four classes a classifier was learnt from training set using the one vs. all others paradigm, in which every character"
I05-3023,W05-0610,1,0.837393,"ing example a value (before thresholding) larger than a predefined parameter (margin). The margin Perceptron has better generalisation capability than the standard Perceptron. Li et al. (2002) proposed the Perceptron algorithm with uneven margins (PAUM) by introducing two margin parameters τ+ and τ− into the update rules for the positive and negative examples, respectively. Two margin parameters allow the PAUM to handle imbalanced datasets better than both the standard Perceptron and the margin Perceptron. PAUM has been successfully used for document classification and information extraction (Li et al., 2005). We used the PAUM algorithm to train a classifier for each of four classes for Chinese word segmentation. For one test example, the output of the Perceptron classifier before thresholding was used for comparison among the four classifiers. The important parameters of the learning algorithm are the uneven margins parameters τ + and τ− . In all our experiments τ+ = 20 and τ− = 1 were used. Table 1 presents the results for each of the four classification problems, obtained from 4-fold cross-validation on training set. Not surprisingly, the classification for middle character of multicharacter wo"
I05-3023,W03-1728,0,0.0930721,"the four classes a classifier was learnt from training set using the one vs. all others paradigm, in which every character in the training data belonging to the class considered was regarded as positive example and all other characters were negative examples. After learning, we applied the four classifiers to each character in test text and assigned the character the class which classifier had the maximal output among the four. This kind of strategy has been widely used in the applications of machine learning to named entity recognition and has also 154 been used in Chinese word segmentation (Xue and Shen, 2003). Finally a word delimiter (often a blank space, depending on particular corpus) was added to the right of one character if it was not the last character of a sentence and it was predicted as end character of word or as a single character word. 3 Learning Algorithm Perceptron is a simple and effective learning algorithm. For a binary classification problem, it checks the training examples one by one by predicting their labels. If the prediction is correct, the example is passed; otherwise, the example is used to correct the model. The algorithm stops when the model classifies all training exam"
L16-1182,E14-2025,1,0.82263,"pages2 . These efforts have motivated the discussion in the rest of this paper. 2. Creation of a manually annotated sentiment corpus: Earth Hour 2015 Earth Hour is an annual global event where people switch off their lights for one hour to show they care about the 1 2 http://www.decarbonet.eu/ https://gate.ac.uk/projects/decarbonet/ future of the planet. We created a twitter corpus by downloading all tweets in English about Earth Hour 2015, and selecting at random 600 of them. This corpus was then annotated manually for sentiment, and is publicly available. Using GATE’s crowdsourcing plugin (Bontcheva et al., 2014), we assigned the dataset to 16 annotators, such that each tweet was triple-annotated. The crowdsourcing plugin offers infrastructural support for mapping documents to crowdsourcing units in CrowdFlower and back, as well as automatically generating reusable crowdsourcing interfaces for NLP classification and selection tasks. Essentially, it provides a workflow enabling users to pre-annotate documents with linguistic units, export the documents to CrowdFlower and set up the task, and then import the resulting annotated documents back into GATE if needed, where manual or automatic adjudication c"
L16-1182,S13-2085,0,0.0306476,"us, it did not affect the results too badly. One can argue convincingly that this is a valid approach to take if the real life datasets for which the tool will be used also exhibit the same skewed nature, but it may account for differences in performance levels on other corpora. 3. Problems with existing annotated corpora Using existing annotated corpora for an evaluation is not always straightforward, because different real life tasks involving the same corpus may require different solutions, and the designers of the task often have different ideas about what constitutes a correct solution. (Reckman et al., 2013) describe the difficulties understanding the development dataset used for the Sem-Eval 2013 Task 2, where target terms must be annotated with positive or negative polarity independently from the sentiment of the sentence as a whole. They found it (unsurprisingly) unusual to label words such as like as positive when they occurred as part of longer negative phrases such as I didn’t like. Depending on how a system treats negative expressions, this may be tricky to break down into smaller segments, e.g. if a system uses phrases pre-annotated with sentiment, rather than individual words. Second, th"
L16-1729,S16-1063,1,0.862706,"Missing"
L16-1729,bentivogli-etal-2010-building,0,0.0187668,"ortable across languages and domains, but 11 To assess the PHEME RTE pilot dataset, we use the MaxEnt classifier-based model (Wang and Neumann, 2007) distributed with the Excitement Open Platform (EOP, Pado et train http://hltfbk.github.io/Excitement-Open-Platform/ generated by the sklearn metric classification report, see http://scikit-learn.org 13 http://nlp.stanford.edu/RTE3-pilot/RTE3 dev 3class.xml 4604 12 requires event and claim annotations. The manual effort spent to create such annotations is feasible to replace by automatic means which are currently being implemented in the project. Bentivogli et al. (2010) stress the importance of creating specialized data sets for RTE, in order to facilitate more targeted assessment and decomposition of the RTE task’s complexity. In our resource, the text snippets that form a RTE pair deliberately keep reoccurring across all three judgement labels in systematically varied pairings, allowing to investigate, model and evaluate linguistic and extralinguistic phenomena that underly semantic inference in the misinformation detection scenario. Previous RTE research has mainly focused on achieving good performance on the Entailment relation, whereas our method is mot"
L16-1729,D15-1075,0,0.0337061,"neither of those. A bottleneck for this task is obtaining training data. The creation of natural language data annotated for inference phenomena is so far a nontrivial and largely manual procedure, yielding expensive resources that are nonetheless problematically portable to new text genres and application domains. Existing initiatives have often created RTE data by syntactic and lexical transformations with predictable effects asking annotators to (re)write sentences taken from gold standards for other tasks such as question answering (Bar-Haim et al., 2006) and image and video description (Bowman et al., 2015; Marelli et al., 2014). RTE tasks may involve 2-way or 3-way inference judgements. In case of a 2-way judgement, the class to guess is either Entailment or Nonentailment. On the 3-way judgement scheme the Nonentailment class is further differentiated into Contradiction and Unknown. The presence of contradictory statements in social media can be indicative 1 for mis-/disinformation, controversy or speculation, which are important triggers in veracity checking procedures. Our present contribution therefore addresses the transformation of a project-internal corpus of annotated microblog texts in"
L16-1729,P08-1118,0,0.593236,"Missing"
L16-1729,N16-1138,0,0.160666,"Missing"
L16-1729,marelli-etal-2014-sick,0,0.035106,"bottleneck for this task is obtaining training data. The creation of natural language data annotated for inference phenomena is so far a nontrivial and largely manual procedure, yielding expensive resources that are nonetheless problematically portable to new text genres and application domains. Existing initiatives have often created RTE data by syntactic and lexical transformations with predictable effects asking annotators to (re)write sentences taken from gold standards for other tasks such as question answering (Bar-Haim et al., 2006) and image and video description (Bowman et al., 2015; Marelli et al., 2014). RTE tasks may involve 2-way or 3-way inference judgements. In case of a 2-way judgement, the class to guess is either Entailment or Nonentailment. On the 3-way judgement scheme the Nonentailment class is further differentiated into Contradiction and Unknown. The presence of contradictory statements in social media can be indicative 1 for mis-/disinformation, controversy or speculation, which are important triggers in veracity checking procedures. Our present contribution therefore addresses the transformation of a project-internal corpus of annotated microblog texts into a 3-way RTE dataset."
L16-1729,S16-1003,0,0.0544845,"t is centered on contradictory claims present in the data, and is extended to the other two classes to a limited extent. The resulting pilot dataset is balanced across the three classes. In future work we will investigate and evaluate the relevance of our data and compilation approach with respect to the RTE-5 Entailment Search pilot task14 and the RTE-6 Entailment Summarisation task15 , in which RTE systems are required to find all sentences in a document or a set that entail a given Hypothesis. RTE and its resources also tend to be utilized in the recently emerging task of stance detection (Mohammad et al., 2016), i.e. classification of the standpoint of an expression such as ”Climate change is a real concern” towards a piece of (social media) text as either supportive, denying, or neutral (Augenstein et al., 2016; Ferreira and Vlachos, 2016). It remains to be evaluated if the approaches built for stance detection are reusable or need specific adaptation to our goal of RTE in social media verification. Our current efforts include further development of the reported approach and the curation of project-internal data in other languages, in order to release16 several monolingual RTE benchmark resources."
L16-1729,D11-1147,0,0.0721362,"hod for the special purpose of information verification, which draws on manually established rumourous claims reported during crisis events. From about 500 English tweets related to 70 unique claims we compile and evaluate 5.4k RTE pairs, while continue automatizing the workflow to generate similar-sized datasets in other languages. Keywords: textual entailment, social media, verification 1. Introduction In this paper, we report on building a special-purpose Recognizing Textual Entailment (RTE) dataset in the context of information verification in user-generated content (Mendoza et al., 2010; Qazvinian et al., 2011; Procter et al., 2013) for the PHEME project1 . The dataset is compiled based on naturally occurring contradiction in manually labeled claims in crisis events discussed on Twitter, and to our knowledge is the first resource for RTE in the social media and verification domain. The detection of semantic inference phenomena between natural language text snippets, such as contradiction, entailment, and stance, is targeted by a number of research communities. Its most focused interest group formalizes inference tasks in the generic framework of RTE2 . RTE is applied to benefit several Natural Lang"
L16-1729,W07-1401,0,\N,Missing
L16-1729,N16-1170,0,\N,Missing
maynard-etal-2004-automatic,A97-1028,0,\N,Missing
maynard-etal-2004-automatic,E99-1001,0,\N,Missing
maynard-etal-2004-automatic,W03-1301,0,\N,Missing
maynard-etal-2004-automatic,W03-1505,1,\N,Missing
P02-1022,bird-etal-2000-atlas,0,\N,Missing
P02-1022,X98-1004,0,\N,Missing
P02-1022,A97-1054,0,\N,Missing
P13-4004,W02-0302,0,0.0294913,"vices, so that users can quickly restrict which types of services they are after and then be shown only the relevant subset. At the time of writing, there are services of the following kinds: • Part-of-Speech-Taggers for English, German, Dutch, and Hungarian. • Chunking: the GATE NP and VP chunkers and the OpenNLP ones; • Parsing: currently the Stanford Parser 4 , but more are under integration; • Stemming in 15 languages, via the Snowball stemmer; • Named Entity Recognition: in English, German, French, Arabic, Dutch, Romanian, and Bulgarian; • Biomedical taggers: the PennBio5 and the AbGene (Tanabe and Wilbur, 2002) taggers; • Twitter-specific NLP: language detection, tokenisation, normalisation, POS tagging, and developer’s stand-alone computer to be deployed seamlessly on distributed hardware resources (the compute cloud) with the aim of processing large amounts of data in a timely fashion. This process needs to be resilient in the face of failures at the level of the cloud infrastructure, the network communication, errors in the processing pipeline and in the input data. The platform layer determines the optimal number of virtual machines for running a given NLP application, given the size of the docu"
P13-4004,P02-1022,1,0.911579,"nt and to create high quality training and evaluation datasets. It is common to use double or triple annotation, where several people perform the annotation task independently and we then measure their level of agreement (Inter-Annotator Agreement, or IAA) to quantify and control the quality of this data (Hovy, 2010). The AnnoMarket platform was therefore designed to offer full methodological support for all stages of the text analysis development lifecycle: 1. Create an initial prototype of the NLP pipeline, testing on a small document collection, using the desktop-based GATE user interface (Cunningham et al., 2002); 2. If required, collect a gold-standard corpus for evaluation and/or training, using the GATE Teamware collaborative corpus annotation service (Bontcheva et al., 2013), running in AnnoMarket; 3. Evaluate the performance of the automatic pipeline on the gold standard (either locally in the GATE development environment or on the cloud). Return to step 1 for further development and evaluation cycles, as needed. 4. Upload the large datasets and deploy the NLP pipeline on the AnnoMarket PaaS; 5. Run the large-scale NLP experiment and download the results as XML or a standard linguistic annotation"
P13-4004,P10-5004,0,\N,Missing
P13-4004,ide-romary-2002-standards,0,\N,Missing
P15-2085,D14-1190,1,0.88248,"Missing"
P15-2085,P13-1004,1,0.818371,"= exp (fi (t)) p fi (t)|EiO dfi . The predictive distribution over counts at a particular time interval of length w with a mid-point t∗ for rumour Ei is Poisson distributed with rate wλi (t∗ |EiO ). Multi-task learning and incorporating text In order to exploit similarities across rumours we propose a multi-task approach where each rumour represents a task. We consider two approaches. First, we employ a multiple output GP based on the Intrinsic Coregionalization Model (ICM) ´ (Alvarez et al., 2012). It is a method which has been successfully applied to a range of NLP tasks (Beck et al., 2014; Cohn and Specia, 2013). ICM parametrizes the kernel by a matrix representing similarities between pairs of tasks. We expect it to find correlations between rumours exhibiting similar temporal patterns. The kernel takes the form trained on the training set of the rumour. We select its intensity λ using maximum likelihood estimate, which equals to the mean frequency of posts in the training intervals. The second baseline is Gaussian Process (GP) used for predicting hashtag frequencies in Twitter by Preotiuc-Pietro and Cohn (2013). Authors considered various kernels in their experiments, most notably periodic kernels."
P15-2085,N13-1039,0,0.0351162,"mour lifespan, which we split into 20 evenly spaced intervals. This way, our dataset consists in total of 2280 intervals. We iterate over rumours using a form of folded cross-validation, where in each iteration we exclude some (but not all) time intervals for a single target rumour. The excluded time intervals form the test set: either by selecting half at random (interpolation); or by taking only the second half for testing (extrapolation). To ameliorate the problems of data sparsity, we replace words with their Brown cluster ids, using 1000 clusters acquired on a large scale Twitter corpus (Owoputi et al., 2013). The mean function for the underlying GP in LGCP methods is assumed to be 0, which results in intensity function to be around 1 in the absence of nearby observations. This prevents our method from predicting 0 counts in these regions. We add 1 to the counts in the intervals to deal with this problem as a preprocessing step. The original counts can be obtained by decrementing 1 from the predicted counts. Instead, one could use a GP with a non-zero mean function and learn the mean function, a more elegant way of approaching this problem, which we leave for future work. kTXT ((t, i), (t0 , i0 ))"
P15-2085,D13-1100,1,0.935962,"ng rumour popularity. There have been several descriptive studies of rumours in social media, e.g. Procter et al. (2013) analyzed rumours in tweets about the 2011 London riots and showed that they follow similar lifecycles. Friggeri et al. (2014) showed how Facebook constitutes a rich source of rumours and conversation threads on the topic. However, none of these studies tried to model rumour dynamics. The problem of modelling the temporal nature of social media explicitly has received little attention. The work most closely related modelled hash tag frequency time-series in Twitter using GP (Preotiuc-Pietro and Cohn, 2013). It made several simplifications, including discretising time and treating the problem of modelling counts as regression, which are both inappropriate. In contrast we take a more principled approach, using a point process. We use the proposed GP-based method as a baseline to demonstrate the benefit of using our approaches. The log-Gaussian Cox process has been applied for disease and conflict mapping, e.g. ZammitMangion et al. (2012) developed a spatio-temporal model of conflict events in Afghanistan. In contrast here we deal with temporal text data, and model several correlated outputs rathe"
P16-2064,D15-1311,1,0.849351,"in society and in some cases even leading to riots. For instance, during an earthquake in Chile in 393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 393–398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics intensity 5 4 3 2 1 0 We define the stance classification task as that in which each tweet dj needs to be classified into one of the four categories, yj ∈ Y , which represents the stance of the tweet dj with respect to the rumour Ri it belongs to. We consider the Leave One Out (LOO) setting, introduced by Lukasik et al. (2015a), where for each rumour Ri ∈ D we construct the test set equal to Ri and the training set equal to D  Ri . The final performance scores we report in the paper are averaged across all rumours. This represents a realistic scenario where a classifier has to deal with a new, unseen rumour. support deny question comment 0 1 2 3 4 time (in hours) 5 6 Figure 1: Intensities of the Hawkes Process for an example Ferguson rumour. Tweet occurrences over time are denoted at the bottom of the figure by different symbols. Intensity for comments is high throughout the rumour lifespan. 3 nered from rumour d"
P16-2064,P15-2085,1,0.865299,"in society and in some cases even leading to riots. For instance, during an earthquake in Chile in 393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 393–398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics intensity 5 4 3 2 1 0 We define the stance classification task as that in which each tweet dj needs to be classified into one of the four categories, yj ∈ Y , which represents the stance of the tweet dj with respect to the rumour Ri it belongs to. We consider the Leave One Out (LOO) setting, introduced by Lukasik et al. (2015a), where for each rumour Ri ∈ D we construct the test set equal to Ri and the training set equal to D  Ri . The final performance scores we report in the paper are averaged across all rumours. This represents a realistic scenario where a classifier has to deal with a new, unseen rumour. support deny question comment 0 1 2 3 4 time (in hours) 5 6 Figure 1: Intensities of the Hawkes Process for an example Ferguson rumour. Tweet occurrences over time are denoted at the bottom of the figure by different symbols. Intensity for comments is high throughout the rumour lifespan. 3 nered from rumour d"
P16-2064,D11-1147,0,0.197497,"Missing"
R13-1011,P05-1045,0,0.00426474,"Missing"
R13-1011,P11-2008,0,0.0145767,"Missing"
R13-1011,P11-1038,0,0.144722,"Missing"
R13-1011,P02-1022,1,0.360301,"tter is a cascade of finite-state transducers which segments text into sentences. This module is required for the POS tagger. The ANNIE sentence splitter is reused without modification, although when processing tweets, it is also possible to just use the text of the tweet as one sentence, without further analysis. The normaliser, the adapted POS tagger, and named entity recognition are discussed in detail in Sections 3.4, 3.5, and 3.6 respectively. The TwitIE IE Pipeline The open-source GATE NLP framework (Cunningham et al., 2013) comes pre-packaged with the ANNIE general purpose IE pipeline (Cunningham et al., 2002). ANNIE consists of the following main processing resources: tokeniser, sentence splitter, POS tagger, gazetteer lists, finite state transducer (based on GATE’s built-in regular expressions over annotations language), orthomatcher and coreference resolver. The resources communicate via GATE’s annotation API, which is a directed graph of arcs bearing arbitrary feature/value data, and nodes rooting this data into document content. The ANNIE components can be used individually or coupled together with new modules in order to create new applications. TwitIE re-uses the sentence splitter and name g"
R13-1011,D12-1039,0,0.00994453,"allenges to existing tools, being rich in previously unseen tokens, elision of words, and unusual grammar. Normalisation is commonly proposed as a solution for overcoming or reducing linguistic noise (Sproat et al., 2001). The task is generally approached in two stages: first, the identification of orthographic errors in an input discourse, and second, the correction of these errors. The TwitIE Normaliser is a combination of a generic spelling-correction dictionary and a spelling correction dictionary, specific to social media. The latter contains entries such as “2moro” and “brb”, similar to Han et al. (2012). Figure 4 shows an example tweet, where the abbreviation “Govt” has been normalised to government. Instead of a fixed list of variations, it is also possible to use a heuristic to suggest correct spellings. Both 86 text edit distance and phonetic distance can be used to find candidate matches for words identified as misspelled. (Han and Baldwin, 2011) achieved good corrections in many cases by using a combination of Levenshtein distance and double-metaphone distance between known words and words identified as incorrectly entered. We also experimented with this normalisation approach in TwitIE"
R13-1011,P12-1055,0,0.100531,"Missing"
R13-1011,P07-1033,0,0.0100292,"Missing"
R13-1011,J93-2004,0,0.042879,"Missing"
R13-1011,P12-1086,0,0.0400909,"Missing"
R13-1011,R13-1026,1,0.446745,"ess grammatical than longer posts, contain unorthodox capitalisation, and make frequent use of emoticons, abbreviations and hashtags, which can form an important part of the meaning. To combat these problems, research has focused on microblog-specific information extraction algorithms (e.g. named entity recognition for Twitter using CRFs (Ritter et al., 2011), Wikipedia-based topic and entity disambiguation (van Erp et al., 2013)). Particular attention is given to microtext normalisation, as a way of removing some of the linguistic noise prior to part-of-speech tagging and entity recognition (Derczynski et al., 2013a; Han and Baldwin, 2011; Han et al., 2012). Named entity recognition of longer texts, such as news, is a very well studied problem (cf. (Nadeau and Sekine, 2007; Roberts et al., 2008; Marrero et al., 2009)). For Twitter, some approaches have been proposed but often they are not freely available. Ritter et al. (Ritter et al., 2011) take a pipeline approach performing first tokenisation and POS tagging before using topic models to find named entities. Liu (Liu et al., 2012) Introduction Researchers have started recently to study the problem of mining social media content automatically (e.g. (Ro"
R13-1011,pak-paroubek-2010-twitter,0,0.0210778,"Missing"
R13-1011,D11-1141,0,0.385284,"er news articles, there is a low amount of discourse information per microblog document, and threaded structure is fragmented across multiple documents, flowing in multiple directions. Second, microtexts also exhibit much more language variation, tend to be less grammatical than longer posts, contain unorthodox capitalisation, and make frequent use of emoticons, abbreviations and hashtags, which can form an important part of the meaning. To combat these problems, research has focused on microblog-specific information extraction algorithms (e.g. named entity recognition for Twitter using CRFs (Ritter et al., 2011), Wikipedia-based topic and entity disambiguation (van Erp et al., 2013)). Particular attention is given to microtext normalisation, as a way of removing some of the linguistic noise prior to part-of-speech tagging and entity recognition (Derczynski et al., 2013a; Han and Baldwin, 2011; Han et al., 2012). Named entity recognition of longer texts, such as news, is a very well studied problem (cf. (Nadeau and Sekine, 2007; Roberts et al., 2008; Marrero et al., 2009)). For Twitter, some approaches have been proposed but often they are not freely available. Ritter et al. (Ritter et al., 2011) take"
R13-1011,roberts-etal-2008-combining,0,0.00920463,"Missing"
R13-1011,N03-1033,0,0.00963143,"Missing"
R13-1015,derczynski-etal-2012-massively,1,0.845999,"However, as with many natural language processing problems, diminishing returns are being seen in the field. Therefore, next efforts must address the temporal expressions that we cannot yet already detect and interpret. It is of interest to consider the automatic extraction of named timex resolution rules, perhaps using the most important timexes (Str¨otgen et al., 2012) from articles describing the corresponding occasion. It is also relevant to merge our named timex corpus with existing timex corpora See https://en.wikipedia.org/wiki/Computus 3 119 See http://derczynski.com/sheffield/ (e.g. Derczynski et al. (2012)), after annotating the conventional timexes in our named timex training data. Such a corpus could be extended by extracting sentences that cite the Wikipedia or DBpedia entries corresponding to named timexes. Evaluation against such a resource is less likely to overreport the variety of expressions recognised by timex annotation systems, and can provide a solid base for future wide-coverage approaches to temporal expression recognition. Decomposing the complex temporal annotation task so that it can be reliably crowdsourced would enable the construction of more resources. Using human computat"
R13-1015,P05-1045,0,0.00613788,"861 sentences (117 060 tokens) were extracted from English Gigaword v5 (Graff et al., 2003), containing 4 180 named timex annotations. The training split contained 1 053 of these sentences. The entire corpus construction method requires no human intervention aside from supplying source Wikipedia pages. Regarding the NTE recognisers, we adapted three entity recognition approaches to the task by discarding their default models and rebuilding new models based solely on this NTE corpus. The recognition tools were CRF-based: a multipurpose system incorporating non-local information, Stanford NER (Finkel et al., 2005); one for temporal entity recognition that uses semantic role information, TIPSem (Llorens et al., 2012b); and TIPSem-B, a baseline temporal entity recognition variant of TIPSem. Recognisers were learned from the training split and evaluated on the test split. As we are attempting to recognise named timexes only, we do not do comparison against tools designed for standard timex recognition, as these are designed for a different task. A na¨ıve gazetteer-matching baseline was used, based on timex strings found in existing resources (TimeBank and the AQUAINT TimeML annotations). This behaved exac"
R13-1015,N12-1049,0,0.0198686,"enEval Discovering such interpretations is a difficult task. For example, based on text, it is difficult to automatically learn or infer the link between “New Year’s Day” and 1st January, or the associations between north/south hemisphere and which months fall in summer, especially given the cost of temporal annotation and resulting scarcity of annotated resources. This often leaves the task of developing such interpretations to human computation (Sabou et al., 2012). The closest computational method for solving this problem uses a more flexible compositional approach to timex interpretation (Angeli et al., 2012), though it is prone to floundering and failing on completely new expressions, such as named timexes. As the named timexes mined from Wikipedia were generally accompanied by a textual description of the time (e.g., as in Figure 3), we used these descriptions to work out how to interpret the expression. We created a custom parser that worked well with the majority of uncurated, natural language descriptions of named timex dates. Having gathered information from Wikipedia, we then encoded it as rules in a popular timex interpretation system, TIMEN (Llorens et al., 2012a). TIMEN operates using ex"
R13-1015,J06-4003,0,0.0190813,"n of named temporal expressions, we moved on to the task of NTE discovery. Our approach was to first develop a statistical tagger adapted to NTE recognition, and then apply it to new data, to observe what expressions it annotates beyond those in the collection extracted from Wikipedia. The collection was used to construct a corpus and then a statistical named temporal expression recogniser. The corpus was constructed as follows. Using our list of monosemous named timexes, we searched the Gigaword corpus to retrieve paragraphs containing the timexes. These paragraphs were split into sentences (Kiss and Strunk, 2006), and the sentences matching any NTE were extracted; the sentences were then broken down into lists of tokens. We marked all monosemous named timexes in the sentences as target entities. Some NTEs are polysemous, having both temporal and non-temporal sense. Observation of a small part of the corpus suggested that these polysemous NTEs generally occurred in a temporal sense when in the same sentence as other temporal phrases. Rather than excluding any sentence containing a polysemous NTE from the corpus on grounds of ambiguity, based on this observation, we adopted a simple heuristic: polysemou"
R13-1015,llorens-etal-2012-timen,1,0.868446,"cato University of Bologna Leon Derczynski University of Sheffield matteo.brucato@studio.unibo.it leon@dcs.shef.ac.uk Hector Llorens Nuance Communications Kalina Bontcheva University of Sheffield Christian S. Jensen Aarhus University hector.llorens@nuance.com kalina@dcs.shef.ac.uk csj@cs.au.dk Abstract Phrases that explicitly describe certain periods of time, or temporal expressions, are particularly useful. They may be calendar dates, mentions of months, relative expressions like “tomorrow”, and so on. In-depth accounts of temporal expressions – timexes – are given by Ferro et al. (2005) and Llorens et al. (2012a). In this paper, we discuss a new class of timexes that signify a date or range of dates, but that do not explicitly include information about which dates these are (e.g., October 31 vs. Halloween). Following the description of expressions that clearly identify one entity from a set of others by use of a proper noun as named entities, we call these named temporal expressions (or NTEs). As with many linguistic phenomena, the phrases used as timexes have a power law-like frequency distribution in text. A few forms of expression make up for the bulk of occurrences of temporal expressions. Howev"
R13-1015,S10-1071,0,0.0811201,"Missing"
R13-1015,S13-2001,1,0.889807,"e named temporal expressions (or NTEs). As with many linguistic phenomena, the phrases used as timexes have a power law-like frequency distribution in text. A few forms of expression make up for the bulk of occurrences of temporal expressions. However, existing research has been typically evaluated on only a small corpus of hand-annotated temporal expressions. With such resources, it is difficult to build or evaluate tools for recognising or interpreting the lessfrequent temporal expressions, and this is reflected in the performance plateau of recent TempEval exercises (Verhagen et al., 2010; UzZaman et al., 2013). Existing temporal expression recognition tools are typically rule-based (Str¨otgen and Gertz, 2010). These perform reasonably well on existing datasets, achieving F-scores of around 0.90, and improving them is an active area of research. However, as temporal annotation is expensive, existing datasets are not particularly large, and therefore do not contain as challenging a variety of forms of expression as general, unannotated text. Therefore, evaluations using these resources This paper introduces a new class of temporal expression – named temporal expressions – and methods for recognising"
R13-1015,S10-1010,0,\N,Missing
R13-1026,R13-1011,1,0.223646,"Missing"
R13-1026,A00-1031,0,0.0624805,"work on bootstrapped PoS tagging is that of Clark et al. (2003), who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpu"
R13-1026,J95-4004,0,0.451368,", who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagse"
R13-1026,gimenez-marquez-2004-svmtool,0,0.0991477,"Missing"
R13-1026,W03-0407,0,0.0502941,"Missing"
R13-1026,P11-2008,0,0.690806,"Missing"
R13-1026,E03-1009,0,0.0208576,"Missing"
R13-1026,W02-2006,0,0.249811,"Missing"
R13-1026,P11-1038,0,0.117493,"Missing"
R13-1026,P02-1022,1,0.755704,"Missing"
R13-1026,P12-1109,0,0.017024,"Missing"
R13-1026,J93-2004,0,0.0447464,"downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagset based on the Penn Treebank tagset,plus four new tags for URLs (URL), hashtags (HT), username mentions (USR) and retweet signifiers (RT). The DCU dataset of 14K tokens (Foster et al., 2011) is also based on the Penn Treebank (PTB) set, but does not have the same new tags as TPos, and uses slightly differe"
R13-1026,D11-1141,1,0.750123,"arning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently available. The T-Pos corpus of 15K tokens introduced by Ritter et al. (2011) uses a tagset based on the Penn Treebank tagset,plus four new tags for URLs (URL), hashtags (HT), username mentions (USR) and retweet signifiers (RT). The DCU dataset of 14K tokens (Foster et al., 2011) is also based on the Penn Treebank (PTB) set, but does not have the same new tags as TPos, and uses slightly different tokenisation. The ARK corpus of 39K tokens (Gimpel et al., 2011) uses a novel tagset, which, while suitable for the microblog genre, is somewhat less descriptive than the PTB sets on many points. For example, its V tag corresponds to any verb, conflating PTB’s VB, VBD, VBG, VB"
R13-1026,N03-1033,0,0.114204,"t al., 2011). Finally, classic work on bootstrapped PoS tagging is that of Clark et al. (2003), who use a cotraining approach to improve tagger performance using unlabeled data. 3 do not occur in the training data. Further, as per Manning (2011) we report the rate of getting whole sentences right, since “a single bad mistake in a sentence can greatly throw off the usefulness of a tagger to downstream tasks”.3 We evaluated four state-of-the-art trainable and publicly available PoS taggers that used the Penn Treebank tagsettrereetagger: SVMTool (Gim´enez and Marquez, 2004), the Stanford Tagger (Toutanova et al., 2003), TnT (Brants, 2000) and a transformation-based learning (TBL) tagger (Brill, 1995) supported by sequential n-gram backoff. The NLTK implementations of TnT and TBL were used (Bird et al., 2009). The ‘left3words’ model was used with the Stanford tagger, and ‘M0’ with SVMTool. For initial comparison, taggers were tested on standard newswire text from the Penn Treebank (Marcus et al., 1993),4 training with Wall Street Journal (WSJ) sections 0-18 and evaluating on sections 19-21. The base performance for each tagger is given in Table 1. 3.2 Three PoS-labeled microblog datasets are currently availa"
R13-1026,W12-0601,0,\N,Missing
R15-1018,aker-etal-2012-assessing,0,0.0219974,"is useful during corpus creation, in this section we examine how to apply it with an increasingly popular new annotation method: crowdsourcing. Crowdsourcing annotation works by presenting a many microtasks to non-expert workers. They typically make their judgements over short texts, after reading a short set of instructions (Sabou et al., 2014). Such judgments are often simpler than those in linguistic annotation by experts; for example, workers might be asked to annotate only a single class of entity at a time. Through crowdsourcing, quality annotations can be gathered quickly and at scale (Aker et al., 2012). There also tends to be a larger variance in reliability over crowd workers than in expert annotators (Hovy et al., 2013). For this reason, crowdsourced annotation microtasks are often all performed by at least two different workers. E.g., every sentence would be examined for each entity type by at least two different non-expert workers. We investigate entity pre-empting of crowdsourced corpora for a challenging genre: social media. Newswire corpora are not too hard to come by, especially for English, and the genre is somewhat biased in style, mostly being written or created by working-age mi"
R15-1018,E14-2025,1,0.477481,"fficult for existing NER tools to achieve good accuracy on (Derczynski et al., 2013; Derczynski et al., 2015) and having no large NE annotated corpora. In our setup, we subdivide the annotation task according to entity type. Workers perform best with light cognitive loads, so asking them to annotate one kind of thing at a time increases their agreement and accuracy (Krug, 2009; Khanna et al., 2010). Person, location and organisation entities are annotated, giving three annotation sub-tasks, following Bontcheva et al. (2015). Jobs were created automatically using the GATE crowdsourcing plugin (Bontcheva et al., 2014). An example sub-task is shown in Figure 1. This Dataset Base: 500 messages 500 msgs + 1k without entities 500 msgs + 1k random 500 msgs + 1k with entities P 70.39 85.00 76.14 71.21 R 31.66 25.15 44.38 54.14 F1 43.67 38.81 56.07 61.51 Table 8: Adding entity-less vs. entity-bearing data to a 500-message base training set means that we must pre-empt according to entity type, instead of just pre-empting whether or not an excerpt contains any entities at all, which has the additional effect of changing entitybearing/entity-free class distributions. We use two sources that share entity classificati"
R15-1018,E14-4014,1,0.82129,"Missing"
R15-1018,N13-1037,0,0.0141674,"ds to be a larger variance in reliability over crowd workers than in expert annotators (Hovy et al., 2013). For this reason, crowdsourced annotation microtasks are often all performed by at least two different workers. E.g., every sentence would be examined for each entity type by at least two different non-expert workers. We investigate entity pre-empting of crowdsourced corpora for a challenging genre: social media. Newswire corpora are not too hard to come by, especially for English, and the genre is somewhat biased in style, mostly being written or created by working-age middle-class men (Eisenstein, 2013), and in topic, being related to major events around unique entities that one might refer to by a special name. In contrast, social media text has broad stylistic variance (Hu et al., 2013) while also being difficult for existing NER tools to achieve good accuracy on (Derczynski et al., 2013; Derczynski et al., 2015) and having no large NE annotated corpora. In our setup, we subdivide the annotation task according to entity type. Workers perform best with light cognitive loads, so asking them to annotate one kind of thing at a time increases their agreement and accuracy (Krug, 2009; Khanna et"
R15-1018,D11-1141,0,0.0976123,"1k with entities P 70.39 85.00 76.14 71.21 R 31.66 25.15 44.38 54.14 F1 43.67 38.81 56.07 61.51 Table 8: Adding entity-less vs. entity-bearing data to a 500-message base training set means that we must pre-empt according to entity type, instead of just pre-empting whether or not an excerpt contains any entities at all, which has the additional effect of changing entitybearing/entity-free class distributions. We use two sources that share entity classification schemas: the UMBC twitter NE annotations (Finin et al., 2010), and the MSM2013 twitter annotations (Rowe et al., 2013). We also add the Ritter et al. (2011) dataset, mapping its geo-location and facility classes to location, and company, sports team and band to organisation. Mixing datasets reduces the impact of any single corpus’ sampling bias on final results. In total, this gives 3 854 twitter messages (tweets). Table 7 shows the entity distribution over this corpus. From this we separated a 500 tweet training set, used as base NER training data and pre-empting training data, and another set of 500 tweets for evalution. Note that each message can contain more than one type of entity, and that names of people are the most common class of entity"
R15-1018,W10-0713,0,0.0321873,"his Dataset Base: 500 messages 500 msgs + 1k without entities 500 msgs + 1k random 500 msgs + 1k with entities P 70.39 85.00 76.14 71.21 R 31.66 25.15 44.38 54.14 F1 43.67 38.81 56.07 61.51 Table 8: Adding entity-less vs. entity-bearing data to a 500-message base training set means that we must pre-empt according to entity type, instead of just pre-empting whether or not an excerpt contains any entities at all, which has the additional effect of changing entitybearing/entity-free class distributions. We use two sources that share entity classification schemas: the UMBC twitter NE annotations (Finin et al., 2010), and the MSM2013 twitter annotations (Rowe et al., 2013). We also add the Ritter et al. (2011) dataset, mapping its geo-location and facility classes to location, and company, sports team and band to organisation. Mixing datasets reduces the impact of any single corpus’ sampling bias on final results. In total, this gives 3 854 twitter messages (tweets). Table 7 shows the entity distribution over this corpus. From this we separated a 500 tweet training set, used as base NER training data and pre-empting training data, and another set of 500 tweets for evalution. Note that each message can con"
R15-1018,sabou-etal-2014-corpus,1,0.867686,"Missing"
R15-1018,P05-1045,0,0.0119251,"ces - 2k without entities - 2k with entities P 85.70 84.89 85.43 R 84.08 84.41 83.17 F1 84.88 84.65 84.29 ∆ F1 -0.23 -0.59 Table 2: Removing data from our training set base corpus of quality annotated data and intends to expand this corpus. 2.1 Experimental Setup For English newswire, we use the CoNLL 2003 dataset (Tjong Kim Sang and Meulder, 2003). The training part of this dataset has 14 040 sentences; of these, 11 131 contain at least one entity and so 2 909 have no entities. We evaluate against the more challenging testb part of this corpus, which contains 5 652 entity annotations. We use Finkel et al. (2005)’s statistical machine learning-based NER system. 2.2 Pre-empting Entity Presence Validation Results Results are shown in Table 1. Adding 2 000 entitybearing sentences gives the largest improvement in F1, and is better than adding 2 000 randomly chosen sentences – the case without pre-empting. Adding only entity-free text decreases overall performance, especially recall. To double check, we try removing training data instead of adding it. In this case, removing content without entities should hurt performance less than removing content with entities. From all 14k sentences of English training"
R15-1018,P04-1075,0,0.0330421,"st to researchers, who often go to great lengths to avoid it. For example, recently, Garrette and Baldridge (2013) demon128 we only outline an approach using 1 000 examples, up to 15 000 have been annotated and made publicly available for some entity types. For future work, the pre-empting feature set could be first adapted to morphologically rich languages, and then also to languages that do not necessarily compose tokens from individual letters, such as Mi’kmaq or Chinese. strated the impressive construction of a part-ofspeech tagger based on just two hours’ annotation. Similar to our work, Shen et al. (2004) proposed active learning for named entity recognition annotation, reducing annotation load without hurting NER performance, based on three metrics for each text batch and an iterative process. We differ from Shen et al. by giving a one-shot approach which does not need iterative re-training and is simple to implement in an annotation workflow, although we do not reduce annotation load as much. Our simplification means that pre-empting is easy to integrate into an annotation process, especially important for e.g. crowdsourced annotation, which is cheap and effective but gives a lot less contro"
R15-1018,N13-1014,0,0.018652,"entropy classifier implementation used allows output of the most informative features. These are reported – for newswire – in Table 12. In this case, the model was trained on 10 000 examples, and is the one for which results were given in Table 3, that achieved an F-score of 96.88. Word shape features are the strongest indicators of named entity presence, and the strongest indicators of entity absence are all character grams. 6 Related Work Avoiding needless annotation is a constant theme in NLP, and of interest to researchers, who often go to great lengths to avoid it. For example, recently, Garrette and Baldridge (2013) demon128 we only outline an approach using 1 000 examples, up to 15 000 have been annotated and made publicly available for some entity types. For future work, the pre-empting feature set could be first adapted to morphologically rich languages, and then also to languages that do not necessarily compose tokens from individual letters, such as Mi’kmaq or Chinese. strated the impressive construction of a part-ofspeech tagger based on just two hours’ annotation. Similar to our work, Shen et al. (2004) proposed active learning for named entity recognition annotation, reducing annotation load with"
R15-1018,N13-1132,0,0.0202626,"ethod: crowdsourcing. Crowdsourcing annotation works by presenting a many microtasks to non-expert workers. They typically make their judgements over short texts, after reading a short set of instructions (Sabou et al., 2014). Such judgments are often simpler than those in linguistic annotation by experts; for example, workers might be asked to annotate only a single class of entity at a time. Through crowdsourcing, quality annotations can be gathered quickly and at scale (Aker et al., 2012). There also tends to be a larger variance in reliability over crowd workers than in expert annotators (Hovy et al., 2013). For this reason, crowdsourced annotation microtasks are often all performed by at least two different workers. E.g., every sentence would be examined for each entity type by at least two different non-expert workers. We investigate entity pre-empting of crowdsourced corpora for a challenging genre: social media. Newswire corpora are not too hard to come by, especially for English, and the genre is somewhat biased in style, mostly being written or created by working-age middle-class men (Eisenstein, 2013), and in topic, being related to major events around unique entities that one might refer"
R15-1018,szarvas-etal-2006-highly,0,0.0660604,"Missing"
R15-1018,W03-0419,0,0.147535,"Missing"
R15-1018,W02-2024,0,0.213211,"8.6 78.6 90.58 92.12 93.28 96.06 94.22 96.46 SVM + Cost, j = 5 79 79 78.6 78.6 86.33 86.53 92.12 92.36 94.15 94.25 R F1 Language 82.28 80.55 92.14 92.22 Dutch Spanish Hungarian 83 97.5 94.85 96.44 97.20 84 90.1 93.94 96.35 96.88 100 100 96.25 95.36 96.18 88 88.0 91.34 94.65 95.33 100 100 97.84 98.09 98.57 88 88.0 86.43 92.24 94.20 Dutch Spanish Hungarian Training data F1 71.33 72.01 3.3 98.2 100 99.9 93.9 86.5 82.6 F1 55.22 52.27 64.90 69.82 71.75 66.65 Other Languages Pre-empting is not restricted to just English. Similar NER datasets are available for Dutch, Spanish and Hungarian (Tjong Kim Sang, 2002; Szarvas et al., 2006). Results regarding the effectiveness of an SVM pre-empter for these languages are presented in Table 5. In each case, we train with 1 000 sentences and evaluate against a 4 000-sentence evaluation partition. Strong above-baseline performance was achieved for each language. For Dutch and Spanish, this pre-empting approach performs in the same class as for English, with a low error rate. The error rate is markedly higher in Hungarian, a morphologically-rich language. This could be attributed to the use of token n-gram features; one would expect these to be sparser in a la"
R15-1018,N03-1033,0,0.00459272,"hims, 1999); we experiment with cost-weighted SVM in order to achieve high recall (Morik et al., 1999). The second is to declare sentences containing proper nouns as entitybearing. We use a random baseline that predicts NE presence based on the prior proportion of entity-bearing to entity-free sentences (≈4.8:1, entity-bearing is the dominant class, for any entity type). For the machine learning approach, we use the following feature representations: character 1,2,3grams; compressed word shape 1,2,3 grams;1 and token 1,2,3 grams. For the proper noun-based approach, we use the Stanford tagger (Toutanova et al., 2003) to label sentences. This is trained on Wall Street Journal data which does not overlap with the Reuters data in our NER corpus. As data we use a base set of sentences as training examples, which are a mixture of entitybearing and entity-free. We experiment with various sizes of base set. Evaluation is performed over a separate 4 000-sentence set, labelled as either having or not having any entities. Table 1: Adding entity-less vs. entity-bearing data to a 2 000-sentence base training set Dataset Base: All sentences - 2k without entities - 2k with entities P 85.70 84.89 85.43 R 84.08 84.41 83."
R15-1018,D11-1143,0,0.0525993,"Missing"
R15-1018,C00-2137,0,0.160373,"Missing"
R15-1018,W09-1119,0,\N,Missing
S16-1063,D15-1075,0,0.0530468,"Missing"
S16-1063,N16-1138,1,0.893114,"Missing"
S16-1063,I13-1191,0,0.00781955,"Missing"
S16-1063,L16-1729,1,0.859973,"Missing"
S16-1063,P15-1107,0,0.00921714,"000. Each index i in input dim[i] corresponds to a word in the vocabulary, input dim[i] is 1 if the tweet contains the corresponding word in the vocabulary and 0 otherwise. During autoencoder training, an encoder, i.e. embedding function is learned which maps input of size input dim to an embedding of size output dim, as well as a decoder which reconstructs the input. We apply the encoder to the training and test data to obtain features of size output dim for supervised learning and disregard the decoder. While it would be possible to train an encoder which preserves word order, i.e. an LSTM (Li et al., 2015), we opt for a simpler bag-of-word autoencoder here, following Glorot et al. (2011). The architecture of the autoencoder is as follows: input dim is 50000, it has one hidden layer of di1 http://scikit-learn.org/stable/ modules/generated/sklearn.linear_ model.LogisticRegression.html 2 http://github.com/sheffieldnlp/ stance-semeval2016 mensionality 100, and output dim is of size 100. A dropout of 0.1 is added to the hidden layer (Srivastava et al., 2014). The autoencoder is trained with Adam (Kingma and Ba, 2014), using the learning rate 0.1, for 2600 iterations. In each iteration, 500 training"
S16-1063,D13-1171,0,0.0158216,"Missing"
S16-1063,S16-1003,0,0.0869902,"Missing"
S16-1063,strapparava-valitutti-2004-wordnet,0,0.0546914,"e concatenated with the tweet features • Aut-twe*tar: the autoencoder is applied to the tweet and the target, and the outer product of the tweet and target features is used • InTwe: A boolean “targetInTweet” feature We evaluate the impact of traditional sentiment analysis gazetteer features, extracted by assessing appearance of each word of the tweet in the gazetteers: • Emo: emoticon recognition7 One gazetteer/binary feature for each of: happy, sad, happy+sad, not available • Aff: WordNet Affect gazetteer features, one binary feature for each of: anger, disgust, fear, joy, sadness, surprise (Strapparava and Valitutti, 2004)8 6 https://radimrehurek.com/gensim/models/ phrases.html 7 https://github.com/brendano/tweetmotif/ blob/master/emoticons.py 8 http://wndomains.fbk.eu/wnaffect.html 391 Aut-twe+inTwe+Aff Stance FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro P 0.1587 0.5544 R 0.1709 0.4020 0.2278 0.5545 0.1538 0.5700 0.2647 0.5179 0.0769 0.2570 FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro 0.1538 0.5680 0.1538 0.7328 0.1652 0.5503 0.1624 0.6539 0.0000 0.5712 0.0000 1.0000 0.2388 0.5709 0.1368 0.7684 0.1731 0.5487 0.0769 0.7888 FAVOR AGAINST Macro"
S16-1063,N12-1072,0,0.00890815,"Missing"
S16-1063,D15-1073,0,0.0194744,"Missing"
S17-2006,S17-2082,0,0.162573,"Missing"
S17-2006,S17-2084,0,0.0230442,"Missing"
S17-2006,S17-2083,1,0.818965,"hin the conversations were performed through crowdsourcing – as performed to satisfactory quality already with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of"
S17-2006,W11-1701,0,0.0223707,"ce a subtask where the goal is to label the type of interaction between a given statement (rumourous tweet) and a reply tweet (the latter can be either direct or nested replies). Secondly, participants need to determine the type of response towards a rumourous tweet from a tree-structured conversation, where each tweet is not necessarily sufficiently descriptive on its own, but needs to be viewed in the context of an aggregate discussion consisting of tweets preceding it in the thread. This is more closely aligned with stance classification as defined in other domains, such as public debates (Anand et al., 2011). The latter also relates somewhat to the SemEval-2015 Task 3 on Answer Selection in Community Question Answering (Moschitti et al., 2015), where the task was to determine the quality of responses in tree-structured threads in CQA platforms. Responses to questions are classified as ‘good’, ‘potential’ or ‘bad’. Both tasks are related to textual entailment and textual similarity. However, Semeval-2015 Task3 is clearly a question answering task, the platform itself supporting a QA format in contrast with the more free-form format of conversations in Twitter. Moreover, as a question answering tas"
S17-2006,D15-1311,1,0.840265,"gorised into one of the following four categories, following Procter et al. (2013b): • Support: the author of the response supports the veracity of the rumour. • Deny: the author of the response denies the veracity of the rumour. • Query: the author of the response asks for additional evidence in relation to the veracity of the rumour. • Comment: the author of the response makes their own comment without a clear contribution to assessing the veracity of the rumour. Prior work in the area has found the task difficult, compounded by the variety present in language use between different stories (Lukasik et al., 2015; Zubiaga et al., 2017). This indicates it is challenging enough to make for an interesting SemEval shared task. 1.2 1.3 Impact Identifying the veracity of claims made on the web is an increasingly important task (Zubiaga et al., 2015b). Decision support, digital journalism and disaster response already rely on picking out such claims (Procter et al., 2013b). Additionally, web and social media are a more challenging environment than e.g. newswire, which has traditionally provided the mainstay of similar tasks (such as RTE (Bentivogli et al., 2011)). Last year we ran a workshop at WWW 2015, Rum"
S17-2006,S16-1003,0,0.0771763,"Missing"
S17-2006,S17-2080,0,0.0322263,"Missing"
S17-2006,D11-1147,0,0.479793,"y participants on these challenges. 1 Introduction and Motivation Rumours are rife on the web. False claims affect people’s perceptions of events and their behaviour, sometimes in harmful ways. With the increasing reliance on the Web – social media, in particular – as a source of information and news updates by individuals, news professionals, and automated systems, the potential disruptive impact of rumours is further accentuated. The task of analysing and determining veracity of social media content has been of recent interest to the field of natural language processing. After initial work (Qazvinian et al., 2011), increasingly advanced systems and annotation schemas have been developed to support the analysis of rumour and misinformation in text (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016b). Veracity judgment can be decomposed intuitively in terms of a comparison between assertions made in – and entailments from – a candidate text, and external world knowledge. Intermediate linguistic cues have also been 1.1 Subtask A - SDQC Support/ Rumour stance classification Related to the objective of predicting a rumour’s veracity, Subtask A deals with the complemen"
S17-2006,S17-2087,0,0.0127388,"lready with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of rumour veracity on social media. Most participants tackled Subtask A, which involves classifying"
S17-2006,S17-2085,0,0.0202189,"ing – as performed to satisfactory quality already with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of rumour veracity on social media. Most participants tackle"
S17-2006,S17-2086,0,0.0386461,"Missing"
S19-2146,Q17-1010,0,0.039453,"as the input for our models. As a simple and easy to calculate compromise between representing the details of the article and as much of a longer article as possible, we represent the article as a sequence of sentence embeddings which are calculated as the average of the word embeddings of a sentence. This can be done using any pre-trained word embeddings and does not require a large training set 2.3 Deep Contextualized Word Representation Traditionally, the input to CNNs is a set of pretrained word vectors such as Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bojanowski et al., 2017). In our model, we use the AllenNLP library to generate ELMo 841 embeddings, in which the word representation is learned from character-based units as well as contextual information from the news articles. These character-based word representations allow our model to pick up on morphological features that word-level embeddings could miss, and a valid word representation can be formed even for outof-vocabulary words. Furthermore, ELMo uses two bi-directional LSTM (Gers et al., 1999) layers to learn the contextual information from the text, which makes it capable of disambiguating the same word"
S19-2146,S19-2145,0,0.0427123,"d word embeddings generated from the pre-trained ELMo model with Convolutional Neural Networks and Batch Normalization for predicting hyperpartisan news. The final predictions were generated from the averaged predictions of an ensemble of models. With this architecture, our system ranked in first place, based on accuracy, the official scoring metric. 1 Introduction Hyperpartisan news is typically defined as news which exhibits an extremely biased opinion in favour of one side, or unreasoning allegiance to one party (Potthast et al., 2017). SemEval2019 Task 4 on “Hyperpartisan News Detection” (Kiesel et al., 2019) is a document-level classification task which requires building a precise and reliable algorithm to automatically discriminate hyperpartisan news from more balanced stories. One of the major challenges of this task is that the model must have the ability to adapt to a large range of article sizes. In one of the training data sets, the by-publisher corpus, the average article length is 796 tokens, but the longest document has 93,714 tokens. Most state-of-the-art neural network approaches for document classification use a token sequence as network input (Kim, 2014; Yin and Sch¨utze, 2016; Conne"
S19-2146,D14-1181,0,0.0126803,"Missing"
S19-2146,D14-1162,0,0.0925837,"irectly use word level representations as the input for our models. As a simple and easy to calculate compromise between representing the details of the article and as much of a longer article as possible, we represent the article as a sequence of sentence embeddings which are calculated as the average of the word embeddings of a sentence. This can be done using any pre-trained word embeddings and does not require a large training set 2.3 Deep Contextualized Word Representation Traditionally, the input to CNNs is a set of pretrained word vectors such as Word2Vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fasttext (Bojanowski et al., 2017). In our model, we use the AllenNLP library to generate ELMo 841 embeddings, in which the word representation is learned from character-based units as well as contextual information from the news articles. These character-based word representations allow our model to pick up on morphological features that word-level embeddings could miss, and a valid word representation can be formed even for outof-vocabulary words. Furthermore, ELMo uses two bi-directional LSTM (Gers et al., 1999) layers to learn the contextual information from the text, which makes it c"
S19-2146,N18-1202,0,0.0484964,"tha von Suttner at SemEval-2019 Task 4: Hyperpartisan News Detection using ELMo Sentence Representation Convolutional Network Ye Jiang, Johann Petrak, Xingyi Song, Kalina Bontcheva, Diana Maynard Department of Computer Science University of Sheffield Sheffield , UK {yjiang18,johann.petrak,x.song, k.bontcheva,d.maynard}@sheffield.ac.uk Abstract sequence length is restricted to a manageable number of initial tokens from the document. In this paper, we introduce the ELMo Sentence Representation Convolutional (ESRC) Network. We first pre-calculate sentence level embeddings as the average of ELMo (Peters et al., 2018) word embeddings for each sentence, and represent the document as a sequence of such sentence embeddings. We then apply a lightweight convolutional Neural Network (CNN), along with Batch Normalization (BN), to learn the document representations and predict the hyperpartisan classification. Two types of data set have been made available for the task. The by-publisher corpus contains 750K articles which were automatically classified based on a categorization of the political bias of the news source. This dataset was split into a training set of 600K articles and a validation set of 150K articles"
S19-2147,N16-1138,0,0.0499239,"n propagation. Initial work on rumour detection and stance classification (Qazvinian et al., 2011) was succeeded by more elaborate systems and annotation schemas (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016). Vosoughi (2015) demonstrated the value of making use of propagation information, i.e. the ensuing discussion, in rumour verification. Stance detection is the task of classifying a text according to the position it takes with respect to a statement. Research supports the importance of this subtask as a first step to 6 veracity identification. (Ferreira and Vlachos, 2016; Enayet and El-Beltagy, 2017). Crowd response, stance and the details of rumour propagation feature in the work by Chen et al. (2016) as well as the most successful system in RumourEval 2017 (Enayet and El-Beltagy, 2017), and the highest performing systems in RumourEval 2019. 1.2 Datasets for rumour verification The UK fact-checking charity Full Fact provides a roadmap7 for development of automated fact checking. They cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area. Shared data"
S19-2147,S17-2083,1,0.672765,"4 (15) 0.4895 (4) 0.1272 (19) 0.3267 (16) 0.3537 (14) 0.3875 (10) 0.4384 (6) 0.3927 (9) 0.6067 (2) 0.4792 (5) 0.3699 (12) 0.4298 (8) 0.3326 0.6846 0.2165 0.1845 0.7857 0.2530 0.3364 0.7806 0.4929 0.3089 0.7698 - 0.2241 0.7115 0.2234 uses the same features as the stance classification system but produces a single output per branch. The veracity prediction for the thread is then decided using majority voting over per-branch outcomes. Stance classification baseline For subtask A we released a Keras (Chollet et al., 2015) implementation of branchLSTM, the winning system of RumourEval 2017 Task A (Kochkina et al., 2017). This system uses the conversation structure by splitting it into linear branches. It is a neural network architecture that uses LSTM layer(s) to process sequences of tweets, outputting a stance label at each time step. Each tweet is represented by the average of its word vectors 11 concatenated with a number of extra features. This baseline was outperformed by 3 submitted systems (BLCU NLP, BUT-FIT, eventAI). 4.2 Subtask B, RMSE 0.6078 (1) 0.7642 (2) 0.8012 (3) 0.8179 (5) 0.8081 (4) 0.8623 (7) 0.8623 (7) 0.8623 (7) 0.8678 (8) 0.8264 (6) 0.9148 (9) - Table 5: Results table. Ranking is in brac"
S19-2147,C18-1288,1,0.869514,"Missing"
S19-2147,P18-1184,0,0.0196785,"cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area. Shared datasets are a crucial part of the joint endeavour. Datasets for rumour resolution are still relatively few, and likely to be in increasing demand. In addition to the data from RumourEval 2017, the dataset released by Kwon et al. (2017) is also suitable for veracity classification. It includes 51 true rumours and 60 false rumours, where each rumour includes a stream of tweets associated with it. Twitter 15 and 16 datasets (Ma et al., 2018) contain claim propagation trees and combine tasks of rumour detection and verification in one four-way classification task (Non-rumour, True, False, Unverified). A Sina Weibo corpus is also available (Wu et al., 2015), in which 5000 posts are classified for veracity, but responses are not available. Partially generated statistical claim checking data is now becoming available in the context of the FEVER shared task, mentioned above, but is not suitable for this type of work. Twitter continues to be a highly relevant platform for rumour verification, being popular with the public as well as po"
S19-2147,N18-1202,0,0.0133727,"t al., 2018) 10 https://github.com/kochkinaelena/ RumourEval2019 11 We are using word2vec (Mikolov et al., 2013) model pretrained on the GoogleNews dataset (300d) 851 training of bidirectional representations to provide additional context. They experiment with different parameter settings and if the model increased overall performance it was added to the classifier. Interestingly the best performing system in task A (BLCU-NLP) and the third best (CLEARumor) also use pre-trained contextual embedding representations with BLCU-NLP using OpenAI GPT (Radford et al., 2018) and ClEARumor using ELMo (Peters et al., 2018). While most systems use single tweets or pairs of tweets (sourceresponse) as their underlying structure to operate on, BLCU-NLP employ an inference chain-based system for this paper. Thus they consider the conversation thread starting with a source tweet, followed by replies, in which each one responds to an earlier one in time sequence. They take each conversation thread as an inference chain and concentrate on utilizing it to solve the problem of class imbalance in subtask A and training data scarcity in subtask B. They also have augmented the training data with external public datasets. Ot"
S19-2147,D11-1147,0,0.461343,"ient skepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2016). One can distinguish several component to a rumour resolution pipeline such as rumour detection, rumour tracking and stance classification, leading to the final outcome of determining the veracity of a rumour (Zubiaga et al., 2018). Thus what characterises rumour verification compared to other types of fact checking is time sensitivity and the importance of dynamic interactions between users, their stance and information propagation. Initial work on rumour detection and stance classification (Qazvinian et al., 2011) was succeeded by more elaborate systems and annotation schemas (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016). Vosoughi (2015) demonstrated the value of making use of propagation information, i.e. the ensuing discussion, in rumour verification. Stance detection is the task of classifying a text according to the position it takes with respect to a statement. Research supports the importance of this subtask as a first step to 6 veracity identification. (Ferreira and Vlachos, 2016; Enayet and El-Beltagy, 2017). Crowd response, stance and the details o"
S19-2147,D15-1312,0,0.023098,"Hostage-taker in supermarket siege killed, reports say. #ParisAttacks LINK [true] Veracity prediction. Example 2: u1: OMG. #Prince rumoured to be performing in Toronto today. Exciting! [false] Table 1: Examples of source tweets with veracity value of local features. Fact checking is a broad complex task, challenging the resourcefulness of even a human expert. Claims such as ”we send the EU 350 million a week” which is partially true would need to be decomposed into statements to be checked against knowledge bases and multiple sources. Ways of automating fact checking has inspired researchers (Vlachos and Riedel, 2015) and has resulted in a new shared task FEVER.6 Other research has focused on stylistic tells of untrustworthiness in the source itself (Conroy et al., 2015; Singhania et al., 2017). Rumour verification is a particular case of fact checking. Rumours are “circulating stories of questionable veracity, which are apparently credible but hard to verify, and produce sufficient skepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2016). One can distinguish several component to a rumour resolution pipeline such as rumour detection, rumour tracking and stance classi"
sabou-etal-2014-corpus,W10-0705,0,\N,Missing
sabou-etal-2014-corpus,W10-1807,0,\N,Missing
sabou-etal-2014-corpus,W10-1839,0,\N,Missing
sabou-etal-2014-corpus,W10-0712,0,\N,Missing
sabou-etal-2014-corpus,W10-0717,0,\N,Missing
sabou-etal-2014-corpus,scharl-etal-2012-leveraging,1,\N,Missing
sabou-etal-2014-corpus,W10-0702,0,\N,Missing
sabou-etal-2014-corpus,rosenthal-etal-2010-towards,0,\N,Missing
sabou-etal-2014-corpus,W10-0719,0,\N,Missing
sabou-etal-2014-corpus,D08-1027,0,\N,Missing
sabou-etal-2014-corpus,W10-0718,0,\N,Missing
sabou-etal-2014-corpus,W02-0817,0,\N,Missing
sabou-etal-2014-corpus,N06-2015,0,\N,Missing
sabou-etal-2014-corpus,D11-1062,0,\N,Missing
sabou-etal-2014-corpus,D11-1065,0,\N,Missing
sabou-etal-2014-corpus,J11-2010,0,\N,Missing
sabou-etal-2014-corpus,P11-1122,0,\N,Missing
sabou-etal-2014-corpus,W10-0728,0,\N,Missing
sabou-etal-2014-corpus,W11-1510,0,\N,Missing
sabou-etal-2014-corpus,W10-0734,0,\N,Missing
sabou-etal-2014-corpus,E14-2025,1,\N,Missing
sabou-etal-2014-corpus,N13-1132,0,\N,Missing
sabou-etal-2014-corpus,W10-0703,0,\N,Missing
sabou-etal-2014-corpus,P10-5004,0,\N,Missing
sabou-etal-2014-corpus,aker-etal-2012-assessing,0,\N,Missing
sabou-etal-2014-corpus,D11-1143,0,\N,Missing
sabou-etal-2014-corpus,W11-0404,0,\N,Missing
sabou-etal-2014-corpus,W10-0713,0,\N,Missing
sabou-etal-2014-corpus,W10-0723,0,\N,Missing
sabou-etal-2014-corpus,abekawa-etal-2010-community,0,\N,Missing
saggion-etal-2002-extracting,X98-1004,0,\N,Missing
saggion-etal-2002-extracting,P00-1036,0,\N,Missing
saggion-etal-2002-extracting,J95-4004,0,\N,Missing
saggion-etal-2002-extracting,W01-1017,1,\N,Missing
saggion-etal-2002-extracting,O98-4002,1,\N,Missing
sanchan-etal-2017-automatic,D10-1007,0,\N,Missing
sanchan-etal-2017-automatic,C00-1072,0,\N,Missing
tablan-etal-2002-unicode,pastra-etal-2002-feasible,1,\N,Missing
tablan-etal-2002-unicode,gamback-olsson-2000-experiences,0,\N,Missing
tablan-etal-2006-user,C92-2090,0,\N,Missing
W00-1501,gamback-olsson-2000-experiences,0,\N,Missing
W00-1501,A97-1034,0,\N,Missing
W00-1501,A97-1035,1,\N,Missing
W00-1501,A97-1051,0,\N,Missing
W00-1501,E99-1035,0,\N,Missing
W00-1501,cunningham-etal-2000-software,1,\N,Missing
W00-1501,C96-1079,0,\N,Missing
W00-1503,A97-1035,1,\N,Missing
W01-1004,J92-4003,0,0.0165778,"Missing"
W01-1004,M98-1007,0,0.0295886,"Missing"
W01-1004,W01-1002,0,0.0327066,"Missing"
W01-1004,J96-2003,0,0.0220405,"Missing"
W01-1004,C00-2136,0,0.0501815,"Missing"
W02-0108,W02-0109,0,0.302305,"For courses where the emphasis is more on linguistic annotation and corpus work, GATE can be used as a corpus annotation environment (see http://gate.ac.uk/talks/tutorial3/). The annotation can be done completely manually or it can be bootstrapped by running some of GATE’s processing resources over the corpus and then correcting/adding new annotations manually. These facilities can also be used in courses and assignments where the students need to learn how to create data for quantitative evaluation of NLP systems. If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well. The graphical development environment and the JAPE language facilitate otherwise difficult tasks. Inter-module consistency is achieved by using the annotations model to hold language data, while extensibility and modularity are the very reason why GATE has been successfully used in many research projects (Maynard et al., 2000). In addition, GATE also offers robustness and scalability, which allow students to experiment with big corpora, such as the British National Corpus (approx. 4GB). In the following subsections we will provide further detail about these a"
W02-0108,tablan-etal-2002-unicode,1,0.875493,"Missing"
W02-0108,X98-1004,0,\N,Missing
W02-0108,P06-4018,0,\N,Missing
W02-0403,tablan-etal-2002-unicode,1,\N,Missing
W02-0403,X98-1004,0,\N,Missing
W02-0403,P00-1036,0,\N,Missing
W02-0403,W00-0401,1,\N,Missing
W02-0403,A97-1051,0,\N,Missing
W03-0101,E03-2009,1,\N,Missing
W03-0803,baker-etal-2002-emille,1,0.880531,"Missing"
W03-0803,ma-etal-2002-models,0,0.063299,"Missing"
W03-0803,daelemans-hoste-2002-evaluation,0,0.0294896,"rs for the chosen learning method (e.g., thresholds, smoothing values). The classes to be learnt (e.g., Person, Organisation) are provided as part of the user profile, which can be edited on a dedicated page. All ML methods compatible with OLLIE have a uniform way of describing attributes and classes (see Section 3.1 for more details on the ML integration); this makes possible the use of a single user interface for all the ML algorithms available. The fine-tuning parameters are specific to each ML method and, although the ML methods can be run with their default parameters, as established by (Daelemans and Hoste, 2002), substantial variation in performance can be obtained by changing algorithm options. The graphical interface facilities provided by a web browser could be used to design an interface for annotating documents but that would mean stretching them beyond their intended use and it is hard to believe that such an interface would rate very high on a usability scale. In order to provide a more ergonomic interface, OLLIE uses a Java applet that integrates seamlessly with the page displayed by the browser. Apart from better usability, this allows for greater range of options for the user. Since OLLIE n"
W03-0803,gamback-olsson-2000-experiences,0,0.0526719,"Missing"
W03-0803,ide-etal-2000-xces,0,\N,Missing
W03-0803,tablan-etal-2002-unicode,1,\N,Missing
W03-0803,P02-1062,0,\N,Missing
W03-2801,W00-1401,0,0.0333294,"xtraction for the training of a content planning NLG component. 4.2 Evaluation Metrics Previous work on learning order constraints has used human subjects for evaluation. For example, (Barzilay et al., 2002) asked humans to grade the summaries, while (Duboue and McKeown, 2001) manually analysed the derived constraints by comparing them to an existing text planner. However, this is not sufficient if different planners or versions of the same planner are to be compared in a quantitative fashion. In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al., 2000) and they have been shown to correlate well with human judgement for quality and understandability. These metrics are two kinds: using string edit distance and using tree-based metrics. The string edit distance ones measure the insertion, deletion, and substitution errors between the reference sentences in the corpus and the generated ones. Two different measures were evaluated and the one that treats deletions in one place and insertion in the other as a single movement error was found to be more appropriate. In the context of content planning we intend use the string edit distance metrics by"
W03-2801,P01-1023,0,0.053321,"Missing"
W03-2801,W02-2101,0,0.0136358,"on metric and a scoring tool, implementing this metric. Below we will discuss each of these components and highlight the outstanding problems and challenges. 4.1 Evaluation Corpora for Content Planning Research on content planning comes from two fields: document summarisation which uses some NLG techniques to generate the summaries; and natural language generation where the systems generate from some semantic representation, e.g., a domain knowledge base or numeric weather data. Here we review some work from these fields that has addressed the issue of evaluation corpora. 4.1.1 Previous Work (Kan and Mckeown, 2002) have developed a corpus-trained summarisation system for indicative summaries. As part of this work they annotated manually 100 bibliography entries with indicative summaries and then used a decision tree learner to annotate automatically another 1900 entries with 24 predicates like Audience, Topic, and Content. For example, some annotations for the Audience predicate are: For adult readers; This books is intended for adult readers. The annotated texts are then used to learn the kinds of predicates present in the summaries, their ordering using bigram statistics, and surface realisation patte"
W03-2801,M98-1002,0,0.183652,"e module and also allow comparative evaluation with other approaches. For example, the MUC corpora and the associated scoring tool are frequently used by researchers working on machine learning for Information Extraction both as part of the development process and also as means for comparison of the performance of dif4 The MIAKT project is sponsored by the UK Engineering and Physical Sciences Research Council (grant GR/R85150/01) and involves the University of Southampton, University of Sheffield, the Open University, University of Oxford, and King’s College London. ferent systems (see e.g., (Marsh and Perzanowski, 1998)). Similarly, automatic quantitative evaluation of content planners needs: an annotated corpus; an evaluation metric and a scoring tool, implementing this metric. Below we will discuss each of these components and highlight the outstanding problems and challenges. 4.1 Evaluation Corpora for Content Planning Research on content planning comes from two fields: document summarisation which uses some NLG techniques to generate the summaries; and natural language generation where the systems generate from some semantic representation, e.g., a domain knowledge base or numeric weather data. Here we r"
W03-2801,J98-2013,0,0.068423,"Missing"
W05-0610,C02-1054,0,0.16269,"our algorithms on small amounts of training data and show their learning curve. The learning algorithms for IE can be classified broadly into two main categories: rule learning and statistical learning. The former induces a set of rules from training examples. There are many rule based learning systems, e.g. SRV (Freitag, 1998), RAPIER (Califf, 1998), WHISK (Soderland, 1999), BWI (Freitag and Kushmerick, 2000), and (LP ) 2 (Ciravegna, 2001). Statistical systems learn a statistical model or classifiers, such as HMMs (Freigtag and McCallum, 1999), Maximal Entropy (Chieu and Ng., 2002), the SVM (Isozaki and Kazawa, 2002; Mayfield et al., 2003), and Perceptron (Carreras et al., 2003). IE systems also differ from each other in the NLP features that they use. These include simple features such as token form and capitalisation information, linguistic features such as part-ofspeech, semantic information from gazetteer lists, and genre-specific information such as document structure. In general, the more features the system uses, the better performance it can achieve. This paper concentrates on classifier-based learning for IE, which typically converts the recognition of each information entity into a set of class"
W05-0610,Y03-1024,1,0.697666,"balanced data. In this paper we explore the application of PAUM to IE. The rest of the paper is structured as follows. Section 2 describes the uneven margins SVM and Perceptron algorithms. Sections 3.1 and 3.2 discuss the classifier-based framework for IE and the experimental datasets we used, respectively. We compare our systems to other state-of-the-art systems on three benchmark datasets in Section 3.3. Section 3.4 discusses the effects of the uneven margins parameter on the SVM and Perceptron’s performances. Finally, Section 4 provides some conclusions. 2 Uneven Margins SVM and Perceptron Li and Shawe-Taylor (2003) introduced an uneven margins parameter into the SVM to deal with imbalanced classification problems. They showed that the SVM with uneven margins outperformed the standard SVM on document classification problem with imbalanced training data. Formally, given a training set Z = ((x1 , y1 ), . . . , (xm , ym )),where xi is the ndimensional input vector and yi (= +1 or −1) its label, the SVM with uneven margins is obtained by solving the quadratic optimisation problem: minw, b, ξ hw, wi + C m X i=1 ξi s.t. hw, xi i + ξi + b ≥ 1 if yi = +1 3 Experiments hw, xi i − ξi + b ≤ −τ if yi = −1 3.1 ξi ≥ 0"
W05-0610,W03-0422,0,0.140468,"Missing"
W05-0610,W03-0425,0,0.027395,"Missing"
W05-0610,W03-0429,0,0.276145,"ounts of training data and show their learning curve. The learning algorithms for IE can be classified broadly into two main categories: rule learning and statistical learning. The former induces a set of rules from training examples. There are many rule based learning systems, e.g. SRV (Freitag, 1998), RAPIER (Califf, 1998), WHISK (Soderland, 1999), BWI (Freitag and Kushmerick, 2000), and (LP ) 2 (Ciravegna, 2001). Statistical systems learn a statistical model or classifiers, such as HMMs (Freigtag and McCallum, 1999), Maximal Entropy (Chieu and Ng., 2002), the SVM (Isozaki and Kazawa, 2002; Mayfield et al., 2003), and Perceptron (Carreras et al., 2003). IE systems also differ from each other in the NLP features that they use. These include simple features such as token form and capitalisation information, linguistic features such as part-ofspeech, semantic information from gazetteer lists, and genre-specific information such as document structure. In general, the more features the system uses, the better performance it can achieve. This paper concentrates on classifier-based learning for IE, which typically converts the recognition of each information entity into a set of classification problems. In t"
W15-4306,J92-4003,0,0.384216,"Missing"
W15-4306,R13-1015,1,0.89064,"Missing"
W15-4306,N15-1075,0,0.11961,"CRF L-BFGS provided the best performance on our dataset for the ten-types task. 3.5 Training Data In our final system, we included the dev 2015 data, to combat drift present in the corpus. We anticipated that the test set would be from 2015. The original dataset was harvested in 2010, long enough ago to be demonstrably disadvantaged when compared with modern data (Fromreide et al., 2014), and so it was critical to include something more. The compensate for the size imbalance – the dev 2015 data is 0.175 the size of the 2010 data – we weighted down the older dataset to by 0.7, as suggested by (Cherry and Guo, 2015), implemented by uniformly scaling individual feature values on older instances. This successfully reduced the negative impact of the inevitable drift. 4 5 5.1 Analysis Features In terms of features, we looked at the strongestweighted observations in the notypes model, to see what the general indicators are of named entities in tweets. The largest of these are shown in Table 3. Of note is that features indicating URLs, hashtags and usernames indicate against an entity; lowercase words including punctuation, or comprising only punctuation, are not entities; being proceeded by at indicates being"
W15-4306,P02-1022,1,0.731761,"ained on more consistent, longer documents, such as newswire, mostly impotent (Derczynski et al., 2015b). Suffering from a sustained dearth of annotated Twitter datasets, it may be useful to understand what makes this genre tick, and how our existing techniques and resources can be generalised better to fit such a challenging text source. This paper has focused on introducing our Named Entity Recognition (NER) entry to the WNUT evaluation challenge (Baldwin et al., 2015), which builds on our earlier experiments with Twitter and news NER (Derczynski and Bontcheva, 2014; Bontcheva et al., 2013; Cunningham et al., 2002). In particular, we push data sources and representations, using what is know about Twitter so far to construct a model that informs our choices. Specifically, we attempt to compensate for entity drift; to harness unsupervised word clustering in a principled fashion; to bring in large-scale gazetteers; to attenuate the impact of terms frequent in this text type; and to pick and choose targeted gazetteers for specific entity types. 3 Method The WNUT Twitter NER task required us to address many data sparsity challenges. Firstly, the datasets involved are simply very small, making it hard to gene"
W15-4306,N13-1037,0,0.0143652,"ge-scale gazetteers; to attenuate the impact of terms frequent in this text type; and to pick and choose targeted gazetteers for specific entity types. 3 Method The WNUT Twitter NER task required us to address many data sparsity challenges. Firstly, the datasets involved are simply very small, making it hard to generalise in supervised learning, and meaning that effect sizes cannot be reliably measured. Secondly, Twitter language is arguably one of the noisiest and idiosyncratic text genres, which manifests as a large number of word types, and very large vocabularies due to lexical variation (Eisenstein, 2013). Thirdly, the language and especially entities found in tweets change over time, which is commonly referred to as drift. The majority of the WNUT training data is from 2010, and only a small amount from 2015, leading to a sparsity in examples of modern language. Therefore, in our machine learning approach, many of the features we introduce are there to combat sparsity. 48 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 48–53, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics 3.1 Unsupervised Clustering NE type company We use an unsupervis"
W15-4306,fromreide-etal-2014-crowdsourcing,0,0.0851378,"h structured learning. Out of CRF using L-BFGS updates, CRF with passive-aggressive updates to combat Twitter noise (Derczynski and Bontcheva, 2014), and structured perceptron (also useful on Twitter noise (Johannsen et al., 2014)), CRF L-BFGS provided the best performance on our dataset for the ten-types task. 3.5 Training Data In our final system, we included the dev 2015 data, to combat drift present in the corpus. We anticipated that the test set would be from 2015. The original dataset was harvested in 2010, long enough ago to be demonstrably disadvantaged when compared with modern data (Fromreide et al., 2014), and so it was critical to include something more. The compensate for the size imbalance – the dev 2015 data is 0.175 the size of the 2010 data – we weighted down the older dataset to by 0.7, as suggested by (Cherry and Guo, 2015), implemented by uniformly scaling individual feature values on older instances. This successfully reduced the negative impact of the inevitable drift. 4 5 5.1 Analysis Features In terms of features, we looked at the strongestweighted observations in the notypes model, to see what the general indicators are of named entities in tweets. The largest of these are shown"
W15-4306,S14-1001,0,0.0267027,"Missing"
W15-4306,P08-1068,0,0.0672496,"the text type. 250 million tweets from 20102012 were used to generate 2,000 word classes using Brown clustering (Brown et al., 1992). Typically 1,000 or fewer are used; the larger number of classes was chosen because it helpfully increased the expressivity of the representation (Derczynski et al., 2015a), while retaining a useful sparsity reduction. These hierarchical classes were represented using bit depths of 3-10 inclusive, and then 12, 14, 16, 18 and 20, one feature per depth. The typical levels are 4, 6, 10 and 20, though selection of bit depths to use often yields brittle feature sets (Koo et al., 2008), and so we leave it to the classifier to decide which ones are useful. These choices are examined in our post-exercise investigations into the model, Section 5.1, and the clusters provided with this paper. Finally, we also include the Brown class paths for the previous token. To aid in filtering out common tokens and reducing the impact they may have as e.g. spurious gazetteer matches, we incorporate a term frequency from our language model. This is applied to terms that are in the top 50,000 found in our garden hose sample, and represented as a feature having a value scaled in proportion to"
W15-4306,P12-3005,0,0.0355868,"Missing"
W15-4306,D11-1141,0,0.712406,"ly corresponding types. To build gazetteers, we therefore retrieved all Freebase types for all entities in the training corpus and selected the most prominent Freebase types per entity type in the gold standard. The list of Freebase types corresponding to each entity type in the gold standard is listed in Table 1. For each Freebase type, separate gazetteers were created for entity names and alternative names (aliases), since the latter tend to be of lower quality. There were several other gazetteer sources that we tried but which did not work very well: IMDb dumps,2 Ritter’s LabeledLDA lists (Ritter et al., 2011) (duplicated in the baseline system), and ANNIE’s other Morpho-Syntactic Features To model context, we used reasonably conventional features: the token itself, the uni- and bigrams in a [−2, 2] offset window from the current token, and both wordshape (e.g. London becomes Xxxxxx) and reduced wordshape (London to Xx) features. We also included a part-of-speech tag for each token. These were automatically generated by a custom tweet PoS tagger using an extension of the PTB tagset (Derczynski et al., 2013b). To capture orthographic information, we take suffix and prefix features of length [1..3]."
W15-4306,W06-2918,0,0.213294,"nal: the all-types and multiple-types tasks are effectively similar when contrasted with the single-types task, in that they require the recognition of many different kinds of named entity. Finally, we found that other gazetteer types were not helpful to performance; taking for example all of the ANNIE gazetteers, gazetteers from IMDb dumps, entity names extracted from other Twitter NER corpora, or entities generated through LLDA (Ritter et al., 2011) all decreased performance. We suspect this is due to their swamping already-small input dataset with too great a profusion of information, c.f. Smith and Osborne (2006). In addition, we tried generating semi-supervised data using vote-constrained bootstrapping, but this was not helpful either – presumably due to the initially low performance of machine-learning based tools on Twitter NER making it hard to develop semi-supervised bootstrapped training data, no matter how stringent the filtering of autogenerated examples. For the final run, we were faced with a decision about fitting. We could either choose a configuration that minimised training loss on all the available training data (train + dev + dev 2015), but risked overfitting to it. Alternatively, we c"
W15-4306,W15-0211,1,0.807824,"r B-company B-geo-loc B-person B-facility B-facility B-sportsteam B-tvshow B-person B-product B-other B-geo-loc B-person B-geo-loc B-person B-company Terms -0.571505 -0.585369 -0.604976 -0.620909 -0.655420 0.699101 0.699101 0.709865 0.714127 -0.717037 0.747492 0.774895 0.804635 -0.894333 0.895203 0.950866 1.044984 5.2 Gold standard When developing the system, we encountered several problems and inconsistencies in the gold standard. These issues are partly a general problem of developing gold standards, i.e. the more complicated the task is, the more humans tend to disagree on correct answers (Tissot et al., 2015). For Twitter NERC with 10 types, some of the tokens are very difficult to label because the context window is very small (140 characters), which then also leads to acronyms being used very frequently to save space, and because world knowledge about sports, music etc. is required. In particular, the following groups of problems in the gold standard training corpus were identified: Table 4: Largest-weighted Brown cluster features in 10-types task numbers rarely start entities; and being matched by an entry in the video games gazetteer suggests being an entity. One cluster prefix was indicative"
W15-4306,E14-4014,1,\N,Missing
W15-4306,R13-1026,1,\N,Missing
W15-4306,R13-1011,1,\N,Missing
W16-5606,P05-1054,0,0.0487202,"C-P from the eight possible classifications in each data set. A classification pipeline was created, taking each user’s corpus of tweets as input, tokenized using a 5 http://openstreetmap.org/ Twitter aware tokenizer (Gimpel et al., 2011). TFIDF transformed word n-grams (1- and 2-grams) were used as features for a multi-class Support Vector Machine (SVM) with a linear kernel. n-grams and SVMs were chosen as they have been shown to consistently perform well at user profiling tasks, both for social media (Rao and Yarowsky, 2010; Rout et al., 2013; Schwartz et al., 2013) and other types of text (Boulis and Ostendorf, 2005; Garera and Yarowsky, 2009), and are as such a useful tool to establish baseline performance. Balanced sets were extracted from the OAC-P and LAC-P datasets with 2000 members per label in both cases. 10-fold crossvalidation was used for all experiments. 5.1 Results The results of the SVM classifier are presented in Table 1, compared with results from a random baseline. Prediction of both OAC and LAC outperform the random baseline, indicating that the training dataset described in this article can be used to create valuable user profiling systems. Results for LAC are encouraging and indicate t"
W16-5606,D11-1120,0,0.0343199,"nalysis indicates that the nature of the demographic information is an important factor in performance of this approach. 1 Introduction Previous research has shown that the language a person uses on-line can be indicative of a wide range of personal characteristics, including gender, age (Schler et al., 2006), personality (Schwartz et al., 2013), political ideology (Sylwester and Purver, 2015) and occupational class (Preot¸iuc-Pietro et al., 2015a). Several user profiling models that predict these characteristics have been developed, some of which have accuracy that exceeds human performance (Burger et al., 2011; Youyou et al., 2015). User profiling models have applications such as gendered behaviour analysis (Purohit et al., 2015) and bias reduction in predictive models (Culotta, 2014). Previous work on user profiling has traditionally relied on profiles annotated with self-reported characteristics for training data. This can be difficult to acquire in large quantities and forms a bottleneck in the development of user profiling systems. Recently, approaches have attempted to build user profiling datasets through other means. Preot¸iucPietro et al. (2015a)(2015b) extracted known job titles from Twitt"
W16-5606,P09-1080,0,0.0213685,"classifications in each data set. A classification pipeline was created, taking each user’s corpus of tweets as input, tokenized using a 5 http://openstreetmap.org/ Twitter aware tokenizer (Gimpel et al., 2011). TFIDF transformed word n-grams (1- and 2-grams) were used as features for a multi-class Support Vector Machine (SVM) with a linear kernel. n-grams and SVMs were chosen as they have been shown to consistently perform well at user profiling tasks, both for social media (Rao and Yarowsky, 2010; Rout et al., 2013; Schwartz et al., 2013) and other types of text (Boulis and Ostendorf, 2005; Garera and Yarowsky, 2009), and are as such a useful tool to establish baseline performance. Balanced sets were extracted from the OAC-P and LAC-P datasets with 2000 members per label in both cases. 10-fold crossvalidation was used for all experiments. 5.1 Results The results of the SVM classifier are presented in Table 1, compared with results from a random baseline. Prediction of both OAC and LAC outperform the random baseline, indicating that the training dataset described in this article can be used to create valuable user profiling systems. Results for LAC are encouraging and indicate that it is possible to achiev"
W16-5606,P11-2008,0,0.109487,"Missing"
W16-5606,P15-1169,0,0.1581,"Missing"
